et al., 2020) significantly enhances the capabilities of large language models (LLMs) by integrating external, non-parametric knowledge provided by retrievers. In RAG framework, the retriever lo- cates and looks up useful documents based on a given query, and then the LLM interacts with these retrieved results to generate a response. The coordi- nation of retrieval and generation achieves impres- sive performance without additional training. Espe- cially in domain-specific and knowledge-intensive *Corresponding author. The source code is available at https://github.com/yefd/RRAG.git. RAG R2AG Retriever LLM Combine Query & DocumentsQuery Top-k Documents R2-Former / Retriever LLM Combine Query & DocumentsQuery Top-k Documents Retrieval-aware Prompting Semantic Gap Figure 1: A comparison between RAG and R 2AG. R2AG employs a trainable R 2-Former to bridge the semantic gap between retrievers and LLMs. Optionally, LLMs can be fine-tuned to understand the retrieval in- formation further. tasks, RAG offers real-time knowledge with high interpretability to LLMs, effectively mitigating the hallucination problem (Mallen et al., 2023). However, there exists a semantic gap between re- trievers and LLMs due to their vastly different train- ing objectives and architectures (BehnamGhader et al., 2022). Specifically, retrievers, typically en- coder architecture, are designed to retrieve the most relevant documents for a query (Zhu et al., 2023b). Conversely, LLMs, generally decoder architecture, are expected to answer questions based on their inherent knowledge or given documents. How- ever, the interaction between retrievers and LLMs in RAG primarily relies on simple text concatena- tion (BehnamGhader et al., 2022). This poor com- munication strategy will lead to several challenges for LLMs. Externally, it is hard for LLMs to uti- lize more information from retrievers in separate processes. In RAG, the retrieved documents that only preserve sequential relationships are unidirec- tionally delivered to LLMs, and LLMs do not fully understand why retrievers provide the documents. arXiv:2406.13249v2 [cs.CL]