been extended to various domains with modality- specific retrievers, including audios (Koizumi et al., 2020), images (Yasunaga et al., 2023), knowledge graphs (He et al., 2024), and so on. Despite its rapid growth, RAG suffers several limitations, such as sensitivity to retrieval results, increased com- plexity, and a semantic gap between retrievers and LLMs (Kandpal et al., 2022; Zhao et al., 2024). 2.2 Enhanced RAG Recent works develop many enhanced approaches based on the standard RAG framework. To directly improve the effectiveness of RAG, REPLUG (Shi et al., 2023) and Atlas (Izacard et al., 2022) lever- age the LLM to provide a supervisory signal for training a better retriever. However, the noise will inevitably appear in retrieval results (Barnett et al., 2024). Recent studies focus on pre-processing the retrieved documents before providing them to LLMs. Techniques such as truncation and se- lection are effective methods to enhance the qual- ity of ranking lists without modifying the content of documents (Gao et al., 2023; Xu et al., 2024). CRAG (Yan et al., 2024) trains a lightweight re- trieval evaluator to exclude irrelevant documents. BGM (Ke et al., 2024) is proposed to meet the preference of LLMs by training a bridge model to re-rank and select the documents. Some studies aim to train small LMs to compress the retrieval documents, thus decreasing complexity or reducing noise. Jiang et al. (2023) propose LongLLMLin- gua to detect and remove unimportant tokens. RE- COMP (Xu et al., 2023) adopts two compressors to select and summarize the retrieved documents. However, the pre-processing methods introduce ad- ditional computational costs during inference and may lead to the loss of essential information. Notably, the above methods target providing higher-quality retrieval results to LLMs and ac- tually treat retrieval and generation as two dis- tinct processes. This separation fails to