R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation Fuda Y e1, Shuangyin Li 1,* , Y ongqi Zhang2, Lei Chen 2,3 1School of Computer Science, South China Normal University 2The Hong Kong University of Science and Technology (Guangzhou) 3The Hong Kong University of Science and Technology fudayip@m.scnu.edu.cn, shuangyinli@scnu.edu.cn, yongqizhang@hkust-gz.edu.cn, leichen@cse.ust.hk Abstract Retrieval augmented generation (RAG) has been applied in many scenarios to augment large language models (LLMs) with external documents provided by retrievers. However, a semantic gap exists between LLMs and retrievers due to differences in their training objectives and architectures. This misalign- ment forces LLMs to passively accept the documents provided by the retrievers, leading to incomprehension in the generation process, where the LLMs are burdened with the task of distinguishing these documents using their in- herent knowledge. This paper proposes R2AG, a novel enhanced RAG framework to fill this gap by incorporating Retrieval information into Retrieval Augmented Generation. Specifically, R2AG utilizes the nuanced features from the retrievers and employs a R2-Former to capture retrieval information. Then, a retrieval-aware prompting strategy is designed to integrate re- trieval information into LLMsâ€™ generation. No- tably, R2AG suits low-source scenarios where LLMs and retrievers are frozen. Extensive ex- periments across five datasets validate the effec- tiveness, robustness, and efficiency of R 2AG. Our analysis reveals that retrieval information serves as an anchor to aid LLMs in the gener- ation process, thereby filling the semantic gap. 1 Introduction Retrieval augmented generation (RAG) (Lewis et al., 2020) significantly enhances the capabilities of large language models (LLMs) by integrating external, non-parametric knowledge provided by retrievers. In RAG framework, the retriever lo- cates and looks up useful documents based on a given query, and then the LLM interacts with these retrieved results to generate a response. The coordi- nation of retrieval and generation achieves impres- sive