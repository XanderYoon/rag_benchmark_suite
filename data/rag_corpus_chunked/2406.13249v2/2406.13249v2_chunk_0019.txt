(Izacard et al., 2022), and Ope- nAI Embedding models (small and large) (Nee- lakantan et al., 2022), with 1024, 768, 768, 1536, and 3072 dimensions, respectively. Note that Ope- nAI Embedding models are closed-source. From the results presented in Figure 4, we easily observe that a stronger retriever leads to better performance, both standard RAG and R2AG. Importantly, R2AG significantly enhances the effectiveness of LLMs, even when the retrieval performance is poor. We conduct experiments on HotpotQA, MuSiQue, and 2Wiki datasets using LLaMA213B as the foundation LLM. Results shown in Figure 5 indicate that R 2AG13B outperforms R 2AG7B, particularly in the accuracy metric. Specially, there is a decline performance in F1 scores for HotpotQA and MuSiQue datasets. We find this primarily because larger LLMs usually tend to output longer answers with explanations (the average response token count in HotpotQA dataset for R 2AG7B is 37.44, compared to 49.71 for R2AG13B). This tendency also can be observed from the results of ChatGPT and GPT4. These results reveal that both a stronger LLM and a more effective retriever lead to better perfor- mance, validating that R2AG is a genetic method that can be efficiently applied in various scenarios. The Effect of Retrieval Information. For a deeper and more intuitive exploration of how re- trieval information improves LLMs’ generation, we present a visualization of the self-attention dis- tribution in R2AG compared with standard RAG. In detail, we analyze a case in NQ-10 dataset in which the foundation LLM is LLaMA27B. We ex- tract the self-attention weights in different layers from LLM’s outputs and visualize the last token’s attention distribution for other tokens. The relevant document is ranked in position 2 in our selected case, while the 1st document is potentially confus- ing. For a clear illustration, we select attention distribution for tokens