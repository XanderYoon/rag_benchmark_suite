the ability of the foundation LLM, and more powerful closed-source LLMs may not be compatible with R2AG. Thirdly, there may be other informative features besides the three retrieval fea- tures - relevance, precedent similarity, and neighbor similarity scores. Lastly, R2AG is evaluated on five datasets, of which relevant documents are provided. However, situations where no relevant documents are available need to be considered. R 2AG may benefit from integrating techniques like self-RAG to better handle such situations. Ethics Statement LLMs can generate incorrect and potentially harm- ful answers. Our proposed method aims to alle- viate this issue by providing LLMs with retrieved documents and retrieval information, thereby en- hancing LLMsâ€™ capability of generation. In the development and execution of our work, we strictly adhered to ethical guidelines established by the broader academic and open-source community. All the datasets and models used in this work are pub- licly available. No conflicts of interest exist for any of the authors involved in this work. Acknowledgments This work was supported by Major Program of National Language Committee (WT145-39), Natural Science Foundation of Guangdong (2023A1515012073) and National Natural Science Foundation of China (No. 62006083). References OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, and et al. 2023. Gpt-4 technical report. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. ArXiv, abs/2310.11511. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, and et al. 2023a. Qwen technical report. ArXiv, abs/2309.16609. Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao- han Zeng, Lei Hou, and et al. 2023b. Longbench: A bilingual,