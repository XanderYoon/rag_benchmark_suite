a maximum context length of 4k tokens. For NQ-20 and NQ-30 datasets, LongChat1.5 7B is selected as the foundation LLM, which extends the context window to 32k tokens. For DuReader dataset, Qwen1.50.5B is the foundation LLM, also with a maximum context length of 32k tokens. These methods were categorized into two groups – frozen and fine-tuned – based on whether they require training the LLMs. The implementation details are in Appendix D. 4.3 Main Results Table 1 and Table 2 provide the main results. We can obtain the following conclusions: (1) Compared with foundation LLMs using stan- dard RAG, R2AG can significantly increase perfor- mance. Even in multi-hot datasets, R2AG improves LLMs’ ability for complex reasoning. In DuReader dataset, with a token length of 16k, R2AG remains effective, demonstrating its robustness and effi- ciency in handling extensive text outputs. These re- sults indicate that R2AG effectively enables LLMs to better understand the retrieval information and Methods NQ-10 NQ-20 LLaMA27B LongChat1.57B R2AG 0.6930 0.7062 w/o r 0.6761 (↓2.45%) 0.6798 (↓3.73%) w/o γ 0.6723 (↓2.99%) 0.6930 (↓1.87%) w/o ζ 0.6252 (↓9.78%) 0.6855 (↓2.93%) w/o LQDM 0.6441 (↓7.07%) 0.7043 (↓0.27%) Table 3: Ablation studies on NQ-10 and NQ-20 datasets. GT T op1 T op2 T op3 T op4 T op5 T op6 T op7 T op8 T op9T op10 0.4 0.5 0.6 0.7 0.8Metric Learnable T okens LLaMA27B Mean Figure 3: Performance of learnable tokens across dif- ferent document counts on NQ-10 dataset. “GT”means only retaining ground-true documents. boosts their capabilities in handling provided doc- uments. (2) Compared with other LLMs using standard RAG, R2AG generally achieves better per- formance except for closed-source LLMs. GPT4 shows superior results in most datasets, establish- ing it as a strong baseline. Notably, R 2AG ex- cels ChatGPT in NQ and HotpotQA datasets. Us- ing LLaMA27B as