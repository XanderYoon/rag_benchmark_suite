classification head that outputs the relevance predictions ˆs. Supporting s={si}k i=1 are the ground-truth labels for docu- ments, we use cross-entropy as the loss function, defined as: LQDM (s, ˆs) = − kX i=1 si log(ˆsi)+(1−si) log(1−ˆsi). (8) For LLM alignment, we utilize the language modeling (LM) task, which involves learning to generate subsequent tokens based on the preceding context and retrieval information. The language modeling loss LLM aims to maximize the log- likelihood of the tokens, rewarding the LLM for predicting subsequent words correctly. The joint training involves instruction fine- tuning with a linear combination of QDM and LM tasks. The final loss is expressed as: L = LQDM +LLM . (9) Notably, R2AG offers the flexibility to train the R2-Former solely while freezing the LLM or to train both together for a deeper understanding of retrieval information. The decision represents a trade-off between lower computational costs and higher accuracy in real-world scenarios. 4 Experiments 4.1 Datasets and Metrics We evaluate R 2AG on five datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019), Hot- potQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2021), 2WikiMultiHopQA (2Wiki) (Ho et al., 2020), and DuReader (He et al., 2018). For NQ dataset, we utilize NQ-10, NQ-20, and NQ-30 datasets built by Liu et al. (2023), which contain 10, 20, and 30 total documents, respectively. DuReader is a multiple documents QA version built by Bai et al. (2023b). Detailed introduction and statistics are shown in Appendix C. Following Mallen et al. (2023); Liu et al. (2023), we adopt accuracy (Acc) as the evaluation met- ric for NQ datasets. Following Bai et al. (2023b), we adopt accuracy (Acc) and F1 score as evalua- tion metrics for HotpotQA, MuSiQue, and 2Wiki datasets. For DuReader dataset, we measure per- formance by F1 score and Rouge (Lin,