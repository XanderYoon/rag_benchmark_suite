dataset in which the foundation LLM is LLaMA27B. We ex- tract the self-attention weights in different layers from LLM’s outputs and visualize the last token’s attention distribution for other tokens. The relevant document is ranked in position 2 in our selected case, while the 1st document is potentially confus- ing. For a clear illustration, we select attention distribution for tokens in top-4 documents. From Figure 6, it is evident that the retrieval informa- tion receives higher attention scores even in deeper layers, and the relevant document can get more at- tention within 1-4 layers. That means the retrieval information effectively acts as an anchor, guiding the LLM to focus on useful documents. 5 Conclusion and Future Work This paper proposed a novel enhanced RAG frame- work named R2AG to bridge the semantic gap be- tween the retrievers and LLMs. By incorporating retrieval information from retrievers into LLMs’ generation process, R2AG captures a comprehen- sive understanding of retrieved documents. Experi- mental results show that R2AG outperforms other competitors. In addition, the robustness and effec- tiveness of R2AG are further confirmed by detailed analysis. In future work, more retrieval features could be applied to R2AG framework. Limitations The following are the limitations associated with R2AG: First, R2AG depends on the semantic rep- resentations modeled by encoder-based retrievers. The suitability of other types of retrievers, such as sparse and cross-encoder retrievers, requires further exploration. Secondly, as mentioned in Section 4.5, R2AG relies on the ability of the foundation LLM, and more powerful closed-source LLMs may not be compatible with R2AG. Thirdly, there may be other informative features besides the three retrieval fea- tures - relevance, precedent similarity, and neighbor similarity scores. Lastly, R2AG is evaluated on five datasets, of which relevant documents are provided. However, situations where no relevant documents are available need