similarity. Then, the feature list {inputi}k i=1 is then fed into R2-Former to further exploit retrieval information. 3.3 R 2-Former Inspired by Li et al. (2023b), we propose the R 2- Former as the trainable module that bridges be- tween retrievers and LLMs. As shown in the right side of Figure 2, R 2-Former is a pluggable Transformer-based model that accepts list-wise fea- tures as inputs and outputs retrieval information. To better comprehend list-wise features from re- trievers, we employ an input embedding layer to linearly transform input features into a higher di- mension space. Positional embeddings are then added before attention encoding to maintain se- quence awareness. Then, a Transformer (Vaswani et al., 2017) encoder is utilized to exploit the input sequences, which uses a self-attention mask where each position’s feature can attend to other positions. Formally, for an input list {inputi}k i=1, the process is formulated by: H = fatt h f→h1  {inputi}k i=1  +p i , (2) where fatt is the Transformer encoder with h1 hid- den dimension, f→h1 is a linear mapping layer, and p ∈ Rk×h1 represents trainable positional embed- dings. The output embeddings H ∈ Rk×h1 thus contain the deeper retrieval information and will be delivered to the LLM’s generation. 3.4 Retrieval-Aware Prompting In the generation process, it is crucial for the LLM to utilize the retrieval information effectively. As shown in the upper part of Figure 2, we introduce a retrieval-aware prompting strategy that injects the retrieval information extracted by R2-Former into the LLM’s generation process. First, we employ a projection layer to linearly transform the retrieval information into the same dimension as the token embedding layer of the LLM. Formally, this is represented as: ER = f→h2(H) = {eR i }k i=1, (3) where f→h2 is a linear projection layer