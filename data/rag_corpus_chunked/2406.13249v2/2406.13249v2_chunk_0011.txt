final input embeddings can be arranged as: E = [ eq 1, · · · , eq nq | {z } query , eR 1 , ed1 1 , · · · , ed1nd1| {z } document1 , · · · , eR k , edk 1 , · · · , edkndk| {z } documentk ], (5) where eR i denotes the retrieval information embed- ding for the i-th document. In this way, the re- trieval information of corresponding document can be well mixed, reducing the burden of the LLM to process all documents. Finally, we can get the responses by: ˆy = fG(E), (6) where ˆy represents the LLM-generated results. No- tably, this part simplifies the instruction prompt, and detailed descriptions and prompt templates can be found in Appendix B. 3.5 Training Strategy As the interdependence of retrieval and generation, we integrate R 2-Former training and LLM align- ment into one stage. The joint training allows R2- Former to better understand list-wise features from the retriever, ensuring retrieval information can be deeply interpreted by the LLM. For R 2-Former training, we perform a query- document matching (QDM) task that enforces R2- Former to learn the relevance relationships from list-wise features. In detail, it is a binary classi- fication task that asks to model each document’s relevance to the query. The formula for prediction is as follows: ˆs = f→1(H) = {ˆsi}k i=1, (7) where f→1 is a binary classification head that outputs the relevance predictions ˆs. Supporting s={si}k i=1 are the ground-truth labels for docu- ments, we use cross-entropy as the loss function, defined as: LQDM (s, ˆs) = − kX i=1 si log(ˆsi)+(1−si) log(1−ˆsi). (8) For LLM alignment, we utilize the language modeling (LM) task, which involves learning to generate subsequent tokens based on the preceding context