et al., 2023) adopts two compressors to select and summarize the retrieved documents. However, the pre-processing methods introduce ad- ditional computational costs during inference and may lead to the loss of essential information. Notably, the above methods target providing higher-quality retrieval results to LLMs and ac- tually treat retrieval and generation as two dis- tinct processes. This separation fails to bridge the semantic gap between retrievers and LLMs fully. Some approaches (Deng et al., 2023; Sachan et al., 2021) enhance LLM comprehension abilities by in- corporating documents into latent representations. However, these methods are typically designed for encoder-decoder LLMs, and constrain their suit- ability for prevailing decoder-only LLMs. While joint modeling methods (Glass et al., 2022; Izac- ard et al., 2024) benefit from the joint optimiza- tion of LLMs and retrievers, they need extra train- ing to align semantic spaces, which may hamper the generality of LLMs (Zhao et al., 2024). Com- pared with these joint modeling methods, a key difference is that R2AG offers a cost-effective and non-destructive manner to bridge the semantic gap between LLMs and retrievers. 3 R 2AG 3.1 Problem Formulation and Overview RAG involves the task that aims to prompt an LLM to generate answers based on a query and documents returned by a retriever. For- mally, given a query q and a list of documents D={d1, d2, · · · , dk} in preference order ranked by the retriever fR, the LLM, a generator fG, is ex- pected to generate the output ˆy. The pipeline can be expressed as: ˆy = fG (P (q, D)) , (1) where P is a predefined prompt template. It shows the retrievers and LLMs are couple in a simplistic prompt-based method, which will lead to miscom- munication and the semantic gap. Figure 2 illustrates the overall framework of