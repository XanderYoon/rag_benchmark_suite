For the retrieval task, we utilize the Sentence- Transformer (Reimers and Gurevych, 2019) to fine- tune a BERT (Devlin et al., 2019) model as the re- triever, which is a siamese dual encoder with shared parameters. The models “bert-base-uncased” and “bert-base-chinese”are used for English datasets and the Chinese dataset, respectively. All retrievers adopt default hyper-parameter settings with 768 embedding dimensions. Cosine similarity is employed as the scoring function for retrieval and feature extraction. The retrieval performance across datasets is shown in Table 4. Contrary to some works (Liu et al., 2023; Jiang et al., 2023) that artificially place ground truth documents in fixed positions, this paper considers that candidate documents are ranked by the retriever to simulate real-world scenarios. For R2-Former, we determine the learning rate as 2e-4 and dropout as 0.1. The number of attention heads and hidden size in Transformer encoder are 4 and 256, respectively. Adam (Kingma and Ba, 2014) is adopted as the optimization algorithm. For LLMs, all methods use default settings and adopt greedy decoding for fair comparison. The ChatGPT version is“gpt-3.5-turbo-0125”with a 16k context window size, and the GPT4 version is “gpt-4-turbo-2024-04-09”with a 128k context window size. In CRAG, the retrieval evaluator only triggered{Correct, Ambiguous} actions to next knowledge refinement process as there are at least one relevant document in retrieval results. In the RAFT method, we employ LoRA (Hu et al., 2021) to effectively fine-tune LLMs, with LoRA rank set at 16, alpha at 32, and dropout at 0.1. Methods Prompts RAG Write a high-quality answer for the given question using only the provided search results (some of which might be irrelevant). Only give me the answer and do not output any other words. [1]{#d1} [2]{#d2} ... [k]{#dk} Only give me the answer and do not output any other words. Question: {#q} Answer: