shown in Appendix C. Following Mallen et al. (2023); Liu et al. (2023), we adopt accuracy (Acc) as the evaluation met- ric for NQ datasets. Following Bai et al. (2023b), we adopt accuracy (Acc) and F1 score as evalua- tion metrics for HotpotQA, MuSiQue, and 2Wiki datasets. For DuReader dataset, we measure per- formance by F1 score and Rouge (Lin, 2004). 4.2 Baselines To fully evaluate R2AG, we compared two types of methods: standard RAG using various LLMs, and enhanced RAG using the same foundation LLM. First, we evaluate standard RAG baselines where LLMs generate responses given the query prepended with retrieved documents. For English datasets, we use several open-source LLMs, includ- ing LLaMA27B, LLaMA213B, LLaMA38B (Tou- vron et al., 2023), and LongChat1.5 7B (Li et al., 2023a). Besides, we adopt ChatGPT (Ouyang et al., 2022) and GPT4 (Achiam et al., 2023) as baselines of closed-source LLMs. For the Chinese dataset, Methods NQ-10 NQ-20 NQ-30 HotpotQA MuSiQue 2Wiki Acc Acc Acc Acc F1 Acc F1 Acc F1 Frozen LLMs LLaMA27B 0.3898 - - 0.2630 0.0852 0.0546 0.0241 0.1205 0.0634 LongChat1.57B 0.6045 0.5782 0.5198 0.5424 0.3231 0.2808 0.1276 0.3882 0.2253 LLaMA38B 0.5141 0.4991 0.5311 0.5901 0.2056 0.2427 0.0891 0.4723 0.1952 LLaMA213B 0.7684 - - 0.3788 0.1000 0.0909 0.0446 0.2405 0.0898 ChatGPT 0.6886 0.6761 0.6347 0.6557 0.6518 0.3376 0.3321 - - GPT4 0.7759 0.7514 0.7514 0.7673 0.6026 0.4853 0.3270 - - CoT 0.4482 0.6026 0.5631 0.2365 0.1028 0.0626 0.0412 0.1627 0.0969 RECOMP 0.0169 0.2222 0.1977 0.2388 0.0265 0.0830 0.0156 0.2666 0.0329 CRAG 0.3974 0.6441 0.6347 0.1194 0.0360 0.0262 0.0047 0.0768 0.0422 LongLLMLingua 0.3635 - - 0.4174 0.1178 0.1939 0.0477 0.2374 0.0888 R2AG 0.6930 0.7062 0.6704 0.6675 0.3605 0.1864 0.1687 0.3342 0.3452 Fine-tuned LLMs Self-RAG 0.1883 - - 0.2475 0.1236 0.0701 0.0378 0.2611 0.1389 RAFT 0.7514 0.8041 0.7307 0.7349 0.3172 0.2529 0.1502 0.7555