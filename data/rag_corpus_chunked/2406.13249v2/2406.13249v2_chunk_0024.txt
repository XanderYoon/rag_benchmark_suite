Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane A. Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Atlas: Few-shot learning with retrieval aug- mented language models. ArXiv, abs/2208.03299. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. ArXiv, abs/2310.06839. Nikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2022. Large language models strug- gle to learn long-tail knowledge. In International Conference on Machine Learning. Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Bridg- ing the preference gap between retrievers and llms. ArXiv, abs/2401.06954. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980. Yuma Koizumi, Yasunori Ohishi, Daisuke Niizumi, Daiki Takeuchi, and Masahiro Yasuda. 2020. Au- dio captioning using pre-trained large-scale language model guided by audio-based similar caption re- trieval. ArXiv, abs/2012.07331. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, and et al. 2019. Natural questions: A benchmark for question answer- ing research. Transactions of the Association for Computational Linguistics, 7:453â€“466. Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Kuttler, Mike Lewis, and et al. 2020. Retrieval- augmented generation for knowledge-intensive nlp tasks. ArXiv, abs/2005.11401. Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian- min Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a. How long can open- source LLMs truly promise on context length? Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Sil- vio Savarese, and Steven C. H. Hoi. 2022. Lavis: A library for language-vision intelligence. ArXiv, abs/2209.09019. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023b. Blip-2: Bootstrapping language-image pre-training