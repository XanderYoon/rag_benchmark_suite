prompting strategy that injects the retrieval information extracted by R2-Former into the LLM’s generation process. First, we employ a projection layer to linearly transform the retrieval information into the same dimension as the token embedding layer of the LLM. Formally, this is represented as: ER = f→h2(H) = {eR i }k i=1, (3) where f→h2 is a linear projection layer via an MLP layer, and h2 is the dimension of LLM’s token embedding layer. Then, we tokenize the query and documents us- ing LLM’s tokenizer and convert them into embed- dings. For example, a document d is tokenized into td={td j }nd j=1, where td j is the j-th token in the docu- ment, nd is the number of tokens in the documentd. And the token embeddings can be transformed by a lookup in the token embedding layer. The process can be expressed as: Ed = femb  td  = {ed j }nd j=1, (4) where femb is the token embedding layer of the LLM, and Ed ∈ Rnd×h2 is the embeddings of document d. A similar process is applied to obtain the query embeddings Eq = {eq j }nq j=1, where nq is the number of query tokens. For nuanced analysis of each document, the cor- responding retrieval information embeddings are then prepended to the front of each document’s embeddings. They are external knowledge and function as an anchor, guiding the LLM to focus on useful documents. The final input embeddings can be arranged as: E = [ eq 1, · · · , eq nq | {z } query , eR 1 , ed1 1 , · · · , ed1nd1| {z } document1 , · · · , eR k , edk 1 , · · · , edkndk| {z } documentk ], (5) where eR