i-th document to its adjacent documents. Specifically, the neighbor similarity of a case in the ranking list is given by: ζi = (sim(xd 1, xd 2), i = 1 [sim(xd i−1, xd i ) + sim(xd i , xd i+1)]/2, i ∈ [2, k) sim(xd k−1, xd k), i = k , (12) where ζi represents the average similarity of i-th document to its adjacent documents. Such that we can get the list-wise features among documents. B Prompt Templates In R2AG, retrieval information, we append k spe- cial tokens (“<R>”) in front of each document to facilitate the incorporation of retrieval information. These tokens do not carry meaningful semantics but serve as placeholders for the retrieval informa- tion within the prompt. This special token facili- tates the integration of retrieval information into the generation process. Table 5 shows the prompt templates for R2AG and other baselines. The prompt templates of DuReader dataset can be found in our source code. C Dataset Introduction We conduct evaluations on five datasets, including: Natural Questions (NQ) (Kwiatkowski et al., 2019) is developed from Google Search and con- tains questions coupled with human-annotated an- swers extracted from Wikipedia. Further, Liu et al. (2023) collectk−1 distractor documents from Wikipedia that do not contain the answers, where k is the total document number for each question. This dataset has three versions: NQ-10, NQ-20, and NQ-30, with total document numbers of 10, 20, and 30, respectively. HotpotQA (Yang et al., 2018) is a well-known multi-hop question answering dataset based on Wikipedia. This dataset involves questions requir- ing finding and reasoning over multiple supporting facts from 10 documents. There are two reasoning types of questions: bridging and comparison. MuSiQue (Trivedi et al., 2021) has questions that involve 2-4 hops and six types of reasoning chains. The dataset is constructed