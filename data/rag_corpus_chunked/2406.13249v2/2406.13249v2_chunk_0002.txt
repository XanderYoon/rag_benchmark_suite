et al., 2022). This poor com- munication strategy will lead to several challenges for LLMs. Externally, it is hard for LLMs to uti- lize more information from retrievers in separate processes. In RAG, the retrieved documents that only preserve sequential relationships are unidirec- tionally delivered to LLMs, and LLMs do not fully understand why retrievers provide the documents. arXiv:2406.13249v2 [cs.CL] 30 Oct 2024 Particularly, low-quality documents inevitably ap- pear in retrieved results (Barnett et al., 2024), but LLMs have to accept this noise passively. Inter- nally, it is hard for LLMs to handle all of the re- trieved documents with their inherent knowledge. LLMs must process all the results and assess which documents are important, impacting their ability to generate accurate answers (Wu et al., 2024). Moreover, LLMs face the lost-in-middle problem in overly long documents (Liu et al., 2023), leading to further misunderstanding. Unfortunately, existing enhanced RAG methods, including pre-processing approaches (Izacard et al., 2022; Yan et al., 2024; Asai et al., 2023; Ke et al., 2024) and compression-based approaches (Yan et al., 2024; Xu et al., 2023; Jiang et al., 2023), do not recognize this semantic gap between retriev- ers and LLMs. They remain to treat retrieval and generation as separate processes and directly add processed or compressed documents into the inputs for LLMs. These strategies ignore the semantic connections necessary for deeper comprehension, which may lead to potentially misleading LLMs even with perfect retrievers. To address these challenges, it is essential to bridge the semantic gap between retrievers and LLMs. As previously mentioned, retrievers can provide high-quality semantic representations that can be beneficial for catching nuanced differences among documents (Zhao et al., 2022). Thus, our in- tuition is to exploit these semantic representations as additional knowledge, empower LLMs to gain a deeper comprehension of the retrieved documents,