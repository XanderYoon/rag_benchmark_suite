frame- work, to incorporate retrieval information into retrieval augmented generation. Notably, R2AG is compatible with low-source scenar- ios where retrievers and LLMs are frozen. • We design a lightweight model, R 2-Former, to bridge the semantic gap between retrievers and LLMs. R2-Former can be seamlessly in- tegrated into existing RAG frameworks using open-source LLMs. • We introduce a retrieval-aware prompting strategy to inject retrieval information into the input embeddings, enhancing LLMs’ ability to understand relationships among documents without much increase in complexity. Experimental results demonstrate the superior per- formance and robustness of R2AG in various sce- narios. Our analysis shows that R 2AG increases latency by only 0.8% during inference. Further- more, it demonstrates that retrieval information anchors LLMs to understand retrieved documents and enhances their generation capabilities. 2 Related Works 2.1 Retrieval Augmented Generation Despite being trained on vast corpora, LLMs still struggle with hallucinations and updated knowl- edge in knowledge-sensitive tasks (Zhao et al., 2023). RAG (Lewis et al., 2020) is regarded as an efficient solution to these issues by combining a re- trieval component with LLMs. In detail, documents gathered by retrievers are bound with the original query and placed into the inputs of LLMs to pro- duce final responses. RAG allows LLMs to access vast, up-to-date data in a flexible way, leading to better performance. Benefiting from the progress of multi-modal alignment techniques (Li et al., 2023b; Zhu et al., 2023a), the idea of RAG has been extended to various domains with modality- specific retrievers, including audios (Koizumi et al., 2020), images (Yasunaga et al., 2023), knowledge graphs (He et al., 2024), and so on. Despite its rapid growth, RAG suffers several limitations, such as sensitivity to retrieval results, increased com- plexity, and a semantic gap between retrievers and LLMs (Kandpal et al., 2022; Zhao et