gen- eration. Moreover, to suit various retrievers, it is intuitive to transform representations in different spaces into unified format features. Inspired by works in retrieval downstream tasks (Ma et al., 2022; Ye and Li, 2024), we align these representations into retrieval features by com- puting relevance, precedent similarity, and neigh- Query & Documents Query <R> Document1, ... , <R> Documentk Combine R2-Former MLP Lookup Training ObjectiveRetrieval-aware Prompting ... Frozen Not Frozen Feature Extraction Input Embedding Transformer Encoder PE input1 inputkinput2 R2-Former ... LLM/ Query Emb Retrieval Info1 Emb Document1 Emb Retrieval Infok Emb Documentk Emb... Retriever QueryDocumenti Precedent Neighbor inputi Figure 2: An illustration of R 2AG. The R2-Former is designed to extract retrieval features, acting as an information bottleneck between retrievers and LLMs. Through the retrieval-aware prompting strategy, the retrieval information serves as an anchor to guide LLMs during generation. “Emb”is short for embedding, “PE”stands for positional embeddings, and“<R>”denotes the placeholder for retrieval information. bor similarity scores. Specifically, these scores are calculated by a similarity function such as dot prod- uct or cosine similarity. The relevance score ri is between the query and the i-th document and is also used to sort the documents. The precedent and neighbor similarity scores are computed between the i-th document representation and its precedent- weighted and adjacent representations, respectively. Detailed formulations are provided in Appendix A. Finally, three features are concatenated as input: inputi={ri, γi, ζi}, representing relevance, prece- dent similarity, and neighbor similarity. Then, the feature list {inputi}k i=1 is then fed into R2-Former to further exploit retrieval information. 3.3 R 2-Former Inspired by Li et al. (2023b), we propose the R 2- Former as the trainable module that bridges be- tween retrievers and LLMs. As shown in the right side of Figure 2, R 2-Former is a pluggable Transformer-based model that