0.0169 0.2222 0.1977 0.2388 0.0265 0.0830 0.0156 0.2666 0.0329 CRAG 0.3974 0.6441 0.6347 0.1194 0.0360 0.0262 0.0047 0.0768 0.0422 LongLLMLingua 0.3635 - - 0.4174 0.1178 0.1939 0.0477 0.2374 0.0888 R2AG 0.6930 0.7062 0.6704 0.6675 0.3605 0.1864 0.1687 0.3342 0.3452 Fine-tuned LLMs Self-RAG 0.1883 - - 0.2475 0.1236 0.0701 0.0378 0.2611 0.1389 RAFT 0.7514 0.8041 0.7307 0.7349 0.3172 0.2529 0.1502 0.7555 0.4869 R2AG+RAFT 0.8192 0.8060 0.7458 0.7351 0.3056 0.2295 0.1533 0.7444 0.6351 Table 1: Main results on four English datasets. All enhanced RAG methods utilize the same foundation LLMs, with results marked in gray background indicating the performance of these foundation LLMs. Results in gray represent the performance of closed-source LLMs. Results in bold and results in underlined mean the best and second-best performance among current classified methods. Methods DuReader F1 Rouge Frozen LLMs LongChat1.57B 0.0914 0.1181 Qwen1.50.5B 0.1395 0.1656 Qwen1.51.8B 0.1533 0.1570 InternLM21.8B 0.1330 0.1391 R2AG 0.1510 0.1663 Fine-tuned LLMs RAFT 0.2423 0.2740 R2AG+RAFT 0.2507 0.2734 Table 2: Performance comparison on DuReader dataset. we employ Qwen1.50.5B, Qwen1.51.8B (Bai et al., 2023a) and InternLM21.8B (Cai et al., 2024). Secondly, we experiment with several meth- ods that can enhance RAG, including CoT (Wei et al., 2022), RECOMP (Xu et al., 2023), CRAG (Yan et al., 2024), Self-RAG (Asai et al., 2023), LongLLMLingua (Jiang et al., 2023), and RAFT (Zhang et al., 2024). For NQ-10, HotpotQA, MuSiQue, and 2Wiki datasets, we use LLaMA27B as the foundation LLM for enhanced RAG methods, which has a maximum context length of 4k tokens. For NQ-20 and NQ-30 datasets, LongChat1.5 7B is selected as the foundation LLM, which extends the context window to 32k tokens. For DuReader dataset, Qwen1.50.5B is the foundation LLM, also with a maximum context length of 32k tokens. These methods were categorized into two groups – frozen and fine-tuned – based on whether