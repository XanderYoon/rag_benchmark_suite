the question; (2) identifying passages that contain relevant information for each requirement; and (3) selecting a subset of passages that collectively pro- vide the most comprehensive and diverse coverage to effectively answer the query. By prompting Large Language Models (LLMs) with both the question and a set of candidate pas- sages retrieved in an earlier retrieval stage, this method enables fine-grained analysis and effec- tive set selection. In contrast to zero-shot listwise reranking, our approach imposes no constraint on the inclusion of all candidate passages and does not enforce any ranking or ordering in the final selection. 3.3 Model Distillation To ensure the proposed approach is practical for real-world applications, we train SETR, a distilled model fine-tuned for the set-wise passage selec- tion task through information requirement identi- fication. While proprietary LLMs exhibit strong performance, their cost and latency would make their use in real-time search systems impractical. Instead, we distill step-by-step reasoning ability into a specialized, lightweight model for efficiency. 3.3.1 Data Construction For distillation, we construct a dataset based on 40K training questions 1 from Pradeep et al. (2023b), originally derived from the MS MARCO v1 passage ranking dataset (Sun et al., 2024). Each query is paired with the top-20 retrieved candidate passages. We then apply set-wise passage selection to generate teacher-labeled selections, which are subsequently distilled into our student model. To perform this labeling, we use GPT-4o with a zero- shot prompting approach. Following Pradeep et al. (2023a), we replaced all instances of [n] in pas- sages with (n) to prevent model confusion during data synthesis and inference. We used the fix_text function from ftfy2 to preprocess all inputs before feeding them into the model. 3.3.2 Training We adopt Llama-3.1-8B-Instruct 4 as the base model and train it using a standard supervised fine- tuning approach. The input