as code generation or conversational AI. While we antic- ipate broader applicability, further evaluation on diverse tasks is needed. Lastly, our approach relies on the quality of the underlying language model for reasoning-based selection. While chain-of-thought reasoning im- proves passage selection, its effectiveness depends on the LLM’s ability to accurately analyze and syn- thesize information. References Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, Mohammed Ali, and Adam Jatowt. 2025. Rankify: A comprehensive python toolkit for re- trieval, re-ranking, and retrieval-augmented gener- ation. arXiv preprint arXiv:2502.02464. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. Preprint, arXiv:2310.11511. Zijian Chen, Ronak Pradeep, and Jimmy Lin. 2024. An early first reproduction and improvements to single- token decoding for fast listwise reranking. Preprint, arXiv:2411.05508. Nadezhda Chirkova, Thibault Formal, Vassilina Nikoulina, and Stéphane Clinchant. 2025. Provence: efficient and robust context pruning for retrieval-augmented generation. arXiv preprint arXiv:2501.16214. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the trec 2020 deep learning track. Preprint, arXiv:2102.07662. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. V oorhees. 2020. Overview of the trec 2019 deep learning track. Preprint, arXiv:2003.07820. Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, and Yulia Tsvetkov. 2024. Don’t hallucinate, abstain: Identifying llm knowl- edge gaps via multi-llm collaboration. Preprint, arXiv:2402.00367. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning , pages 3929–3938. PMLR. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi- hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th Inter- national Conference on Computational Linguistics , pages 6609–6625, Barcelona, Spain (Online). Inter- national Committee on Computational Linguistics. Giwon