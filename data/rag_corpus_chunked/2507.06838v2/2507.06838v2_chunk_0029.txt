used to rescore top-k results in retrieval pipelines. As a strong open-source baseline, it reflects state-of-the-art conventional reranking focused on individual passage relevance. • RankLlama (Ma et al., 2024) is a pointwise reranker based on LLaMA-2 7B model (Tou- vron et al., 2023). Given a query and a can- didate passage, it outputs a score to reorder retrieved documents by their relevance. It demonstrates strong performance in both in- domain and zero-shot settings, serving as a competitive open-source baselines for passage reranking. • RankVicuna (Pradeep et al., 2023a) is a 7B open-source listwise reranker built on the Vicuna 7B model (Zheng et al., 2023). It takes a query and a list of passages as input and outputs a ranked list of passage indices. Trained with GPT generated supervision, it achieves performance comparable to GPT-3.5 on benchmarks like TREC DL (Craswell et al., 2020, 2021), providing a transparent alternative to proprietary rerankers. • RankZephyr (Pradeep et al., 2023b) is a zero- shot listwise reranker built on the Zephyr-7B model (Tunstall et al., 2023). Fine-tuned us- ing GPT-4 generated ranking, it outputs or- dered lists of passage indices given a query and candidate passages. It achieves perfor- mance close to GPT-4 and even surpassing it on some benchmarks. Its open-source nature and reproducibility make it a robust baseline for evaluating listwise reranking methods. • FirstMistral (Chen et al., 2024) is a zero-shot listwise reranker based on Mistral 7B (Jiang et al., 2023). It reframes reranking as a single- token decoding task, enabling fast and ef- ficient passage selection. Despite its sim- plicity, it achieves competitive performance and serves as a strong open-source baseline to assess the raw ranking ability of modern instruction-tuned LLMs. • RankGPT4 (Sun et al., 2024) is a GPT-4- based reranker accessed via OpenAI’s API, used in a zero-shot