Full prompt details are provided in Ap- pendix A.4. • SETR-Selection only is a model trained to generate only the final selected passages with- out any reasoning process. • SETR-CoT is a model trained with general CoT reasoning using a standard “Let’s think step-by-step prompt” prompt, but does not explicitly identifying distinct information re- quirements. • SETR-CoT & IRI is the full model that in- corporates both CoT reasoning and explicit information requirement identification, and performs passage selection accordingly. 4 Experiments In this section, we first introduce the experimental setup (§4.1). Then, we show the results for both the generation (§4.2) and retrieval stages (§4.3), high- lighting the effectiveness of the proposed SETR. More details are shown in Appendix A. Retrieval Model # of HotpotQA 2WikiMultiHopQA MuSiQue MultiHopRAG Passages EM F1 EM F1 EM F1 Accuracy BM25 RETRIEVAL ONLY - 5.00 26.90 25.86 29.79 21.79 5.46 8.22 39.20 RERANKING bge-reranker-large 5.00 29.71 28.08 30.16 21.84 6.12 10.00 42.13 RankLlama 5.00 29.48 27.82 30.30 21.91 6.04 9.26 42.09 RankVicuna 5.00 28.69 27.31 30.46 22.42 5.99 9.03 40.53 RankZephyr 5.00 28.96 27.76 30.29 22.34 6.78 10.03 40.10 FirstMistral 5.00 26.71 26.10 30.15 21.97 5.29 8.42 40.29 RankGPT (gpt-4o) 5.00 30.89 29.24 31.71 23.31 6.91 9.98 44.36 SET SELECTION (OURS) SetR-Selection only 2.95 31.61 30.55 32.22 24.20 8.02 11.07 43.62 SetR-CoT 2.48 30.79 30.12 32.07 24.43 7.03 10.87 41.63 SetR-CoT & IRI 2.63 32.20 30.57 32.17 24.22 6.62 10.57 44.13 bge-large-en-v1.5 RETRIEVAL ONLY - 5.00 30.07 30.97 31.17 25.22 7.44 10.78 41.82 RERANKING bge-reranker-large 5.00 32.48 33.24 31.92 25.47 8.06 12.50 43.50 RankLlama 5.00 31.88 32.95 32.24 25.78 7.61 11.77 43.51 RankVicuna 5.00 32.08 32.83 32.66 26.85 7.78 11.35 42.76 RankZephyr 5.00 31.83 32.97 32.68 26.59 8.02 11.72 41.55 FirstMistral 5.00 30.10 31.07 31.43 25.31 6.53 10.64 42.05 RankGPT (gpt-4o) 5.00 33.85 34.45