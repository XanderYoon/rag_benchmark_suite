To validate the effective- ness of our approach, we conduct extensive evaluations on both the retrieved passage sets and the final generated outputs. Our ex- periments on multi-hop RAG benchmarks demonstrate that our method outperforms both proprietary LLM-based rerankers and open- source alternatives, achieving better answer correctness, while also improving retrieval precision and recall. â€¢ Open-Source Contribution: We release the complete and fully reproducible recipe of SETR, implementing our set-wise passage se- lection approach. We hope this work facil- itates future research and communty-driven advancements in retrieval strategies for RAG systems. 2 Related Work 2.1 Retrieval-Augmented Generation (RAG) Retrieval-Augmented Generation (RAG) systems combine retrieval modules with language models to enhance factual accuracy and reduce hallucina- tions (Lewis et al., 2021; Guu et al., 2020). Stan- dard pipelines typically employ a first-stage re- triever such as BM25 (Robertson and Zaragoza, 2009) or DPR (Karpukhin et al., 2020) fol- lowed by a reranking module that estimates rel- evance via pointwise, pairwise, or listwise strate- gies (Nogueira and Cho, 2019; Qin et al., 2023; Zhuang et al., 2023; Yoon et al., 2024). Recently, Large Language Models (LLMs) have been applied to listwise reranking in RAG systems through prompting and distillation (Sun et al., 2024; Pradeep et al., 2023b), demonstrating strong per- formance. However, these models primarily focus on individual passage relevance and often overlook set-level properties such as diversity or coverage, which are crucial for generating complete and ac- curate answers in multi-hop or compositional ques- tion answering tasks (Tang and Yang, 2024). 2.2 Refinement and Iteration in RAG To address the limitations of ranking-based re- trieval, recent studies have explored retrieval strate- gies better aligned with the needs of RAG. These include adaptive retrieval based on query com- plexity (Jeong et al., 2024), multi-step reasoning with agent-based ranking (Niu et al., 2024),