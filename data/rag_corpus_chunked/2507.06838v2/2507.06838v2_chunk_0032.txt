and Mixtral-8x7B achieve rel- atively low accuracy (0.32 and 0.36, respectively), while GPT-4 attains a significantly higher score of 0.89, highlighting a substantial performance gap. The prompts used for each benchmark are shown in Figure 4 and Figure 5, respectively. Prompt for General Multi-hop QA {context} Based on these texts, answer these questions: Q: {question} A: Figure 4: Prompt template used for general multi-hop QA datasets including HotpotQA, 2WikiMultiHopQA, and MusiQue. A.4 Prompt Templates We provide the prompt template used in our ex- periment for both SETR-CoT and SETR-Selection only. Each prompt consists of a question and a set of passages, where each passage is assigned a unique numerical identifier. In SETR-CoT, the prompt concludes with a CoT reasoning, “Let’s think step by step.”, to encourage intermediate rea- soning before producing the final selection. In con- trast, SETR-Selection only removes this reasoning step and directly instructs the model to output only the final selection without any explanation. Fig- ure 6 and 7 illustrate the two prompts, respectively. Prompt for MultiHopRAG Below is a question followed by some context from different sources. Please answer the question based on the context. The answer to the question is a word or entity. If the provided information is insufficient to answer the question, respond ‘Insufficient Information’. Answer directly without explanation. Question: {question} Context: {context} Figure 5: Prompt template used for MultiHopRAG. Prompt for SETR-CoT I will provide you with {num} passages, each indicated by a numerical identifier []. Select the passages based on their relevance to the search query: {question}. {context} Search Query: {question} Select the passages that mostly cover clear and diverse information to answer the query. Number of passages is unlimited. The format of final output should be ‘### Final Selection: [] []’, e.g., ### Final Selection: [2] [1]. Let’s think step