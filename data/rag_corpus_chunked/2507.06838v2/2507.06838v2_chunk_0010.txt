- 5.00 30.07 30.97 31.17 25.22 7.44 10.78 41.82 RERANKING bge-reranker-large 5.00 32.48 33.24 31.92 25.47 8.06 12.50 43.50 RankLlama 5.00 31.88 32.95 32.24 25.78 7.61 11.77 43.51 RankVicuna 5.00 32.08 32.83 32.66 26.85 7.78 11.35 42.76 RankZephyr 5.00 31.83 32.97 32.68 26.59 8.02 11.72 41.55 FirstMistral 5.00 30.10 31.07 31.43 25.31 6.53 10.64 42.05 RankGPT (gpt-4o) 5.00 33.85 34.45 34.36 28.06 9.43 13.25 45.69 SET SELECTION (OURS) SetR-Selection only 3.41 36.68 37.84 34.84 29.40 10.38 15.28 46.20 SetR-CoT 2.88 36.46 38.20 35.34 30.34 9.76 14.31 45.26 SetR-CoT & IRI 2.91 36.62 38.11 35.44 30.35 10.79 15.43 47.14 Table 1: End-to-end question answering results across various ranking models. Each model applies reranking or selection over the top-20 passages retrieved using either BM25 or bge-large-en-v1.5. The bold and underlined indicate the best and second-best performances respectively. "# of Passages" indicates the average number of passages included in the prompt context during answer generation. 4.1 Setup Benchmarks. For evaluation, we conduct exper- iments in two folds: (1) end-to-end QA, and (2) retrieval task. For comprehensive evaluation, we adopt four widely used complex multi-hop QA datasets: HotpotQA (Yang et al., 2018), 2Wiki- MultiHopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), and MultiHopRAG (Tang and Yang, 2024). These datasets cover diverse question types and multi-hop reasoning scenarios, offering a com- prehensive evaluation of QA models in complex, real-world contexts. Baselines. We compare SETR with state-of-the- art ranking baselines across different model cate- gories. Specifically, we include traditional unsu- pervised ranking models such as BM25 (Robert- son and Zaragoza, 2009), supervised dense ranking models including bge-large-en-v1.5 (Xiao et al., 2023), and bge-reranker-large (Xiao et al., 2023), as well as LLM-based ranking models such as Ran- kLlama (Ma et al., 2024), RankVicuna (Pradeep et al., 2023a), RankZephyr (Pradeep et al., 2023b), FirstMistral (Chen et