It reframes reranking as a single- token decoding task, enabling fast and ef- ficient passage selection. Despite its sim- plicity, it achieves competitive performance and serves as a strong open-source baseline to assess the raw ranking ability of modern instruction-tuned LLMs. • RankGPT4 (Sun et al., 2024) is a GPT-4- based reranker accessed via OpenAI’s API, used in a zero-shot setting to rank pas- sages given a query. It delivers state-of-the- art performance but is closed-source, non- reproducible, and costly. RankGPT4 serves as an upper-bound baseline to evaluate how well our approach performs against the strongest proprietary reranker. A.3 Additional Experimental Details Training. All SETR variants are fine-tuned using Llama-3.1-8B-Instruct as the base model. Training is conducted for 5 epochs using AdamW optimizer with a learning rate of 5 × 10−6 and an effective batch size of 512. We use 16 ×A100 GPUs and utilize Axolotl6 framework, which integrates vari- ous efficiency-oriented training techniques. Each model is trained on 40k GPT-4o generated exam- ples, where each input prompt includes a query and 20 retrieved passages. Evaluation Metrics. We evaluate model perfor- mance using both retrieval and QA metrics. For retrieval, we report Mean Receprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG), Precision, and Recall, which collectively measure the quality of passage ranking and coverage of rel- evant information. For end-to-end QA, we adopt Exact Match (EM), F1 score, and Accuracy to quan- tify answer correctness and completeness. Additionally, we evaluate information coverage (IC@k) on MultiHopRAG benchmark as it pro- vides gold evidence lists, extractively collected from documents. IC@k = | Sk i=1{e|e ∈ pi} ∩ {e|e ∈ E gold}| |Egold| where pi is the i-th top-ranked passage, e is an evidence span, Egold is the complete set of anno- tated gold evidences. For each question, we collect the complete set