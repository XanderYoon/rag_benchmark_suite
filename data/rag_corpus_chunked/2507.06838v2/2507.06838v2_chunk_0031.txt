information coverage (IC@k) on MultiHopRAG benchmark as it pro- vides gold evidence lists, extractively collected from documents. IC@k = | Sk i=1{e|e ∈ pi} ∩ {e|e ∈ E gold}| |Egold| where pi is the i-th top-ranked passage, e is an evidence span, Egold is the complete set of anno- tated gold evidences. For each question, we collect the complete set of gold evidence Egold required to answer it, as annotated in the MultiHopRAG dataset. Given a top-k set of retrieved passages, we identify gold evidence spans e within the passage text using reg- ular expression matching and remove duplicated 6https://github.com/axolotl-ai-cloud/axolotl evidences if redundant passage exist. IC@k is then computed as the proportion of gold evidence that appears within the top-k passages. Generation. For answer generation, we utilize Rankify (Abdallah et al., 2025) toolkit to imple- ment the full RAG pipeline, which consists of three components: Retrieval, Reranking/Selection, and Generation. For retrieval, we use the bge-large- en-v1.5 (Xiao et al., 2023) model to retrieve the top-20 passages per query. These candidates are then reranked or filtered via a set-wise selection mechanism. For generation, we employ Llama- 3.1-8B-Instruct to generate final answers on gen- eral multi-hop QA benchmarks, including Hot- potQA, 2WikiMultiHopQA, and MusiQUE. For MultiHopRAG benchmark, we follow the evalua- tion protocol of (Tang and Yang, 2024) and employ gpt-4o-2024-08-06 as the generator due to its strong reasoning performance. Notably, even when provided with gold evidences, open-source models such as Llama2-70B and Mixtral-8x7B achieve rel- atively low accuracy (0.32 and 0.36, respectively), while GPT-4 attains a significantly higher score of 0.89, highlighting a substantial performance gap. The prompts used for each benchmark are shown in Figure 4 and Figure 5, respectively. Prompt for General Multi-hop QA {context} Based on these texts, answer these questions: Q: {question} A: Figure 4: Prompt template