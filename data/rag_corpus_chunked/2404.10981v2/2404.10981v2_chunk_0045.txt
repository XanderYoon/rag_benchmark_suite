! ! !KnowledGPT [140]2023 ! ! ! !Selfmem [21]2023 ! ! ! ! !MEMWALKER [13]2023 ! ! ! !RECOMP [147]2023 ! ! !Rewrite-Retrieve-Read [94]2023 ! ! !Atlas [94]2023 ! ! ! ! ! !DKS-RAC [53]2023 ! ! ! ! !In-Context RALM [112]2023 ! !Fid-light [47]2023 ! ! !FLARE [65]2023 ! ! !Chameleon [63]2023 ! ! ! !ERAGent [123]2024! ! ! ! ! ! ! !PipeRAG [64]2024 ! ! ! ! !GenRT [148]2024! ! ! !PersonaRAG [160]2024! ! ! ! ! ! ! ! !CRAG [149]2024! ! ! ! ! ! !IMRAG [150]2024 ! ! ! ! ! !AiSAQ [126]2024! ! ! !ROPG [118]2024! ! ! ! !RQ-RAG [12]2024 ! ! ! ! !PlanRAG [81]2024 ! ! ! ! !RARG [159]2024 ! ! ! ! !DRAGIN [124]2024 ! ! ! ! !LRUS-CoverTree [93]2024! ! ! ! ! Table 2. The comprehensive summary of RAG studies. A !in the “Multi-hop” column signifies that the research involves multiple search rounds. Similarly, a !in the “Training” column indicates that the study included training phases. It is important to note that in this context, “Training” encompasses both initial model training and fine-tuning processes. 8 Comparisons of RAG 8.1 The Comprehensive Summary of RAG Table 2 presents a detailed analysis of the RAG studies discussed in this paper. The analysis shows that the majority of these studies have utilized external data sources to enrich the content of LLMs. A preference for multiple-hop over single-hop retrieval was noted, indicating that iterative search rounds generally yield superior results. In other words, most methods employ dense retrieval to secure higher quality candidate documents. Compared to modifying datasets in the pre-retrieval stage, more studies focus on manipulating the query to improve retrieval performance. Additionally, there is a significant emphasis on optimizing the retrieval phase, highlighting its crucial role in the