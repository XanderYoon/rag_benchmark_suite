one based on criteria like consistency, relevance, or factual accuracy. An instance of this strategy is seen in FiD [58], which encodes multiple retrieved passages independently before fusing them in the decoder to create a coherent answer. By treating each passage separately during encoding and then merging them during decoding, the model effectively combines evidence from multiple sources. Meanwhile, in REPLUG [ 122], an ensemble approach is adopted where each retrieved document is independently prepended to the query and processed separately. The outputs are then aggregated, with relevance scores guiding the weighting of each document’s contribution. Through this process, the model capitalizes on diverse information across several sources, leading to improvements in answer accuracy, coverage, and scalability as more data becomes available. Enhance with Feedback. In contrast to approaches that process retrieved information in a single pass, this method introduces iterative refinement into the generation process by incorporating feedback loops. Initially, the generator produces a draft response, which is then evaluated and adjusted based on feedback mechanisms, such as self-reflection or predefined criteria focused on factual accuracy and fluency. This iterative approach aims to incrementally improve the output by identifying and correcting errors or fine-tuning content to better align with quality standards, ultimately producing a polished and reliable response. PRCA [151] offers an example by positioning itself between the retriever and generator, distilling retrieved information based on feedback from the generator. This distilled information serves as a reward model to guide context optimization, leveraging reinforcement learning and metrics like ROUGE-L scores to iteratively refine which details should be emphasized or downplayed. DSP [73], on the other hand, refines both queries and retrieved passages through a multi-hop retrieval process that incorporates programmatically bootstrapped feedback. Here, the language model generates intermediate queries, retrieves relevant passages, and updates the context in subsequent steps—each