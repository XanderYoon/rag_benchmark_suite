to facilitate more focused searches, allowing for the pinpointing of segments containing pertinent keywords. The integration of deep learning has revolutionized indexing through the use of pretrained LMs for generating semantic vector representations of texts. These vectors are stored, enabling rapid and precise retrieval from extensive data collections, significantly enhancing retrieval efficiency. 2.1.2 Retrieval. While traditional retrieval methods, such as the BM25 algorithm [44], focus on term frequency and presence for document ranking, they often overlook the semantic information of queries. Current strategies leverage pretrained LMs like BERT [29], which capture the semantic essence of queries more effectively. These models improve search accuracy by considering synonyms and the structure of phrases, thereby refining document ranking through the detection of semantic similarities. This is typically achieved by measuring vector distances between documents and queries, combining traditional retrieval metrics with semantic understanding to yield search results that are both relevant and aligned with user intent. 2.1.3 Generation. The generation phase is tasked with producing text that is both relevant to the query and reflective of the information found in the retrieved documents. The usual method involves concatenating the query with the retrieved information, which is then fed into an LLM for text generation [ 85]. Although ensuring the generated text’s alignment and accuracy with the retrieved content presents challenges, it is also essential to strike a balance between adhering closely to the source material and infusing the output with creativity. The generated text should accurately convey the information from the retrieved documents and align with the query’s intent, while also offering the flexibility to introduce new insights or perspectives not explicitly contained within the retrieved data. 2.2 RAG Paradigm The RAG paradigm organizes research within the domain, offering a straightforward yet robust framework to enhance LLM performance. Central to RAG is its