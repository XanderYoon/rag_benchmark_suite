F1 scores. These metrics have been applied across a , Vol. 1, No. 1, Article . Publication date: August 2018. 20 Huang et al. Evaluation Framework Aspects Methods Metrics Datasets RAGAS [33] Quality of RAG Systems Context Relevance Extracted Sentences / Total Sentences WikiEval7Answer Relevance Average Cosine Similarity Faithfulness Supported Statements / Total Statements ARES [117] Improving RAGAS Context Relevance Confidence Intervals KILT [109]SuperGLUE [134]Answer Relevance Answer Faithfulness RECALL [91] Counterfactual Robustness Response Quality Accuracy (QA)BLEU, ROUGE-L (Generation)EventKG [41]UJ [50]Robustness Misleading Rate (QA)Mistake Reappearance Rate (Generation) RGB [14] Impact of RAG on LLMs Noise Robustness Accuracy Synthetic Negative Rejection Rejection Rate Information Integration Accuracy Counterfactual RobustnessError Detection RateError Correction Rate MIRAGE [144] RAG in Medical QA Zero-Shot Learning Accuracy MMLU-Med [45]MedQA-US [66]MedMCQA [105]PubMedQA [67]BioASQ-Y/N [132] Multi-Choice Evaluation Retrieval-Augmented Generation Question-Only Retrieval eRAG [119] Retrieval Quality in RAG Downstream Task Accuracy, ROUGE KILTSet-based Precision, Recall, Hit Rate Ranking MAP, MRR, NDCG BERGEN [114] Standardizing RAG ExperimentsSurface-Based EM, F1, Precision, Recall QA Datasets [69, 76]Semantic BEM [11], LLMeval [114] Table 1. The Comparison of Different RAG Evaluation Frameworks. wide array of datasets, including TriviaQA [69], HotpotQA [153], FEVER [129], Natural Questions (NQ) [76], Wizard of Wikipedia (WoW) [30], and T-REX [32], which are often used to benchmark the effectiveness of retrieval and generation components in knowledge-intensive tasks. While downstream task evaluations provide valuable insights, they fail to address the multifaceted challenges that arise as RAG systems continue to evolve. To fill this gap, recent research has proposed various frameworks and benchmarks that aim to evaluate these systems from multiple perspectives, considering not only the quality of the generated text but also the relevance of retrieved documents and the systemâ€™s resilience to misinformation, as shown in Table 1. These evaluations include metrics that assess noise robustness, negative prompting, information integration, and counterfactual robustness,