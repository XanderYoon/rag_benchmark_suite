to maintain both relevance , Vol. 1, No. 1, Article . Publication date: August 2018. 18 Huang et al. and completeness. For instance, the RETRO [9] model enhances generation by integrating retrieved text chunks with the user’s query using a chunked cross-attention mechanism, where relevant information from the retrieved neighbors is directly injected into the generation process. This method involves first retrieving similar document chunks based on the query and then using a cross- attention module to align and combine these chunks with the input sequence during generation. In-Context RALM [112] takes a comparable approach, directly prepending the retrieved documents to the input query. In this way, the language model can generate responses conditioned on both the query and the retrieved content without requiring changes to the model’s architecture. Both examples illustrate a straightforward yet effective method: concatenating the query and retrieved documents into a single input sequence that the LLMs process together, yielding outputs that are contextually enhanced. Enhance with Ensemble. When multiple sources are synthesized, the generation process can achieve a more coherent and well-rounded response. Rather than relying solely on a single source, this approach aggregates information from various documents, allowing the generator to reconcile conflicting details, blend diverse perspectives, and select the most reliable or comprehensive output. The ensemble process can manifest in different ways: it may involve combining insights from several sources into a unified narrative, or generating multiple candidate outputs and choosing the best one based on criteria like consistency, relevance, or factual accuracy. An instance of this strategy is seen in FiD [58], which encodes multiple retrieved passages independently before fusing them in the decoder to create a coherent answer. By treating each passage separately during encoding and then merging them during decoding, the model effectively combines evidence from multiple sources. Meanwhile, in