supervised fine-tuning. For example, In-Context RALM [112] employs , Vol. 1, No. 1, Article . Publication date: August 2018. 16 Huang et al. a zero-shot approach where an off-the-shelf language model is used to re-rank the top-k documents retrieved by a BM25 retriever. This process involves selecting the document that maximizes the likelihood of the generated text, effectively using the LM’s semantic understanding to improve document relevance without requiring additional supervised training. The paper also explores training a dedicated re-ranker using self-supervised learning to further enhance the selection of relevant documents, demonstrating that training a re-ranker with domain-specific data can be more effective than zero-shot re-ranking. Supervised Re-ranking. Supervised re-rankers involve fine-tuning LLMs on specific ranking datasets. This category can be further divided into models like BERT that process query-document pairs to compute relevance scores, models like T5 that treat ranking as a generation task and use generated tokens to determine relevance, and models like RankLLaMA [ 95] that employ a prompt-based approach, focusing on the last token’s representation for relevance calculation [165]. For instance, the re-ranker in Re2G [40] is based on a BERT model trained on labeled data (such as MS MARCO) and fine-tuned to improve the relevance ranking of retrieved documents. FiD-Light [47] employs a supervised approach where the model is fine-tuned on specific datasets to learn how to re-rank passages effectively using source pointers during autoregressive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process. GenRT [148] utilizes a combination of an encoder to capture global list-level features and a sequential decoder to reorder documents based on relevance. The model is trained to learn relevance scores through supervised learning, guided by labeled relevance data, ensuring that the most