no relevant information is available, has emerged as a , Vol. 1, No. 1, Article . Publication date: August 2018. The Survey of Retrieval-Augmented Text Generation in Large Language Models 21 key indicator of responsible output generation. Similarly, Error Detection Rate [14] evaluates the model’s capability to identify and filter out incorrect or misleading information, ensuring that the generation process is based on trustworthy sources. Another important consideration is Context Relevance, which assesses the alignment of retrieved documents with the specific query, emphasizing the need for content directly relevant to the generation task’s context. Faithfulness [33] is also critical in determining whether the generated text accurately reflects the information found in the retrieved documents, thereby minimizing the risk of generating misleading or incorrect content. The eRAG framework [119] introduces a more refined approach to evaluating retrieval quality in RAG systems by focusing on individual documents rather than the entire retrieval process. It operates by feeding each document in the retrieval list into the LLM alongside the query and evaluating the generated output against downstream task metrics such as Accuracy. The document- level scores are then aggregated using ranking metrics like MAP to produce a single evaluation score. This focus on document-level contributions offers a more precise assessment of retrieval quality while being significantly more computationally efficient than traditional end-to-end evaluations. Notably, eRAG demonstrates that its document-level evaluation correlates more strongly with downstream RAG performance compared to conventional methods like human annotations or provenance labels. This correlation underscores that the LLM, as the primary consumer of the retrieved results, is the most reliable judge of retrieval performance [119]. Regardless of the retrieval model or the number of retrieved documents, eRAG consistently outperforms other evaluation approaches, indicating that directly evaluating how each document supports the LLM’s output is the most effective way