indexing structures in traditional information retrieval systems is the inverted index. This structure associates documents with words to form a vocabulary list, allowing users to quickly locate references where a specific word appears within a collection of documents. The vocabulary list here refers to the set of all unique words present in the document collection, while the reference includes the documents where the word appears, along with the word’s position and weight within those documents. However, traditional indexing structures struggle to retrieve documents that are semantically related to a user’s query but do not contain the exact query terms. To address this limitation, retrieval methods using dense vectors generated by deep learning models have become the preferred choice. These vectors, also known as embeddings, capture the semantic meaning of words and documents, allowing for more flexible and accurate retrieval. Dense vector-based indexing methods can be categorized into three main types: graphs, product quantization (PQ) [62], and locality-sensitive hashing (LSH) [28]. Since generating dense vectors with large language models requires substantial resources, and the document collections to be searched are typically vast, the core strategy of these indexing methods is based on approximate nearest neighbor search (ANNS) [3]. This approach significantly speeds up the search process at the cost of a slight reduction in search accuracy. Graph. Using graphs to build indexes is a common practice in RAG. By indexing vectors with a graph structure, the range of nodes where distances need to be computed during retrieval can be limited to a local subgraph, thereby enhancing search speed. Several prominent methods and tools have been developed using this approach. For example, k-nearest neighbor language models kNN-LMs [72], as demonstrated by Khandelwal et al., integrate the kNN algorithm with pre-trained language models. This method employs a datastore created from collections of texts