on dynamically adapting the knowledge source and using prompts to rewrite the context in which a query is interpreted. This approach leverages prompt-based rewriting to enable the LLM to effectively integrate and ground its responses based on various heterogeneous knowledge sources. Additionally, Promptagator [27] discusses using prompt-based techniques to adapt and rewrite the query to better align with the retrieval system’s expectations, particularly in few-shot learning scenarios. These prompts guide the model in generating or refining the query to optimize retrieval results. , Vol. 1, No. 1, Article . Publication date: August 2018. The Survey of Retrieval-Augmented Text Generation in Large Language Models 9 3.3 Data Modification Document modification techniques play a critical role in enhancing retrieval performance, particu- larly when integrated with LLMs. These techniques can be broadly categorized into Internal Data Augmentation and External Data Enrichment. Internal Data Augmentation focuses on maximizing the value of existing information within documents or models, while External Data Enrichment introduces supplementary data from outside sources to fill gaps, provide additional context, or broaden the scope of the content. Internal Data Augmentation. Internal Data Augmentation leverages information already present within documents or taps into the inherent knowledge embedded in LLMs. Techniques like para- phrasing, where content is rewritten for improved readability or multiple perspectives, and summa- rization, which condenses information while retaining core content, are commonly employed. Other methods involve generating supplementary content or explanations that are contextually related without introducing external data. For instance, RECITE [125] utilizes a model’s internal memory to recite relevant information before generating responses, thus enhancing performance in tasks like closed-book question answering without external data. KnowledGPT [140] similarly refines the internal knowledge embedded within LLMs, optimizing its use during generation. GENREAD [156] further demonstrates how pre-existing knowledge within LLMs can be used to generate context that enhances