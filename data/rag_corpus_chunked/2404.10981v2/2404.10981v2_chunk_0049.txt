73.37±1.34 64.73±1.34 54.75±0.77 67.20±2.10 88.51±1.28 69.71 Wikipedia (29.9M) BM25 73.37±1.34 63.47±1.35 54.10±0.77 26.40±1.97 71.36±1.82 57.74 Contriever 74.10±1.33 65.99±1.33 54.03±0.77 26.40±1.97 69.90±1.85 58.08 SPECTER 72.18±1.36 63.63±1.35 52.71±0.77 22.20±1.86 66.83±1.89 55.51 MedCPT 71.99±1.36 65.12±1.34 55.15±0.77 29.00±2.03 73.46±1.78 58.95 RRF-2 74.20±1.33 64.57±1.34 54.72±0.77 31.00±2.07 76.21±1.71 60.14 RRF-4 73.19±1.34 64.96±1.34 54.53±0.77 31.00±2.07 72.01±1.81 59.14 Table 4. Part results of Accuracy (%) of GPT-3.5 across different corpora and retrievers on Mirage. Red and green highlight declines and improvements compared to CoT (first row), with shading intensity reflecting the degree of change. Data sourced from Mirage [144]. Impact of the Retriever. The results shown in Table 4 highlight the accuracy of GPT-3.5 across different corpora and retrievers on the Mirage benchmark [144]. These findings underscore how retriever performance closely depends on the alignment between training data and the target corpus. For example, in the MEDRAG system, MedCPT—trained specifically on PubMed user logs—significantly improves retrieval performance when accessing the PubMed corpus. This illus- trates the benefits of using domain-specific retrievers tailored to specialized datasets. In contrast, general-purpose retrievers like Contriever, which incorporate Wikipedia data during training, excel in retrieving information from Wikipedia, especially for tasks like MMLU-Med and MedQA-US. On the other hand, SPECTER, which focuses more on regularizing pairwise article distances than opti- mizing query-to-article relevance, underperforms on the MedCorp corpus. The study also explores combining multiple retrievers using Reciprocal Rank Fusion (RRF). However, results show that adding more retrievers does not always lead to better outcomes; for instance, excluding SPECTER in RRF-2 on Wikipedia yields better results than RRF-4, indicating that simply increasing the number of retrievers is not beneficial unless their strengths align with the retrieval task. Figure 5a illustrates how eRAG investigates the correlation between LLM performance and retrieval effectiveness on the NQ dataset using three retrievers with different characteristics: BM25 (lexical sparse),