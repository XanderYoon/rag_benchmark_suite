annotations or provenance labels. This correlation underscores that the LLM, as the primary consumer of the retrieved results, is the most reliable judge of retrieval performance [119]. Regardless of the retrieval model or the number of retrieved documents, eRAG consistently outperforms other evaluation approaches, indicating that directly evaluating how each document supports the LLM’s output is the most effective way to measure retrieval quality in RAG systems. 7.2 Generation-based Aspect The evaluation of text produced by large language models involves analyzing performance across a range of downstream tasks using standard metrics that assess linguistic quality, coherence, accuracy, and alignment with ground-truth data. Metrics like BLEU [107] and ROUGE-L [87] are often used to measure fluency, similarity to human-produced text, and the overlap with reference summaries, respectively, providing insights into how well the generated content captures key ideas and phrases. In addition to these metrics, which focus on the quality of linguistic output, Accuracy and overlap with ground-truth data are evaluated using EM and F1 scores, which respectively measure the percentage of completely correct answers and offer a balanced view of precision and recall. This ensures that relevant answers are retrieved while inaccuracies are minimized. Beyond these standard evaluation techniques, more specialized criteria have been introduced to assess RAG systems in specific contexts. For dialogue generation, for instance, metrics like perplexity and entropy are employed to evaluate response diversity and naturalness. In scenarios where misinformation is a concern, metrics like Misleading Rate and Mistake Reappearance Rate [91] have been developed to measure a model’s ability to avoid generating incorrect or misleading content. Other advanced metrics include Answer Relevance [33], which assesses the precision of responses to queries, Kendall’s tau [117], used for evaluating the accuracy of system rankings, and Micro-F1 [117], which fine-tunes accuracy evaluation in tasks involving multiple correct answers.