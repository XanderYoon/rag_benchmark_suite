guide context optimization, leveraging reinforcement learning and metrics like ROUGE-L scores to iteratively refine which details should be emphasized or downplayed. DSP [73], on the other hand, refines both queries and retrieved passages through a multi-hop retrieval process that incorporates programmatically bootstrapped feedback. Here, the language model generates intermediate queries, retrieves relevant passages, and updates the context in subsequent steps—each stage building on the last to refine the final output. Feedback-driven enhancements are also evident in models like Selfmem [21], which focus on generating self-memory. The model first produces an unbounded pool of outputs and then selects the most relevant one as memory for the next generation, guided by metrics like BLEU or ROUGE. Finally, RECITE [ 122] integrates feedback by generating multiple recitations from the model’s internal knowledge and using self-consistency techniques to aggregate the outputs. By introducing diversity in the recitations and leveraging passage hints during generation, this approach selects the best content through majority voting. Together, these methods demonstrate , Vol. 1, No. 1, Article . Publication date: August 2018. The Survey of Retrieval-Augmented Text Generation in Large Language Models 19 how feedback loops and iterative refinements can lead to outputs that are not only more accurate but also increasingly coherent and contextually grounded as they evolve. 6.2 Customization Customization focuses on tailoring content to the user’s personality and needs. It involves adjusting the output either to align with specific knowledge retrieved during earlier stages (content alignment) or to adapt the generated response to meet the user’s preferences, context, or audience needs (contextual adaptation). In LAPDOG [52], customization is achieved primarily through content alignment by integrating persona profiles with external stories to enrich the context used for generation. The story retriever identifies relevant narratives based on the persona, expanding the limited profiles with additional information. The generator