supplying additional context C to the iterative loop module. Subsequently, the iterative loop module enhances the gener- ated output through multiple interactions with the retrieved information. Fig. 1: LoRAG Architecture B. Iterative Loop Mechanism The core component of LoRAG is its iterative loop mech- anism, which facilitates the progressive enhancement of gen- erated text. The mechanism entails the following steps: Algorithm 1 LoRAG Iterative Loop Initialize: y ← GenerativeModel(x), where x denotes the input text and y represents the initial output text C ← RetrievalMechanism(x), where C signifies the set of relevant information retrieved from external sources for t = 1 to T do, where T denotes the maximum number of iterations yt ← LoRAG(y≤t, C), where yt represents the refined output text at iteration t and y≤t signifies the output text up to iteration t − 1 C ← RetrievalMechanism(x, yt), where C denotes the updated set of relevant information based on the current output text end for Return: yT , the final output text after T iterations This mechanism initiates by generating an initial output text y using a generative model, such as GPT-4, capable of producing fluent and coherent text given an input text x. Subsequently, it retrieves a set of relevant information C from external sources, such as Bing Search [12], which can offer additional knowledge or context for the input text x. Then, it iteratively refines the output text yt by employing LoRAG, a novel model that harnesses both the previous output text y≤t and the retrieved information C to generate text that is more accurate, informative, and diverse. At each iteration, the retrieved information C is updated based on the current output text yt to ensure that the generation process is dynamic and contextually sensitive. The mechanism halts after a predetermined number of iterations