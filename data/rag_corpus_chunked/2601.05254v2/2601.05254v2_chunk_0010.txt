properties. How- ever, with the advantage of hierarchical knowledge graph, we are able to further extract higher-level global knowledge: (V ′ c , S ′ c) =Ret chain(V ′ t ), where Retchain(·) extracts the corresponding chains, V ′ c indicates the domain tags on the chain corre- sponding to V ′ t , and S ′ c represents the summaries implied byV ′ c . 4.2.3 Tag Knowledge-Fused Generation With the retrieved summaries, global information is achieved for response generation. Constrained by the input length of the LLMs, we prioritize putting in the related domain tag summaries S ′ t, and then adding the related chain summaries S ′ c until the upper limit is reached. Given the question and the domain knowledge summaries, the answer is generated by the LLM: a=LLM(q, S ′ t, S ′ c). 4.3 Analysis of Retrieval Complexity GraphRAG’s inefficiency stems from its multi- stage pipeline, requiring extensive LLM calls for entity extraction and community detection (e.g., Leiden algorithm), leading to high computational costs and frequent full graph reconstructions for dynamic data. In contrast, TagRAG’s domain tag chains enable hierarchical information aggrega- tion during graph building, replacing iterative com- munity partitioning with linear chain processing. During inference, TagRAG utilizes vector match- ing and tag chain linking, avoiding GraphRAG’s costly full graph traversal. Both stages demonstrate TagRAG’s superior efficiency over GraphRAG. 5 Experimental Setup 5.1 Datasets and Baselines To demonstrate the high applicability of TagRAG, following standard evaluation on Graph-based RAG methods (Edge et al., 2024; Guo et al., 2025; Chen et al., 2025), we chose four corpus from the comprehensive UltraDomain (Qian et al., 2025) benchmark, Agriculture, CS, Legal and Mix. Fol- lowing LightRAG, we used GPT-4o-mini to gener- ate 125 global questions for each dataset, covering different domains and different tasks. The dataset