in- sights on the question?Empowerment: How well does the answer help the reader understand and make informed judgments about the topic?Over- all: Which answer is better overall? We utilize the powerful model GPT-4o-mini, gemini-2.5-pro and claude-sonnet-4.5-20250929 to determine the winner for each of the two com- parison methods based on the above metrics. For each dataset, we also exchange the order of the results of the two comparison methods to avoid po- sition bias. Finally, we report the average of 3*2=6 evaluation results. The evaluation prompt can be seen in Appendix C. 5.3 Implementation Details We use Qwen3-4B (Team, 2025) as the backbone to conduct the experiments without thinking. bge- large-en-v1.5 (Xiao et al., 2023) is employed to embed questions and documents. The chunk size and overlap size are 1200 and 100, respectively. The vector database used in this work is nano- vectordb 1. The top-k number for Domain-centric Knowledge Retrieval is 3. More detailed experi- mental settings can be found in Appendix A. 6 Results and Analysis 6.1 Main Results Table 1 evaluates the winning rate of TagRAG against NaiveRAG, GraphRAG, LightRAG, and MiniRAG on four datasets: UltraDomain Agri- culture, CS, Legal, and Mix. We can draw the following conclusions: (1)TagRAG leads in per- formance, achieving an average winning per- centage of 82.85% compared with NaiveRAG, 1https://github.com/gusye1234/nano-vectordb Agri CS Legal Mix Avg Zero-shot LLM Generation v.s. Qwen3-4B Comprehensiveness 85.9 69.6 70.3 58.8 71.2 Diversity 78.9 64.4 62.0 53.9 64.8 Empowerment 80.7 57.5 48.9 45.7 58.2 Overall 83.2 59.3 53.7 49.3 61.4 v.s. Qwen3-30B-A3B Comprehensiveness 84.8 68.8 72.7 59.6 71.5 Diversity 79.5 65.1 62.1 59.6 66.6 Empowerment 76.4 53.2 45.6 40.4 53.9 Overall 80.3 55.5 51.6 44.7 58.0 v.s. Llama-3.3-70B-Instruct Comprehensiveness 68.1 51.5 49.7 27.7 49.2 Diversity 67.5 58.5 43.1 35.9 51.2 Empowerment 56.9 43.1 27.2 22.8 37.5 Overall 65.9