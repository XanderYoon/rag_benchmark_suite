GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation Linhao Luo1∗, Zicheng Zhao2∗, Gholamreza Haffari1, Chen Gong3, Dinh Phung1, Shirui Pan4† 1Monash University, 2Nanjing University of Science and Technology, 3Shanghai Jiao Tong University,4Griffith University, {Linhao.Luo,gholamreza.haffari,dinh.phung}@monash.edu, zicheng.zhao@njust.edu.cn, chen.gong@sjtu.edu.cn, s.pan@griffith.edu.au Project page:https://rmanluo.github.io/gfm-rag Abstract Retrieval-augmented generation (RAG) has proven effective in integrating knowl- edge into large language models (LLMs). However, conventional RAGs struggle to capture complex relationships between pieces of knowledge, limiting their per- formance in intricate reasoning that requires integrating knowledge from multiple sources. Recently, graph-enhanced retrieval augmented generation (GraphRAG) builds graph structure to explicitly model these relationships, enabling more effec- tive and efficient retrievers. Nevertheless, its performance is still hindered by the noise and incompleteness within the graph structure. To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for retrieval augmented gener- ation. GFM-RAG is powered by an innovative graph neural network that reasons over graph structure to capture complex query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage training process on large-scale datasets, comprising 60 knowledge graphs with over 14M triples and 700k documents. This results in impressive performance and generalizability for GFM-RAG, making it the first graph foundation model applicable to unseen datasets for retrieval without any domain-specific fine-tuning required. Extensive experiments on three multi-hop QA datasets and seven domain-specific RAG datasets demonstrate thatGFM-RAG achieves state-of-the-art performance while maintaining efficiency and alignment with neural scaling laws, highlighting its potential for further improvement. 1 Introduction Recent advancements in large language models (LLMs) [ 47, 42, 70] have greatly propelled the evolution of natural language processing, positioning them as foundational models for artificial general intelligence (AGI). Despite the remarkable reasoning ability [ 48], LLMs are still limited in accessing real-time information and lack of domain-specific knowledge, which is outside the pre-training corpus. To address these limitations, retrieval-augmented generation