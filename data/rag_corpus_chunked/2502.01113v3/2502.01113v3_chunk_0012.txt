learn complex relationships between queries and entities by taking into account both the semantics and structure of the KG through training on large-scale datasets. 3.2.2 Training Process Training Objective.The training objective of the GFM retriever is to maximize the likelihood of the relevant entities to the query, which can be optimized by minimizing the binary cross-entropy (BCE) loss: LBCE =− 1 |Aq| X e∈Aq logP q(e)− 1 |E -| X |E -| log(1−P q(e)),(11) where Aq denotes the set of target relevant entities to the query q, and E - ⊆ E \ A q denotes the set of negative entities sampled from the KG. However, due to the sparsity of the target entities, the BCE loss may suffer from the gradient vanishing problem [36]. To address this issue, we further introduce the ranking loss [2] to maximize the margin between the positive and negative entities: LRANK =− 1 |Aq| X e∈Aq Pq(e)P e′∈E - Pq(e′) .(12) The final training objective is the weighted combination of the BCE loss and ranking loss: L=αL BCE + (1−α)L RANK.(13) Self-supervised KG Completion Pre-training.To enhance the graph reasoning capability of the GFM retriever, we first pre-train it on a large-scale knowledge graph (KG) completion task. We sample a set of triples from the KG index and mask either the head or tail entity to create synthetic queries in the form q= (e,r, ?)or(?,r,e ′), with the masked entity serving as the target entity Aq ={e}or{e ′}. The GFM retriever is then trained to predict the masked entity using both the query and the KG, as outlined in equation 13. Supervised Document Retrieval Fine-tuning.After self-supervised pre-training, we supervised fine-tune the GFM retriever on a labeled document retrieval task. In this task, queries q are natural language questions, and target entities Aq are extracted from