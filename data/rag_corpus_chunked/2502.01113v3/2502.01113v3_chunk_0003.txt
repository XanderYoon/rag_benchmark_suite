(GFM-RAG), thereby enhancing LLMs’ reason- ing ability. As shown in Figure 1, we create a knowledge graph index(KG-index) from doc- uments in each dataset. The KG-index con- sists of interconnected factual triples pointing to the original documents, which serves as a structural knowledge index across multiple sources, enhancing the integration of diverse knowledge for complex reasoning tasks [ 16]. Then, we present thegraph foundation model retriever(GFM retriever), driven by a query-dependent GNN that captures complex query-knowledge relationships in a unified, transferable space of semantics and graph structure. Through multi-layer message passing, the GFM retriever enables efficient multi-hop retrieval in a single step, surpassing previous multi-step methods. The GFM retriever, with 8M parameters, undergoes a two-stage training:self-supervised KG completion pre-trainingandsupervised document retrieval fine-tuningon large-scale datasets, including 60 knowledge graphs with over 14M triples and 700k documents. This large-scale training ensures the generalizability of GFM retriever to be applied to unseen datasets without further training. In experiments, GFM-RAG achieves state-of-the-art performance across three multi-hop QA datasets, demonstrating its effectiveness and efficiency in multi-hop reasoning. It also generalizes well across seven RAG datasets from diverse domains, such as biomedical, customer service, and general knowledge, without requiring additional training. Furthermore, GFM-RAG follows the neural scaling law [19], whose performance benefits from training data and model size scaling, emphasizing its potential as a foundational model for future improvements. The main contributions of this paper are as follows: • We introduce a graph foundation model for retrieval augmented generation ( GFM-RAG), powered by a novel query-dependent GNN to enable efficient multi-hop retrieval within a single step. • We train a large-scale model with 8M parameters, marking the first graph foundation model (GFM) that can be applied directly to various unseen datasets for retrieval augmented generation. • We evaluate GFM-RAG on three multi-hop QA datasets and seven