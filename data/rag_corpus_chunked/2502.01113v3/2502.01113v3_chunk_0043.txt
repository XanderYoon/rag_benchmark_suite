Sentence Embedding Model HotpotQA MuSique 2Wiki R@2 R@5 R@2 R@5 R@2 R@5 sentence-transformers/all-mpnet-base-v2 70.2 82.1 46.0 55.1 81.185.6 BAAI/bge-large-en 68.1 81.1 45.955.9 80.786.3 Alibaba-NLP/gte-Qwen2-1.5B-instruct 69.9 81.5 46.0 55.0 79.8 86.2 Alibaba-NLP/gte-Qwen2-7B-instruct 68.5 81.5 45.5 55.1 80.8 85.6 nvidia/NV-Embed-v2 69.2 81.4 46.354.9 80.3 85.5 Table 9: Comparison ofGFM-RAGwith pre-trained and fine-tuned sentence embedding models. Method HotpotQA MuSiQue 2Wiki R@2 R@5 R@2 R@5 R@2 R@5 GFM-RAG 78.3 87.1 49.1 58.2 90.8 95.6 all-mpnet-v2 (pre-trained) 59.4 73.3 33.2 46.3 48.5 59.4 all-mpnet-v2 (finetuned) 67.0 82.3 41.7 55.0 65.1 76.7 can create a query as (Barack Obama,born_in, ?) , which is encoded as a sentence embedding and fed into the GFM to predict the target entityHonoluluon graphs. In supervised document retrieval fine-tuning, we obtain natural language questions and supporting documents from the multi-hop QA datasets. For each question, we identify the entities from its supporting documents as the targets. For instance, given the question“Where was Barack Obama born in?”, we can extract two entities such as [Honolulu, USA] from its supporting documents (e.g., Doc. 2 in Figure 2). The GFM is trained to maximize the likelihood of these two target entities. In the self-supervised KG completion pre-training, the GFM is trained on the mixture of 60 constructed KG-indexes for 30,000 steps. Then, we conduct the supervised document retrieval fine-tuning on the labeled question-document pairs for 5 epochs. The weight α between losses is set to 0.3. We use AdamW optimizer, learning rate of 5e-4 with batch sizes of both training stages set to 4. Each batch contains only one KG-index and training samples associated to it, where we randomly sample from different KG-indexes during training. The model is trained on 8 NVIDIA A100s (80G) with 14 hours pre-training and 5 hours supervised fine-tuning. The detailed settings are summarized in Table 7. E Additional Experiments