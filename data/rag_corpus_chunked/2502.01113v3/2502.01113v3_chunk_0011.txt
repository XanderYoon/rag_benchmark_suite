Query-dependent Message Passing.The query-dependent message passing will propagate the information from the question entities to other entities in the KG to capture their relevance to the query. The message passing process can be formulated as: Triple-level: h0 r =SentenceEmb(r),h 0 r ∈R d,(7) ml+1 e =Msg(h l e,g l+1(hl r),h l e′), (e,r,e ′)∈ G,(8) Entity-level: hl+1 e =Update(h l e,Agg({m l+1 e′ |e′ ∈ N r(e),r∈ R})),(9) where hl e,h l r denote the entity and relation embeddings at layer l, respectively. The relation em- beddings h0 r are also initialized using the same sentence embedding model as the query, reflecting their semantics (e.g., “born_in”), and updated by a layer-specific function gl+1(·), implemented as a 2-layer MLP. The Msg(·) is operated on all triples in the KG to generate messages, which is implemented with a non-parametric DistMult [71] following the architecture of NBFNet [78]. For each entity, we aggregate the messages from its neighbors Nr(e) with relation r using sum and update the entity representation with a single linear layer. 5 After L layers message passing, a final MLP layer together with a sigmoid function maps the entity embeddings to their relevance scores to the query: Pq =σ(MLP(H L q )),P q ∈R |E|×1 .(10) Generalizability.Since the query, entity, and relation embeddings are initialized using the same sentence embedding model with identical dimensions, the query-dependent GNN can be directly applied to different queries and KGs. This allows it to learn complex relationships between queries and entities by taking into account both the semantics and structure of the KG through training on large-scale datasets. 3.2.2 Training Process Training Objective.The training objective of the GFM retriever is to maximize the likelihood of the relevant entities to the query, which can be optimized by minimizing the binary cross-entropy (BCE) loss: LBCE =−