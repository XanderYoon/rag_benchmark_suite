supervised document retrieval fine-tuning significantly decreases the performance of GFM-RAG. This highlights the importance of supervised fine-tuning, as it enables the model to understand usersâ€™ queries and better capture the relevance between questions and knowledge for improved retrieval. Although the pre-training has a relatively small impact on the final performance, its primary purpose is to learn the general graph reasoning ability, following previous studies like ULTRA [11]. This would enhance the generalization and robustness of the GFM, which could be beneficial to its performance on other tasks, such as knowledge graph completion. To further validate this, we conduct an ablation study to compare GFM-RAG with different training strategies on the knowledge graph completion task. We report the knowledge graph completion (KGC) performance on the KG-index from the test set of the HotpotQA dataset. The results are shown in Table 11. From the knowledge graph completion results, we can observe that the GFM-RAG undergoes only the pre-training (GFM-RAGw/o Fine-tune) achieves the best performance, which indicates that the pre- training is effective in learning the general graph reasoning ability. The performance ofGFM-RAG with only supervised fine-tuning (GFM-RAGw/o Pre-train) is significantly lower than that ofGFM-RAG with pre-training. This indicates that the supervised fine-tuning is only learning the specific downstream task, which would limit the generalization ability of GFM-RAG as the foundation model. The GFM trained with both pre-training and supervised fine-tuning achieves the second-best performance on the knowledge graph completion task and the best performance on the multi-hop QA task. This indicates that both training strategies are essential for GFM-RAG to learn the general graph reasoning ability and benefit specific downstream tasks. E.3 Effectiveness of Loss Weights In this section, we examine the effectiveness of the weights assigned to the BCE loss and ranking loss in training GFM-RAG. We compare performance by varying