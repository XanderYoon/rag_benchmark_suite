retrieving and reasoning over documents, which can be integrated with arbitrary retrieval methods. • Adaptive-RAG [23] proposes an adaptive multi-step retrieval method that can dynamically select the most suitable retrieval strategy based on the complexity of the query. • FLARE [24] introduces a multi-step retrieval method that actively decide when and how to retrieve documents. It also predicts the future content to the guide the retrieval in next steps. • IRCoT [64] is a powerful multi-step retrieval pipeline that integrates the retrieval with the chain-of-thought (CoT) reasoning of LLMs. It guides the retrieval with CoT and in turn using retrieved documents to improve CoT. IRCoT can be compatible with arbitrary retrievers to conduct multi-step retrieval and reasoning. 19 Table 7: The detailed implementation and training settings ofGFM-RAG. SettingGFM-RAG KG-index Construction OpenIE GPT-4o-mini Entity resolution ColBERTv2 τ0.8 GFM Model # Layer 6 Hidden dim 512 Message DistMult Aggregation Sum gl(·)2-layer MLP Sentence embedding model all-mpnet-v2 Doc. ranker entitiesT20 KGC Pre-training α1 Optimizer AdamW Learning rate 5e-4 Batch size 4 Training steps 30,000 # Negative sample 128 Supervised Retrieval Fine-tuning α0.3 Optimizer AdamW Learning rate 5e-4 Batch size 4 Training epochs 5 # Negative sampleE \ A q D Implementations and Training Details D.1 Training Data Construction We extract 60,000 samples from the training set of HotpotQA, MuSiQu, and 2Wiki to construct KG-indexes and conduct large-scale training. Specifically, we merge the candidate passages as the document corpus. In the KG-index construction, we use the GPT-4o-mini [ 47] with the OpenIE prompts described in HippoRAG [16] to extract the entities, relations, and triples from the document corpus. Then, we use the ColBERTv2 [55] to conduct the entity resolution by computing the similarity between entities as s(ei,e j) =Emb.(e i)⊤Emb.(ej),(21) where a new triple (ei,equivalent,e j) is generated if s(ei,e j)> τ and ei