official data split to obtain the training samples and follow existing methods [64, 16] to use the same 1,000 samples from each validation set to avoid data leakage. We merge the candidate passages as the document corpus for KG-index construction. The statistics of the training and test data are presented in Table 5 and Table 6, respectively. B.2 Domain-specific RAG Datasets To test the generalizability of GFM-RAG, we evaluate it on seven domain-specific RAG datasets [10] including, (1)biomedical: PubMedQA [ 25]; (2)customer support: DelucionQA [ 54], TechQA [4], ExpertQA [39], EManual [44]; (3)general knowledge: MS Marco [ 45], HAGRID [27]. We provide a brief overview of these datasets below. • PubMedQA [25] is a collection of PubMed research abstracts with corresponding questions paired with 4 abstract chunks. • DelucionQA [54] is a domain-specific RAG dataset leveraging Jeep’s 2023 Gladiator model manual as the source of knowledge, where each question is associated with 4 context documents and only 1 relevant passage. • TechQA [4] is a collection of real-world user questions posted on IBMDeveloper and Devel- operWorks forums, along with 10 technical support documents relating to each question. • ExpertQA [39] is a collection of curated questions from domain experts in various fields of science, arts, and law. The dataset also contains expert-curated passages relevant to each question. • EManual [ 44] is a question-answering dataset comprising consumer electronic device manuals and realistic questions about them composed by human annotators, where each question is related with up to 3 context documents. • MS Marco [45] is an open-domain question-answering dataset sourced from Bing search engine user query logs. Each question is associated with 10 context passages retrieved via Bing web search. • HAGRID [27] is a multi-lingual information retrieval dataset with questions and passages from MIRACL [75]. 18 In experiments, we