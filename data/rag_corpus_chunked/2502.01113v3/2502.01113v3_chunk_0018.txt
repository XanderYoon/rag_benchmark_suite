single step. As shown in Table 3, while the naive single-step methods get the best efficiency whose performance is not satisfying. Admittedly, the multi-step framework IRCoT could improve the performance, but it suffers from high computational costs due to the iterative retrieval and reasoning with LLMs. In contrast, GFM-RAG conducts multi-hop reasoning within a single-step GNN reasoning, which is more effective than single-step methods and more efficient than multi-step ones. 8 Table 3: Retrieval efficiency and performance comparison. Method HotpotQA MuSiQue 2Wiki Time (s) R@5 Time (s) R@5 Time (s) R@5 ColBERTv2 0.03579.3 0.03049.2 0.02968.2 HippoRAG 0.255 77.7 0.251 51.9 0.158 89.1 LightRAG 0.861 54.7 1.109 34.7 0.911 59.1 GraphRAG (MS) 2.759 76.6 3.037 49.3 1.204 77.3 IRCoT + ColBERTv2 1.146 82.0 1.152 53.7 2.095 74.4 IRCoT + HippoRAG 3.162 83.0 3.104 57.6 3.441 93.9 GFM-RAG 0.10787.1 0.12458.2 0.060 95.6 /uni00000033/uni00000058/uni00000045/uni00000030/uni00000048/uni00000047/uni00000034/uni00000024 /uni00000027/uni00000048/uni0000004f/uni00000058/uni00000046/uni0000004c/uni00000052/uni00000051/uni00000034/uni00000024 /uni00000028/uni00000030/uni00000044/uni00000051/uni00000058/uni00000044/uni0000004f /uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057/uni00000034/uni00000024/uni00000037/uni00000048/uni00000046/uni0000004b/uni00000034/uni00000024 /uni00000030/uni00000036/uni00000003/uni00000030/uni00000044/uni00000055/uni00000046/uni00000052 /uni0000002b/uni00000024/uni0000002a/uni00000035/uni0000002c/uni00000027 /uni00000018/uni0000001b/uni00000011/uni00000018 /uni0000001a/uni00000013/uni00000011/uni0000001b /uni00000019/uni00000013/uni00000011/uni00000019 /uni00000019/uni00000015/uni00000011/uni0000001a /uni00000017/uni00000019/uni00000011/uni00000019 /uni0000001a/uni00000014/uni00000011/uni00000013 /uni0000001b/uni00000017/uni00000011/uni0000001a /uni0000002a/uni00000029/uni00000030/uni00000010/uni00000035/uni00000024/uni0000002a/uni0000002b/uni0000004c/uni00000053/uni00000053/uni00000052/uni00000035/uni00000024/uni0000002a/uni0000002f/uni0000004c/uni0000004a/uni0000004b/uni00000057/uni00000035/uni00000024/uni0000002a Figure 3: Model generalizability comparison. /uni00000013/uni00000011/uni00000013/uni0000001b/uni00000030 /uni00000013/uni00000011/uni00000015/uni00000030 /uni00000013/uni00000011/uni0000001a/uni00000030 /uni00000015/uni00000030 /uni0000001b/uni00000030 /uni0000004f/uni00000052/uni0000004a/uni00000014/uni00000013/uni0000000b/uni0000005c/uni0000001d/uni00000003/uni00000006/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni0000000c /uni00000016/uni0000004e /uni00000019/uni0000004e /uni00000014/uni00000015/uni0000004e /uni00000015/uni00000017/uni0000004e /uni00000017/uni00000018/uni0000004e /uni0000004f/uni00000052/uni0000004a/uni00000014/uni00000013/uni0000000b/uni0000005b/uni0000001d/uni00000003/uni00000006/uni00000027/uni00000044/uni00000057/uni00000044/uni0000000c /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni00000018/uni00000015 /uni00000013/uni00000011/uni00000018/uni00000017 /uni00000013/uni00000011/uni00000018/uni00000019 /uni00000013/uni00000011/uni00000018/uni0000001b /uni0000005d/uni0000001d/uni00000003/uni00000030/uni00000035/uni00000035 Neural Scaling Law z = 0.24x0.05 + 0.11y0.03, R2 = 0.95 Figure 4: Neural scaling law ofGFM-RAG. 4.5 Ablation Study We conduct ablation studies to investigate the effectiveness of different components in GFM-RAG, including: different sentence embedding models (Section E.1), pre-training strategies (Section E.2), loss weighting strategies (Section E.3), ranking methods (Section E.4), training datasets (Section E.5), and the construction of KG-index (Section E.9). The results show that GFM-RAG is not sensitive to different sentence embedding models, and the pre-training strategy, as well as the loss weighting strategy, are both crucial for the performance ofGFM-RAG. 4.6 Model Generalizability To demonstrate the generalizability of GFM-RAG as a foundation model, we test the performance (R@5) of GFM-RAG on seven RAG datasets without any domain-specific fine-tuning.