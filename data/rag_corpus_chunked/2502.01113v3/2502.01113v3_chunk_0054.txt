of GFM-RAG. We conduct experiments using different LLMs for KG-index construction, including GPT-4o-mini and GPT-3.5-turbo5. Then, we reevaluate the performance of GFM-RAG and HippoRAG with the constructed KG-index. The results are shown in Table 21. From the results, the performance of both methods on the KG extracted by GPT-4o-mini is higher than the ones by GPT-3.5-turbo. This supports the opinion that GPT-4o-mini generally outperforms GPT-3.5-turbo in constructing high quality KG-index, which is crucial for the graph-enhanced retrieval. However, the performance 4https://platform.openai.com/docs/models/o4-mini 5https://platform.openai.com/docs/models/gpt-3-5-turbo Table 20: Graph Indexing time comparison. Method Indexing time (s) LightRAG 1430.32 GraphRAG (MS) 1796.43 GFM-RAG93.55 26 Table 21: Comparison of the model performance under the KG-index constructed by different LLMs. Method HotpotQA MuSiQue 2Wiki R@2 R@5 R@2 R@5 R@2 R@5 GFM-RAG (gpt-4o-mini) 78.3 87.1 49.1 58.2 90.8 95.6 HippoRAG (gpt-4o-mini) 62.2 79.3 41.7 53.6 72.1 89.5 GFM-RAG (gpt-3.5-trubo) 75.6 84.7 46.1 55.8 85.2 90.4 HippoRAG (gpt-3.5-trubo) 60.5 77.7 40.9 51.9 70.7 89.1 of GFM-RAG is significantly higher than HippoRAG under both KG-indexes. This indicates that GFM-RAG is more robust to the quality of the KG-index, demonstrating the effectiveness of the GFM in graph reasoning and retrieval. F Prompts In experiments, we follow the prompts used in HippoRAG [ 16] to extract the triples from the document corpus, which is shown in Table 22. G Limitations The limitations of GFM-RAG are as follows: (1) The construction of KG-index can be costly and time-consuming, especially when using LLMs for OpenIE extraction. We would explore the use of efficient KG construction methods in future work and optimize the construction process. (2) The model size of the GFM-RAG is relatively small (8M) compared to other foundation models like large language models with billions of parameters. Although it is not faired to directly compare the GNN-based model with transformer-based LLMs, we would