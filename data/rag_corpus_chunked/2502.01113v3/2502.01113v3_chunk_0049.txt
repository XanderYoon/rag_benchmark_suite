datasets, validating its ability to generalize effectively across domains and benefit from training on diverse KGs by learning generalizable reasoning ability across domains. E.6 Model Transferability In this section, we evaluate GFM-RAGâ€™s transferability by conducting domain-specific fine-tuning on the training split of dataset on each domain. As shown in 15, GFM-RAG performs well in zero- shot generalization, with further improvements achieved through domain-specific fine-tuning. This highlights its transferability when adapted to domain-specific datasets. 23 Table 14: Ablation study of GFM-RAG trained on each dataset. Best results are highlighted inbold. The second best is underlined. Test Dataset HotpotQA MuSiQue 2Wiki Training Dataset R@2 R@5 R@2 R@5 R@2 R@5 HotpotQA 79.3 87.8 46.9 57.2 86.6 92.4 MusiQue 68.8 81.8 47.6 57.5 84.4 89.6 2Wiki 72.2 77.9 46.6 55.5 89.3 93.2 All 78.3 87.1 49.1 58.2 90.8 95.6 Table 15: Model performance (R@5) and transferability comparsion. Model DelucionQA EManual ExpertQA TechQA MS Marco HAGRID HippoRAG (zero-shot) 59.0 50.0 55.1 39.5 51.1 75.5 LightRAG (zero-shot) 46.1 46.2 59.4 36.8 48.3 75.9 GFM-RAG(zero-shot) 70.8 60.6 62.7 46.6 71.0 84.7 GFM-RAG(domain-specific fine-tuning)82.7 75.9 60.8 49.5 77.5 86.6 E.7 Details of Model Neural Scaling In this section, we provide more details on the neural scaling experiments. We evaluate the changes of the model performance with respect to different parameter sizes and training data sizes. In GFM-RAG, the model parameter sizes are primarily influenced by the hidden dimension of the GFM. Thus, we vary the dimension from 32 to 512 which results in the model parameter sizes ranging from 0.08M to 8M. The detailed settings are shown in Table 16. We test models with different sizes on different scales of training data ranging from 3k to 45k samples. We separately report the fitted trend line of performance changing with model parameter size and training data size in Figure