with batch sizes of both training stages set to 4. Each batch contains only one KG-index and training samples associated to it, where we randomly sample from different KG-indexes during training. The model is trained on 8 NVIDIA A100s (80G) with 14 hours pre-training and 5 hours supervised fine-tuning. The detailed settings are summarized in Table 7. E Additional Experiments E.1 Effectiveness of Different Sentence Embeddings In this section, we first study the effectiveness of different sentence embeddings in the GFM. We compare the all-mpnet-v2 [ 57], bge-large-en [ 69], gte-Qwen2-1.5B-instruct and gte-Qwen2-7B- instruct [34] as well as NV-Embed-v2 [31]. We download the official pre-trained model from the Huggingface3. The details of the models are shown in Table 8. From the results, we can observe that the performance variance between different sentence embeddings is relatively small, where the all- mpnet-v2 achieves the best performance with respect to 3 metrics. This indicates that GFM-RAG is not sensitive to the choice of sentence embedding models. In experiments, we use the all-mpnet-v2 as the default sentence embedding model due to its efficiency. However, it has relative smaller context-size (512) which limits the length of input text. We leave the exploration of larger context-size sentence embedding models (e.g., NV-Embed-v2 with 32k context) for future work. Then, we expand our ablation study to compareGFM-RAG with variants without GNN and using solely the pre-trained all-mpnet-v2 embeddings and those fine-tuned on multi-hop QA data, respectively. The results are shown in Table 9. We can observe that GNN plays a crucial role in retrieval. The sentence embedding model all-mpnet-v2 is pre-trained on large-scale text data and could potentially see the QA data. However, it is not specifically trained for the multi-hop QA task, which leads to 3https://huggingface.co/ 21 Table 10: Effectiveness of KGC pre-training and supervised retrieval fine-tuning