best performance on the multi-hop QA task. This indicates that both training strategies are essential for GFM-RAG to learn the general graph reasoning ability and benefit specific downstream tasks. E.3 Effectiveness of Loss Weights In this section, we examine the effectiveness of the weights assigned to the BCE loss and ranking loss in training GFM-RAG. We compare performance by varying the weight α between the two losses: L=αL BCE + (1−α)L RANK, with results presented in Table 12. The findings indicate that using only either the BCE loss or ranking loss leads to suboptimal performance ( α= 0or1 ). The best 22 Table 12: Effectiveness (MRR) for the weightαof two losses. α HotpotQA MuSique 2Wiki 0 0.5189 0.3252 0.4425 1 0.5096 0.3214 0.4282 0.7 0.5202 0.3249 0.4348 0.3 0.5243 0.3260 0.4490 performance occurs when α is set to 0.3, which aligns with previous studies [36] suggesting that a smaller weight for BCE loss is preferable when positive samples are rare in the training data. E.4 Effectiveness of Ranking Methods In this section, we investigate the effectiveness of different ranking methods based on inverted index used in GFM-RAG. We compare four ranking methods including (1)IDF + Top-T Pred: Our proposed method (eqs. (14) to (16)), which maps the top-T entities predicted by GFM to documents using inverse document frequency (IDF)-weighted scores. (2)IDF + All Pred: Uses all predicted entities from GFM and weights them by IDF (w/o eq. (14)). (3)Top-T Pred: Uses only the top-T predicted entities without applying IDF weighting (w/o eq. (15)). (4)All Pred: Use all entity predictions and directly map to document scores (w/o eqs. (14) and (15)). The results are shown in Table 13. The results show that the proposedIDF + Top-k Predperforms the best. This indicates that the inverted index is a crucial component of GFM-RAG,