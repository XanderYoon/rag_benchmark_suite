the query is a description of the desired expressive character of a performance provided in the form of text. Expressivity-aware Music Performance Retrieval FIRE 2023, December 15–18, 2023, Panjim, India For each query, the piece is known, so the search space for the system is the set of performances of that piece. This is arguably a hard task; the retrieval system will have to be sensitive to very subtle musical differences. In our experiments, we will consider a retrieval result "correct" if the output of the system ranks the performance corresponding to the input text the highest. We take the basic framework of Music-Text Representation (MTR) by Doh et al. [ 14] and modify the text and audio encoders. We hypothesise that two kinds of modifications might improve the model’s effectiveness for expressivity-aware performance retrieval. First we need an audio encoder trained to extract features that are more attentive to the expressive qualities in music. Mid-level percep- tual features [5] have a significant capacity to capture such musical qualities and hold good predictive power for music emotion [ 8]. We thus replace the audio encoder in MTR with a mid-level feature model. This pre-trained model takes audio spectrograms as inputs and outputs seven mid-level features (seven real-valued scalars). To this we add an eighth feature: onset density [11], intended to model the ‘perceptual speed’ of a performance. The assumption here is that points close together in the space spanned by the mid-level features are similar in their expressive quality. Second, we investi- gate if the system works better with a text encoder trained using emotion labels. While the text encoder in MTR is a state-of-the-art BERT sentence model, in our task, the sentence structure has less of a consequence than obtaining accurate representations of the descriptive words. We thus experiment