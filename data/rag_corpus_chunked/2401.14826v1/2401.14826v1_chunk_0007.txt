performance. The Mid-level Perceptual Features Dataset [5] consists of 5k song snippets of 15 seconds each annotated according to seven mid-level descriptors: melodiousness, articulation, rhythm stability, rhythm complexity , dissonance, tonal stability , and modality (or â€˜minornessâ€™). The ratings for the seven mid-level perceptual features were collected through crowd-sourcing. We use a model trained on this dataset as our audio encoder, and we refer to this trained model as the â€œmid-level model", or â€œmid-level encoder" in the following paragraphs. We addonset density as an additional feature according to [11] (this gives the best results in our case). Additional Datasets: MusicCaps and Pitchfork Track Re- views. In order to effectively train a model, we require a substan- tial amount of audio-text paired data, and unfortunately, the Con Espressione dataset is insufficient in size for this particular task. To expand our dataset, we look towards two sources: the Music- Caps dataset, and a trusted and well-known music review website, Pitchfork (www.pitchfork.com). The MusicCaps dataset [ 2] consists of 5.5k music clips from diverse genres with paired text descriptions 4 APPROACH We define performance retrieval as the task of retrieving a par- ticular musical performance from a set of ğ¾ performances ğ‘ƒ = {ğ‘1, ğ‘2, . . . , ğ‘ğ¾ } of a musical piece such that the returned perfor- mance best matches the query ğ‘„ = {ğ‘¤ 1, ğ‘¤2, . . . , ğ‘¤ğ‘ } comprised of ğ‘ words. In our case, the query is a description of the desired expressive character of a performance provided in the form of text. Expressivity-aware Music Performance Retrieval FIRE 2023, December 15â€“18, 2023, Panjim, India For each query, the piece is known, so the search space for the system is the set of performances of that piece. This is arguably a hard task; the retrieval