thereby constitute the search space for each query. The assumption here is that we already know the piece (and thus the search space for that piece) for each query, and we want to retrieve the closest matching performance in terms of its expressive character. Inferring the piece itself from the query, while a necessary component of an end-to-end retrieval system, is not the focus of the present paper. The reader is encouraged to look at works on music search and retrieval such as [26] for more details on this topic. The subjective nature of the task also motivated us to consider human-oriented, perceptually relevant descriptors for representing expressive qualities extracted from audio recordings. In particular, 1As stated by a major music streaming platform, “Classical music is different. It has longer and more detailed titles, multiple artists for each work, and hundreds of recordings of well-known pieces" [1]. arXiv:2401.14826v1 [cs.SD] 26 Jan 2024 FIRE 2023, December 15–18, 2023, Panjim, India Chowdhury and Widmer we focus on so-called Mid-level Perceptual Features – relatively high-level musical qualities that are considered to be perceptually important [5]; in our work, these are learnt from human annotations. Previous research has shown these features to have the capacity to predict musical emotions as well as to disentangle different performances based on emotion [7–9]. Furthermore, on the text side, we note the inefficiency of tradi- tional word embeddings (Word2Vec, GloVe) to capture the intended emotion in descriptions of musical performances. Such language representation models carry the inductive bias that words used in the same context tend to possess similar meanings, thus result- ing in emotionally dissimilar words like happy and sad having close proximity in the representation space, due to them often oc- curring in similar contexts in the training data. To derive correct emotional meanings from our