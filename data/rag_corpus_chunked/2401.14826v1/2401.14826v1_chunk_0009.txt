the mid-level features are similar in their expressive quality. Second, we investi- gate if the system works better with a text encoder trained using emotion labels. While the text encoder in MTR is a state-of-the-art BERT sentence model, in our task, the sentence structure has less of a consequence than obtaining accurate representations of the descriptive words. We thus experiment with an emotion-enriched word embedding model (EWE) [3]. 4.1 Mid-level Features Audio Encoder and Emotion Enriched Text Encoder As reasoned earlier in this paper, we propose replacing the au- dio encoder in MTR with a trained mid-level feature model ( ùëìùëö in Figure 2). The input to this model is an audio spectrogram, the output is a vector of 8 mid-level features (including onset density). This model is domain-adapted for piano music, since that is our domain of interest for this paper. As shown in [10], domain adap- tation can improve mid-level prediction for piano audio without compromising the performance for other styles of music. The text encoder is also replaced by an emotion enriched word embedding (EWE) model ( ùëîEWE in Figure 2), which outputs an embedding of dimension 300 for each word. For a set of descrip- tive words, we take the resultant vector by adding all individual embeddings element-wise. Now since these two encoders are pre-trained, they do not project to a common embedding space. Hence we need a model to project the outputs of the encoders to a common space for enabling cross- modal retrieval. We choose to project the text embeddings onto the mid-level feature space using a linear model ‚Ñé. Preserving the mid- level feature predictions has the additional advantage of providing explainable insight into the retrieval process, due to the dimensions possessing interpretable musical meanings. This is described in the next section. For