the joint embedding space [24]. A ResNet-50 [17] is used for the audio backbone, and a downsized-Transformer [27] for the text backbone. They use 250k audio-text pairs from a production music library as the dataset. In “MuLan" [18] a much larger dataset of 44m recordings and weakly-associated, free-form text annotations is used. They use a pre-trained BERT [13] as their text encoder and experiment with two different audio encoders – ResNet-50 and Audio Spectrogram Transformer [16]. A more recent work is “Music and Text Representation" learning (MTR) [14]. It lays out an investigation into effective design choices for universal representation learning for text-to-music retrieval systems. This work uses a set of 500k music-text pairs. They use a modified version of the Music Tagging Transformer [28] as the audio encoder, and two different text encoders – pre-trained GloVe [23] and pre-trained BERT. We were able to obtain the trained models for this work, which we use as a baseline for our work. Emotion Embedding Spaces : In [ 30], the authors propose “emotion embeddings" for retrieval of musical pieces that match the emotional characteristic of stories. They use a ResNet model [29] as the audio backbone and a pre-trained DistilBERT [25] as the text backbone. However, they do not use natural language in the audio-text pairs in their experiments; rather the text component comes from the labels or tags associated with the audio clips, which are mapped to the embedding space using a Word2Vec model or emotion lexicons. 2.1 Music-Text Representation (MTR) We consider the Music-Text Representation (MTR) model by Doh et al. [14] as the baseline system. It is a cross-modal retrieval sys- tem that projects audio and text representations onto a common embedding space and reduces the distance between paired vectors during training. It is trained on 500k audio-text