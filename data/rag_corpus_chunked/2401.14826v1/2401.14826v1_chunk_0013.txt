Table 1: Retrieval results using different audio and text encoders. The values here are for a model trained with both Pitchfork and Mu- sicCaps augmentation and no PCA on text embeddings (see Table 2). For the random baseline, performances are chosen at random. Text Enc Audio Enc Top-1 Top-2 MRR (Random Baseline) 0.18 0.37 0.44 MTR MTR 0.20 0.42 0.46 MTR Mid-level 0.22 0.53 0.50 EWE MTR 0.22 0.42 0.47 EWE Mid-level 0.38 0.64 0.61 we obtain with random guessing (Table 1, 1st row). Moreover, only 20% of the queries return the correct performance as the top result. On the other hand, our model with emotion word embeddings (EWE) and mid-level features returns the correct performance for Table 2: Effect of data augmentation and PCA on retrieval based on EWE embeddings and Mid-level features Augmentation Text Emb Metrics Pitchfork MusicCaps Transform (PCA) Top-1 Top-2 MRR ✗ ✗ ✗ 0.22 0.46 0.48 ✗ ✗ ✓ 0.29 0.49 0.52 ✓ ✗ ✗ 0.24 0.44 0.48 ✓ ✗ ✓ 0.40 0.58 0.60 ✗ ✓ ✗ 0.33 0.57 0.57 ✗ ✓ ✓ 0.27 0.53 0.52 ✓ ✓ ✗ 0.38 0.64 0.61 ✓ ✓ ✓ 0.36 0.56 0.57 38% of queries, with an MRR value of 0.61 (Table 1, last row), mean- ing the average rank of the correct performance is about 1.63. While our method shows significant improvement over the baseline, it is important to note that this is a highly subjective task and the dataset we have at hand is not large. The main objective of this paper is to provide a proof-of-concept that models trained using domain-specific perceptual features can lend a significant advan- tage in cross-modal retrieval applications. From Table 2, we see that augmenting with additional data has a minor but positive effect on the results whereas PCA tends to