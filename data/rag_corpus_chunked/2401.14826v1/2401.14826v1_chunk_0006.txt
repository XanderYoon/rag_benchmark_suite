using a Word2Vec model or emotion lexicons. 2.1 Music-Text Representation (MTR) We consider the Music-Text Representation (MTR) model by Doh et al. [14] as the baseline system. It is a cross-modal retrieval sys- tem that projects audio and text representations onto a common embedding space and reduces the distance between paired vectors during training. It is trained on 500k audio-text pairs with the text formed by concatenating tags from different sources. This data is a subset of the Million Song Dataset [6] and contains songs from a mix of different genres. We use weights of the best version of their system according to the results on their paper, which is a contrastive model type with a BERT text encoder and stochastically sampled text representations. 3 DATASETS The Con Espressione Dataset [7] consists of recordings of 9 pi- ano pieces played by different famous pianists (making a total of 45 performances), and associated free-text descriptions for each performance, collected from a large number of listeners. The study participants were asked to describe the expressive character of each performance. Typical characterisations in the dataset are adjec- tives like “cold”, “playful”, “dynamic”, “passionate”, but also more complex phrases such as “controlled with speed", “smooth tempo variation", “emotional with dynamics" etc. In this work, we use the aggregated answers for each performance. That is, all words or phrases used by different participants for a performance are concatenated into a single text description of the performance. The Mid-level Perceptual Features Dataset [5] consists of 5k song snippets of 15 seconds each annotated according to seven mid-level descriptors: melodiousness, articulation, rhythm stability, rhythm complexity , dissonance, tonal stability , and modality (or ‘minorness’). The ratings for the seven mid-level perceptual features were collected through crowd-sourcing. We use a model trained on this dataset as our audio