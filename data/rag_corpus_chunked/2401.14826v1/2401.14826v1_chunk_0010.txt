to a common space for enabling cross- modal retrieval. We choose to project the text embeddings onto the mid-level feature space using a linear model â„. Preserving the mid- level feature predictions has the additional advantage of providing explainable insight into the retrieval process, due to the dimensions possessing interpretable musical meanings. This is described in the next section. For â„, a simple linear regression model proved suffi- cient. In some cases (see Table 2), transforming the text embeddings with principal component analysis (PCA) improved the results. errant drum machine noises, playful synths, organic, electronic Paired Music Audio Content Text Description Pretrained Mid-level Encoder Pretrained Text Encoder Embeddings to Mid-level Model Loss (MSE) Figure 2: In our system, the audio and text encoders of a MTR model [14] are replaced by a mid-level feature model and emotion enriched word representation model respectively. 4.2 Experiments and Results We perform piece-wise cross-validation over the Con Espressione dataset to fully utilise the available data, i.e., in each run the test set is the set of all performances of a piece in the dataset. The rest of the dataset is the train set for that run. This results in 9-fold cross-validation as we have 9 pieces in the dataset. For retrieval, we use the cosine similarity in the mid-level feature space: cosine_similarity(m1, m2) = m1 Â·m2 âˆ¥m1 âˆ¥ Â· âˆ¥m2 âˆ¥ (1) We use top-ğ‘˜ ratio and Mean Reciprocal Rank (MRR) as evalu- ation metrics. Top-ğ‘˜ ratio (ranges from 0 to 1, with higher scores indicating better retrieval performance) is defined as the number of queries for which the correct performance has a rank equal to or better than ğ‘˜ among all performances of the piece for which the query is made. We use ğ‘˜ = 1 and ğ‘˜ = 2. Mean Reciprocal Rank