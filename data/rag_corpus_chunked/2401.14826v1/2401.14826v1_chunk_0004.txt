musical performances. Such language representation models carry the inductive bias that words used in the same context tend to possess similar meanings, thus result- ing in emotionally dissimilar words like happy and sad having close proximity in the representation space, due to them often oc- curring in similar contexts in the training data. To derive correct emotional meanings from our text queries, we experiment with emotion-enriched word embeddings from Agrawal et al. [3] and find that they improve retrieval results significantly, particularly when combined with mid-level features on the audio side. 2 RELATED WORK Our work sits in the broader area of cross-modal retrieval. This is an area of active research in general, and is of great significance to music information retrieval [14, 20, 22]. Language-based audio retrieval is currently witnessing much interest from the research community as can be seen from active participation in competitions like DCASE [12]. In this section, we look at some recent work in text-based retrieval for music. Audio-Language Learning for Music : Text-based audio re- trieval is typically done by learning a common embedding space of aligned audio and text embeddings. In â€œMusCALL" [21], this is done using a multimodal contrastive learning (MCL) approach. Two en- coders, ğ‘“ğ‘ (Â·) for audio and ğ‘“ğ‘¡ (Â·) for text, are learnt such that for any audio-text pair (ğ‘ğ‘–, ğ‘¡ğ‘– ), the resulting embeddings ğ‘§ğ‘,ğ‘– = ğ‘“ğ‘ (ğ‘ğ‘– ) and ğ‘§ğ‘¡,ğ‘– = ğ‘“ğ‘¡ (ğ‘¡ğ‘– ) lie close in the joint embedding space [24]. A ResNet-50 [17] is used for the audio backbone, and a downsized-Transformer [27] for the text backbone. They use 250k audio-text pairs from a production music library as the dataset. In â€œMuLan" [18] a much larger dataset of 44m recordings and weakly-associated, free-form text annotations is used. They use a pre-trained BERT [13] as their