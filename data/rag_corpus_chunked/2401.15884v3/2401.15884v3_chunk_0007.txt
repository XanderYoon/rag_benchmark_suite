for generating the output Y. This framework can be formulated as: P (Y|X ) = P (D|X )P (Y, D|X ). (1) It shows that the retriever and generator are seam- lessly coupled, exhibiting low risk tolerance. Any unsuccessful retrieval can result in an unsatisfac- tory response, regardless of the impressive abilities of the generator. This is exactly the focus of this paper to improve the robustness of generation. 4 CRAG 4.1 Overview of Model Inference Figure 2 and Algorithm 1 present an overview of CRAG at inference, which designs corrective strategies to improve the robustness of generation. Given an input query and the retrieved documents from any retriever, a lightweight retrieval evaluator is constructed to estimate the relevance score of retrieved documents to the input query (Sec- tion 4.2). The relevance score is quantified into a total of three confidence degrees and then triggered the corresponding actions: {Correct, Incorrect, Ambiguous} (Section 4.3). If the action Correct is triggered, the retrieved documents will be re- fined into more precise knowledge strips. This refinement operation involves knowledge decom- position, filter, and recomposition (Section 4.4). If the action Incorrect is triggered, the retrieved documents will be discarded. Instead, web searches are resorted to and regarded as complementary knowledge sources for corrections (Section 4.5). Eventually, when it cannot confidently make a correct or incorrect judgment, a soft and balanced action Ambiguous which combines both of them is triggered. After optimizing the retrieval results, an arbitrary generative model can be adopted. 4.2 Retrieval Evaluator It is natural to wonder whether the retrieved docu- ments are accurate or not before using them, which is significant since irrelevant or misleading mes- sages can be identified in this way. The accuracy of the retrieval evaluator undeniably plays a pivotal role in shaping the overall system performance, as