Corrective Retrieval Augmented Generation Shi-Qi Yan1*, Jia-Chen Gu2*, Yun Zhu3, Zhen-Hua Ling1 1National Engineering Research Center of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China 2Department of Computer Science, University of California, Los Angeles 3Google DeepMind yansiki@mail.ustc.edu.cn, gujc@ucla.edu, yunzhu@google.com, zhling@ustc.edu.cn Abstract Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Al- though retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved docu- ments, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG ) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval ac- tions can be triggered. Since retrieval from static and limited corpora can only return sub- optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then- recompose algorithm is designed for retrieved documents to selectively focus on key infor- mation and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches. 1 1 Introduction Large language models (LLMs) have attracted increasing attention and exhibited impressive abili- ties to understand instructions and generate fluent language texts (Brown et al., 2020; Ouyang et al., 2022; Touvron et al., 2023a). Nevertheless, LLMs inevitably manifest hallucinations (Ji et al., 2023) due to their struggle with factual errors (Mallen et al., 2023; Min et al., 2023) and inability to secure