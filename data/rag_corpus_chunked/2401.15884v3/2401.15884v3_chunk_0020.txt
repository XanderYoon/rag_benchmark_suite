questions were not rewritten into queries consisting of keywords during knowledge searching. Eventually, removing knowledge selection denoted that all searched con- tent of web pages was all regarded as the external knowledge without selection. These results help derive the findings that the performance of the final system degraded no matter which knowledge utilization operation was removed, revealing that each knowledge utilization operation contributed to improving the utilization of knowledge. 5.5 Accuracy of the Retrieval Evaluator The quality of the retrieval evaluator significantly determined the performance of the entire system. Given the document retrieval results, we assessed whether the retrieval evaluator can accurately determine the overall quality of these results. The assessment accuracy on the PopQA dataset of our retrieval evaluator and the commercial LLM ChatGPT on the document retrieval results was shown in Table 4. The prompts of ChatGPT, ChatGPT-CoT, and ChatGPT-few-shot used in our experiments can be referred to in Appendix A. Results reveal that the lightweight T5-based re- trieval evaluator significantly outperformed the competitive ChatGPT in all settings. 5.6 Robustness to Retrieval Performance To further verify the robustness of the proposed method to retrieval performance, we studied how the generation performance changed given different 69.8 (Actual) 60 50 40 30 20 10 Accuracy of retrieval 20 30 40 50 60 70Accuracy of generation no retrieval Self-RAG Self-CRAG Figure 3: The generation performance of Self-RAG and Self-CRAG given different retrieval performance on the PopQA dataset with SelfRAG-LLaMA-7b. The lower horizontal line demonstrates the performance of the generator without retrieval. LLaMA2-hf-7b SelfRAG-LLaMA2-7b PopQA CRAG 54.9 59.8 RAG 50.5 52.8 RAG w. web 52.2 53.8 Self-CRAG 49.0 61.8 Self-RAG 29.0 54.9 Self-RAG w. web 24.9 57.9 Table 5: Comparison results between CRAG , Self- CRAG and RAG, Self-RAG with the same input in terms of accuracy. retrieval performance. A part of