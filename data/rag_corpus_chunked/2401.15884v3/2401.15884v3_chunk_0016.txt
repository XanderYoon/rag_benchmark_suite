bold scores indicate the best performance using a specific LLM. * indicates the results reproduced by us, otherwise results except ours are cited from their original papers. to improve the factuality of LLM generations. Propriety LLMs such as LLaMA2-chat 13B and ChatGPT are also included. Standard RAG. We evaluated the standard RAG (Lewis et al., 2020) where an LM generates output given the query prepended with the top retrieved documents using the same retriever as in our system. Here we adopted several pub- lic instruction-tuned LLMs, including LLaMA2- 7B, 13B (Touvron et al., 2023b), Alpaca-7B,13B (Dubois et al., 2023), as well as LLaMA2-7B instruction-tuned in Self-RAG (Asai et al., 2024). Advanced RAG.(1) SAIL (Luo et al., 2023) that instruction-tuned an LM on the Alpaca instruction- tuning data with top retrieved documents inserted before instructions. (2) Self-RAG (Asai et al., 2024) that tuned the LLaMA2 on the instruction- tuning data comtaining several sets of reflection tokens which were labeled by GPT-4 (OpenAI, 2023). (3) Following Asai et al. (2024), we also cited the results of retrieval-augmented baselines trained with private data: Ret-ChatGPT and Ret- LLaMA-chat, which deploy the same augmenta- tion technique above, as well as perplexity.ai, an InstructGPT-based production search system. 5.3 Results Table 1 presents the results on four datasets. The model coupling the proposed method with standard RAG is named CRAG and that coupling with Self- RAG is named Self-CRAG. Readers can refer to Appendix B.3 for more implementation details of our proposed methods. From these results, we can conclude the following findings: First, the proposed method can significantly improve the performance of RAG and Self-RAG. Specifically, as shown in table 1, CRAG outper- formed RAG by margins of 7.0% accuracy on PopQA, 14.9% FactScore on Biography, 36.6% accuracy on PubHealth, and 15.4% accuracy on Arc-Challenge when based