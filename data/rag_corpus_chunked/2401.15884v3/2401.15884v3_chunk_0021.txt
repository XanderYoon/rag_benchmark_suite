horizontal line demonstrates the performance of the generator without retrieval. LLaMA2-hf-7b SelfRAG-LLaMA2-7b PopQA CRAG 54.9 59.8 RAG 50.5 52.8 RAG w. web 52.2 53.8 Self-CRAG 49.0 61.8 Self-RAG 29.0 54.9 Self-RAG w. web 24.9 57.9 Table 5: Comparison results between CRAG , Self- CRAG and RAG, Self-RAG with the same input in terms of accuracy. retrieval performance. A part of accurate retrieval results were deliberately removed at random to imitate a low-quality retriever and evaluate how the performance changed. Figure 3 demonstrated the performance change of Self-RAG and Self- CRAG on the PopQA dataset. It can be seen that the generation performance of Self-RAG and Self-CRAG dropped as the retrieval performance dropped, indicating that the generator relied heavily on the quality of the retriever. Furthermore, as the retrieval performance dropped, the generation performance of Self-CRAG dropped more slightly than that of Self-RAG. These results imply the superiority of Self-CRAG over Self-RAG on en- hancing the robustness to retrieval performance. 5.7 Consistent Supplementation of Web Search Knowledge This paper highlights the necessity of enhancing the retrieved context by incorporating additional information when the initial retrieval results are irrelevant and unreliable. Meanwhile, it is also crucial to confirm that the primary improvements in our method stem from the self-correction mech- TFLOPs per token executing time(s) RAG 26.5 0.363 CRAG 27.2 0.512 Self-RAG 26.5 ∼132.4 0.741 Self-CRAG 27.2 ∼80.2 0.908 Table 6: computational overhead assessment of RAG, CRAG , Self-CRAG, and Self-RAG about FLOPs per token on GPUs and executing time per instance. The upper bound of Self-CRAG is lower because only three passages are provided as input (correct, incorrect and ambiguous content). All the data in the table only represents a rough estimate of the generation phase, the retrieval and data-processing stages are not included. anism, rather than solely from the supplementary