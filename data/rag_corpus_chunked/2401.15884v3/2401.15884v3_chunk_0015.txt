strategies. 5.2 Baselines We primarily compared CRAG with both ap- proaches with and without retrieval, where the latter can be further split into standard RAG and latest advanced RAG, including: Baselines without retrieval. We evaluated some public LLMs, LLaMA2-7B,13B (Touvron et al., 2023b), instruction-tuned models, Alpaca-7B,13B (Dubois et al., 2023), and CoVE 65B (Dhuliawala et al., 2024) which introduces iterative engineering 3In this study, Google Search API is utilized for searching. PopQA Bio Pub ARC Method (Accuracy) (FactScore) (Accuracy) (Accuracy) LMs trained with propriety data LLaMA2-c13B 20.0 55.9 49.4 38.4 Ret-LLaMA2-c13B 51.8 79.9 52.1 37.9 ChatGPT 29.3 71.8 70.1 75.3 Ret-ChatGPT 50.8 - 54.7 75.3 Perplexity.ai - 71.2 - - Baselines without retrieval LLaMA27B 14.7 44.5 34.2 21.8 Alpaca7B 23.6 45.8 49.8 45.0 LLaMA213B 14.7 53.4 29.4 29.4 Alpaca13B 24.4 50.2 55.5 54.9 CoVE65B - 71.2 - - Baselines with retrieval LLaMA27B 38.2 78.0 30.0 48.0 Alpaca7B 46.7 76.6 40.2 48.0 SAIL - - 69.2 48.4 LLaMA213B 45.7 77.5 30.2 26.0 Alpaca13B 46.1 77.7 51.1 57.6 LLaMA2-hf-7b RAG 50.5 44.9 48.9 43.4 CRAG 54.9 47.7 59.5 53.7 Self-RAG* 29.0 32.2 0.7 23.9 Self-CRAG 49.0 69.1 0.6 27.9 SelfRAG-LLaMA2-7b RAG 52.8 59.2 39.0 53.2 CRAG 59.8 74.1 75.6 68.6 Self-RAG 54.9 81.2 72.4 67.3 Self-CRAG 61.8 86.2 74.8 67.2 Table 1: Overall evaluation results on the test sets of four datasets. Results are separated based on the generation LLMs. Bold numbers indicate the best performance among all methods and LLMs. Gray-colored bold scores indicate the best performance using a specific LLM. * indicates the results reproduced by us, otherwise results except ours are cited from their original papers. to improve the factuality of LLM generations. Propriety LLMs such as LLaMA2-chat 13B and ChatGPT are also included. Standard RAG. We evaluated the standard RAG (Lewis et al., 2020) where an LM generates