While Toolformer (Schick et al., 2023) is pre-trained for calling APIs such as Wikipedia. In addition, in some long-text generation tasks, external knowl- edge is needed more than once, and when to retrieve should be concerned. Jiang et al. (2023) actively anticipate future content and decide when and what to retrieve in long-form generation. Compared with recent studies (Schick et al., 2023; Luo et al., 2023; Asai et al., 2024) that are the most relevant to our work, a main difference should be highlighted. These approaches target on exploiting retrieval as a useful tool to augment generation or whether retrieval is necessary, while this study particularly studies the scenarios where the retriever returns inaccurate results. To the best of our knowledge, this paper makes the first attempt to explore and design corrective strategies for RAG to improve its robustness of generation. 3 Task Formulation Following previous work (Lewis et al., 2020; Asai et al., 2024), given input X and an accessible corpus containing a large amount of knowledge documents C = {d1, ..., dN }, the system is ex- pected to generate the output Y. The entire framework is usually divided into a retriever R and a generator G. The retriever R aims to retrieve the top-K documents D = {dr1, ..., drk } that are relevant to the input X from the corpus C. Based on the input X and the retrieved results D, the generator G is responsible for generating the output Y. This framework can be formulated as: P (Y|X ) = P (D|X )P (Y, D|X ). (1) It shows that the retriever and generator are seam- lessly coupled, exhibiting low risk tolerance. Any unsuccessful retrieval can result in an unsatisfac- tory response, regardless of the impressive abilities of the generator. This is exactly the focus