be seen that CRAG still showed competitive performance when the underlying LLMs was changed from SelfRAG-LLaMA2-7b to LLaMA2-hf-7b, while the performance of Self- RAG dropped significantly, even underperforming the standard RAG on several benchmarks. The reason for these results is that Self-RAG needs to be instruction-tuned using human or LLM annotated data to learn to output special critic tokens as needed, while this ability is not learned in common LLMs. CRAG does not have any requirements for this ability. As you can imagine, when more advanced LLMs are available in the future, they can be coupled withCRAG easily, while additional instruction tuning is still necessary for Self-RAG. LLaMA2-hf-7b SelfRAG-LLaMA2-7b CRAG 54.9 59.8 w/o. Correct 53.2 58.3 w/o. Incorrect 54.4 59.5 w/o. Ambiguous 54.0 59.0 Self-CRAG 49.0 61.8 w/o. Correct 43.6 59.6 w/o. Incorrect 47.7 60.8 w/o. Ambiguous 48.1 61.5 Table 2: Ablation study for removing each single action on the PopQA dataset in terms of accuracy. LLaMA2-hf-7b SelfRAG-LLaMA2-7b CRAG 54.9 59.8 w/o. refinement 49.8 54.2 w/o. rewriting 51.7 56.2 w/o. selection 50.9 58.6 Self-CRAG 49.0 61.8 w/o. refinement 35.9 52.2 w/o. rewriting 37.2 58.4 w/o. selection 24.9 57.9 Table 3: Ablation study for removing each knowledge utilization operation on the PopQA in terms of accuracy. 5.4 Ablation Study The impact of each triggered action. To fur- ther verify the effectiveness of triggered actions designed in the retrieval evaluator, ablation tests for removing each single action in the proposed method were conducted as shown in Table 2. Evaluations on the PopQA dataset were conducted to demonstrate the performance change in terms of accuracy. Specifically, when the action Correct or Incorrect was removed, it was merged with Ambiguous so that the proportion that originally triggered Correct or Incorrect would trigger Ambiguous. On the other hand, when the action Ambiguous was removed, there