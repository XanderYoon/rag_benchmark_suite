2023) is a short-form generation task. Generally, only one entity of factual knowledge is expected to be answered for each single question. In our experiments, we exactly followed the setting in Self-RAG (Asai et al., 2024) which evaluated methods on a long-tail subset consisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100. Accuracy was adopted as the evaluation metric. Biography (Min et al., 2023) is a long-form generation task that is tasked to generate a detailed biography about a certain entity. Following previ- ous work, FactScore (Min et al., 2023) was adopted to evaluate the generated biographies. PubHealth (Zhang et al., 2023a) is a task in health care domain consisting of true-or-false questions. Claims are represented about health with factual information, and the model is tasked to verify the authenticity and give the judgment. Accuracy was adopted as the evaluation metric. Arc-Challenge (Bhakthavatsalam et al., 2021) is a multiple-choice question task about some daily commonsense science phenomena. Given a scientific event that occurs in daily life, the model is required to select the correct description among 3 or 4 optional choices. Accuracy was adopted as the evaluation metric as well. B.2 Experiments compute Resources We used NVIDIA A800 80GB GPU for experi- ments. For LLaMA-2 (7B) generation, it occupies over 40GB memory during inference. For T5-large (0.77B) fine-tuning, it takes much less compared with LLaMA-2. B.3 Implementation Details Retrieval Evaluator: We fine-tuned the retrieval evaluator based on the lightweight T5-large (Raffel et al., 2020) pre-trained model. The dataset we used is the version provided by Self-RAG (Asai et al., 2024). Specifically, the original PopQA dataset consists of 14k samples, 1,399 of which were used for testing following Self-RAG (Asai et al., 2024), and the remaining were used for fine-tuning to avoid information leakage. Besides,