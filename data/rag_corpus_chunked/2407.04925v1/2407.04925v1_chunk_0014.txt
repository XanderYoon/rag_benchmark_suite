course titles, ratings, and URLs—while green highlights how we addressed the “ cold- start” problem, resulting in recommendations of the three most popular (based on course ratings) and easiest courses (based on its difficulty level), as depicted in the output mod- ule labeled 1 in Figure 6. The generated response in re- sponse to varied prompts underscores the system’s robust- ness; for instance, when the template specifies “ recommend three courses at a time”, the output consistently includes ex- actly three courses. Similarly, if the prompt contains ‘course URLs and titles’, the system reliably appends this informa- tion to each recommended course, ensuring that the output meticulously adheres to the specified criteria. 5. CONCLUSIONS In this study, we have demonstrated the application of LLMs as course recommender systems, particularly within MOOCs. Our findings confirm the potential of LLMs to deliver per- sonalized course recommendations based on user’s different requirements. We initially compared four LLMs, including GPT-3.5 Turbo and GPT-4. Ultimately, we selected GPT- 3.5 as the back-end model for the RAMO system due to its comparable performance to GPT-4 at a lower cost. Al- though the Llama models are free to access, we found that the GPT models were significantly faster. Specifically, GPT- 3.5 had an approximate response time of 3 seconds, whereas Llama 2 and Llama 3 took approximately 5 minutes and 8 minutes, respectively. Furthermore, the integration of RAG has enhanced the quality of recommendation outputs, as evidenced by the generated responses based on various user prompts, which are highly related to user’s needs and all came from the knowledge base. Additionally, our system supports conversational interaction with users, which could be seamlessly integrated into numerous online educational platforms. Our use of open-source LLMs (e.g. Llama 2 and Llama 3 [27]) has also been validated, proving to be