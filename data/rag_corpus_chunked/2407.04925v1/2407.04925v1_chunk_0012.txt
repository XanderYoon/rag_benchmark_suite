to the same user interest—a certain topic the user wants to learn, and this delay increased with the complexity of the user’s input regarding relevant skills. 4.2 LLM vs. LLM with RAG To explore how well our LLMs can provide personalized course recommendations, we used prompts that specified a 2https://www.kaggle.com/code/sagarbapodara/ coursera-course-recommendation-system-webapp particular skill to be learned. The non-RAG LLM (based on GPT-3.5) delivered detailed suggestions for relevant courses available on Coursera, utilizing its internal database of courses. In contrast, the recommendations from the RAG-enhanced LLM varied according to the specific prompt template used by the retriever. This adaptability allows developers to tai- lor the quantity and detail of the courses recommended, showcasing the flexibility of the RAG approach. The user interface and the outcomes for a query focused on learning a specific skill are illustrated in Figure 4. Figure 4: Output for a specific user question We modified the retrieval prompts and generation queries to test the adaptability of our recommendation system. First, we conducted tests on various user queries using the same prompt template to compare the variations in output. The first module in Figure 5 illustrates the system’s response to a “cold start ” problem, while modules 2 through 6 demon- strate how the output varies based on user questions about the number of courses recommended and the level of de- tail provided, such as reasons for recommendations, URLs, and other specifics. For example, when user asks question like “I want to learn python, can you recommend me some courses?”, RAMO can give the output to the user: “ Sure! Here are some recommended Python courses for you: 1. In- troduction to Python 2. Crash Course on Python 3. First Python Program 4. Python Basics These courses cover a range of topics from basic syntax to building interactive