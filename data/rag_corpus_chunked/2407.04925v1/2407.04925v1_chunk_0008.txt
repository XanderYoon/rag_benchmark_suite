or four characters, according to Open AI [1]) that the model can process in a single input. While some models, like Llama 2 and Llama 3, are free to use on small- scale dataset, due to their open-source nature, others may incur costs based on usage or subscription plans [27]. Table 2: Cost and token limit of models we used LLM Model Output Cost Token Limit GPT-3.5 Turbo 0.50 per 1M tokens 4,096 tokens GPT-4 30.00 per 1M tokens 8,192 tokens Llama-2 Free 4,096 tokens Llama-3 Free 8,000 tokens We then leveraged the RAG approach to enhance the sys- tem’s understanding of the user context. As shown in Figure ??, RAG consists of two primary components: the retriever and the generator. The retriever aims to enhance the prompt templates, which ‘augment’ the retrieval process, tailoring it to specific user queries. The knowledge base used for the re- trieval process can contain any format of course data (e.g., Figure 2: Workflow for the RAMO System csv, pdf, and json), providing a flexible and rich source of in- formation for generating responses and we used the largest MOOC platform—coursera’s course dataset in csv format as the knowledge base. The dataset was transformed into text embeddings and stored in the vector database. These embeddings were then used to find high-quality, relevant in- formation, which was incorporated into the prompt for the generator. Here we use OpenAI embedding model ( text- embedding-ada-002 [29]) to tokenize the course data and store the embeddings in vector store, considering its ad- vantage over BERT (Bidirectional Encoder Representations from Transformers) [7], while OpenAI embeddings [29] of- fer better generalization and contextual understanding [28], making them more suitable for diverse educational content. The generator is powered by LLMs, which generate the tex- tual contents based on the engineered prompts.