focus on the matter of unjudged documents. It is pos- sible that we are underestimating the effectiveness of the retrieval pipelines when facing query variations if ( I) the number of un- judged documents in the top-10 ranked lists increases and (II) they turn out to be relevant. When counting the amount of judged doc- uments in the top-10 ranked lists of the retrieval pipelines, we find that on average the number actually increases (4.30% for TREC-DL- 2019 and 0.36% for ANTIQUE), meaning that the performance drops of the retrieval pipelines can not be attributed to un- judged documents being brought up in the ranking by the query variations. Woodstock ’18, June 03–05, 2018, Woodstock, NY Gustavo Penha, Arthur Câmara, and Claudia Hauff TREC−DL−2019ANTIQUE BM25 RM3 KNRM CKNRM EPIC BERT T5 −1.0 −0.5 0.0 0.5 −1.0 −0.5 0.0 0.5 nDCG@10 Δ category misspelling naturality ordering paraphrase Figure 2: Distribution of nDCG@10 Δ when replacing the original query by the methods of each category. Table 6: Pearson correlation between the nDCG@10 Δ of ranking models when we replace the original query by query varia- tions of different categories for the ANTIQUE dataset. Misspelling Naturality BM25 RM3 KNRM CKNRM EPIC BERT T5 BM25 RM3 KNRM CKNRM EPIC BERT T5 BM25 1.00 0.88 0.54 0.54 0.58 0.48 0.51 BM25 1.00 0.90 0.25 0.26 0.43 0.34 0.25 RM3 1.00 0.52 0.53 0.58 0.44 0.47 RM3 1.00 0.22 0.25 0.39 0.29 0.24 KNRM 1.00 0.74 0.67 0.60 0.68 KNRM 1.00 0.63 0.54 0.40 0.40 CKNRM 1.00 0.63 0.54 0.59 CKNRM 1.00 0.46 0.35 0.39 EPIC 1.00 0.67 0.74 EPIC 1.00 0.48 0.50 BERT 1.00 0.80 BERT 1.00 0.57 T5 1.00 T5 1.00 Ordering Paraphrasing BM25 RM3 KNRM CKNRM EPIC BERT T5 BM25 RM3 KNRM CKNRM EPIC BERT T5 BM25 1.00 1.00 -0.01 0.01 0.00 -0.06