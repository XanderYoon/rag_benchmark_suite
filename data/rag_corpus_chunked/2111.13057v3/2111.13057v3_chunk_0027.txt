The second highest effect on both datasets are the query varia- tions from the paraphrasing category (-0.08 and -0.03 of nDCG@10 Î”) followed by naturality (-0.05 and -0.03). Compared to the mis- spelling variations which in most cases degrade the effectiveness of our models, paraphrasing and naturality have more queries for which the effect is positive, rendering the overall nDCG@10 Î” smaller. Queries from theordering category have the least effect (less than 0.01). Since traditional methods are in fact bag-of-words models, changing the word order will not have any effect on them, which makes the average of all modelsâ€™ nDCG@10 Î” closer to zero. In the following section, we take a further look at how each type of ranking model is affected by each query variation method. Evaluating the Robustness of Retrieval Pipelines with Query Variation Generators Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY BM25 BERT KNRM RM3 T5 CKNRM EPIC BM25 BERT KNRM RM3 T5 CKNRM EPIC TRECâˆ’DLâˆ’2019 ANTIQUE âˆ’600 âˆ’400 âˆ’200 0 âˆ’600 âˆ’400 âˆ’200 0 âˆ’200 âˆ’100 0 100 TSNE_0 TSNE_1 a a aNN TNN Trad Figure 3: tSNE dimensionality reduction where each model is represented by the nDCG@10 Î” values obtained for each query and variation method ( #ğ‘„ Ã— #ğ‘€). 5.1.2 Robustness by Model Category. When we consider how dif- ferent models are affected by the query variations, we see from Figure 2 that with the exception of ordering, which has no effect on BM25, RM3 and KNRM, other transformations have a similar over- all distribution of nDCG@10 Î” amongst different models. In order to understand if models (and category of models) make mistakes on the same queries, we label the models as follows: BM25 and RM3 are labelled as Trad (lexical matching), KNRM and CKNRM (neural network based) are labelled as NN and EPIC,