change one or more query terms in the process of paraphrasing. BackTranslation Applies a translation method to the query to a pivot language, i.e. an auxiliary language, and from the pivot language back to the original language of the query, i.e. English. In our experiments we employ the M2M100 [20] model, a multilin- gual model that can translate between any pair of 100 languages, and we use ‘German’as the pivot language, which yielded better results—shown by manual inspection of the generated variations— than the other two languages for which the model had the most data for training (‘Spanish’and ‘French’). This technique has been used before as a way to generate paraphrases [21, 36]. T5QQP Applies an encoder-decoder transformer model (here we employ T5 [47]) that was fine-tuned on the task of generating a paraphrase question from the original question 4. The model employs the Quora Question Pairs5 dataset for fine-tuning, which has 400k pairs of questions like the following: ‘How do you start a bakery? ’ → ‘How can one start a bakery business? ’. We also tested T5 models fine-tuned for PAWS [59] and the combination of PAWS and Quora Question Pairs, but the manual inspection of the generated queries revealed that T5 fine-tuned for Quora Question Pairs generated a higher number of valid variations. WordEmbedSynSwap Replaces a non-stop word by a synonym as defined by the nearest neighbour word in the embedding space according to a counter fitted-Glove embedding which yields better synonyms than standard Glove embeddings [39]. WordNetSynSwap Replaces a non-stop word by a the first synonym found on WordNet 6. If there are no words with valid synonyms it will not output a valid variation. 4As available here https://huggingface.co/ramsrigouthamg/t5_paraphraser 5https://www.kaggle.com/c/quora-question-pairs 6https://wordnet.princeton.edu/ Table 4: Statistics of the datasets. TREC-DL-2019 ANTIQUE #Q train 367013 2426 #Q valid 5193