stop-words to modify) ‚àí1.0 ‚àí0.5 0.0 0.5 10 100 1000 Re‚àíranking threshold nDCG@10 Œî category misspelling naturality ordering paraphrase Figure 1: Distribution of nDCG@10 Œî for different re- ranking thresholds when using BERT as a re-ranker. 5.1 Robustness to Query Variations In order to explore the robustness of our three types of ranking models (traditional, neural and transformer-based), we compare the effectiveness of our models when we replace the original query with the respective query variation. The results of this experiment are displayed in Table 5 for both the TREC-DL-2019 and ANTIQUE datasets. Each row shows the effectiveness of the ranking models (columns) when using the queries obtained from each automatic query variation method. The last column (#ùëÑ) displays the number of valid queries generated by each query variation method; the invalid queries are replaced with the original ones10. The results show that for most of the query variations and ranker combinations we observe a statistical significant effectiveness drop (49 out of 70 times for TREC-DL-2019 and 54 out of 70 times for ANTIQUE), and that no set of query variations improves statisti- cally over using the original query. If we look into the percentage of overall effectiveness decreases considering only the valid queries, we see on average that the models become 20.62% and 19.21% less effective for TREC-DL-2019 and ANTIQUE respectively. This an- swers our main research question indicating that retrieval pipelines are not robust to query variations . This confirms previous empirical evidence that query variations induce a big vari- ability effect on different IR systems [ 6, 63]. We show that even with newer large-scale collections such as TREC-DL-2019, pipelines with neural ranking models are not robust to such variations. There are several potential explanations for this drop in effective- ness besides the lack of robustness of