success of pre-trained transformer-based language models such as BERT [19] and T5 [47] on several IR benchmarks—a comprehen- sive account of the effectiveness gains can be found in [29]—has lead to research on understanding their behaviour and the reasons behind their significant gains in ranking effectiveness [12, 32, 43, 46, 61]. Câmara and Hauff [12] showed that BERT does not adhere to IR axioms, i.e., heuristics that a reasonable IR model should fulfill, through the use of diagnostic datasets. MacAvaney et al. [32] ex- panded on the axiomatic diagnostic datasets [48] with ABNIRML, a framework to understand the behaviour of neural ranking models using three different strategies: measure and match (controlling certain measurements such as relevance or term frequency and changing another), manipulation of the documents’ text (for exam- ple by shuffling words or replacing it with the query) and through the transfer of Natural Language Processing (NLP) datasets (for example comparing documents that are more/less fluent or formal with inferred queries). We expand on MacAvaney et al. [32]’s work by proposing textual manipulations—unlike previous methods we are inspired by user-created variations—to the queries instead of the Evaluating the Robustness of Retrieval Pipelines with Query Variation Generators Woodstock ’18, June 03–05, 2018, Woodstock, NY documents and examine the robustness in terms of effectiveness of neural ranking models to such manipulations. A different direction of research in NLP has challenged how well current evaluation schemes through the use of held-out test sets are actually evaluating the desired capabilities of the models. For example, Gardner et al. [23] proposed the manual creation of con- trast sets—small perturbations that preserve artifacts but change the true label—in order to evaluate the models’ decision boundaries for different NLP tasks. They showed that the model effectiveness on such contrast sets can be up to 25% lower