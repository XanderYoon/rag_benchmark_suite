embedding which yields better synonyms than standard Glove embeddings [39]. WordNetSynSwap Replaces a non-stop word by a the first synonym found on WordNet 6. If there are no words with valid synonyms it will not output a valid variation. 4As available here https://huggingface.co/ramsrigouthamg/t5_paraphraser 5https://www.kaggle.com/c/quora-question-pairs 6https://wordnet.princeton.edu/ Table 4: Statistics of the datasets. TREC-DL-2019 ANTIQUE #Q train 367013 2426 #Q valid 5193 - #Q test 43 200 # terms/ Q test 5.51 10.51 # valid query variations 334 1706 4 EXPERIMENTAL SETUP In this section we describe our experimental setup aimed to answer the following research question: are retrieval pipelines robust to different variations in queries that do not change its semantics? 4.1 Datasets We consider the following datasets in our experiments: TREC-DL- 2019 [16] for the passage retrieval task and ANTIQUE [ 25] for non-factoid question answering task, containing 43 and 200 queries respectively in their test sets. For each of the test set queries, we generate one query variation for each of the proposed methods, and we use the manual annotation described in this section ( ยง4.4) to take into account only the valid generated query variations in our experiments. The statistics of the datasets can be found in Table 4. 4.2 Ranking Models We use different ranking models that cover from lexical traditional models (Trad) such as BM25, to neural ranking models (NN) such as KNRM and neural ranking models that employ transformer-based language models (TNN) such as BERT. For all of our experiments, we apply BM25 as a first stage retriever and re-rank the top 100 results with the neural ranking models, which is an established and efficient approach [29]. For BM25 [50] and RM3 [1] we resort to the default hyperpa- rameters and implementation provided by the PyTerrier toolkit [35]. We trained the kernel-based ranking models KNRM