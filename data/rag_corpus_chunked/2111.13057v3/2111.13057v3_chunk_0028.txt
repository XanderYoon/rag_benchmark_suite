RM3 and KNRM, other transformations have a similar over- all distribution of nDCG@10 Œî amongst different models. In order to understand if models (and category of models) make mistakes on the same queries, we label the models as follows: BM25 and RM3 are labelled as Trad (lexical matching), KNRM and CKNRM (neural network based) are labelled as NN and EPIC, BERT, T5 are labelled as TNN (transformer language model based). We then represent each model with the nDCG@10 Œî values obtained for each query and variation method resulting in a total of #ùëÑ √ó #ùëÄ features per model. In order to visualize them we reduce this representation to 2 factors with tSNE [53], as shown in Figure 3. We observe that even though models have similar magnitudes and direction of nDCG@10 Œîs, classes of models as indicated by color are clustered indicating that the query variations have similar effects for each type of model. We make a similar observation when we consider the correlation of nDCG@10 Œî amongst models for the different types of transforma- tions as displayed in Table 6, where there are groups with higher similarity that roughly correspond to the different types of models. While Trad models have decreases of -0.03 (TREC-DL-2019) and -0.01 (ANTIQUE) nDCG@10 for naturality query variations, the ef- fect is higher on TNN: -0.05 and -0.04 respectively. This is evidence that neural ranking models based on heavily pre-trained language models have a slight preference for natural language queries as op- posed to keyword queries, which is a finding aligned with previous work [17]. Another interesting finding is that the word order does not have a great effect on TNN models (decreases smaller than 0.01). This is in line with recent research that indicates that the word order might not be as important as