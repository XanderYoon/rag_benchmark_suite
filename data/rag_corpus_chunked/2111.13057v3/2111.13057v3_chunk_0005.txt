An oracle that always select the best query achieves gains of 0.08 and 0.06 nDCG@10 points on TREC-DL-2019 and ANTIQUE respectively. Our work indicates that more research is required to improve the robustness of retrieval pipelines. Evaluation benchmarks should aim to have multiple query variations for the same information need in order to evaluate whether ranking pipelines are indeed robust, and we provide here a number of methods to automatically generate such query variations for any dataset. 2 RELATED WORK To put our work in context, we now describe prior research into query variations and then move on to research analyzing neural (IR) models. 2.1 Query Variation A number of studies have argued that evaluation in IR tasks should take into account multiple instantiations of the same information 1To our knowledge, UQV100 is the only publicly available dataset that contains a large number of query variations for a set of information needs. need, i.e. query variations, due to their impact on the effectiveness of ranking models [4â€“7, 11, 37, 52, 63]. Zuccon et al. [63] proposed a mean-variance framework to explicitly take into account query variations when comparing different IR systems. Bailey et al. [6] argued that a model should be consistent to different query varia- tions, and proposed a measure of consistency which gives additional information to effectiveness measurements. Besides a better evaluation of models, query variations can also be employed to improve the overall effectiveness of ranking mod- els, for instance by combining the different rankings obtained from them [8, 10] or by modelling relevance of multiple query varia- tions [30]. They have also shown to been helpful for the problem of query performance prediction [60]. Different methods to automatically generate query variations have been proposed. Benham et al. [9] proposed to obtain query expansions through a relevance