employ different automatic methods that when applied to a query generate a query variation. We con- duct experiments on two datasets (TREC-DL-2019 and ANTIQUE) and create a total of 2430 query variations from 243 topics across both datasets. Our experimental results for two different IR tasks reveal that retrieval pipelines are not robust to query variations that maintain the content the same, with effectiveness drops of ∼20% on average when compared with the original query as provided in the datasets. Our findings indicate that further work is required to make retrieval pipelines with neural ranking models more robust and that IR collections should include query variations, e.g. using the methods proposed here, for a single information need to better understand models capabilities. The code and datasets are available at https://github.com/Guzpenha/query_variation_generators. 1 INTRODUCTION Heavily pre-trained transformers for language modeling such as BERT [19] have been shown to be remarkably effective for a wide range of Information Retrieval (IR) tasks [41, 44, 58]. Commonly, IR benchmarks organized as part of TREC or other evaluation cam- paigns, evaluate the effectiveness of ranking models—neural or otherwise—based on small sets of topics and their corresponding relevance judgments. Importantly, each topic is typically repre- sented by a single query. However, previous research has shown Woodstock ’18, June 03–05, 2018, Woodstock, NY 2022. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn that queries created by users given a fixed information need may vary widely [6, 63]. In the UQV100 [5] dataset for instance, crowd workers on average created 57.7 unique queries for a given infor- mation need as instantiated as a backstory, e.g. “You have heard quite a lot about cheap computing as being the way of the future, including one recent model called a Raspberry Pi. You start thinking about buying one, and wonder how much they