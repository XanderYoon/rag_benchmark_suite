els, for instance by combining the different rankings obtained from them [8, 10] or by modelling relevance of multiple query varia- tions [30]. They have also shown to been helpful for the problem of query performance prediction [60]. Different methods to automatically generate query variations have been proposed. Benham et al. [9] proposed to obtain query expansions through a relevance model which is built by issuing the original query against an external corpora and expanding it with additional terms from the set of external feedback documents. Lu et al. [30] employed a query-url click graph and generated query variations automatically using a two-step backward walk process. Chakraborty et al. [13] generated query variations automatically based on an external knowledge base with a prior term distribution or by building a relevance model in a iterative manner. Our work differs from previous work on automatic query vari- ation generation in the following ways: ( I) our methods do not require access to external corpora, a relevance model or a query- url click graph; (II) we are not concerned with generating queries with the sole purpose of improving effectiveness, but in generating queries that are likely to occur in practice; and ( III) each of our generator methods follows a category of our taxonomy of query variations which allows us to diagnose ranking models’ effective- ness by analyzing what types of variations are more detrimental to what ranking models. 2.2 Model Understanding The success of pre-trained transformer-based language models such as BERT [19] and T5 [47] on several IR benchmarks—a comprehen- sive account of the effectiveness gains can be found in [29]—has lead to research on understanding their behaviour and the reasons behind their significant gains in ranking effectiveness [12, 32, 43, 46, 61]. Câmara and Hauff [12] showed that BERT does not