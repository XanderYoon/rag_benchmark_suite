This confirms previous empirical evidence that query variations induce a big vari- ability effect on different IR systems [ 6, 63]. We show that even with newer large-scale collections such as TREC-DL-2019, pipelines with neural ranking models are not robust to such variations. There are several potential explanations for this drop in effective- ness besides the lack of robustness of neural rankers. The first-stage ranker may be the point of failure, being unable to retrieve suffi- ciently many relevant documents for the neural rankers to re-rank. It is also possible that the query variations lead to unjudged doc- uments being ranked highly by the retrieval pipelines, which in the standard retrieval evaluation setup are considered non-relevant. We now present two experiments to show that these alternative explanations are not the cause in drop of retrieval effectiveness. Let‚Äôs focus first on the first-stage ranker. Figure 1 shows the effect of increasing the re-ranking threshold on the distribution of nDCG@10 Œî when using BERT, revealing that although the number 10While rows are directly comparable, methods with fewer valid queries are a lower bound of the potential decreases in effectiveness. Evaluating the Robustness of Retrieval Pipelines with Query Variation Generators Woodstock ‚Äô18, June 03‚Äì05, 2018, Woodstock, NY Table 5: Effectiveness (nDCG@10) of different methods for TREC-DL-2019 and ANTIQUE when faced with different query variations. Bold indicates the highest values observed for each model and ‚Üì/‚Üë subscripts indicate statistically significant losses/improvements, using two-sided paired Student‚Äôs T-Test at 95% confidence interval when compared against the same model with the original queries. #ùëÑ indicates the number of valid query variations for the method (invalid query variations are replaced by the original query). TREC-DL-2019 Category Query Variation BM25 RM3 KNRM CKNRM EPIC BERT T5 #Q - original query 0.4795 0.5156 0.5015 0.4931 0.6240 0.6449 0.6997 43 Misspelling