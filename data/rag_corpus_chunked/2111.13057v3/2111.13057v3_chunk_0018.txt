For all of our experiments, we apply BM25 as a first stage retriever and re-rank the top 100 results with the neural ranking models, which is an established and efficient approach [29]. For BM25 [50] and RM3 [1] we resort to the default hyperpa- rameters and implementation provided by the PyTerrier toolkit [35]. We trained the kernel-based ranking models KNRM [57] and CK- NRM [18] on the training sets of TREC-DL-2019 and ANTIQUE using default settings from the OpenNIR [31] implementation. For the BERT-based methods EPIC [33], an efficiency focused model that encodes query and documents separately, andBERT [41], also known as monoBERT, which concatenates query and the document and makes predictions based on the[CLS] token representation, we fine-tune the bert-base-uncased model for the train datasets. For T5 [47] we use the monoT5 [42] implementation from PyTerrier T5 plugin 7 which has the pre-trained weights for MSMarco [40] by the original authors of monoT5. 4.3 Query Generators Implementation As for our methods of generating query variations, forT5DescToTitle and T5QQP we rely on pre-trained T5 models (t5-base) and we fine- tune them using the Huggingface transformers library [ 54]. For BackTranslation we use the facebook/m2m100_418M pre-trained 7https://github.com/terrierteam/pyterrier_t5 Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gustavo Penha, Arthur CÃ¢mara, and Claudia Hauff model from the transformers library8. For all other methods, we use the implementations from the TextAttack [38] library. 4.4 Quality of Query Generators Given the automatic nature of the methods we introduced, we need to evaluate their quality: how good are these methods at generating query variations that a user would also generate? To this end, we consider two properties of the generated queries: (I) Ë†ğ‘ maintains the same semantics asğ‘, and (II) the syntax difference between ğ‘ and Ë†ğ‘ can be attributed to the category ğ¶. All pairs of