0.1546↓ 0.1602↓ 0.1438↓ 0.2036↓ 0.3045↓ 0.2584↓ 93 T5QQP 0.2201 0.2065 0.2095 0.1962 0.2614 0.3926 ↓ 0.3214↓ 105 WordEmbedSynSwap 0.1759↓ 0.1719↓ 0.1902↓ 0.1690↓ 0.2142↓ 0.3245↓ 0.2828↓ 124 WordNetSynSwap 0.1791↓ 0.1751↓ 0.1957↓ 0.1765↓ 0.2117↓ 0.3244↓ 0.2733↓ 71 of relevant documents on the re-ranking set increases (e.g. BM25 has Recalls @10, @100 and @1000 on average of 0.06, 0.25 and 0.48 for misspelling query variations), BERT still struggles (negative Δ) with query variations11. This indicates that even if we increase the number of relevant documents in the list to be re-ranked, the re-rankers still fail when facing the query variations. To further isolate the effect of the first-stage retrieval module, we analyzed whether the effectiveness of the pipelines would not degrade in case the first-stage retrieval was performed on the orig- inal query. In this experiment only the re-ranker models use the query variations and we check whether the effectiveness drops persist. The results reveal that there are still statistically significant effectiveness drops when only the re-ranker models use the query variations, although in smaller magnitude. While the drops in ef- fectiveness of the pipelines when using query variations for the 11Similar results are obtained for other neural rankers. entire pipeline are on average of 20% in nDCG@10, when using the query variations only for re-ranking they are of 9%.This indicates that not only the first stage retrieval module is not robust to query variations, but also the neural re-rankers . Let’s now focus on the matter of unjudged documents. It is pos- sible that we are underestimating the effectiveness of the retrieval pipelines when facing query variations if ( I) the number of un- judged documents in the top-10 ranked lists increases and (II) they turn out to be relevant. When counting the amount of judged doc- uments in the top-10 ranked