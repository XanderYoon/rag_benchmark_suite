on the recent TREC-DL-2019 [16] and ANTIQUE [25] datasets to answer the following research ques- tion: Are retrieval pipelines robust to different variations in queries that do not change its semantics? To this end we consider seven ranking approaches: two traditional lexical models (BM25 [50] and RM3 [1]), two neural re-ranking approaches that do not make use of transformers (KNRM [57] and CKNRM [18]) and three transformer- based re-ranking approaches (EPIC [33], BERT [41] and T5 [42]). Additionally, motivated by the fact that certain query variations can improve the retrieval effectiveness compared to using the orig- inal query [8, 10], we study the combination of automatic query variations with rank fusion [15]. Our main findings are as follows: • The four types of syntax-changing query variations differ in the extent to which they degrade retrieval effectiveness: misspellings have the largest effect (with an average drop of 0.25 nDCG@10 points across seven retrieval models for TREC-DL-2019) while the word ordering has the least effect (with an average drop of nDCG@10 smaller than 0.01 for TREC-DL-2019). • Different types of ranking models make similar mistakes. For example, effectiveness decreases for models based on transformer language models are higher for naturality query variations com- pared to decreases when using traditional lexical models. • While rank fusion mitigates the drops in retrieval effectiveness when compared to using a single query variation, it does not achieve the full potential of the combination of query variations. An oracle that always select the best query achieves gains of 0.08 and 0.06 nDCG@10 points on TREC-DL-2019 and ANTIQUE respectively. Our work indicates that more research is required to improve the robustness of retrieval pipelines. Evaluation benchmarks should aim to have multiple query variations for the same information need in order to evaluate whether ranking pipelines are indeed robust,