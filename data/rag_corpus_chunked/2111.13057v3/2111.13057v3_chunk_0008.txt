sets are actually evaluating the desired capabilities of the models. For example, Gardner et al. [23] proposed the manual creation of con- trast sets—small perturbations that preserve artifacts but change the true label—in order to evaluate the models’ decision boundaries for different NLP tasks. They showed that the model effectiveness on such contrast sets can be up to 25% lower than on the original test sets. Inspired by behavioral testing, i.e. validating input out- put behaviour without knowledge about internal structure, from software engineering tests, Ribeiro et al. [49] proposed to test NLP models with three different types of tests: minimum functionality tests (simple examples where the model should not fail), label (such as positive, negative and neutral in sentiment analysis) invariant changes to the input, and modifications to the input with known outcomes. With such tests at hand they were able to find action- able failures in different commercial NLP models that had already been extensively tested. It has also been shown that neural models developed for different NLP tasks can be tricked by adversarial ex- amples [2, 22, 24], i.e. examples with perturbations indiscernible by humans which get misclassified by the model. In terms of queries modifications, [56, 62] found typos to be detrimental to the effec- tiveness of neural rankers. Wu et al. [56] analyzed the robustness of neural rankers with respect to three dimensions: difficult queries from similar distribution, out-of-domain cases, and defense against adversarial operations. Our work differs from the adversarial line of research by evaluating the robustness of models to query modifica- tions that could be generated by humans, i.e. transformations that naturally occur, and not modifications optimized to trick neural models. 3 AUTOMATIC QUERY V ARIATIONS We now first describe in Section 3.1 how we arrived at our query variation categories in a