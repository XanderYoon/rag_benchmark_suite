UQV100 [5] dataset for instance, crowd workers on average created 57.7 unique queries for a given infor- mation need as instantiated as a backstory, e.g. “You have heard quite a lot about cheap computing as being the way of the future, including one recent model called a Raspberry Pi. You start thinking about buying one, and wonder how much they cost. ” Table 1: Examples of BERT effectiveness drops (nDCG@10 Δ) when we replace the original query from TREC-DL- 2019 by an automatic (except for the first two lines that were produced manually) query variation. We focus here on transformations that change the query syntax , but not its semantics . Original Query Query Variation nDCG@10 Δ popular food in switzerland popular food in zurich gen./specialization cost of interior con- crete flooring concrete flooring finishing aspect change what is theraderm used for what is thrraderm used for misspelling -1.00 (-100%) anthropological defi- nition of environment anthropological definition of environment naturality -0.15 ( -26%) right pelvic pain causes causes pelvic pain right ordering -0.18 ( -46%) define visceral what is visceral paraphrasing -0.26 ( -38%) We thus argue that it is necessary to investigate the robustness of retrieval pipelines in light of query variations (i.e., different ex- pressions of the same information need) that are likely to occur in practice. That different query variations lead to vastly different ranking qualities is anecdotally shown in Table 1 for a vanilla BERT model for ranking [41]. If, for example, the word order of the origi- nal query from TREC-DL-2019 right pelvic pain causes is changed to causes pelvic pain right , the retrieval effectiveness of the resulting ranking drops by 46%. Similarly, paraphrasing define visceral to what is visceral reduces the retrieval effectiveness by 38%. In our work, we quantify the extent to