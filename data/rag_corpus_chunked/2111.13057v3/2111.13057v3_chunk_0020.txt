terms that are impor- tant for the query and thus change its semantics (e.g. ‘ if i had a bad breath what should i do ’→ ‘if i had a ’ ), (II) BackTranslation and T5QQP methods can generate an identical copy of the input query which was automatically labelled as invalid (e.g. ‘ what is dark energy’→ ‘what is dark energy ’) and (III) transformations that replace words by their presumed synonyms (WordEmbedSynSwap and WordNetSynSwap) at times adds words that are not in fact syn- onymous in the query context (e.g. ‘what is dark energy ’→ ‘what is blackness energy’ and ‘what is a active margin ’→ ‘what is a active border’). To evaluate the robustness of the ranking models, we re- sort to using only the valid queries as defined by the manual annotations. Overall, we have thus 2,040 valid queries for datasets TREC-DL-2019 and ANTIQUE that we employ in the experiments that follow. 5 RESULTS In this section we first describe our main results on the robustness of models to query variations, analyzing them by category of variation and by category of ranking model. We then move on to discussing the fusion of the ranking list obtained by the query variations. 8https://huggingface.co/facebook/m2m100_418M 9misspelling methods can generate invalid queries when all words of the query are stop-words (e.g. ‘how is it being you ’ from ANTIQUE would generate the same query as output since there is no non stop-words to modify) −1.0 −0.5 0.0 0.5 10 100 1000 Re−ranking threshold nDCG@10 Δ category misspelling naturality ordering paraphrase Figure 1: Distribution of nDCG@10 Δ for different re- ranking thresholds when using BERT as a re-ranker. 5.1 Robustness to Query Variations In order to explore the robustness of our three types of ranking models (traditional, neural and transformer-based), we compare