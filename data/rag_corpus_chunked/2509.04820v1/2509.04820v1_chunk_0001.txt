code and dataset will be released upon acceptance. 1 Introduction Retrieval-Augmented Generation (RAG) (Chen et al., 2024; Lewis et al., 2020; Gao et al., 2023) based on Large Language Models (LLMs) has been intensively deployed to conduct question- answering (QA) towards closed-source or inter- nal documents. Although Large Language Mod- els (LLMs) (Singh, 2023; Zhao et al., 2023; Zhu et al., 2024) have demonstrated remarkable capa- bilities in natural language understanding and gen- eration, LLMs rely on static training data, mak- ing them prone to hallucinations and limiting their Catching more“fish”one time with a largerfishing net. … Catching “fish”multiple timeswith small fishing nets. :Chunk :Retriever :V ector Database Catching “fish” one time with asmall fishing net. Golden chunks retrieved, but LLM wrong27%Golden chunks fall outside top-k48% Golden chunks not found25% (a) Error Analysis (b) Comparison Catching “fish” one time with a small fishing net.Catching more“fish” one time with a largernet.Catching “fish”multiple times with a small net. Strategy 1Traditional RAGStrategy 2 Figure 1: (a) Error analysis: 48% of traditional RAG failures is due to not finding the golden chunk in the top-k; (b) Comparison of three retrieval methods: Tra- ditional RAG is casting a small fishing net once; We propose either to use a “larger” net or a same net but “multi-times”. ability to provide accurate, up-to-date information in knowledge-intensive tasks (Rawte et al., 2023; Zhang et al., 2023; Huang et al., 2025). By integrat- ing relevant information from external knowledge bases or search engines, RAG enhances factual ac- curacy and broadens the model’s domain coverage (Zhao et al., 2024; Li et al., 2024). Traditional RAG methods typically follow a one- round retrieval paradigm, where the user’s query is first embedded into a vector representation and matched against a vector-based index of document chunks stored in a knowledge base or internal