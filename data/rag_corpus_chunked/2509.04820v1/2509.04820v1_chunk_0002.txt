bases or search engines, RAG enhances factual ac- curacy and broadens the model’s domain coverage (Zhao et al., 2024; Li et al., 2024). Traditional RAG methods typically follow a one- round retrieval paradigm, where the user’s query is first embedded into a vector representation and matched against a vector-based index of document chunks stored in a knowledge base or internal repos- itory. The retriever then selects the top-ranked chunks based on vector similarity, and these re- trieved chunks, together with the original query, are provided to a generative LLM, which synthesizes the final answer using both the retrieved evidence and the query context. Despite the effectiveness of basic RAG methods, they often struggle when applied to real-world sce- narios involving complex and heterogeneous gov- ernment documents. In our analysis of QA tasks on such documents (Fig. 1(a)), we find that nearly half of the incorrect answers result from the failure to retrieve the golden chunks using the traditional top-k selection strategy. This retrieval failure pre- arXiv:2509.04820v1 [cs.IR] 5 Sep 2025 vents the generative model from accessing crucial evidence, ultimately degrading the quality of the final answer. Since the one-round retrieval may miss the golden chunks, then the intuitive practical solution is: (1) either we increase the number of chunks retrieved, or (2) keep the same number of chunks and just retrieve multiple times. Take the analogy of fishing (Fig. 1(b)), the traditional RAG’s top- k retrieval can be seen as casting a small fishing net once (e.g., top-5 usually), hoping to capture all relevant evidence in a single pass. However, this “single-shot trawling” approach may miss the golden chunks altogether, especially in complex or diverse knowledge domains. The first solution is basically to dase a larger net, but it may catch too much irrelevant information (“noise” or “junk fish”); The