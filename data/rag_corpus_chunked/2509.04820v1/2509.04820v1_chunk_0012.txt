cor- pus of over 40,000 real-world government docu- ments. Specifically, the questions are categorized into four levels (ranging from Level 1 to Level 4) based on two key criteria: whether retrieval re- quires reasoning and whether answering requires reasoning. We ensure that all answers are retriev- able from the original document corpus. Figure 4 illustrates the difference between each level. LevelRetrieval(require reasoning?)Answer(require reasoning?)Level 1NoNoLevel 2NoYesLevel 3YesNoLevel 4YesYesLevel1:WhatistheGDPofKunmingin2023?Answer:786.476billionyuan.Level2:WhichofKunmingâ€™seightmajorannualconsumercategoriessawthehighestpriceincreasein2023?Answer:Thepricesofeducation,cultureandentertainmentsawthehighestincrease(upby3.7%).Level3:HowmanydaysdidtheairqualityinKunmingmeetthegoodorexcellentstandardsin2022and2023respectively?Answer:In2022and2023,thedaysofgoodairqualitywere365and356,respectively.Level4:HowmanydomesticandforeigntouristsdidKun-mingreceiveintotalfrom2021to2023inthreeyears?Answer:173.5185(2021)+218.0641(2022)+270.7351(2023)=662.3177millionpeople. One Relevant Chunk AnswerAnalyze step by step. AnswerOne Relevant Chunk MultipleRelevantChunks Answer MultipleRelevantChunks AnswerAnalyze step by step. Level1 Level2 Level3 Level4 Figure 4: Categories of questions in our dataset. The table in the upper-left shows differences between ques- tions of various levels. The lower-left provides examples of different-level questions and their reference answers. The diagrams on the right illustrate the solving processes for questions of different levels. Basic RAG Setting. Our baseline system em- ploys traditional single-turn retrieval with a fixed Top-5 strategy. This basic RAG approach retrieves the top-5 most relevant document chunks based on vector similarity and provides them to the language model for answer generation without any iterative refinement or optimization mechanisms. All ex- periments are conducted using Qwen3-32B as the language model. Evaluation Setting. We employ an LLM-as- a-judge evaluation framework to assess answer quality across multiple dimensions including fac- tual accuracy, completeness, and relevance. We adopt SenseChat-5 to perform answer evaluation. The evaluation scores are normalized to a 0-100 scale for consistent comparison across different approaches. Method L1 L2 L3 L4 Avg Basic RAG (Top-5) 89.5 88.5 76.0 70.0 81.0 Token-Constrained Top-Kmax 90.0 91.0 85.5 83.5 87.5 + Chunk Filter 92.0 93.0 90.0 89.0 91.0 + Chunk Cropping 91.0 92.5 86.5 84.0 88.5 + Chunk Filter & Cropping92.0 93.0 87.5 83.5 89.0 Improvement over Baseline+2.5 +4.5 +14.0+19.0+10.0 Table 1: Ablation study of One-SHOT retrieval compo- nents. Each row