gFishing for Answers: Exploring One-shot vs. Iterative Retrieval Strategies for Retrieval Augmented Generation Huifeng Lin, Gang Su, Rui Zhao SenseTime Research zhaorui@sensetime.com Jintao Liang BUPT, Beijing ljt2016@bupt.edu.cn You Wu HKUST, Hong Kong wuyouscut@gmail.com Ziyue Li Technical University of Munich, Germany ziyue.li@tum.de Abstract Retrieval-Augmented Generation (RAG) based on Large Language Models (LLMs) is a power- ful solution to understand and query the indus- try’s closed-source documents. However, basic RAG often struggles with complex QA tasks in legal and regulatory domains, particularly when dealing with numerous government documents. The top- k strategy frequently misses golden chunks, leading to incomplete or inaccurate an- swers. To address these retrieval bottlenecks, we explore two strategies to improve evidence coverage and answer quality. The first is a One-SHOT retrieval method that adaptively se- lects chunks based on a token budget, allowing as much relevant content as possible to be in- cluded within the model’s context window. Ad- ditionally, we design modules to further filter and refine the chunks. The second is an iterative retrieval strategy built on a Reasoning Agen- tic RAG framework, where a reasoning LLM dynamically issues search queries, evaluates retrieved results, and progressively refines the context over multiple turns. We identify query drift and retrieval laziness issues and further design two modules to tackle them. Through extensive experiments on a dataset of govern- ment documents, we aim to offer practical in- sights and guidance for real-world applications in legal and regulatory domains. The code and dataset will be released upon acceptance. 1 Introduction Retrieval-Augmented Generation (RAG) (Chen et al., 2024; Lewis et al., 2020; Gao et al., 2023) based on Large Language Models (LLMs) has been intensively deployed to conduct question- answering (QA) towards closed-source or inter- nal documents. Although Large Language Mod- els (LLMs) (Singh, 2023; Zhao et al., 2023; Zhu et