calls at different context lengths. Results:As shown in Table 6, the model’s propensity to continue searching decreases dramat- ically as context length increases. With shorter contexts, the model correctly identifies information gaps and performs follow-up searches in the major- ity of cases. However, this rate drops significantly when the context becomes longer, demonstrating severe retrieval laziness in extended contexts. This experiment confirms our hypothesis that longer working contexts exacerbate retrieval lazi- ness, leading to premature termination of the search process even when critical information is missing. C Chunk Delete Tool Failure in Combined Strategy To illustrate the chunk delete tool failure discussed in Section 4.4, we provide a concrete case study demonstrating how longer working contexts impair the tool’s effectiveness. Query:"What was the area of flower culti- vation in Kunming in the year when the first ’China·Kunming Dounan Flower Exhibition’ was held?" Context:The combined strategy (Token- Constrained Top-Kmax + Iterative) retrieved 10 chunks in the first round, creating a working con- text of 14,507 tokens. Among these chunks, one contained the crucial information that the first exhi- bition was held in 2023. Chunk Delete Tool Behavior:When presented with this extended context, the chunk delete tool incorrectly removed 9 out of 10 chunks, including the chunk containing the correct answer about the 2023 exhibition. Resulting Error:Without the correct chunk, the model provided an incorrect answer based on incomplete information. Analysis:This demonstrates how extended con- texts cause cognitive overload in the chunk delete tool, leading to the removal of genuinely useful information critical for accurate answers.