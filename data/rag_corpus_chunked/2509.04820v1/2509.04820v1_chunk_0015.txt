80 71 72 76 Table 2: Chunk retention ratio (%) after applying chunk refinement modules under different QA difficulty lev- els. Values indicate the percentage of retrieved chunks retained after each processing step. Models MetricsBasic RAG (Top-5) One-SHOT Strategy SenseChat-5Scores 77.5 83.5Mins 30 24 Qwen2.5-32BScores 80.5 91.5Mins 23 21 Qwen3-32B no thinkScores 77.0 91.0Mins 26 24 Qwen3-32B thinkScores 75.0 89.5Mins 51 83 Table 3: Evaluation scores and time costs on different LLMs for One-SHOT strategy. 4.2.3 Applications on Different LLMs As One-SHOT strategy basically increase the recall, so theoretically it won’t behave too much differ- ently on L1 to L4, instead, we would like to see how the different choice of LLMs will affect the overal result, thus, Table 3 only presents the overall eval- uation scores and time costs of applying the One- SHOT strategy across different LLMs. Compared to the baselineBasic RAG (Top-5), all models show substantial performance improvements when using “a big net”, i.e., the token-constrained One-SHOT strategy. For instance,Qwen2.5-32Bachieves the highest score of 91.5 (+11.0), whileSenseChat-5 improves by 6.0 points to reach 83.5. In terms of ef- ficiency, the One-SHOT strategy generally reduces time costs for most models, as it avoids multi-turn reasoning and minimizes redundant computations. For example, the time forSenseChat-5drops from 30 to 24 minutes. However, forQwen3-32Bin thethinkconfiguration, we observe a significant increase in latency (from 51 to 83 minutes). This overhead is attributed to the additional reasoning steps performed within the <think> blocks during relevance evaluation. These results demonstrate the effectiveness and generality of the One-SHOT strat- egy across LLMs with different sizes and reasoning capabilities. They also highlight the trade-off be- tween reasoning complexity and latency, suggest- ing that careful system design is needed to balance performance gains and computational efficiency. 4.3 Results on Iterative Retrieval Strategy 4.3.1 Ablation studies We evaluated