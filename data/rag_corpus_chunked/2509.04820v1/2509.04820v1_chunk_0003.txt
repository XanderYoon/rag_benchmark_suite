casting a small fishing net once (e.g., top-5 usually), hoping to capture all relevant evidence in a single pass. However, this “single-shot trawling” approach may miss the golden chunks altogether, especially in complex or diverse knowledge domains. The first solution is basically to dase a larger net, but it may catch too much irrelevant information (“noise” or “junk fish”); The second is to adopt a “multi-cast” fish- ing strategy, where smaller, more targeted nets are cast repeatedly, progressively refining the retrieval toward high-value chunks. Motivated by the two retrieval ideas introduced above: casting a larger net and casting multiple times, we implement two corresponding strategies. The first is a One-SHOT retrieval strategy that re- moves the fixed top-k constraint and instead selects as many relevant chunks as fit within a predefined token budget, ranking them by relevance-per-token to maximize evidence density. To further enhance precision, we incorporate rule-based modules, such aschunk filterto filter out irrelevant chunks based on chunk meta information (e.g., year, location in our case). The second is an iterative retrieval strat- egy based on an Agentic RAG framework, where a reasoning-capable LLM manages retrieval over multiple turns by issuing intermediate queries, as- sessing retrieved results, and progressively refining the context. These strategies offer practical solu- tions to improve evidence coverage and robustness in complex QA scenarios. The complex QA usually lies in two abilities of RAG system, which we refer to as the reason- ing ability: (1) the ability to decompose the com- plex question (generation reasoning), and (2) the ability to retrieve multiple cues (retrieval reason- ing). To systematically evaluate the performance of traditional RAG systems and our two proposed strategies on government documents, we design a benchmark that categorizes questions into four levels based on whether generation reasoning and retrieval reasoning are required. This paper