user query in par- allel to the model’s reformulated query. This dual- retrieval approach is specifically applied only to the first turn to provide a strong foundation for sub- sequent iterations—ensuring the model starts with a good baseline of relevant information from the original query while still benefiting from its own query reformulation. Adaptive Context Management: The system incorporates the chunk_delete function, which allows the model to remove irrelevant retrieved in- formation from the working context by specifying the chunk IDs of unwanted chunks. This mecha- nism helps maintain focus on pertinent information and prevents the accumulation of noise that could degrade answer quality. These optimization components are essential for addressing two key challenges: • Query drift: where autonomous query refor- mulation by LRM drifts away from the origi- nal query and leads to less relevant results, as shown in Appx. A. • Retrieval laziness: where in the iterative agentic retrieving, the model prematurely ter- minates search due to context overload: specif- ically, the heavier the context retrieved from previous round, the less likely the agent will initiate the next retriecal action due to “cog- nitive burden”, as shown in Appx. B. Such a cognitive burden is also observed in function calling (Yang et al., 2025b). The effectiveness of these components is demon- strated in our experiments in Section 4.3. 4 Experiment 4.1 Experiment Settings Datasets. Our dataset is composed of 1,000 care- fully designed questions, all extracted from a cor- pus of over 40,000 real-world government docu- ments. Specifically, the questions are categorized into four levels (ranging from Level 1 to Level 4) based on two key criteria: whether retrieval re- quires reasoning and whether answering requires reasoning. We ensure that all answers are retriev- able from the original document corpus. Figure 4 illustrates the difference between each level.