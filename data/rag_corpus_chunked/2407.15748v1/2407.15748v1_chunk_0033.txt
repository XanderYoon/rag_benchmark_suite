Answer (Score 5) : [Reference Answer] As illustrated in Prompt 2, our methodology involves utilizing GPT-4 0125-Preview to assign scores ranging from 1 to 5 to each model answers based on a Top-Scoring Reference. These scores are based on a comparison with a reference answer, which is given a perfect score of 5. The procedure for this scoring corresponds to the methodology described in 16. As can be seen from Table VI, MoRSE outperforms in all question categories (General questions, Multi-Hop questions, and CVE questions). GPT-4 0125-Preview follows MoRSE as the second most competent model and shows strong versatility in all categories, but with a significantly lower performance than MoRSE. Other models, GEMINI, HACKERGPT and MIXTRAL, show different levels of performance, with none reaching the effectiveness of MoRSE or GPT-4 0125-Preview in terms of general cybersecurity knowledge or the more targeted questions on Multi-Hop and CVE. E. Test Case Analysis for MoRSE RAG Retrievers This section reports the evaluation on the performance of both the structured and unstructured RAG components within the MoRSE system, as shown in Table VII. We evaluated each retriever against a customized set of 100 test questions based on their specialized knowledge in cybersecurity, focusing on processing time efficiency, dense retriever size, denoted by Size and number of documents ( No. Doc ). We also test the reliability of the structured Retrievers using test queries with error rate analysis to estimate future reliability using a confidence interval. a) Analysis of the performance of Structured Retrievers : Table VII shows the performance metrics for Structured RAG Retrievers within the MoRSE system. TheExploitDB Retriever Size is not listed as it uses TF-IDF for retrieval. The Malware Retriever processes queries with average times of 0.061 sec- onds on GPU and 0.110 seconds on CPU. The CWE Retriever and the