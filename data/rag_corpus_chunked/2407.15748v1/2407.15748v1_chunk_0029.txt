Q. Ret. Sys. 0 20 40 1 31 18 10 5 10 14 Impact (%) (c) Impact of Retrievers on CVE Questions Fig. 6: Impact of Retrievers on General, MultiHop and CVE Questions In our evaluation, we used GPT-4 0125-Preview as a judge to evaluate and compare the performance of MoRSE, GPT-4 0125-Preview, MIXTRAL 7X8, GEMINI 1.0 Pro and HACK- ERGPT. This method is based on the research proposed by Zheng et al. [24], which shows that GPT-4 has a high agreement with human judgment with an 80% agreement rate. As shown in Prompt 1, given a query and the corresponding reference response, we ask GPT-4 0125-Preview to choose the best response between Model A and Model B. After GPT-4 0125-Preview completed its evaluation of all possible battles among the competing models, documented in Table IV, we derived three different metrics for each model: Elo Rating , Bootstrap-enhanced Elo Ratings and Maximum Likelihood Estimation: • Elo Ratings: it quantifies the comparative skill levels across entities in competitive scenarios, making it apt for evaluating models based on their head-to-head outcomes. This approach involves initial computation using a linear update algorithm, opting for a conservative K-factor to ensure stability in ratings, minimizing bias from recent encounters. Equation 5 shows the Elo rating formula used in our context: Rnew = Rold + K × (S − E), (5) where Rnew and Rold represent the new and old Elo ratings, respectively. The constant K, set equal to 4, controls the volatility of the rating, S denotes the actual match outcome (1 for a win, 0.5 for a tie, and 0 for a loss), and E is the expected outcome, as calculated in Equation 6: E = 1 1 + 10 Rnew −Rold 400 . (6) • Maximum Likelihood Estimation: Utilizing logistic re- gression for