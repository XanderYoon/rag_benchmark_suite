each retriever was able to suc- cessfully retrieve relevant information within these contexts. The frequency of successful retrievals for each retriever was then calculated as a percentage of the total questions in each category. In this way, we were able to quantify the performance and impact of each retriever in both general and multi-hop question scenarios. Figures 6a, 6b and 6c show the impact of the different retrievers for each question category. For general questions, the Question Retrieval System has the highest impact at 56.3%, followed by the Entity Retriever with 21.7%. Other retrievers, such as MITRE, CWE, ExploitDB, Metasploit and Malware Retriever, have an impact of between 6.1% and 9.0%. For multi-hop questions, the Question Retrieval System significantly influences results with a 35.4% contribution, and the Entity Retriever also plays an important role with a 28.3% contribution. The Metasploit Re- triever has an increased influence of 12%. The other retrievers (Malware, CWE, ExploitDB and MITRE) have an influence of between 5% and 7%. Figure 6c shows the impact on CVE questions, where the ExploitDB and Metasploit Retrievers have the highest impact at 18% and 31% respectively. The other retrievers (Malware, CWE, MITRE, Entity, and Question Ret. Sys.) have an impact ranging from 1% to 14%. C. Second Test Suite with LLM as Judge: Reference-Guided Pairwise Comparison Prompt 1: The prompt for reference-guided pairwise com- parison. Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer, assistant A’s answer, and assistant B’s answer. Your job is to evaluate which assistant’s answer is better. Begin your evaluation by comparing both assistants’ answers with the reference answer. Iden- tify and correct any mistakes. Avoid