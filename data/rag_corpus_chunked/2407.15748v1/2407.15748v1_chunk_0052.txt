with a unified text-to-text transformer,” Journal of machine learning research, vol. 21, no. 140, pp. 1–67, 2020. [48] Y . Tang and Y . Yang, “Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries,” 2024. [49] W. Xiong, X. L. Li, S. Iyer, J. Du, P. Lewis, W. Y . Wang, Y . Mehdad, W.- t. Yih, S. Riedel, D. Kielaet al., “Answering complex open-domain ques- tions with multi-hop dense retrieval,” arXiv preprint arXiv:2009.12756 , 2020. [50] A. Abdallah and A. Jatowt, “Generator-retriever-generator: A novel approach to open-domain question answering,” arXiv preprint arXiv:2307.11278, 2023. [51] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval without relevance labels,” in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), A. Rogers, J. Boyd-Graber, and N. Okazaki, Eds. Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 1762–1777. [Online]. Available: https://aclanthology.org/2023.acl-long.99 [52] J. Ramos et al., “Using tf-idf to determine word relevance in document queries,” in Proceedings of the first instructional conference on machine learning, vol. 242, no. 1. Citeseer, 2003, pp. 29–48. [53] M.-E. Brunet, C. Alkalay-Houlihan, A. Anderson, and R. Zemel, “Un- derstanding the origins of bias in word embeddings,” in International conference on machine learning . PMLR, 2019, pp. 803–811. [54] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al., “Mistral 7b,” arXiv preprint arXiv:2310.06825 , 2023. [55] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, “Lost in the middle: How language models use long contexts,” Transactions of the Association for Computational Linguistics , vol. 12, pp. 157–173, 2024. [56] S. Wang, S. Zhuang, and G. Zuccon, “Bert-based dense retrievers require interpolation with bm25 for effective