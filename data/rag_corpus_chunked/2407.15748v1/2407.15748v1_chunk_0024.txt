answers against ground truth. It combines semantic similarities and factual correctness: AC = wF C · F C + wSS · SS, (3) where F C is the factual correctness, quantified using the F1 score that considers True Positives ( T P), False Positives (F P), and False Negatives ( F N): F C = |T P| |T P| + 0.5 · (|F P| + |F N|) , (4) and SS is the semantic similarity between the generated and ground truth answers. wF C and wSS are the weights assigned to F C and SS, respectively 0.75 and 0.25. In order to calculate TP, FP and FN, RAGAS framework uses the following prompt instruction: Extract the following from the given question and ground truth: “TP”: statements that are present in both the answer and the ground truth, “FP”: statements present in the answer but not found in the ground truth, “FN”: relevant statements found in the ground truth but omitted in the answer. Each of these three metrics requires an embedding model to compute distances between sentences and a large language model (LLM) for evaluating answer relevance and correctness. We chose GPT-4 0125-Preview as the LLM and ( β) as the embedding model. a) Performance Analysis on General and Multi-Hop Ques- tions: Table II shows the results of MoRSE and the other models for General Cybersecurity Questions and Multi-Hop Cybersecurity Questions . The metrics for each model are expressed as mean ( µ) and standard deviation ( σ), which indicate the average performance and variability, respectively. Insights from General Cybersecurity Questions: For the general cybersecurity questions, MoRSE showed superior per- formance in all metrics, with a mean relevance score of 0.90, a similarity score of 0.95, and a correctness score of 0.71, 8 indicating a high degree of agreement between