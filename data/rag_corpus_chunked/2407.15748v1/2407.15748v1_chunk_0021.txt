Augmented Generation systems and Large Language Models in the field of cybersecurity is particularly challenging due to their dual role in information retrieval and content generation. The lack of standardized benchmarks covering a wide range of real-world operational cyber tasks complicates the evaluation of LLMs for cyberse- curity [23], [58], [59]. Key evaluation challenges include verifying the accuracy of the retrieved information, the effectiveness of its use by LLM, and the overall quality of the content generated. Traditional methods that focus on language comprehension may not adequately reflect real-world performance [60]. To effectively address these challenges, we developed a three- part evaluation strategy for MoRSE and compared its per- formance with other known LLMs and RAG systems in answering cybersecurity questions. MoRSE was compared to competing models such as GPT-4 0125-Preview, MIXTRAL, HACKERGPT, and GEMINI 1.0 Pro. The three different evaluation test suites are: • Using the RAGAS framework [23], we evaluate MoRSE’s responses against a ground truth using a set of metrics. • Using a method proposed by Zheng et al. [24], we computed Elo Ratings for MoRSE and competing models by reference-guided pairwise comparison using GPT-4 0125-Preview as a judge. This provides a quantitative measure of relative performance. 7 TABLE II: Comparative Assessment of RAG Models on 156 General and 150 Multi-Hop Cybersecurity Questions General Cybersecurity Questions Multi-Hop Cybersecurity Questions Model Relevance Similarity Correctness Relevance Similarity Correctness µ σ µ σ µ σ µ σ µ σ µ σ MoRSE 0.90 0.05 0.95 0.02 0.71 0.08 0.93 0.05 0.93 0.03 0.70 0.09 GPT-4 0125-Preview 0.74 0.35 0.93 0.03 0.59 0.07 0.75 0.33 0.92 0.03 0.62 0.10 MIXTRAL 7X8 0.77 0.43 0.92 0.03 0.58 0.08 0.60 0.42 0.92 0.04 0.61 0.09 HACKERGPT 0.66 0.33 0.92 0.04 0.58 0.08 0.73 0.18 0.79 0.05 0.47 0.03 GEMINI 1.0 Pro 0.61 0.43