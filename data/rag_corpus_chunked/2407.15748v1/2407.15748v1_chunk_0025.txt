mean ( µ) and standard deviation ( σ), which indicate the average performance and variability, respectively. Insights from General Cybersecurity Questions: For the general cybersecurity questions, MoRSE showed superior per- formance in all metrics, with a mean relevance score of 0.90, a similarity score of 0.95, and a correctness score of 0.71, 8 indicating a high degree of agreement between the answers and the query prompts, as well as factual accuracy. In comparison, all other models showed lower consistency and effectiveness, especially in terms of correctness. Insights Multi-Hop Cybersecurity Questions: When eval- uating complex multi-hop cybersecurity queries, the MoRSE model outperforms its competitors and proves that it is ca- pable of answering complicated questions. The data shows that MoRSE scores consistently high on all metrics, with average scores of 0.93 for relevance and similarity and 0.70 for correctness. Other models show significant performance degradation, particularly in correctness, with GPT-4 0125- Preview achieving a mean score of 0.62 and MIXTRAL 0.61, indicating a lower capacity to handle multi-hop questions. b) Performance Analysis on CVE Questions: Table III shows how MoRSE and GPT-4 0125 preview approach 300 CVE queries. We chose GPT-4 0125 preview because it performed best as the second model in both general and multi-hop contexts (see Table II). We focus on answer similarity and correctness metrics for scoring answers because they are strictly based on ground truth and answer relevance does not measure factuality. Moreover, we compute Accuracy metric, calculated by checking whether the models correctly identified the vulnerability in the given queries. Regarding Correctness, the MoRSE model scored 0.64 because its responses often include explanation of related exploit codes, which are not present in the ground truth that only describes the vulnerability specifics. This extra information, while useful, lowers the correctness score as it deviates from the expected