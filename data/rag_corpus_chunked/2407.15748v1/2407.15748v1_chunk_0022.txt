σ MoRSE 0.90 0.05 0.95 0.02 0.71 0.08 0.93 0.05 0.93 0.03 0.70 0.09 GPT-4 0125-Preview 0.74 0.35 0.93 0.03 0.59 0.07 0.75 0.33 0.92 0.03 0.62 0.10 MIXTRAL 7X8 0.77 0.43 0.92 0.03 0.58 0.08 0.60 0.42 0.92 0.04 0.61 0.09 HACKERGPT 0.66 0.33 0.92 0.04 0.58 0.08 0.73 0.18 0.79 0.05 0.47 0.03 GEMINI 1.0 Pro 0.61 0.43 0.91 0.05 0.58 0.08 0.70 0.37 0.90 0.07 0.58 0.10 • Following Zheng et al. [24], GPT-4 0125-Preview also rates the responses on a scale of 1 to 5 based on their comparison with the ground truth references, which have 5 as the highest default score. We conducted the assessment using three different types of cybersecurity questions. The first category, General Cyberse- curity Questions, includes 150 simple one-liners that address a wide range of cybersecurity topics. The second category, Multi-Hop Cybersecurity Questions , includes 150 complex queries that require a thorough, multi-layered understanding. The third category focuses on 300 Common Vulnerability Exposure (CVE) questions that address specific security vul- nerabilities. The questions were classified based on the Dia- mond Model [20], which we used to create questions repre- sentative of real cybersecurity world needs, and the sample size was statistically selected based on standard methods 15. Two experts with 12 and 2 years of experience validated the ground truth. We used Cohen’s Kappa [21], [22] as a metric to evaluate the agreement between the two experts. They categorized the answers as [Correct], [Incorrect] or [Partially_Correct] depending on the context of the questions. The experts agreed well (Cohen’s Kappa = 0.82), indicating an Almost Perfect agreement [61]. For all three evaluation test suites, we generated the an- swers from MoRSE using the mistralai/Mistral-7B-Instruct- v0.2 model, operating on an NVIDIA A100 80GB GPU. A. First Test Suite: Ground Truth Assessing alignment using