text embedding benchmark,” arXiv preprint arXiv:2210.07316 , 2022. [40] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton- Brown, and Y . Shoham, “In-context retrieval-augmented language mod- els,” Transactions of the Association for Computational Linguistics , vol. 11, pp. 1316–1331, 2023. [41] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing,” ACM Computing Surveys, vol. 55, no. 9, pp. 1–35, 2023. [42] D. Chandrasekaran and V . Mago, “Evolution of semantic similarity—a survey,” ACM Computing Surveys (CSUR) , vol. 54, no. 2, pp. 1–37, 2021. [43] A. Aynetdinov and A. Akbik, “Semscore: Automated evaluation of instruction-tuned llms based on semantic textual similarity,” CoRR, vol. abs/2401.17072, 2024. [Online]. Available: https://doi.org/10.48550/ arXiv.2401.17072 [44] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y . Nie, “Retrieve anything to augment large language models,” arXiv preprint arXiv:2310.07554 , 2023. [45] P. Zhang, Z. Liu, S. Xiao, N. Shao, Q. Ye, and Z. Dou, “Soaring from 4k to 400k: Extending llm’s context with activation beacon,”arXiv preprint arXiv:2401.03462, 2024. [46] C. Li, Z. Liu, S. Xiao, and Y . Shao, “Making large language models a better foundation for dense retrieval,” arXiv preprint arXiv:2312.15503, 2023. [47] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning 15 with a unified text-to-text transformer,” Journal of machine learning research, vol. 21, no. 140, pp. 1–67, 2020. [48] Y . Tang and Y . Yang, “Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries,” 2024. [49] W. Xiong, X. L. Li, S. Iyer, J. Du, P. Lewis, W. Y . Wang, Y . Mehdad, W.- t. Yih, S. Riedel, D. Kielaet al.,