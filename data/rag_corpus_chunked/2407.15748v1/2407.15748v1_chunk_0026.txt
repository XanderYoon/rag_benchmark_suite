calculated by checking whether the models correctly identified the vulnerability in the given queries. Regarding Correctness, the MoRSE model scored 0.64 because its responses often include explanation of related exploit codes, which are not present in the ground truth that only describes the vulnerability specifics. This extra information, while useful, lowers the correctness score as it deviates from the expected response. GPT-4 0125-Preview lags behind in this domain-specific chal- lenge. MoRSE achieved an accuracy of 84%, surpassing the GPT-4 0125-Preview model, which had an accuracy of 34%. Our comparison reveals that MoRSE significantly outperforms GPT-4 0125-Preview in accurately identifying vulnerabilities. TABLE III: Performance comparison of models on 300 CVE Queries. Model Similarity Correctness Accuracy µ σ µ σ MoRSE 0.903 0.028 0.640 0.111 84% GPT-4 0125-Preview 0.852 0.004 0.558 0.107 34% We cannot calculate the accuracy metric for general and multi-hop queries because, unlike CVE queries, they lack strictly factual data points, such as a specific vulnerability to identify. For CVEs, accuracy is straightforward: we check if the model identified the correct vulnerability. In contrast, general and multi-hop questions often lack such clear data and require assessment based on multiple aspects depending on the question type. B. Retrievers Impact analysis To calculate the impact of each retriever on 600 questions, we applied a systematic methodology. First, we collected all contexts generated for 150 general questions, 150 multi- hop questions and 300 CVE questions. We then analyzed the frequency with which each retriever was able to suc- cessfully retrieve relevant information within these contexts. The frequency of successful retrievals for each retriever was then calculated as a percentage of the total questions in each category. In this way, we were able to quantify the performance and impact of each retriever in both general and multi-hop question scenarios. Figures 6a, 6b and