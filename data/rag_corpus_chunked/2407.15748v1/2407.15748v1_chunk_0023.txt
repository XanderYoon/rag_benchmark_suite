as [Correct], [Incorrect] or [Partially_Correct] depending on the context of the questions. The experts agreed well (Cohen’s Kappa = 0.82), indicating an Almost Perfect agreement [61]. For all three evaluation test suites, we generated the an- swers from MoRSE using the mistralai/Mistral-7B-Instruct- v0.2 model, operating on an NVIDIA A100 80GB GPU. A. First Test Suite: Ground Truth Assessing alignment using the RAGAS framework Using the RAGAS framework [23], we focused on three metrics: answer relevance , answer similarity , and answer correctness. To calculate these metrics, we used GPT-4 0125- Preview as the underlying model for all calculations. Answer Relevance , shown in Equation 1, measures how pertinent the generated answer is to the given prompt. It is calculated by generating related questions from the model’s answer and comparing their embeddings to the original ques- tion using cosine similarity: Answer Relevance = 1 N NX i=1 cos(Ei g, Eo), (1) 15https://github.com/Mixture-of-RAGs-Security-Experts/MoRSE/tree/ main/CohensKappaEvaluation where Ei g and Eo are the β-embeddings of the generated and original questions, respectively, and N equal to 3, is the number of generated questions. Answer Similarity, shown in Equation 2, evaluates semantic congruence between model-generated responses and prede- fined correct answers, calculated as: Answer Similarity = Vground truth · Vgenerated ∥Vground truth ∥∥Vgenerated∥ , (2) where Vground truth and Vgenerated represent vector representa- tions of the ground truth and generated answers, respectively. Answer Correctness, shown in Equation 3 and 4, evaluates the factual accuracy of generated answers against ground truth. It combines semantic similarities and factual correctness: AC = wF C · F C + wSS · SS, (3) where F C is the factual correctness, quantified using the F1 score that considers True Positives ( T P), False Positives (F P), and False Negatives ( F N): F C = |T P| |T P| +