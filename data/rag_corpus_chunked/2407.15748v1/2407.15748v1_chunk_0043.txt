as AI sparring partners in security testing. Lu et al. [94] integrate graph structural information and in-context learning into LLM-based software vulnerability detection, significantly outperforming conventional models. Yu et al. [95] use GPT- 3 to generate semantic honeywords containing usersâ€™ PII, which improves their indistinguishability and increases de- fenses against security breaches. Unlike traditional LLMs and chatbots, MoRSE stands out by providing instant access to a constantly updating cybersecurity knowledge base, quickly integrating the latest threats and solutions, and enabling extensive customization for various cybersecurity needs. MoRSE does not focus on niche cy- bersecurity topics; instead, it aims to provide comprehensive coverage of cybersecurity knowledge. In addition, MoRSE enhances the user experience by providing user-friendly access to complex cybersecurity information, increasing flexibility and expanding accessibility. VI. C ONCLUSIONS AND FUTURE WORK With cyber threats on the rise, effective cybersecurity strategies are becoming increasingly important. Integrating continuous learning and Retrieval Augmented Generation (RAG) into large language models (LLMs) improves their accuracy and timeliness in responding to these threats. In this paper, we have investigated the use of two Retrieval Augmented Generation (RAG) systems, namely Structured RAG and Unstructured RAG, to provide precise and structured answers to cyberse- curity queries. In Structured RAG, we have focused on imple- menting parallel retrievers to quickly and effectively find the relevant document in response to a user query. Unstructured RAG, on the other hand, is designed to answer even the most complicated cybersecurity queries. We have implemented an evaluation suite to assess the relevance, similarity, and correctness of the answers generated by our system compared to ground truth. The performance of our system was com- pared to other renowned commercial Large Language Models (LLMs) using two additional test suites that adhere to the LLM as a Judge paradigm. The results show that our system outperforms