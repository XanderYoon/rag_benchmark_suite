similarity measure. The measure computes similarity between two sets of embed- dings in two steps. First, for a set of embeddings, the pair-wise similarity scores between all entries within this set are computed using the kernel function. Thus, row k of the resulting similarity matrix contains entries representing the similarity between embed- ding k and all other embeddings, including itself. Computing two such embedding similarity matrices for different models with the same number of embeddings then leads to two matrices E and Eâ€™ of matching dimensions. These are compared directly in the second step with the Hilbert-Schmidt Independence Criterion (HSIC) [10] using the following formula: ğ¶ğ¾ğ´ (ğ¸, ğ¸â€²) = ğ»ğ‘†ğ¼ğ¶ (ğ¸, ğ¸â€²)âˆšï¸ ğ»ğ‘†ğ¼ğ¶ (ğ¸, ğ¸)ğ»ğ‘†ğ¼ğ¶ (ğ¸â€², ğ¸â€²) (1) The resulting similarity scores are bounded in the interval [0, 1] with a score of 1 indicating equivalent representations. CKA assumes that representations are mean-centered. 3.2 Retrieval Similarity While a pair-wise comparison of embeddings offers insights into the similarities of the representations learned by these models, it does not suffice to quantify the similarities in outcomes when these embedding models are deployed for specific tasks. Therefore, in context of RAG systems, we consider the similarity of retrieved text chunks for a given query, when different embedding models are used. As a first step, for a given dataset, we generate embeddings of Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems Table 2: We compare a diverse set of open source models from different families as well as proprietary models with varying performance on MTEB. Model Embedding dimension Max. Tokens MTEB Average Open Source SFR-Embedding-Mistral 4096 32768 67.56 âœ“ mxbai-embed-large-v1 1024 512 64.68 âœ“ UAE-Large-V1 1024 512 64.64 âœ“ text-embedding-3-large 3072 8191 64.59 âœ— Cohere embed-english-v3.0 1024 512 64.47 âœ— bge-large-en-v1.5 1024 512 64.23 âœ“ bge-base-en-v1.5 768 512 63.55 âœ“ gte-large