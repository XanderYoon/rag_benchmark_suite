scores on all datasets are rather close to those of other models. In absolute terms, however, retrieval similarity between Mistral and OpenAI models is only low to moderate. On smaller datasets, the highest Jaccard similarity to text-embedding-3-large only reaches about 0.6 (see Figure 5), while on TREC-COVID, the largest dataset, Jaccard similarity goes down to merely 0.18 (see Figure 6). For Cohere‚Äôs model, the most similar model for top-10 Jaccard similarity is different for each dataset, with the highest scores of 0.51 occurring on ArguAna shwon in Figure 5. For all proprietary models, even the best retrieval similarity at top-10 still suggests that the embeddings that would be presented to an LLM can differ notably. Once again, we could also observe dataset- dependent variance in scores, with lower retrieval similarity on larger datasets. 6 DISCUSSION While a pair-wise comparison of embeddings using CKA shows intra- and inter-family model clusters, retrieval similarity over dif- ferent ùëò offers a more nuanced picture. Especially for smallùëò, which are of most interest from a practical perspective, retrieval similarity varies. When comparing the top-10 retrieved text chunks, the low Jaccard similarity scores indicate little overlap in retrieved chunks, even when CKA scores are high. Especially for the two larger datasets FiQA-2018 and TREC-COVID, these scores are extremely Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval Augmented Generation Systems SFR-Embedding-Mistral UAE-Large-V1 bge-base-en-v1.5 bge-large-en-v1.5 bge-small-en-v1.5 e5-base-v2 e5-large-v2 e5-small-v2 embed-english-v3.0 gte-base gte-large gte-small gtr-t5-base gtr-t5-large mxbai-embed-large-v1 sentence-t5-base sentence-t5-large text-embedding-3-large text-embedding-3-small SFR-Embedding-Mistral UAE-Large-V1 bge-base-en-v1.5 bge-large-en-v1.5 bge-small-en-v1.5 e5-base-v2 e5-large-v2 e5-small-v2 embed-english-v3.0 gte-base gte-large gte-small gtr-t5-base gtr-t5-large mxbai-embed-large-v1 sentence-t5-base sentence-t5-large text-embedding-3-large text-embedding-3-small 1.00 0.29 0.32 0.30 0.21 0.29 0.24 0.19 0.34 0.32 0.26 0.24 0.22 0.28 0.30 0.13 0.16 0.47 0.38 0.29 1.00 0.36 0.59 0.23 0.27 0.23 0.19 0.35 0.35 0.36 0.28 0.18 0.21 0.76 0.12 0.14 0.31 0.29 0.32