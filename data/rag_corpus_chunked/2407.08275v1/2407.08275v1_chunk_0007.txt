models from different families as well as proprietary models with varying performance on MTEB. Model Embedding dimension Max. Tokens MTEB Average Open Source SFR-Embedding-Mistral 4096 32768 67.56 âœ“ mxbai-embed-large-v1 1024 512 64.68 âœ“ UAE-Large-V1 1024 512 64.64 âœ“ text-embedding-3-large 3072 8191 64.59 âœ— Cohere embed-english-v3.0 1024 512 64.47 âœ— bge-large-en-v1.5 1024 512 64.23 âœ“ bge-base-en-v1.5 768 512 63.55 âœ“ gte-large 1024 512 63.13 âœ“ gte-base 768 512 62.39 âœ“ text-embedding-3-small 1536 8191 62.26 âœ— e5-large-v2 1024 512 62.25 âœ“ bge-small-en-v1.5 384 512 62.17 âœ“ e5-base-v2 768 512 61.5 âœ“ gte-small 384 512 61.36 âœ“ e5-small-v2 384 512 59.93 âœ“ gtr-t5-large 768 512 58.28 âœ“ sentence-t5-large 768 512 57.06 âœ“ gtr-t5-base 768 512 56.19 âœ“ sentence-t5-base 768 512 55.27 âœ“ queries and document chunks with each of the embedding models. We then retrieve the ğ‘˜ most similar embeddings in terms of the cosine similarity for a particular query. As these embeddings cor- respond to specific chunks of text, we derive the sets of retrieved chunks C and Câ€™ for a pair of models. To measure the similarity of these sets, we use the Jaccard similarity coefficient as follows: ğ½ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘ (ğ¶, ğ¶â€²) = |ğ¶ âˆ© ğ¶â€² | |ğ¶ âˆª ğ¶â€² | (2) Here, |ğ¶ âˆ© ğ¶â€² | corresponds to the overlap in text chunks by counting how often the two models retrieved the same chunks. Similarly, we can compute the union |ğ¶ âˆª ğ¶â€² |, which corresponds to all retrieved text chunks, counting chunks present in both sets only once. The resulting score is bounded in the interval [0, 1] with 1 indicating that both models retrieved the same set of text chunks. While Jaccard similarity computes the percentage to which two sets overlap, it ignores the order in the sets. Rank similarity [36], on the other hand, considers the order of