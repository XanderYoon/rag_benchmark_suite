Evaluation of retrieval results ; Re- trieval models and ranking ; Language models. KEYWORDS Large language model, Retrieval-augmented generation, Model similarity 1 MOTIV ATION Retrieval-Augmented Generation (RAG) is an emerging paradigm that helps mitigate the problems of factual hallucination [13] and outdated training data [ 27] of large language models (LLMs) by providing these models with access to an external, non-parametric knowledge source (e.g. a document corpus). Central to the func- tioning of RAG frameworks is the retrieval step, wherein a small subset of candidate documents is retrieved from the document cor- pus, specific to the input query or prompt. This retrieval process, known as dense-retrieval, hinges on text embeddings. Typically, the generation of these embeddings is assigned to an LLM, for which there are several options due to the rapid evolution of the field. Consequently, selecting the most suitable embedding model from an array of available choices emerges as a critical aspect in the development of RAG systems. The information to guide this choice is currently primarily limited to architectural details (which are also on occasion scarce due to the prevalence of closed models) and performance benchmarks such as the Massive Text Embedding Benchmark (MTEB) [28]. We posit that an analysis of the similarity of the embeddings gen- erated by these models would significantly aid this model selection process. Given the large number of candidates and ever increas- ing scale of the models, a from-scratch empirical evaluation of the embedding quality of these LLMs on a particular task can incur significant costs. This challenge becomes especially pronounced when dealing with large-scale corpora comprising potentially mil- lions of documents. While the relative performance scores of these models on benchmark datasets offer the simplified perspective of comparing a single scalar value on an array of downstream tasks, such a view of