of those in- cluded in the benchmarks [8] or where the evaluation encompasses other aspects of the retriever beyond the embedding model being used [34]. Another approach, that does not rely on ground-truth labels, is given by the Retrieval Augmented Generation Assessment (RAGAS) framework, which uses an LLM to determine the ratio of sentences in the retrieved context that are relevant to the answer being generated [7]. To the best of our knowledge, there are no Table 1: The datasets used for generating embeddings with their number of queries and corpus size. Dataset Name Queries Corpus TREC-COVID 50 171k NFCorpus 323 3.6k FiQA-2018 648 57k ArguAna 1406 8.67k SciFact 300 5k works that evaluate the similarity of embedding models from a retrieval perspective. 3 METHODS We evaluate embedding model similarity using two approaches. The first directly compares the embeddings of text chunks generated by the models. The second approach is specific to the RAG context, where we evaluate the similarity of retrieved results for a given query. These approaches are discussed in detail in the following sections. 3.1 Pair-wise Embedding Similarity There are several metrics defined in the literature that measure representational similarity [15]. Many of these metrics require the representation spaces of the embeddings to be compared to be aligned and/or the dimensionality of the embeddings across the models to be identical. To avoid these constraints, we pick Centered Kernel Alignment (CKA) [16] with a linear kernel as our similarity measure. The measure computes similarity between two sets of embed- dings in two steps. First, for a set of embeddings, the pair-wise similarity scores between all entries within this set are computed using the kernel function. Thus, row k of the resulting similarity matrix contains entries representing the similarity between embed- ding k and all other embeddings, including itself.