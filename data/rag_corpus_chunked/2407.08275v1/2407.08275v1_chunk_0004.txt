literature. A few examples include model-stitching [2], [18], [1], disagreement measures between output classes [25], [41], and quantifying the similarity between the class-wise out- put probabilities [22]. We would point the reader to the survey by Klabunde et al. [15] for a detailed overview of representational and functional similarity measures. Recently, a few works have also focused on specifically evaluat- ing the similarity of LLMs. While Wu et al. [39] evaluate language models along several perspectives, such as their representational and neuron-level similarities, their evaluation pre-dates the intro- duction of the recent wave of large scale models. Freestone and Santu [9] consider similarities of word embeddings, and evaluate if LLMs differ significantly to classical encoding models in terms of their representations. The works by Klabunde et al. [ 14] and Brown et al. [3] are more recent, and evaluate the representational similarity of LLMs, with the latter also considering the similarities between models of different sizes in the same model family. Much of the literature on evaluation of LLM embeddings focuses on their performance on downstream tasks, with benchmarks such as BEIR [35] (for retrieval specifically) and MTEB [28] providing a unified view of embedding quality across metrics and datasets. The metrics used here mostly include typical information retrieval met- rics such as precision, recall, and mean reciprocal rank at certain cutoffs. Some works specifically evaluate the retrieval components in a RAG context, where they either use a dataset outside of those in- cluded in the benchmarks [8] or where the evaluation encompasses other aspects of the retriever beyond the embedding model being used [34]. Another approach, that does not rely on ground-truth labels, is given by the Retrieval Augmented Generation Assessment (RAGAS) framework, which uses an LLM to determine the ratio of sentences in the retrieved context that are