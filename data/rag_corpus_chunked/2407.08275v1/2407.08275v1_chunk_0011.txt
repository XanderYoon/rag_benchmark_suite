0.72 0.72 0.74 0.73 0.72 1.00 0.87 0.71 0.74 0.76 0.70 0.73 0.71 0.70 0.66 0.74 0.73 0.72 0.72 0.71 0.71 0.74 0.72 0.71 0.87 1.00 0.73 0.76 0.77 0.71 0.75 0.72 0.69 0.67 0.72 0.78 0.76 0.76 0.74 0.75 0.78 0.77 0.77 0.71 0.73 1.00 0.87 0.84 0.76 0.79 0.76 0.73 0.68 0.72 0.80 0.78 0.78 0.76 0.77 0.81 0.80 0.79 0.74 0.76 0.87 1.00 0.90 0.78 0.81 0.77 0.74 0.70 0.74 0.80 0.78 0.78 0.77 0.78 0.81 0.81 0.80 0.76 0.77 0.84 0.90 1.00 0.78 0.82 0.78 0.75 0.63 0.68 0.76 0.75 0.76 0.72 0.71 0.75 0.76 0.73 0.70 0.71 0.76 0.78 0.78 1.00 0.93 0.83 0.79 0.67 0.71 0.79 0.78 0.79 0.76 0.76 0.81 0.81 0.79 0.73 0.75 0.79 0.81 0.82 0.93 1.00 0.81 0.78 0.66 0.69 0.78 0.77 0.77 0.75 0.74 0.76 0.79 0.76 0.71 0.72 0.76 0.77 0.78 0.83 0.81 1.00 0.81 0.63 0.66 0.74 0.73 0.74 0.75 0.73 0.73 0.74 0.72 0.70 0.69 0.73 0.74 0.75 0.79 0.78 0.81 1.00 0.7 0.8 0.9 1.0 Figure 1: Mean CKA similarity across all five datasets. Models tend to be most similar to models belonging to their own family, though some interesting inter-family patterns are visible as well. two from OpenAI (text-embedding-3-large and -small) [31] and one from Cohere (Cohere embed-english-v3.0) [5]. We also compare the mxbai-embed-large-v1 (mxbai) [17] and UAE-Large-V1 (UAE) [19] models, that not only report very similar performances on MTEB, but also identical embedding dimensions, model size and memory usage. Finally, we include SFR-Embedding-Mistral (Mistral) [24] as the best-performing model on the leaderboard at the time of our experiments. A detailed overview of all selected models can be seen in Table 2. To compare embedding similarity across models and datasets, we employ different strategies depending on the similarity measure. We apply CKA by retrieving