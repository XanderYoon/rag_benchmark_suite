embedding quality of these LLMs on a particular task can incur significant costs. This challenge becomes especially pronounced when dealing with large-scale corpora comprising potentially mil- lions of documents. While the relative performance scores of these models on benchmark datasets offer the simplified perspective of comparing a single scalar value on an array of downstream tasks, such a view of model similarity might overlook the nuances of the relative behaviour of the models [15]. As an example, the absolute difference in precision@k between two retrieval systems only pro- vides a weak indication of the overlap of retrieved results. We argue that identifying clusters of models with similar behaviour would allow practitioners to construct smaller, yet diverse candidate pools of models to evaluate. Beyond model selection, as highlighted by Klabunde et al., [ 14], such an analysis also facilitates the identi- fication of common factors contributing to strong performance, easier model ensembling, and detection of potential instances of unauthorized model reuse. In this paper, we analyze different LLMs in terms of the simi- larities of the embeddings they generate. Our similarity analysis serves as an unsupervised evaluation framework for these embed- ding models, in contrast to performance benchmarks that require labelled data. We do this from a dual perspective - we directly com- pare the embeddings using representational similarity measures. Additionally, we evaluate model similarity specifically in terms of their functional impact on RAG systems i.e. we look at how sim- ilar the retrieved results are. Our evaluation focuses on several prominent model families, to analyze similarities both within and across them. We also compare proprietary models (such as those by arXiv:2407.08275v1 [cs.IR] 11 Jul 2024 Caspari et al. OpenAI or Cohere) to open-sourced ones in order to identify the most similar alternatives. Our experiments are carried out on five popular