the implementation of our experiments. As we focus on the retrieval component of RAG systems, we select five publicly available datasets from the BEIR benchmark [35]. As generating embeddings for large datasets is a time-intensive process, especially for a larger number of models, we opt for five of the smaller datasets from the benchmark. This approach allows us to compare embeddings generated by a variety of models while at the same time allowing us to evaluate embedding similarity accross datasets. An overview of the datasets is shown in Table 1. For each dataset, we create embeddings by splitting documents into text chunks such that each chunk contains 256 tokens. The embedding vectors are stored with Chroma DB [12], an open source embedding database. For each vector, we additionally store information about the document and text chunk ids it encodes to be able to match embeddings generated by different models for evaluation. For model selection, we primarily use publicly available models from the MTEB leaderboard [28]. We do not simply pick the best performing models on the leaderboard; instead, our choices are influenced by several factors. Firstly, we focus on analyzing similar- ities within and across model families and pick models belonging to the e5 [37], t5 [29, 30], bge [40], and gte [23] families. Secondly, we recognize that it might be of interest to users to avoid pay-by-token policies of proprietary models by identifying similar open-source al- ternatives. Therefore, we pick high-performing proprietary models, Caspari et al. gtr-t5-base gtr-t5-large mxbai-embed-large-v1 UAE-Large-V1 bge-large-en-v1.5 bge-small-en-v1.5 gte-small gte-large bge-base-en-v1.5 gte-base sentence-t5-base sentence-t5-large SFR-Embedding-Mistral text-embedding-3-large text-embedding-3-small e5-large-v2 embed-english-v3.0 e5-base-v2 e5-small-v2 gtr-t5-base gtr-t5-large mxbai-embed-large-v1 UAE-Large-V1 bge-large-en-v1.5 bge-small-en-v1.5 gte-small gte-large bge-base-en-v1.5 gte-base sentence-t5-base sentence-t5-large SFR-Embedding-Mistral text-embedding-3-large text-embedding-3-small e5-large-v2 embed-english-v3.0 e5-base-v2 e5-small-v2 1.00 0.81 0.64 0.63 0.64 0.64 0.61 0.63 0.65 0.62 0.71 0.66 0.67 0.68 0.70