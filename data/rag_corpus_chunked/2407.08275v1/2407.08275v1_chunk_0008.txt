counting chunks present in both sets only once. The resulting score is bounded in the interval [0, 1] with 1 indicating that both models retrieved the same set of text chunks. While Jaccard similarity computes the percentage to which two sets overlap, it ignores the order in the sets. Rank similarity [36], on the other hand, considers the order of common elements, with closer elements having a higher impact on the score. The measure assigns ranks to common text chunks according to their similarity to the query, i.e. ğ‘Ÿğ¶ ( ğ‘—) = ğ‘› if chunk ğ‘— was the top-ğ‘› retrieved result for the query. Ranks are then compared using: ğ‘…ğ‘ğ‘›ğ‘˜ (ğ‘Ÿğ¶ ( ğ‘—), ğ‘Ÿğ¶ â€² ( ğ‘—)) = 2 (1 + |ğ‘Ÿğ¶ ( ğ‘—) âˆ’ ğ‘Ÿğ¶ â€² ( ğ‘—)|) (ğ‘Ÿğ¶ ( ğ‘—) + ğ‘Ÿğ¶ â€² ( ğ‘—)) (3) With this, rank similarity for two sets of retrieved text chunks C, Câ€™ is calculated as: ğ‘…ğ‘ğ‘›ğ‘˜ğ‘†ğ‘–ğ‘š (ğ¶, ğ¶â€²) = 1 ğ» (|ğ¶ âˆ© ğ¶â€² |) âˆ‘ï¸ ğ‘— âˆˆ |ğ¶âˆ©ğ¶ â€² | ğ‘…ğ‘ğ‘›ğ‘˜ (ğ‘Ÿğ¶ ( ğ‘—), ğ‘Ÿğ¶ â€² ( ğ‘—)) (4) with ğ» (|ğ¶ âˆ© ğ¶â€² |) = Ãğ¾=|ğ¶âˆ©ğ¶ â€² | ğ‘˜=1 1 ğ‘˜ denoting the K-th harmonic number, normalizing the score. Like the other measures, rank sim- ilarity is bounded in the interval [0, 1] with 1 indicating that all ranks are identical. 4 EXPERIMENTAL SETUP The following paragraphs describe our choice of datasets and mod- els, along with details of the implementation of our experiments. As we focus on the retrieval component of RAG systems, we select five publicly available datasets from the BEIR benchmark [35]. As generating embeddings for large datasets is a time-intensive process, especially for a larger number of models, we opt for five of the smaller datasets from the benchmark. This approach allows us to compare