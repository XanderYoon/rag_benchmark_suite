lower (std.0.12vs.1.03–2.66), ensur- ing more predictable and stable performance. The main overhead instead arises from the preprocessing stage, where vocal detection and segmentation remain relatively costly compared to the inference of LIVI itself. Reducing this cost therefore represents an important direction for future work. Yet LIVI offers a favorable trade-off between complexity and accuracy when compared to base- lines. With31.9M parameters, it is substantially lighter than large systems such as ByteCover2 (202.3M) and CLEWS (196.8M) while remaining competitive in accuracy, and it surpasses similarly sized models like CQTNet and DViNet, particularly on large-scale benchmarks such as Discogs-VI. 6 Conclusion This work introduced LIVI, an approach for Music Cover Retrieval that bal- ances retrieval accuracy with computational efficiency. With no task-specific fine-tuning and a relatively simple architecture, it stands in contrast to the pre- vailing trend toward increasingly complex and resource-intensive models. Yet, when tracks have lyrics, LIVI achieves performance on par with these systems, demonstrating that musically informed inputs—rather than architectural esca- lation—can deliver state-of-the-art results while remaining computationally ef- ficient at both training and inference. Several limitations of the proposed approach should be acknowledged. First, the text embedding model is used off-the-shelf and is not fine-tuned specifically for the retrieval task, leaving room for further performance gains. Second, LIVI is inherently restricted to musical content with sufficient vocal material; purely instrumental tracks or songs with minimal vocals are therefore excluded, limiting the universality of the method and making its applicability dependent on the availability and quality of lyrics information. Incorporating complementary har- monic features could help mitigate this limitation, particularly for cover versions that preserve melodic structure while substantially altering lyrics or falling out- side the scope of the current model. Third, the vocal detection component used during preprocessing is proprietary, which impacts full reproducibility; however, open-source alternatives [8,