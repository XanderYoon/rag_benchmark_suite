∈R d is obtained by composing an encoder-decoder ASR modelftransc with a pre-trained text embedding model ftext. This composition can be seen as a fixed encoder: gtext =f text ◦f transc : ( C →R d xi 7→t i Next,wedefineanaudioencoderthatprojectsrawaudiointothelyrics-informed embedding space. Given the same audio excerptxi, latent features are extracted from the ASR encoder viaf extract and projected byf proj to yield the audio embedding: gaudio =f proj ◦f extract : ( C →R d xi 7→a i The objective is to learngaudio such that audio embeddingsa i are aligned with their corresponding lyric embeddingsti under cosine similarity. Formally, this corresponds to minimizing the loss: Lcos = X xi∈C  1−s gaudio(xi), gtext(xi)  6 J. Affolter et al. Yet the training objective can be pushed further given the data available: since the lyrics-informed space is fixed and the target lyrics embeddings are ac- cessible during training, one can leverage not only pointwise alignment but also the geometry of the target space. More specifically, the inter-sample distances between lyrics embeddings can serve as an additional supervision signal to guide thetrainingoftheaudioencoder.Thisisachievedbyenforcingthatpairwisesim- ilarities between audio embeddings,s(ai, aj), match those of the corresponding lyrics embeddings,s(ti, tj). Formally, given a batch{(xi, ti)}B i=1, this component of the training objective takes the form: LMSE = 1 B2 BX i,j=1  s(ai, aj)−s(t i, tj) 2 . This yields the final objective optimized during training, combining a pointwise alignment termLcos with a geometry-preservation termLMSE: Ltotal =αL cos + (1−α)L MSE, α∈[0,1] Unlike standard multimodal representation learning [20, 27, 48], which jointly trains both audio and text encoders to construct a joint embedding space, our method fixes the textual space and adapts only the audio encoder. This sub- tle difference allows the training process to exploit the geometry of the lyrics- informed embedding space directly, rather