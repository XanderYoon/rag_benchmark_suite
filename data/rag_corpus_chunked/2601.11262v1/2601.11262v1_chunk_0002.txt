has been underexplored, mainly due to two obstacles: the avail- ability of editorial lyrics at scale—often restricted by third-party licensing—and the challenge of extracting lyrics from polyphonic audio [49], a task where re- cent advances have improved accuracy but remain computationally demanding. While early works that use lyrics [1, 46] relied on relatively simple approaches to derive lyric representations, resulting in limited downstream performance, more recent work [15, 31] achieved stronger results but integrates transcription into a complex multimodal architecture, increasing model size and computational cost. Our work builds on the hypothesis that songs with semantically similar lyrics are likely to be covers [10]. To test this idea, we first construct a pipeline that represents songs in a lyric-informed embedding space, obtained by applying an Automatic Speech Recognition (ASR) system followed by a multilingual text encoder. This design is motivated by two factors: (i) clean editorial lyrics are rarely available at scale, making transcription a necessary step, and (ii) modern multilingual text encoders, pretrained for semantic similarity, provide a powerful and readily applicable representation space. While this pipeline achieves strong performance, its reliance on full transcription makes it computationally costly. Motivated by the need for efficiency in real-world deployment, we introduce LIVI (Lyrics-Informed Version Identification), a model that learns to project latent audio representations directly into the lyric embedding space defined by the pipeline. In doing so, LIVI removes the transcription step, reducing inference cost while preserving retrieval accuracy. Despite its relative simplicity and the absence of explicit fine-tuning for the downstreamtask,LIVIachievesperformanceonparwith—orsuperiorto—state- of-the-art systems. It delivers an efficient, reproducible3, and domain-grounded alternative, challenging the dominance of complexity-heavy multimodal systems. By design, our method applies only to tracks with sufficient vocal content, with a preprocessing stage used to exclude those lacking it. While this restriction narrows the scope of applicability, its