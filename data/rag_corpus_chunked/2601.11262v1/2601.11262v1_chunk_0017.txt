tracks). We compare Word Error Rate (WER) between editorial lyrics and Whisper tran- scriptions obtained either from vocal segments extracted with the proprietary model or from non-overlapping 30-second segments processed with Whisper’s VAD. In addition, we analyze hallucinated outputs commonly observed in Whis- per transcriptions of non-spech audio (e.g., “thank you”, “music”, “subtitle”) [6]. 5 gte-multilingual-base (gte-b), multilingual-e5-{small, large, large-instruct} (e5-{s, l, l-inst}), jina- embeddings-v3 (jina), multilingual-mpnet-base-v2 (mpnet). Scalable Music Cover Retrieval Using Lyrics-Aligned Audio Embeddings 11 While WER differences are not statistically significant, the average number of hallucinations per track is significantly lower with the proprietary model (p = 1.24×10 −6), decreasing from 0.51 to 0.25. The total number of hallucinations across all transcriptions is likewise reduced (1,023 vs. 509). 5.3 Alignment of Audio with Lyrics-Informed Embeddings We evaluate the performance of the audio encodergaudio by examining whether it fulfills its training objective, namely aligning audio embeddings with their lyric-based counterparts. Alignment is assessed at both thesegmentandtrack levels under cosine similarity. At the segment level, cosine similarity is computed for each of the 167,484 audio–lyrics embedding pairs(ai, ti)taken from the test split of the audio encoder training phase, withai =g audio(xi)andt i =g text(xi). At the track level, segment embeddings from the same recording are averaged to form a global representation, which is then compared against the lyrics embed- ding derived from the full transcription (60,524 tracks). Segment-level embed- dings yield a mean similarity of0.8574(std:0.0757), while aggregated track-level embeddingsreach0.9109(std:0.0379).Whilethehighermeanandlowervariance at the track level suggest that the encoder integrates local cues into stable global representations, the results further demonstrate that our approach achieves tight audio-lyric alignment. 5.4 Application to Music Cover Retrieval We evaluate in Table 2 the audio encodergaudio on the retrieval task by comput- ing segment-level embeddingsa i =g audio(xi)for all vocal segmentsx i of each track (see