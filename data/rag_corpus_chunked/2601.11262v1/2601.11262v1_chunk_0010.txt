to smooth temporal boundaries. The resulting segments are then trun- cated or zero-padded to a fixed length of 30s to match the input requirements 4 Model architecture and pretrained weights are available athttps://github.com/jordipons/musicnn. Scalable Music Cover Retrieval Using Lyrics-Aligned Audio Embeddings 7 of the ASR model. For a given trackx∈ C, this process produces multiple au- dio segmentsx i,which are treated independently from one another and used as inputs to both the frozen text encodergtext and the audio encodergaudio. 4.2 Lyrics-Informed Embedding Space The lyrics-informed embedding space (Figure 1.a) is defined asT={gtext(xi)| xi ∈ C},whereg text =f text ◦f transc maps an audio excerptx i to its lyric embeddingt i ∈R d. This space clusters semantically similar lyrics— assumed to represent versions—closer together than unrelated ones, and serves as the target structure for training the audio encodergaudio. Transcription modelftransc Givenitssuitabilityforthistask(moredetailsinSec- tion 2), we usewhisper-large-v3-turbo, an optimized variant ofwhisper-large-v3 that offers comparable transcription accuracy with up to 8×faster inference. Its architecture combines a convolutional front-end, 32 Transformer encoder layers, and a 4-layer Transformer decoder generating transcriptions autoregressively. Text embedding modelftext As mentioned in 3, the objective is to construct a se- mantically meaningful space for lyric transcriptions, providing a robust structure for the audio encoder to align with. Recent progress in Natural Language Pro- cessing has produced text embedding models especially well suited to this pur- pose. Most are derived from large pre-trained language models and fine-tuned for semantic similarity, typically within the Sentence-BERT framework [39], which maps full sentences into fixed-size embeddings that preserve semantic proxim- ity—even in multilingual settings. Based on an evaluation of six multilingual text embedding models for the downstream task (see 5.1), we selectgte-multilingual- base[56], an encoder-only Transformer that produces 768-dimensional embed- dings across more than 70 languages. It achieves SOTA results in multilingual retrieval for