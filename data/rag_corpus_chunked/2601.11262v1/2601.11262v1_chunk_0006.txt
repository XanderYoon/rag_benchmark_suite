transcripts are encoded as sequences of frame- and token-level embeddings. A similarity matrix is then computed to determine the optimal alignment path, enabling precise word-level synchroniza- tion. Yu et al. [52] addressed cross-modal retrieval by training parallel audio and lyric encoders with a Deep Canonical Correlation Analysis (DCCA) loss. Their approach projects spectrogram-based audio features and text embeddings into a joint space, but the reliance on correlation objectives makes training computa- tionally expensive. In contrast, our method learns global song-level embeddings that integrate lyric semantics directly into the audio representation. 3 Methodology We present LIVI (Lyrics-Informed Version Identification), an approach that leverages the invariance of lyrics across renditions to identify covers (Figure 1). Our starting point is a lyrics-informed pipeline designed to maximize retrieval accuracy, without regard to efficiency. Audio is first transcribed using an en- coder–decoder ASR model, and the resulting text is then embedded with a mul- tilingual model fine-tuned for semantic similarity. This produces an embedding space in which semantically similar lyrics—even when expressed in different lan- guages—cluster closely, while unrelated text lies further apart. Fig. 1: Overview of the proposed LIVI framework.(a) A frozen text encoder (g text) combines an ASR model with a pre-trained text embedding model to produce lyrics embeddingsti. (b) An audio encoder (gaudio) projects ASR encoder latent representations into the same embedding space. (c)Trainingoptimizesacombinedobjective:pointwisealignmentofa i witht i undercosinesimilarity, and geometry preservation ensuring that pairwise similarities between audio embeddings mirror those of their corresponding lyric embeddings. Scalable Music Cover Retrieval Using Lyrics-Aligned Audio Embeddings 5 Its key drawback lies in the reliance on transcription, where the ASR autore- gressive decoder introduces considerable computational overhead. To overcome this, LIVI discards the decoder and trains an audio encoder to map latent ASR statesdirectlyintothelyric-informedembeddingspacederivedfromthepipeline. This removes the need for full transcription while preserving retrieval accuracy, resulting