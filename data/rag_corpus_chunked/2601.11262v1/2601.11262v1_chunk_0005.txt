difficult to isolate the impact of lyrics. Reproducibility is further constrained by the lack of open- source implementations and technical details [1]. Automatic Lyrics Recognition.Modern Automatic Speech Recognition (ASR) systems are based on end-to-end neural architectures that map raw audio di- rectly to text, in contrast to older hybrid pipelines combining hidden Markov models with deep neural networks (HMM-DNN) [29]. Recent advances in Au- tomatic Lyrics Transcription have been driven by systems such as AudioShake v3 [9], which achieves state-of-the-art accuracy but remains proprietary. As an open-sourcealternative,Whisper[36]hasbeenwidelyadoptedforlyrictranscrip- 4 J. Affolter et al. tion [43]. Its robustness to noise, accents, and other real-world variability makes it particularly suitable for the heterogeneous conditions of music audio [9, 37]. As an encoder–decoder Transformer, Whisper encodes audio into latent representa- tions, which the decoder attends to via cross-attention to generate transcriptions autoregressively. Audio to Text Alignment.Recent advances in audio–text modeling focus on learning aligned representations across modalities, typically through contrastive pretraining in a shared embedding space. Inspired by CLIP [35], CLAP-style models [12, 20, 25, 27, 55] jointly train audio and text encoders to maximize sim- ilarity between paired data, achieving strong zero-shot performance in tagging, retrieval, and captioning. However, these methods typically rely on high-level textual descriptors rather than structured content such as lyrics. Several works have instead explored aligning audio with lyrics, though with different objectives. Durand et al. [19] proposed a contrastive learning frame- work in which singing audio and lyric transcripts are encoded as sequences of frame- and token-level embeddings. A similarity matrix is then computed to determine the optimal alignment path, enabling precise word-level synchroniza- tion. Yu et al. [52] addressed cross-modal retrieval by training parallel audio and lyric encoders with a Deep Canonical Correlation Analysis (DCCA) loss. Their approach projects spectrogram-based audio features and text embeddings into a