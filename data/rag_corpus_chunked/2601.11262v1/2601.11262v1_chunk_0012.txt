2.a).We adopt the encoder of the Whisper model used in the lyrics-informed embedding space as our audio backbone. This choice is motivated by two considerations: (i) its internal representations, shaped by the ASR training objective, are expected to capture phonetic and linguistic infor- mation [24] that makes them suitable for alignment with lyrics embeddings; and (ii) reusing the encoder from the same ASR model enables our model to specif- ically target the decoder in order to have an efficient alternative. Consequently, we keep the encoder frozen to preserve this alignment and maintain the latent structure learned during Whisper’s large-scale training. Given an 80-channel log- Mel spectrogram, the encoder produces a sequence of hidden statesH∈RL×dw, whered w = 1280is the latent dimension andL= 1500the number of frames for a 30s input. Attention-based Temporal Pooling (Fig. 2.b).To reduce frame-level represen- tations into a fixed-dimensional vector suitable for projection into the lyrics- informed embedding space, we adopt an attention-based pooling mechanism in- spired by [44]. A learnable [CLS] tokenqcls ∈R dw is appended to the hidden statesHand acts as the sole query in a single-head attention mechanism with Rotary positional embeddings (RoPE) [42]. Formally: Attention(Q, K, V)= softmax  QK ⊤ √dk  V, Q=q clsWQ, K=HW K , V=HW V whereW Q, WK, WV ∈R dw×dk are learnable projection matrices, anddk denotes the dimensionality of the key vector, which we set todw in our implementation. The attention weights determine the relative importance of each frame, guiding how information from the sequence is aggregated into the [CLS] representation. This representation is then passed through a residual feed-forward block with LayerNorm to yield the final pooled embeddingh∈Rdw. Projection Head (Fig. 2.c).The pooled vectorhis finally projected into the 768-dimensional lyric-informed embedding space through a four-layer MLP with hidden sizes [3072, 2048, 2048, 1536]—a configuration selected