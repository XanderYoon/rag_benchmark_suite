scribed in Section 4.2. We retain a subset of 1.5M pairs, of which 1.2M are used for training, 170k for validation and 170k for testing (80/10/10 split). Benchmarks.We use the standard Covers80 [21] and SHS100k-TEST [7], along withthetestsetofDiscogs-VI[3],restrictedtoentrieswithanavailableYouTube link. All datasets follow the same preprocessing pipeline: (1) tracks are linked to a proprietary catalog via fingerprinting after matching against YouTube audio with the provided links; and (2) tracks with insufficient vocal content (see Sec- tion 4.1) are discarded. At this point, it should be acknowledged that due to our lyrics-centered method, we exclude some of the tracks in the reference datasets. Consequently, 82.76% of Covers80 (116 tracks in 58 cliques of average size 2), 81.95% of SHS100k (890 tracks in 105 cliques of average size 7.28±6.29), and 85.29% of Discogs-VI (72,316 tracks in 33,660 cliques of average size 3.04±2.34) are retained for evaluation. We leave as future work the use of different musical modalities to retrieve these instances. Nevertheless, our experiments operate at a large scale of72,316tracks for Discogs-VI, yielding an evaluation setting that reflects real-world conditions. Evaluation.We follow the standard evaluation setting for the retrieval task [50]. Given a query track, the system ranks all other tracks in the dataset according to their cosine similarity with the query in the embedding space (see 3.1), and retrieval performance is assessed using standard metrics: MR1, the mean rank of the first true positive; HR@1, the fraction of queries with the correct cover ranked first; and MAP@10, which evaluates precision within the top 10 results. Note that a query may correspond to multiple covers, with an average of 2 covers per clique in Covers80, 12 in SHS100k, and 6 in Discogs-VI. 5.1 V alidation of the Lyrics-Informed Embedding Space We evaluate the lyrics embeddingsti =g text(xi)in the downstream retrieval task to assess