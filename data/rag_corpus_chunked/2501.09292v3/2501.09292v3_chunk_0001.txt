y owed to their exposure to massive training data and successive ﬁne-tuning of instruction dat asets. To increase the helpfulness and decrease the harmfulness of the models, they are being furth er ﬁne-tuned over preference collec- tions Bai et al. (2022); Ouyang et al. (2022); Rafailov et al. (2024). Further, Retrieval Augmented Generation (RAG) Lewis et al. (2020); Dhole (2024a); Dhole et al. (2024), in the effort to mitigate hallucinations, enriches these models with domain-speciﬁc informa- tion and tackles scenarios where the intrinsic knowledge of the base model falls short. By integrating externally retrieved content during the generation phase, RAG enhances the model’s ability to pro- duce less hallucinatory and domain-conditioned responses . This approach has been particularly valuable in complex applications such as long-form generat ion like multi-hop question answering, which often requires multiple retrievals to address a query comprehensively. However, to optimize the efﬁciency of RAG, retrieval should only be invoked when necessary — also referred to as conditional retrieval. Previous condit ional RAG setups have explored multiple paradigms like low token probabilities Jiang et al. (2023), external classiﬁers Wang et al. (2023), or low entity popularity Mallen et al. (2023) as indicators of t he LLMs’ knowledge gaps. However, most of these methods fall short in either approximating kno wledge gaps of the LLMs or lacking the ability to invoke retrieval dynamically. On the other hand, with the potential of LLMs to hallucinate, there has been an increasing interest in uncertainty detection methods to gauge LLMs’ conﬁdence in their outputs Fadeeva et al. (2023). Unlike traditional methods that rely on rigid heuristics or external classiﬁers, uncertainty detection leverages the inherent variability in LLM-generated respo nses to estimate conﬁdence dynamically. 1 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at