increasing interest in uncertainty detection methods to gauge LLMs’ conﬁdence in their outputs Fadeeva et al. (2023). Unlike traditional methods that rely on rigid heuristics or external classiﬁers, uncertainty detection leverages the inherent variability in LLM-generated respo nses to estimate conﬁdence dynamically. 1 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 For instance, semantic sets-based UD approaches Lin et al. ( 2023) group responses based on mean- ing, and use the number of clusters to directly reﬂect the lev el of uncertainty — with greater variabil- ity signaling higher uncertainty. Similarly, spectral met hods using eigenvalue Laplacians quantify response diversity by identifying strong or weak clusterin g patterns in pairwise similarity graphs. These approaches align with the probabilistic nature of LLMs as well as adaptively gauge uncertainty based on output coherence, making them more robust against a dversarial or ambiguous inputs. In this work, we evaluate if such uncertainty detection meth ods can indeed enhance the reliability of conditionally invoking retrieval, by measuring its impa ct on a downstream task of multi-hop question answering. In that regard, we resort to a conditional RAG system and empl oy numerous uncertainty detection metrics to test the need for invoking retrieval. Our RAG syst em performs forward-looking active retrieval in the style of Jiang et al. (2023). Speciﬁcally, we contribute the following: • We design a method that performs retrieval augmented gener ation with dynamic retrieval through uncertainty detection • We perform an exhaustive analysis of various conditions fr om the “uncertainty quantiﬁca- tion” literature to gauge the best strategy to dynamically r etrieve during generation • Based on the results, we present insights for future resear ch Our insights are useful to gauge whether uncertainty detect ion methods can help improve the