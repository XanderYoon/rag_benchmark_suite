URL https://www.aclweb.org/anthology/2020.coling-main.5 80. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Ja ne Dwivedi-Y u, Yiming Y ang, Jamie Callan, and Graham Neubig. Active retrieval aug mented generation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empir- ical Methods in Natural Language Processing , pp. 7969–7992, Singapore, December 2023. 6 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 Association for Computational Linguistics. doi: 10.18653 /v1/2023.emnlp-main.495. URL https://aclanthology.org/2023.emnlp-main.495. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatﬁeld-Dodds, Nova DasSarma, Eli T ran-Johnson, et al. Language mod- els (mostly) know what they know. arXiv preprint arXiv:2207.05221 , 2022. Lorenz Kuhn, Y arin Gal, and Sebastian Farquhar. Semantic un certainty: Linguis- tic invariances for uncertainty estimation in natural lang uage generation. In The Eleventh International Conference on Learning Representa tions, 2023. URL https://openreview.net/forum?id=VD-AYtP0dve. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petro ni, Vladimir Karpukhin, Naman Goyal, Heinrich K¨ uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨ aschel, et al. Retrieval-augmented genera- tion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33: 9459–9474, 2020. Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating wit h conﬁdence: Uncertainty quantiﬁ- cation for black-box large language models. Transactions on Machine Learning Research, 2023. Craig Macdonald, Nicola Tonellotto, Sean MacAvaney, and Iadh Ounis. Pyterrier: Declarative exper- imentation in python from bm25 to dense retrieval. In Proceedings of the 30th acm international conference on information & knowledge management , pp. 4526–4533, 2021. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigati ng effectiveness of paramet- ric and non-parametric memories. In Anna Rogers, Jordan Boy d-Graber, and Naoaki Okazaki (eds.), Proceedings of