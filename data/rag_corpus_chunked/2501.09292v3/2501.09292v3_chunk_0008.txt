(2023) generates a retrieval query for the missing en- tity in the temporary sentence by using the sentence with the low probability token removed or by prompting an external question generator to generate a ques tion for the missing entity as the answer. We generalize this by instead prompting the model to generat e a subquery to ﬁgure out the missing information needed to answer the user query in an open-ended manner. We deﬁne a subquery generator SQ which takes in as input few-shot exemplars of subqueries, th e current user query q, and the current partial answer sentences uttered in chain- of-thought Wei et al. (2022) fashion. It seeks to generate subqueries to get a spec iﬁc piece of information not generated in the partial answer sentences but is needed to answer q. Once this subquery is generated, we use this subquery to retrieve additional passages from the exte rnal retriever R. These passages are then appended to the user input, and the generation continues. For instance, for the question, “Which ﬁlm has the director w ho died ﬁrst, Promised Heaven or Fire Over England?”, and the partially generated answer, “The ﬁl m Promised Heaven was directed by Eldar Ryazanov. Fire Over England was directed by William K. Howard. Eldar Ryazanov died on November 30, 2015.”, we expect the model to generate a subque ry, “When did William K. Howard die?”. 5 S ETUP The generator used in all experiments was GPT-3 (davinci-00 2) Brown et al. (2020), and the re- triever employed was BM25 through PyTerrier Macdonald et al . (2021); Dhole (2024b). The base code used for conducting the experiments and computing the m etrics presented in the tables was obtained from the active RAG setup by Jiang et al. (2023). For uncertainty detection, we resort to