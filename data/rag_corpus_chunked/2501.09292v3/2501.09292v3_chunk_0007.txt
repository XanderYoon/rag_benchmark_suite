pair of responses contradict, entail, or are neutral to each other. 3 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 Uncertainty Estimator Trigger Retrieval When Retrieval Query #examples #search #steps f1 Always Retrieve U ≥ 0 Temporary Sentence 25 4.60 3.60 0.552 Always Retrieve Sub-Query 25 5.00 4.00 0.538 FLARE-Instruct “...[Search” 25 4.80 3.80 0.531 Degree Matrix Jaccard U > 0.4 Sub-Query 24 1.46 3.67 0.593 Eccentricity U > 2 Sub-Query 22 2.23 4.05 0.605 Semantic Sets U > 2 Sub-Query 23 2.52 4.09 0.411 Degree Matrix NLI U > 0.5 Sub-Query 24 2.25 4.00 0.535 Table 1: Performance Metrics over a smaller seed set Uncertainty Estimator Trigger Retrieval When #search #steps ret ratio correct incorrect f1 Always Retrieve Always 4.63 3.63 1.32 0.493 0.493 0.578 4.61 3.61 1.33 0.52 0.467 0.594 4.61 3.61 1.33 0.493 0.493 0.571 0.581 Degree Matrix Jaccard U > 0.4 1.80 3.61 0.57 0.453 0.533 0.538 1.92 3.60 0.61 0.44 0.547 0.525 1.85 3.61 0.57 0.419 0.568 0.508 0.524 Eccentricity U > 2 2.17 3.60 0.64 0.44 0.547 0.525 2.25 3.63 0.67 0.467 0.533 0.565 2.23 3.63 0.64 0.507 0.493 0.594 0.561 Table 2: Performance Metrics for Different Uncertainty Est imators for 75 examples. 4.3 S UBQUERY GENERATION FOR RETRIEVAL We resort to retrieving relevant knowledge to account for th e information that the model is lacking to answer the question. FLARE Jiang et al. (2023) generates a retrieval query for the missing en- tity in the temporary sentence by using the sentence with the low probability token removed or by prompting an external question generator to generate a ques tion for the missing entity as the answer. We generalize this by instead prompting the model to generat e a subquery to ﬁgure out the