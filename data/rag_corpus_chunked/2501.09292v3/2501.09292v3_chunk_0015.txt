to dense retrieval. In Proceedings of the 30th acm international conference on information & knowledge management , pp. 4526–4533, 2021. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigati ng effectiveness of paramet- ric and non-parametric memories. In Anna Rogers, Jordan Boy d-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association fo r Computa- tional Linguistics (V olume 1: Long Papers) , pp. 9802–9822, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653 /v1/2023.acl-long.546. URL https://aclanthology.org/2023.acl-long.546. OpenAI. Gpt-4 technical report, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wa inwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Tr aining language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730– 27744, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Y our language model is secretly a reward model. Advances in Neural Information Processing Systems , 36, 2024. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christop her Potts, and Matei Zaharia. Colbertv2: Effective and efﬁcient retrieval via lightweig ht late interaction. arXiv preprint arXiv:2112.01488, 2021. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu A wal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri` a Garr iga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabil ities of language models. Transactions on Machine Learning Research , 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Y onghui Wu, Je an-Baptiste Alayrac, Jiahui Y u, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et a l. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023. Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxi ang Wang,