experiments was GPT-3 (davinci-00 2) Brown et al. (2020), and the re- triever employed was BM25 through PyTerrier Macdonald et al . (2021); Dhole (2024b). The base code used for conducting the experiments and computing the m etrics presented in the tables was obtained from the active RAG setup by Jiang et al. (2023). For uncertainty detection, we resort to the Fadeeva et al. (2023)’s LM-Polygraph library. Since running GPT-3 (davinci-002) along with many of the unc ertainty detection metrics could be expensive to run (due to making multiple calls), we ﬁrst perf orm a run for a small seed set of 25 4 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 queries across all metrics and then choose the 3 best metrics for a rerun across a larger set of 75 examples. We perform each run three times. 6 R ESULTS We now present the results in Tables 1 and 2 for the smaller and the larger sets respectively. The baseline method where retrieval was always invoked yiel ded an F1 score of 0.552 when using temporary sentences as retrieval queries and 0.538 when subqueries were generated for retrieval but required most number of retrieval operations. Triggering retrieval, when uncertainty computed through Eccentricity i.e. U > 2, led to the high- est F1 score of 0.605, with a lesser number of search operations. This approach ba lanced retrieval efﬁciency and task performance better than other methods. I t required half the number of search operations than an Always Retrieve approach. Semantic Sets’ innovative clustering approach per- formed poorly, with an F1 score of 0.411. Using entailment-based similarity to compute uncertaint y via the Degree Matrix NLI measure achieved an F1 score of 0.535, comparable to the baseline. The lightweight