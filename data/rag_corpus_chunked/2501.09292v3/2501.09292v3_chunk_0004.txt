experiments on the 2WikiMultihopQA dataset Ho et al. (2020), a multi-hop open do- main question answering (QA) dataset that tests the reasoni ng and inference skills of question- answering models. Questions in this dataset generally requ ire two steps of reasoning to deduce the ﬁnal answer, and the information for each step of reasoni ng can be obtained through referencing external information viz., Wikipedia passages. 4 A PPROACH We now describe our uncertainty-aware, retrieval-augment ed generation in the following two sub- sections. 2 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 4.1 U NCERTAINTY EVALUATION OF FUTURE SENTENCE Given a query q, a retriever R, a text generator G, and a black box uncertainty estimation function U, and partially generated sequence t<i until time step i, – we ﬁrst generate a temporary sentence tn in the style of FLARE Jiang et al. (2023). We use a prompt template P, which could take the form of a zero-shot or a few-shot instru ction. This instruction takes as input the query, zero or more retri eved documents d1 . . . dk, and the answer tokens generated until now. Here, we use ti to represent the ith temporary sentence and y<i to represent all the initialised and generated sentences {0 . . .(i − 1)}. ti is ﬁrst obtained without performing retrieval: ti = G(P{q, . . . , yi−1}) (1) During generation, we evaluate the uncertainty of this temp orary sentence tn to gauge if the gen- erator needs more information. If the uncertainty U(tn) exceeds a threshold θU, the model is not certain and may lack the necessary knowledge to provide an ac curate answer. The next sentence yi is then computed by appending retrieved information to the m odel