on a 1.8M-clip subset from a large-scale multi- modal video dataset with comprehensive annotations. This represents one of the largest multimodal retrieval evaluation benchmarks to date, spanning 29,259 source videos across diverse content categories. The complete dataset descrip- tion and additional benchmark tasks will be detailed in con- current work [4]; here we focus on the multimodal retrieval evaluation that validates our routing approach. Each video clip contains rich multimodal annotations in- cluding speech transcripts, extracted scene text, and mul- tiple levels of visual descriptions. The annotation pipeline generates: • Speech Transcript: ASR-generated transcriptions of spoken content • Scene Text (VLM): OCR text extracted from video frames using vision-language models • Visual Caption: Detailed descriptions of visual content and scenes • Narrative Caption: High-level summaries of video pur- pose and context • Fused Caption: Comprehensive descriptions combining multiple modalities Example Video Clip Annotation: Figure 2 shows a rep- resentative video clip with its comprehensive multimodal annotations. This cooking tutorial demonstrates the rich in- formation available across modalities: visual kitchen scene with ingredients and cooking actions, spoken anniversary message, and on-screen recipe title overlay. Clip Details: • ID: -A9zM1jeNfk s0 e10 (Howto & Style category) • Visual: “The video clip shows a man in a white polo shirt standing in a kitchen. He is facing the camera and talk- ing. There is a large stainless steel stovetop behind him with a pot on one burner. There are two large windows on the left side of the frame, and a white cabinet with draw- ers on the right side of the frame. There are also some kitchen utensils hanging from the ceiling above the stove- top. The man is surrounded by various ingredients for a soup, including onions, celery, and garlic. The text ‘An- 3 Figure 2. Example video clip (-A9zM1jeNfk s0