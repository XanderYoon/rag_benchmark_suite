realistic query characteristics. Each query is annotated with its source modality to enable fine-grained analysis of rout- ing performance. Queries are systematically generated from different modality sources and manually validated for relevance and clarity. The annotation process labels each query with the modality that contains the relevant information: • ASR-derived: Queries targeting spoken content (e.g., “Who says ’I’m not going anywhere’ at the end?”) • Visual-derived: Queries about visual scenes (e.g., “De- scribe the color and shape of the vehicle passing by”) • OCR-derived: Queries about visible text (e.g., “2020 election protest sign ’Was the 2020 Election Stolen?”’) • Dense caption-derived: Queries from comprehensive multimodal descriptions The distribution reflects realistic usage patterns, with speech-related queries being most common, followed by visual and text-based information needs. This diversity en- sures comprehensive evaluation across different query types and modality requirements. 4.4. Evaluation Methodology Evaluation metrics include Recall@1, Recall@5, Re- call@10, Mean Reciprocal Rank (MRR), and NDCG at multiple cutoffs. Our NDCG computation employs graded relevance scoring that provides partial credit for temporally adjacent video segments: exact matches receive relevance 1.0, while clips from the same video within ±10 seconds receive relevance 0.5. This graded approach acknowledges content continuity in video data and provides more nuanced evaluation than binary relevance judgments. 4 Graded Relevance NDCG: Our NDCG computation uses a custom graded relevance function for video retrieval: rel(qi, cj) =    1.0 if cj = gold(qi) 0.5 if same video(cj, gold(qi)) ∧|tj − tgold| ≤ 10s 0.0 otherwise (2) where cj is a candidate clip, gold(qi) is the ground truth clip for query qi, and tj, tgold are temporal positions. NDCG is then computed as: NDCG@k = DCG@k IDCG@k = Pk i=1 2reli −1 log2(i+1) Pk i=1 2rel∗ i −1 log2(i+1) (3) where rel∗ i represents the ideal relevance ordering with