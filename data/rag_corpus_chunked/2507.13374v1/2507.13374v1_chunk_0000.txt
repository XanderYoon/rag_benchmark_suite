Smart Routing for Multimodal Video Retrieval: When to Search What Kevin Dela Rosa Cloudglue kdr@cloudglue.dev Abstract We introduce ModaRoute, an LLM-based intelligent rout- ing system that dynamically selects optimal modalities for multimodal video retrieval. While dense text captions can achieve 75.9% Recall@5, they require expensive offline pro- cessing and miss critical visual information present in 34% of clips with scene text not captured by ASR. By analyzing query intent and predicting information needs, ModaRoute reduces computational overhead by 41% while achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR (speech), OCR (text), and visual indices, averaging 1.78 modalities per query versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips demon- strates that intelligent routing provides a practical solution for scaling multimodal retrieval systems, reducing infras- tructure costs while maintaining competitive effectiveness for real-world deployment. 1. Introduction Video content has become increasingly central to infor- mation dissemination and consumption, spanning educa- tional materials, entertainment, news, and social media. As video repositories grow to massive scale, efficient retrieval systems that can understand and search multimodal con- tent—combining visual scenes, spoken dialogue, and on- screen text—have become essential infrastructure for con- tent platforms, educational systems, and digital libraries. Modern video platforms process millions of queries daily, where even small efficiency improvements translate to substantial infrastructure savings. The computational challenge is significant: a single multimodal query typi- cally requires searching across speech transcripts, visual descriptions, and scene text indices. However, analysis of real query patterns reveals that most information needs tar- get only 1-2 modalities, suggesting substantial optimization opportunities through intelligent routing. Recent advances in vision-language models enable rich textual descriptions of multimodal content, but critical lim- itations remain: (1) scene text is often missed or inaccu- rately described in visual captions, (2) generating compre- hensive dense captions requires expensive