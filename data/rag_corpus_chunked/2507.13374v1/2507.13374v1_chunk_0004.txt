rele- vance. Selected modalities are queried in parallel, and re- sults are combined through rank-based fusion to produce the final ranking. Figure 1. ModaRoute system architecture showing LLM-based routing decision. For the query “What does the chef say about sea- soning?”, the router correctly identifies speech content and routes only to the ASR index, avoiding unnecessary searches of OCR and Visual indices. 3.1. LLM-Based Router Implementation Our routing component uses GPT-4.1 with a carefully engi- neered prompt that performs dual functions: modality selec- tion and query optimization. The system analyzes queries for three modality types: ASR (spoken content), OCR (on- screen text), and VISUAL (purely visual scenes). Router Prompt Design: The routing prompt instructs the LLM to: (1) analyze user intent for each modality based on linguistic cues, (2) produce optimized queries for relevant modalities, and (3) respond in JSON format for program- matic processing. The prompt explicitly defines modality characteristics: • ASR: Queries about spoken content (“who says...”, “what word is spoken...”) • OCR: Queries about on-screen text (“what does this sign say...”, “read the subtitle...”) • VISUAL: Queries about visual scenes (“what color is...”, “what object is...”, “describe the gesture...”) While the system generates optimized queries for debug- ging and analysis, our evaluation focuses primarily on rout- ing decisions. Query optimization showed inconclusive re- sults and represents a promising direction for future work requiring specialized fine-tuning. Routing Examples from Real System: • Speech Query: “Who says ‘I’m not going anywhere’ at the end?” Decision: {"asr": "speaker says ’I’m not going anywhere’" } • Text Query: “What phrase appears on the protest sign?” Decision: {"ocr": "protest sign text" } • Visual Query: “Describe the color and shape of the vehicle” 2 Decision: {"visuals": "vehicle passing by, focus on color" } • Scene Query: “A man is walking down a