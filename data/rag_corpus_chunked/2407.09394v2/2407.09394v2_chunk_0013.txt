golden answers are converted to lowercase, and string matching (StringEM) is performed between each golden answer and the model prediction to calculate accuracy. To evaluate user-centric adaptability, the BLEU-2 score is mea- sured to assess the text similarity between different RAG and base- line setups and how well the generated answers resemble each other. This metric provides insights into the system’s ability to generate consistent and coherent responses across various configurations. Additionally, the average sentence length and the average number of syllables of the answers from different RAG setups are reported as a post-hoc analysis. These measures validate whether the RAG system effectively adjusts its responses based on user knowledge levels, ensuring that the generated answers are tailored to the user’s understanding and expertise. Combining these evaluation strategies provides a comprehensive view of both the effectiveness and user-centric adaptability of the RAG system. The accuracy metric ensures that the system generates correct answers, while the BLEU-2 score and post-hoc analysis of sentence length and syllable count confirm the system’s ability to adapt to user knowledge levels. As the understanding of user needs and system capabilities evolves, it is essential to continuously refine these metrics to maintain the RAG system’s effectiveness in delivering personalized, context-aware responses that cater to the diverse requirements of users in the field of IR. 4.4 Implementation Details For a fair comparison and following the work of Mallen et al. [23] and Trivedi et al . [33], the same retriever, a term-based sparse retrieval model known as BM25 [ 26], is used across all different models. The retrieval model is implemented using the OpenMatch toolkit [38]. For the external document corpus, the KILT-Wikipedia corpus preprocessed by Petroni et al . [25] is used, and the top-k relevant documents are retrieved. Regarding the LLMs used to generate answers,