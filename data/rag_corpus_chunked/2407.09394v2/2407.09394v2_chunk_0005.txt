we present the methodology underlying our Per- sonaRAG approach, which aims to enhance the ability of Language Large Models (LLMs) to actively engage with, understand, and lever- age user profile information for personalized content generation. We begin by discussing the fundamental concepts of Retrieval- Augmented Generation (RAG) models (Section 3.1) and then intro- duce our PersonaRAG technique, which encourages LLMs to ac- tively assimilate knowledge from live search sessions (Section 3.2). 3.1 Fundamentals of Retrieval-Augmented Generation (RAG) Models State-of-the-art RAG models, as described in previous studies [10, 13, 29], employ retrieval systems to identify a set of passages ğ·= {ğ‘‘1, . . . , ğ‘‘ğ‘› } when given a query q. These passages are intended PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents , Figure 2: Overview of Our PersonaRAG Model showcasing the dynamic interaction among specialized agents within the system, facilitated by a global message pool for structured communication. The diagram illustrates the flow from user query input through various agents, including User Profile, Context Retrieval, Session Analysis, Document Ranking, and Feedback Agents, highlighting their contributions to real-time adaptation and personalized content generation by integrating live user data and feedback for continuous improvement and contextually relevant search experiences. to enhance the generative capabilities of LLMs by providing them with contextually relevant information. Early versions of RAG models typically employ a traditional retrieval-generation framework, in which the retrieved data set ğ·={ğ‘‘ 1, . . . , ğ‘‘ğ‘› } is directly fed into LLMs to generate responses to the query ğ‘. However, these passages often contain irrelevant information, and the direct utilization approach in RAG has been shown to restrict the potential benefits of the RAG framework [9]. This limitation has sparked further discussion on how to improve LLMs by integrating retrieval results and outputs generated by the models themselves [36].