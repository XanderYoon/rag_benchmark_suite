with articles about famous art heists. Step 3: Cognitive Dynamic Adaptation The user has a consistent interest in famous art heists and historical mysteries. They are drawn to unsolved mysteries and historic crimes. They favor detailed, chronological accounts, so the final response should highlight theft motives, the method, and the investigation leading to the arrest, providing comprehensive and authoritative sources to satisfy their engagement with articles on art heists. Answer:Vincenzo Peruggia, a Louvre employee, stole the Mona Lisa from the Louvre Museum on August 21, 1911. He claimed he stole the painting to return it to Italy, and the theft remained unsolved for two years before he was arrested in Florence in December 1913. The painting was later recovered and returned to the Louvre. Table 4: PersonaRAG Case Study. the applicability of external information to user queries. The cog- nitive dynamic adaptation module refines initial chain-of-thought responses using these insights, generating accurate answers. For example, including knowledge about the "theft of the Mona Lisa in 1911, " "Vincenzo Peruggia, " and "Florence" enhances the reasoning process’s precision and detail. This demonstrates PersonaRAG’s effectiveness in helping IR agents combine external knowledge with intrinsic user data to produce well-informed responses. 6 Conclusion This paper proposes PersonaRAG, which constructs the retrieval- augmentation architecture incorporating user interaction analysis and cognitive dynamic adaptation. PersonaRAG builds the user interaction agents and dynamic cognitive mechanisms to facilitate the understanding of user needs and interests and enhance the system capabilities to deliver personalized, context-aware responses with the intrinsic cognition of LLMs. Furthermore, PersonaRAG demonstrates effectiveness in lever- aging external knowledge and adapting responses based on user profiles, knowledge levels, and information needs to support LLMs in generation tasks without fine-tuning. However, this approach requires multiple calls to the LLM’s API, which can introduce addi- tional time latency and