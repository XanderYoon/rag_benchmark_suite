(except for Chain-of-Thought using Top-5, which performed equally). On the NQ dataset, Per- sonaRAG maintained similarly robust performance with scores of 49.02% and 48.78%, outperforming all baselines (except for Chain- of-Thought and Self-Rerank (SR) using Top-5). This pattern was further validated by experiments on other datasets, with results showing that PersonaRAG consistently outperforms conventional RAG models with the capability of providing an answer tailored to the user’s interaction and information need. The comprehensive understanding it provides contributes to the generation of accurate and user-centric answers across various question complexities. 5.2 Comparative Analysis of RAG Configurations Further experiments explored PersonaRAG’s adaptive capabilities (Figure 3). BLEU-2 scores compared outputs from Chain-of-Note (consistently best outside PersonaRAG) with other methods. Per- sonaRAG showed higher similarity scores, indicating its ability to generate responses that address user needs rather than just sum- marizing input. Additionally, PersonaRAG provides personalized answers tailored to user profiles, extending beyond mere informa- tion provision. The Chain-of-Note approach demonstrated comparable perfor- mance to the Chain-of-Thought approach, implying that both tech- niques effectively extract pertinent information from the retrieved passages and adapt it to align with the user’s information need. In contrast, vanillaGPT and vanillaRAG outputs differed signifi- cantly from the Chain-of-Note approach, indicating that counterfac- tual cognition often leads to diverse outcomes rather than focusing solely on query-relevant content. This suggests LLMs can construct knowledge from multiple perspectives and customize responses based on user understanding. Post-hoc analyses of average sentence length and syllable count across RAG configurations provided insights into the system’s abil- ity to adapt responses to user comprehension levels. These obser- vations highlight PersonaRAG’s capacity to synthesize knowledge from various perspectives and tailor responses to different levels of user expertise. 5.3 Analysis on Generalization Ability This experiment evaluates the quality of knowledge construction using different large language models (LLMs). As