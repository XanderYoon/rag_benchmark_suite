of Information Retrieval (IR): Natu- ralQuestions (NQ) [19], TriviaQA [17], and WebQuestions (WebQ) [6]. NQ is a well-known dataset in Natural Language Understand- ing (NLU), consisting of structured questions and corresponding Wikipedia pages annotated with long and short answers. TriviaQA comprises question-answer pairs collected from trivia and quiz- league websites, while WebQ consists of questions selected using the Google Suggest API, with answers being entities in Freebase. PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents , Table 1 summarizes the datasets used in our initial study. Due to the high cost of using language models and the large number of API calls required, we randomly sampled 500 questions from each raw dataset to create more manageable subsets for our ex- periments. While this sampling approach limits the scope of our study, it allows us to conduct an initial investigation into the perfor- mance of different RAG systems on these datasets. We acknowledge that future work with larger sample sizes and more comprehen- sive experiments will be necessary to draw definitive conclusions. Nonetheless, we believe this preliminary study provides valuable insights into the relative strengths and weaknesses of the tested RAG approaches. Dataset #Query #Corpus Sampling Rate NQ 8,757 79,168 5.7% TriviaQA 8,837 78,785 5.7% WebQ 2,032 3,417 24.6% Table 1: Summary of datasets. Each dataset consists of ran- domly sampled 500 questions from the raw dataset. 4.2 Models We compare PersonaRAG with several baseline models, including prompt learning and RAG models. The prompt templates used in user interaction analysis and dynamic adaptation are presented in Section 4.4. Initially, the question-answering (QA) instruction is fed to ChatGPT to conduct the vanilla answer generation model. Following the work of Wei et al. [34], the Chain-of-Thought model is implemented, which generates question rationale results to pro- duce the final results. Additionally, the Guideline