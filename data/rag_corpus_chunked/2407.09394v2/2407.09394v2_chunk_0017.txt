behaviors, search queries, and user feedback. This structure should provide all agents with meaningful data points and strategic recommendations, reducing redundant communication and improving the system’s overall efficiency. 4.5.2 Prompts Used in Cognitive Dynamic Adaptation. Chain-of-Thought. To solve the problem, Please think and reason step by step, then answer. PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents , Question:{question} Passages:{passages} Reasoning process: 1. Read the given question and passages to gather relevant information. 2. Write reading notes summarizing the key points from these passages. 3. Discuss the relevance of the given question and passages. 4. If some passages are relevant to the given question, provide a brief answer based on the passages. 5. If no passage is relevant, directly provide the answer without considering the passages. Answer: Cognitive Agent. Your task is to help the Cognitive Agent enhance its understanding of user insights to continuously improve the system’s responses. Question:{question} Initial Response:{cot_answer} User Insights from Interaction Analysis: User Profile Agent: {user_profile_answer}, Contextual Retrieval Agent: {contextual_answer}, Live Session Agent: {live_session_answer}, Document Ranking Agent: {document_ranking_answer}, Feedback Agent: {feedback_answer} Task Description: Verify the reasoning process in the initial response for errors or misalignments. Use insights from user interaction analysis to refine this response, correcting any inaccuracies and enhancing the query answers based on user profile. Ensure that your refined response aligns more closely with the user’s immediate needs and incorporates foundational or advanced knowledge from other sources. Answer: 5 Experimental Results and Analyses In this section, we show the overall experimental results and offer in-depth analyses of our method. 5.1 Main Results Table 2 summarizes the primary findings for PersonaRAG across various single-hop question answering datasets. The approach was evaluated against multiple baseline models, including large language models (LLMs) without retrieval-augmented generation (RAG), the conventional RAG model, and self-refined variants, such as utilizing