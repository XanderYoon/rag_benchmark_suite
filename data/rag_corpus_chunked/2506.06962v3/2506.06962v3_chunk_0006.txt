such as LlamaGen [37], Show-o [44] and V AR [38]. Quantized Autoencoder The quantized autoencoder used in Janus-Pro consists of an encoder θenc, a decoder θdec, and a codebook Z. The encoder, a convolutional neural network, downsamples and compresses raw pixel inputs into compact patch representations. During the quantization process, each patch representation is mapped to an index in the codebook by identifying its nearest neighbor vector in the codebook. In the decoding stage, these patch indices are mapped back to their corresponding vector representations via the codebook, and the decoder, another convolutional neural network, reconstructs the image from these compact representations. In our implementation, we leverage this autoencoder to build the coupled database for Janus-pro which is detailed in Section 3.1. 3 AR-RAG: Patch-based Autoregressive Retrieval Augmentation 3.1 Patch-based Retrieval Database Construction We build a patch-based retrieval database based on several large-scale, real-world image datasets, including CC12M [5] and JourneyDB [ 36]. Specifically, for each image I, we encode it into N 2https://huggingface.co/datasets/playgroundai/MJHQ-30K 3 Patch-based Retrieval h -hop surrounding patches AR Model D model Distribution-Augmentation in Decoding (DAiD) Generated Next Image Patches D merge ？ Generated Patches D Retrieval Figure 2: The decoding process in Distribution-Augmentation in Decoding (DAiD). patches using the quantized autoencoder [37], θEnc, from Janus-Pro: V = θenc(I) ∈ R √ N × √ N ×d, where d is the hidden dimension, and Vij corresponds to the latent representation of the patch at position (i, j). We utilize each patch vectorVij as the value of a database entry and the representation of its h-hop surrounding patches as the key. Here, the h-hop surrounding patch representation is formed by concatenating the vectors of adjacent patches centering around (i, j) in a top-to-bottom, left-to-right order. For example, for a patch at position (i, j), the 1-hop surrounding representation