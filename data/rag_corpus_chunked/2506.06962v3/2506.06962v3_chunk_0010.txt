where L denotes the total number of decoder layers and b is a hyperparameter. Multi-Scale Feature Smoothing The key of effective patch integration lies in ensuring spatial coherence between retrieved patches and the surrounding image context. To achieve this, we propose multi-scale feature smoothing (Algorithm 1 in Appendix A), where multi-scale convolutions are applied to retrieved patches within the generation context, so that the retrieved visual features are smoothed to preserve structural and stylistic consistency with the surrounding context of the predicted token. Specifically, at each step when predicting the next image token vij, we first construct a 2D spatial representation Hl ∈ R √ N × √ N ×D of the current partially-generated image by arranging their hidden states [hl 1, hl 2, ...,hl n−1, hl n] from the current decoder layer l. We use 0 vectors as placeholders for positions that have not yet been generated. Then, we transform the retrieved patch representations [ˆv1, ˆv2, ..., ˆvK] into the generation model’s hidden space by mapping each patch ˆvk to a discrete token index via the codebook Z and embedding it through the pretrained image embedding layer Embimg: [ˆh1, ˆh2, ..., ˆhK] = Embimg([Z(ˆv1), Z(ˆv2), ..., Z(ˆvK)]) (5) For each retrieved patch ˆhk, we create a copy of Hl where position (i, j) (the location of vij) is replaced with ˆhk. We then apply convolution operations at multiple scales (2 × 2 through Q × Q) to capture contextual patterns at different resolutions. To maintain computational efficiency, we only perform convolution operations when the kernel covers position (i, j), rather than processing the entire image. Each convolution kernel Convq×q produces a refined representation ˆhq k for the retrieved patch at scale q. The final refined representation for each retrieved patch is computed as a weighted sum of these multi-scale