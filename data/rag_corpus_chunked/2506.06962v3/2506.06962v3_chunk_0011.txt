patterns at different resolutions. To maintain computational efficiency, we only perform convolution operations when the kernel covers position (i, j), rather than processing the entire image. Each convolution kernel Convq×q produces a refined representation ˆhq k for the retrieved patch at scale q. The final refined representation for each retrieved patch is computed as a weighted sum of these multi-scale features: ˆhk ← QX q=2 softmax(Ω)q · ˆhq k (6) where Ω = [ω2, ..., ωQ] are learnable parameters that determine the importance of each scale. 5 Feature Augmentation After feature smoothing, some of the retrieved patch features may still not be able to fit into the surrounding neighbors and hence we need to lower their impact in the final repre- sentation. Thus, we compute a compatibility score for each of the refined patches. This is achieved by projecting each refined retrieved patch representation through a linear transformation parameterized by a weight matrix W ∈ R1×D, yielding the score sk = ˆhkWT . The final representation for the next image token vij after layer j is computed as: h(l+1) ij = hl ij + ∆hl ij + KX k=1 skˆhk (7) Here, hl ij is the residual, ∆hl ij is the updated representation from the transformer layer l, andPK k=1 skˆhk is the contribution of the retrieved image patches. 4 Experiment Setup Patch-based Retrieval Database To construct our patch-level retrieval database, we randomly sample 5.7 million images from CC12M [5], 3.3 million from JourneyDB [36], and 4.6 million from DataComp [12], while ensuring that any samples included in the testing set are excluded to prevent data leakage. Each image is encoded into a sequence of patch-level representations and image tokens using the same image tokenizer employed in the Janus-Pro model. For efficient similarity search, we implement our retriever using the