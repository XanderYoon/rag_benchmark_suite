position (i, j) in the partially generated image, we extract the eight-neighborhood representation as the retrieval query and obtain the top- K most similar patches [ˆv(i,j) 1 , ˆv(i,j) 2 , ..., ˆv(i,j) K ] from our database. We then construct position-specific retrieval distributions D(i,j) retrieval ∈ R|Z| using the same softmax formulation over retrieval distances as described in the main paper. These retrieval distributions are merged with Show-o’s predicted distributions for each patch position using the weighted average D(i,j) merge = (1 − λ) · D(i,j) model + λ · D(i,j) retrieval, where λ controls the retrieval influence across all positions. FAiD on Show-o The adaptation of FAiD to Show-o involves both training and inference modifica- tions to accommodate the model’s masked token generation process. During training, we prepare the training dataset by applying Show-o’s noise injection process to generate intermediate noisy representations at each time step, which serve as ground truth targets for the denoising process. For each training instance, we save these intermediate representations and apply patch-level retrieval to obtain relevant patches for all time steps. The training objective remains consistent with the standard Show-o formulation, but with augmented input representations that incorporate retrieved patch information. We insert FAiD modules into every L/b decoder layers of Show-o’sΦ model, where each module processes all patch positions simultaneously rather than focusing on a single next token. At each qualifying time step and for each FAiD-equipped layer l, we construct the 2D spatial representation Hl ∈ R √ N × √ N ×D from the current hidden states and perform multi-scale feature smoothing for all patch positions. For each position (i, j) and its corresponding retrieved patches 15 [ˆh(i,j) 1 , ˆh(i,j) 2 , ..., ˆh(i,j) K ], we apply the convolution operations {Conv2×2, Conv3×3, ..., ConvQ×Q} to capture contextual