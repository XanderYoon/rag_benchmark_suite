the 2D spatial representation Hl ∈ R √ N × √ N ×D from the current hidden states and perform multi-scale feature smoothing for all patch positions. For each position (i, j) and its corresponding retrieved patches 15 [ˆh(i,j) 1 , ˆh(i,j) 2 , ..., ˆh(i,j) K ], we apply the convolution operations {Conv2×2, Conv3×3, ..., ConvQ×Q} to capture contextual patterns at multiple scales. The refined representations are computed as ˆh(i,j) k ←PQ q=2 softmax(Ω)q · ˆh(i,j) k,q , where ˆh(i,j) k,q represents the output of the q × q convolution for patch k at position (i, j). The final augmented representation for each position is calculated as h(l+1) ij = hl ij + ∆hl ij +PK k=1 s(i,j) k ˆh(i,j) k , where ∆hl ij represents the standard transformer layer updates including self-attention and feed-forward components, and s(i,j) k are position-specific com- patibility scores computed through learned linear projections. During inference, we follow the same procedure but apply retrieval and feature blending only during the final half of the generation time steps to ensure sufficient contextual information is available for effective patch integration. B.3 Training Setup Training Datasets For model training, we utilize two large-scale image-caption datasets: CC12M [5] and Midjourney-v6 4. From the training sets of these datasets, we randomly sam- ple a total of 50, 000 image-caption pairs (25, 000 from each dataset) to fine-tune our model. Each image is encoded into 576 patch features and corresponding image tokens with the same image tokenizer [37] employed in the Janus-Pro model. For each image patch, we further retrieve the top-K image tokens from our retrieval database that exhibit similar neighborhood relationships. Conse- quently, each training instance comprises: (1) a textual image caption that serves as the conditioning input, (2) a sequence of 576 image tokens representing the ground-truth