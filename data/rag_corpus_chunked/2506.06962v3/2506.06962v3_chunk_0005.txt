distribution of retrieved patches into that predicted by the image generation models, enabling easy integration into existing architectures. • We introduce Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning frame- work that progressively refines and blends retrieval signals via lightweight convolutional modules, enhancing spatial coherence and visual quality across layers. • Extensive experiments and analysis show that AR-RAG significantly improves performance of state-of-the-art image generation model across diverse metrics. In particular, Janus-Pro with FAiD achieves 6.67 FID on Midjourney-30K and 0.78 overall score on GenEval, establishing a new state of the art among autoregressive image generation models of comparable scale. 2 Preliminary Autoregressive Image Generation Models We implement both DAiD and FAiD based on Janus- Pro [9], an autoregressive (AR) unified generation model, due to its strong performance. Janus-Pro is initialized from a transformer-based pre-trained large-language model [2], and employs a quantized autoencoder [37] to encode images into discrete image tokens. During multimodal pretraining, the model learns to predict a sequence of discrete image tokens [v1, v2, ...vN] conditioned on an input text prompt [t1, t2, ...tM]. The training objective is formally defined as: arg max ϕ DX NX n=1 Pϕ(vn|t1, t2, ..., tM , v1, ...vn−1) (1) where D is the training corpus. This is the same training objective used in our FAiD method in Section 3.3. We argue that DAiD and FAiD can be extended to any image generation model that autoregressively predicts probability distributions of discrete image tokens such as LlamaGen [37], Show-o [44] and V AR [38]. Quantized Autoencoder The quantized autoencoder used in Janus-Pro consists of an encoder θenc, a decoder θdec, and a codebook Z. The encoder, a convolutional neural network, downsamples and compresses raw pixel inputs into compact patch representations. During the quantization process, each patch representation is mapped to an index in the