Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. CoRR, abs/2309.02591, 2023. [49] Huaying Yuan, Ziliang Zhao, Shuting Wang, Shitao Xiao, Minheng Ni, Zheng Liu, and Zhicheng Dou. FineRAG: Fine-grained retrieval-augmented text-to-image generation. In Owen Ram- bow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert, editors, Proceedings of the 31st International Conference on Computational Lin- guistics, pages 11196–11205, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. 13 A Multi-Scale Feature Smoothing Algorithm Algorithm 1: Multi-Scale Feature Smoothing Input: Image Representations Hl ∈ R √ N × √ N ×D, Retrieved Patch Representations [ˆh1, ˆh2, . . . , ˆhK ], Next Patch Index (i, j) Output: Updated hidden states [ˆh1, ˆh2, . . . , ˆhK ] 1 foreach ˆhi ∈ [ˆh1, . . . , ˆhK ] do 2 for q = 2 to Q do 3 Initialize tensor: M ← 0 ∈ RQ×Q×D; 4 Initialize tensor: ˆhq ← 0 ∈ RD; 5 for m = q down to 1 do 6 for n = q down to 1 do 7 Hl loc ← Hl[i − m : i + q − m, j − n : j + q − n]; 8 Mmn ← Conv1 q×q(Hl loc); 9 ˆhq += Conv2 q×q(M); 10 ˆhi ← ˆhq Q−1 ; Algorithm A illustrates the multi-scale fea- ture smoothing, which is the core computa- tional procedure for refining retrieved patch representations within their generation con- text. This algorithm ensures that retrieved visual elements are spatially and stylisti- cally coherent with the surrounding image content through systematic