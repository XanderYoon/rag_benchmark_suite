we propose two parallel frameworks: (1) Distribution-Augmentation in Decoding (DAiD), a training-free plug-and-use decoding strategy that directly merges the distribu- tion of model-predicted patches with the distribution of retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning method that progressively smooths the features of retrieved patches via multi-scale convolution operations and leverages them to augment the image generation pro- cess. We validate the effectiveness of AR-RAG on widely adopted benchmarks, 1Jingyuan Qi and Zhiyang Xu contributed equally to this work. Preprint. Under review. including Midjourney-30K, GenEval and DPG-Bench, demonstrating significant performance gains over state-of-the-art image generation models.1 1 Introduction Recent advancements in image generation have demonstrated remarkable capabilities in producing photorealistic images based on user prompts [ 31, 28, 7, 37, 10, 41, 9, 43, 45, 27, 6]. However, despite these improvements, the generated images often exhibit local distortions and inconsistencies, particularly in visual objects that possess complex structures [ 11], frequently interact with other objects and the surrounding scene [ 22, 26], or are underrepresented in the training data [ 8]. A promising approach to mitigating these challenges is retrieval-augmented generation (RAG), which enhances the generation process by incorporating real-world images as additional references [8, 3]. While RAG has been extensively explored in the language domain [23, 13], its application to image and multimodal generation remains largely underdeveloped. A few existing studies [3, 8, 46, 48, 49] bridge this gap by performing a single-step retrieval based on the input prompt prior to generation, conditioning the entire image generation process on fixed visual cues (Figure 1 (b)). However, as demonstrated in our pilot study (Section 5.2), such static, coarse-grained retrieval approaches [3, 8, 49] frequently introduce irrelevant or weakly aligned visual contents that persist throughout generation. Since the retrieved images are selected once, before decoding begins, and remain