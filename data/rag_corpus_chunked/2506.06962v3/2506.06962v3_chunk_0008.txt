similar patch representations from the database constructed in Section 3.1 using l2 distance. We denote the representations of the top-K retrieved patches as [ˆv1, ˆv2, ..., ˆvK] and their corresponding l2 distances as [s1, s2, ..., sK]. These retrieved representations are then mapped back to discrete token indices using the codebook: ˆvk = Z(ˆvk). To augment the generation process with the retrieved image tokens [ˆv1, ˆv2, ..., ˆvK], we create a retrieval-based distribution Dretrieval ∈ R|Z| over the entire codebook Z, where |Z| is the codebook size. Tokens not included in the top-K retrieved set are assigned a probability of 0. For tokens within the top-K, we compute their probabilities using a softmax over their l2 distance to the query, scaled by a retrieval temperature hyperparameter τ: Dretrieval[v] = p(ˆvk) if v = ˆvk for some m ∈ {1, 2, ..., K} 0 otherwise (2) p(ˆvk) = exp(−sk/τ)PK k=1 exp(−sk/τ) , (3) This creates a sparse distribution where only the top-K retrieved tokens have non-zero probabilities. Finally, we merge this retrieval distribution with the model’s predicted distributionDmodel using a weighted average: Dmerge = (1 − λ) · Dmodel + λ · Dretrieval, (4) where λ ∈ [0, 1] is the retrieval weight hyperparameter controlling the influence of retrieved patches on the final distribution. The next token is then sampled from this merged distribution: vij ∼ Dmerge. 3.3 Feature-Augmentation in Decoding (FAiD) While DAiD offers a training free approach to directly augment the probability distribution of predicted patches using retrieved ones, it suffers from noise propagation and limited flexibility in 4 ？ Retrieval Database ... Patch-based Image Retriever Generated Patches Convolutional Layer MLP Layer Feature-Augmentation in Decoding (FAiD) Decoder Layer RMS Norm Self-Attn RMS Norm FFD RMS Norm Previous Patch Embeddings Retrieval Embeddings Janus-Pro SPB ... 1 2 3 4 5