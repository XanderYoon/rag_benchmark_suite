generation of the next target patch (gray boxes in Figure 1(c)), AR-RAG retrieves the top-K most relevant patches (blue boxes) by measuring similarity between the surrounding generated context patches (red boxes) and database keys (also red boxes). These retrieved patches are then integrated into the model to inform and enhance the prediction of the next patch, enabling the model to dynamically adjust to local generation needs. By conditioning on the evolving generation context as retrieval queries, AR-RAG ensures that retrieved visual references remain relevant throughout the generation process, encouraging local semantic coherence. Moreover, the patch-level retrieval allows for precise integration of visual elements without overcommitting to entire reference images, avoiding the limitations of over-copying or irrelevant conditioning observed in static retrieval. To realize the AR-RAG framework, we introduce two parallel implementations: (1) Distribution- Augmentation in Decoding (DAiD), a training-free, plug-and-play decoding strategy that merges the model’s predicted patch distribution with that of the retrieved patches. Specifically, the top-K retrieved patches are assigned probabilities inversely proportional to their normalizedℓ2 distances computed from the query and key patch embeddings. These probabilities are then linearly combined with the model’s native output distribution to guide the next patch prediction, enabling retrieval-aware generation without any additional training. (2) Feature-Augmentation in Decoding (FAiD) , a parameter-efficient fine-tuning approach that integrates retrieved patches into the generation process through learned smoothing and blending mechanisms. Specifically, when generating the next image token, FAiD operates in two stages: (1) refining the retrieved patch features by adjusting them to better fit the local context of the already generated surrounding patches, based on parameterized convolutional operations of varying kernel sizes; and (2) blending the refined features of retrieved patches with the model’s predicted feature representation for the next patch, based on compatibility 1Code and model checkpoints can be found at https://github.com/PLUM-Lab/AR-RAG. 2