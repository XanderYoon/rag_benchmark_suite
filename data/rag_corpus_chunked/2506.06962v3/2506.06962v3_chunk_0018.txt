illustrated in Figure 5 (a), when generating an image of an apple, image-level retrieval approaches retrieve a reference image showing an apple on a tree branch and subsequently incorporate both the apple and the surrounding branches, despite the prompt making no mention of them. Similarly, for the prompt “a green cup and a yellow bowl ” in Figure 5 (b), the image-level retrieval augmentation approach retrieves a green Starbucks cup and reproduces the pattern on the cup in the generated image, 8 despite this element not being part of the original instruction. This overcopying behavior directly compromises the instruction-following capability of generative models. Figure 5 (c) demonstrates that when prompted to generate “A photo of a white dog and a blue potted plant,” image-level retrieval methods produce an image containing only the white dog, omitting the blue potted plant entirely. Similarly, for “a photo of a green couch and an orange umbrella” in Figure 5 (d), the generated image fails to include the umbrella. This degradation in instruction following occurs because image-level retrieval biases the generation process toward the compositional structure of retrieved reference images, which may not align with the multi-object relationships specified in the prompt. In contrast, by autoregressively retrieving and integrating visual information at the fine-grained patch level rather than the image level, AR-RAG enables selective incorporation of relevant visual elements while maintaining independence from irrelevant contextual features present in the reference images. 5.3 Inference Time Cost Single GPU (L40)Model Total (s) A verage (s) ImageRAG 879.64 8.80 Janus-Pro 457.74 4.58 + DAiD 459.34 4.59 (+0.22%) + FAiD 623.01 6.23 (+36.03%) Table 4: Inference time for generat- ing 100 images on a single L40 card. Table 4 shows the inference time comparisons across different models when generating 100 images using both a single L40 GPU. The DAiD