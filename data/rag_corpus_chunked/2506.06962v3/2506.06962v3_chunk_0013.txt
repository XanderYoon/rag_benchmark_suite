employ three benchmarks: (1) GenEval [14], which assesses models’ ability to generate images with specific attributes and relationships described in text prompts; (2) DPG-Bench [18], which evaluates performance on detailed prompts with complex requirements; and (3) Midjourney-30k [40], where we employ three complementary metrics: FID [17] for measuring statistical similarity between generated and real image distributions, CMMD [20] for assessing alignment with human perception using CLIP embeddings, and FWD [39] for evaluating spatial and frequency coherence through wavelet packet coefficients. For all three metrics, lower scores indicate higher quality generated images. Detailed descriptions of these benchmarks and metrics can be found in Appendix B.4. 5 Results and Discussion 5.1 Text-to-Image Generation Results Tables 1, 2, and 3 present performance comparisons across multiple benchmarks, where our AR- RAG methods consistently outperform existing approaches. Notably, previous retrieval-augmented approaches such as RDM and ImageRAG perform worse than their non-retrieval counterparts (LDM and SDXL, respectively) on both GenEval and DPG-Bench. We provide detailed analysis for existing image-level retrieval methods and highlight the unique advantages of ourAR-RAG frameworks in the following discussion and Section 5.2. Appendix C.1 provides a benchmark analysis to demonstrate the effectiveness of patch-level retrieval in our AR-RAG methods. 3https://huggingface.co/datasets/brivangl/midjourney-v6-llava 6 Method Params Single Obj.Two Obj.CountingColors PositionColor Attri.Overall↑ Non Retrieval-Augmented Model PixArt-α 0.6B 0.98 0.50 0.44 0.80 0.08 0.07 0.48 LlamaGen 0.8B 0.71 0.34 0.21 0.58 0.07 0.04 0.32 SDv1.5 0.9B 0.97 0.38 0.35 0.76 0.04 0.06 0.43 SDv2.1 0.9B 0.98 0.51 0.44 0.85 0.07 0.17 0.50 Janus-Pro 1.0B 0.98 0.77 0.52 0.84 0.61 0.55 0.71 Show-o 1.3B 0.98 0.80 0.66 0.84 0.31 0.50 0.68 LDM 1.4B 0.92 0.29 0.23 0.7 0.02 0.05 0.37 SD3 (d=24) 2.0B 0.98 0.74 0.63 0.67 0.34 0.36 0.62 SDXL 2.6B 0.98 0.74 0.39 0.85 0.15 0.23 0.55 DALL-E 2 6.5B 0.94 0.66 0.49 0.77 0.10 0.19