Question Answering Datasets PaQa AT-CoT 23.59 7.07 22.93 85.97 – – Q-Cond 42.46 16.62 41.58 90.12 – – QP-Zeroshot 33.79 10.42 35.84 88.66 – – RACSFT(ours) 46.83 † 20.17† 47.97† 90.85† 43.36 27.62 +RACDPO(ours) 45.26† 18.32† 46.40† 90.41† 45.75 28.54 CAmbigNQ AT-CoT 10.33 2.10 8.53 84.02 – – Q-Cond 28.41 8.90 33.06 87.17 – – QP-Zeroshot 18.20 4.27 19.48 85.15 – – RACSFT(ours) 36.66 † 14.81† 43.37† 88.93† 47.62 87.99 +RACDPO(ours) 35.47† 14.40† 41.99† 88.89† 49.95 88.05 These findings highlight both the benefit of passage conditioning and the added value of preference-based optimization. We further validate these results through qualitative analysis and LLM-based judgments in subsequent experi- ments. 5.2 LLM-based Evaluation To further addressRQ2, we evaluate the faithfulness of our approach using GPT-4 as a evaluator, comparingRACDPO againstRAC SFT. Results are shown in Table 2. Across all datasets,RACDPO achieves higher win rates compared to RACSFT, in some cases by more than a factor of two, whereas a large fraction of outputs are judged as ties. These results suggest that supervised fine-tuning already provides a strong baseline, preference optimization yield further gains on harder cases, reinforcingRQ3by enhancing faithfulness beyond supervised training. 5.3 Impact of the Number of Input Passages We next examine the impact of the number and quality of retrieved passages on RAC. Because RAC relies on retrieval to expose potential ambiguities, both the quantity and relevance of the input passages directly affect its ability to generate effective clarifications. 10 A-R Kebir et al. Table 2: GPT-4 preference results comparingRACDPO andRAC SFT. Results with*are statistically significantly different based on the one-sided McNemar’s test withp < 0.05. DatasetRAC DPO% Tie% RAC SFT% Qulac28.88 * 50.56 20.56 ClariQ28.36 * 48.79 22.85 PaQa36.2430.36 33.40 CAMbigNQ16.27 * 72.23 11.50 As shown in Fig. 3, performance improves as the number of passages in- creases, but