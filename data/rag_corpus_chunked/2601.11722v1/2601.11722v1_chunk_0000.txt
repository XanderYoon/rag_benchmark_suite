RAC: Retrieval-Augmented Clarification for Faithful Conversational Search Ahmed Rayane Kebir1,2[0009−0009−2512−832X], Vincent Guigue3[0000−0002−1450−5566], Lynda Said Lhadj2[0009−0005−3850−9229], and Laure Soulier1[0000−0001−9827−7400] 1 Sorbonne Université, CNRS, ISIR, F-75005 Paris, France 2 Ecole nationale Supérieure d’Informatique (ESI), Algeria 3 AgroParisTech, UMR MIA-PS, Palaiseau, France Abstract.Clarificationquestionshelpconversationalsearchsystemsre- solve ambiguous or underspecified user queries. While prior work has fo- cused on fluency and alignment with user intent, especially through facet extraction, much less attention has been paid to grounding clarifications in the underlying corpus. Without such grounding, systems risk asking questions that cannot be answered from the available documents. We introduce RAC (Retrieval-AugmentedClarification), a framework for generating corpus-faithful clarification questions. After comparing sev- eral indexing strategies for retrieval, we fine-tune a large language model to make optimal use of research context and to encourage the genera- tion of evidence-based question. We then apply contrastive preference optimization to favor questions supported by retrieved passages over un- grounded alternatives. Evaluated on four benchmarks, RAC demonstrate significant improvements over baselines. In addition to LLM-as-Judge as- sessments, we introduce novel metrics derived from NLI and data-to-text to assess how well questions are anchored in the context, and we demon- strate that our approach consistently enhances faithfulness. Keywords:Conversational Search·Clarifying Questions·RAG. 1 Introduction In open-domain information-seeking tasks, user queries are often short, am- biguous, or under-specified. Such characteristics make it difficult for traditional search systems to accurately capture user intent, as they typically provide only a ranked list of documents or passages without engaging in clarifying interac- tions [22]. Recent work has explored generating clarifying questions that are relevant, diverse, and human-plausible [8,28,30]. However, little attention has been given to whether these questions are grounded in the document corpus, even though unsupported clarifications may mislead users and harm retrieval effectiveness [10,18]. This is the author’s version of the work. It is posted here for