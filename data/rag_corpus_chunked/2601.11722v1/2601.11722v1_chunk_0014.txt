A-R Kebir et al. Table 2: GPT-4 preference results comparingRACDPO andRAC SFT. Results with*are statistically significantly different based on the one-sided McNemar’s test withp < 0.05. DatasetRAC DPO% Tie% RAC SFT% Qulac28.88 * 50.56 20.56 ClariQ28.36 * 48.79 22.85 PaQa36.2430.36 33.40 CAMbigNQ16.27 * 72.23 11.50 As shown in Fig. 3, performance improves as the number of passages in- creases, but the effect saturates after approximately four passages, suggesting that the most salient query-related ambiguities are typically captured within the top-ranked results. Passage quality is equally important: using random pas- sages results in performance close to the”Q-Cond”baseline, whereas BM25 and dense retrievers achieve substantially higher scores. BM25’s advantage is likely due to a domain mismatch, since the dense retriever is trained on MS MARCO, whose passage structure and content differ from the chunked ClueWeb passages used in our setting. These findings indicate that RAC benefits from informative retrieval signals and can extract relevant facets from high-quality passages rather than relying on arbitrary content, thereby addressingRQ1. 5.4 Impact of the Quality of Noisy Generated Elements We study the effect of our noisy generation method by comparingpuncond, fine- tuned with only the query as input, withpLM, the initial language model. We then measure their impact on preference tuning (positive samples always being generated byp θ0). Table 3 shows thatpuncond provides more effective negative samples thanpLM. Unlike the approach of Duong et al. [6], which relies on generic 0 1 2 3 4 5 Number of Passages 20 25 30 35 40 45 50Score (%) [ROUGE / BLEU / METEOR] ROUGE-L BLEU METEOR BERTScore (F1) 90.4 90.6 90.8 91.0 91.2 91.4 Score (%) [BERTScore] ROUGE BLEU METEOR BERTScore Evaluation Metric 0 5 10 15 20 25 30 35Score (ROUGE / BLEU / METEOR) 27.82 10.32 24.23 27.88 10.39 25.56 33.11 12.63 30.32 36.25 14.88