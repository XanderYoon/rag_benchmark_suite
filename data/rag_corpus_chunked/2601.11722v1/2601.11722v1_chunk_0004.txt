examined the role of the retriever in selecting corpus-grounded clarifications among candidate suggestions [18], whereas more recent work has shifted the focus toward generation [10], with particular atten- tion to maximizing faithfulness during inference. In addition, [26] demonstrates that the RAG paradigm can be combined with knowledge bases to enhance dis- ambiguation in domain-specific applications. However, these approaches rely on a zero-shot paradigm, whereas we demonstrate the benefit of fine-tuning the generator to better exploit the retrieved passages. Preference Tuning.Reinforcement learning from human feedback was intro- duced to align LLMs with human preferences [19], but reward-model methods were costly and often unstable. More recent techniques such as direct preference optimization (DPO) [23] and extensions [32] have improved efficiency by learning directly from pairwise comparisons. Beyond general alignment, generating both faithful and unfaithful baseline sentences allows contrastive learning algorithms 4 A-R Kebir et al. to be effectively applied for improving text generation. Such approaches have demonstrated strong performance in tasks such as automatic summarization [4] and data-to-text generation [6]. To be useful, text variants must be generated carefully, and previous work has relied on mixture-of-logits decoding. Such tech- niques are directly relevant to conversational search, where clarifying questions must remain faithful to the corpus. Faithfulness Evaluation.Faithfulness measures whether generated text re- mains consistent with its input. In summarization, state-of-the-art approaches employ entailment-based metrics that leverage NLI models to score the con- sistency of summaries with source documents (e.g., RoBERTa-based entail- ment [16]). These methods provide fine-grained judgments of factual align- ment on a continuous scale. In data-to-text generation, metrics such as PAR- ENT [5] evaluate whether candidate outputs faithfully express entities and rela- tions from structured inputs. By contrast, clarifying question generation has not been systematically assessed for faithfulness. Existing evaluations rely mainly on reference-based metrics (e.g., BLEU, METEOR) or indirect