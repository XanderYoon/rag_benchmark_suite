8 A-R Kebir et al. This yields hypotheses compatible with AL’s premise–hypothesis structure while preserving the semantic content of the questions. 4.2 Baselines We evaluate RAC against several baselines. First, we include (AT-CoT), the am- biguity taxonomy chain-of-thought prompting baseline of Tang et al. [29], which applies few-shot prompting conditioned only on the query. Following Sekulic et al. [27], we use the widely adopted (Q-Cond) fine-tuned model, which generates clarifications from the query alone. To assess the impact of supervision, we com- pare RAC to a (QP-Zeroshot) variant conditioned on both query and passages in a zero-shot setting. Finally, on ClariQ, where facet annotations are available, we also report results for the template-based (TB) and facet-based (QF-Cond) base- lines of Sekulic et al. [27]. For LLM-based methods, we use the same underlying model to ensure a fair comparison. 4.3 Implementation Details and Hyperparameters We build on the pre-trainedLLaMA3.1-8B-basecheckpoint from the Hugging- Face Hub, using theTransformersandTRLlibraries [31]. For supervised fine- tuning (SFT), we train for 2 epochs with a learning rate of1×10 −5, batch size 32, and a linear learning rate schedule. For direct preference optimiza- tion (DPO), we use 2 epochs with a learning rate of2×10 −6, batch size 32, andβ= 0.1. In our joint loss, we setγ= 0.5, based on ablation results. Zero-shot baselines rely on theInstructvariant of the base model. All ex- periments are run on NVIDIA A100 GPUs (80GB). Source code is available at: https://github.com/RayaneA7/RAC-Retrieval-augmented-clarifcation. 5 Results 5.1 Main Results The main evaluation results are reported in Table 1. We find thatRACsig- nificantly outperforms the baselines across all metrics and datasets, confirming that passage conditioning substantially improves clarifying question generation, answeringRQ2. Moreover, results show that reference-based measures fail to capture the gains from preference tuning, consistent with prior findings [4,6,23]. In contrast, reference-free evaluation –reported only for