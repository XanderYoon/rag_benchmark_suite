that clarifications are grounded in information the system can realistically access. Our RAC framework combines retrieval context with preference tuning to improve both the relevance and corpus-faithfulness of generated questions. Experiments on four benchmarks demonstrate that both RACSFT and RACDPO significantly outperform existing baselines, Q-Cond and QP-Zeroshot, across all reference-based metrics (ROUGE- L, BLEU, METEOR, and BERTScore). We further employ LLM-as-Judge eval- uations and novel metrics derived from NLI and data-to-text to quantify the gains in faithfulness to retrieved content of RACDPO over RACSFT, which is critical for conversational search, where the objective is to disambiguate and an- swer user queries based on retrieved evidence rather than knowledge internal to the language model. As future work, we plan to extend this task to multi-turn clarification and evaluate its impact on downstream retrieval performance. Acknowledgments.TheauthorsacknowledgetheANR–FRANCE(FrenchNational Research Agency) for its financial support of the GUIDANCE project n◦ANR-23-IAS1- 0003 as well as the Chaire Multi-Modal/LLM ANR Cluster IA ANR-23-IACL-0007. This work was granted access to the HPC resources of IDRIS under the allocation AD011016470 made by GENCI. Disclosure of Interests.The authors have no competing interests to declare that are relevant to the content of this article. Retrieval-Augmented Clarification for Faithful CS 13 References 1. Aliannejadi, M., Kiseleva, J., Chuklin, A., Dalton, J., Burtsev, M.: Building and evaluating open-domain dialogue corpora with clarifying questions. arXiv preprint arXiv:2109.05794 (2021) 2. Aliannejadi, M., Zamani, H., Crestani, F., Croft, W.B.: Asking clarifying questions in open-domain information-seeking conversations. In: Proceedings of the 42nd international acm sigir conference on research and development in information retrieval. pp. 475–484 (2019) 3. Banerjee, S., Lavie, A.: Meteor: An automatic metric for mt evaluation with im- proved correlation with human judgments. In: Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summa- rization. pp. 65–72 (2005) 4.