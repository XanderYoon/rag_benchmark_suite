+ - Preference tuningFine-tuning on 1 p(Cq | Uq, D) Training Data-generation Fine-tuning on 1 p(Cq | Uq) p L M pθ pθ 0 * p u n c o n d Fig.2: Overview of our proposed training pipeline. Retrieval-Augmented Clarification for Faithful CS 5 question generation rather than clarification need prediction [17]. Each passage maycapturedifferentsemanticfacetsofthequery,butwerestricttoasingle-turn setup, generating one clarifying question targeting the most useful facet. 3.1 Supervised Clarifying Question Generation Retrieval-augmented generation (RAG) has shown that conditioning large lan- guage models on retrieved passages improves factual grounding and reduces re- liance on parametric memory [9,12]. However, previous work has focused on generating direct zero-shot answers. Our contribution is to propose a fine-tuned model (twice) to better exploit the retrieved passages for the clarification task. To this end, we employ supervised fine-tuning (SFT) as the first stage of training: a large language model is trained to generate clarifying questionsCq conditioned on both the user queryU q and the corresponding retrieved passagesD(lead- ing top θ0). Given a datasetT1 of query–passage–ground-truth-question tuples (Uq,D, C + q ), the model is optimized with the negative log-likelihood objective: LSFT(θ) =−E ∼T1   |C+ q |X t=1 logp θ(Cq,t |U q,D, C q,<t)   (1) Here, each token of the clarifying questionCq,t is predicted sequentially, condi- tioned on the user query, the retrieved passages, and the previously generated tokens (denotedC q,<t). SFT establishes a strong baseline for clarification. By learning to ask ques- tionssupportedbyretrievedpassages,themodelreducesambiguityinuserintent and provides an evidence-aligned starting point for the subsequent preference based alignment stage. This further improves faithfulness and mitigates halluci- nations. 3.2 F aithfulness Alignment Althoughthep θ0 modelisalreadyfine-tunedtogenerateclarifyingquestionsthat are much more relevant than the initialpLM model, one of its main limitations is its tendency to hallucinate: it may generate details that are absent from the retrieved passagesD. Preference tuning.To