tuples (Uq,D, C q)that support retrieval-conditioned clarification generation. Metrics.We employ both reference-based and reference-free metrics to evaluate the quality of generated clarifying questions. Reference-based metrics measure similarity to gold questions, while reference-free metrics assess faithfulness to the input query and associated passages. In addition, we use GPT-4 to assess faithfulness, serving as a model-based proxy for human judgment. Reference-based evaluation.We report BLEU [21], ROUGE-L [14], ME- TEOR [3], and BERTScore [33]. BLEU and ROUGE-L capture n-gram and longest common subsequence overlap, respectively, while METEOR accounts for synonym and stem matches. BERTScore computes semantic similarity via contextualized token embeddings, providing a finer-grained assessment of mean- ing preservation. These metrics are consistent with prior work in clarification question generation and facilitate direct comparison. Faithfulness evaluation.We evaluate faithfulness using PARENT Recall (PAR) [5] and AlignScore (AL) [13]. PAR, originally proposed for data-to-text genera- tion, computes n-gram recall against both the input and the reference, serving as a proxy for input-groundedness. To apply it to unstructured passages, we adapt the metric by extracting named entities, multi-word noun phrases, and subject–verb–object triples with SpaCy5, allowing content-level overlap mea- surement without reliance on structured data. AL is an entailment-based metric built on RoBERTa [16] and trained on multiple NLI datasets. Because clarify- ing questions are often interrogative and not well-suited for direct entailment evaluation, we convert them into declarative statements by removing question templates, retaining only content-bearing tokens, and filtering query overlaps. 4 https://lemurproject.org/clueweb09/ 5 https://spacy.io/ 8 A-R Kebir et al. This yields hypotheses compatible with AL’s premise–hypothesis structure while preserving the semantic content of the questions. 4.2 Baselines We evaluate RAC against several baselines. First, we include (AT-CoT), the am- biguity taxonomy chain-of-thought prompting baseline of Tang et al. [29], which applies few-shot prompting conditioned only on the query. Following Sekulic et al. [27], we