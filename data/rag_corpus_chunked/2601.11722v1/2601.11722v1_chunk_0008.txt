of Duong et al. [6] to the clarification setting. The approach relies on two complementary models: Grounded modelp θ0 :obtained by fine-tuning a pretrained base modelpLM on half of the datasetT 1. Given a query and retrieved passages(U q,D), it out- puts generally faithful clarifying questionsC q ∼p θ0 (· |U q,D), though minor inaccuracies remain. Ungrounded modelp uncond:obtained by fine-tuning the same base model but conditioned only on the user queryUq, i.e.,C q ∼p uncond(· |U q). It produces flu- entandrelevantclarifyingquestions,yetthesearenotguaranteedtobegrounded in the retrieved passagesD. Whilep uncond produces overly unconstrained questions andpθ0 tends to re- main faithful, their combination yields plausible but unfaithful clarifying ques- tions (the balance is critical, as highlighted by Duong et al. [6]). Specifically, we decode token-by-token from a mixture distribution (Eq. 3), using stochastic decoding (temperature and top-ksampling) to promote diversity and encourage hallucinated tokens. Cq,t ∼(1−α t)p θ0 (· |C q,<t, Uq,D) +α t puncond(· |C q,<t, Uq),(3) whereα t ∼Bernoulli(α)controls the injection of ungrounded content. The noise parameterα∈[0,1]determines the faithfulness–fluency trade-off:α= 0recov- ers clarifying questions fromp θ0, whereasα= 1generates ungrounded ones fromp uncond. The resulting questions remain fluent but contain ungrounded spans, yielding both intrinsic errors (contradictions with retrieved passages) and extrinsic hallucinations (additions not inferable fromD). These are used as un- faithful clarifying questionsC− q in the augmented datasetT2 = (U q,D, C + q , C− q ), enabling preference optimization for faithfulness alignment. 3.3 Joint T raining Objective Supervised fine-tuning and preference optimization address complementary ob- jectives: supervised fine-tuning operates at the token level, teaching the model to produce clarifying questions, while preference optimization encourages it to prefer faithful outputs over unfaithful ones. To leverage both, we propose a com- bined training objective:LRAC(θ) =γ· L DPO(θ) + (1−γ)· L SFT(θ). 4 Experimental Setup 4.1 Datasets and Evaluation Datasets.We evaluate