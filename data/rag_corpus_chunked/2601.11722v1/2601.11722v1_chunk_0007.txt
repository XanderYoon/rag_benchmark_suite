tionssupportedbyretrievedpassages,themodelreducesambiguityinuserintent and provides an evidence-aligned starting point for the subsequent preference based alignment stage. This further improves faithfulness and mitigates halluci- nations. 3.2 F aithfulness Alignment Althoughthep θ0 modelisalreadyfine-tunedtogenerateclarifyingquestionsthat are much more relevant than the initialpLM model, one of its main limitations is its tendency to hallucinate: it may generate details that are absent from the retrieved passagesD. Preference tuning.To mitigate this, we introduce a second training stage focused on faithfulness. We augment the training data with pairs of faith- ful (C + q ) and unfaithful (C − q ) clarifying questions and apply a contrastive learning approach. In particular, we employ DPO [23] over a datasetT 2 = {(Uq,D, C + q , C− q )}, where the model is explicitly trained to prefer faithful clar- ifying questions over unfaithful ones. In DPO, the learning objective (Eq. 2) aligns a policy modelp θ with a preference signal, favoringC+ q overC − q , given the same input(Uq,D), as defined below: LDPO(θ) =−E ∼T2 h logσ  βlog pθ(C + q |U q,D) pθ0 (C + q |U q,D) −βlog pθ(C − q |U q,D) pθ0 (C − q |U q,D) i (2) 6 A-R Kebir et al. Unfaithful clarifying questions generation.Preference-based alignment requires faithful–unfaithful question pairs, but manual creation is costly and automatic detection remains difficult. We propose an unsupervised method that simulates unfaithful questions by injecting controlled noise during decoding. Our method adapts the noisy decoding strategy of Duong et al. [6] to the clarification setting. The approach relies on two complementary models: Grounded modelp θ0 :obtained by fine-tuning a pretrained base modelpLM on half of the datasetT 1. Given a query and retrieved passages(U q,D), it out- puts generally faithful clarifying questionsC q ∼p θ0 (· |U q,D), though minor inaccuracies remain. Ungrounded modelp uncond:obtained by