39.4â˜… mBERT 49.2 57.6 58.6 42.7 57.5 41.4 55.9 51.8 mBERT + xQG 51.5 60.9 â˜… 58.3 43.6 58.6 41.4 60.1 â˜… 53.5â˜… mBERT (zero-shot) 38.5 36.5 47.5 38.2 48.1 35.0 48.7 41.8 mBERT (zero-shot) + xQG 43.7â˜… 40.8â˜… 50.0â˜… 40.2 49.8 39.7 â˜… 51.7 45.1â˜… xQG model and the xDRs. We evaluate the effectiveness of our ap- proach and baselines using recall at ğ‘š kilo-tokens (R@ğ‘škt), which is the datasetâ€™s official evaluation metric. This metric computes the fraction of queries for which the minimal answer is contained in the top ğ‘š tokens of the retrieved passages. We consider ğ‘š = 2ğ‘˜, 5ğ‘˜ (R@2kt and R@5kt), as per common practice for this dataset. Statistical significant differences are reported with respect to a two-tailed t-test with Bonferroni correction. Baselines. Following common practice on XOR-TyDi [ 1], we adapt DPR [14] to the cross-lingual setting by initializing it with different multilingual pre-trained language models (PLMs). Specifi- cally, in our experiments we use the multilingual variants of BERT [6] (mBERT) and XLM-RoBERTa [ 5] (XLM-R) for this purpose. In addition to the standard supervised baselines, we also explore how our xQG embedding augmentation approach can improve a zero- shot xDR model, where the xDR is initialized with mBERT but it is trained with English only passage-query pairs and is directly applied to the XOR-TyDi cross-lingual retrieval task. Implementation details. We initialize our xQG model with the multilingual T5-base checkpoint provided by Google1 and available in the Huggingface library [38]. We fine-tune this checkpoint with the passage-query pairs in the XOR-TyDi training set. We train the xQG model for 1600 steps using a batch size of 128 and a learning rate of 1e-4. After training, we generate 5 queries per passage for each target language, resulting in about 7 * 5 * 18M =