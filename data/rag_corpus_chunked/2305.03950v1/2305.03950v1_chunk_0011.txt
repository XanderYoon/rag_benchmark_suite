average R@2kt of 33.0; this also improves when combined with xQG, achieving an average score of 36.0. This improvement is statistically significant (ğ‘ < 0.05). Similar trends are found for R@5kt. Overall, we find that our xQG can significantly improve all investigated xDR models. In terms of per language effectiveness, xQG improves almost all models across all languages with the exceptions of mBERTâ€™s R@2kt for Japanese (Ja) and mBERTâ€™s R@5kt for Finnish (Fi). In summary, mBERT performs better than XLM-R for both R@2kt and R@5kt. The use of xQG embedding augmentation statistically significantly improves the effectiveness of both backbones. 5.2 RQ1: Impact of number of generated queries Figure 2 reports the impact of using different amounts of generated queries to augment passage embeddings when using mBERT xDR. The results suggest that using more generated queries is beneficial for both R@2tk and R@5tk. The improvements become statistically significant when 4 or more generated queries are used for each target language. While the curves do not plateau, indicating that using even more generated queries could further improve the effect- iveness, our experiments were limited to up to 5 generated queries per target language due to computational constraints. 5.3 RQ2: Impact of augmentation ratio We report the impact of the augmentation ratio ğ›¼ on the effective- ness of xDR in Figure 3. Higher values ofğ›¼ correspond to assigning more weight to the generated query embeddings during embedding aggregation, and ğ›¼ = 0 corresponds to no augmentation. As shown in the figure, even a small value of ğ›¼ (0.01) leads to a significant improvement in both R@2kt and R@5kt. The best effectiveness is achieved when ğ›¼ = 0.02. However, using higher values of ğ›¼ does not result in further improvements â€“ rather, it can even hurt the effectiveness when ğ›¼ > 0.05. Based on