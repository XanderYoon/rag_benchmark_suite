46] and cross-lingual dense retrieval [3]. On the other hand, the HyDE method uses large pre-trained language models to generate hypo- thetical passages for a given query and directly uses the embedding of the hypothetical passage to perform the search [ 9]. While all these works focus on augmenting query embeddings for DRs at query time, our work focuses on augmenting passage embeddings at indexing time, thereby avoiding the extra overhead in terms of query latency. 3 METHOD Our approach consists of two components: (1) a cross-lingual query generation model that, for each passage, generates high-quality queries in different languages, and (2) an embedding aggregation function that augments the original passage embedding with the embeddings of the generated queries. To obtain a xQG model, we fine-tune a mT5 model with labeled relevant (ğ‘ğ‘¡ , ğ‘) pairs, where ğ‘¡ is the target query language. We use a seq2seq objective akin to docTquery-T5â€™s objective [25]; in our case the input is the passage text with language-specific prompts: prompt (ğ‘¡, ğ‘) = Generate a [ğ‘¡] question for this passage: [ğ‘], (1) where [ğ‘¡] and [ğ‘] are prompt placeholders for the target language and passage text, respectively. Once we have trained a xQG model, we can generate a set of queries for each target language and passage by replacing the placeholders accordingly: ğ‘„ğ‘ = Ã˜ ğ‘¡ âˆˆğ‘‡ ğ‘„ğ‘¡ ğ‘ ; ğ‘„ğ‘¡ ğ‘ = xQG (prompt (ğ‘¡, ğ‘ )) Ã— ğ‘›, (2) where ğ‘‡ is the target language set and ğ‘„ğ‘ is the set of all generated queries for passage ğ‘ which includes ğ‘› generated queries for each target language. In our experiments, we use a top- ğ‘˜ sampling scheme [7] to generate queries, and set ğ‘˜ = 10. We find that these simple language-specific prompts can effectively lead the T5 model to generate the