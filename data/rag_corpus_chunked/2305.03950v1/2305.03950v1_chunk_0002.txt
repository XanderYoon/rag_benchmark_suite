a PLM encoder into dense embeddings. They then perform approximate nearest neighbor (ANN) searching in the embedding space. Compared to traditional bag-of-words approaches, DRs benefit from semantic soft matching, which helps overcome the problem of word mismatch in passage retrieval [33, 45]. To leverage the semantic modelling power of DRs, recent research has extended English-only DRs to support cross-lingual settings [1, 2, 19, 22, 27, 31], i.e, where queries and passages are in different languages. This is achieved using multi-lingual PLMs, such as mul- tilingual BERT [ 6], in place of the English-only PLMs. This ap- proach is particularly important in this setting where traditional bag-of-words methods are ineffective due to the limited number of matching terms across languages. In contrast, cross-lingual DRs (xDRs) are able to encode queries and passages in different languages into a shared embedding space, enabling efficient ANN search across languages. However, such multi-lingual PLM-based xDRs usually are less effective on the cross-lingual passage retrieval task than DRs in the English-only setting [1]. The hypothesis to explain this result is that, in the English-only setting, a DR only needs to learn relevance matching between queries and passages. In contrast, a xDR not only needs to learn the relevance matching task, but also needs to learn how to align the embeddings of texts with similar semantic meaning but in different language [24, 41]. It is this language gap that makes cross lingual retrieval a relatively harder task for xDRs. Based on this hypothesis, this paper proposes the use of cross- lingual query generation (xQG) to bridge the language gap for xDRs. Our approach is illustrated in Figure 1. Specifically, we fine-tune a multilingual T5 (mT5) model using language-specific prompts to generate several queries per passage for each target language. In the indexing phase, we use a given