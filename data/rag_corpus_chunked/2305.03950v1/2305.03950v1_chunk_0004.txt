single encoder and self-training, and DR.DECR [19], which utilizes parallel queries and sentences for cross-lingual knowl- edge distillation. Among these, our work is most closely related to QuiCK [27], which also utilizes a cross-lingual query generator for xDR. However, unlike QuiCK, which uses the xQG as a teacher model in knowledge distillation for xDR in the training phase, our method directly augments passage embeddings with xQG queries without any xDR training involved. Query generation for information retrieval.Query generation is a well-established technique that has been widely used to improve retrieval performance in various retrieval models [11, 25]. In addition, it has been shown to be effective for domain adaptation in dense passage retrieval tasks [ 23, 32, 34], as well as for enhancing the effectiveness of other PLM-based rankers [37, 48]. In our approach, we also rely on a query generation model to generate high-quality queries for downstream passage embedding augmentation tasks. Augmenting embeddings for dense retrievers. Our method relies on effectively augmenting representations encoded by DR encoders â€“ a direction recently explored also by other works. For instance, Li et al. [16, 17] use the top retrieved passages as pseudo- relevant feedback to augment query embeddings using the Rocchio aggregation function. Similarly, Zhuang et al. [47] extend this idea by using embeddings of clicked passages to augment query em- beddings. Other PRF methods have also been extensively researched to enhance both English-only dense retrieval [ 18, 35, 36, 42, 46] and cross-lingual dense retrieval [3]. On the other hand, the HyDE method uses large pre-trained language models to generate hypo- thetical passages for a given query and directly uses the embedding of the hypothetical passage to perform the search [ 9]. While all these works focus on augmenting query embeddings for DRs at query time, our work focuses on