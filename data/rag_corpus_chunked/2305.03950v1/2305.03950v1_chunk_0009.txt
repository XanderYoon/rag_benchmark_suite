available in the Huggingface library [38]. We fine-tune this checkpoint with the passage-query pairs in the XOR-TyDi training set. We train the xQG model for 1600 steps using a batch size of 128 and a learning rate of 1e-4. After training, we generate 5 queries per passage for each target language, resulting in about 7 * 5 * 18M = 630M generated queries in total. We use four A100 GPUs to generate all queries; this process took about 70 hours to complete. We release our generated queries on the Huggingface hub 2 to allow the community to reuse this resource [30]. For training the xDRs, we use the Tevatron dense retriever training toolkit [ 10], which uses with BM25 hard negative passages. We train the xDRs with a batch size of 128, initializing them with mBERT base 3 or XLM-R base4 checkpoints and training them on the XOR-TyDi training set for 40 epochs. For each training sample, we set the number of hard negative passages in the contrastive loss to 7 and applied in-batch negatives training. We use a learning rate of 1e-5 1https://huggingface.co/google/mt5-base 2https://huggingface.co/datasets/ielab/xor-tydi-xqg-augmented 3https://huggingface.co/bert-base-multilingual-cased 4https://huggingface.co/xlm-roberta-base Figure 2: Impact of the amount of generated queries for target language. Scores are averaged across all languages. Statistical significant improvements over no augmentation (number of qu- eries ğ‘› = 0) are labelled with stars ( ğ‘ < 0.05). for mBERT-based xDRs and of 2e-5 for XLM-R-based xDRs. For the zero-shot xDR, we use the same training configurations as for the mBERT-based xDR trained on XOR-TyDi but using the Natural Questions (NQ) training data [ 15] which contains English-only query-passage training samples. 5 RESULTS 5.1 Main results Table 1 presents the effectiveness of xDR models initialized with the XLM-R and mBERT backbone PLMs and trained on the XOR- TyDi dataset. Zero-shot denotes the