for xDRs. Based on this hypothesis, this paper proposes the use of cross- lingual query generation (xQG) to bridge the language gap for xDRs. Our approach is illustrated in Figure 1. Specifically, we fine-tune a multilingual T5 (mT5) model using language-specific prompts to generate several queries per passage for each target language. In the indexing phase, we use a given xDR model to encode passages and their associated generated queries in different languages into embeddings. Finally, we augment the original passage embeddings with the embeddings of the generated queries before adding them to the ANN index. By doing so, we move the passage embeddings to a space that is closer to the target user query language. Our approach does not add extra query latency and can be applied to any existing xDRs. 2 RELATED WORKS Cross-lingual dense retrievers. The development of cross-lingual pre-trained language models has significantly contributed to the arXiv:2305.03950v1 [cs.IR] 6 May 2023 SIGIR ’23, July 23–27, 2023, Taipei, Taiwan Shengyao Zhuang, Linjun Shou, and Guido Zuccon Figure 1: Augmenting passage representations with cross-lingual generated query embeddings. The query examples shown in this figure were generated using our trained xQG model. For each query, we report the corresponding translation obtained using Google’s translation service. progress of cross-lingual dense retrieval (xDR) [4, 5, 8, 39]. Notable examples of recent xDR methods include CORA [2], which employs a generator to facilitate retrieval training data mining, Sentri [31], which proposes a single encoder and self-training, and DR.DECR [19], which utilizes parallel queries and sentences for cross-lingual knowl- edge distillation. Among these, our work is most closely related to QuiCK [27], which also utilizes a cross-lingual query generator for xDR. However, unlike QuiCK, which uses the xQG as a teacher model in knowledge distillation for xDR in the training phase, our method