use only 500k training instances of T-REx in our experiments to increase the speed of experi- mentation. Since the transformers for passage encoding and generation can accept a limited sequence length, we segment the documents of the KILT knowledge source (2019/08/01 Wikipedia snapshot) into pas- sages. The ground truth provenance for the slot ﬁlling tasks is at the granularity of paragraphs, so we align our passage segmentation on paragraph boundaries when possible. If two or more para- graphs are short enough to be combined, we com- bine them into a single passage and if a single paragraph is too long, we truncate it. 4.1 KGI Hyperparameters We have not done hyperparameter tuning, instead using hyperparameters similar to the original works Hyperparameter DPR RAG learn rate 5e-5 3e-5 batch size 128 128 epochs 2 1 warmup instances 0 10000 learning schedule linear triangular max grad norm 1 1 weight decay 0 0 Adam epsilon 1e-8 1e-8 Table 3: KGI hyperparameters on training DPR and RAG. Table 3 shows the hy- perparameters used in our experiments. We train our models on T-REx using only the ﬁrst 500k instances. For KGI1 we use the same hyperparam- eters except that zsRE is trained for two epochs. In both KGI systems we use the default of ﬁve passages retrieved for each query for use in RAG. 4.2 Model Details Number of parameters KGI is based on RAG and has the same number of parameters: 2× 110M for the BERTBASE query and passage en- coders and 400M for the BARTLARGE sequence- to-sequence generation component: 620M in total. Computing infrastructure Using a single NVIDIA V100 GPU DPR training of two epochs takes approximately 24 hours for T-REx and 2 hours for zsRE. Using a single NVIDIA P100 GPU RAG training for 500k T-REx instances takes two days