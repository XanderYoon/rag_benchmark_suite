that co-occur withe in the corpus, we can frame the zero-shot slot ﬁlling as a ranking problem: argmaxi scoreM (e, s, vi). scoreM is a function that takes as input a triple and provide a score based on the model M. Turning the slot ﬁlling into a ranking problem has two advantages: 1) we can compare the generative approach with a new set of baselines, and 2) we can limit the generation of the slot values to a pre-deﬁned set of domain speciﬁc entities. Models In order to adapt KGI1, as pre-trained on T-REx, to the TACRED corpus, we indexed the textual passages using DPR, as described in Sec- tion 3. Then we replaced the original Wikipedia index with this new index. During the inference step, we restrict the generation of the slot values using the list of object candidates, i.e. the entities which co-occur with the subject from the inverted index, to facilitate comparability to a set of rank- ing baselines. To this aim, we adopt the technique described by Cao et al. (2021) to restrict the vocab- ulary of tokens during the generation. We use three baselines to compare with our ap- proach for this zero-shot slot ﬁlling task. PMI is implemented using the pointwise mutual informa- tion between e and vi based on their co-occurrence in the corpus. Also, we train aWord2Vec(Mikolov et al., 2013) skip-gram model on the textual corpus, and we use it to implement the scoring function as cosine(e + s, vi), for each candidate ﬁller vi. It is based on the assumption that a relation s be- tween two (multi)word embeddings e and v can be represented as an offset vector (v− e) = s ⇐⇒ (e+s) = v (Rossiello et al., 2019; Vylomova et al., 2016). Finally, GPT-2 computes the perplexity of