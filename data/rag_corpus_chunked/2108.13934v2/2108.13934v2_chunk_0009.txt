the generator pre- dicts a probability distribution over the possible next tokens for each sequence. These predictions are weighted according to the score between the query and passage - the inner product of the query vector and passage vector. tailhead [SEP] relation Query EncoderANN Index Generator Marginalize Passages Figure 4: RAG Architecture Marginalization then combines the weighted probability distributions to give a single probabil- ity distribution for the next token. This enables RAG to train the query encoder through its impact in generation, learning to give higher weight to passages that contribute to generating the correct tokens. Formally, the inputs to the BART model are sequences (sj = pj [SEP] q) that comprise a query q plus retrieved passage pj. The probability for each sequence is determined from the softmax over the retrieval scores (zr) for the passages. The prob- ability for each output token ti given the sequence sj is a softmax over BART’s token prediction log- its. Therefore the total probability for each token ti is the log-likelihood summed over all sequences, weighted by each sequence’s probability. P (sj) = sof tmax(zr)j P (ti|sj) = sof tmax(BART(sj)i)ti P (ti) = ∑ j P (ti|sj)· P (sj) Beam search is used at inference time to select the overall most likely tail entity. This is the stan- dard beam search for natural language generation in deep neural networks (Sutskever et al., 2014), the only difference is in the way the next-token probabilities are obtained. 3.3 Dense Negative Sampling As Figure 2 shows, the DPR question encoder is trained both by DPR and later by RAG. To examine the inﬂuence of this additional training from RAG on the retrieval performance, we compare retrieval metrics before and after RAG ﬁne-tuning. Table 1 shows the large gains from training with RAG after DPR.