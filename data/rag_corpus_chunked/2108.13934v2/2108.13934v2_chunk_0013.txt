110M for the BERTBASE query and passage en- coders and 400M for the BARTLARGE sequence- to-sequence generation component: 620M in total. Computing infrastructure Using a single NVIDIA V100 GPU DPR training of two epochs takes approximately 24 hours for T-REx and 2 hours for zsRE. Using a single NVIDIA P100 GPU RAG training for 500k T-REx instances takes two days and 147k instances of zsRE takes 15 hours. The FAISS index on the KILT knowledge source requires a machine with large memory, we use 256GB memory - 128GB is insufÔ¨Åcient for the indexes without scalar quantization. 4.3 Slot Filling Evaluation As an initial experiment we tried RAG with its de- fault index of Wikipedia, distributed through Hug- ging Face. We refer to this as RAG-KKS, or RAG without the KILT Knowledge Source, as reported in Table 4. Since the passages returned are not aligned to the KILT provenance ground truth, we do not report retrieval metrics for this experiment. Motivated by the low retrieval performance re- ported for the RAG baseline by Petroni et al. (2021), we experimented with replacing the DPR retrieval with simple BM25 (RAG+BM25) over the KILT knowledge source. We provide the raw BM25 scores for the passages to the RAG model, to weight their impact in generation. We also exper- imented with the Natural Questions trained DPR, R-Prec R@5 Acc. F1 zsRE RAG-KKS 38.72 46.94 RAG+BM25 58.86 80.24 45.73 55.18 RAG+DPRN Q 68.13 79.19 46.03 55.75 KGI0 96.24 97.53 69.58 77.24 KGI1 98.60 99.70 71.32 78.85 T-REx RAG-KKS 63.28 67.67 RAG+BM25 46.40 67.31 69.10 73.11 RAG+DPRN Q 53.04 65.54 73.02 76.97 KGI0 61.30 71.18 76.58 80.27 KGI1 74.34 82.89 84.04 86.89 Table 4: Dev sets performance for different retrieval methods with only RAG training on KILT (RAG+DPRN Q). We use the approach explained in Section 3