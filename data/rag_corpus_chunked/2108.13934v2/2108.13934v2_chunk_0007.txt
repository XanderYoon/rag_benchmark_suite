contain a correct answer. The re- maining top ranked result is used as a hard negative for DPR training. This is the hard negative mining strategy used by DPR (Karpukhin et al., 2020) and Multi-DPR (Maillard et al., 2021). head1 [SEP] relation1 head2 [SEP] relation2 head3 [SEP] relation3 Passage1 + Passage1 - Passage2 + Passage2 - Passage3 + Passage3 - p1 + p1 - p2 + p2 - p3 + p3 - q1 q2 q3 softmax by row positive hard negative batch negatives Passage Encoder Query Encoder Figure 3: DPR Training After locating a hard negative for each query, the DPR training data is a set of triples: query, positive passage (given by the KILT ground truth provenance) and the hard negative passage. Figure 3 shows the training process for DPR. For each batch of training triples, we encode the queries and passages independently. The passage and query encoders are BERT (Devlin et al., 2019) models. Then we Ô¨Ånd the inner product of all queries with all passages. The negatives for a given query are therefore the hard negative and the batch negatives, i.e. the positive and hard negative passages for other queries in the batch. After applying a softmax to the score vector for each query, the loss is the negative log-likelihood for the positive passages. Using the trained DPR passage encoder we gen- erate vectors for the approximately 32 million pas- sages in our segmentation of the KILT knowledge source. Though this is a computationally expensive step, it is easily parallelized. The passage-vectors are then indexed with an ANN (Approximate Near- est Neighbors) data structure, in this case HNSW (Hierarchical Navigable Small World)(Malkov and Yashunin, 2018) using the open source FAISS li- brary (Johnson et al., 2017)4. We use scalar quanti- zation down to 8 bits to reduce