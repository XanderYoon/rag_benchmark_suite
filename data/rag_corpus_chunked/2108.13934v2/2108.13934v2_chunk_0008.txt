knowledge source. Though this is a computationally expensive step, it is easily parallelized. The passage-vectors are then indexed with an ANN (Approximate Near- est Neighbors) data structure, in this case HNSW (Hierarchical Navigable Small World)(Malkov and Yashunin, 2018) using the open source FAISS li- brary (Johnson et al., 2017)4. We use scalar quanti- zation down to 8 bits to reduce the memory size. 3https://github.com/castorini/anserini 4https://github.com/facebookresearch/ faiss The query encoder is also trained for slot ﬁll- ing alongside the passage encoder. We inject the trained query encoder into the RAG model for Nat- ural Questions. Due to the loose coupling between the query encoder and the sequence-to-sequence generation of RAG, we can update the pre-trained model’s query encoder without disrupting the qual- ity of the generation. Unlike previous work on zero-shot slot ﬁlling, we are training the DPR model speciﬁcally for the slot ﬁlling task. In contrast, the RAG baseline (Petroni et al., 2021) used DPR pre-trained on Nat- ural Questions, and Multi-DPR (Maillard et al., 2021) trained on all KILT tasks jointly. 3.2 RAG for Slot Filling Figure 4 illustrates the architecture of RAG (Lewis et al., 2020b). The RAG model is trained to predict the ground truth tail entity from the head and rela- tion query. First the query is encoded to a vector and the top-k (we use k = 5 ) relevant passages are retrieved from the ANN index. The query is concatenated to each passage and the generator pre- dicts a probability distribution over the possible next tokens for each sequence. These predictions are weighted according to the score between the query and passage - the inner product of the query vector and passage vector. tailhead [SEP] relation Query EncoderANN Index Generator Marginalize Passages Figure 4: RAG Architecture Marginalization then combines the weighted probability distributions to