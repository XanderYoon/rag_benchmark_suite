next-token probabilities are obtained. 3.3 Dense Negative Sampling As Figure 2 shows, the DPR question encoder is trained both by DPR and later by RAG. To examine the inﬂuence of this additional training from RAG on the retrieval performance, we compare retrieval metrics before and after RAG ﬁne-tuning. Table 1 shows the large gains from training with RAG after DPR. Note that RAG training is using the weak supervision of the passage’s impact in pro- ducing the correct answer, rather than the ground truth provenance of DPR training. Since this is likely a disadvantage, we explore the other key dif- ference with DPR and RAG training: RAG uses negatives drawn from the trained index rather than from BM25. T-REx zsRE R-Prec R@5 R-Prec R@5 DPRN Q 19.50 29.80 45.49 60.77 DPRN Q+RAG 53.04 65.54 68.13 79.19 DPRBM 25 49.02 63.34 94.55 98.17 DPRBM 25+RAG 65.02 75.52 96.89 98.01 DPRDN S 42.62 55.09 97.53 99.30 DPRDN S+RAG 74.34 82.89 98.60 99.70 Table 1: Analysis of retrieval by DPR and RAG on Dev sets To replicate this feature of RAG in DPR, we introduce hard negatives mined from the learned index. Using the KILT trained DPR models, we index the passages. Then we gather hard negatives for DPR training, with one difference: rather than locating the hard negative passages by BM25, we ﬁnd the passage by ANN search over the learned dense vector index. We train for an additional two epochs using these hard negatives. Table 1 shows the performance of the different approaches to retrieval. DPRN Q is the DPR model pre-trained Instances Relations Dataset Train Dev Test Train Dev Test zsRE 148K 3724 4966 84 12 24 T-REx 2284K 5000 5000 106 104 104 Table 2: Slot ﬁlling datasets in KILT on Natural Questions. DPR BM 25 further trains DPRN