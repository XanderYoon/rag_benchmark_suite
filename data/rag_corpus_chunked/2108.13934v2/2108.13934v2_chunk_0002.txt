for slot ﬁlling (Petroni et al., 2020), opening a new research direction that might provide an ef- fective solution to the aforementioned problems. In particular, the KILT benchmark (Petroni et al., 2021), standardizes two zero-shot slot ﬁlling tasks, zsRE (Levy et al., 2017) and T-REx (Elsahar et al., 2018), providing a competitive evaluation frame- work to drive advancements in slot ﬁlling. How- ever, the best performance achieved by the current retrieval-based models on the two slot ﬁlling tasks in KILT are still not satisfactory. This is mainly arXiv:2108.13934v2 [cs.CL] 14 Sep 2021 due to the lack of retrieval performance that affects the generation of the ﬁller as well. In this work, we propose KGI (Knowledge Graph Induction), a robust system for slot ﬁll- ing based on advanced training strategies for both Dense Passage Retrieval (DPR) and Retrieval Aug- mented Generation (RAG) that shows large gains on both T-REx (+38.24% KILT-F1) and zsRE (+21.25% KILT-F1) datasets if compared to previ- ously submitted systems. We extend the training strategies of DPR withhard negative mining (Simo- Serra et al., 2015), demonstrating its importance in training the context encoder. In addition, we explore the idea of adaptingKGI to a new domain. The domain adaptation process consists of indexing the new corpus using our pre- trained DPR and substituting it in place of the orig- inal Wikipedia index. This enables zero-shot slot ﬁlling on the new dataset with respect to a new schema, avoiding the additional effort needed to re- build NLP pipelines. We provide a few additional examples for each new relation, showing that zero- shot performance quickly improves with a few-shot learning setup. We explore this approach on a vari- ant of the TACRED dataset (Alt et al., 2020) that we speciﬁcally introduce to evaluate the zero/few- shot slot ﬁlling task for domain