train the DPR passage and query encoder. The top-3 passages returned by the resulting passage index were then combined into a single sequence with the query and a BART model was used to produce the answer. This resulted in large gains in retrieval performance. DensePhrases (Lee et al., 2021) is a different approach to knowledge intensive tasks with a short answer. Rather than index passages which are then consumed by a reader or generator component, it indexes the phrases in the corpus that can be poten- tial answers to questions, or ﬁllers for slots. Each phrase is represented by the pair of its start and end token vectors from the ﬁnal layer of a transformer initialized from SpanBERT (Joshi et al., 2020). GENRE (Cao et al., 2021) addresses the retrieval task in KILT slot ﬁlling by using a sequence-to- sequence transformer to generate the title of the Wikipedia page where the answer can be found. This method can produce excellent scores for re- trieval but it does not address the problem of pro- ducing the slot ﬁller. It is trained on BLINK (Wu et al., 2020) and all KILT tasks jointly. Open Retrieval Question Answering (ORQA) (Lee et al., 2019) introduced neural information retrieval for the related task of factoid question answering. Like DPR, the retrieval is based on a bi- encoder BERT (Devlin et al., 2019) model. Unlike DPR, ORQA projects the BERT [CLS] vector to a lower dimensional (128) space. It also uses the inverse cloze pre-training task for retrieval, while DPR does not use retrieval speciﬁc pre-training. 3 Knowledge Graph Induction Figure 2 shows KGI, our approach to zero-shot slot ﬁlling, combining a DPR model and RAG model, both trained for slot ﬁlling. We initialize our mod- els from the Natural Questions (Kwiatkowski et al., 2019) trained