gains of over 14 percentage points in zsRE and even more in T-REx. The combined metrics of KILT-AC and KILT-F1 show even larger gains, suggesting that the KGI1 approach is effective at providing justify- ing evidence when generating the correct answer. We achieve gains of 21 to 41 percentage points in KILT-AC. Relative to Multi-DPR, we see the beneﬁt of weighting passage importance by retrieval score and marginalizing over multiple generations, com- pared to the strategy of concatenating the top three passages and running a single sequence-to- sequence generation. GENRE is still best in re- trieval for T-REx, suggesting that at least for a corpus such as Wikipedia, generating the title of the page can be very effective. A possible explana- tion for this behaviour is that most relations for a Wikipedia entity are mentioned in its correspond- ing page. 4.4 Analysis To explore the effect of retrieval on downstream per- formance we consider two variants of our systems: one using random passages from the index, forcing the system to depend on implicit knowledge, and the another using passages from the ground truth provenance, to measure the upper bound perfor- mance for the ideal retrieval system. Evaluation is reported in Table 6 for 3 systems. By supplying these systems with the gold standard passages, we can see both the improvement possible through bet- ter retrieval, and the value of good retrieval during training. The best system, KGI1 is the most effec- tive at generating slot ﬁllers from relevant explicit knowledge because it was trained on more cases of justifying explicit knowledge. However, given random passages it is the worst. It has sacriﬁced some implicit knowledge for better capabilities in using explicit knowledge. As shown in Table 5, BART LARGE, which is the best implicit-knowledge baseline system for KILT slot ﬁlling,