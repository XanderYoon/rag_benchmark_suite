ﬁlling is a sub-task of Knowledge Base Pop- ulation (KBP), where the goal is to recognize a pre-determined set of relations for a given entity and use them to populate infobox like structures. This can be done by exploring the occurrences of the input entity in the corpus and gathering infor- mation about its slot ﬁllers from the context in which it is located. A slot ﬁlling system processes and indexes a corpus of documents. Then, when prompted with an entity and a number of relations, 1Our source code is available at: https://github. com/IBM/kgi-slot-filling Figure 1: Slot Filling task it ﬁlls out an infobox for the entity. Some slot ﬁll- ing systems provide evidence text to explain the predictions. Figure 1 illustrates the slot ﬁlling task. Many KBP systems described in the literature commonly involve complex pipelines for named en- tity recognition, entity co-reference resolution and relation extraction (Ellis et al., 2015). In particu- lar, the task of extracting relations between entities from text has been shown to be the weakest com- ponent of the chain. The community proposed dif- ferent solutions to improve relation extraction per- formance, such as rule-based (Angeli et al., 2015), supervised (Zhang et al., 2017), or distantly su- pervised (Glass et al., 2018). However, all these approaches require a considerable human effort in creating hand-crafted rules, annotating training data, or building well-curated datasets for boot- strapping relation classiﬁers. Recently, pre-trained language models have been used for slot ﬁlling (Petroni et al., 2020), opening a new research direction that might provide an ef- fective solution to the aforementioned problems. In particular, the KILT benchmark (Petroni et al., 2021), standardizes two zero-shot slot ﬁlling tasks, zsRE (Levy et al., 2017) and T-REx (Elsahar et al., 2018), providing a competitive evaluation frame- work to drive advancements in slot