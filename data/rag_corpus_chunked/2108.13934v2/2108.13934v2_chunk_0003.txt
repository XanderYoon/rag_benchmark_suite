additional effort needed to re- build NLP pipelines. We provide a few additional examples for each new relation, showing that zero- shot performance quickly improves with a few-shot learning setup. We explore this approach on a vari- ant of the TACRED dataset (Alt et al., 2020) that we speciﬁcally introduce to evaluate the zero/few- shot slot ﬁlling task for domain adaption. The contributions of this work are as follows: 1. We describe an end-to-end solution for slot ﬁlling, calledKGI, that improves the state-of- the-art in the KILT slot ﬁlling benchmarks by a large margin. 2. We demonstrate the effectiveness of hard neg- ative mining for DPR when combined with end-to-end training for slot ﬁlling tasks. 3. We evaluate the domain adaptation of KGI using zero/few-shot slot ﬁlling, demonstrat- ing its robustness on zero-shot TACRED, a benchmark released with this paper. 4. We publicly release the pre-trained models and source code of the KGI system. Section 2 present an overview of the state of the art in slot ﬁlling. Section 3 describes ourKGI system, providing details on the DPR and RAG models and describing our novel approach to hard negatives. Our system is evaluated in Sections 4 and 5 which include a detailed analysis. Section 6 concludes the paper and highlights some interesting direction for future work. 2 Related Work The use of language models as sources of knowl- edge (Petroni et al., 2019; Roberts et al., 2020; Wang et al., 2020; Petroni et al., 2020), has opened tasks such as zero-shot slot ﬁlling to pre-trained transformers. Furthermore, the introduction of re- trieval augmented language models such as RAG (Lewis et al., 2020b) and REALM (Guu et al., 2020) also permit providing textual provenance for the generated slot ﬁllers. KILT (Petroni et al., 2021) was introduced with a number of baseline