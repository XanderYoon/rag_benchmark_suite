these hard negatives. Table 1 shows the performance of the different approaches to retrieval. DPRN Q is the DPR model pre-trained Instances Relations Dataset Train Dev Test Train Dev Test zsRE 148K 3724 4966 84 12 24 T-REx 2284K 5000 5000 106 104 104 Table 2: Slot ﬁlling datasets in KILT on Natural Questions. DPR BM 25 further trains DPRN Q on the KILT data with BM25 hard nega- tives. Rows with +RAG further train the question encoder through RAG. The row DPRDN S (Dense Negative Sampling) shows the performance of re- trieval immediately after DNS training. Surpris- ingly, this results in lower performance for T-REx relative to DPR BM 25. However, further training the DNS model with RAG results in our best per- formance for both T-REx and zsRE. Since RAG does not update the context encoder, DNS training is the only training for the context encoder when negatives are drawn from the dense vector index. After training with DNS the FAISS indexing with scalar quantization becomes prohibitively slow. We therefore remove all quantization and use four shards (the index is split into four, with the results of each query merged) for our experiments with DNS enabled KGI. 4 KILT Experiments Table 2 gives statistics on the two zero-shot slot ﬁlling datasets in KILT. While the T-REx dataset is larger by far in the number of instances, the train- ing sets have a similar number of distinct relations. We use only 500k training instances of T-REx in our experiments to increase the speed of experi- mentation. Since the transformers for passage encoding and generation can accept a limited sequence length, we segment the documents of the KILT knowledge source (2019/08/01 Wikipedia snapshot) into pas- sages. The ground truth provenance for the slot ﬁlling tasks is at the granularity of