ﬁllers that can be further validated. For this purpose, we also conduct few-shot ex- periments to understand the robustness of KGI1 by ﬁne-tuning it with very limited amounts of train- ing examples. We randomly pick n example(s) for each relation type from the TACRED training set, with n = [1, 4]. Table 9 gives our hyperparameters for the TACRED few-shot experiments. We show that our system beneﬁts from additional domain speciﬁc training data selected from TACRED. Just using one example and four examples per relation, HIT@1 improves∼ 5 and∼ 10 percentage points respectively. 6 Conclusion In this paper, we presented KGI, a novel approach to zero-shot slot ﬁlling.KGI improves Dense Pas- sage Retrieval using hard negatives from the dense index, and implements a robust training procedures for Retrieval Augmented Generation. We evaluated KGI on both T-REx and zsRE slot ﬁlling datasets, ranking at top-1 position in the KILT leaderboard with a net improvement of +38.24 and +21.25 per- centage points in KILT-F1, respectively. More- over, we proposed and release a new benchmark for zero/few-shot slot ﬁlling based on TACRED to evaluate domain adaptation where our system obtained much better zero-shot results compared with the baselines. In addition, we have observed signiﬁcant improvement in results for KGI when rapidly ﬁne-tuned in a few-shot setting. This work opens promising future research directions for slot ﬁlling and other related tasks. We plan to apply DPR with dense negative sampling to other tasks in the KILT benchmark, including dialogue and ques- tion answering. Likewise, an in-depth investigation on more effective strategies for domain adaptation, such as the combination of zero-shot and few-shot learning involving human-in-the-loop techniques, would be another interesting direction to explore. References Christoph Alt, Aleksandra Gabryszak, and Leonhard Hennig. 2020. TACRED revisited: A thorough eval- uation of the TACRED relation extraction task.