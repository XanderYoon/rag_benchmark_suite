2025, pp. 17533–17547, 2025b. Yibo Zhao, Jiapeng Zhu, Ye Guo, Kangkang He, and Xiang Li. Eˆ 2graphrag: Streamlining graph- based rag for high efficiency and effectiveness.arXiv preprint arXiv:2505.24226, 2025. 12 A DATASETSTATISTICS LiHuaWorld.LiHuaWorld is an English dialogue dataset comprising one year of conversational records from a virtual user, spanning diverse daily-life topics such as social interactions, fitness, entertainment, and personal affairs. It contains442 documentsand637 queries, including506 single- hop,66 multi-hop, and65 unanswerable (null)questions. Each query is paired with manually annotated answers and supporting documents, allowing fine-grained evaluation of both retrieval and reasoning. MultiHop-RAG.MultiHop-RAG is a news-based multi-hop QA dataset constructed from English news articles. It consists of609 documentsand2,556 querieswith ground-truth answers and supporting evidence. We use the first500 queriesas the test set, primarily to ensure computational feasibility and enable extensive ablation studies, while maintaining coverage across query types. Queries are categorized intoInference (153),Comparison (181),Temporal (100), andNull (66) types. Together, these datasets provide complementary evaluation scenarios: LiHuaWorld targets long-term personal memory QA, while MultiHop-RAG emphasizes news-based multi-hop reasoning. B IMPLEMENTATIONDETAILS B.1 EMBEDDINGMODELSETTINGS We employed pre-trained sentence embedding models from the Sentence-Transformers5 library for encoding text segments. Unless otherwise specified, the default model is all-MiniLM-L6-v2, which produces 384-dimensional embeddings. This model is lightweight and optimized for efficiency, making it suitable for large-scale retrieval scenarios. For comparison, we also experimented with all-mpnet-base-v2, a stronger encoder that outputs 768-dimensional embeddings. While this model provides higher representational capacity and improved semantic alignment, it incurs increased computational and memory costs. Both models were applied in a zero-shot manner without task-specific fine-tuning. During retrieval, embeddings were ℓ2-normalized before similarity computation to ensure stable and consistent cosine similarity scores. B.2 LLM SETTINGS All large language models (LLMs) were accessed via the BaiLian 6 API platform with default generation parameters: temperature= 0.7 , top p= 0.8 , top k= 20