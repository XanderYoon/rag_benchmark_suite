Qc denotes the subset of queries in Q∗ associated with chunk c. Chunks are ranked by s(c), and the top-Kare retained: Ctop-K =Top-K{c∈ C ∗ |s(c)}. Step 4: Generating Responses.Finally, the answer is generated by conditioning the LLM on the user queryq u and the top-Kselected chunksC top-K: a=LLM qu | Ctop-K  . The Query-Centric Graph Retrieval & Generation process enhances traditional RAG by retrieving through queries rather than directly over chunks, expanding to multi-hop neighbors, and aggregating evidence with controllable granularity. This design ensures better coverage of relevant document segments, improved response accuracy, and interpretable reasoning paths, distinguishing it from conventional chunk-based and graph-based RAG approaches. 4 EXPERIMENTS 4.1 EXPERIMENTALSETTINGS Datasets.We evaluate on two QA benchmarks:LiHuaWorld(Fan et al., 2025) andMultiHop- RAG(Tang & Yang, 2024). LiHuaWorld contains one year of English chat records from a virtual user, with queries spanning single-hop, multi-hop, and unanswerable types, each paired with annotated answers and supporting documents. MultiHop-RAG is constructed from English news articles and provides multi-hop queries with ground-truth answers and evidence. Together, these datasets cover long-term personal memory QA and news-based multi-hop reasoning. Detailed statistics are provided in Appendix A. Evaluation Metric.We evaluate the RAG system outputs using automatic exact-matchAccuracy and anLLM-as-a-Judgeprotocol (Gu et al., 2024).Accuracyis defined as the proportion of queries for which the predicted answer exactly matches the reference semantically. ForLLM-as-a-Judge, we employ a strong instruction-tuned model (Qwen2.5-72B-Instruct) as the evaluator. The judge receives the user query, candidate response(s), and the reference answer, and outputs a categorical correctness decision (correct/incorrect). To ensure reproducibility, we adopt a fixed prompt and deterministic decoding. Baselines.We compare QCG-RAG with representative RAG and graph-based RAG methods: •Naive RAG(Lewis et al., 2020): A standard baseline that retrieves relevant documents via a dense retriever and conditions an LLM generator on the retrieved content. •D2QRAG & D2Q--RAG: