Doc2Query--.Doc2Query (Nogueira et al., 2019) is a document expansion technique that trains sequence-to-sequence models (e.g., T5 (Raffel et al., 2020)) to generate queries likely associated with a given document, thereby improving retrieval by appending the generated queries to the document chunks. Doc2Query-- (Gospodinov et al., 2023) refines this approach by filtering out irrelevant or hallucinated queries based on their similarity to the original chunks, which improves retrieval quality while reducing index overhead. The queries produced by these methods naturally operate at an intermediate granularity: richer and more interpretable than fine-grained entity triples, yet more precise than coarse-grained document chunks. This property makes them well suited for integration with graph-based RAG, where query nodes enable controllable, query-centric graph (QCG) construction that balances granularity and enhances both the accuracy and interpretability of multi-hop retrieval. 3 METHODOLOGY 3.1 PRELIMINARIES We consider the task of open-domain question answering (QA) under the retrieval-augmented generation (RAG) paradigm. Formally, let C={c 1, c2, . . . , cN } denote a collection of text chunks derived from a document corpus D, where each chunk ci(i∈[1, N]) is a contiguous segment of text (e.g., a dialogue or passage). Given a user query qu ∈ Q, the goal is to generate an answer a by retrieving relevant chunks from C and conditioning a large language model (LLM) on both qu and the retrieved chunk evidence. Doc2Query.Doc2Query (Nogueira et al., 2019) is a document expansion technique that enrich the retrieval space, where each chunk ci is used as a prompt to an LLM to generate multiple synthetic query: Qg,i ={q 1 g,i, q2 g,i, . . . , qM g,i}, where qj g,i(j∈[1, M]) denotes a generated query that is grounded in the chunk content of ci. M is the number of generated queries per chunk ci. This document expansion