and memory costs. Both models were applied in a zero-shot manner without task-specific fine-tuning. During retrieval, embeddings were ℓ2-normalized before similarity computation to ensure stable and consistent cosine similarity scores. B.2 LLM SETTINGS All large language models (LLMs) were accessed via the BaiLian 6 API platform with default generation parameters: temperature= 0.7 , top p= 0.8 , top k= 20 , max input tokens= 129,024, and max tokens= 8,192 . For query generation, question answering, and evalua- tion, we primarily employed Qwen2.5-72B-Instruct. For comparison, we also considered Qwen2.5-3B-Instruct in query-centric graph construction. The 72B model was mainly applied to computationally intensive tasks requiring stronger reasoning capacity and long-context process- ing. Unless otherwise specified, all models were used in a zero-shot setting without task-specific fine-tuning. B.3 QCG-RAG HYPERPARAMETERS We report the default hyperparameter settings for each dataset and the candidate search ranges considered during tuning. 5https://huggingface.co/sentence-transformers 6https://bailian.console.aliyun.com/ 13 Default Values.For the LiHuaWorld dataset, we set M= 20 , α= 80% , h= 1 , k= 2 , n= 10 , and γ= 1.5 . For the MultiHop-RAG dataset, we set M= 20 , α= 80% , h= 1 , k= 3 , n= 15 , and γ= 1.0. Candidate Ranges.The hyperparameters were selected from the following ranges: α∈ {20%,40%,60%,80%,100%} , k∈ {1, . . . ,5} , h∈ {0,1,2} , n∈ {5,10,15,20} , and γ∈ {1.0,1.5} . Here, M is the number of generated queries per chunk, α controls the query trun- cation ratio, h denotes the number of retrieval hops, k specifies the number of retrieved query nodes, n is the number of neighbors per node, and γ is the similarity threshold. Unless otherwise specified, reported results correspond to the default settings. C LLM PROMPTS We show LLM prompts for query generation and answer evaluation, including Doc2Query Prompt, Response Generation