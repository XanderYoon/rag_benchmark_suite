vector-based RAG, thereby improving the quality and relevance of the retrieved evidence. The resulting generation quality can be systematically evalu- ated using a language model fine-tuned on natural-language inference to measure logical coherence, and another model fine-tuned on semantic similarity tasks to assess semantic relatedness. A remaining limitation of this approach is that KG traversal is primarily used for query enrichment, while the actual re- trieval step remains decoupled from the KG structure. As a re- sult, although the enriched query preserves a clear provenance trail to the KG, the final retrieval can still occur out of context, since the vector-space retriever is not directly conditioned on the KG traversal or its evolving semantic structure. This moti- vates treating KG traversal, query enrichment, and retrieval as a joint optimization problem, rather than sequential, disjoint components. Under such a formulation, the retrieval distribu- tion would be progressively refined as KG traversal unfolds, allowing graph-derived evidence to dynamically shape ranking decisions. This perspective naturally aligns with graph-based ranking losses, analogous to PageRank-style propagation, in which node importance is iteratively updated based on both structural connectivity and query-specific signals, ultimately enabling a fully graph-aware, semantically grounded retrieval process. We refer to this as KG-Path RAG and it can be formulated as follows: 1. Query-to-Graph Mapping & Traversal::Given a user queryq e, map it to initial nodes in knowledge graphG= (V, E): N(q e) ={e i ∈E:sim(q e, ei)> τ} Fig. 1. Overview of KG-Path RAG training and inference pipeline.Top: Graph-based ranking loss training process across three steps. Initially, the knowledge graph contains uncertain relationships (Step 1). During training, the system learns to identify satisfied edges (green) and filter noisy connections (red X), progressively building reliable reasoning paths through entities (e1, e2, e3, e4) to satisfy query constraints (qe) (Steps 2-3).Bottom: Inference pipeline showing