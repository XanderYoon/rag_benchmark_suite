n trained with knowledge distillation. The increase in perfo rmance is relatively substantial. Such gains usually require an in crease in model complexity, however with knowledge distillation, we are ef- fectively gaining a free boost in performance as there is no e xtra cost at inference time. The best results were obtained with a n α of 0.5. This indicates that in response retrieval, unlike ot her tasks such as sentiment classiﬁcation and natural language infer ence [16], the gold labels cannot be replaced entirely with teach er out- puts. 4.2 Prediction Eﬃciency We demonstrate the trade-oﬀ in speed and performance betwee n the BERT bi-encoder and our enhanced BERT cross-encoder. We measure the time it takes to process test samples in the DSTC7 SIGIR ’20, July 25-30, 2020, Xi’an, China Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery Table 2: Prediction quality metrics across all datasets. Metrics for models trained with knowledge distillation, which are signif- icant relative to models trained without it, are marked in bo ld. We use paired two-tailed t-tests with a p-value<0.05 to p erform signiﬁcance tests. For easier reading metrics have been mul tiplied by 100. No data augmentation has been used and traini ng samples are used as is. +KD indicates a model trained with kno wledge distillation. UDC20% UDC MANtIS DSTC7 R@1 MRR R@1 MRR R@1 MRR R@1 MRR BERT cross 66.1 76.8 76.5 84.8 59.8 72.0 36.9 47.9 BERT cross enhanced 76.2 84.5 79.5 86.9 66.7 77.3 53.3 63.3 - SubMult 73.4 82.6 — — — — — — - Attention 67.2 78.6 — — — — — — BiLSTM bi-encoder 59.2 72.4 69.4 80.2 35.6 55.1 34.3 46.1 BiLSTM bi-encoder + KD 63.0 75.2 70.4 80.8 45.5 61.4 39.4 50.1 BERT bi-encoder 64.9 76.9 72.9 82.7 47.9 58.4 39.9 51.8