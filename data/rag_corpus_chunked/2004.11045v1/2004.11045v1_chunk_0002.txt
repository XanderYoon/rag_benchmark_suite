dates according to a conversation history. By pre-training large scale language models on vast corporaand subsequently ﬁne-tuning these models on downstream tasks, re- searchers have achieved state-of-the-art results in a widevariety of natural language tasks [3]. This process has also been succe ssfully applied to the task of response retrieval [4, 6, 13]. Current state-of- the-art response retrieval focuses on using these pre-trained trans- former language models such as BERT [3]. When using a deep pre-trained transformer for the task of comparing two text i nputs, two approaches are common: either encoding representations sep- arately (bi-encoding) or encoding the concatenation of the two (cross-encoding). The BERT bi-encoder encodes two separat e rep- resentations using pre-trained deep multi-layer transfor mers and compares them using a dot product operation. The BERT cross- encoder concatenates the conversation history and candida te re- sponse and encodes them into a single representation, which is fed into a fully connected network that gives a matching score. The lat- ter method achieves better prediction quality but is far too slow for practical use [6]. While bi-encoding does give worse results, previous work ha s shown that one can signiﬁcantly reduce its inference time by pre- encoding candidate responses oﬄine so that during inferenc e, only the conversation history needs to be encoded. This, in turn, means that at inference time, bi-encoders can potentially perfor m pair- wise comparisons between a conversation history and millio ns of candidate responses. Such a feat is impossible to do with cro ss- encoders as they must recalculate encodings for each conver sa- tion history and candidate response pair. Naturally, this m akes bi- encoders a desirable solution in conversational systems where real- time response selection is required [6]. Because of this imp roving the performance of bi-encoders is a popular avenue