thus, we had to make compromises on the number of input tokens, number of negative samples, and mode l depth. 4 RESULTS AND DISCUSSION In this section, we go over the results of our experiments. We ana- lyze both prediction quality and eﬃciency. 4.1 Prediction Quality The ﬁrst two rows of table 2 demonstrate the eﬀectiveness our the enhanced BERT cross-encoder relative to the regular BERT cr oss- encoder. These results indicate that employing a task-spec iﬁc sin- gle layer cross-attention mechanism on top of separately en coded inputs is highly eﬀective for the task of response retrieval . Of par- ticular note is the increased gap between the performance of the two methods when using smaller training sets (UDC 20%, MANtIS, DSTC7). This shows that the regular bert-cross model strugg les when ﬁne-tuned with smaller response-retrieval sets and data aug- mentation or a some other method must be used to achieve accep t- able results. In contrast, our enhanced BERT cross-encoder ’s R@1 only dropped by 3.3 points when its training set was reduced t o a ﬁfth. To further demonstrate the eﬀectiveness of our modiﬁcation s to the BERT cross-encoder architecture, we perform an ablat ion study on the reduced UDC dataset. We replace the SubMult func- tion with a concatenation operation. We also try removing cr oss- attention (3). In both cases, their removal signiﬁcantly de grades model quality. Across the datasets, bi-encoders show signiﬁcant gains whe n trained with knowledge distillation. The increase in perfo rmance is relatively substantial. Such gains usually require an in crease in model complexity, however with knowledge distillation, we are ef- fectively gaining a free boost in performance as there is no e xtra cost at inference time. The best results were obtained with a n α of 0.5. This indicates