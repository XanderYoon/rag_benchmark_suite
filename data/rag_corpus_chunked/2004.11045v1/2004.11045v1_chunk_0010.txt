2. Comparing the BERT bi-encoder with and without distillation. Here we wish to demonstrate the eﬀectiveness of the knowledge distillation approach. 3. Finally, we also train a BiLSTM bi-encoder with and without distillation in order to con- ﬁrm the distillation process works with shallow student mod els. The BiLSTM bi-encoder uses the same tokens as BERT models, but their embeddings are not pre-trained and initialized rando mly. We use the same aggregation strategy (eq. 4) to aggregate the Bi LSTM hidden states. Our code will be released as open-source. 3.3 Implementation Details Our models are implemented in the PyTorch framework [12]. Fo r our BERT component, we used Distilbert [14] since it provide s re- sults somewhat close to the original implementation despit e hav- ing only 6 layers of transformers instead of 12. We tune α from a set of { 0.25, 0.5, 0.75} . We train models using Adam optimizer [8]. We use a learning rate of 5 × 10− 5 for BERT models and 10 − 3 for the BiLSTM bi-encoder. For consistency, we set the batch siz e to 8 for all models. For each dataset, we set the maximum number of tokens in the conversation history and candidate responses so that no more than 20% of inputs are truncated. Unfortunately, due to limited computing resources, we are u n- able to beat state-of-the-art results reported by [6]. Our models are trained on a single GPU; thus, we had to make compromises on the number of input tokens, number of negative samples, and mode l depth. 4 RESULTS AND DISCUSSION In this section, we go over the results of our experiments. We ana- lyze both prediction quality and eﬃciency. 4.1 Prediction Quality The ﬁrst two rows of table 2 demonstrate the eﬀectiveness our the enhanced BERT