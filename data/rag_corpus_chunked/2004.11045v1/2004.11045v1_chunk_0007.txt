= W1 · SubMult ( r ′, Att( r ′, c ′)) (3) where W1 ∈ R4d × d is a a learned parameter. We aggregate ˆc ∈ Rm× d and ˆr ∈ Rn× d by concatenating the ﬁrst token (corre- sponding to [CLS]), the max pool and average pool over the tokens: ¯c = ˆc1 ⊕ max 1≤ i ≤ m ˆci ⊕ mean 1≤ i ≤ m ˆci ¯r = ˆr1 ⊕ max 1≤ i ≤ n ˆri ⊕ mean 1≤ i ≤ n ˆri (4) We compare the aggregated ¯c, ¯r ∈ Rd vectors using a ﬁnal Sub- Mult function and a two layer fully connected network: /afii10069.ital( c, r ) = W2( ReLU ( W3 · SubMult ( ¯c, ¯r ))) where W2 ∈ R12d × d ,W3 ∈ Rd × 1 are learned parameters. Our en- hanced BERT architecture essentially encodes the conversation his- tory and candidate response tokens separately using BERT, t hen applies as single layer of cross-attention on those encodin gs. We believe our enhanced cross-encoder architecture will pe r- form better than regular cross-encoders for two reasons. Firstly, we do not concatenate conversation history and candidate resp onses. This means we can use the encoded candidate response tokens o f other samples in a training batch as negative samples [11]. S caled dot-product attention is simple enough that recalculating it for other candidates in the batch does not add signiﬁcant overhe ad, especially when compared to rerunning BERT for every possib le conversation history and candidate response pair. Thus we can pro- cess more negative samples than would be feasible in a regula r cross-encoder. Previous research has already shown that in creas- ing the number of negative samples is eﬀective for response r e- trieval [6]. Secondly, the addition of