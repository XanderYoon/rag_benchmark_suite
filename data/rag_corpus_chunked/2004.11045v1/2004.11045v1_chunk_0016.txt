An Empirical Study on Conversation Response Ranking. In European Conference on Information Retrieval. Springer. [14] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thom as Wolf. 2019. Dis- tilBERT, a distilled version of BERT: smaller, faster, chea per and lighter. arXiv preprint arXiv:1910.01108 (2019). [15] Heung-Yeung Shum, Xiao-dong He, and Di Li. 2018. From El iza to XiaoIce: chal- lenges and opportunities with social chatbots. Frontiers of Information Technol- ogy & Electronic Engineering (2018). [16] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechto mova, and Jimmy Lin. 2019. Distilling task-speciﬁc knowledge from BERT into simple neural networks. arXiv preprint arXiv:1903.12136 (2019). [17] Chongyang Tao, Wei Wu, Can Xu, Wenpeng Hu, Dongyan Zhao, and Rui Yan. 2019. Multi-Representation Fusion Network for Multi-Turn Response Selection in Retrieval-Based Chatbots. InProceedings of the Twelfth ACM International Con- ference on Web Search and Data Mining . [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. At tention is all you need. In Advances in neural information processing systems . [19] Shuohang Wang and Jing Jiang. 2017. A Compare-Aggregat e Model for Match- ing Text Sequences. In 5th International Conference on Learning Representations , Distilling Knowledge for Fast Retrieval-based Chat-bots SIGIR ’20, July 25-30, 2020, Xi’an, China ICLR 2017, Toulon, France, April 24-26, 2017, Conference Tr ack Proceedings. [20] Seunghak Yu, Nilesh Kulkarni, Haejun Lee, and Jihie Ki m. 2018. On-device neu- ral language model based word prediction. In Proceedings of the 27th Interna- tional Conference on Computational Linguistics: System De monstrations.