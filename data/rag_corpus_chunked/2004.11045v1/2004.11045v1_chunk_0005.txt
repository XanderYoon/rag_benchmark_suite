choice for ci . ti are tokens extracted from text . The goal of a model should be to learn a function /afii10069.ital( c, r ) that predicts the matching degree between any new conversation history c and a candidate response r . Once a given model ranks a set of candi- dates, its prediction quality is then measured using recall @1 (1 if the model’s ﬁrst choice is correct otherwise 0) and mean reci procal rank (MRR). 2.2 Model Architecture For the student network, we use the previously proposed BERT bi-encoder [6]. The conversation history and response cand idate tokens are encoded separately using BERT. To aggregate the ﬁ - nal layer encodings into a single vector, the ﬁrst token’s en coding, which corresponds to an individual [CLS] token, is selected . BERT requires all inputs to be prepended with this special token.The two aggregated vectors are compared using a dot-product operat ion. Similarly, our teacher model uses a BERT transformer to en- code the conversation history and candidate response. Howe ver, for comparing the last layer encodings we use a combination o f scaled dot-product attention [18] and the SubMult function [19] for calculating the matching score. Below we give a brief exp lana- tion of these components before describing how they are used . In an attention mechanism, each entry of a key vectork ∈ Rnk × d is weighted by an importance score deﬁned by its similarity to each entry of query q ∈ Rnq × d . For each entry of q the entries of k are then linearly combined with the weights to form a new rep- resentation. Scaled dot-product attention is a particular version of attention deﬁned as: Att( q, k) = so f tmax ( q · kT √ d ) ·