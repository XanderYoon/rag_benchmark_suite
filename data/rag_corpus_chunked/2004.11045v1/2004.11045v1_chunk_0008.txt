ad, especially when compared to rerunning BERT for every possib le conversation history and candidate response pair. Thus we can pro- cess more negative samples than would be feasible in a regula r cross-encoder. Previous research has already shown that in creas- ing the number of negative samples is eﬀective for response r e- trieval [6]. Secondly, the addition of the SubMult function means we can achieve much more reﬁned text matching between the con - versation history and candidate response. Distilling Knowledge for Fast Retrieval-based Chat-bots SIGIR ’20, July 25-30, 2020, Xi’an, China 2.3 Distillation Objective Distillation achieves knowledge transfer at the output lev el. The student learns from both dataset gold labels and teacher pre dicted probabilities, which are also a useful source of information [1]. For example, in sentiment classiﬁcation, certain sentences mi ght have very strong or weak polarities and binary labels are not enou gh to convey this information. Similar to previous work [16], we add a distillation objecti ve to our loss function which penalizes the mean squared error l oss between the student and teacher model outputs: Ldistill = || z( T ) − z( S ) || 2 where z( T ) , z( S ) are the teacher and student model outputs. At training time the distillation objective is used in conjunc tion with regular cross entropy loss as follows: L = α · L C E + ( 1 − α ) · L distill where α is a hyper-parameter. This procedure is model agnostic and can transfer information between entirely diﬀerent arc hitec- tures. 3 EXPERIMENTS In this section we give a brief overview of experiments setti ngs. 3.1 Datasets We consider three information-seeking conversation datasets widely used in the training of neural ranking models for response retrieval. The