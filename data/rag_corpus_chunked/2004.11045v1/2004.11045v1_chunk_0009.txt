α ) · L distill where α is a hyper-parameter. This procedure is model agnostic and can transfer information between entirely diﬀerent arc hitec- tures. 3 EXPERIMENTS In this section we give a brief overview of experiments setti ngs. 3.1 Datasets We consider three information-seeking conversation datasets widely used in the training of neural ranking models for response retrieval. The Ubuntu Dialogue Corpus (UDC) [10] and DSTC7 sentence se- lection track dataset [2] are collected form a chatroom dedi cated to the support of the Ubuntu operating system. We also includ e a version of UDC where the training set has been reduced to 20% so as to study the eﬀects of limited training data. MANtIS [13 ] was built from conversations of 14 diﬀerent sites of the Stack Exchange Network. The statistics for these datasets are provided in T able 1. Data augmentation, where each conversation is split into mu lti- ple samples, is a popular method in dialog research for boost ing the performance of response retrieval models. In this paper , we refrain from using this approach as our focus is not beating s tate- of-the-art results but empirically demonstrating the eﬀec tiveness of knowledge distillation even in limited-resource settin gs. 3.2 Baselines We divide our experiments into three parts. 1. Comparing the reg- ular BERT cross-encoder and our enhanced BERT cross-encode r. Here we aim to demonstrate the superiority of our proposed cr oss- encoder architecture 2. Comparing the BERT bi-encoder with and without distillation. Here we wish to demonstrate the eﬀectiveness of the knowledge distillation approach. 3. Finally, we also train a BiLSTM bi-encoder with and without distillation in order to con- ﬁrm the distillation process works with shallow student mod els. The BiLSTM bi-encoder uses the same tokens as BERT models, but their embeddings