responses. Such a feat is impossible to do with cro ss- encoders as they must recalculate encodings for each conver sa- tion history and candidate response pair. Naturally, this m akes bi- encoders a desirable solution in conversational systems where real- time response selection is required [6]. Because of this imp roving the performance of bi-encoders is a popular avenue of resear ch when it comes to response retrieval. In this paper, we demonstrate one possible improvement to bi - encoders, which will boost their prediction quality withou t aﬀect- ing their prediction speed. We propose transferring knowledge from the better performing BERT cross-encoder to the much fasterBERT bi-encoder. This method will raise BERT bi-encoder prediction qual- ity without increasing inference time. We employ knowledgedistil- lation, which is an approach where a model teaches another model to mimic it as a student [5]. Essentially, the student model learns to reproduce the outputs of the more complex teacher model. Unl ike gold labels, the output of a neural network is not constraine d to a binary variable and as such it can provide a much richer sign al when training the student model. Knowledge distillation ha s been successfully applied in natural language understanding, m achine translation, and language modeling tasks [7, 16, 20]. We also introduce a new cross-encoder architecture we call t he enhanced BERT cross-encoder. This architecture is speciﬁc ally de- signed for the task of response retrieval and gives better re sults SIGIR ’20, July 25-30, 2020, Xi’an, China Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery Table 1: Statistics for the datasets. UDC DSTC7 MANtIS № of candidates 10 100 11 Trn Vld Tst Trn Vld Tst Trn Vld Tst № of samples 500k 50k 50k 100k 5k 1k 82k 18k 18k than the regular BERT