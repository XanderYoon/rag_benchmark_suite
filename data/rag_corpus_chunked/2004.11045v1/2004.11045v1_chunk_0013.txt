76.2 84.5 79.5 86.9 66.7 77.3 53.3 63.3 - SubMult 73.4 82.6 — — — — — — - Attention 67.2 78.6 — — — — — — BiLSTM bi-encoder 59.2 72.4 69.4 80.2 35.6 55.1 34.3 46.1 BiLSTM bi-encoder + KD 63.0 75.2 70.4 80.8 45.5 61.4 39.4 50.1 BERT bi-encoder 64.9 76.9 72.9 82.7 47.9 58.4 39.9 51.8 BERT bi-encoder + KD 66.1 77.6 75.8 84.6 53.4 67.3 53.8 54.7 Table 3: Average milliseconds to process a single test sample. № of candidates 10 100 BERT bi-encoder 5.6 6.2 BERT cross-encoder enhanced 81.1 981.2 dataset and show the average time for each example in table 3. Time taken by the cross-encoder to process a set of candidate re- sponses grows exponentially large as the set increases in si ze. In the case of BERT bi-encoders, since candidate vectors can be com- puted oﬄine, increasing candidates has a negligible impact on in- ference time. 5 CONCLUSION AND FUTURE WORK In this paper, we introduced an enhanced BERT cross-encoder ar- chitecture modiﬁed for the task of response retrieval. Alon gside that, we utilized knowledge distillation to compress the co mplex BERT cross-encoder network as a teacher model into the stude nt BERT bi-encoder model. This increases the BERT bi-encoders pre- diction quality without aﬀecting its inference speed. We ev aluate our approach on three domain-popular datasets. The proposed meth- ods were shown to achieve statistically signiﬁcant gains. One possible avenue for research is the exploration of other knowledge transfer methods. Substituting the relatively simple BERT bi-encoder architecture with a more complex architecture [4] or de- veloping further improvements to the BERT cross-encoder are also viable alternatives. REFERENCES [1] Jimmy Ba and Rich Caruana. 2014. Do deep nets really need t o be deep?. In Advances in neural information processing systems