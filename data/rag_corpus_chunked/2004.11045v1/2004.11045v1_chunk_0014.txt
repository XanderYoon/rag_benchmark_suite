for research is the exploration of other knowledge transfer methods. Substituting the relatively simple BERT bi-encoder architecture with a more complex architecture [4] or de- veloping further improvements to the BERT cross-encoder are also viable alternatives. REFERENCES [1] Jimmy Ba and Rich Caruana. 2014. Do deep nets really need t o be deep?. In Advances in neural information processing systems . [2] Lazaros Polymenakos Chulaka Gunasekara, Jonathan K. Ku mmerfeld and Wal- ter S. Lasecki. 2019. DSTC7 Task 1: Noetic End-to-End Respon se Selection. In 7th Edition of the Dialog System Technology Challenges at AA AI 2019. [3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Langu age Understanding. In Proceedings of the 2019 Conference of the North American Chap ter of the Asso- ciation for Computational Linguistics: Human Language Tec hnologies. [4] Matthew Henderson, Iñigo Casanueva, Nikola Mrkšić, Pei -Hao Su, Ivan Vulić, et al. 2019. ConveRT: Eﬃcient and Accurate Conversational R epresentations from Transformers. arXiv preprint arXiv:1911.03688 (2019). [5] Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. 2015. Disti lling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015). [6] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Ja son Weston. 2020. Poly-encoders: architectures and pre-training strategie s for fast and accurate multi-sentence scoring. In 8th International Conference on Learning Representa- tions, ICLR 2020 . [7] Yoon Kim and Alexander M Rush. 2016. Sequence-Level Know ledge Distillation. In Proceedings of the 2016 Conference on Empirical Methods in Na tural Language Processing. [8] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for S tochastic Opti- mization. In 3rd International Conference on Learning Representations , ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedin gs. [9] Feng-Lin Li, Minghui Qiu, Haiqing Chen, Xiongwei Wang, X ing Gao,