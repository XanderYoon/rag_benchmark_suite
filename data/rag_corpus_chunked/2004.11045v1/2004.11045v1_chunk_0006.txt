similarity to each entry of query q ∈ Rnq × d . For each entry of q the entries of k are then linearly combined with the weights to form a new rep- resentation. Scaled dot-product attention is a particular version of attention deﬁned as: Att( q, k) = so f tmax ( q · kT √ d ) · k (1) The SubMult function [19] is a function designed for comparing two vectors a ∈ Rd and b ∈ Rd which has been used to great eﬀect in various text matching tasks including response retrieva l [17]. It is deﬁned as follows: SubMult ( a,b) = a ⊕ b ⊕ ( a − b) ⊕ ( a ⊙ b) (2) where ⊕ and ⊙ are concatenation and hadamard product oper- ators respectively. Utilizing these components we build our enhanced cross-encoder architecture. First, like the bi-encoder, we encode the conversation history c ∈ Rm× d and candidate response r ∈ Rn× d as follows: c ′ = T ( c) , r ′ = T ( r ) where T is the BERT transformer and c ′ ∈ Rm× d , r ′ ∈ Rn× d are the encoded tokens. To compare the encoded conversation history c ′ and encoded candidate response r ′, ﬁrst we perform a cross attention operation using the previously described components: ˆc = W1 · SubMult ( c ′, Att( c ′, r ′)) ˆr = W1 · SubMult ( r ′, Att( r ′, c ′)) (3) where W1 ∈ R4d × d is a a learned parameter. We aggregate ˆc ∈ Rm× d and ˆr ∈ Rn× d by concatenating the ﬁrst token (corre- sponding to [CLS]), the max pool and average pool over the tokens: ¯c = ˆc1 ⊕ max 1≤ i