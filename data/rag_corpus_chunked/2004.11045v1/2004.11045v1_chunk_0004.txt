and gives better re sults SIGIR ’20, July 25-30, 2020, Xi’an, China Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery Table 1: Statistics for the datasets. UDC DSTC7 MANtIS № of candidates 10 100 11 Trn Vld Tst Trn Vld Tst Trn Vld Tst № of samples 500k 50k 50k 100k 5k 1k 82k 18k 18k than the regular BERT cross-encoder. It also has the advanta ge of being faster to train. This model serves as our teacher, and w e use the BERT bi-encoder [6] as our student model. We evaluate our approach on three response retrieval data-sets. Our experi ments show that our knowledge distillation approach enhances the pre- diction quality of BERT the bi-encoder. This increase comes to a no-cost during inference time. 2 METHOD First, we explain the task in further detail. Next, we descri be the teacher and student models used for the knowledge distillat ion ap- proach. Then we describe the knowledge distillation proced ure. 2.1 Task Deﬁnition The task of response retrieval can be formalized as follows: Sup- pose we have a dataset D = { ci , ri , /y.alti } N i =1 where ci = { t1, · · · , tm } represents the conversation and ri = { t1, · · · , tn } represents a can- didate response and /y.alti ∈ { 0, 1} is a label. /y.alti = 1 means that ri is a suitable choice for ci . ti are tokens extracted from text . The goal of a model should be to learn a function /afii10069.ital( c, r ) that predicts the matching degree between any new conversation history c and a candidate response r . Once a given model ranks a set of candi- dates, its prediction quality is then measured using