nlp tasks,” Advances in Neural Information Processing Systems , vol. 33, pp. 9459–9474, 2020. [3] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval augmented language model pre-training,” in International conference on machine learning . PMLR, 2020, pp. 3929–3938. [4] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, H. Wang, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” arXiv preprint arXiv:2312.10997 , vol. 2, 2023. [5] G. Izacard and E. Grave, “Leveraging passage retrieval with gener- ative models for open domain question answering,” arXiv preprint arXiv:2007.01282, 2020. [6] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot learning with retrieval augmented language models,” arXiv preprint arXiv:2208.03299, vol. 2, no. 3, 2022. [7] Z. Jiang, L. Gao, J. Araki, H. Ding, Z. Wang, J. Callan, and G. Neubig, “Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer,” arXiv preprint arXiv:2212.02027 , 2022. [8] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval- augmented generation for knowledge-intensive nlp tasks,” Advances in Neural Information Processing Systems , vol. 33, pp. 9459–9474, 2020. [9] Y . Zhou, Z. Liu, J. Jin, J.-Y . Nie, and Z. Dou, “Metacognitive retrieval- augmented large language models,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 1453–1463. [10] H. Zeng, C. Luo, B. Jin, S. M. Sarwar, T. Wei, and H. Zamani, “Scalable and effective generative information retrieval,” in Proceedings of the ACM on Web Conference 2024 , 2024, pp. 1441–1452. [11] W. Wu, H. Yin, N. Wang, M. Xu, X. Zhao, Z.