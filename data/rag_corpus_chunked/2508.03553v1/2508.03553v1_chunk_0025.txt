Performance comparison on HotpotQA and 2WikiMultiHopQA datasets Method HotpotQA 2WikiMultiHopQA Precision Recall@5 Precision Recall@5 Standard RAG 34.1 33.5 25.6 26.2 GPT-3.5-Turbo+CoT 33.9 47.2 35.0 45.1 IRCoT 41.6 41.2 42.3 40.9 ChatKBQA 47.8 42.1 46.5 43.7 MDQA 48.6 52.5 44.1 45.8 RQ-RAG 51.6 49.3 45.3 44.6 MetaRAG 51.1 49.9 50.7 52.2 MultiRAG 59.3 62.7 55.7 61.2 making the connections between data sparser while ensuring that the query answers are still retrievable. 2) Consistency of multi-source data: We added 30%, 50%, and 70% of triple increments (the new triples are copies of the original triples) to the four pre-processed datasets, and completely shuffled the relationship edges of the added triples to disrupt the consistency of multi-source data. Subsequently, we employed MultiRAG to experiment with datasets under both perturbation schemes. Firstly, to address data sparsity, we conducted experiments on MultiRAG (Ours) and ChatKBQA (SOTA). The experi- mental results demonstrate that MultiRAG exhibits significant robustness when faced with the challenge of data sparsity. Specifically, after applying 30%, 50%, and 70% relationship masking, the F1 score of MultiRAG on the Books dataset only dropped from 66.8% to 60.0%. On the Stocks dataset, its F1 score decreased from 78.6% to 71.0%, which have been shown in Fig.5b and Fig.5d. These moderate decreases indicate that MultiRAG can effectively maintain its performance even when a substantial number of relationships are masked. In contrast, ChatKBQA’s performance decline under the same conditions is more significant. On the Books dataset, ChatKBQA’s F1 score dropped from 59.1% to 53.0%, and on the Stocks dataset, its F1 score decreased from 68.0% to 62.0%. This outcome reveals the challenges ChatKBQA faces when dealing with sparse data, especially when a large number of data connections are masked, significantly impacting its performance. Next, we conducted robustness experiments on multi-source data consistency. We perturbed the Books and