the information was not readily available in the knowledge base. Q5: What are the time costs of the two modules in MultiRAG? Intuitively, MLG aggregates homologous data from several sources, ensuring the density of the retrieval subgraphs without the need to traverse and store an excessive number of invalid nodes, thereby significantly reducing the time cost associated with traversing and querying in traditional knowledge graphs. Furthermore, although the SOTA methods are not specifi- cally tailored for low-resource, high-noise data scenarios, they still exhibit considerable robustness and retrieval performance in such environments. Both the MDQA and ChatKBQA mod- els employ LLM-based data retrieval approaches, with the primary temporal and spatial overheads focusing on token consumption and LLM-based searching. In contrast, MultiRAG concentrates its overhead on the construction of the MLG. While in the original context of the MLG, construction times are often within seconds and highly efficient, the introduction of an LLM still incurs additional temporal costs due to text generation, which remains accept- able. Ultimately, these methods all demonstrate satisfactory retrieval performance; however, due to the inherent noise in the datasets, improvements in the accuracy of question-answering are somewhat limited. D. Case Study MultiRAG’s effectiveness in multi-source integration is demonstrated through a real-world flight status query for ”CA981 from Beijing to New York”. As detailed in Table V, case study exemplifies MultiRAG’s unique strength in transforming fragmented, conflicting inputs into trustworthy answers through systematic source weighting and consensus modeling. Firstly, MultiRAG integrated three data formats: structured departure schedules, semi-structured delay codes from airline systems, and unstructured weather alerts. The MKA mod- ule extracted key relationships (flight-delay-typhoon) with a confidence score of 0.87. Subsequently, the MCC module resolved conflicts through hierarchical verification by filter- ing out low-reliability sources, such as user forums (con- fidence score of 0.47), while prioritizing data from