the characteristics of MultiRAG. All methods were implemented in a Python 3.10 and CUDA 11.6 environment. Except for the experiments using GPT-3.5-Turbo for CoT, the rest of the work utilized Llama3-8B-Instruct as the base model. For each different data format, after slicing into Chunks, we stored the slice numbers, data source locations, and transformed triple nodes in the multi-source line graph using JSON-LD format, thereby enabling simple cross-indexing. For hyperparameter settings, the temperature parameter β was set to 0.5. The number of entities in historical queries was initialized to 50, the initial node confidence threshold was defined as 0.7, and the graph confidence threshold was set to 0.5. All experiments were conducted on a device equipped with an Intel(R) Core(TM) Ultra 9 185H 2.30GHz and 512GB of memory. d) Baseline Models : To demonstrate the superiority of the MultiRAG method, we compare it with basic data fusion methods and SOTA methods, including the multi-document question-answering methods and knowledge base question- answering methods. Thanks to Zhu’s work3 [34], we compare with the following baseline methods: 3https://github.com/JunHao-Zhu/FusionQuery TABLE II: Comparison with baseline methods and SOTA methods for multi-source knowledge fusion Datasets Data source Data Fusion Methods (Baseline) SOTA Methods Our Method TF LTM IR-CoT MDQA ChatKBQA FusionQuery MCC F1/% Time/s F1/% Time/s F1/% Time/s F1/% Time/s F1/% Time/s F1/% Time/s F1/% Time/s Movies J/K 37.1 9717 41.4 1995 43.2 1567 46.2 1588 45.1 3809 53.2 122.4 52.6 98.3 J/C 41.9 7214 42.9 1884 45.0 1399 44.5 1360 42.7 3246 52.7 183.1 54.3 75.1 K/C 37.8 2199 41.2 1576 37.6 1014 45.2 987 40.4 2027 42.5 141.0 49.1 86.0 J/K/C 36.6 11225 40.8 2346 41.5 2551 49.8 2264 44.7 5151 53.6 137.8 54.8 157 Books J/C 40.2 1017 42.4 195.3s 35.2 147.6 55.7 124.2 56.1 165.0 58.5 22.7 63.5 13.66 J/X 35.5 1070