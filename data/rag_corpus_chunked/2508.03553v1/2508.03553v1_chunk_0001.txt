to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. Our code is available in https://github.com/wuwenlong123/MultiRAG. Index Terms —Retrieval Augmented Generation, Large Lan- guage Models, Multi-source Retrieval, Knowledge Graphs, Hal- lucination Mitigation I. I NTRODUCTION Large Language Models (LLMs) have achieved remarkable success in handling a variety of natural language processing tasks, attributable to their robust capabilities in understanding and generating language and symbols [1]. In knowledge- intensive retrieval tasks, Retrieval Augmented Generation (RAG) has become a standardized solution paradigm [2]– [4]. Previous works [5]–[11] have made significant strides in Wenlong Wu and Haofen Wang contributed equally to this work. Bohan Li is the corresponding author. addressing the inherent knowledge limitations of LLMs. By introducing external knowledge bases, it has markedly im- proved the accuracy and fidelity of LLM responses. However, recent studies have highlighted a significant drawback: the retrieval results of RAG are imperfect, including irrelevant, misleading, and even malicious information, ultimately leading to inaccurate LLM responses. To address these limitations, the synergy between LLMs and Knowledge Graphs (KGs) has been proposed to achieve more efficient information retrieval [12]. On one hand, KG can efficiently store data with fixed characteristics (such as temporal KGs, event KGs, etc.), thereby enhancing the processing capabilities of LLMs on specific data [13]–[20]. On the other hand, the collaboration between LLMs and KGs has significantly improved performance in multi-hop and multi-document question answering, including the credibility and interpretability of retrieval [21]. Furthermore, LLM-KG collaborative methods have also provided the latest solutions for knowledge-intensive retrieval tasks [22]–[26], propelling the deep reasoning capabilities of RAG. Nevertheless, existing frameworks still fail to account for the