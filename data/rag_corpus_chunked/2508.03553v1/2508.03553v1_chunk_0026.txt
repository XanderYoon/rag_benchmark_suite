F1 score dropped from 59.1% to 53.0%, and on the Stocks dataset, its F1 score decreased from 68.0% to 62.0%. This outcome reveals the challenges ChatKBQA faces when dealing with sparse data, especially when a large number of data connections are masked, significantly impacting its performance. Next, we conducted robustness experiments on multi-source data consistency. We perturbed the Books and Stocks datasets to varying degrees to test the performance changes of Mul- tiRAG and ChatKBQA when data consistency is disrupted. The experimental results show that MultiRAG demonstrates excellent robustness in the face of data consistency disrup- tion, while ChatKBQA’s performance declines rapidly under perturbation. Specifically, as is shown in Fig. 5a, on the Movies dataset, we added 30%, 50%, and 70% triple increments to the original dataset and randomized the relationship edges of the added triples. The results show that MultiRAG’s F1 score slightly decreased from 54.8% to 52.1%, 51.5%, and 49.9%, while ChatKBQA’s F1 score significantly dropped from 53.6% to 51.6%, 47.2%, and 40.8%. On the Flights dataset shown in Fig. 5c, we performed the same perturbation operations, and MultiRAG’s F1 score slightly decreased from 74.9% to 73.4%, 72.9%, and 71.4%, while ChatKBQA’s F1 score substantially dropped from 74.2% to 69.7%, 64.3%, and 55.8%. These results indicate that even when data consistency is severely compromised, MultiRAG can still maintain a high level of performance stability, whereas ChatKBQA’s perfor- mance is more sensitive to disruptions in data consistency. C. Evaluation of Multi-level Confidence Computing Calculating the confidence of subgraphs and nodes to filter trustworthy answers is of significant demand in critical do- mains such as finance and law. Considering the high temporal and spatial overhead of directly calculating the confidence of all nodes, we draw inspiration from the workflow of recommendation systems, mimicking the process of coarse and fine ranking,