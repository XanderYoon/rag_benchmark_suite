, 2024. [23] B. J. Guti ´errez, Y . Shu, Y . Gu, M. Yasunaga, and Y . Su, “Hipporag: Neu- robiologically inspired long-term memory for large language models,” arXiv preprint arXiv:2405.14831 , 2024. [24] C. Mavromatis and G. Karypis, “Gnn-rag: Graph neural retrieval for large language model reasoning,” arXiv preprint arXiv:2405.20139 , 2024. [25] S. Ma, C. Xu, X. Jiang, M. Li, H. Qu, C. Yang, J. Mao, and J. Guo, “Think-on-graph 2.0: Deep and faithful large language model reasoning with knowledge-guided retrieval augmented generation,” arXiv preprint arXiv:2407.10805, 2024. [26] L. Liang, M. Sun, Z. Gui, Z. Zhu, Z. Jiang, L. Zhong, Y . Qu, P. Zhao, Z. Bo, J. Yang et al., “Kag: Boosting llms in professional domains via knowledge augmented generation,” arXiv preprint arXiv:2409.13731 , 2024. [27] W. Ding, J. Li, L. Luo, and Y . Qu, “Enhancing complex question answering over knowledge graphs through evidence pattern retrieval,” in Proceedings of the ACM on Web Conference 2024 , 2024, pp. 2106– 2115. [28] X. Wang, Z. Chen, H. Wang, Z. Li, W. Guo et al. , “Large language model enhanced knowledge representation learning: A survey,” arXiv preprint arXiv:2407.00936, 2024. [29] Y . Gao, Y . Xiong, W. Wu, Z. Huang, B. Li, and H. Wang, “U-niah: Unified rag and llm evaluation for long context needle-in-a-haystack,” arXiv preprint arXiv:2503.00353 , 2025. [30] F. Wang, X. Wan, R. Sun, J. Chen, and S. ¨O. Arık, “Astute rag: Overcoming imperfect retrieval augmentation and knowledge conflicts for large language models,” arXiv preprint arXiv:2410.07176 , 2024. [31] V . Fionda and G. Pirr `o, “Learning triple embeddings from knowledge graphs,” in proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 04, 2020, pp. 3874–3881. [32] P. Yi, L. Liang, D. Zhang, Y . Chen, J. Zhu, X. Liu, K. Tang,