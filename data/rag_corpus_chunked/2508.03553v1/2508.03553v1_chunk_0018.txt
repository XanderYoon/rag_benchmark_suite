aims to answer the following questions. • Q1: How does the retrieval recall performance of Multi- RAG compare with other data fusion models and SOTA data retrieval models? • Q2: What are the respective impacts of data sparsity and data inconsistency on the quality of retrieval recall? • Q3: How effective are the two modules of MultiRAG individually? • Q4: How is the performance of MultiRAG in multi-hop Q&A datasets after incorporating multi-level confidence calculation? • Q5: What are the time costs of the various modules in MultiRAG? A. Experimental Settings a) Datasets: To validate the efficiency of multi-source line graph construction and its enhancement of retrieval performance, the article conducts multi-source data fusion experiments on four real-world benchmark datasets [35]–[37], as is shown in Table I. (1) The movie dataset comprises movie data collected from 13 sources. (2) The book dataset includes book data from 10 sources. (3) The flight dataset gathers information on over 1200 flights from 20 sources. (4) The stock dataset collects transaction data for 1000 stock symbols from 20 sources. In the experiments, we issue 100 queries for each of the four datasets to verify their retrieval efficiency. It is noteworthy that the Movies dataset and the Flights dataset are relatively dense, while the Books dataset and the Stocks dataset are relatively sparse, which can impact the model’s performance. Additionally, to validate the robustness of the MultiRAG on complex Q&A datasets, we selected two multi-hop question answering datasets, HotpotQA [38] and 2WikiMultiHopQA [39]. Both datasets are constructed based on Wikipedia doc- uments, allowing us to utilize a consistent document corpus and retriever to provide external references for LLMs. Con- sidering the constraints of experimental costs, we conducted a subsample analysis on 300 questions from the validation sets of each experimental dataset. TABLE I: Statistics of