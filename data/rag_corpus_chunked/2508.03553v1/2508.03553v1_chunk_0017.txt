in the homologous subgraph, ensuring the quality of the knowledge graph embedded in the LLM. The algorithm is shown in Algorithm1. It should be noted that the MCC algorithm does not directly provide the final graph confidence and node confidence; these values must be obtained through prompt to achieve the ulti- mate results. E. Multi-source knowledge line graph prompting We propose the Multi-source Knowledge Line Graph Prompting (MKLGP) algorithm for multi-source data retrieval. Given a user query q, LLM is firstly employed to extract the intent, entities, and relationships from q, and generates the cor- responding logical relationships. The dataset then undergoes multi-document filtering to derive text chunks, followed by constructing a Multi-source Line Graph (MLG) for knowledge aggregation. Further, it matches homogeneous subgraphs and utilizes the MCC algorithm to obtain a set of credible query nodes and isolated points SV s, LVs. Finally, by leveraging the prompt, the graph confidence is obtained, and the node confidence is calculated to enhance the credibility of the answer. The results are then embedded into the context of the LLM to generate a credible retrieval answer. IV. E XPERIMENTS This section will conduct experiments and performance analysis on the construction of homologous line graphs and the multi-level confidence calculation modules. Baseline methods will be compared with other SOTA multi-document retrieval QA methods, data fusion methods, and KBQA methods. Extensive experiments will be conducted to assess the robust- ness and efficiency of MultiRAG, which aims to answer the following questions. • Q1: How does the retrieval recall performance of Multi- RAG compare with other data fusion models and SOTA data retrieval models? • Q2: What are the respective impacts of data sparsity and data inconsistency on the quality of retrieval recall? • Q3: How effective are the two modules of MultiRAG individually? • Q4: