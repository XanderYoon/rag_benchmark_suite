question answering datasets, HotpotQA [38] and 2WikiMultiHopQA [39]. Both datasets are constructed based on Wikipedia doc- uments, allowing us to utilize a consistent document corpus and retriever to provide external references for LLMs. Con- sidering the constraints of experimental costs, we conducted a subsample analysis on 300 questions from the validation sets of each experimental dataset. TABLE I: Statistics of the datasets preprocessed Datasets Data source Sources Entities Relations Queries Movies JSON(J) 4 19701 45790 100KG(K) 5 100229 264709 CSV(C) 4 70276 184657 Books JSON(J) 3 3392 2824 100CSV(C) 3 2547 1812 XML(X) 4 2054 1509 Flights CSV(C) 10 48672 100835 100 JSON(J) 10 41939 89339 Stocks CSV(C) 10 7799 11169 100 JSON(J) 10 7759 10619 b) Evaluation Metrics: To assess effectiveness, we adopt the F1 score as the evaluation metric for the data fusion results, following previous experimental metrics [37], [40]–[42]. The F1 score is the harmonic mean of precision (P) and recall (R), calculated as follows: F 1 = 2 × P × R P + R (12) Furthermore, to evaluate the retrieval credibility of MKLGP Algorithm, we utilize the recall metric, specifically Recall@K, to assess performance at three distinct stages: before subgraph filtering, before node filtering, and after node filtering. In addition, we employ the query response time T (measured in seconds) as an evaluative metric to verify the efficiency of knowledge aggregation. c) Hyper-parameter Settings: For all baselines, we care- fully adjusted the parameters according to the characteristics of MultiRAG. All methods were implemented in a Python 3.10 and CUDA 11.6 environment. Except for the experiments using GPT-3.5-Turbo for CoT, the rest of the work utilized Llama3-8B-Instruct as the base model. For each different data format, after slicing into Chunks, we stored the slice numbers, data source locations, and transformed triple nodes in the multi-source line