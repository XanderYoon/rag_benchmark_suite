procedural modifications compared to RAG. (1) Build Knowledge Retrieval Database. As shown in the stage 1 (blue section) of Figure 3-RAD, the bonafide audio dataset ğ‘¥ â€² is segmented into smaller audio segments {ğ‘¥ğ‘› } at the ğ‘›th sample. These audio segments can be encoded to latent long feature repre- sentations n ğ‘¦â€² ğ‘›,ğ‘™ o âˆˆ Rğ‘ Ã—ğ¿Ã—ğ‘‡ â€² Ã—ğ¹ by WavLM feature extractor E (Â·), where ğ‘‡ â€² is the time dimension of long features,ğ‘ is the number of audio segments, and ğ‘™ indexes the encoder layer of WavLM. Subse- quently, two operations are performed on the long features n ğ‘¦â€² ğ‘›,ğ‘™ o : (a) The features are embedded into dense vector representations ğ‘£ğ‘›,ğ‘™ âˆˆ Rğ‘ Ã—ğ¿Ã—ğ¹ via the mapping M (Â·) in Equation 1, where each ğ‘£ğ‘›,ğ‘™ summarizes the time-wise features ğ‘¦â€² ğ‘›,ğ‘™ by temporal averag- ing to eliminate the time dimension ğ‘‡ â€²; (b) The time dimension is shortened to form short feature  ğ‘¦ğ‘›,ğ‘™ âˆˆ Rğ‘ Ã—ğ¿Ã—ğ‘‡ Ã—ğ¹ for improved efficiency by the functionS (Â·) (details in Â§ 3.4), whereğ‘‡ is the time dimension of short feature. ğ‘£ğ‘›,ğ‘™ = M  ğ‘¦â€² ğ‘›,ğ‘™  = 1 ğ‘‡ ğ‘‡âˆ‘ï¸ ğ‘¡ =1 ğ‘¦â€² ğ‘›,ğ‘™ ğ‘¡ . (1) Importantly, each embedding ğ‘£ğ‘›,ğ‘™ maintains an index linking to its original short feature ğ‘¦ğ‘›,ğ‘™, enabling the retrieval of the original audio segment ğ‘¥ğ‘› for the source content. Finally, the collection of embeddings  ğ‘£ğ‘›,ğ‘™ are stored in ğ‘™ vector databases Vğ‘™ to enable efficient similarity search and retrieval. (2) Retrieve Knowledge. As shown in the stage 2 (red section) of Figure 3-RAD, a sample to be detected Ëœğ‘¥ğ‘ can be embedded into a query embedding Ëœğ‘£ğ‘,ğ‘™ âˆˆ Rğ¿Ã—ğ¹ by function E, M, and can be con- verted to short features Ëœğ‘¦ğ‘,ğ‘™ âˆˆ Rğ¿Ã—ğ‘‡ Ã—ğ¹ by function E, S. This query embedding