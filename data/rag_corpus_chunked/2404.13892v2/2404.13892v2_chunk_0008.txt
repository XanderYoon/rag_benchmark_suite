prompt ๐, which contains both the initial query and relevant contextual information, is subsequently input to a large language model (LLM). The LLM analyzes the overall content and relationships within ๐ to generate the final answer ๐ง. 3 METHODOLOGY 3.1 Self-supervised Feature with WavLM The overall framework of our proposed method is illustrated in Fig- ure 1. Unlike traditional methods (described in ยง 2.1), our proposed framework leverages the state-of-the-art WavLM [4] feature extrac- tor and incorporates an additional retrieval module after feature extraction to overcome performance bottlenecks. Specifically, we adopt a retrieval-augmented structure similar to RAG (described in ยง 2.2), which retrieves a few similar features from the bonafide samples and fuse them with the original test features before feeding into the detection model. By incorporating retrieved features highly similar to the test sample, our model can make much more reliable predictions through joint analysis. In the following section, we describe these modules in detail, including the WavLM feature extractor (described in ยง 3.1), the retrieval augmented mechanism (described in ยง 3.2), and the design of the detection model classifier (described in ยง 3.3). Meanwhile, since this framework is complex, we present a number of speed- up techniques (described in ยง 3.4) to greatly reduce the space and time complexity. To further improve performance, we also jointly optimize the WavLM feature extractor with the detection model in an end-to-end manner (described in ยง 3.1). WavLM Feature Extraction. Recent advanced feature extractor WavLM [4] employs wav2vec 2.0 [ 1] as its backbone and is trained with larger real multilingual, multi-channel unlabeled speech data for much better performance. WavLM utilizes a masked speech de- noising and prediction framework that artificially adds noise and overlapping speech to clean input audio before masking certain time segments, and the model should then predict the