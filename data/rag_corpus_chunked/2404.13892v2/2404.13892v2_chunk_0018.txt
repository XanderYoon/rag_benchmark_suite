Applying this speedup technique enables significant savings in storage space. However, there is an additional question that needs to be experimentally verified: • Question 4: Does time-wise speedup method affect down- stream DF detection performance? This question is validated in § 4.4. 4 EXPERIMENTS The following section describes the datasets and assessment metrics (described in § 4.1) used for all of the reported experimental work, as well as details of the reproducible implementation (described in § 4.2). The experimental results (described in § 4.3) will list the evaluation results compared to the existing SOTA. The ablation studies (described in § 4.4) will then focus on several experiments related to the Four Research Questions collected from § 3.2, 3.4, and why our proposed RAD framework could be effective. 4.1 Datasets and Metrics ASVspoof 2019 LA Database. The ASVspoof 2019 [23] logical ac- cess (LA) dataset is comprised of bonafide and spoofed utterances generated using totally 19 different spoofing algorithms, including TTS, VC, and replay attacks. The dataset contains separate parti- tions for training, development, and evaluation. The training and development sets contain samples of 6 spoofing algorithms, while the evaluation set contains samples from 2 algorithms seen during training as well as 11 unseen spoofing algorithms not present in the training data. The training set trains the model, the development set selects the best-performing model, and the evaluation set impar- tially evaluates the performance of the selected model. In addition, all bonafide samples will be used to build the retrieval database. This experimental design aims to evaluate the generalization ability of the DF detection system against unknown spoofing at- tacks. Furthermore, the dataset may not contain complete bonafide recordings of all speakers that were impersonated in the spoof- ing dataset. The lack of target speaker data may limit the ability of