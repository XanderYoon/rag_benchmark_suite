feature extractor WavLM [4] employs wav2vec 2.0 [ 1] as its backbone and is trained with larger real multilingual, multi-channel unlabeled speech data for much better performance. WavLM utilizes a masked speech de- noising and prediction framework that artificially adds noise and overlapping speech to clean input audio before masking certain time segments, and the model should then predict the speech content of the original frames of these masked segments. This denoising process allows WavLM to learn robust representations that capture not only a variety of speech features but also the acoustic envi- ronment. In addition, WavLM performs excellently in a variety of downstream speech tasks such as automatic speech recognition (ASR), automatic speaker verification (ASV), and text-to-speech (TTS) with minimal fine-tuning. This suggests that WavLM already understands and is familiar with many high-level speech charac- teristics of bonafide audio, which are particularly appropriate for unseen DF-synthesized audio, since it often contains features that are very different from bonafide audio. In the proposed framework, the feature extraction component utilizes the complete set of latent features ğ‘¦ âˆˆ Rğ¿Ã—ğ‘‡ Ã—ğ¹ from all layers of the WavLM encoder transformer when processing an input audio segment ğ‘¥. Where ğ¿ is the number of WavLM encoder transformer layers, ğ‘‡ is the number of frames, ğ¹ is the dimension of features (same as WavLM feature size). This enables the model to leverage speech information encompassing low-level acoustic features as well as higher-level semantic abstractions extracted by the deeper layers. WavLM Fine-Tuning. Since WavLM is trained only on bonafide audio during pre-training, it may not have exposure to spoofed samples, which potentially leads to incorrect classifications. There- fore, we first fine-tune the entire WavLM feature extractor in an Figure 2: The baseline structure for fine-tuning. ICMR â€™24, June 10â€“14, 2024, Phuket, Thailand Zuheng Kang et al.