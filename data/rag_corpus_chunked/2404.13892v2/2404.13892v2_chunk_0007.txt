plain text and format-rich text database ğ‘¥ is partitioned into smaller chunks {ğ‘¥ğ‘› } at ğ‘›th sample. These text chunks are then embedded into dense vector represen- tations {ğ‘£ğ‘› } by a language model. In addition, each embedding ğ‘£ğ‘› maintains an index that links it to its original text chunk ğ‘¥ğ‘›, allowing the retrieval of the original text content. Finally, these em- beddings {ğ‘£ğ‘› } can be stored in a vector database V that facilitates efficient similarity search and retrieval. (2) Retrieve Knowledge. As shown in the stage 2 (red section) of Figure 3-RAG, the userâ€™s query text Ëœğ‘¥ğ‘ is embedded into a query embedding Ëœğ‘£ğ‘ using the same language model. This query embed- ding is then leveraged to perform a similarity search across the vector database V containing all the document chunk embeddings. The top ğ¾ most similar embeddings { Ëœğ‘£ğ‘˜ } related to its document chunks { Ëœğ‘¥ğ‘˜ } are retrieved based on their semantic proximity to the query embedding Ëœğ‘£ğ‘ in the vector space. These most relevant ğ¾ document chunks { Ëœğ‘¥ğ‘˜ } can be used as augmented contextual information to complement the original user query. (3) Get Results (Answer Generation). As shown in stage 3 (green section) of Figure 3-RAG, the original user query Ëœğ‘¥ğ‘ is concate- nated with the retrieved document chunks { Ëœğ‘¥ğ‘˜ } to construct an expanded prompt ğ‘ with its function P. Where ğ‘ = P Ëœğ‘¥ğ‘, { Ëœğ‘¥ğ‘˜ }. This enriched prompt ğ‘, which contains both the initial query and relevant contextual information, is subsequently input to a large language model (LLM). The LLM analyzes the overall content and relationships within ğ‘ to generate the final answer ğ‘§. 3 METHODOLOGY 3.1 Self-supervised Feature with WavLM The overall framework of our proposed method is illustrated in Fig- ure 1. Unlike traditional methods