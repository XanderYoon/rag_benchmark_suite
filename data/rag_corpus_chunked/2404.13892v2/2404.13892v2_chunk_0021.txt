Detection ICMR ’24, June 10–14, 2024, Phuket, Thailand Table 1: Comparison with other anti-spoofing systems in the ASVspoof 2019 LA evaluation set, reported in terms of pooled min t-DCF and EER (%). System Configuration min t-DCF EER(%) Hua et al. [11] DNN+ResNet 0.0481 1.64 Zhang et al. [31] FFT+SENet 0.0368 1.14 Ding et al. [6] SAMO 0.0356 1.08 Tak et al. [22] RawGAT-ST 0.0335 1.06 Jung et al. [26] AASIST 0.0275 0.83 Huang et al. [12] DFSincNet 0.0176 0.52 Fan et al. [7] f0+Res2Net 0.0159 0.47 Guo et al. [9] WavLM+MFA 0.0126 0.42 Ours WavLM+RAD-MFA 0.0115 0.40 Model Training. The front-end feature extractor utilized in this work is WavLM. During fine-tuning of the front-end WavLM, the Adam optimizer is employed with a learning rate of 3e-6 and a batch size of 4. For training the MFA, the batch size is changed to 32 and the learning rate is 3e-5. All experiments were performed utilizing two NVIDIA GeForce RTX 3090 GPUs. Each model configuration is trained for approximately 30 epochs. 4.3 Experimental Results To demonstrate the superior performance of our proposed method over existing approaches, we compare our proposed method to recent SOTA methods. Results on ASVspoof 2019 LA evaluation set. The experimental re- sults in Table 1 compare the performance of our proposed methods to existing approaches on the ASVspoof 2019 LA evaluation dataset. Our method achieves an EER of 0.40% and a min t-DCF of 0.0115 which is the best reporting result, demonstrating the effectiveness and superiority of our proposed method. Notably, although Guo et al. [9] utilizes a similar WavLM feature extractor and MFA net- work, our proposed RAD framework improves its performance, overcoming the limitations of single-model approaches. In our analysis, the RAD framework first retrieves the most sim- ilar audio samples, which are likely from the