original ClimRetrieve relevance labels. All denotes all sampled disagreed labels. Rel. ( Irr.) denotes the subset where the original label is relevant (irrelevant). Conf ≤ 0.95 (> 0.95) denotes the subset where Llama3-Tok’s confidence is lower (higher) than 0.95. ClimRetrieve (62.12%) means that 62.12% of data are annotated with > 0.95 confidence. Setting Kendall’s τ BGE-Base 35.71 BGE-Base-ft 36.34 BGE-Large 34.74 BGE-Large-ft 36.55 Table 6: Different embedding models’ performance benchmarked by student model Ms’s prediction on all 43K (query, document) pairs of ClimRetrieve. “ft” de- notes the model is fine-tuned on in-domain data. the dataset allows us to investigate our model’s ca- pabilities to counteract biases. For this purpose, we sample 200 disagreements between Llama3-Tok’s annotation and the original ClimRetrieve labels. Then, we reannotate these samples with a human labler. To account for different confidence levels in Llama3-Tok’s prediction, we differentiate predic- tion with confidence higher or lower than 0.95. As Table 5 indicates, the model can be success- fully used to overturn decisions of unseen, as irrel- evant assumed documents (91.30% for confidence > 0.95). Strikingly, even samples annotated by humans, i.e., those labeled as relevant, can be over- turned, though with a lower certainty. We attribute this to differences in the unknown mental model of the ClimRetrieve labeler and our explicit rele- vance definitions (for details, see App. L). However, for us, it is reaffirming to observe that the DIRAS model is consistent with its own definition. Thus, we conclude that DIRAS’ labeling is effective and helps to mitigate annotation selection bias. RQ4: Benchmarking IR . We use Ms to anno- tate all 43K ClimRetrieve datapoints and obtain a benchmarking dataset to select IR algorithms. This approach can be especially helpful when lacking human annotation and annotation selection bias is prevalent. Specifically, we compare the perfor- mance of