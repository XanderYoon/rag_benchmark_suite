are widely used in RAG benchmark- ing. DIRAS addresses queries for broad informa- tion and IR recall. Thus, we include long-form QA datasets from ALCE (Gao et al., 2023), including ELI5 (Fan et al., 2019), ASQA (Stelmakh et al., 2023), and QAMPARI (Amouyal et al., 2023). We also include RAG-Bench (Fang et al., 2024) that consists of questions from TriviaQA (Joshi et al., 2017), WebQ (Berant et al., 2013), and Natural Questions (Kwiatkowski et al., 2019). RAG-Bench is chosen since it has labels for partial vs. full rel- evance, which is a focus of DIRAS. Importantly, the context relevance labels from ALCE and RAG- Bench are derived from reference answers to ques- tions, using heuristics instead of manual annotation. Thus, they are to some extent noisy. We first run the DIRAS pipeline7 on each dataset to obtain corresponding Llama3-Tok models. Then we compare them with the teacher model – GPT- 6We fine-tune on the test instead of the training set to (1) leverage high-quality human annotation for fine-tuning; and (2) avoid indirect data leakage as Ms is fine-tuned on the training set. 7Different from Climate change datasets, QA datasets do not require nuanced relevance definition. Thus we use a fixed relevance definition: “the document is helpful only if its con- tent answers the query” . See full prompt in Fig. 16. ELI5 ASQA QAMPARI RAG-Bench N N@5 N@10 N N@5 N@10 N N@5 N@10 N N@5 N@10 GPT-4 48.43 15.97 17.81 64.62 38.82 46.32 56.21 27.54 35.34 42.91 31.84 41.35 Llama3-Tok 48.73 17.04 20.08 64.90 39.37 48.44 56.58 28.70 35.49 48.27 41.13 47.88 Table 7: Applying DIRAS to QA datasets, the IR performance of student model Llama3-Tok and GPT-4. N denotes nDCG. ELI5(85.05%) ASQA(66.32%) QAMPARI(72.66%) RAG-Bench(60.33%) All Rel. Irr. All Rel. Irr. All Rel. Irr. All Rel. Irr. Conf ≤