applications (Gao et al., 2024). One core phase of RAG systems is Information Retrieval (IR), which leverages cheap retrievers to filter relevant information and thus save LLM in- ference costs. However, IR can be a performance bottleneck for RAG (Chen et al., 2023; Gao et al., 2024). Both leaving out important relevant informa- tion (low recall) as well as including excessively re- lated but irrelevant information (low precision) may *Equal Contributions. 1All code, LLM generations, and human annotations in https://github.com/EdisonNi-hku/DIRAS. Query Domain-Specific Definition DIRAS-fine-tuned small LLM: Relevance Annotator User Input Output Documents For All Documents relevant irrelevant Well-Calibrated Relevance Ranking Using Relevance Score Binary Verdict: e.g. Yes Per Document Relevance Score: e.g. 0.95 Figure 1: Overview of the functionality of DIRAS tak- ing (query, relevance definition, document) triplets as input and output a binary verdict and a well-calibrated relevance score, which is sensitive to the grey-scale of partial relevance. lead to severe decrease in performance (Ni et al., 2023; Cuconasu et al., 2024; Niu et al., 2024; Schi- manski et al., 2024a). Furthermore, evaluation re- sults on general-domain benchmarks (Thakur et al., 2021) may hardly indicate the IR performance on RAG systems, as the definition of relevance varies drastically across different domains and use cases (Schimanski et al., 2024b; Bailey et al., 2008). To address these concerns, Saad-Falcon et al. (2023a) propose ARES to fine-tune an in-domain LM judge to evaluate context relevance. Although showing effectiveness in evaluating RAG systems for QA datasets like HotpotQA (Yang et al., 2018), ARES does not address two significant Challenges that real-world RAGs are faced with: C1. IR Recall : RAG systems are more gener- ally purposed than QA models, where many user queries require an integrative information analysis. For example, Write an overview of major events in World War 2; or What is