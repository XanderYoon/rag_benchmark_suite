It becomes appar- ent that the majority of documents are not relevant while still a significant number is labeled as par- tially relevant and relevant. F LLM Fine-Tuning Settings We use the default QLoRA hyperparameter settings 9, namely, an effective batch size of 32, a lora r of 64, a lora alpha of 16, a warmup ratio of 0.03, a constant learning rate scheduler, a learning rate of 0.0002, an Adam beta2 of 0.999, a max gradient norm of 0.3, a LoRA dropout of 0.1, 0 weight decay, a source max length of 2048, and a target max length of 512. We use LoRA module on all linear layers. All fine-tunings last 2 epochs. All experiments are conducted on two clusters, one with 4 V100 GPUs and the other with 4 A100 (80G) GPUs. 1 GPU hour is used per fine-tuning. 9https://github.com/jondurbin/qlora G DIRAS Prompt Template P Fig. 9 shows the full prompt DIRAS prompt tem- plate for the Chain-of-Thought setup. The non-CoT setup just excludes the â€œ[Reason]: ..."" part of the prompt. You are a helpful assistant who assists human analysts in identifying useful information within climate reports for their analysis . You will be provided with a < question > the analyst seeks to answer , a < paragraph > extracted from a lengthy report , and < background_information > that explains the < question >. < background_information > first explains the < question > and then raises examples to help you to better understand the < question >. Your job is to assess whether the < paragraph > is useful in answering the < question >. < background_information >: "{ b a c k g r o u n d _ i n f o r m a t i o n }" < question >: "{