examples to help you to better understand the < question >. Your job is to assess whether the < paragraph > is useful in answering the < question >. < background_information >: "{ b a c k g r o u n d _ i n f o r m a t i o n }" < question >: "{ question }" < paragraph >: "{ paragraph_chunk }" Is < paragraph > helpful for answering < question >? Note that the < paragraph > can be helpful even it only addresses part of the < question > without fully answering it . Provide your best guess for this question and your confidence that the guess is correct . Reply in the following format : [ Reason ]: < Reason why and how the paragraph is helpful or not helpful for answering the question . Clearly indicate your stance . > [ Guess ]: < Your most likely guess , should be one of " Yes " or " No ". > [ Confidence ]: < Give your honest confidence score between 0.0 and 1.0 about the correctness of your guess . 0 means your previous guess is very likely to be wrong , and 1 means you are very confident about the guess . > Figure 9: Full DIRAS Chain-of-Thought prompt for LLMs predicting relevance labels and calibrating. H Alternative Prompts Fig. 9, Fig. 10, and Fig. 11 show the alternative prompts with which we experimented. I Creation of the Improved Relevance Definitions Fig. 14 shows the prompt for the creation process of the improved relevance definitions. Following the procedure in Schimanski et al. (2024b), we make use of the text parts labeled as relevant. There exists a relevance score from 1-3 where 1 signals the least and 3 is