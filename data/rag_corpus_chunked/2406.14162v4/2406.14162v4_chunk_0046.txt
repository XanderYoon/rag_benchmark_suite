annotation. We use 31 climate-related queries for data sampling among 80 climate reports. For the separation of the train dataset and test dataset, these queries are clas- sified into 3 categories: vague, specific, and TCFD. Within each category, queries are split randomly for training and testing, resulting in 20 queries for the train set and 11 queries for the test set. Meanwhile, the 80 climate reports are randomly split into 50 for train and 30 for test. E Expert Annotation Process As described in App. D, the data is obtained from real climate reports and split into chunks of around 150 words with the IBM deepsearch parser (Team, 2022). Table 9 shows an overview of statistical properties of the number of words in test set data. Then, we form a group of three expert annota- tors. The expert annotators comprise one graduate and one PhD student working in NLP for climate change. These two experts label the entire dataset with three labels: the document is relevant, partially relevant, or not relevant for the query including the definition. Following a simple annotation guide- line: • Please first carefully read the provided rele- vance definition to understand what the ques- tion is looking for. The definition consists of a question explanation and examples of relevant information. • If a paragraph clearly falls into the definition of relevance, i.e., explicitly mentioned by the question explanation or examples, please an- notate relevant. • If the paragraph is not explicitly covered by the definition but you think it somehow helps answering the question. Please annotate par- tially relevant. • Otherwise please annotate irrelevant. Additionally, one PhD student focusing on cli- mate change and sustainability research serves as a subject-matter meta-annotator to resolve conflicts or investigate cases where both labelers arise at the label partially. Comparing