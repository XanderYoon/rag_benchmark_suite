give a Prompt: <question>: What is the firm’s Scope 3 emission? <question_definition>: This question is looking for infor- mation about the firm’s emission in ... <paragraph>: {one text chunk from a climate report} Is <paragraph> helpful for answering <question>? Pro- vide your best guess, and confidence score from 0 to 1. Teacher LLM Mt: [Reason]: {Reason why the paragraph is (un)helpful.} [Guess]: {Yes or No.} [Confidence]: {confidence score between 0.0 and 1.0.} Figure 3: Our prompt template P for distilling training data from Mt. “[Reason]” is only used in the CoT setup. It is shortened for presentation. Full P is in App. Fig. 9. Setting Unc. Bin. Cal. Info. Avg. List-2/1 - - - 76.86 - List-2/1-D - - - 74.72 - List-10/5 - - - 84.74 - List-10/5-D - - - 84.45 - List-20/10 - - - 78.05 - List-20/10-D - - - 82.54 - RAGAs - 68.15 - 37.13 - ARES-0-Shot 25.48 52.63 79.35 77.67 58.79 ARES-2-Shot 17.38 3.85 63.49 44.97 32.42 ARES-4-Shot 18.16 5.13 69.51 49.68 35.62 ARES-8-Shot 16.99 28.81 66.77 48.75 40.33 ARES-16-Shot 20.65 24.20 64.76 44.85 38.61 Point-Ask 39.27 84.07 90.59 87.57 75.37 Point-Ask-Prob-D 44.74 84.52 91.31 88.39 77.24 Point-Tok-D 28.83 86.32 84.48 80.90 70.53 Point-Ask-D 54.01 86.32 91.10 88.48 80.00 Table 1: GPT-4’s performance on ChatReportRetrieve test set with different ranking methods (Point- or List- wise), RAGAs and ARES-few-shot, with/without rel- evance definition (D), and calibration method (Ask or Tok). Bin., Cal., Info., and Unc. stand for evaluation dimensions in § 3.1. calibrated relevance probability which can be used to rank documents for each query. To directly eval- uate the ranking performance, we measure nDCG and MAP upon relevance labels. (4) Uncertainty: If the models understand the difficulty caused by partial relevance, they should have lower confi- dence scores on samples that humans found