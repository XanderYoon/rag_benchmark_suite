Ask): One calibra- tion method is to get the relevance confidence by probing the model’s generation probability of the token Yes/No when predicting a document’s rel- evance (Tok, Liang et al., 2023). An alternative way is directly asking LLMs to verbalize confi- dence score, which may work better for instruction following LLMs (Ask, Tian et al., 2023). With vs. without relevance definition: As Cha- tReportRetrieve test data is annotated based on the relevance definition, performance should increase if the model correctly takes the in-context relevance definition into consideration. Following the takeaways of Thomas et al. (2024), we design the prompt P for the pointwise method, relevance definition and CoT prompting (see Fig. 3 and Fig. 9, prompt without definition in Fig. 11). We use the listwise ranking prompt from Sun et al. (2023b) and Pradeep et al. (2023) (see prompt with- /without definition in Fig. 13/Fig. 12). For the pointwise method, we run one variation to test prompt sensitivity: directly asking for relevance probability instead of confidence for guess (prompt in Fig. 10). As the listwise ranking is sensitive to window/step size, we run three variations with window/step sizes of 2/1, 10/5, and 20/10. text- embedding-3-small is used for listwise methods’ initial ranking. ARES and RAGAs settings are from the original papers. For few-shot ICL, we keep relevant/irrelevant samples balanced to avoid bias. Takeaways: Results in Table 1 show that: (1) Few- shot ICL fails to teach domain-specific relevance. The ICL illustrations (even balanced) seem to bias 3Original ARES uses few-shot ICL to create synthetic queries instead of relevance labels, which is not applicable for ChatReportRetrieve. Thus we use their ChatGPT prompt for relevance judgement, while replacing GPT-3.5 with GPT-4. Setting Unc. Bin. Cal. Info. Avg. Small-embed - - - 66.34 - Large-embed - - - 69.36 - BGE-Gemma -