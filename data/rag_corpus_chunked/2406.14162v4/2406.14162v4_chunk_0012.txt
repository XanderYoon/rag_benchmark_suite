cases, while that improves the pointwise performance. Thus we choose pointwise to be our distillation strategy. 3.3 Optimizing DIRAS student LLMs DIRAS student LLMs Ms will be used to annotate all (query, document) combinations. Two method- ological choices might influence the quality of fine- tuned student LLMs. First, we explore the role of Chain-of-Thought (CoT) reasoning. Second, we investigate the choice of calibration method (Tian et al., 2023). To explore these aspects, we fine-tune Ms in four settings: Ms-CoT-Ask, Ms-CoT-Tok, Ms-Ask, Ms-Tok, where CoT meansMs is tuned to generate [Reason], [Guess], and [Confidence]; without CoT denotes Ms is tuned to only generate [Guess] and [Confidence]; “Ask” means the result is calibrated by the generated confidence score in [Confidence] field; and “Tok” means we take the token-level probability of “Yes/No” after “[Guess]:” as the confidence score for calibration. The prompt in Fig. 3 is used for fine-tuning. The “[Reason]:” line is removed in settings without CoT. We fine-tune Llama-3-8B-instruct (AI@Meta, 2024), gemma-7b-it (Team et al., 2024b), and Phi- CoT_Ask Ask CoT_T ok T ok 0.2 0.4 0.6 0.8Avg. Scores Llama-3-8B-Instruct CoT_Ask Ask CoT_T ok T ok Phi-3-mini-4k-instruct CoT_Ask Ask CoT_T ok T ok Gemma-7b-it Figure 4: Shaded bars denote the performance of orig- inal models. Colored bars denote the improvement brought by fine-tuning. 3-mini-4k-instruct (Abdin et al., 2024) (details in App. F). We compare these fine-tuned student mod- els with baselines including GPT-3.5 and GPT-4 us- ing prompt P; the OpenAI embedding models text- embedding-3-small, and text-embedding-3-large; and BGE Gemma reranker4, a popular LLM-based reranker for general domain. As Fig. 4 shows, fine-tuning improves original models in all settings. Furthermore, Table 2 shows the results of all fine-tuned student models in com- parison to all baselines. We observe that Ms-Tok outperforms other settings for all LLM architec- tures. The best setting