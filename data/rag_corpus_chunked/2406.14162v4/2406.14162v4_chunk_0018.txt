we conclude that DIRAS’ labeling is effective and helps to mitigate annotation selection bias. RQ4: Benchmarking IR . We use Ms to anno- tate all 43K ClimRetrieve datapoints and obtain a benchmarking dataset to select IR algorithms. This approach can be especially helpful when lacking human annotation and annotation selection bias is prevalent. Specifically, we compare the perfor- mance of embedding models before and after in- domain fine-tuning. If the Ms-annotated bench- mark gives higher scores to the fine-tuned check- points, that means it is capable of selecting a better model for this specific domain. For this experiment, we first fine-tune open- sourced embedding models bge-large-en-v1.5 and bge-base-en-v1.5 (Chen et al., 2024) on Cha- tReportRetrieve test set 6 (fine-tuning details in App. K). We then compare embedding models’ relevance ranking with the predicted ranking of Llama3-Tok on all 43K (query, document) pairs in ClimRetrieve. We use Kendall’s τ as the metric, which directly compares the correlation between two ranks. The results are shown in Table 6. We find the Llama3-Tok-annotated benchmark success- fully picks out the fine-tuned checkpoints, showing a capability of benchmarking information retrieval algorithms. Interestingly, the unfine-tuned BGE- Base correlates more to Llama3-Tok compared to BGE-Large, although the latter shows stronger per- formance on MTEB (Muennighoff et al., 2023). This indicates the necessity of domain-specific benchmarking to tell the in-domain performance. 4.2 Applying to QA Datasets In this section, we apply the DIRAS pipeline to QA datasets that are widely used in RAG benchmark- ing. DIRAS addresses queries for broad informa- tion and IR recall. Thus, we include long-form QA datasets from ALCE (Gao et al., 2023), including ELI5 (Fan et al., 2019), ASQA (Stelmakh et al., 2023), and QAMPARI (Amouyal et al., 2023). We also include RAG-Bench (Fang et al., 2024) that consists of questions from TriviaQA