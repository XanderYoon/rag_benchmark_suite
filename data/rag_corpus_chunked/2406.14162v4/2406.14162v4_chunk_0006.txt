ChatReportRetrieve (§ 3.1). Sec- ond, we assess design choices and related work baselines to find the best-performing strategy for our relevance-annotation data creation with the teacher model Mt (§ 3.2). Third, we optimize the fine-tuning of student LLMs Ms by investigating implementation variants (§ 3.3). 3.1 ChatReportRetrieve To evaluate the teacher LLMs’ (Mt) and student LLMs’ (Ms) comprehension of nuanced relevance definitions, we need to provide them with the same annotation guidelines (i.e., relevance definitions) and compare their annotation performance. To the best of our knowledge, there is no existing IR dataset that provides a nuanced relevance definition for each query. Hence, we annotate ChatReportRe- trieve for our evaluation. Data and annotation guideline preparation: Cha- tReportRetrieve is based on real-user integrative queries from ChatReport – a chat tool for answer- ing climate-related questions based on corporate reports2 (Ni et al., 2023). We sample a wide range of climate reports and representative user queries about the reports to construct ChatReportRetrieve. Then, we conduct a train-test split, making sure no test set report or query is seen in the training 2https://reports.chatclimate.ai/ set. Finally, we prompt GPT-4 to draft relevance definitions for all queries. GPT-4 drafted relevance definitions show a good understanding of the cli- mate disclosure domain, according to a domain expert’s feedback. See Appendices for details of data preprocessing (App. D) and relevance defini- tion generation (App. B). Test Data Annotation: We leverage relevance def- initions as the annotation guidelines. If and only if a document addresses the relevance definition, it is deemed as (partially) relevant. We explicitly account for partial relevance when the document addresses the periphery of the definition. The data labeling process follows two steps. First, we em- ploy two annotators who independently annotate all test data to be either relevant, irrelevant, or par- tially relevant. Second, we