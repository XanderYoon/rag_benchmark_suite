are used as context rele- vance labels in our experiment. For each dataset, we randomly sample 100 queries to construct DI- RAS training data, following the process in Fig. 2. Top-5 is selected for balanced sampling, thus re- sulting in 1000 (query, document) pairs for training. We sample 50 questions for test data, and include all (query, document) pairs for them, resulting in 5K (query, document) pairs for each dataset. RAG-Bench Data: RAG-Bench classifies RAG sources into four types: (A) relevant and with an- swers, (B) relevant topic but without answers, (C) irrelevant topic, and (D) with counterfactual an- swers. We find (B) addresses partial relevance that DIRAS cares about. Therefore, we leverage its dev set for training and test set for testing, where (A) and (D) become relevant documents, and (B) and (C) are used as irrelevant ones. Disagreement Sampling for Table 8: We sample 200 disagreed annotations four each dataset, 50 samples from each confidence range: Conf < 90, 90 <Conf< 95, 95 <Conf< 98, and 98 <Conf< 100 to balancedly cover different confidence scores (these bins are of similar size). 11Data files in https://github.com/princeton-nlp/ ALCE """ An analyst posts a < question > about a climate report . Your task is to explain the < question > in the context of climate reporting . Please first explain the meaning of the < question > , i . e . , meaning of the question itself and the concepts mentioned . And then give a list of examples , showing what information from the climate report the analyst is looking for by posting this < question >. For < the question 's meaning > , please start by repeating the question in the following format : ''' The question " < question >" is asking for information about