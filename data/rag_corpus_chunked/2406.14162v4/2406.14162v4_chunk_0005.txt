for each query. While sampling in top-k aims at covering some relevant documents, sampling outside of top-k en- sures covering the broader distribution of (query, document) pairs. Obtaining Relevance Definitions: Each query in the sampled (query, document) pairs needs to be accompanied by an explicit definition indicating what is relevant or irrelevant to the question. The relevance definition can be generated by human experts, LLMs, or in a collaboration of both. In our experiments, we find GPT-4 generates suitable relevance definitions using the prompt in App. B. Chunkify & Dense Search Non-Top-k Top-k Balanced Sampling Fine-tuning (small LLM): 1. Annotate all 2. Nice calibration 3. Predict human uncertainty : [Guess]: Yes. [Confidence]: 0.9 Prompt: is helpful for answering ? Generate Training Data Reports explains or Relevance Definition Figure 2: DIRAS pipeline. Domain-specific queries, and documents as inputs; calibrated student LLM annotators as outputs. Distilling Relevance Annotations from Teacher to Student : With the sampled (query, defini- tion, document) triplets, DIRAS creates relevance- annotation data with a SOTA generic teacher LLM Mt and the prompt template P (illustrated in Fig. 3). Finally, the created data is used to fine-tune student LLMs Ms, which will be used to conduct broad binary relevance annotation with confidence scores for calibration (Tian et al., 2023). 3 Optimizing Relevance Annotation To train a highly performant student LLM Ms for relevance annotation, we proceed in three steps. First, we annotate a novel task-specific dataset for our evaluation, ChatReportRetrieve (§ 3.1). Sec- ond, we assess design choices and related work baselines to find the best-performing strategy for our relevance-annotation data creation with the teacher model Mt (§ 3.2). Third, we optimize the fine-tuning of student LLMs Ms by investigating implementation variants (§ 3.3). 3.1 ChatReportRetrieve To evaluate the teacher LLMs’ (Mt) and student LLMs’ (Ms) comprehension of nuanced