model Llama3-Tok model is effective in mitigating anno- tation selection bias and therefore incorporates the perspective of IR recall (see Table 5). However, it is interesting that our student model Llama3-Tok successfully overrules human deci- sions. We attribute this to the fact that our created relevance definitions might differ from the mental model of the human annotator in ClimeRetrieve. Thus, humans in ClimRetrieve might have been consistent with their own mental model. For us, however, it is more important and reaffirming to observe that Llama3-Tok is consistent with its own, explicit relevance definitions. We can view the (query, definition, document) pair in Fig. 15 as an example. When analyzing the query "Do the environmental/sustainability tar- gets set by the company reference external climate change adaptation goals/targets?", the ClimRe- trieve labeler interpreted the question broader, i.e., deeming this as relevant: "As a global technology leader, we are also committed to helping build the enabling societal conditions that will support a net zero economy.". However, for our student model, it is in line with the definition to assign a "not rele- vant" label. There is no explicit standard mentioned in the document. M Implementation Details of Experiments on QA Datasets ALCE Data: We obtain ELI5, ASQA, and QAM- PARI from ALCE (Gao et al., 2023), where they parse the original open-domain QAs into RAG forms11. For each question, ALCE annotates 5 documents as oracle based on retrieval recall and reference answers, which are used as context rele- vance labels in our experiment. For each dataset, we randomly sample 100 queries to construct DI- RAS training data, following the process in Fig. 2. Top-5 is selected for balanced sampling, thus re- sulting in 1000 (query, document) pairs for training. We sample 50 questions for test data, and include all (query, document) pairs for