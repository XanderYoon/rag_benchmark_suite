for QA datasets like HotpotQA (Yang et al., 2018), ARES does not address two significant Challenges that real-world RAGs are faced with: C1. IR Recall : RAG systems are more gener- ally purposed than QA models, where many user queries require an integrative information analysis. For example, Write an overview of major events in World War 2; or What is good news for the stock market today? For such integrative queries, a good IR recall is necessary for comprehensive responses. Saad-Falcon et al.â€™s (2023a) approach evaluates the relevance of retrieved contexts (precision), but is agnostic to other important information that the RAG retriever might leave out (recall). Besides, current RAG literature (Yan et al., 2024; Wang arXiv:2406.14162v4 [cs.IR] 23 Jan 2025 et al., 2024) mostly relies on QA datasets (Joshi et al., 2017; Yang et al., 2018; Dinan et al., 2019; Trivedi et al., 2022) for evaluation, where the ques- tions are less integrative and can mostly be an- swered by specific facts from one or few sources. For such questions, IR precision is more important than recall. As a result, the challenges of IR recall for integrative queries are heavily under-explored. C2. Relevance Definitions and Partial Rele- vance: To thoroughly gather relevant information for integrative queries, the IR model should go beyond shallow semantic relationships and con- sider domain-specific relevance definitions. Fur- thermore, domain-specific requirements and sub- jectivity in IR annotation create a rich gray scale of partial relevance between relevant and irrele- vant (Bailey et al., 2008; Saracevic, 2008; Thomas et al., 2024; also see App. A). However, partial relevance is neglected entirely in RAG context rel- evance evaluation (Saad-Falcon et al., 2023a; Es et al., 2024). As a combined solution for these challenges, we propose DIRAS, a framework for efficient and effective relevance annotation. To address C1,