information. Therefore, with con- trolled annotation budget (i.e., number of (query, document) pairs), we are prone to have fewer (but representative) queries and more documents for each query. The queries are strategically sampled to ensure representativeness and diversity. Specifi- cally, 11 queries are the core queries used in Cha- tReport, which cover essential topics of sustainabil- ity disclosure. 20 questions are selected from users’ customized questions posed to the ChatReport tool. Climate reports are sampled randomly from openly accessible user submissions 8 Finally, we prompt GPT-4 to draft relevance definitions for all queries (see App. B). PDF Parsing : We use IBM deepsearch parser (Team, 2022) to parse corporate reports into chunks. For chunks shorther than 120 tokens, we concate- nate them with adjacent chunks to form chunks longer than 120. Figure Fig. 8 shows the formatted chunks length distribution. Train-Test Split: We split the questions into 11 for testing and 20 for training. Similarly, we split the reports into 30 for testing and 50 for training. This ensures the evaluation on unseen queries and reports. For each query, we randomly sample 60 documents – 30 each from in the top-5 and outside the top-5 (using OpenAI text-embedding-3-small as the dense retriever). Ultimately, (query, doc- 8See https://github.com/EdisonNi-hku/ chatreport. ument) pairs in training split are used to create training data with relevance label and confidence score predictions (details in § 3.2). Data points in the test split are passed to human annotation. We use 31 climate-related queries for data sampling among 80 climate reports. For the separation of the train dataset and test dataset, these queries are clas- sified into 3 categories: vague, specific, and TCFD. Within each category, queries are split randomly for training and testing, resulting in 20 queries for the train set and 11 queries for the test