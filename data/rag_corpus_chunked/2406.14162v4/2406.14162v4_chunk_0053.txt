query }. { passages } Search Query : { query }. Here are some background information that explains the query : { r e l e v a n c e _ d e f i n i t i o n } Rank the { num } passages above based on their relevance to the search query . All the passages should be included and listed using identifiers , in descending order of relevance . The output format should be [] > [] , e . g . , [4] > [2]. Only respond with the ranking results , do not say any word or explain . Figure 13: Listwise prompt with an extra input of ex- plicit definition. J MAP and nDCG Scores for Different Relevance Definitions In the improved definition experiment, we compare two settings. First, we compare the predictions on the 595 relevant-only (query, document) pairs. This is a replication of the setting in ??. Since we do not have non-relevant samples, we can only compare the nDCG. Table 12 shows the results. It becomes apparent that only for the general nDCG score, the improved query performs better. For the nDCG@5, and nDCG@10, the best-performing model remains with the generic prompt. The pic- ture turns again when widening to nDCG@15. This could be a result of the definition creation. We use examples of relevance labels 2 and 3 to create the improved definition with GPT-4. Thus, we implic- itly equalize relevance 2 and 3 in importance. This means we are likely less effective in differentiat- Question Generic Definition Expert-informed Definition Does the company provide definitions for climate change adaptation? " Meaning of the question : The question "" Does the company provide definitions for climate change adaptation ?"" is asking for information about whether the company