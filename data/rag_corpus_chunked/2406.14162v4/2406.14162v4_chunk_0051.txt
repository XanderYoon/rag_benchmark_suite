You will be provided with a < question > the analyst seeks to answer , and a < paragraph > extracted from a lengthy report . Your job is to assess whether the < paragraph > is useful in answering the < question >. < question >: "{ question }" < paragraph >: "{ paragraph_chunk }" { Same output requirements } Figure 11: Task description and input part for the alter- native prompt setting Pw/o_e. Output requirements are the same as Fig. 9. (2024b), we use the text samples with a score of 2 or higher to create the improved relevance def- inition. We include relevant text samples in the prompt for creating the relevance definitions to ob- tain improved definitions. The logic behind this definition creation is that we assume we know the mental model of ClimRetrieve human analysts, and thus know what information is relevant before an- notation. This is common in corporate report anal- ysis where experts will have fixed concepts in their heads, maybe even inspired by prior search pro- cesses. Plugging the examples into the prompt results in a set of improved relevance definitions. When com- paring these relevance definitions to the generic ones, it becomes apparent that GPT-4 already in- corporated the majority of the concepts that the ex- perts were looking for. Therefore, the adjustment of the relevance definition is visible but rather sub- tle. One example is displayed in Table 11. While the meaning of the question remains rather static, there are nuanced differences in the examples that guide the relevance labeling. <| system | > You are RankLLM , an intelligent assistant that can rank passages based on their relevancy to the query . <| user | > I will provide you with { num } passages , each indicated by