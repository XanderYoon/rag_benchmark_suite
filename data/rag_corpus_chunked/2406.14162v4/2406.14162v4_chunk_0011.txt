ICL illustrations (even balanced) seem to bias 3Original ARES uses few-shot ICL to create synthetic queries instead of relevance labels, which is not applicable for ChatReportRetrieve. Thus we use their ChatGPT prompt for relevance judgement, while replacing GPT-3.5 with GPT-4. Setting Unc. Bin. Cal. Info. Avg. Small-embed - - - 66.34 - Large-embed - - - 69.36 - BGE-Gemma - - - 68.47 - GPT-3.5 29.71 45.27 85.46 74.16 58.65 GPT-4 54.01 86.32 91.10 88.48 80.00 Llama3-CoT-Ask 36.57 76.58 89.30 86.15 72.15 Llama3-CoT-Tok 41.74 76.58 86.61 85.96 72.72 Llama3-Ask 40.18 82.11 90.14 86.02 74.61 Llama3-Tok 41.60 82.11 91.35 89.19 76.06† Phi3-CoT-Ask 36.08 72.95 88.76 80.56 69.59 Phi3-CoT-Tok 35.49 72.95 84.20 80.64 68.32 Phi3-Ask 32.30 73.23 85.56 80.05 67.79 Phi3-Tok 38.00 73.23 89.52 86.94 71.92 † Gemma-CoT-Ask 31.60 72.38 86.38 81.39 67.94 Gemma-CoT-Tok 39.03 72.38 83.49 80.33 68.81 Gemma-Ask 25.74 67.13 81.80 77.43 63.02 Gemma-Tok 50.72 67.13 90.07 81.17 72.27 † Table 2: Comparison between the fine-tuned student Ms and different baselines on ChatReportRetrieve test data. The best scores are bolded and the second bests are underlined.† denotes the best score achieved by each backbone LLM. GPT-4 to underperform the zero-shot setting. (2) With the proper calibration method (Ask), the point- wise method outperforms the listwise method. (3) The listwise method is sensitive to window size, while the pointwise method gives more consistent performance across prompts. (4) Adding a rele- vance definition drops the listwise performance in 2 out of 3 cases, while that improves the pointwise performance. Thus we choose pointwise to be our distillation strategy. 3.3 Optimizing DIRAS student LLMs DIRAS student LLMs Ms will be used to annotate all (query, document) combinations. Two method- ological choices might influence the quality of fine- tuned student LLMs. First, we explore the role of Chain-of-Thought (CoT) reasoning. Second, we investigate the