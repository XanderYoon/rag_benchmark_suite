to draft the relevance definitions. To investigate the effect of improved definitions, we compare two setups: (1) The generic relevance definition: the definition drafted by GPT-4. (2) The improved relevance defi- nition: The only way to improve the definition is to align it closer to ClimRetrieve annotators’ men- tal model of document relevance. We achieve this by adding relevant text samples to the prompt for generating the definition with GPT-4 (see App. I for details). After creating the improved definitions, we re- peat predicting the relevance scores with Llama3- Tok. Since we involve examples with various rele- vance scores to improve relevance definitions, these definitions might not help distinguish the granular level of partial relevance. However, the improved definitions might especially help to distinguish rel- evant documents from irrelevant ones. Calculating the nDCG and MAP score for all 43K (query, doc- ument) pairs, we find evidence for this notion (see Table 4). Thus, the inclusion of improved defini- tions seems to enhance the performance (see App. J for details). RQ3: Mitigating Annotation Selection Bias . ClimRetrieve employs a real-world analyst sce- nario. This entails that the human only selec- tively annotates documents that are likely to be relevant and assumes unannotated documents as irrelevant (see e.g., Thakur et al., 2021). Therefore, ClimRetrieve (62.00%) All Rel. Irr. Conf ≤ 0.95 36.84 27.90 48.48 Conf > 0.95 85.48 78.18 91.30 Table 5: Accuracy of student model Llama3-Tok annota- tions that disagree with original ClimRetrieve relevance labels. All denotes all sampled disagreed labels. Rel. ( Irr.) denotes the subset where the original label is relevant (irrelevant). Conf ≤ 0.95 (> 0.95) denotes the subset where Llama3-Tok’s confidence is lower (higher) than 0.95. ClimRetrieve (62.12%) means that 62.12% of data are annotated with > 0.95 confidence. Setting Kendall’s τ BGE-Base 35.71 BGE-Base-ft 36.34 BGE-Large