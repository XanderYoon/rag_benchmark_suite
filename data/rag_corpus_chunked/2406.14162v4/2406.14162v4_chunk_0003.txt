between relevant and irrele- vant (Bailey et al., 2008; Saracevic, 2008; Thomas et al., 2024; also see App. A). However, partial relevance is neglected entirely in RAG context rel- evance evaluation (Saad-Falcon et al., 2023a; Es et al., 2024). As a combined solution for these challenges, we propose DIRAS, a framework for efficient and effective relevance annotation. To address C1, DIRAS distills relevance annotation ability from SOTA generic teacher LLMs to small student LLMs, which can cost-efficiently annotate broad (query, document) pairs for IR recall evaluation. For better efficiency, student LLMs conduct point- wise annotation – annotating (query, document) pairs one-by-one – which is under-explored in re- lated work (Sun et al., 2023a; Qin et al., 2024) but achieves good performance for relevance annota- tion. To address C2, DIRAS student LLMs are trained to comprehend nuanced relevance defini- tions, thus handling queries with various require- ments. The student LLMs annotate binary rele- vance labels with well-calibrated relevance scores. Thus, the relevance scores can be used for relevance ranking and to calibrate the annotation accuracy (Ni et al., 2024). Thereby, these continuous rele- vance scores also address the grayscale of partial relevance (see Fig. 1 for an illustration of DIRAS functionality). We evaluate DIRAS in three steps. First, we annotate ChatReportRetrieve to evaluate the design decisions for making DIRAS models optimized rel- evance annotators (§ 3.1). The evaluation shows that the fine-tuned student ( ≤ 8B) LLMs effec- tively understand nuanced relevance definitions – achieving GPT-4-level performance (§ 3.3). Sec- ond, we showcase how DIRAS assists in real-life IR annotation by re-annotating ClimRetrieve (Schi- manski et al., 2024b). Results show that DIRAS student LLMs can effectively capture partial rel- evance, leverage improved relevance definitions, mitigate annotation selection bias, and annotate benchmarking datasets for IR algorithms (§ 4.1). Third, we re-annotate document