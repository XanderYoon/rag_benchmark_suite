52.77 54.45 Small-embed 74.52 61.28 60.36 61.69 Large-embed 76.30 63.13 63.36 64.67 GPT-3.5 74.62 60.08 61.49 61.91 GPT-4 75.55 60.89 63.23 65.26 Llama3-Ask 77.23 67.60 66.18 67.57 Llama3-Tok 76.55 67.20 66.23 65.83 Table 3: Performance on ranking the relevant (query, document) pairs in ClimRetrieve. 4.1 Applying to ClimRetrieve ClimRetrieve (Schimanski et al., 2024b) records human analysts’ real-life workflow of reading full reports, searching for relevant information, and annotating useful information for climate-related questions with relevance scores 1-3. ClimRetrieve contains 43K (query, document) pairs (8K unique documents but each document in a report is multi- plied by the amount of analyzed queries per report) out of which 595 are gold labels for relevant (query, document) pairs. Other not annotated (query, docu- ment) combinations might be either irrelevant or a part of annotation selection bias – a widely exist- ing problem in IR annotation (Thakur et al., 2021). To succeed on this dataset, the IR model needs to (1) capture the analysts’ mental model about use- ful information (i.e., relevance definition), and (2) understand fine-grained differences in degree of relevance (score 1-3). Since ClimRetrieve is still in the climate domain, we re-annotate its data with the best Ms in § 3 (Llama3-Tok), and explore whether Ms’s annota- tions can (1) RQ1: reflect fine-grained differences in degree of relevance; (2) RQ2: be improved through refining relevance definitions; (3) RQ3: mitigate annotation selection bias for better IR re- call evaluation; and (4)RQ4: benchmark and select IR algorithms. RQ1: Reflecting Fine-Grained Relevance Levels. We first evaluate Llama3-Tok’s annotation on 595 gold labels of ClimRetrieve to verify whether it can effectively recover analysts’ ranking for rele- vant content by understanding which documents are more helpful than others. Relevance definitions are drafted with GPT-4 with the same procedure as § 3. We report nDCG 5 scores to