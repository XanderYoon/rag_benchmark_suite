DIRAS: Efficient LLM Annotation of Document Relevance for Retrieval Augmented Generation Jingwei Ni1,2*, Tobias Schimanski2*, Meihong Lin4, Mrinmaya Sachan1, Elliott Ash1, Markus Leippold2,3 1ETH Zurich 2University of Zurich 3 Swiss Finance Institute (SFI) 4 University of Electronic Science and Technology of China {jingni, msachan, ashe}@ethz.ch, meihong_lin@uestc.edu.ch {tobias.schimanski, markus.leippold}@df.uzh.ch Abstract Retrieval Augmented Generation (RAG) is widely employed to ground responses to queries on domain-specific documents. But do RAG implementations leave out important information when answering queries that need an integrated analysis of information (e.g., Tell me good news in the stock market today. )? To address these concerns, RAG developers need to annotate information retrieval (IR) data for their domain of interest, which is challeng- ing because (1) domain-specific queries usu- ally need nuanced definitions of relevance be- yond shallow semantic relevance; and (2) hu- man or GPT-4 annotation is costly and cannot cover all (query, document) pairs (i.e., anno- tation selection bias), thus harming the effec- tiveness in evaluating IR recall. To address these challenges, we propose DIRAS (Domain- specific Information Retrieval Annotation with Scalability), a manual-annotation-free schema that fine-tunes open-sourced LLMs to consider nuanced relevance definition and annotate (par- tial) relevance labels with calibrated relevance scores. Extensive evaluation shows that DIRAS enables smaller (8B) LLMs to achieve GPT-4- level performance on annotating and ranking unseen (query, document) pairs, and is helpful for real-world RAG development. 1 1 Introduction RAG has become one of the most popular paradigms for NLP applications (Gao et al., 2024). One core phase of RAG systems is Information Retrieval (IR), which leverages cheap retrievers to filter relevant information and thus save LLM in- ference costs. However, IR can be a performance bottleneck for RAG (Chen et al., 2023; Gao et al., 2024). Both leaving out important relevant informa- tion (low recall) as well as including