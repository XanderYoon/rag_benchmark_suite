Unc. stand for evaluation dimensions in § 3.1. calibrated relevance probability which can be used to rank documents for each query. To directly eval- uate the ranking performance, we measure nDCG and MAP upon relevance labels. (4) Uncertainty: If the models understand the difficulty caused by partial relevance, they should have lower confi- dence scores on samples that humans found un- certain. Thus we compute average precision (AP) scores between confidence and uncertainty labels. Details of computing all metrics are in App. C. 3.2 Optimizing the Training Data Creation We aim to train a highly performant student LLM Ms for relevance annotation. Thus, it is crucial to identify best-performing implementation choices that can be used to distill high-quality training data from the teacher LLM Mt. Specifically, we com- pare the following four implementation choices: ARES few-shot vs. relevance definitions: ARES (Saad-Falcon et al., 2023a) and DIRAS have differ- ent strategies to create target-domain training data: the former uses few-shot ICL3 while the latter uses relevance definitions. We also include RAGAs (Es et al., 2024) relevance judgement as a baseline. Pointwise vs. Listwise : The listwise method is popular in ranking data creation given its moder- ate cost and good performance (Sun et al., 2023b; Pradeep et al., 2023). However, the more efficient pointwise method is under-explored in prior work – majorly due to the concern about poor calibration (Sun et al., 2023a; Qin et al., 2024). Calibration method (Tok vs. Ask): One calibra- tion method is to get the relevance confidence by probing the model’s generation probability of the token Yes/No when predicting a document’s rel- evance (Tok, Liang et al., 2023). An alternative way is directly asking LLMs to verbalize confi- dence score, which may work better for instruction following LLMs (Ask, Tian et al., 2023). With vs. without