al- lowing more partial relevance for summary queries. Fig. 6 shows the F1 Scores of GPT-4 and Llama3- Tok with different relevance thresholds. Llama3- Tok achieves good F1 scores over a wide range of thresholds. Thanks to its compact size (8B), it can be efficiently deployed as a reranker in RAG systems. Optimizing Relevance Definitions: Results in Ta- ble 2 and Table 3 are obtained with GPT-4-drafted relevance definitions (i.e., relevance definitions). Although this approach is useful in large-scale ap- plications, there is still space for improvement by optimizing relevance definition, as shown in § 4.1. According to Bailey et al. (2008), the question orig- inators are the gold standard for relevance defini- tion. Hence, with the help of DIRAS, future RAG systems may allow users to customize their require- ments for relevant information. 6 Background and Related Work IR plays an important role in RAG but also be- comes a performance bottleneck (Gao et al., 2024). Low precision in IR may cause LLMs to halluci- 16 Questions 0.0 0.2 0.4 0.6Avg. Relevance Score Amount of Relevant Info from Most to Least Figure 5: The proximate amount of relevant information for 16 questions in all ClimRetrieve reports, according to Llama3-Tok’s relevance scores. 0.000.050.100.150.200.250.300.350.400.450.500.550.600.650.700.750.800.850.900.95 0.0 0.2 0.4 0.6 0.8F1 Score F1 Score by Threshold on ChatReport T est Llama3-T ok GPT-4-Ask with Figure 6: Instead of always retrieving top-k, we can retrieve documents if they have relevance scores higher than a threshold. This figure shows the change of F1 scores for obtaining relevant documents by thresholds. nate or pick up irrelevant information (Cuconasu et al., 2024; Schimanski et al., 2024a). Low re- call may leave out critical information for analysis (Ni et al., 2023). Domain-specific knowledge is also important for retrieval performance (Tang and Yang, 2024). Prior work on IR in