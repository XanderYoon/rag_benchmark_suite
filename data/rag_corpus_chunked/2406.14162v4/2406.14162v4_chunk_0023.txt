threshold. This figure shows the change of F1 scores for obtaining relevant documents by thresholds. nate or pick up irrelevant information (Cuconasu et al., 2024; Schimanski et al., 2024a). Low re- call may leave out critical information for analysis (Ni et al., 2023). Domain-specific knowledge is also important for retrieval performance (Tang and Yang, 2024). Prior work on IR in RAG has already explored the idea of using LLMs to judge relevance. The closest to our work are RAGAs and ARES. Es et al. (2024) develop RAGAs to evaluate the rel- evance of each sentence in a paragraph with a closed-source LLM and create an aggregated score by dividing the number of relevant sentences over all sentences. Saad-Falcon et al. (2023b) aim to bring a relevance judge to a target domain through few-shot in-context learning (ICL): they first gen- erate synthetic questions to given target-domain passages, and then fine-tune small classifiers for relevance judgment. However, many real-world challenges remain unaddressed. Specifically, IR recall, partial relevance, and domain-specific rele- vance definitions are neglected. ARES appears to have target-domain IR evaluation, but the synthetic data approach focuses on less integrative queries: each question is generated given a single passage and hard negatives are passages sampled from the same document, which could be (partially) relevant for integrative queries asking for broader informa- tion. Furthermore, we also find that few-shot ICL fails to teach domain-specific relevance to LLMs (ยง 3.2). Besides, Sun et al. (2023b,a); Pradeep et al. (2023); Qin et al. (2024) find that SOTA generic LLMs are good rerankers and such ability can be distilled to open-sourced LLMs. These studies all focus on pairwise or listwise ranking methods, and discusses that pointwise methods may not work due to bad calibration (Qin et al., 2024). How- ever, when it comes to relevance annotation