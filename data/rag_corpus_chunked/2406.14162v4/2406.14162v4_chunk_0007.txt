and only if a document addresses the relevance definition, it is deemed as (partially) relevant. We explicitly account for partial relevance when the document addresses the periphery of the definition. The data labeling process follows two steps. First, we em- ploy two annotators who independently annotate all test data to be either relevant, irrelevant, or par- tially relevant. Second, we employ a subject-matter expert in corporate climate disclosure to resolve conflicts to obtain final relevance labels. Besides relevance labels , we also obtain uncertainty la- bels from human annotations: Whenever there is strong disagreement (co-existence of relevance and irrelevance labels) or agreement on partial rele- vance (two or more annotators agree on partial relevance), the data point is labeled as uncertain. Inter-annotator agreement and other details can be found in App. E. Evaluation Metrics: LLM predictions contain a binary relevance annotation and a confidence score. They will be evaluated against relevance or uncer- tainty of ChatReportRetrieve labels on four dimen- sions: (1) Binary Relevance: We compute the F1 Score of models’ binary relevance prediction us- ing relevance labels. Binary relevance labels are important for deciding which documents should be passed to LLMs. (2) Calibration: Confidence scores should calibrate the binary accuracy to indi- cate annotation quality. We use Expected Calibra- tion Error (ECE), Brier Score, and AUROC to mea- sure calibration performance, following Kadavath et al. (2022) and Tian et al. (2023). (3) Informa- tion Retrieval: The confidence scores also give a Prompt: <question>: What is the firm’s Scope 3 emission? <question_definition>: This question is looking for infor- mation about the firm’s emission in ... <paragraph>: {one text chunk from a climate report} Is <paragraph> helpful for answering <question>? Pro- vide your best guess, and confidence score from 0 to 1. Teacher LLM Mt: [Reason]: {Reason why the paragraph is