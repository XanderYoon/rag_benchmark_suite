select IR algorithms. RQ1: Reflecting Fine-Grained Relevance Levels. We first evaluate Llama3-Tok’s annotation on 595 gold labels of ClimRetrieve to verify whether it can effectively recover analysts’ ranking for rele- vant content by understanding which documents are more helpful than others. Relevance definitions are drafted with GPT-4 with the same procedure as § 3. We report nDCG 5 scores to measure the ranking performance on ClimRetrieve. Gold labels 1, 2, and 3 are assigned with relevance scores 1/3, 2/3, and 1. Besides OpenAI 3rd generation embed- ding models, we also have a random baseline where 5MAP can only measure binary relevance and since we only investigate relevant samples, it cannot be calculated. Setting nDCG MAP Llama3-Askgeneric 29.95 26.51 Llama3-Askimproved 30.89 29.31 Llama3-Tokgeneric 31.17 28.73 Llama3-Tokimproved 32.53 32.65 Table 4: Comparison of using the generic and the im- proved relevance definitions for ranking all ClimRe- trieve (query, document) pairs. all (query, document) pairs are assigned a random relevance score between 0 and 1. The random base- line results are averaged over 5 random seeds (40 to 44). Importantly, all ClimRetrieve annotations are to some degree relevant, so improvement over the random baseline is challenging as the system needs to understand the trivial different degrees of relevance. Table 3 presents different systems’ performance. There is a clear trend of outperformance of the fine- tuned Llama-3 models in this challenging setting. RQ2: Improving Performance through Improv- ing Definitions. So far, we used GPT-4 to draft the relevance definitions. To investigate the effect of improved definitions, we compare two setups: (1) The generic relevance definition: the definition drafted by GPT-4. (2) The improved relevance defi- nition: The only way to improve the definition is to align it closer to ClimRetrieve annotators’ men- tal model of document relevance. We achieve this by adding relevant text