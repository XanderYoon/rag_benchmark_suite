17.81 64.62 38.82 46.32 56.21 27.54 35.34 42.91 31.84 41.35 Llama3-Tok 48.73 17.04 20.08 64.90 39.37 48.44 56.58 28.70 35.49 48.27 41.13 47.88 Table 7: Applying DIRAS to QA datasets, the IR performance of student model Llama3-Tok and GPT-4. N denotes nDCG. ELI5(85.05%) ASQA(66.32%) QAMPARI(72.66%) RAG-Bench(60.33%) All Rel. Irr. All Rel. Irr. All Rel. Irr. All Rel. Irr. Conf ≤ 0.95 62.35 48.48 71.15 67.27 71.43 66.67 74.11 73.33 74.23 58.41 38.10 63.04 Conf > 0.95 84.71 83.12 100.0 90.09 95.83 88.51 90.00 81.25 91.49 96.36 95.24 96.63 Table 8: Accuracy of student model Llama3-Tok annotations that disagree with original relevance labels. Same setup as Table 5. 4. Results in Table 7 show that Llama3-Tok out- performs GPT-4 in IR. Then for each dataset, we repeat the annotation selection bias assessment of RQ3 in § 4.1. Again, we sample 200 disagreements between Llama3-Tok annotation and the original (potentially noisy) relevance labels, and manually check whether Llama3 or original labels are cor- rect. As shown in Table 8, Llama3-Tok’s annota- tions are predominately correct with a confidence > 0.95 (e.g., 85.05% of ELI5). When there is a disagreement, relying on Llama3-Tok leads to less error (Acc. > 50%), especially when the confi- dence > 0.95, thanks to the good calibration. For ASQA, QAMPARI, and RAG-Bench, the major- ity (> 90%) of the disagreement lies in originally irrelevant labeled part of the dataset ( Irr.), possi- bly due to (query, document) pairs are selectively annotated. DIRAS achieves high accuracy in Irr. disagreements. Therefore, we reaffirm the notion that applying DIRAS to annotate broader (query, document) pairs can effectively reduce annotation selection bias, and thus improve IR recall bench- marking. All implementation details are in App. M. 5 Recommendation for Future RAG Avoiding Top-K Retrieval: Naive RAG systems (Ni et al., 2023) usually