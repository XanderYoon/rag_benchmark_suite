the paragraph is not explicitly covered by the definition but you think it somehow helps answering the question. Please annotate par- tially relevant. • Otherwise please annotate irrelevant. Additionally, one PhD student focusing on cli- mate change and sustainability research serves as a subject-matter meta-annotator to resolve conflicts or investigate cases where both labelers arise at the label partially. Comparing the two base annotators in the setup, we can calculate inter-annotator agreement. The Number of words per document Dataset Size Mean Std Min 25% 50% 75% Max 660 150 28.5 107 131 143 162 318 Table 9: Statistical properties of the number of words in ChatReportRetrieve test set data. Label Occurance Relevant 121 Partially 65 Not Relevant 474 Table 10: Label distribution in the ChatReportRetrieve testset. Cohen’s kappa between the two labelers is 0.683 (substantial agreement). We also calculate annota- tors’ agreement on partial relevance. The Cohen’s Kappa turns out to be 0.129, suggesting that there are uncertainty and subjectivity associated with par- tial labels. Besides the relevance, we also obtain an uncer- tainty label whenever there is strong disagreement (co-existence of relevance and irrelevance labels) or agreement on partial relevance (two or more anno- tators agree on partial relevance), the data point is labeled as uncertain. There are 103 (557) uncertain (certain) (query, document) pairs in the dataset. Finally, the third expert annotator resolves the existing conflicts in the dataset. This results in a label distribution of Table 10. It becomes appar- ent that the majority of documents are not relevant while still a significant number is labeled as par- tially relevant and relevant. F LLM Fine-Tuning Settings We use the default QLoRA hyperparameter settings 9, namely, an effective batch size of 32, a lora r of 64, a lora alpha of 16, a warmup ratio of 0.03, a