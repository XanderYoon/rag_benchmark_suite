n s w e r ( user_question , r a g _ s y s t e m ) print ( f " Answer : { answer } " ) Functionality: The script takes a user query from the terminal. It retrieves relevant documents using FAISS. Then it generates a answer using the re- trieved context with OLlama and Llama 3.1. Common Mistakes and Best Practices Incompatible embeddings The FAISS index is typically cre- ated using a specific embeddings model. If a different embed- dings model is used during querying (e.g., a different version of sentence-transformers), it may lead to retrieval mismatches or errors. Always ensure that the same model is used both during in- dexing and querying. Model version issues Using an incorrect or unsupported model version (e.g., referencing a non-existent version like Llama 4.5 ) can lead to failures during the model loading process. Always verify that the model version you are using is supported and available. Overly general prompts Prompts that are too broad or generic can result in vague or irrelevant responses from the model. Craft precise and targeted prompts to ensure more accurate and relevant answers. Ignoring context Language models can generate incorrect or hal- lucinated responses when the retrieved documents lack the necessary information, leading the model to fill in gaps inaccurately. Always ensure sufficient context is provided in the query. Memory leaks Extended use of FAISS and OLlama in continuous loops without proper memory management can result in memory leaks, gradually consuming system resources. Monitor memory usage and free up resources after each loop to avoid system slowdowns. Model re-initialization Reloading or re-initializing models unnec- essarily can slow down your system. Reuse initialized models when- ever possible to improve system efficiency and reduce overhead. OLlama (Llama 3.1) is a local language model that