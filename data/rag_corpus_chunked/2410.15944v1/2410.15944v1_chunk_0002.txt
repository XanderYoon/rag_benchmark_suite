traceability of responses is required, such as healthcare, legal analysis, customer service, or technical support. 2.1 What is RAG? The concept of Retrieval Augmented Generation (RAG) models is built on in- tegrating two core components of NLP: Information Retrieval (IR) and Natural Language Generation (NLG). The RAG framework, first introduced by Lewis et al.[5] combines dense retrieval methods with large scale generative models to produce responses that are both contextually relevant and factually accurate. By explicitly retrieving relevant passages from a large corpus and augmenting this information in the generation process, RAG models enhance the factual grounding of their outputs from the up-to-date knowledge. A generic workflow of Retrieval Augmented Generation (RAG) system, show- casing how it fundamentally enhances the capabilities of Large Language Models (LLMs) by grounding their outputs in real-time, relevant information is illus- trated in the Fig[1]. Unlike static models which generate responses based only on closed-world knowledge, the RAG process is structured into the following key steps: 1. Data Collection: The workflow begins with the acquisition of relevant, domain specific textual data from various external sources, such as PDFs, structured documents, or text files. These documents represent raw data important for building a tai- lored knowledge base that the system will query during the retrieval process 2 enhancing the modelâ€™s ability to respond. Fig. 1: Architecture of Retrieval Augmented Generation(RAG) system. 2. Data Preprocessing: The collected data is then preprocessed to create manageable and meaning- ful chunks. Preprocessing involves cleaning the text (e.g., removing noise, formatting), normalizing it, and segmenting it into smaller units, such as to- kens (e.g., words or group of words), that can be easily indexed and retrieved later. This segmentation is necessary to ensure that the retrieval process is accurate and efficient. 3. Creating Vector Embeddings: After preprocessing, the chunks of data