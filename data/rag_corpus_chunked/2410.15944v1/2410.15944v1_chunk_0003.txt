chunks. Preprocessing involves cleaning the text (e.g., removing noise, formatting), normalizing it, and segmenting it into smaller units, such as to- kens (e.g., words or group of words), that can be easily indexed and retrieved later. This segmentation is necessary to ensure that the retrieval process is accurate and efficient. 3. Creating Vector Embeddings: After preprocessing, the chunks of data are transformed into vector repre- sentations using embedding models (e.g., BERT, Sentence Transformers). These vector embeddings capture the semantic meaning of the text, allow- ing the system to perform similarity searches. The vector representations are stored in a Vector Store, an indexed database optimized for fast retrieval based on similarity measures. 4. Retrieval of Relevant Content: When a Query is input into the system, it is first transformed into a vec- tor embedding, similar to the documents in the vector store. The Retriever component then performs a search within the vector store to identify and retrieve the most relevant chunks of information related to the query. This retrieval process ensures that the system uses the most pertinent and up-to- 3 date information to respond to the query. 5. Augmentation of Context: By merging two knowledge streams - the fixed, general knowledge embed- ded in the LLM and the flexible, domain-specific information augmented on demand as an additional layer of context, aligns the Large Language Model (LLM) with both established and emerging information. 6. Generation of Response by LLM: The context-infused prompt, consisting of the original user query combined with the retrieved relevant content is provided to a Large Language Model (LLM) like GPT, T5 or Llama. The LLM then processes this augmented in- put to generate a coherent response not only fluent but factually grounded. 7. Final Output: By moving beyond the opaque outputs of traditional models, the final output