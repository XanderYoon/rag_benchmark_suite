= client . files . retrieve ( f i l e _ c i t a t i o n . file_id ) ci ta ti on s . append ( f " [{ index }] { c i t e d _ f i l e . filename } " ) words = m e s s a g e _ c o n t e n t . value . split () for word in words : print ( word , end = ’ ’ , flush = True ) time . sleep (0.05) p r o c e s s e d _ m e s s a g e _ i d s . add ( message . id ) if any ( msg . role == " a ss is ta nt " and msg . content for msg in n e w _ m e s s a g e s ) : 23 break time . sleep (1) if c it at io ns : print ( " \ nSources : " , " , " . join ( ci ta ti on s ) ) print ( " \ n " ) The flexibility of configuring the assistant and thread or assistant-level tools OpenAI’s API makes this approach highly versatile for various use cases. By following the steps outlined in this section and adhering to the provided best practices, developers can effectively build a powerful RAG system. 4.2.2 Using Open-Source LLM Model: Llama We will utilize Ollama, an open-source framework that implements the Llama model, to incorporate Llama-based question generation capabilities within our application. By em- ploying Ollama, we can process user input and generate contextually relevant questions directly through the terminal, offering an efficient and scalable solution for natural language processing