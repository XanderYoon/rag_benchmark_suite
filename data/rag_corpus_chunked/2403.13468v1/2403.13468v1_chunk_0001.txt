promising results in situ- ations where understanding the semantic context of queries and documents is crucial for accurate retrieval. In contrast to their traditional counterparts, which heavily rely on lexical similarities captured by scoring functions such as TF-IDF or BM25, dense retrieval techniques naturally capture query and document se- mantics and can be easily adapted to handle multi-modal data and cross-lingual arXiv:2403.13468v1 [cs.IR] 20 Mar 2024 2 Kasela et al. retrieval [19]. However, their training requires large labeled datasets, and the re- sulting models are typically highly specialized to the task they are trained on and do not generalize well to a new task or domain without additional fine-tuning. Numerous efforts have been directed towards creating a single neural model that can generalize across many domains, but achieving this goal has proven chal- lenging [26]. In attaining this objective, we must also consider that the queries in many IR tasks are often brief and concise, sometimes lacking sufficient infor- mation for comprehensive semantic matching. Moreover, users typically do not explicitly specify the domain of their query, so, if necessary, the system must deduce it in a latent manner. A sub-field of neural IR is open-domain Q&A, where the questions are posed in natural language and the answer is retrieved from an extensive collection of documents. In this work, to address the above issues, we propose DESIRE- ME, a model for open-domain Q&A that can specialize in multiple domains without changing the underlying pre-trained language model. This specialization is achieved by adaptively focusing the retrieval on the current query domain by leveraging the Mixture-of-Experts (MoE) framework [14]. The MoE framework provides a machine learning architecture combining multiple specialized models, called “specializers” or “experts”, to collectively solve a task, such as Q&A. Each specializer within the framework is designed to excel in