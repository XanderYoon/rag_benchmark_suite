that enforcing specialized MoE IR mod- els should yield better performance. In this work, we rely on these pre-trained dense retrieval models and focus on improving their performance by injecting domain specialization based on a supervised variant of MoE. 3 DESIRE-ME In this section, we introduce the DESIRE-ME model: in Section 3.1, we give an overview of the MoE models; in Section 3.2 we describe DESIRE-ME, detailing its components and the training procedure, along with the differences from the classical MoE models. 3.1 MoE background Mixture-of-Experts [14] (MoE) is an ensemble learning model that relies on the collective information provided by multiple expert models, which we will also call domain specializer, or simply specializer from hereon. Each of these specializers is dedicated to a specific topical domain or to a specific sub-task within a broader problem domain. One of the most remarkable aspects of MoEs is their versatility as they can be employed for various types of data and tasks [3,18,20]. In the context of MoEs, a key issue is determining which specializer(s) to rely on for a specific input. This decision process is managed by a gating function, a significant component of a MoE model, which aims to determine the contribution of each specializer in producing the final outcome for a given input. The gating function is trained alongside the specializers to ensure that the gating mechanism and the specializers work together to improve the overall modelâ€™s performance. For DESIRE-ME 5 example, let us assume to tackle a complex primary task; MoE can be employed to learn to divide it into M sub-tasks, each handled by a distinct specializer. The gating mechanism learns to predict which sub-task the input will likely belong to and select the appropriate specializer accordingly. MoE operates as an ensemble model, aggregating the outputs of