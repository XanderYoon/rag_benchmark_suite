and overcome some of the limitations of the lexical models, e.g. query-document vocabulary mismatch. They include models such as docT5query [21] that uses sequence-to-sequence models to expand document terms by generating possible queries for which the document would be relevant. Late-interaction models rely on a bi-encoder ar- chitecture to encode the query and documents at a token level. The relevance is assessed by computing the similarity between the representations of queries terms and document terms. Late-interaction models allow the pre-computation of documentsâ€™ representation by delaying the interaction between the query and document representations. A notable example is ColBERT [16], which computes contextualized token-level embeddings for both documents and queries and uses them at retrieval and scoring time. Re-ranking models employ a computation- ally expensive neural model to re-rank documents retrieved by a fast first-stage ranker. The best-performing re-ranking model in a zero-shot retrieval scenario is currently based on a MonoT5 cross-encoder and utilizes BM25 as the first stage ranker. [24]. Dense retrieval models project the query and the documents (or pas- sages) in a common semantic dense vector space and leverage similarity functions to score the documents according to a given query. Many different dense models have been recently proposed because they empirically perform better than lexical and sparse models in many tasks while not being computationally expensive like cross-encoder re-ranking models. Two dense models, namely COCO-DR [29] and Contriever [13], are specifically attractive in this regard for open-domain Q&A as they generalize very well to new domains without the need for labeled data. They are currently among the best performing dense retrieval models on the BEIR benchmarks 5. Both models rely on contrastive learning, a method that 5 Official BEIR performance spreadsheet [Deprecated since Jan 10, 2023] 4 Kasela et al. uses pairs of positive and negative examples to