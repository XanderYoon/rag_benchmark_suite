Wikipedia category graph backward in a breadth-first manner until we reach the category to which the subcategory belongs. The second scenario occurs when the relevant article pertains to multiple categories and/or two or more Wikipedia pages are perti- nent to the same query. In such cases, we identify the categories for each page and simply label the query with all the categories of all relevant pages. By following this approach, we successfully label the vast majority of ques- tions in the datasets. The percentage of labeled documents and queries and the average number of per-query labels are reported in Table 1 for the three datasets having training queries. The labels per query are not equally distributed: for in- stance, in FEVER there are ∼5000 queries in the category Life, meanwhile only ∼500 queries belong to the category Mathematics. MoE specializers and training hyperparameters. Since in DESIRE-ME each specializer focuses on a specific query category, we employ 37 distinct MoE specializers, a number equal to the number of distinct query categories in the datasets. DESIRE-ME specializers feature a simple architecture: they consist of a down-projection layer using a feed-forward network (FFN) that reduces the input dimension by half. The output layer comprises an up-projection FFN layer, which restores the vector dimension to match the input dimension. This design draws inspiration from the adapter layer proposed in [12]. However, we opted not to use that complete adapter layer in our setup, as the skip connection is 6 https://en.wikipedia.org/wiki/Wikipedia:FAQ/Categories DESIRE-ME 9 already introduced within the MoE module. The gating function classifier has two up-projection layers, which increase the vector dimension to 2 × and 4 ×, respectively. The output layer is a down-projection FFN with the same size as the number of categories, i.e., 37 in our case. We set the training batch