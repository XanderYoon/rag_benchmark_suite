0.571* Contriever Base 0.553 0.819 0.777 0.638 0.758 0.592 Fine-tuned 0.575 0.799 0.848 0.657 0.728 0.600 RND-G 0.552 0.817 0.780 0.636 0.757 0.592 DESIRE-ME 0.567 0.824 0.787 0.648 0.767 0.606 COCO-DRXL Base 0.549 0.819 0.756 0.633 0.763 0.592 Fine-tuned 0.542 0.757 0.831 0.622 0.681 0.563 RND-G 0.555 0.819 0.767 0.637 0.763 0.595 DESIRE-ME 0.564* 0.821 0.780 0.646* 0.767 0.602 * improves over the COCO-DR and Contriever retrievers baselines, with a relative margin of 6% and 9% in NDCG@10 and P@1, respectively. It is crucial to outline that while we could replicate the COCO-DR and COCO-DRXL results on the NQ dataset, our results diverged slightly from those reported in the original paper [29] for FEVER and HotpotQA. The Contriever results, instead, align exactly with those reported in the original article [13]. In summary, independently of these minor differences, our experiments on the three datasets demonstrate a consistent and significant improvement in retrieval performance obtained by integrating DESIRE-ME into the respective dense retrieval models. We can thus definitely answer positively RQ1. Answering RQ2. We evaluate DESIRE-ME trained on FEVER in a zero-shot scenario on Climate-FEVER. This experiments aims to assess the generaliza- tion power of DESIRE-ME on a similar yet distinct dataset. Climate-FEVER and FEVER share a substantial portion of their corpus. However, an important distinction lies in the queries: Climate-FEVER relies on real-world user queries, while FEVER employs synthetic queries. We report in Table 5 the results of the experiments conducted using the DESIRE-ME models trained on the FEVER on the questions of Climate-FEVER. Despite the difference in query types, we notice improvements over the baselines across all models, similar to the previous three experiments. Specifically, the improvements over the respective base mod- els are statistically significant for all the metrics measured with both COCO-DR retrievers. The relative margin in terms