the training data with a batch size of 32 and a learning rate of 10 −6 for 10 epochs. All the other training hyper-parameters are taken from their original settings. – Random gating (RND-G). We use randomly generated weights to merge spe- cializers’ outputs. This baseline is introduced to assess the benefits of our supervised gating function. All other DESIRE-ME settings are unchanged. 4.2 Results and Discussion Answering RQ1. To answer RQ1, we conduct multiple experiments using the NQ, HotpotQA, and FEVER datasets to assess DESIRE-ME capability to enhance the effectiveness of the underlying dense retrieval model. The results on the three datasets are reported in Table 2, Table 3, and Table 4, respectively. Table 2 reports the results of the experiments conducted with the NQ dataset. The figures reported in the table show that fine-tuning the base model using the training data does not yield any benefit and that the integration of DESIRE- ME into the different dense retrieval systems always results in a remarkable improvement of the performances. Irrespective of the metrics considered and the dense retriever used, our solution boosts the base models of a statistically signifi- cant margin. The Contriever relative improvement reaches an astonishing 12% in 7 https://amenra.github.io/ranxhub 8 Available on HuggingFace: COCO-DR, COCO-DR XL and Contriever. 10 Kasela et al. T able 2. Results on the NQ dataset. In italic the best results per model, in boldface the best results overall. Symbol * indicates a statistically significant difference over Base, Fine-tuned and RND-G. Retriever Variant MAP@100 MRR@100 R@100 NDCG@10 P@1 NDCG@3 BM25 - 0.292 0.295 0.758 0.339 0.198 0.268 COCO-DR Base 0.441 0.455 0.923 0.504 0.325 0.424 Fine-tuned 0.433 0.446 0.942 0.501 0.310 0.411 RND-G 0.434 0.448 0.926 0.499 0.313 0.417 DESIRE-ME 0.463* 0.477* 0.941 0.526 * 0.339* 0.448* Contriever Base 0.432 0.446 0.927