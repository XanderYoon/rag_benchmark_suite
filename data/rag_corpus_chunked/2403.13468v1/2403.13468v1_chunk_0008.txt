DESIRE-ME model The overall structure of DESIRE-ME is very similar to that of the underlying bi-encoder dense retrieval model: we have a query encoder, which computes the query representation, and a document encoder, which computes the document representation. A scoring function, e.g., the dot product or cosine similarity, is used to compute the similarity between the dense vectors representing the query and the document. For efficiency purposes, the embeddings of all the documents in the collection are computed offline using the document encoder and indexed for fast retrieval. In addition to the components of the underlying dense retriever, we introduce in DESIRE-ME a MoE module acting on the query representation only. Such a component inputs the embedding computed by the query encoder and outputs a modified representation of the query having the same dimensionality. The transformation is made utilizing theDESIRE-ME MoE specializers detailed in the following. Since the documents are encoded and indexed offline for fast retrieval,DESIRE-ME applies the MoE only to the query representation that is typically computed online; document representations are not modified based on the specific query processed. The DESIRE-ME MoE is detailed in Figure 1. The component has three major modules: the gating function , the specializers, and the pooling module. 6 Kasela et al. Context representation 1Specializer 1 Gating Function Sigmoid Specializer 2 Specializer M Query representation Context representation 2 Context representation M Domain classiÔ¨Åcation Pooling + DESIRE-ME query representation Fig. 1. The MoE module of the proposed model. The gating function. It has the primary purpose of computing the likelihood for the query to belong to any of M predefined domains. Our gating mechanism differs from classical MoE gating functions in several ways. Firstly, it relies on a multi-label domain classifier. Using a classifier as a gating function is not entirely novel in MoE;