the skip connection is 6 https://en.wikipedia.org/wiki/Wikipedia:FAQ/Categories DESIRE-ME 9 already introduced within the MoE module. The gating function classifier has two up-projection layers, which increase the vector dimension to 2 × and 4 ×, respectively. The output layer is a down-projection FFN with the same size as the number of categories, i.e., 37 in our case. We set the training batch size to 512, the learning rate to 10−5, and train for 60 epochs. We use 5% of the training set for validation and keep only the checkpoint with the lowest validation loss. Metrics and baselines. We assess the results of the experiments using: MAP@100, MRR@100, R@100, NDCG@10, NDCG@3 and P@1. While NDCG@10 and R@100 are commonly used on BEIR benchmarks, the additional metrics allow us to have a deeper understanding of the potential improvement of DESIRE-ME at small cutoffs. We also report statistically significant differences according to a Bonferroni corrected two-sided paired Student’s t-tests with p-value < 0.001. We rely on the ranx library [1] for evaluation. To simplify comparative evaluations and to give the possibility of computing other evaluation metrics, all the runs are made publicly available on ranxhub7 [2]. We compare DESIRE-ME variants integrated within the following different state-of-the-art dense retrieval models8: COCO-DR, COCO-DR XL, and Contriever against the following baselines, for each dense retrieval model: – Base. The original dense retrieval model without MoE in a zero-shot sce- nario. – Fine-tuned. We fine-tuned the base models with the training data with a batch size of 32 and a learning rate of 10 −6 for 10 epochs. All the other training hyper-parameters are taken from their original settings. – Random gating (RND-G). We use randomly generated weights to merge spe- cializers’ outputs. This baseline is introduced to assess the benefits of our supervised gating function. All other DESIRE-ME