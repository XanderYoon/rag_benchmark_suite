using the DESIRE-ME models trained on the FEVER on the questions of Climate-FEVER. Despite the difference in query types, we notice improvements over the baselines across all models, similar to the previous three experiments. Specifically, the improvements over the respective base mod- els are statistically significant for all the metrics measured with both COCO-DR retrievers. The relative margin in terms of NDCG@10 reaches 9%. These results outlines the capacity of DESIRE-ME to adapt to incoming queries that differs substantially from the ones seen at training time. We can thus answer positively also the second research question (RQ2) even if further experiments involving 12 Kasela et al. T able 4. Results on the FEVER dataset. In italic the best results per model, in boldface the best results overall. Symbol * indicates a statistically significant difference over Base, Fine-tuned and RND-G. Retriever Variant MAP@100 MRR@100 R@100 NDCG@10 P@1 NDCG@3 BM25 - 0.707 0.744 0.931 0.753 0.646 0.719 COCO-DR Base 0.660 0.698 0.935 0.715 0.586 0.670 Fine-tuned 0.544 0.568 0.928 0.607 0.431 0.544 RND-G 0.652 0.690 0.937 0.710 0.565 0.666 DESIRE-ME 0.696* 0.736* 0.945* 0.749* 0.623* 0.712* Contriever Base 0.708 0.749 0.949 0.758 0.642 0.724 Fine-tuned 0.466 0.483 0.920 0.531 0.343 0.458 RND-G 0.709 0.749 0.947 0.761 0.640 0.725 DESIRE-ME 0.722* 0.764* 0.948 0.772* 0.655* 0.739* COCO-DRXL Base 0.699 0.740 0.946 0.749 0.633 0.713 Fine-tuned 0.421 0.434 0.916 0.487 0.296 0.406 RND-G 0.716 0.759 0.948 0.765 0.654 0.733 DESIRE-ME 0.745* 0.789* 0.952 0.792 * 0.691* 0.762* also other corpora are needed to undoubtedly assess the generalization power of DESIRE-ME across totally different Q&A scenarios. 5 Conclusions In this work we introducedDESIRE-ME, a new retrieval model for open-domain Q&A task that leverages the Mixture-of-Experts (MoE) framework to improve the performance of state-of-the-art dense retrieval models. The proposed MoE component uses supervised methods in the