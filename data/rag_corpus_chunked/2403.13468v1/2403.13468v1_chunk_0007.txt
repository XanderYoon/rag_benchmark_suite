DESIRE-ME 5 example, let us assume to tackle a complex primary task; MoE can be employed to learn to divide it into M sub-tasks, each handled by a distinct specializer. The gating mechanism learns to predict which sub-task the input will likely belong to and select the appropriate specializer accordingly. MoE operates as an ensemble model, aggregating the outputs of each spe- cializer in a final pooling stage. Let x be the vector encoding the input item and fi(x) the output of the function, fi, learned by the i-th specializer. Moreover, let gi(x) be the weight of the i-th specializer computed by the gating mechanism for input x. Various pooling methods have been proposed in the literature to aggregate the output of the specializers. The simplest pooling stage proposed in [30], often referred to as Top-1 gating, is a trivial decision model that always chooses the output of the specializer with the highest weight, i.e.: m = arg max i=1,...,M (gi(x)) y = fm(x) Alternatively, probability scores can be derived from the gating function’s out- put values, possibly using asoftmax normalization [15]. The resulting probability distribution indicates the likelihood of a specializer being the most appropriate for a given input. In this case, the pooling method makes use of the probabil- ity values from the above probability distribution as weights to compute the weighted sum of the M specializers’ outputs: y = MX i=1 fi(x) · gi(x) (1) 3.2 The DESIRE-ME model The overall structure of DESIRE-ME is very similar to that of the underlying bi-encoder dense retrieval model: we have a query encoder, which computes the query representation, and a document encoder, which computes the document representation. A scoring function, e.g., the dot product or cosine similarity, is used to compute the similarity between the dense vectors representing the