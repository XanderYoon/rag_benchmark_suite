for NDCG@3. For the other metrics, except R@100, we observe a slight improvement, but not always statistically significant. The relative perfor- mance improvement over the base model on HotpotQA is lower than that mea- sured on NQ, reaching a margin of 3% in MAP@100 and 2% in NDCG@10. For Contriever, instead, the fine-tuned model outperforms DESIRE-ME in terms of R@100 and NDCG@10; meanwhile, for the other metrics DESIRE-ME per- forms slightly better than all baselines but not statistically significantly. Table 4 shows the performance achieved on the FEVER dataset. FEVER presents a unique set of challenges compared to the other two datasets: the queries in FEVER are not questions but statements, and the relevant docu- ments support or refute the claim made in the query statement. On this dataset, fine-tuning the base model, surprisingly, deteriorates the model performances, while BM25 performs very well, showing that the statement and the relevant documents share a similar vocabulary. As in the previous cases, DESIRE-ME DESIRE-ME 11 T able 3. Results on the HotpotQA dataset. In italic the best results per model, in boldface the best results overall. Symbol * indicates a statistically significant difference over Base, Fine-tuned and RND-G. Retriever Variant MAP@100 MRR@100 R@100 NDCG@10 P@1 NDCG@3 BM25 - 0.521 0.770 0.740 0.603 0.707 0.558 COCO-DR Base 0.519 0.795 0.727 0.604 0.737 0.563 Fine-tuned 0.527 0.753 0.805 0.608 0.678 0.553 RND-G 0.523 0.794 0.742 0.607 0.734 0.566 DESIRE-ME 0.530 0.795 0.753 0.614 0.734 0.571* Contriever Base 0.553 0.819 0.777 0.638 0.758 0.592 Fine-tuned 0.575 0.799 0.848 0.657 0.728 0.600 RND-G 0.552 0.817 0.780 0.636 0.757 0.592 DESIRE-ME 0.567 0.824 0.787 0.648 0.767 0.606 COCO-DRXL Base 0.549 0.819 0.756 0.633 0.763 0.592 Fine-tuned 0.542 0.757 0.831 0.622 0.681 0.563 RND-G 0.555 0.819 0.767 0.637 0.763 0.595 DESIRE-ME 0.564* 0.821 0.780 0.646* 0.767 0.602 *