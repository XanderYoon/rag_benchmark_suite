are detailed in Section 4. The specializers. They are very similar to those proposed in [14]. Each of the M specializers focuses on tuning the input query representation for the corre- sponding domain. At training time they learn via the contrastive loss function how to contextualize the query for the specific domain. The pooling module. Finally we have the pooling module that merges the domain context representations computed by the specializers on the basis of the domain likelihood estimated by the gating function in the form of a normalized vector of M weights. Merging is accomplished by simply weighting and summing up the outputs of the specializers, as shown in Equation 1 and depicted in Figure 1. DESIRE-ME 7 We note that a consequence of the enforced domain independence condition is that an input query can be classified by our gating function as not belonging to any of the predefined domains. This is the reason why DESIRE-ME model has a skip connection for the input query representation that is updated with the domain context representation computed by the previous modules. Thanks to such a skip connection, when DESIRE-ME encounters an out-of-domain query, it outputs the unmodified representation of the query not benefiting from specialization. 4 Experimental analysis In the following we detail the extensive experiments conducted to answer the following research questions: RQ1: Can DESIRE-ME enhance the effectiveness of state-of-the-art dense re- trieval models for open-domain Q&A? RQ2: Does a DESIRE-ME model trained on a dataset generalize to datasets having similar characteristics in a zero-shot scenario? 4.1 Experimental settings In this Section, we detail the characteristics of the datasets used for the experi- ments; we then discuss how the datasets are used to train and testDESIRE-ME. Datasets. In our experiments, we use four datasets included in BEIR (BEnch- marking IR