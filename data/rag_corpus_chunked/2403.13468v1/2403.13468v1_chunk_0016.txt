a statistically significant difference over Base, Fine-tuned and RND-G. Retriever Variant MAP@100 MRR@100 R@100 NDCG@10 P@1 NDCG@3 BM25 - 0.292 0.295 0.758 0.339 0.198 0.268 COCO-DR Base 0.441 0.455 0.923 0.504 0.325 0.424 Fine-tuned 0.433 0.446 0.942 0.501 0.310 0.411 RND-G 0.434 0.448 0.926 0.499 0.313 0.417 DESIRE-ME 0.463* 0.477* 0.941 0.526 * 0.339* 0.448* Contriever Base 0.432 0.446 0.927 0.498 0.311 0.414 Fine-tuned 0.427 0.438 0.940 0.497 0.295 0.406 RND-G 0.441 0.457 0.928 0.510 0.320 0.426 DESIRE-ME 0.493* 0.511* 0.941 0.559 * 0.379* 0.480* COCO-DRXL Base 0.480 0.495 0.937 0.546 0.359 0.465 Fine-tuned 0.465 0.478 0.955 0.537 0.331 0.447 RND-G 0.488 0.503 0.939 0.553 0.371 0.473 DESIRE-ME 0.510* 0.527* 0.951 0.577* 0.390* 0.497* NDCG@10 and 22% in P@1 over the base model. This indicates that DESIRE- ME contributes significantly to enhancing the ranking quality of retrieved doc- uments, particularly in the top positions. Furthermore, it is also worth noting that the RND-G model, which relies on a random gating mechanism, does not improve substantially the base model. This observation, which holds also for the experiments presented in the following, proves that our gating mechanism is an important factor contributing to improved retrieval performance. In Table 3, we report the results on the HotpotQA dataset. In this case, fine-tuning the base model improves model performance, especially for R@100. For COCO-DR and COCO-DRXL DESIRE-ME improves the performance over the baselines across all three models. The improvements are consistently statisti- cally significant for NDCG@3. For the other metrics, except R@100, we observe a slight improvement, but not always statistically significant. The relative perfor- mance improvement over the base model on HotpotQA is lower than that mea- sured on NQ, reaching a margin of 3% in MAP@100 and 2% in NDCG@10. For Contriever, instead, the fine-tuned model outperforms DESIRE-ME in terms of R@100