* 0.691* 0.762* also other corpora are needed to undoubtedly assess the generalization power of DESIRE-ME across totally different Q&A scenarios. 5 Conclusions In this work we introducedDESIRE-ME, a new retrieval model for open-domain Q&A task that leverages the Mixture-of-Experts (MoE) framework to improve the performance of state-of-the-art dense retrieval models. The proposed MoE component uses supervised methods in the gating mechanism and predicts the likelihood of a query belonging to predefined domains, while the specializer mod- ules focus on contextualizing the query vector for specific domains. We conducted extensive experiments across multiple datasets to investigate two research ques- tions. For the first experiment, we chose three diverse datasets. Our experiments show that integrating the DESIRE-ME model into dense retrieval models leads to significant improvements in various retrieval metrics, answering positively the RQ1. These findings highlight the robustness and adaptability ofDESIRE-ME. In response to the RQ2, the experiment performed on the Climate-FEVER dataset, using a model trained on FEVER shows that MoE can generalize to new datasets in a zero-shot scenario. This also shows the potential of lever- aging knowledge from a similar corpus and encourages further exploration of techniques, such as transfer learning in the open-domain Q&A tasks. Limitations and future work. Our primary focus was understanding the improve- ments achieved by using domain specialization in open-domain Q&A; we did not concentrate on optimizing the underlying neural architectures for the specializ- ers and gating mechanism. The main limitation of this work is the assumption DESIRE-ME 13 T able 5. Results on the Climate-FEVER dataset using models trained on FEVER. In italic the best results per model, in boldface the best results overall. Symbol * indicates a statistically significant difference over Base and RND-G. Retriever Variant MAP@100 MRR@100 R@100 NDCG@10 P@1 NDCG@3 BM25 - 0.162 0.293 0.436 0.213 0.205 0.179