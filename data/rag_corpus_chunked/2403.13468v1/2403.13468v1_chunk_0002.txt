the underlying pre-trained language model. This specialization is achieved by adaptively focusing the retrieval on the current query domain by leveraging the Mixture-of-Experts (MoE) framework [14]. The MoE framework provides a machine learning architecture combining multiple specialized models, called “specializers” or “experts”, to collectively solve a task, such as Q&A. Each specializer within the framework is designed to excel in a specific topical subdomain or under certain conditions, and the MoE model dynamically selects and combines these specializers to make predictions tailored to the input data. A gating mechanism determines which specializer(s) to use for a given input. This gating mechanism is a trained neural network that takes the input query and assigns an importance weight to each expert. The weights indicate the relevance of each specializer for the current input and determine their contribution to the final prediction. The DESIRE-ME approach applied to a complex and faceted task such as open-domain Q&A permits learning a robust and adaptive MoE model that handles the heterogeneity of questions better than state-of-the-art monolithic dense retrievers. To summarize, our research contributions are: – A modular MoE framework for open-domain Q&A integrated into a dense retrieval system that significantly boosts the performance of the underlying model by exploiting domain specialization; – A supervised gating method able to understand the query topic and corre- spondingly weighting the domain contextualization computed by the various MoE specializers; – A novel experimental framework exploiting the folksonomy of Wikipedia to derive automatically the domains of documents and queries used to train the supervised gating mechanisms; We evaluate our proposal against state-of-the-art baselines with reproducible ex- periments on three different datasets 4. The results of the experiments show that DESIRE-ME consistently improves the performance of the underlying dense re- triever with an increase of up to 12% in NDCG@10 and