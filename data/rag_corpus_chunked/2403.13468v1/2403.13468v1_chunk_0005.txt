as they generalize very well to new domains without the need for labeled data. They are currently among the best performing dense retrieval models on the BEIR benchmarks 5. Both models rely on contrastive learning, a method that 5 Official BEIR performance spreadsheet [Deprecated since Jan 10, 2023] 4 Kasela et al. uses pairs of positive and negative examples to learn meaningful and discrim- inative representations for queries and passages. This is generally done using a synthetic dataset pseudo-labeled in a self-supervised fashion using the target domain corpus. 2.2 Mixture-of-Experts In this work we employ COCO-DR and contriever in a MoE [14] framework for open-domain Q&A. MoE has been used in many different contexts by the machine learning community [3,7,22]. Shazeer et al. [25] introduced MoE in natural language processing. Their proposal routes a token-level representation through a fixed number of experts. Many works later used MoE in NLP [5,8,9]. MoE models have also been applied in the field of IR for various tasks, for example, for question answering in the biomedical domain [4], visual question answering [20], and for rank fusion for multi-task dense retrieval [18]. MoE allows the creation of expert sub-networks that specialize in an unsuper- vised manner and improve performance. Even though COCO-DR and Contriever perform exceptionally well on the BEIR benchmark, the domain knowledge is not explicitly leveraged in their training. Due to the high domain specialization of neural networks in NLP tasks, we argue that enforcing specialized MoE IR mod- els should yield better performance. In this work, we rely on these pre-trained dense retrieval models and focus on improving their performance by injecting domain specialization based on a supervised variant of MoE. 3 DESIRE-ME In this section, we introduce the DESIRE-ME model: in Section 3.1, we give an overview of the MoE models;