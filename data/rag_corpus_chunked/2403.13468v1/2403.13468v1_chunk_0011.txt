DESIRE-ME model trained on a dataset generalize to datasets having similar characteristics in a zero-shot scenario? 4.1 Experimental settings In this Section, we detail the characteristics of the datasets used for the experi- ments; we then discuss how the datasets are used to train and testDESIRE-ME. Datasets. In our experiments, we use four datasets included in BEIR (BEnch- marking IR [26]), a valuable resource for tackling the issue of models’ generaliza- tion. The datasets are: NaturalQuestion [17], HotpotQA [28], FEVER [27], and Climate-FEVER [6]. The main characteristics of the four datasets are resumed in Table 1. They all rely on a corpus based on Wikipedia, and provide binary relevance assessments for query-document pairs: – NaturalQuestion (NQ) contains queries submitted to the Google search en- gine and their answers drawn from Wikipedia articles. The passages within the Wikipedia articles that provide satisfactory answers to the questions have been identified by human annotators. – HotpotQA focuses on complex questions that a single span of text might not answer and could involve reasoning over multiple documents. Queries and relevance labels have been generated with crowd-sourcing. – FEVER is a resource proposed to tackle fact-checking and verification claims. It encompasses queries and documents from various domains and relies, as the previous datasets, on a Wikipedia-based corpus. – Climate-FEVER is a dataset for verifying climate change-related claims. It includes ∼1500 test queries (no training data). The corpus is the same as FEVER, with the addition of 25 more documents unavailable in FEVER. 8 Kasela et al. T able 1.Characteristics of the datasets used. Labeled queries and the average number of labels per query refer to training queries only. Dataset #Docs #Training #Validation #Test Labeled docs Labeled queries Avg labels NaturalQuestions [17] 2,681,468 132,803 - 3,452 97.1% 97.8% 2.04 HotpotQA [28] 5,233,329 85,000 5,447 7,405 95.45%