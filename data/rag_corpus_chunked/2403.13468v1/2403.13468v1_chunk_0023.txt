retrieval runs. In: Proceedings of the 46th International ACM SIGIR Conference on Re- search and Development in Information Retrieval. p. 3210–3214. SIGIR ’23, Association for Computing Machinery, New York, NY, USA (2023). https://doi.org/10.1145/3539618.3591823, https://doi.org/10.1145/3539618. 3591823 3. Collobert, R., Bengio, S., Bengio, Y.: A parallel mixture of svms for very large scale problems. In: Dietterich, T., Becker, S., Ghahramani, Z. (eds.) Advances in Neural Information Processing Systems. vol. 14. MIT Press (2001), https://proceedings.neurips.cc/paper files/paper/2001/file/ 36ac8e558ac7690b6f44e2cb5ef93322-Paper.pdf 4. Dai, D., Jiang, W.J., Zhang, J., Peng, W., Lyu, Y., Sui, Z., Chang, B., Zhu, Y.: Mixture of experts for biomedical question answering. ArXiv abs/2204.07469 (2022), https://api.semanticscholar.org/CorpusID:248218762 5. Dauphin, Y.N., Fan, A., Auli, M., Grangier, D.: Language modeling with gated convolutional networks. In: Precup, D., Teh, Y.W. (eds.) Proceedings of the 34th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 70, pp. 933–941. PMLR (06–11 Aug 2017), https://proceedings.mlr. press/v70/dauphin17a.html 6. Diggelmann, T., Boyd-Graber, J., Bulian, J., Ciaramita, M., Leippold, M.: Climate-fever: A dataset for verification of real-world climate claims (2021) 7. Eigen, D., Ranzato, M., Sutskever, I.: Learning factored representations in a deep mixture of experts (2014) 8. Fedus, W., Zoph, B., Shazeer, N.: Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res. 23(1) (jan 2022) 9. Gaur, N., Farris, B., Haghani, P., Leal, I., Moreno, P.J., Prasad, M., Ram- abhadran, B., Zhu, Y.: Mixture of informed experts for multilingual speech recognition. In: ICASSP 2021 - 2021 IEEE International Conference on Acous- tics, Speech and Signal Processing (ICASSP). pp. 6234–6238 (June 2021). https://doi.org/10.1109/ICASSP39728.2021.9414379 10. Gururangan, S., Lewis, M., Holtzman, A., Smith, N.A., Zettlemoyer, L.: Demix layers: Disentangling domains for modular language modeling (2021) 11. Hashemi, H., Zhuang, Y., Kothur, S.S.R., Prasad, S., Meij, E., Croft, W.B.: Dense retrieval adaptation using target domain description (2023) 12.