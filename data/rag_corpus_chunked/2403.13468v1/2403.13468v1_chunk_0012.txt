of 25 more documents unavailable in FEVER. 8 Kasela et al. T able 1.Characteristics of the datasets used. Labeled queries and the average number of labels per query refer to training queries only. Dataset #Docs #Training #Validation #Test Labeled docs Labeled queries Avg labels NaturalQuestions [17] 2,681,468 132,803 - 3,452 97.1% 97.8% 2.04 HotpotQA [28] 5,233,329 85,000 5,447 7,405 95.45% 99.9% 3.62 FEVER [27] 5,416,568 109,810 6,666 6,666 91.96% 99.1% 2.28 Climate-FEVER [6] 5,416,593 - - 1,535 91.95% - - Query-domain labels. As discussed in the previous section, the DESIRE-ME gating function is trained in a supervised way by exploiting domain labels avail- able for the training queries. We automatically generated such labels for all the questions in the first three datasets by resorting to the category assigned by con- tributors to their Wikipedia articles6. For example, the page onEleventh Amend- ment to the United States Constitution belongs to the category Law. In contrast, the page on Chinese New Year belongs to categories Human behavior, Cul- ture, Society, and Religion. The straightforward approach we employ to create query labels involves assigning to each query the category of the corresponding Wikipedia article containing the relevant passage. However, this basic method- ology proved inadequate in specific situations, necessitating the implementation of more specific actions. The first issue arises when the relevant Wikipedia ar- ticle lists specific subcategories without mentioning the main category. In such instances, starting from each subcategory, we navigate the Wikipedia category graph backward in a breadth-first manner until we reach the category to which the subcategory belongs. The second scenario occurs when the relevant article pertains to multiple categories and/or two or more Wikipedia pages are perti- nent to the same query. In such cases, we identify the categories for each page and simply label the query with all