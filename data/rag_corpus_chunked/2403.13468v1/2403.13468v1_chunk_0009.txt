of the proposed model. The gating function. It has the primary purpose of computing the likelihood for the query to belong to any of M predefined domains. Our gating mechanism differs from classical MoE gating functions in several ways. Firstly, it relies on a multi-label domain classifier. Using a classifier as a gating function is not entirely novel in MoE; for example, in [10] a Bayes posterior probability model is used to compute the output values of the gating function. Instead, we do not make the assumption of mutual exclusivity of labels, and we allow an input to belong to multiple domains. To handle multiple labels per query, we enforce that each domain is classified independently by applying a sigmoid function to the gating function output, as opposed to the commonly used softmax function. The use of softmax could compel the model to specialize even for out-of-domain queries, potentially resulting in unexpected outcomes. Another difference from the clas- sical MoE models, where the gating function and the specializersâ€™ representation are trained together, is that we train end-to-end the gating function and the specializers using two distinct loss functions. While the multi-label classifier is trained using binary cross-entropy, the MoE specializers rely on the contrastive loss computed on query-document similarity, i.e., the same loss function em- ployed for training the underlying dense retrieval bi-encoder architecture. The multi-label classifier used and the process followed for generating the query labels and training it are detailed in Section 4. The specializers. They are very similar to those proposed in [14]. Each of the M specializers focuses on tuning the input query representation for the corre- sponding domain. At training time they learn via the contrastive loss function how to contextualize the query for the specific domain. The pooling module. Finally we have the pooling