on NQ). This suggests that text-based docids, such as n-grams or ti- tles, offer more flexibility and generalization. Text-based docids can inherently adapt to newly added documents, which often contain similar linguistic features. Consequently, models using text-based docids are better equipped to maintain high retrieval performance as the corpus evolves. These models can leverage the semantic richness of text-based representations, allowing them to effectively handle the continuous expansion of the document set without sig- nificant performance loss. Incremental training vs direct generalization. We implemented incremental training inspired by the DSI++ framework for both DSI and SEAL. Our approach involves two key components: (i) incremental indexing training, where models are fine-tuned on newly added documents to learn their docid mappings, and (ii) random replay training, which reintroduces a subset of historical documents during fine-tuning to mitigate forgetting. As shown in Table 3, incremental training significantly enhances the models’ Table 3: Comparison of retrieval performance (Hit@10) on the NQ dataset for initial and newly added documents. Model NQ (Hit@10) D0 D1 D2 D3 D4 D5 Initial documents DSI 0.718 0.710 0.706 0.702 0.699 0.696 DSI++ 0.718 0.697 0.687 0.682 0.676 0.673 SEAL 0.809 0.806 0.788 0.774 0.774 0.763 SEAL++ 0.809 0.791 0.788 0.781 0.776 0.766 Newly added documents DSI 0.718 0.231 0.203 0.221 0.185 0.205 DSI++ 0.718 0.677 0.671 0.667 0.657 0.644 SEAL 0.809 0.744 0.736 0.727 0.727 0.725 SEAL++ 0.809 0.768 0.756 0.744 0.743 0.733 ability to retrieve newly added documents for DSI. For instance, DSI++ improves Hits@10 for new documents on NQ from 0.205 to 0.644 on D5, demonstrating better generalization to unseen doc- uments. However, this comes at a cost: DSI++ exhibits noticeable forgetting on initial documents. For SEAL, incremental training yields only a slight improvement in both settings. This is because SEAL’s text-based docids inherently accommodate semantic varia-