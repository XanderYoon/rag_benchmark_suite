[33]), and text-based methods that leverage titles, URLs, or n-grams as docids [2, 5]. However, most GR approaches are built and evaluated in static scenarios, where the document collection remains fixed [9, 17, 29, 32]. Although these models perform strongly in such settings, their effectiveness in real-world environments, where documents contin- uously evolve over time, has not been thoroughly evaluated [3, 11]. Unlike static corpora, dynamic corpora impose higher demands on a model’s generalization and robustness as new documents are not encountered during training. While previous studies have found that several GR models perform poorly over the dynamic corpora [3, 12], only limited efforts have been made to comprehen- sively investigate the generalization ability of different GR models under fair experimental settings. An intuitive solution is to retrain the GR model from scratch when the underlying corpus is updated. However, the prohibitive computational costs of model training make this approach unfeasi- ble [4]. Consequently, prior studies have explored continual (i.e., in- cremental) learning techniques to enhance the generalization ability of GR models with less computational overhead [7, 23]. For example, Mehta et al. [23] proposes DSI++, which employs sharpness-aware minimization to optimize for flat loss basins, thereby enabling the model to effectively learn from newly added documents. Guo et al. [7] develop task-specific adapters [8, 21] and pre-training objectives to adapt the dynamic corpora. As shown in Table 3, we find that SEAL [2], which employs text-based docids, can achieve comparable performance over dynamic corpora without re-training. Therefore, our reproducibility study mainly focuses on the mod- els’ ability to generalize to unseen documents without additional training, and on analyzing the factors that influence the perfor- mance in dynamic corpora. Specifically, we first construct two dy- namic corpora datasets based on the NQ [13] and MS-MARCO [1] datasets, by partitioning the