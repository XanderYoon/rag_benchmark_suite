âˆ‘ï¸ ğ‘§ğ‘– âˆˆğ‘‘ ğ‘— 1 rank(ğ‘§ğ‘– ) , (8) where Coverage Count(ğ‘‘ ğ‘— ) is the number of distinct docids in ğ‘‘ ğ‘— ; rank(ğ‘§ğ‘– ) is the position of docid ğ‘§ğ‘– in the beam search (with 1 being the highest rank); and ğ›½ is a hyperparameter that controls the importance of the ranking term relative to the coverage count. Replication and Exploration of Generative Retrieval over Dynamic Corpora SIGIR â€™25, July 13â€“18, 2025, Padua, Italy Table 6: Comparison of retrieval performance (Hit@10) on the NQ dataset for newly added documents. Method NQ (Hit@10) D0 D1 D2 D3 D4 D5 BM25 0.647 0.620 0.588 0.598 0.552 0.571 DPR-HN 0.826 0.645 0.644 0.626 0.621 0.624 NCI 0.871 0.464 0.437 0.433 0.358 0.323 SEAL 0.809 0.744 0.736 0.727 0.727 0.725 MDGR 0.824 0.717 0.704 0.695 0.647 0.633 Ablation Study w/o constrain 0.824 0.447 0.424 0.408 0.377 0.356 w/o multi-docid 0.831 0.523 0.511 0.497 0.488 0.473 MDGR (size=64) 0.843 0.472 0.436 0.417 0.409 0.381 MDGR (size=8192) 0.674 0.574 0.565 0.533 0.531 0.512 6.3 Implementation and evaluation results Our model is implemented using the T5-base architecture. We ini- tialize the model with pre-trained weights from Hugging Faceâ€™s T5-base checkpoint. Following previous work, the training objec- tive includes three parts: (i) Training with synthetic queries. Minimize the loss between generated pseudo-queries and their cor- responding docids. (ii) Encoding document contexts. Minimize the loss between each document chunk ğ‘ğ‘– and its original docid. (iii) Incorporating real queries. Minimize the loss between user queries and target docids. For training, we employ the AdamW optimizer with a base learn- ing rate of 1 Ã— 10âˆ’5 and a linear warmup over the first 10% of training steps. We set the batch size to 64 and train for 10 epochs, which requires approximately 4 hours on 8 NVIDIA A100 GPUs.