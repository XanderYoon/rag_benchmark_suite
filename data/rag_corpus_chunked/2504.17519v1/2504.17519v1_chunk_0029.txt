queries. Minimize the loss between user queries and target docids. For training, we employ the AdamW optimizer with a base learn- ing rate of 1 × 10−5 and a linear warmup over the first 10% of training steps. We set the batch size to 64 and train for 10 epochs, which requires approximately 4 hours on 8 NVIDIA A100 GPUs. To ensure efficient document processing, we split documents into chunks using a sliding window approach: each chunk contains 256 tokens with a stride of 128 tokens. To get the numeric-based do- cids, we first generate semantic embeddings for text chunks using a frozen BERT-base model, then apply product quantization to con- vert these continuous vectors into docids. The docids have a size of 1024 and a length of 4. As shown in Table 6, the MDGR framework exhibits competitive effectiveness in comparison to existing retrieval models. While the performance of MDGR is not the absolute best across all document sets, it achieves solid results over dynamic corpora. We also show some ablation variant to evaluate three critical design of our method: First, removing the constrained docid expan- sion strategy leads to significant deterioration, demonstrating that generating new docids for updated documents would disrupt the semantic consistency of numbering-based docids. Second, when disabling multi-docid indexing (single-docid per document), the performance drops significantly, confirming our hypothesis that fine-grained docid design can better preserves granular seman- tic associations. Furthermore, we investigate the impact of docid size. Small docid sizes (64) lead to substantially degrade on newly added documents. Conversely, excessively large sizes (8192) create an over-discretized learning space where the model struggles to establish stable query-docid mappings. Table 7: Experiments about memory costs and efficiency. Method Memory Tok-K Latency DPR 980MB 100 152ms NCI 865MB 10 216ms 100 269ms SEAL 2200MB 10 619ms