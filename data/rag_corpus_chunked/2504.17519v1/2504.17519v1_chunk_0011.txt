NQ (Hit@10) MS-MARCO (Hit@10) D0 D1 D2 D3 D4 D5 ğ¹ğ‘› â†“ D 0 D1 D2 D3 D4 D5 ğ¹ğ‘› â†“ Sparse retrieval BM25 Term Weight 0.647 0.625 0.611 0.598 0.573 0.573 0.051 0.653 0.640 0.632 0.629 0.619 0.614 0.026 Dense retrieval DPR Dense Vector 0.725 0.704 0.696 0.686 0.670 0.660 0.042 0.683 0.681 0.668 0.656 0.651 0.648 0.022 DPR-HN Dense Vector 0.826 0.801 0.797 0.776 0.773 0.768 0.043 0.723 0.712 0.692 0.685 0.672 0.664 0.038 Generative retrieval DSI-SE Category Nums 0.718 0.710 0.706 0.702 0.699 0.696 0.015 0.605 0.601 0.597 0.594 0.592 0.589 0.010 Ultron-PQ Category Nums 0.795 0.785 0.780 0.780 0.762 0.755 0.023 0.663 0.655 0.647 0.643 0.637 0.632 0.020 NCI Category Nums 0.871 0.856 0.844 0.839 0.811 0.802 0.041 0.702 0.693 0.673 0.667 0.654 0.633 0.038 GenRET Category Nums 0.858 0.853 0.836 0.829 0.812 0.796 0.033 0.717 0.697 0.688 0.674 0.659 0.652 0.043 Ultron-URL URL Path 0.816 0.810 0.794 0.781 0.780 0.768 0.029 0.626 0.620 0.618 0.614 0.611 0.608 0.012 SEAL N-gram 0.809 0.806 0.788 0.774 0.774 0.763 0.028 0.661 0.641 0.625 0.616 0.602 0.598 0.045 MINDER Multi-text 0.838 0.828 0.813 0.811 0.801 0.773 0.033 0.667 0.649 0.633 0.625 0.612 0.600 0.043 LTRGR Multi-text 0.862 0.857 0.846 0.827 0.813 0.807 0.032 0.688 0.675 0.660 0.649 0.636 0.621 0.040 generation strategies to train the model. (v) GenRET [27] uses con- strained cluster centroids as docids and trains the representations of docids through document tokenization and document reconstruc- tion tasks. (vi) SEAL [2] uses arbitrary n-grams from documents as docids and generates a series of n-grams under the constraints of the FM-index to retrieve the corresponding documents. (vii) MIN- DER [15] adopts multiple text types to represent docids, such as titles, URLs, and n-grams. Different types of scores are generated at the same time and documents are retrieved based