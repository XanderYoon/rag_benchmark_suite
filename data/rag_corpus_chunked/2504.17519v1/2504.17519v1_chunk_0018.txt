not all text-based generative retrieval models perform equally well. For instance, models like SEAL demonstrate better performance compared to others like Ultron-URL. Further exploration is needed to determine which specific text features are most suitable for dynamic corpora scenario. 5 Analysis of Docid Design To understand the pros and cons of different docid designs for GR in dynamic corpus scenarios, we conduct a series of experiments. In Section 5.1, we introduce the Initial Document Bias Index (IDBI) to analyze the bias of different GR methods towards older documents when new documents are added. We observe that numeric-based docids exhibit significantly higher bias compared to text-based docids, which, to some extent, explains the poor performance of these methods in dynamic corpus settings. Then, in Section 5.2, we conduct a comprehensive ablation study on text-based docid methods to investigate how docid type, granularity, and vocabulary size affect modelsâ€™ generalization to new documents. Our analysis shows that more semantic, finer-grained, and larger vocabulary choices often lead to superior results. 5.1 Bias to initial documents To explain the poor performance of numeric-based docids on new documents, we hypothesize as follows: Hypothesis 1 (Semantic Familiarity). The effectiveness of gen- erative retrieval (GR) on new documents is correlated with how well the docid representations align with the language modelâ€™s pretraining dis- tribution. Formally, let ğ‘ƒLM (ğ‘¥) denote the token distribution learned by the language model over the vocabulary V, and let ğ‘ƒdocid (ğ‘¥) rep- resent the probability distribution induced by the docid representation space. We define the semantic familiarity of a docid system as: S = Eğ‘¥âˆ¼ğ‘ƒdocid [log ğ‘ƒLM (ğ‘¥)] . (6) A higher value of S indicates better alignment between the docid distribution and the language modelâ€™s pretraining distribution. Text-based docids inherently preserve distributional alignment with the underlying language model. By leveraging substrings (e.g., n-grams)