Replication and Exploration of Generative Retrieval over Dynamic Corpora SIGIR ’25, July 13–18, 2025, Padua, Italy Table 4: Comparison of retrieval performance (Hit@10) on the NQ dataset for initial and newly added documents across various text-type docid representations. Text Type NQ (Hit@10) D0 D1 D2 D3 D4 D5 Initial documents n-gram 0.753 0.712 0.688 0.666 0.642 0.626 url 0.759 0.757 0.757 0.754 0.754 0.752 title 0.633 0.630 0.615 0.588 0.579 0.558 query 0.664 0.651 0.647 0.645 0.644 0.637 Newly added documents n-gram 0.753 0.695 0.677 0.632 0.627 0.607 url 0.759 0.325 0.296 0.263 0.208 0.168 title 0.633 0.362 0.341 0.336 0.322 0.308 query 0.664 0.341 0.335 0.312 0.318 0.306 semantic alignment, they still exhibit retrieval bias, particularly for titles and URLs. This is likely due to their abstract nature, which fails to capture fine-grained document content. In contrast, BM25 and DPR, which perform direct text similarity matching without relying on predefined docids, show minimal retrieval bias. Their ability to retrieve new documents equitably stems from their reliance on content-based representations rather than fixed identifier mappings. N-gram docids achieve comparable performance by structurally aligning with the generative model’s learning paradigm. Unlike titles or URLs, which require abstract semantic interpretation, n- grams preserve raw token sequences, which the model inherently optimizes for during generation. This alignment ensures that even previously unseen documents benefit from the model’s pre-existing familiarity with local linguistic patterns. 5.2 Ablation study of text-based docid To examine the key factors of text-based docid design, we conduct an ablation study across three aspects: (i) Docid type, where we compare different docid types, such as title, URL, n-grams, etc. (ii) Docid granularity, where we compare docids defined at different levels of granularity, including document, paragraph, sentence, and n-gram levels. (iii) Docid lexical diversity, where we compare the number of possible tokens