Corpora SIGIR â€™25, July 13â€“18, 2025, Padua, Italy 2.2 Task formulation The dynamic corpora involves continuously adapting to an expand- ing document collection while maintaining retrieval capabilities. Initially, we are given: â€¢ an initial document set D0 = {ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘› }, and â€¢ corresponding query-document pairs P0 = {âŸ¨ğ‘ğ‘–, ğ‘‘ğ‘— âŸ© | ğ‘ğ‘– âˆˆ Q, ğ‘‘ğ‘— âˆˆ D 0}, where each document ğ‘‘ ğ‘— âˆˆ D 0 is a text sequence and Q = {ğ‘1, ğ‘2, . . . ,ğ‘ğ‘š } denotes the initial query set. A GR model is first trained using standard sequence-to-sequence objectives on P0. The core challenge arises with incremental updates: when new documents Dnew = {ğ‘‘ğ‘›+1, . . . , ğ‘‘ğ‘›+ğ‘˜ } are introduced. For each new document ğ‘‘ğ‘›+ğ‘– âˆˆ D new, the indexer generates a corresponding docid: zğ‘›+ğ‘– = {ğ‘§ (ğ‘›+ğ‘– ) 1 , . . . , ğ‘§(ğ‘›+ğ‘– ) ğ‘€ }, ğ‘§ (ğ‘›+ğ‘– ) ğ‘¡ âˆˆ { 1, . . . , ğ¾}, (3) preserving the original token vocabulary and sequence length ğ‘€. These new identifiers are incorporated into the existing prefix tree TD. The retriever must then handle the expanded search space Dâ€² = D0 âˆª Dnew. Two adaptation strategies emerge: (i) Model adap- tation: Continuously train or retrain the model on P0 âˆª Pnew to learn updated document representations. (ii) Index adaptation: Maintain frozen model parameters while updating only the index- ing structures. As mentioned in the Introduction Section, retraining or continuing training the GR model would cost much more compu- tational overhead. Therefore, our work only focuses on the second paradigm, where the Retriever must leverage learned knowledge to handle novel documents through constrained decoding over the updated prefix tree T â€² D. This setting tests the modelâ€™s ability to generalize to unseen document representations