but avoids the tendency to generate previously seen docids. It achieves finer granularity and larger docid size, and demonstrates superior performance and efficiency in dynamic corpora scenario. In summary, our main contributions are as follows: (i) We in- troduce a comprehensive evaluation of existing GR approaches over dynamic corpora, focusing on their ability to generalize to unseen documents without additional training. (ii) We reveal the limitations of numeric-based docids in dynamic retrieval scenarios that they have a tendency toward the initial document set. (iii) We observed that the text-based docid performs better on dynamic corpora and analyzed key factors for its performance, including semantic alignment, fine-grained docids design, and high lexical diversity. (iv) Building on our findings, we propose a novel GR framework that integrates the high efficiency and low memory of numeric-based docids with the strong generalization capabilities of text-based docids. 2 Background and Preliminaries 2.1 Generative retrieval The document retrieval task retrieves a relevant document ğ‘‘ from a document collection D given a user query ğ‘. Traditional retrieval approaches typically rely on inverted indexing or similarity-based ranking; whereas GR retrieves documents by autoregressively gen- erating the docid ğ‘§ corresponding to the most relevant document. A GR model typically consists of two key components: anindexer and a retriever. The Indexer acts as a document encoder that maps each document ğ‘‘ âˆˆ D to a unique identifier sequence (docid) z = {ğ‘§1, ğ‘§2, . . . , ğ‘§ğ‘€ }. Here each element ğ‘§ğ‘¡ âˆˆ [ ğ¾] is drawn a predefined vocabulary ğ‘‰ , which may include numerical tokens, lexical items, or other semantic identifiers. There are mainly two types of docid design: numeric-based and text-based: â€¢ numeric-based: Each document corresponds to a numeric se- quence, typically generated by converting document embeddings into numeric sequences using clustering or quantization meth- ods.