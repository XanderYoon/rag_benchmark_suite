0.425 0.396 0.350 0.331 0.388 Ultron-URL URL Path 0.816 0.553 0.545 0.543 0.541 0.532 0.543 0.626 0.397 0.376 0.364 0.354 0.342 0.367 SEAL N-gram 0.809 0.744 0.736 0.727 0.727 0.725 0.732 0.661 0.611 0.607 0.584 0.571 0.559 0.586 MINDER Multi-text 0.838 0.803 0.751 0.746 0.742 0.736 0.756 0.667 0.614 0.608 0.587 0.569 0.546 0.585 LTRGR Multi-text 0.862 0.831 0.803 0.811 0.779 0.773 0.799 0.688 0.621 0.612 0.601 0.589 0.577 0.600 achieves ğºğ´ğ‘› scores of 0.586 on NQ and 0.618 on MS-MARCO, while DPR attains 0.562 and 0.612 respectively. Generative retrieval models exhibit huge divergence in gen- eralization performance ( ğºğ´ğ‘›): Numeric-based docid (e.g., DSI- SE, Ultron-PQ) demonstrate critical limitations in adapting to un- seen documents. DSI-SEâ€™sğºğ´ğ‘› values (0.209 on NQ, 0.184 on MS- MARCO) align with its severe Hits@10 degradation when retriev- ing new documents (e.g., dropping from 0.718 to 0.205 on NQâ€™s D5). This failure stems from rigid numeric docid mappings learned during training, which lack inherent semantic connections to new content. Similarly, Ultron-PQâ€™s numeric identifiers yield unstable ğºğ´ğ‘› (0.542 on NQ, 0.400 on MS-MARCO), as its quantization-based docid system struggles to encode novel document semantics with- out retraining. Text-based docid methods, such as SEAL and LTRGR, demon- strate much more promising results. For example, LTRGR attain ğºğ´ğ‘› = 0.799 on NQ, with only a 0.089 Hits@10 drop from D0 to D5, outperforming even optimized dense retrieval (DPR-HN: ğºğ´ğ‘› = 0.632). SEALâ€™s n-gram docids similarly excel (ğºğ´ğ‘› = 0.732 on NQ). This suggests that text-based docids, such as n-grams or ti- tles, offer more flexibility and generalization. Text-based docids can inherently adapt to newly added documents, which often contain similar linguistic features. Consequently, models using text-based docids are better equipped to maintain high retrieval performance as the corpus evolves. These models can leverage the semantic richness of text-based representations,