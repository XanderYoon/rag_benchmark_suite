documents. Based on these findings, we use docid size of ğ‘˜ = 1024 for our following study. Designing fine-grained docids. We explore a multi-docid ap- proach to designing numeric-based docids with finer granular- ity. Specifically, we partition a documentğ‘‘ into semantic chunks {ğ‘1, ğ‘2, . . . , ğ‘ğ‘ } using a sliding window approach, where each ğ‘ğ‘– represents a text fragment of the document. Each chunk is inde- pendently mapped to a numeric-based docid ğ‘§ğ‘– through product quantization (PQ). This results in multiple number sequences per document, analogous to the multi-docid design of n-grams. Constrained docid expansion. To address the semantic gap in numeric-based docids, we propose a constrained docids expansion strategy on dynamic corpora. We store all docids from the initial document collection, and constrain the indexer to use only these existing docids when indexing new document chunks. 6.2 Inference strategy of MDGR To retrieve the relevant documents, we propose a simple document ranking strategy for our approach. Given a query, we first per- form constrained beam search to generate several candidate docids {ğ‘§1, ğ‘§2, . . . , ğ‘§ğ‘˜ }. We then retrieve all documents containing at least one of these docids. For each candidate document ğ‘‘ ğ‘— , we compute a score based on the weighted sum of the beam search positions of the contained docids. The score for document ğ‘‘ ğ‘— is given by: Score(ğ‘‘ ğ‘— ) = Coverage Count(ğ‘‘ ğ‘— ) + ğ›½ Ã— âˆ‘ï¸ ğ‘§ğ‘– âˆˆğ‘‘ ğ‘— 1 rank(ğ‘§ğ‘– ) , (8) where Coverage Count(ğ‘‘ ğ‘— ) is the number of distinct docids in ğ‘‘ ğ‘— ; rank(ğ‘§ğ‘– ) is the position of docid ğ‘§ğ‘– in the beam search (with 1 being the highest rank); and ğ›½ is a hyperparameter that controls the importance of the ranking term relative to the coverage count.