design that retains the efficiency advantages of previous approaches while addressing their limitations in dynamic corpora scenarios. 6.1 Docid design of MDGR Optimizing vocabulary size. Building on the insights from anal- ysis of lexical diversity of text-based docid, we hypothesize that expanding the decoding dimension for number-type docids in GR models could similarly influence their adaptability to dynamic cor- pora. To test this hypothesis, we adapt the Ultron-PQ framework by modifying its PQ parameters - the number of clusters in each subspace. In PQ-based docid generation, document embeddings are partitioned into ğ‘š subvectors, each quantized into ğ‘˜ clusters via k-means. By varying ğ‘˜ while fixing ğ‘š, we systematically control D0 D1 D2 D3 D4 D5 /uni00000027/uni00000052/uni00000046/uni00000058/uni00000050/uni00000048/uni00000051/uni00000057/uni00000003/uni00000026/uni00000052/uni0000004f/uni0000004f/uni00000048/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056 /uni00000013/uni00000011/uni00000017/uni00000013 /uni00000013/uni00000011/uni00000018/uni00000013 /uni00000013/uni00000011/uni00000019/uni00000013 /uni00000013/uni00000011/uni0000001a/uni00000013 /uni00000013/uni00000011/uni0000001b/uni00000013/uni0000002b/uni0000004c/uni00000057/uni00000023/uni00000014/uni00000013 /uni00000027/uni0000004c/uni00000050/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000052/uni00000051 /uni00000019/uni00000017 /uni00000015/uni00000018/uni00000019 /uni00000014/uni00000013/uni00000015/uni00000017 /uni00000017/uni00000013/uni0000001c/uni00000019 /uni0000001b/uni00000014/uni0000001c/uni00000015 Figure 3: Hit@10 performance of different decoding dimen- sion of Ultron-PQ on NQ dataset the decoding dimension: our experiments explore ğ‘˜ âˆˆ { 64, 256, 1024, 4096, 8192} with ğ‘š = 4. As shown in Figure 3, the experimental results highlight two key patterns in docid dimension scaling. First, expanding the docid size from ğ‘˜ = 64 to ğ‘˜ = 1024 shows minimal impact on initial document retrieval but substantially improves performance on new documents. Second, excessive dimensions (ğ‘˜ = 4096, 8192) cause sharp declines across all test sets. This could due to oversized size create sparse, under-trained docid mappings, which cause the model fails to establish reliable semantic-code relationships, particularly for new documents. Based on these findings, we use docid size of ğ‘˜ = 1024 for our following study. Designing fine-grained docids. We explore a multi-docid ap- proach to designing numeric-based docids with finer granular- ity. Specifically, we partition a documentğ‘‘ into semantic chunks {ğ‘1, ğ‘2, . . . , ğ‘ğ‘ } using a sliding window approach, where each ğ‘ğ‘– represents a