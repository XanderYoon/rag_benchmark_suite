This process comprises four steps: (1) Document Parsing, (2) MDoc2Query, (3) Graph Assembly, and (4) Vector & Graph Storage. (1) Document Parsing.We adopt an existing multimodal PDF parsing tool, such as MinerU [40], to extract structured information from the document. Given a PDF document ğ·={ğ‘ƒ 1, ğ‘ƒ2, . . . , ğ‘ƒğ‘‹ } of ğ‘‹ pages, we extract a layout-preserving sequence of heterogeneous document components, including: (i) paragraphs (text only); (ii) fig- ures (images with captions); (iii) tables (table images accompanied by textual captions and OCR-converted Markdown text); and (iv) equations (parsed into Markdown text). The extracted content is stored in a JSON format ordered by visual layout position. Based on this structured output, we define a set of modality-specific chunks: C={ğ‘ 1, ğ‘2, . . . , ğ‘ğ‘ }, ğ‘ ğ‘– âˆˆ {text,image}.(1) Paragraph text is segmented into overlapping spans using a sliding window with a maximum token length and a fixed stride. Equa- tions are treated as part of the regular text content. In contrast, each figure or table is treated as an image-modality chunk, while preserving its associated caption and any structured OCR-derived textual content. Visual noise filtering can be further applied to remove uninformative images (e.g., blank pages, string-only im- ages, or logos; see Appendix B) via visual chunk classification using zero-shot CLIP inference. (2) MDoc2Query.To bridge document content with potential information needs, we extend the Doc2Query paradigm to mul- timodal settings. For each chunk ğ‘ğ‘– âˆˆ C , we employ a Large Vision-Language Model (LVLM) to generate a set of answerable queryâ€“answer pairs: Qğ‘– = n (ğ‘ (1) ğ‘– , ğ‘(1) ğ‘– ), . . . ,(ğ‘ (ğ‘€ğ‘– ) ğ‘– , ğ‘(ğ‘€ğ‘– ) ğ‘– ) o , ğ‘€ ğ‘– â‰¤ğ‘€ max.(2) Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu Figure