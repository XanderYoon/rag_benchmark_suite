intro- duces a query-centric graph structure that connects queries to their source textual chunks, enabling multi-hop retrieval and structured evidence aggregation in long-document scenarios. Yet, these meth- ods remain unimodal, lacking explicit modeling of cross-modal relationships. Extending query-centric expansion to multimodal documents remains an open problem for capturing fine-grained, heterogeneous associations. MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY 3 Methodology 3.1 Preliminaries We consider the task of multimodal long-context document question answering, where the goal is to answer a user natural language question ğ‘ğ‘¢ based on a multimodal long-context document ğ· that spans multiple pages and contains heterogeneous content. Each document ğ· originates from a PDF file and is processed into a sequence of pages ğ·={ğ‘ƒ 1, ğ‘ƒ2, . . . , ğ‘ƒğ‘‹ } via OCR. Each page ğ‘ƒğ‘¥ consists of a set of modality-specific chunks ğ¶ğ‘– ={ğ‘ 1, ğ‘2, . . . , ğ‘ğ‘ }, where each chunk ğ‘ is associated with a modality (text ( ğ‘¡ğ‘¥ğ‘¡ ) or image (ğ‘–ğ‘šğ‘”)) and a content type (e.g., paragraph, figure, table, or equation). Text chunks are contiguous paragraphs or extracted titles; image chunks consist of a visual region paired with an associated caption; table chunks include a caption, a rendered table image, and an OCR- converted Markdown-style textual representation [40]. The QA task follows a retrieval-augmented generation (RAG) paradigm: given a query ğ‘, a retriever selects the top-ğ¾ relevant multimodal chunks {ğ‘â€² 1, . . . , ğ‘â€² ğ¾ } from ğ·, and a LVLM conditions on these chunks to generate the final answer Ë†ğ‘. The primary challenge lies in retrieving semantically aligned and cross-modally grounded chunks from the long, heterogeneous document to support accurate and coherent generation. 3.2 Framework Overview To tackle the challenges of cross-modal heterogeneity and long- range reasoning in