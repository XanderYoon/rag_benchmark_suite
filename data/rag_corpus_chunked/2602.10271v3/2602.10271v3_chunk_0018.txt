ments from PDF into structured JSON files. Chunks are constructed as follows: (1) Text: Paragraphs are segmented using a maximum length of 1200 tokens with an overlap of 100 tokens. Equations parsed as Markdown are treated as regular text. (2) Image: Figures and tables, together with their captions and OCR-derived content, are treated as individual image-modality chunks. In addition, doc- ument pages are rendered as images for page-level methods and MDoc2Query optimization. We further apply visual noise filter- ing to image chunks via zero-shot classification using CLIP1, with details in Appendix B. Model Configuration.We employ the BGE-m3 encoder to gener- ate dense embeddings for queries, which are stored and indexed in ElasticSearch [22] as the core vector database to support efficient similarity search. For query generation and final answer generation, we use Qwen2.5-VL-32B2 [4] by default, whileQwen2.5-VL-7B3 [4] is adopted for MDoc2Query optimization. For evaluation, we adopt an LLM-as-a-Judge setup using Qwen2.5-72B4 [36, 45]. All LLM- s/LVLMs are deployed via the SGLang [51] framework on NVIDIA H20 GPUs to enable high-throughput inference. Default Hyperparameters.For experiments on both MMLongBench- Doc and LongDocURL, we adopt a unified set of default hyperpa- rameters. Specifically, we set the KNN neighborhood size to ùëò= 3 for query‚Äìquery edge construction, retrieve up to ùëõ= 10query nodes with a similarity threshold ùõº= 1.2, and perform ‚Ñé= 2-hop query expansion. For answer generation, the top- ùêæ= 5ranked multimodal chunks are selected as the retrieval context. 5 Experimental Results In this section, we analyse the experimental results with respect to the four research questions stated in Section 4 to gauge the effectiveness of our proposed MLDocRAG. 5.1 MLDocRAG vs. Baselines (RQ1) Table 1 reports the performance of MLDocRAG and representative baselines on MMLongBench-Doc and LongDocURL in terms of accu- racy (%). Overall, MLDocRAG achieves the best overall performance on