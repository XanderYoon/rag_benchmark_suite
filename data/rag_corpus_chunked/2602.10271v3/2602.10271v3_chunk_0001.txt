NY, USA, 15 pages. https://doi. org/XXXXXXX.XXXXXXX 1 Introduction Multimodal long-context documents, such as research papers, re- ports, and books, often span tens to hundreds of pages and contain diverse multimodal components/chunks including text, images, and tables [7, 21, 28, 33, 38]. Understanding such lengthy multimodal documents presents two central challenges [ 7]: (1)cross-modal heterogeneity, which requires identifying and localizing relevant Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym ’XX, Woodstock, NY ©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX information across heterogeneous modalities; and (2)cross-page reasoning, which demands integrating evidence scattered across multiple pages to support coherent inference. Addressing these challenges necessitates the ability ofmultimodal long-context asso- ciation—accurately identifying, connecting, and integrating seman- tically relevant information across modalities and segments. Large Vision-Language Models (LVLMs) have shown strong cross-modal understanding capabilities of effectively aligning and interpreting multimodal information within localized short con- texts [9, 19, 46]. Representative examples include GPT-4o [20], Gem- ini [35], Qwen2.5-VL [3], and InternVL [6], which demonstrate im- pressive performance on short-range multimodal reasoning bench- marks [23, 43]. However, they often struggle to maintain consistent semantic modeling within limited context windows when applied to long-document scenarios [24]. In particular, their performance deteriorates when evidence is sparsely distributed across pages and modalities