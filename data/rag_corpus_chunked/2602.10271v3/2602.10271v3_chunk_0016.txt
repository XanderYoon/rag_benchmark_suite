perspectives? 4.1 Datasets & Metrics Datasets.We evaluate our MLDocRAG on two multimodal long- context document QA benchmarks: (1)MMLongBench-Doc[ 25]: A curated benchmark for multimodal long-document understand- ing, consisting of documents in PDF format with diverse content including text, images, tables, and charts. Questions are paired with answerable evidence spread across pages and modalities. (2) Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu LongDocURL[ 8]: A newly collected dataset containing web-based scientific and technical documents. Each instance includes a docu- ment (in PDF or HTML format), a natural language question, and annotated answerable segments across multimodal content. Metrics.We adopt exact match Accuracy as the primary evalua- tion metric to measure the factual correctness of generated answers. To ensure consistent and scalable judgment across modalities and formats, we follow recent work and employ anLLM-as-a-Judge protocol [16], using a strong LLM (e.g., Qwen2.5-72B [36, 45]) to verify whether the predicted answer matches the gold reference answer. The evaluation prompt is shown in Appendix A. 4.2 Baselines We compare MLDocRAG with representative baselines from five categories: â€¢Text Only (txt).Use only text chunks with or without basic retrieval (BM25 [30] / BGE-m3 [5]): LCğ‘¡ğ‘¥ğ‘¡ with textual long-context reasoning, RAGğ‘¡ğ‘¥ğ‘¡ BM25 with sparse retrieval, RAGğ‘¡ğ‘¥ğ‘¡ BGE-m3 with dense retrieval. â€¢Image2Text (txt+i2t).Augment text with LVLM-generated image captions, treated as plain text: LCğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ , RAGğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ BM25 , RAGğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ BGE-m3. â€¢Multimodal (txt+img).Encode both text and image chunks using multimodal embedding models for dense retrieval: LCğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” for multimodal long-context reasoning, MRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” CLIP with CLIP [29], MRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” SigLIP with SigLIP [37, 47], MRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” ColPali with ColPali [14]. â€¢Page-level (page).Render full document pages as images and perform vision-only retrieval followed by VQA: LCğ‘ğ‘ğ‘”ğ‘’ for visual long-context reasoning, VRAGğ‘ğ‘ğ‘”ğ‘’ CLIP, VRAGğ‘ğ‘ğ‘”ğ‘’ SigLIP, VRAGğ‘ğ‘ğ‘”ğ‘’ ColPali. â€¢Graph.Construct knowledge graphs from extracted multimodal entities with the augmented text (i.e.,txt+i2t) for graph-based