MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Yongyue Zhang Independent Researcher Singapore yongyue002@gmail.com Yaxiong Wu Independent Researcher Singapore wuyashon@gmail.com Abstract Understanding multimodal long-context documents that comprise multimodal chunks such as paragraphs, figures, and tables is chal- lenging due to (1) cross-modal heterogeneity to localize relevant information across modalities, (2) cross-page reasoning to aggre- gate dispersed evidence across pages. To address these challenges, we are motivated to adopt a query-centric formulation that projects cross-modal and cross-page information into a unified query repre- sentation space, with queries acting as abstract semantic surrogates for heterogeneous multimodal content. In this paper, we propose a Multimodal Long-Context Document Retrieval Augmented Gener- ation (MLDocRAG) framework that leverages a Multimodal Chunk- Query Graph (MCQG) to organize multimodal document content around semantically rich, answerable queries. MCQG is constructed via a multimodal document expansion process that generates fine- grained queries from heterogeneous document chunks and links them to their corresponding content across modalities and pages. This graph-based structure enables selective, query-centric retrieval and structured evidence aggregation, thereby enhancing ground- ing and coherence in multimodal long-context question answering. Experiments on datasets MMLongBench-Doc and LongDocURL demonstrate that MLDocRAG consistently improves retrieval qual- ity and answer accuracy, demonstrating its effectiveness for multi- modal long-context understanding. ACM Reference Format: Yongyue Zhang and Yaxiong Wu. 2018. MLDocRAG: Multimodal Long- Context Document Retrieval Augmented Generation. InProceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym â€™XX).ACM, New York, NY, USA, 15 pages. https://doi. org/XXXXXXX.XXXXXXX 1 Introduction Multimodal long-context documents, such as research papers, re- ports, and books, often span tens to hundreds of pages and contain diverse multimodal components/chunks including text, images, and tables [7, 21, 28, 33, 38]. Understanding such lengthy multimodal documents presents two central challenges [ 7]: (1)cross-modal heterogeneity, which requires identifying and localizing relevant