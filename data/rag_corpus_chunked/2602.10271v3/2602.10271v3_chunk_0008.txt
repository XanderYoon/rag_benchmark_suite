and image-modality chunks correspond to figures or tables, each associated with its caption and any OCR-derived structured textual content. We then apply the MDoc2Query pro- cess to generate a set of answerable queries for each chunk using a Large Vision-Language Model (LVLM). These queries are explicitly linked to their source chunks and further connected to semanti- cally similar queries across the document, forming theMultimodal Chunk-Query Graph (MCQG). This graph captures both intra-modal and inter-modal associations, and provides a structured retrieval index that aligns semantically meaningful questions with relevant multimodal evidence. MCQG Usage.At inference time, a user query is embedded and matched against nodes in the MCQG using KNN-based retrieval in the query embedding space. By retrieving semantically similar generated queries and aggregating their linked source chunks, the system collects a compact yet relevant set of multimodal content. The retrieved chunks are further ranked based on their semantic similarity to the user query and provided as context to a Large Lan- guage or Vision-Language Model (LLM/LVLM) for final answer gen- eration. This query-centric retrieval strategy enables interpretable, structured evidence aggregation over multimodal long contexts. 3.3 MCQG Construction The MCQG Construction stage aims to transform a multimodal long-context document into a query-centric retrieval structure that supports fine-grained, semantically aligned, and cross-modal ev- idence retrieval. Specifically, we convert the long document into a collection of multimodal chunks, generate answerable queries from each chunk, and construct a graph that encodes chunk-query and inter-query associations. This process comprises four steps: (1) Document Parsing, (2) MDoc2Query, (3) Graph Assembly, and (4) Vector & Graph Storage. (1) Document Parsing.We adopt an existing multimodal PDF parsing tool, such as MinerU [40], to extract structured information from the document. Given a PDF document ğ·={ğ‘ƒ 1, ğ‘ƒ2, . . . , ğ‘ƒğ‘‹ } of ğ‘‹ pages, we extract a layout-preserving