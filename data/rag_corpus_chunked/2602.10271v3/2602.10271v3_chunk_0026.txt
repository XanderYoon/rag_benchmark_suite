with the pathogenesis outlined in Figure 1. Final Answer: Deletion / Duplication / Rearrangement of Genetic Material. Figure 7: Use Case on MMLongBench-Doc. gap, MLDocRAG successfully retrieves the dispersed evidence and performs the required multi-hop reasoning. Specifically, MLDocRAG accurately aligns the “chromosomal mutations” visually depicted in “Figure 10” on Page 14 with the corresponding “Deletion / Duplica- tion / Rearrangement” stage in “Figure 1” on Page 3, demonstrating its superior capability in fine-grained information extraction and cross-page visual alignment. More use cases are provided in Ap- pendix D. 6 Conclusion We presented MLDocRAG, a framework for multimodal long-document QA built on the Multimodal Chunk-Query Graph (MCQG), which enables unified, query-centric retrieval. By extending document expansion to the multimodal setting via MDoc2Query, MCQG orga- nizes heterogeneous chunks and their generated queries into a struc- tured graph that captures cross-modal and cross-page associations. This query-centric representation supports selective, multi-hop re- trieval and semantically grounded evidence aggregation, achieving consistent gains on MMLongBench-Doc and LongDocURL. Our results demonstrate the effectiveness of query-based multimodal retrieval and the scalability of graph-structured organization for multimodal long-context understanding. MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym ’XX, June 03–05, 2018, Woodstock, NY 7 Limitations While MLDocRAG achieves promising results, several limitations remain. First, it currently supports only text and image, limiting generalization to richer modalities such as video or audio. Second, its effectiveness depends on the quality of generated queries—noisy or incomplete queries may reduce retrieval accuracy. Finally, con- structing large multimodal graphs can be computationally expen- sive, posing challenges for scaling to massive document collections. References [1] Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Moham- madali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soley- mani Baghshah, and Ehsaneddin Asgari. 2025. Ask in any modality: A compre- hensive survey on multimodal retrieval-augmented generation.arXiv preprint arXiv:2502.08826(2025). [2] Martin Aumüller,