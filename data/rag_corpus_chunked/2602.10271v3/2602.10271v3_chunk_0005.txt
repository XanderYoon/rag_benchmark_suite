where key information is sparsely located, as highlighted in benchmarks such as MMLongBench- Doc [25] and LongDocURL [8]. Retrieval-Augmented Generation (RAG) offers partial relief by introducing external memory, yet struggles with retrieving semantically aligned multimodal content at scale. Retrieval-Augmented Generation (RAG) partly mitigates this by introducing external memory, but struggles to retrieve and integrate semantically aligned multimodal information at scale. These challenges call for new retrieval and representation strate- gies tailored to multimodal long-document understanding. Multimodal RAG.Recent efforts in multimodal Retrieval Aug- mented Generation (RAG) have explored various strategies to adapt long-document understanding to the multimodal setting. Common approaches include OCR-based text extraction, image captioning fused with text chunks, shared embedding retrieval via multimodal encoders (e.g., CLIP [29], SigLIP [37, 47]), and vision-based page retrieval using rendered document images (e.g., ColPali [14]). Some integrate structured knowledge via document-derived graphs to enhance reasoning [1, 26, 49, 50]. However, existing pipelines oper- ate at coarse granularity, overlooking fine-grained cross-modal and cross-page associations, leading to incomplete grounding and re- trieval mismatches. This motivates developing semantically aligned and structurally informed retrieval for multimodal contexts. Document Expansion.Document expansion has been widely adopted in RAG settings to improve retrieval coverage by gen- erating synthetic contents that anticipate potential information needs [12, 31, 34, 39]. Methods such as Doc2Query [ 27] and its enhanced variant Doc2Query-- [15] generate diverse, semantically meaningful queries from document content, effectively enriching the retrieval index. Building on this idea, QCG-RAG [ 44] intro- duces a query-centric graph structure that connects queries to their source textual chunks, enabling multi-hop retrieval and structured evidence aggregation in long-document scenarios. Yet, these meth- ods remain unimodal, lacking explicit modeling of cross-modal relationships. Extending query-centric expansion to multimodal documents remains an open problem for capturing fine-grained, heterogeneous associations. MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym