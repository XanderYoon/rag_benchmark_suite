In this section, we analyse the experimental results with respect to the four research questions stated in Section 4 to gauge the effectiveness of our proposed MLDocRAG. 5.1 MLDocRAG vs. Baselines (RQ1) Table 1 reports the performance of MLDocRAG and representative baselines on MMLongBench-Doc and LongDocURL in terms of accu- racy (%). Overall, MLDocRAG achieves the best overall performance on both datasets, with an accuracy of 47.9% on MMLongBench- Doc and 50.8% on LongDocURL, consistently outperforming all baseline methods. In particular, compared to text-only and image- to-text baselines, MLDocRAG benefits from explicitly modeling multimodal evidence without collapsing visual information into flat textual descriptions, thereby preserving fine-grained visual semantics that are critical for layout-, chart-, and figure-centric questions. In contrast to multimodal dense retrieval methods that independently retrieve text and image chunks, MLDocRAG orga- nizes multimodal content around generated, answerable queries and performs query-centric multi-hop expansion, enabling effective aggregation of semantically related evidence scattered across pages. This advantage is particularly evident in multi-page settings, where simple chunk-level retrieval or page-level visual reasoning fails to capture long-range dependencies. Moreover, in contrast to the text-only graph-based baseline, MLDocRAG explicitly models cross- modal associations through the Multimodal Chunkâ€“Query Graph, 1https://huggingface.co/openai/clip-vit-base-patch32 2https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct 3https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct 4https://huggingface.co/Qwen/Qwen2.5-72B-Instruct MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY MMLongBench-Doc LongDocURL Model Modality Evidence Source Evidence Page Overall Evidence Source Evidence Page Overall Method Retriever GEN TXT IMG TXT LAY CHA TAB FIG SIN MUL UNA ACC TXT LAY FIG TAB SP MP CE ACC LCğ‘¡ğ‘¥ğ‘¡ - LLM âœ“ âœ— 44.3 31.9 20.8 12.8 22.0 32.8 21.9 69.3 36.9 59.1 38.8 26.0 41.5 43.3 45.7 29.2 40.2 RAGğ‘¡ğ‘¥ğ‘¡ ğµğ‘€25 BM25 LLM âœ“ âœ— 44.9 26.1 20.2 14.2 17.1 31.6 18.9 76.3 36.8 60.2 37.4 24.9 37.3 43.1 45.9 26.8 39.5 RAGğ‘¡ğ‘¥ğ‘¡ ğµğºğ¸âˆ’ğ‘š3 BGE-m3 LLM âœ“