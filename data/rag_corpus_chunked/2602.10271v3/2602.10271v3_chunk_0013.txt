Qexp represents the expanded query set that augments the ini- tially retrieved queries with semantically related queries discovered through multi-hop graph traversal. (2) Chunk Node Ranking.Each query ğ‘âˆˆ Q exp is linked to a source multimodal chunk ğ‘âˆˆ C . We collect all chunks associated with the expanded query set: Ccand =  ğ‘ğ‘– | âˆƒğ‘âˆˆ Q exp s.t.(ğ‘, ğ‘ ğ‘– ) âˆˆ E .(8) To prioritize the most relevant evidence, we assign a relevance score to each candidate chunk ğ‘ğ‘– âˆˆ Ccand based on the maximum semantic similarity between the user queryğ‘ğ‘¢ and any query linked toğ‘ ğ‘–: score(ğ‘ğ‘– )=max ğ‘âˆˆ Qexp (ğ‘,ğ‘ğ‘– ) âˆˆ E sim ğ‘ğ‘¢, ğ‘.(9) Here, {ğ‘| (ğ‘, ğ‘ ğ‘– ) âˆˆ E} denotes the subset of expanded queries in Qexp that are connected to chunkğ‘ ğ‘– in the MCQG. (3) Context Collection.Based on the relevance scores, we select the top-ğ¾(e.g.,ğ¾=5) ranked multimodal chunks: Crel =Top ğ¾ {ğ‘ğ‘– âˆˆ Ccand} ,(10) where the ranking is determined by score(ğ‘ğ‘– ). The selected chunks are concatenated to form the multimodal retrieval context for the LVLM. These chunks may include text blocks, image regions with captions, and tables augmented with structured and OCR-derived textual content. (4) Answer Generation.Finally, the selected multimodal context Crel is provided to a Large Visionâ€“Language Model (LVLM), to- gether with the original user queryğ‘ğ‘¢, to generate the final answer: Ë†ğ‘=LVLM (ğ‘ğ‘¢,C rel) .(11) This generation step benefits from the query-centric retrieval strat- egy and graph-based evidence aggregation, which together improve grounding, coverage, and factual consistency when answering com- plex questions over multimodal long-context documents. 3.5 MDoc2Query Optimization The effectiveness of MLDocRAG largely depends on the quality of the Multimodal Chunkâ€“Query Graph (MCQG), which is con- structed via MDoc2Query. In particular, the quality and granularity of the generatedanswerable queriesproduced by MDoc2Query are critical, as they