further explore parametric optimization by fine-tuning a pretrained Large Visionâ€“Language Model (LVLM) on a curated set of high-quality multimodalchunk-to-queryexemplars. Each train- ing instance consists of a multimodal chunk ğ‘ (including text, an image with its caption, or a table with structured content) paired with a set of human-curated or automatically synthesized answer- able queryâ€“answer (ğ‘, ğ‘) pairs. The LVLM is trained using stan- dard teacher forcing, where the model conditions on the input chunk to generate the corresponding answer and subsequently au- toregressively decodes the associated queries. Through parametric adaptation, the model learns to produce more semantically pre- cise, context-aware, and structurally grounded queries, thereby improving the expressiveness and reliability of MDoc2Query for downstream MCQG construction. 4 Experimental Setup In this section, we evaluate the effectiveness of the proposed ML- DocRAG framework for multimodal long-context document ques- tion answering (QA), and compare it with existing approaches (as illustrated in Figure 1). Specifically, our experimental study is de- signed to address the following research questions: â€¢RQ1:How does MLDocRAG perform on multimodal long-context document QA compared with baseline methods? â€¢RQ2:What are the effects on MLDocRAG of different MCQG node variants, including query node choices (query vs. answer), chunk ranking strategies (max vs. mean), and visual noise filtering? â€¢RQ3:How do key hyperparameters of MCQG usage affect the per- formance of MLDocRAG, such as expansion hopsâ„, KNN neighbors ğ‘˜, and max nodesğ‘›? â€¢RQ4:What is the impact of MDoc2Query optimization from both non-parametric and parametric perspectives? 4.1 Datasets & Metrics Datasets.We evaluate our MLDocRAG on two multimodal long- context document QA benchmarks: (1)MMLongBench-Doc[ 25]: A curated benchmark for multimodal long-document understand- ing, consisting of documents in PDF format with diverse content including text, images, tables, and charts. Questions are paired with answerable evidence spread across pages and modalities. (2) Conference acronym â€™XX, June 03â€“05, 2018,