images, and tables. The resulting MCQG links each query to its corresponding multimodal content, enabling selective retrieval and structured evidence aggregation across modalities and pages. This design effectively addresses cross-modal and cross- page associations, improving retrieval accuracy and grounding in multimodal long-document question answering (QA). Figure 1(f) illustrates the overall pipeline of our proposed MLDocRAG frame- work. Our main contributions are as follows: •We proposeMLDocRAG (Multimodal Long- Context Docu- ment Retrieval-Augmented Generation), a unified framework for multimodal long-document QA that integrates multimodal doc- ument expansion with query-centric, graph-based retrieval for fine- grained and interpretable evidence selection. •We introduce theMCQG (Multimodal Chunk-Query Graph), which links generated queries to corresponding multimodal chunks and connects semantically related information across pages. •To construct MCQG, we leverageMDoc2Query, a multimodal document expansion process that generates answerable queries from multimodal chunks. •Extensive experiments on MMLongBench-Doc [ 25] and Long- DocURL [8] show that MLDocRAG consistently improves QA accu- racy, advancing multimodal long-context document understanding. 2 Related Work Long-Context Document Understanding.Understanding multi- modal long-context documents, such as research papers or techni- cal reports, requires resolving both cross-modal heterogeneity and long-range cross-page reasoning—capabilities still limited in cur- rent models [8, 25]. Despite the strong local alignment abilities of Large Vision-Language Models (LVLMs) like GPT-4o [20], Qwen2.5- VL [3], and Gemini [35], their fixed context windows limit their ef- fectiveness in capturing globally relevant evidence dispersed across pages and modalities. This limitation leads to degraded performance in complex reasoning tasks, where key information is sparsely located, as highlighted in benchmarks such as MMLongBench- Doc [25] and LongDocURL [8]. Retrieval-Augmented Generation (RAG) offers partial relief by introducing external memory, yet struggles with retrieving semantically aligned multimodal content at scale. Retrieval-Augmented Generation (RAG) partly mitigates this by introducing external memory, but struggles to retrieve and integrate semantically aligned multimodal information at scale.