âˆˆ C , we employ a Large Vision-Language Model (LVLM) to generate a set of answerable queryâ€“answer pairs: Qğ‘– = n (ğ‘ (1) ğ‘– , ğ‘(1) ğ‘– ), . . . ,(ğ‘ (ğ‘€ğ‘– ) ğ‘– , ğ‘(ğ‘€ğ‘– ) ğ‘– ) o , ğ‘€ ğ‘– â‰¤ğ‘€ max.(2) Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu Figure 2: Overview of our proposed MLDocRAG framework, consisting of (a)MCQG Constructionfor building a multimodal chuck-query graph and (b)MCQG Usagefor retrieving relevant multimodal chunks in long-document QA. Here, ğ‘ (ğ‘—) ğ‘– denotes a generated query and ğ‘ (ğ‘—) ğ‘– its corresponding answer, both grounded in chunk ğ‘ğ‘–. The number of pairs per chunk is adaptively determined by the richness of its content, up to a global cap ğ‘€max per chunk. Each queryâ€“answer pair (ğ‘ (ğ‘—) ğ‘– , ğ‘(ğ‘—) ğ‘– ) is embedded into a dense vector representation using a pretrained text encoderğœ™(Â·)(e.g., BGE-m3 [5]): v(ğ‘—) ğ‘– =ğœ™  [ğ‘ (ğ‘—) ğ‘– ;ğ‘ (ğ‘—) ğ‘– ]  âˆˆR ğ‘‘,(3) where [ğ‘; ğ‘] denotes concatenation of the query and answer as the retrieval unit. We simplify notation in the remainder of the paper by referring toğ‘ (ğ‘—) ğ‘– as a shorthand for the full queryâ€“answer pair. (3) Graph Assembly.We build the Multimodal Chunk-Query Graph (MCQG) as a heterogeneous graph G=(V,E) with nodes: V=C âˆª Q,whereQ= ğ‘Ã˜ ğ‘–=1 Qğ‘– .(4) Nodes in V includeChunk Nodes C andQuery Nodes Q. Edges in E include: (1)Chunkâ€“Query (C-Q) Edges: Each query ğ‘ (ğ‘—) ğ‘– is connected to its originating chunk ğ‘ğ‘– via a directed anchor edge. (2)Queryâ€“Query (Q-Q) Edges: We compute semantic similarity between queries using inner product of embeddings and connect each query to its top-ğ‘˜nearest neighbors (KNN): sim(ğ‘, ğ‘â€²)=âŸ¨ğœ™( [ğ‘;ğ‘]), ğœ™( [ğ‘ â€²;ğ‘ â€²])âŸ© +ğœ–.(5) Here, ğ‘ and ğ‘â€² denote two generated queries. âŸ¨Â·,Â·âŸ© denotes