graph-based evidence aggregation, which together improve grounding, coverage, and factual consistency when answering com- plex questions over multimodal long-context documents. 3.5 MDoc2Query Optimization The effectiveness of MLDocRAG largely depends on the quality of the Multimodal Chunkâ€“Query Graph (MCQG), which is con- structed via MDoc2Query. In particular, the quality and granularity of the generatedanswerable queriesproduced by MDoc2Query are critical, as they directly determine the semantic coverage, retriev- ability, and grounding fidelity of the overall pipeline. To this end, we explore optimization strategies for MDoc2Query from both non-parametricandparametricperspectives. Non-Parametric Optimization.By default, MDoc2Query in ML- DocRAG employs a LVLM to generate a set of answerable queries from parsed document chunks, such as cropped images or parsed ta- ble segments. However, document parsing tools (e.g., MinerU [40]) often strip away essential contextual informationâ€”including figure captions, table headers, and hierarchical section titlesâ€”resulting in isolated chunks that can be semantically ambiguous and con- sequently degrade the quality of the generated queries. To miti- gate this issue, we adopt aPage-Context-A ware Generationstrategy. Specifically, for a given chunk ğ‘ğ‘– located on page ğ‘¥ with the cor- responding page rendering image ğ‘ƒ page ğ‘¥ , we construct the LVLM input as a tuple (ğ‘ğ‘–, ğ‘ƒpage ğ‘¥ ). Incorporating page-level visual context enables the model to resolve ambiguities arising from incomplete local information, such as coreference resolution (e.g., linking a chunk labeled â€œTable 3â€ to its corresponding description on the same page). Parametric Optimization.In addition to non-parametric strate- gies, we further explore parametric optimization by fine-tuning a pretrained Large Visionâ€“Language Model (LVLM) on a curated set of high-quality multimodalchunk-to-queryexemplars. Each train- ing instance consists of a multimodal chunk ğ‘ (including text, an image with its caption, or a table with structured content) paired with a set of human-curated or automatically synthesized answer- able queryâ€“answer (ğ‘, ğ‘) pairs. The LVLM is