vision-based methods (e.g., ColPali [14]) prior to LVLM decoding; and (e)ğºğ‘Ÿğ‘ğ‘â„ğ‘…ğ´ğº ğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ ğ·ğ‘’ğ‘›ğ‘ ğ‘’ : constructing knowledge graphs (KGs) from document content and applying Graph-based RAG [11, 13]. However, these approaches often struggle to capture fine-grained cross-modal and cross-page associations, leading to incomplete grounding and suboptimal retrieval. Document expansion methods such as Doc2Query [27] provide a principled way to map document content into a unified query representation space, enhancing retrieval by generating synthetic queries that capture a documentâ€™s latent information needs. Build- ing upon this idea, QCG-RAG [44] constructs a query-centric graph that explicitly links generated queries to their corresponding textual document chunks, enabling query-aware indexing and multi-hop retrieval over long textual contexts, thereby improving evidence arXiv:2602.10271v3 [cs.IR] 13 Feb 2026 Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu Figure 1: Illustration of RAG for multimodal long-context documents, comparing (a)â€“(e) baselines with (f) our MLDocRAG. aggregation and grounding in long-document question answering. However, these approaches remain largely unexplored in the set- ting of multimodal long-context documents, where information is distributed across heterogeneous modalities and pages. Building on this idea, we extend query-centric formulation to the multimodal setting and propose MLDocRAG (Multimodal Long- Context Document Retrieval-Augmented Generation), a framework for multimodal long-context document understanding. MLDocRAG leverages a unified retrieval structureâ€”the Multimodal Chunk- Query Graph (MCQG)â€”constructed via MDoc2Query, which ex- tends Doc2Query [27] to multimodal scenarios and generates se- mantically rich, answerable queries from heterogeneous chunks spanning text, images, and tables. The resulting MCQG links each query to its corresponding multimodal content, enabling selective retrieval and structured evidence aggregation across modalities and pages. This design effectively addresses cross-modal and cross- page associations, improving retrieval accuracy and grounding in multimodal long-document question answering (QA). Figure 1(f) illustrates the overall pipeline of our proposed MLDocRAG frame- work. Our main contributions