. . . , ùëê‚Ä≤ ùêæ } from ùê∑, and a LVLM conditions on these chunks to generate the final answer ÀÜùëé. The primary challenge lies in retrieving semantically aligned and cross-modally grounded chunks from the long, heterogeneous document to support accurate and coherent generation. 3.2 Framework Overview To tackle the challenges of cross-modal heterogeneity and long- range reasoning in multimodal long-document QA, we propose a Multimodal Long-Context Document Retrieval Augmented Genera- tion (MLDocRAG)framework based on the construction and usage of aMultimodal Chunk-Query Graph (MCQG)that organizes mul- timodal document content around semantically rich, answerable queries. To construct MCQG, we move beyond conventional chunk- based retrieval paradigms that treat multimodal content as flat and independent units, and instead hypothesize that organizing mul- timodal long-context document understanding around generated, answerable queries enables more fine-grained, semantically aligned, and interpretable retrieval. Inspired by prior work on document expansion via query generation (e.g., Doc2Query [27]), we extend this idea to the multimodal setting by proposing MDoc2Query‚Äîa multimodal document expansion framework that generates seman- tically rich queries from heterogeneous document chunks. These generated queries act asretrieval anchorsthat bridge the gap be- tween user information needs and multimodal document content. Figure 2 illustrates the overall architecture of our proposed ML- DocRAG framework which consists of two main stages:MCQG ConstructionandMCQG Usage. MCQG Construction.In this stage, a multimodal long document parsed from a PDF is decomposed into modality-specific chunks, where text-modality chunks correspond to paragraph text (includ- ing equations), and image-modality chunks correspond to figures or tables, each associated with its caption and any OCR-derived structured textual content. We then apply the MDoc2Query pro- cess to generate a set of answerable queries for each chunk using a Large Vision-Language Model (LVLM). These queries are explicitly linked to their source chunks and further connected to semanti- cally similar queries across