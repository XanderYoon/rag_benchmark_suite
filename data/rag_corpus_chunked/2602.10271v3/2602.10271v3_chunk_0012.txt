relevant mul- timodal content in response to a user query by leveraging the struc- ture of the Multimodal Chunk-Query Graph (MCQG). Rather than retrieving from raw document chunks, our MLDocRAG approach retrieves and aggregates content via generated queries that serve as semantically aligned retrieval anchors. This process involves four main steps: (1) Query Node Retrieval, (2) Chunk Node Ranking, (3) Context Collection, and (4) Answer Generation. (1) Query Node Retrieval.Given a user query ğ‘ğ‘¢, we first compute its vector embedding ğœ™(ğ‘ ğ‘¢ ) using the same query encoder used dur- ing MCQG construction. We perform approximate nearest neighbor (ANN) search over the queryâ€“answer vectors in the vector database to retrieve the top-ğ‘›semantically similar generated queries: Qret =Top ğ‘› {ğ‘âˆˆ Q | sim(ğ‘ğ‘¢, ğ‘) â‰¥ğ›¼ } .(6) MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY In practice, the number of retrieved queries is jointly constrained by the maximum node budget ğ‘› and a similarity threshold ğ›¼âˆˆ [ 0, 2] (e.g., ğ›¼= 1.0), such that only queries with sim(ğ‘ğ‘¢, ğ‘) â‰¥ğ›¼ are retained. To capture broader contextual evidence, we expand each retrieved query node ğ‘âˆˆ Q ret in the MCQG via multi-hop neighbor expansion. For each queryğ‘âˆˆ Q ret, we traverseâ„ hops in the graph (through Queryâ€“Query Edges) and collect its neighboring queries: Qexp =Q ret âˆª Ã˜ ğ‘âˆˆ Qret Nbrâ„ (ğ‘),(7) where Nbrâ„ (ğ‘) denotes the set ofâ„-hop neighbors ofğ‘ in the MCQG, and Qexp represents the expanded query set that augments the ini- tially retrieved queries with semantically related queries discovered through multi-hop graph traversal. (2) Chunk Node Ranking.Each query ğ‘âˆˆ Q exp is linked to a source multimodal chunk ğ‘âˆˆ C . We collect all chunks associated with the expanded query set: Ccand =  ğ‘ğ‘– | âˆƒğ‘âˆˆ Q exp s.t.(ğ‘, ğ‘