(RAG) to develop best practices for bu ilding LLM-based systems. The following recommen- dations are grounded in lessons learned from our build-test cycles and are designed to address key areas such as data governance, system design, user engagement, and ongoing ev aluation. 6.2.1 Prompt Engineering Prompt Structure: RAG allows LLMs to stay focused and minimize hallucinationsby providing clear context through the prompt. We developed structured templates that enhance readability and ﬂexibility during development. These templates also segment instructions into distinct section s, which is crucial for maintaining clarity. Incorporating a preﬁx section in the prompt helps deﬁne the LLM’s persona and provides structural information, ensuring the model understands the organization of the prompt. Custom begin an d end tags (e.g., <|im-start|>, <|im-end|>) facilitate eas y parsing of the prompt’s sections. 24 Deploying Large Language Models with Retrieval Augmented G eneration Prompt Writing: Clarity and speciﬁcity in prompt instructions are vital for guiding the LLM effectively. For instance, instructions like "If there is not enough information, say ’ I don’t know’" or "Be brief in your answers" help the model deliver more accurate responses. Capitalizing key directi ves (e.g., "DO NOT," "ONLY") and numbering instructions can further emphasize critical points. When expecting outp ut in a speciﬁc format (e.g., JSON), providing an example is beneﬁcial. Additionally, using the function-calling fe ature of LLMs, such as in ChatGPT, can help ensure the output follows a particular pattern. 6.2.2 Data Governance V ector Databases: Experimenting with various vector databases and search mec hanisms is crucial for optimizing RAG systems. We tested databases like Chroma, Pinecone, and Elastic Relevance Search Engine and compared dif- ferent search strategies, including exact-match and seman tic search, or a combination of both. Proper data governance involves selecting and managing these databases to ensure d ata accuracy, consistency, and