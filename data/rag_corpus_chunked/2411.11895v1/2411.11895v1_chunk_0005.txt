B) documents. An LLM is typically trained on vast amounts of pub lic data but can be customized by additional training on proprietary documents (using full or parameter-efﬁcien t ﬁne-tuning). There could even be an overlap between proprietary and public documents ( A ∩ B) if private documents are released or public documents are r etracted. Of course, the documents generated by an LLM can be re-cycled as training data as well. Using an LLM involves prompt engineering with or without examples, which can be dr awn from seen or unseen documents. Several possible approaches to reduce hallucinations are outlined below. • Full ﬁne-tuning : The pre-trained LLM understands the basic language constr uct, but it is not adapted for domain-speciﬁc context and vocabulary. This approach basi cally modiﬁes the LLM’s weights to adopt the context from the domain-speciﬁc dataset. It is one of the old est approaches for domain-speciﬁc tuning of an LLM. • Parameter-efﬁcient ﬁne-tuning: In this approach, the LLM is ﬁne-tuned by modifying only a sm all number of parameters [2] [4] rather than all of them, as in the case of full ﬁne-tuning. This approach reduces the load with respect to computations, memory use and execution time . • Prompt engineering: Prompt engineering is an approach where the LLM is given ins tructions to generate a certain response. Different approaches have been shown to enhance the quality of responses generated using prompt engineering. Some of them include zero-shot pr ompting [6], where the model is given only instructions but no examples, few-shot prompting where the model is given instructions and a few examples 2 Deploying Large Language Models with Retrieval Augmented G eneration Figure 1: Large Language Model Landscape [7], chain-of-thought prompting [8] where the model is guid ed through steps for problem-solving, R EACT [9]