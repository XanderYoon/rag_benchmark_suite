After the pilot run, a post-survey (Appendix C) was conducted to gather feed back on the application’s performance, user satisfaction, and areas for improvement. Detailed results are tabulated i n Table 6 5.1 T est Strategies Testing LLM-based systems necessitates a distinct approac h from traditional software testing methodologies. Estab- lishing a ground truth dataset is crucial for assessing the s emantic accuracy of the generated responses. The testing process must be thorough, addressing both the generative an d retrieval components of the system. When evaluating applications developed for Retrieval-Aug mented Generation (RAG) with Large Language Models (LLMs) on proprietary documents, organizations should imp lement a robust testing strategy to ensure the accuracy, reliability, and adherence to organizational standards. K ey strategies to consider include: • Ground Truth Comparison Develop a robust ground truth dataset consisting of correct and validated responses for a set of queries relevant to the proprietary documents. This dataset serves as a bench mark to evaluate the performance of the RAG application. Regularly compare the outputs of the RAG syste m against the ground truth dataset to assess the accuracy of the generated responses. Focus on key metrics su ch as precision, recall, and relevance. • Scenario-Based T esting Create test scenarios that reﬂect real-world use cases, ens uring that the RAG application can correctly inter- pret and respond to complex queries within the appropriate c ontext. • Fact-Checking Implement automated fact-checking mechanisms to verify th at the RAG system’s generated content accu- rately reﬂects the source documents. One method could inclu de embedding links to the original documents to aid in veriﬁcation. • Consistency To assess the system’s consistency in responding to similar or related queries and to prevent contradictory outputs, an external LLM—often referred to as ’LLM-as-a-ju dge’ [41] —can be utilized. We applied