of human-artiﬁcial intelligence collaboration, to promote the development of robust, reliable, and compliant AI solutions across variou s industries. The paper concludes with a summary of our ﬁnding s. 2 Related Work Prior to LLMs, the approaches to retrieval of information fr om unstructured text included statistical and machine learning-based approaches. Examples of statistical appro aches include bag-of-words and term frequency-inverse doc - ument frequency (TF-IDF) [15]. For instance, the TF-IDF app roach has been used for extracting ontological data from unstructured text [16]. On the other hand, machine learning approaches are based on generating word embeddings such as W ORD 2VEC [17], which captures the idea of similarity between words wi thin documents. Recently, new technologies have emerged, such as large lang uage models that generate new text and embeddings from existing documents. Although very promising in their c apabilities, these models can suffer from hallucinations, generating responses that are not grounded in facts. LLMs co me up with their responses mainly based on next-word probability [18]. These hallucinatory tendencies impede t he application of LLMs to mainstream industry applications because of obvious questions about the reliability and robu stness of their responses [19] [20]. To address this, a lot of research has been published that suggests a variety of appro aches which are summarized in table 1 Figure 1 provides an overview of the LLM landscape highlight ing the use of both proprietary ( A) and public ( B) documents. An LLM is typically trained on vast amounts of pub lic data but can be customized by additional training on proprietary documents (using full or parameter-efﬁcien t ﬁne-tuning). There could even be an overlap between proprietary and public documents ( A ∩ B) if private documents are released or public documents are r etracted. Of course, the documents