option has distinct a dvantages, depending on the business case, and should be evaluated in terms of scalability, cost, and data security. 6.2.4 User Engagement Human Factors: Engaging end users early in the development process is key to creating a system that meets their needs. Request users to provide a set of frequently used ques tions for quality testing and unit testing. This approach helps mitigate developers’ bias and avoids tunnel vision, e nsuring the system is tested against real-world scenarios that users will encounter. Continuous feedback from users s hould be integrated into the development cycle to reﬁne the system’s performance. 6.2.5 Ongoing Evaluation Continuous Monitoring and Feedback: Implementing a robust system for ongoing evaluation is crucial for maintain- ing the effectiveness and relevance of the RAG system. This i ncludes regular monitoring of the system’s performance, user satisfaction, and compliance with data governance pol icies. Feedback loops should be established to capture insights from users and operational data, allowing for iter ative improvements. Regular updates to the system based on these evaluations will help ensure that it continues to meet evolving user needs and technological advancements. By following these best practices that we contribute based o n the insights from our ﬁeld study, organizations can successfully implement RAG-based solutions using LLMs, en suring that the technology is applied in a way that is effective, responsible, and aligned with industry standar ds and user expectations. 7 Discussion and Implications This research and ﬁeld study uncovered several technologic al, enterprise, and human factors that must be addressed as Retrieval-Augmented Generation (RAG) systems continue to evolve. From a technological perspective, one major challenge encountered was the limitations associated with using external AI models like C HATGPT via APIs, such as rate limits, delayed responses, and token constraints. T hese limitations directly