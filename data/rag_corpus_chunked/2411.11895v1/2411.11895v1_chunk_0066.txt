2024. [35] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, E Buchatskaya, T Cai, E Rutherford, DdL Casas, LA Hendricks, J Welbl, A Clark, et al. Training compute-opti mal large language models. arxiv 2022. arXiv preprint arXiv:2203.15556, 10, 2022. [36] Fernando van der Vlist, Anne Helmond, and Fabian Ferrar i. Big ai: Cloud infrastructure dependence and the industrialisation of artiﬁcial intelligence. Big Data & Society , 11(1):20539517241232630, 2024. [37] OpenAI. Introducing apis for gpt-3.5 turbo and whisper , April 9, 2024. [Accessed 06-08-2024]. [38] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Car los Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern cata log to enhance prompt engineering with ChatGPT. arXiv preprint arXiv:2302.11382 , 2023. [39] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawt e, Aman Chadha, and Amitava Das. A comprehen- sive survey of hallucination mitigation techniques in larg e language models. arXiv preprint arXiv:2401.01313 , 2024. [40] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Ten ghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efﬁcient ﬁne-tuning is better and chea per than in-context learning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022. [41] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhua ng, Zhanghao Wu, Y onghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge wit h mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. 28 Deploying Large Language Models with Retrieval Augmented G eneration [42] Anupam Datta, Matt Fredrikson, Klas Leino, Kaiji Lu, Sh ayak Sen, Ricardo Shih, and Zifan Wang. Exploring conceptual soundness with TruLens. In Douwe Kiela, Marco Ci ccone, and Barbara Caputo, editors, Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track , volume 176 of Proceedings of Machine Learning Research, pages 302–307. PMLR, 06–14 Dec 2022. [43] Jeffrey