[2] Edward J Hu, Y elong Shen, Phillip Wallis, Zeyuan Allen-Z hu, Y uanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021. [3] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Y ang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to ﬁne-tuning universal ly across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. [4] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. [5] Xiao Liu, Y anan Zheng, Zhengxiao Du, Ming Ding, Y ujie Qia n, Zhilin Y ang, and Jie Tang. GPT understands, too. AI Open, 2023. [6] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Y utaka Matsuo, and Y usuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems , 35:22199–22213, 2022. [7] Archit Parnami and Minwoo Lee. Learning from few example s: A summary of approaches to few-shot learning. arXiv preprint arXiv:2203.04291 , 2022. [8] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large lan guage models. Advances in Neural Information Pro- cessing Systems, 35:24824–24837, 2022. [9] Shunyu Y ao, Jeffrey Zhao, Dian Y u, Nan Du, Izhak Shafran, Karthik Narasimhan, and Y uan Cao. React: Syner- gizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 , 2022. [10] Harrison Chase. LangChain, October 2022. [11] Jerry Liu. LlamaIndex, November 2022. [12] Microsoft. semantic-kernel, 2023. [13] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaoku n Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applica tions via multi-agent conversation framework. arXiv preprint arXiv:2308.08155 , 2023. [14] Alan Hevner and V