at the RAG system’s generated content accu- rately reﬂects the source documents. One method could inclu de embedding links to the original documents to aid in veriﬁcation. • Consistency To assess the system’s consistency in responding to similar or related queries and to prevent contradictory outputs, an external LLM—often referred to as ’LLM-as-a-ju dge’ [41] —can be utilized. We applied this approach by using the external LLM to evaluate the cosine sim ilarity between responses generated for the same query to ensure consistency. Based on these evaluation s, we were able to adjust parameters such as temperature, top-p, frequency penalty, and presence penal ty. • Performance and Scalability T esting Load T esting: Assess the application’s performance under varying loads t o ensure it can handle high query volumes without degradation in response quality or speed. Scalability T esting:Evaluate the system’s capacity to handle increased data vol ume and user demand, ensur- ing consistent performance as the application expands. Whi le using API-based LLM calls, we encountered rate limits during peak hours, which could degrade applicat ion performance. Frequent rate limit issues may necessitate considering alternatives such as hosted LLMs o r setting token limits for API-based calls. Addi- tionally, scalability testing will inﬂuence the choice of c ontext summarization strategies and determine the hosting infrastructure requirements, including the need f or auto-scaling. • Security and Data Privacy T esting Evaluate the system to conﬁrm that proprietary data is not un intentionally disclosed in generated responses. V erify that the RAG application adheres to data privacy regu lations and organizational policies, especially when dealing with sensitive proprietary information. To ac hieve this, we involved our legal and compliance team from the onset of the project. • User Experience and Feedback Integration User Acceptance T esting (UA T):Engage end-users in the