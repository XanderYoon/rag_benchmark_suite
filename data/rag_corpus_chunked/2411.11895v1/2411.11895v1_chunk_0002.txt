tools have emerged to support the fast develop ment of LLM systems based on information retrieval from proprietary data. [10] [11] [12] These tools enable the LLMs to behave as agents, [13] function as reasoning engines, and invoke various tools supplied to the LLM to generate the required outcome. In addition, the LLM itself can be used to evaluate the efﬁcacy of these generated outcomes. What re mains for those implementing the Retrieval Augmented Generation (RAG) system is to make the right choices of these tools and to design a system that is most effective Deploying Large Language Models with Retrieval Augmented G eneration for their particular use cases. As simple as this might seem, the complexity arises from needing to implement such a system at scale while conforming to the industry’s regulato ry standards. As with any emerging technology, there is a lag between advan cements in large language models (LLMs) with Retrieval-Augmented Generation (RAG), and their adoption and implementation in the industry. We believe that valu- able insights can be gained from the practical application o f RAG with LLMs, which can help advance research and facilitate the next breakthroughs. This paper seeks to shar e these learnings and bridge the gap between the challenges encountered in the industry and the issues being tackled by r esearchers. We draw parallels between the challenges mentioned in a recent paper [14], which discusses key aspect s of human interactions with AI systems, and those we encountered in our ﬁeld study. We suggest best practice reco mmendations for organizations venturing into the space of generative AI with RAG. We hope that this paper, detailing our challenges and insights, will not only serve as a valuable guide for organizations planning to implement RAG-based solutions using LLMs but also provide researchers with a