question gets sent to the backend Python Flask API in a serialized JSON object along with the co ntext history. Note that here, C HATGPT is expected to understand the meaning of the “latest” release. It would have to scan through all the documents to identify which release is the latest in this case. 8. When the question arrives at the backend, we ﬁrst encode it using the same OpenAI embeddings used for encoding the document. This is necessary for retrieving sim ilar documents from the vector store. 9. We run a semantic search on the vector store, C HROMA , and return the top 3 matching documents. Below is the sample code, which shows one approach of implementing th is. 1 query = " What is in the March release ? " 2 3 retriever = vectordb . as_retriever ( search_type = " similarity " , 4 search_kwargs ={ " k ":3}) 5 r = retriever . g e t _ r e l e v a n t _ d o c u m e n t s( query ) [:3] Listing 3: Query the Chroma vector database for top 3 documen ts based on the user question. 15 Deploying Large Language Models with Retrieval Augmented G eneration 10. We then embed these documents as “sources” in the prompt a long with the question asked by the user and the context history and send it to the C HATGPT API. We are using the GPT-4-Turbo model [37]. 11. When we get a response from C HATGPT, we send it back to the user interface (UI) for formatting and presentation. Below is a sample of the response with proprie tary information redacted. [Document (page_content=’Summer Release 2022 Release Notes \n March 30, 2022 Release (Summer Release)\n The March 30, 2022