ching results for retrieval 7: Instantiate retriever 8: Save vector database in a persistent directory 9: while Chat session not exited do 10: Get the query from the application’s UI 11: Generate Embeddings for the query 12: Search top 3 results from the vector database that have embed dings "similar" to the query embeddings 13: Update the prompt template with the query and retrieved docu ment and the chat history 14: Send the prompt to the LLM ChatGPT API 15: Receive the response from the ChatGPT API 16: Save information on query, retrieved document titles and re sponse in a log ﬁle 17: Reformat the response for presentation to the user via UI 18: end while 1. The PDF documents are read from a speciﬁc directory locati on. 2. They are split into chunks, with each chunk containing 100 0 characters and overlap of 50 between chunks. 3. Next, these chunks are indexed. Below is the sample code, w hich shows one approach to implementing this using L ANG CHAIN libraries. [10] 1 from langchain . indexes import V e c t o r s t o r e I n d e x C r e a t o r 13 Deploying Large Language Models with Retrieval Augmented G eneration Figure 3: Workﬂow for Document Pre-processing Figure 4: System Operation Workﬂow 14 Deploying Large Language Models with Retrieval Augmented G eneration 2 from langchain . embeddings . openai import OpenAIEmbedd i ng s 3 from langchain . text_splitter import CharacterT ex t Sp l i tt e r 4 from langchain . vectorstores import Chroma 5 6 text_splitter = Character Te x t Sp l it t er ( chunk_size =1000 , 7 chunk_overlap =50) 8 docs = text_splitter . split_documen t s ( documents ) 9 embeddings =