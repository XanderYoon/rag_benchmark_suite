4. DISCUSSION 4.1. Limitations and constraints An apparent limitation of the proposed architecture concern the computational resources required to process the data for in an iterative way: each iteration requires indeed theconversational chainto be rebuilt on the basis of the adapteduser prompt. As a result, this solution is neither optimized in terms of computational resources, nor in terms of financial cost as soon as the utilization of the generative model may be subject to a charge. This iteration can however not be avoided due to the limited numberoftokens(relatedtothe state of the artgenerative models) which restrict the size of the answer. In addition, it would be difficult to make the model understand on a one-shot request (i.e. without iteration architecture) the necessity to retrieve the totality of the similarity scores at once. Another limitation concerns the sensitivity of the model to the expected output for the similarity score. Even with the temperature parameter being set to 0 and despite the fact that the Retrieval-Augmented Generation is a promising approach for mitigating the large language models hallucination (Li et al., 2022), thereâ€™s no strict guarantee to avoid anynoise or counterfactual robustness that would conduct the generative model to ignore the requested output format, or to give a score based on criteria that would not have been defined in the prompt. The last limitation concern the non-reproducibility of the results. Even if it has never occurred during the tests for this research (particularly due to the use of theseed command in the build of the conversational chain), the non-deterministic nature of LLMs models implies that the same input prompt might produce different responses over different runs. The evaluation of the method would therefore be easier with a methodology enabling an absolute guarantee to get the results being fixed and no longer influenced by