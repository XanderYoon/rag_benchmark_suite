due to the use of theseed command in the build of the conversational chain), the non-deterministic nature of LLMs models implies that the same input prompt might produce different responses over different runs. The evaluation of the method would therefore be easier with a methodology enabling an absolute guarantee to get the results being fixed and no longer influenced by modelâ€™s internal state or specific conditions under which it operates. 4.2. Areas for improvement Many improvements for the presented similarity search method could be proposed, starting with the optimization of the used prompt to retrieve the similarity. Chen et al., 2023 research demonstrated how the use of prompt engineering techniques such asrole-prompting, one-shot, and few-shot prompting, and even more sophisticated practices such aschain-of-thought, can improve the overall performance of LLMs. J. Bertin, 2024 6 Adv ancing Similarity Search with GenAI Another area for improvement concerns the type of generative model used. As mentioned in paragraph 2.3, a vast catalog of generative models more or less adapted to the similarity search case already exist. Through simulations and human experiments, Ichien et al., 2022 showed for instance that theBART-Gen model produced more human-like responses for generative inference than BERT, a popular model for natural language processing. These results demonstrate just how essential explicit representations are in human generative reasoning. A last area for improvement concerns the extension of the proposed similarity search method to other study variants : as proposed by Noh et al., 2010, the advanced semantic understanding of generative models could by used to address the nearest search case. The proposed method in this article could therefore draw inspiration to address the case of nearest neighbor search using a generative model instead of traditional methods such as K-Nearest Neighbor algorithm. In the context of the studied case, it would be