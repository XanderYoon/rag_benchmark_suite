where the input is the concatenation of the question and top-retrieved passages and the target output is the desired answer. Such generative read- ers are adopted in recent methods such as SpanSe- 5We ﬁnd that the number of spans used for normalization in each passage does not have signiﬁcant impact on the ﬁnal performance (we takeN = 5) and using the raw or normalized strings for aggregation also perform similarly. qGen (Min et al., 2020) and Longformer (Belt- agy et al., 2020). Speciﬁcally, we use BART-large (Lewis et al., 2019) as the generative reader, which concatenates the question and top-retrieved pas- sages up to its length limit (1,024 tokens, 7.8 pas- sages on average). Generative GAR is directly com- parable with SpanSeqGen (Min et al., 2020) that uses the retrieval results of DPR but not comparable with Fusion-in-Decoder (FID) (Izacard and Grave, 2020) since it encodes 100 passages rather than 1,024 tokens and involves more model parameters. 5 Experiment Setup 5.1 Datasets We conduct experiments on the open-domain ver- sion of two popular QA benchmarks: Natural Ques- tions (NQ) (Kwiatkowski et al., 2019) and Trivi- aQA (Trivia) (Joshi et al., 2017). The statistics of the datasets are listed in Table 1. Dataset Train / Val / Test Q-len A-len #-A NQ 79,168 / 8,757 / 3,610 12.5 5.2 1.2 Trivia 78,785 / 8,837 / 11,313 20.2 5.5 13.7 Table 1: Dataset statistics that show the number of sam- ples per data split, the average question (answer) length, and the number of answers for each question. 5.2 Evaluation Metrics Following prior studies (Karpukhin et al., 2020), we use top-k retrieval accuracy to evaluate the per- formance of the retriever and the Exact Match (EM) score to measure the performance of the reader. Top-k retrieval accuracy is deﬁned as the pro- portion