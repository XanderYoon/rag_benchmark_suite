2020b) show that there are signiﬁcant ques- tion and answer overlaps between the training and test sets of popular OpenQA datasets. Speciﬁcally, 60% to 70% test-time answers also appear in the 7This claim is later veriﬁed by the best systems in the NeurIPS 2020 EfﬁcientQA competition (Min et al., 2021). 8We ﬁnd that taking 500 passages during reader inference instead of 100 as in Karpukhin et al. (2020) improves the performance of BM25 but not DPR. training set and roughly 30% test-set questions have a near-duplicate paraphrase in the training set. Such observations suggest that many questions might have been answered by simple question or answer memorization. To further examine model generalizability, we study the per-category perfor- mance of different methods using the annotations in Lewis et al. (2020b). Method Total Question Overlap Answer Overlap Only No Overlap DPR 41.3 69.4 34.6 19.3 GAR+ (E) 43.8 66.7 38.1 23.9 BART 26.5 67.6 10.2 0.8 RAG 44.5 70.7 34.9 24.8 GAR+ (G) 45.3 67.9 38.1 27.0 Table 7: EM scores with question-answer overlap category breakdown on NQ. (E) and (G) denote ex- tractive and generative readers, respectively. Results of baseline methods are taken from Lewis et al. (2020b). The observations on Trivia are similar and omitted. As listed in Table 7, for theNo Overlap category, GAR + (E) outperforms DPR on the extractive setup and GAR + (G) outperforms RAG on the generative setup, which indicates that better end-to-end model generalizability can be achieved by adding GAR for retrieval. GAR + also achieves the best EM un- der the Answer Overlap Only category. In addition, we observe that a closed-book BART model that only takes the question as input performs much worse than additionally taking top-retrieved pas- sages, i.e., GAR + (G), especially on the questions that require generalizability. Notably, all