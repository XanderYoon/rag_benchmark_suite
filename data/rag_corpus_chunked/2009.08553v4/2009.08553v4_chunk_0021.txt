2020) 42.2 - - RAG (Lewis et al., 2020a) 44.5 56.1 68.0 FID (Izacard and Grave, 2020)51.4 67.6 80.1 BM25 (ours) 35.3 58.6 - GAR 38.1 62.2 - GAR+ 45.3 - - Table 6: End-to-end comparison with the state-of- the-art methods in EM . For Trivia, the left column denotes the open-domain test set and the right is the hidden Wikipedia test set on the public leaderboard. methods or baselines that use DPR as input such as SpanSeqGen (Min et al., 2020) and RAG (Lewis et al., 2020a). Also, GAR outperforms BM25 sig- niﬁcantly under both extractive and generative se- tups, which again shows the effectiveness of the generated query contexts, even if they are heuristi- cally discovered without any external supervision. The best performing generative method FID (Izacard and Grave, 2020) is not directly compara- ble as it takes more (100) passages as input. As an indirect comparison, GAR performs better than FID when FID encodes 10 passages (cf. Fig. 2 in Izac- ard and Grave (2020)). Moreover, since FID relies on the retrieval results of DPR as well, we believe that it is a low-hanging fruit to replace its input with GAR or GAR + and further boost the perfor- mance.7 We also observe that, perhaps surprisingly, extractive BM25 performs reasonably well, espe- cially on the Trivia dataset, outperforming many recent state-of-the-art methods.8 Generative BM25 also performs competitively in our experiments. Model Generalizability. Recent studies (Lewis et al., 2020b) show that there are signiﬁcant ques- tion and answer overlaps between the training and test sets of popular OpenQA datasets. Speciﬁcally, 60% to 70% test-time answers also appear in the 7This claim is later veriﬁed by the best systems in the NeurIPS 2020 EfﬁcientQA competition (Min et al., 2021). 8We ﬁnd that taking 500 passages during reader inference instead