86.9 83.8 88.6 What 15.0% 76.5 82.6 81.5 86.0 Where 10.9% 77.4 89.1 87.0 90.8 Other 9.1% 79.3 78.1 81.8 84.2 How 5.0% 78.2 83.8 83.2 85.5 Which 3.3% 89.0 90.7 94.1 94.9 Why 0.3% 90.0 90.0 90.0 90.0 Table 5: Top-100 retrieval accuracy breakdown of question type on NQ . Best and second best methods in each category are bold and underlined, respectively. among extractive methods on both NQ and Trivia datasets, despite that it is more lightweight and computationally efﬁcient. GenerativeGAR outper- forms most of the generative methods on Trivia but does not perform as well on NQ, which is some- what expected and consistent with the performance at the retrieval stage, as the generative reader only takes a few passages as input and GAR does not outperform dense retrieval methods on NQ when k is very small. However, combining GAR with DPR achieves signiﬁcantly better performance than both Method NQ Trivia Extractive Hard EM (Min et al., 2019a) 28.1 50.9 - Path Retriever (Asai et al., 2019) 32.6 - - ORQA (Lee et al., 2019) 33.3 45.0 - Graph Retriever (Min et al., 2019b) 34.5 56.0 - REALM (Guu et al., 2020) 40.4 - - DPR (Karpukhin et al., 2020) 41.5 57.9 - BM25 (ours) 37.7 60.1 - GAR 41.8 62.7 74.8 GAR+ 43.8 - - Generative GPT-3 (Brown et al., 2020) 29.9 - 71.2 T5 (Roberts et al., 2020) 36.6 60.5 - SpanSeqGen (Min et al., 2020) 42.2 - - RAG (Lewis et al., 2020a) 44.5 56.1 68.0 FID (Izacard and Grave, 2020)51.4 67.6 80.1 BM25 (ours) 35.3 58.6 - GAR 38.1 62.2 - GAR+ 45.3 - - Table 6: End-to-end comparison with the state-of- the-art methods in EM . For Trivia, the left column denotes the open-domain test set and the right is the hidden