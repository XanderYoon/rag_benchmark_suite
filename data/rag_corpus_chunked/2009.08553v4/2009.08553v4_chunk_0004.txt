and DPR (denoted as GAR +), we obtain consistently better performance than either method used individ- ually. (4) GAR outperforms DPR in the end-to-end QA performance (EM) when the same extractive reader is used: EM=41.8 (43.8 for GAR +) on NQ 2Strictly speaking, GAR with sparse representations han- dles semantics before retrieval by enriching the queries, while maintaining the advantage of exact matching. and 62.7 on Trivia, creating new state-of-the-art re- sults for extractive OpenQA.GAR also outperforms other retrieval methods under the generative setup when the same generative reader is used: EM=38.1 (45.3 for GAR +) on NQ and 62.2 on Trivia. Contributions. (1) We propose Generation- Augmented Retrieval ( GAR), which augments queries with heuristically discovered relevant con- texts through text generation without external su- pervision or time-consuming downstream feedback. (2) We show that using generation-augmented queries achieves signiÔ¨Åcantly better retrieval and QA results than using the original queries or ex- isting unsupervised QE methods. (3) We show that GAR, combined with a simple BM25 model, achieves new state-of-the-art performance on two benchmark datasets in extractive OpenQA and com- petitive results in the generative setting. 2 Related Work Conventional Query Expansion . GAR shares some merits with query expansion (QE) meth- ods based on pseudo relevance feedback (Rocchio, 1971; Abdul-Jaleel et al., 2004; Lv and Zhai, 2010) in that they both expand the queries with relevant contexts (terms) without the use of external super- vision. GAR is superior as it expands the queries with knowledge stored in the PLMs rather than the retrieved passages and its expanded terms are learned through text generation. Recent Query Reformulation. There are recent or concurrent studies (Nogueira and Cho, 2017; Zaiem and Sadat, 2019; Yu et al., 2020; Vaku- lenko et al., 2020; Lin et al., 2020) that reformu- late queries with generation