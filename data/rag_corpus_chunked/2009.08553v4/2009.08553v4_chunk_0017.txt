improve retrieval results.6 Case Studies . In Table 2, we show several ex- amples of the generated query contexts and their ground-truth references. In the ﬁrst example, the correct album release date appears in both the gen- erated answer and the generated sentence, and the generated title is the same as the Wikipedia page 6We use F1 instead of recall to avoid the unfair favor of (longer) generation-augmented query. Context ROUGE-1 ROUGE-2 ROUGE-L Answer 33.51 20.54 33.30 Sentence 37.14 24.71 33.91 Title 43.20 32.11 39.67 Table 3: ROUGE F1 scores of the generated query contexts on the validation set of the NQ dataset. title of the album. In the last two examples, the generated answers are wrong but fortunately, the generated sentences contain the correct answer and (or) other relevant information and the generated titles are highly related to the question as well, which shows that different query contexts are com- plementary to each other and the noise during query context generation is thus reduced. 6.2 Generation-Augmented Retrieval Comparison w. the state-of-the-art . We next evaluate the effectiveness of GAR for retrieval. In Table 4, we show the top-k retrieval accuracy of BM25, BM25 with query expansion (+RM3) (Abdul-Jaleel et al., 2004), DPR (Karpukhin et al., 2020), GAR, and GAR + (GAR +DPR). On the NQ dataset, while BM25 clearly under- performs DPR regardless of the number of retrieved passages, the gap between GAR and DPR is signiﬁ- cantly smaller and negligible whenk≥ 100. When k≥ 500, GAR is slightly better than DPR despite Method NQ Trivia Top-5 Top-20 Top-100 Top-500 Top-1000Top-5 Top-20 Top-100 Top-500 Top-1000 BM25 (ours) 43.6 62.9 78.1 85.5 87.8 67.7 77.3 83.9 87.9 88.9 BM25 +RM3 44.6 64.2 79.6 86.8 88.9 67.0 77.1 83.8 87.7 88.9 DPR 68.3 80.1 86.1 90.3 91.2 72.7 80.2 84.8 -