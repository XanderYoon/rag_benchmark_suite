in- formation due to hallucination in text generation (Mao et al., 2020) and introduce noise during re- trieval, they are beneﬁcial rather than harmful over- all, as our experiments show that GAR improve both retrieval and QA performance over BM25 sig- niﬁcantly. Also, since we generate 3 different (com- plementary) query contexts and fuse their retrieval results, the distraction of hallucinated content is further alleviated. 3.3 Retrieval with Generation-Augmented Queries After generating the contexts of a query, we append them to the query to form a generation-augmented query.3 We observe that conducting retrieval with the generated contexts ( e.g., answers) alone as queries instead of concatenation is ineffective be- cause (1) some of the generated answers are rather irrelevant, and (2) a query consisting of the correct answer alone (without the question) may retrieve false positive passages with unrelated contexts that happen to contain the answer. Such low-quality passages may lead to potential issues in the follow- ing passage reading stage. If there are multiple query contexts, we conduct retrieval using queries with different generated con- texts separately and then fuse their results. The per- formance of one-time retrieval with all the contexts appended is slightly but not signiﬁcantly worse. For simplicity, we fuse the retrieval results in a straightforward way: an equal number of passages are taken from the top-retrieved passages of each source. One may also use weighted or more so- phisticated fusion strategies such as reciprocal rank fusion (Cormack et al., 2009), the results of which are slightly better according to our experiments.4 Next, one can use any off-the-shelf retriever for passage retrieval. Here, we use a simple BM25 model to demonstrate that GAR with sparse repre- sentations can already achieve comparable or better performance than state-of-the-art dense methods while being more lightweight and efﬁcient (includ- ing