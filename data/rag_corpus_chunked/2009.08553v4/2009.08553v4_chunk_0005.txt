it expands the queries with knowledge stored in the PLMs rather than the retrieved passages and its expanded terms are learned through text generation. Recent Query Reformulation. There are recent or concurrent studies (Nogueira and Cho, 2017; Zaiem and Sadat, 2019; Yu et al., 2020; Vaku- lenko et al., 2020; Lin et al., 2020) that reformu- late queries with generation models for other re- trieval tasks. However, these studies are not eas- ily applicable or efﬁcient enough for OpenQA be- cause: (1) They require external resources such as paraphrase data (Zaiem and Sadat, 2019), search sessions (Yu et al., 2020), or conversational con- texts (Lin et al., 2020; Vakulenko et al., 2020) to form the reformulated queries, which are not available or showed inferior domain-transfer per- formance in OpenQA (Zaiem and Sadat, 2019); (2) They involve time-consuming training process such as RL. For example, Nogueira and Cho (2017) reported a training time of 8 to 10 days as it uses retrieval performance in the reward function and conducts retrieval at each iteration. In contrast, GAR uses freely accessible in-domain contexts like passage titles as the generation targets and standard seq2seq learning, which, despite its simplicity, is not only more efﬁcient but effective for OpenQA. Retrieval for OpenQA. Existing sparse retrieval methods for OpenQA (Chen et al., 2017) solely rely on the information of the questions. GAR extends to contexts relevant to the questions by extracting information inside PLMs and helps sparse meth- ods achieve comparable or better performance than dense methods (Guu et al., 2020; Karpukhin et al., 2020), while enjoying the simplicity and efﬁciency of sparse representations. GAR can also be used with dense representations to seek for even better performance, which we leave as future work. Generative QA. Generative QA generates answers through seq2seq learning instead of extracting