et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open- domain questions. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1870– 1879, Vancouver, Canada. Association for Computa- tional Linguistics. Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outper- forms condorcet and individual rank learning meth- ods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 758–759. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M V oorhees. 2020. Overview of the trec 2019 deep learning track. arXiv preprint arXiv:2003.07820. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al. 2020. The second conversational in- telligence challenge (convai2). In The NeurIPS’18 Competition, pages 187–208. Springer. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Ming-Wei Chang. 2020. Realm: Retrieval- augmented language model pre-training. arXiv preprint arXiv:2002.08909. Gautier Izacard and Edouard Grave. 2020. Lever- aging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282. Jeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale dis- tantly supervised challenge dataset for reading com- prehension. In Proceedings