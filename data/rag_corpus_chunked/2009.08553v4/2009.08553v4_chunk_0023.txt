can be achieved by adding GAR for retrieval. GAR + also achieves the best EM un- der the Answer Overlap Only category. In addition, we observe that a closed-book BART model that only takes the question as input performs much worse than additionally taking top-retrieved pas- sages, i.e., GAR + (G), especially on the questions that require generalizability. Notably, all methods perform signiﬁcantly better on theQuestion Over- lap category, which suggests that the high Total EM is mostly contributed by question memoriza- tion. That said, GAR + appears to be less dependent on question memorization given its lower EM for this category.9 6.4 Efﬁciency of G AR GAR is efﬁcient and scalable since it uses sparse representations for retrieval and does not in- volve time-consuming training process such as RL (Nogueira and Cho, 2017; Liu et al., 2019). The only overhead of GAR is on the generation of query contexts and the retrieval with generation- 9The same ablation study is also conducted on the retrieval stage and similar results are observed. More detailed discus- sions can be found in App. A. Training Indexing Retrieval DPR 24h w. 8 GPUs 17.3h w. 8 GPUs 30 min w. 1 GPU GAR 3∼6h w. 1 GPU 0.5h w. 35 CPUs 5 min w. 35 CPUs Table 8: Comparison of computational cost between DPR and G AR at different stages. The training time of GAR is for one generation target but different gener- ators can be trained in parallel. augmented (thus longer) queries, whose computa- tional complexity is signiﬁcantly lower than other methods with comparable retrieval accuracy. We use Nvidia V100 GPUs and Intel Xeon Plat- inum 8168 CPUs in our experiments. As listed in Table 8, the training time of GAR is 3 to 6 hours on 1 GPU depending on the generation