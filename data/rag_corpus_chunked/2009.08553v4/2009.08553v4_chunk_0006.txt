sparse meth- ods achieve comparable or better performance than dense methods (Guu et al., 2020; Karpukhin et al., 2020), while enjoying the simplicity and efﬁciency of sparse representations. GAR can also be used with dense representations to seek for even better performance, which we leave as future work. Generative QA. Generative QA generates answers through seq2seq learning instead of extracting an- swer spans. Recent studies on generative OpenQA (Lewis et al., 2020a; Min et al., 2020; Izacard and Grave, 2020) are orthogonal to GAR in that they focus on improving the reading stage and directly reuse DPR (Karpukhin et al., 2020) as the retriever. Unlike generative QA, the goal of GAR is not to generate perfect answers to the questions but perti- nent contexts that are helpful for retrieval. Another line in generative QA learns to generate answers without relevant passages as the evidence but solely the question itself using PLMs (Roberts et al., 2020; Brown et al., 2020). GAR further conﬁrms that one can extract factual knowledge from PLMs, which is not limited to the answers as in prior studies but also other relevant contexts. 3 Generation-Augmented Retrieval 3.1 Task Formulation OpenQA aims to answer factoid questions with- out pre-speciﬁed domains. We assume that a large collection of documents C (i.e., Wikipedia) are given as the resource to answer the questions and a retriever-reader architecture is used to tackle the task, where the retriever retrieves a small subset of the documents D⊂ C and the reader reads the documents D to extract (or generate) an answer. Our goal is to improve the effectiveness and efﬁ- ciency of the retriever and consequently improve the performance of the reader. 3.2 Generation of Query Contexts In GAR, queries are augmented with various heuris- tically discovered relevant contexts in order to re- trieve