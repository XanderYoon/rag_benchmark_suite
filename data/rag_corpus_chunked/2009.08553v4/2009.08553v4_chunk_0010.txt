fusion (Cormack et al., 2009), the results of which are slightly better according to our experiments.4 Next, one can use any off-the-shelf retriever for passage retrieval. Here, we use a simple BM25 model to demonstrate that GAR with sparse repre- sentations can already achieve comparable or better performance than state-of-the-art dense methods while being more lightweight and efﬁcient (includ- ing the cost of the generation model), closing the gap between sparse and dense retrieval methods. 4 OpenQA with G AR To further verify the effectiveness of GAR, we equip it with both extractive and generative read- ers for end-to-end QA evaluation. We follow the 3One may create a title ﬁeld during document indexing and conduct multi-ﬁeld retrieval but here we append the titles to the questions as other query contexts for generalizability. 4We use the fusion tools at https://github.com/ joaopalotti/trectools. reader design of the major baselines for a fair com- parison, while virtually any existing QA reader can be used with GAR. 4.1 Extractive Reader For the extractive setup, we largely follow the de- sign of the extractive reader in DPR (Karpukhin et al., 2020). Let D = [d1, d2, ..., dk] denote the list of retrieved passages with passage relevance scores D. Let Si = [s1, s2, ..., sN ] denote the top N text spans in passage di ranked by span relevance scores Si. Brieﬂy, the DPR reader uses BERT-base (De- vlin et al., 2019) for representation learning, where it estimates the passage relevance score Dk for each retrieved passage dk based on the [CLS] to- kens of all retrieved passages D, and assigns span relevance scores Si for each candidate span based on the representations of its start and end tokens. Finally, the span with the highest span relevance score from the passage with the highest passage