2017), which ﬁrst retrieves a small sub- set of documents using the question as the query and then reads the retrieved documents to extract (or generate) an answer. The retriever is crucial as it is infeasible to examine every piece of information in the entire document collection ( e.g., millions of Wikipedia passages) and the retrieval accuracy bounds the performance of the (extractive) reader. ∗Work was done during internship at Microsoft Azure AI. 1Our code and retrieval results are available at https: //github.com/morningmoni/GAR. Early OpenQA systems (Chen et al., 2017) use classic retrieval methods such as TF-IDF and BM25 with sparse representations. Sparse methods are lightweight and efﬁcient, but unable to per- form semantic matching and fail to retrieve rele- vant passages without lexical overlap. More re- cently, methods based on dense representations (Guu et al., 2020; Karpukhin et al., 2020) learn to embed queries and passages into a latent vector space, in which text similarity beyond lexical over- lap can be measured. Dense retrieval methods can retrieve semantically relevant but lexically differ- ent passages and often achieve better performance than sparse methods. However, the dense mod- els are more computationally expensive and suffer from information loss as they condense the entire text sequence into a ﬁxed-size vector that does not guarantee exact matching (Luan et al., 2020). There have been some recent studies on query re- formulation with text generation for other retrieval tasks, which, for example, rewrite the queries to context-independent (Yu et al., 2020; Lin et al., 2020; Vakulenko et al., 2020) or well-formed (Liu et al., 2019) ones. However, these methods re- quire either task-speciﬁc data (e.g., conversational contexts, ill-formed queries) or external resources such as paraphrase data (Zaiem and Sadat, 2019; Wang et al., 2020) that cannot or do not trans- fer well to OpenQA.