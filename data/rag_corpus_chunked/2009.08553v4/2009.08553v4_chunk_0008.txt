to the passages that contain the correct answers (e.g., co- occur with the correct answers). Context 2: Sentence containing the default tar- get. The sentence in a passage that contains the answer is used as another generation target. Sim- ilar to using answers as the generation target, the generated sentences are still beneﬁcial for retriev- ing relevant passages even if they do not contain the answers, as their semantics is highly related to the questions/answers (examples in Sec. 6.1). One can take the relevant sentences in the ground-truth passages (if any) or those in the positive passages of a retriever as the reference, depending on the trade-off between reference quality and diversity. Context 3: Title of passage containing the de- fault target. One can also use the titles of rele- vant passages as the generation target if available. Speciﬁcally, we retrieve Wikipedia passages using BM25 with the question as the query, and take the page titles of positive passages that contain the an- swers as the generation target. We observe that the page titles of positive passages are often entity names of interest, and sometimes (but not always) the answers to the questions. Intuitively, if GAR learns which Wikipedia pages the question is re- lated to, the queries augmented by the generated titles would naturally have a better chance of re- trieving those relevant passages. While it is likely that some of the generated query contexts involve unfaithful or nonfactual in- formation due to hallucination in text generation (Mao et al., 2020) and introduce noise during re- trieval, they are beneﬁcial rather than harmful over- all, as our experiments show that GAR improve both retrieval and QA performance over BM25 sig- niﬁcantly. Also, since we generate 3 different (com- plementary) query contexts and fuse their retrieval results, the distraction of