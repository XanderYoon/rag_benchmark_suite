Guu et al., 2020) with dense representations of the original queries, while being more lightweight and efﬁcient in terms of both training and inference (including the cost of the generation model) (Sec. 6.4). Speciﬁcally, we expand the query (question) by adding relevant contexts as follows. We conduct seq2seq learning with the question as the input and various freely accessible in-domain contexts as the output such as the answer, the sentence where the answer belongs to , and the title of a passage that contains the answer. We then append the gen- erated contexts to the question as the generation- augmented query for retrieval. We demonstrate that using multiple contexts from diverse gener- ation targets is beneﬁcial as fusing the retrieval results of different generation-augmented queries consistently yields better retrieval accuracy. We conduct extensive experiments on the Nat- ural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Trivia) (Joshi et al., 2017) datasets. The results reveal four major advantages of GAR: (1) GAR, combined with BM25, achieves signif- icant gains over the same BM25 model that uses the original queries or existing unsupervised query expansion (QE) methods. (2) GAR with sparse rep- resentations (BM25) achieves comparable or even better performance than the current state-of-the-art retrieval methods, such as DPR (Karpukhin et al., 2020), that use dense representations. (3) Since GAR uses sparse representations to measure lexical overlap2, it is complementary to dense representa- tions: by fusing the retrieval results of GAR and DPR (denoted as GAR +), we obtain consistently better performance than either method used individ- ually. (4) GAR outperforms DPR in the end-to-end QA performance (EM) when the same extractive reader is used: EM=41.8 (43.8 for GAR +) on NQ 2Strictly speaking, GAR with sparse representations han- dles semantics before retrieval by enriching the queries, while maintaining the advantage