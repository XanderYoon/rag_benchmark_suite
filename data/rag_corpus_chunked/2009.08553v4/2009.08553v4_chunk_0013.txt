data split, the average question (answer) length, and the number of answers for each question. 5.2 Evaluation Metrics Following prior studies (Karpukhin et al., 2020), we use top-k retrieval accuracy to evaluate the per- formance of the retriever and the Exact Match (EM) score to measure the performance of the reader. Top-k retrieval accuracy is deﬁned as the pro- portion of questions for which the top-k retrieved passages contain at least one answer span, which is an upper bound of how many questions are “an- swerable” by an extractive reader. Exact Match (EM) is the proportion of the pre- dicted answer spans being exactly the same as (one of) the ground-truth answer(s), after string normal- ization such as article and punctuation removal. 5.3 Compared Methods For passage retrieval, we mainly compare with BM25 and DPR, which represent the most used state-of-the-art methods of sparse and dense re- trieval for OpenQA, respectively. For query ex- pansion, we re-emphasize that GAR is the ﬁrst QE approach designed for OpenQA and most of the recent approaches are not applicable or efﬁcient enough for OpenQA since they have task-speciﬁc objectives, require external supervision that was shown to transfer poorly to OpenQA, or take many days to train (Sec. 2). We thus compare with a clas- sic unsupervised QE method RM3 (Abdul-Jaleel et al., 2004) that does not need external resources for a fair comparison. For passage reading, we compare with both extractive (Min et al., 2019a; Asai et al., 2019; Lee et al., 2019; Min et al., 2019b; Guu et al., 2020; Karpukhin et al., 2020) and gen- erative (Brown et al., 2020; Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020a; Izacard and Grave, 2020) methods when equipping GAR with the corresponding reader. 5.4 Implementation Details Retriever. We use Anserini