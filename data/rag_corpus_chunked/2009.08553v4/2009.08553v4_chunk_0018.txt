and negligible whenk≥ 100. When k≥ 500, GAR is slightly better than DPR despite Method NQ Trivia Top-5 Top-20 Top-100 Top-500 Top-1000Top-5 Top-20 Top-100 Top-500 Top-1000 BM25 (ours) 43.6 62.9 78.1 85.5 87.8 67.7 77.3 83.9 87.9 88.9 BM25 +RM3 44.6 64.2 79.6 86.8 88.9 67.0 77.1 83.8 87.7 88.9 DPR 68.3 80.1 86.1 90.3 91.2 72.7 80.2 84.8 - - GAR 60.9 74.4 85.3 90.3 91.7 73.1 80.4 85.7 88.9 89.7 GAR+ 70.7 81.6 88.9 92.0 93.2 76.0 82.1 86.6 - - Table 4: Top-k retrieval accuracy on the test sets . The baselines are evaluated by ourselves and better than reported in Karpukhin et al. (2020). G AR helps BM25 to achieve comparable or better performance than DPR. Best and second best methods are bold and underlined, respectively. that it simply uses BM25 for retrieval. In con- trast, the classic QE method RM3, while showing marginal improvement over the vanilla BM25, does not achieve comparable performance with GAR or DPR. By fusing the results of GAR and DPR in the same way as described in Sec. 3.3, we further obtain consistently higher performance than both methods, with top-100 accuracy 88.9% and top- 1000 accuracy 93.2%. On the Trivia dataset, the results are even more encouraging – GAR achieves consistently better retrieval accuracy than DPR when k ≥ 5. On the other hand, the difference between BM25 and BM25 +RM3 is negligible, which suggests that naively considering top-ranked passages as relevant (i.e., pseudo relevance feedback) for QE does not always work for OpenQA. Results on more cutoffs of k can be found in App. A. Effectiveness of diverse query contexts . In Fig. 1, we show the performance of GAR when different query contexts are used to augment the queries. Although the individual performance when using each query context