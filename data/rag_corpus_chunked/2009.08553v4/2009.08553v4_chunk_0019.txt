as relevant (i.e., pseudo relevance feedback) for QE does not always work for OpenQA. Results on more cutoffs of k can be found in App. A. Effectiveness of diverse query contexts . In Fig. 1, we show the performance of GAR when different query contexts are used to augment the queries. Although the individual performance when using each query context is somewhat similar, fusing their retrieved passages consistently leads to better performance, conﬁrming that different generation-augmented queries are complementary to each other (recall examples in Table 2). Performance breakdown by question type . In Table 5, we show the top-100 accuracy of the com- pared retrieval methods per question type on the NQ test set. Again, GAR outperforms BM25 on all types of questions signiﬁcantly andGAR + achieves the best performance across the board, which fur- ther veriﬁes the effectiveness of GAR. 6.3 Passage Reading with G AR Comparison w. the state-of-the-art . We show the comparison of end-to-end QA performance of extractive and generative methods in Table 6. Ex- tractive GAR achieves state-of-the-art performance 1 5 10 20 50 100 200 300 500 1000 k: # of retrieved passages 30 40 50 60 70 80 90Top-k Accuracy (%) Answer+Sentence+Title Answer+Sentence Answer+Title Answer Title Sentence Figure 1: Top-k retrieval accuracy on the test set of NQ when fusing retrieval results of different generation-augmented queries. Type Percentage BM25 DPR G AR GAR+ Who 37.5% 82.1 88.0 87.5 90.8 When 19.0% 73.1 86.9 83.8 88.6 What 15.0% 76.5 82.6 81.5 86.0 Where 10.9% 77.4 89.1 87.0 90.8 Other 9.1% 79.3 78.1 81.8 84.2 How 5.0% 78.2 83.8 83.2 85.5 Which 3.3% 89.0 90.7 94.1 94.9 Why 0.3% 90.0 90.0 90.0 90.0 Table 5: Top-100 retrieval accuracy breakdown of question type on NQ . Best and second best methods in each category are