where it estimates the passage relevance score Dk for each retrieved passage dk based on the [CLS] to- kens of all retrieved passages D, and assigns span relevance scores Si for each candidate span based on the representations of its start and end tokens. Finally, the span with the highest span relevance score from the passage with the highest passage rel- evance score is chosen as the answer. We refer the readers to Karpukhin et al. (2020) for more details. Passage-level Span Voting. Many extractive QA methods (Chen et al., 2017; Min et al., 2019b; Guu et al., 2020; Karpukhin et al., 2020) measure the probability of span extraction in different retrieved passages independently, despite that their collec- tive signals may provide more evidence in deter- mining the correct answer. We propose a simple yet effective passage-level span voting mechanism, which aggregates the predictions of the spans in the same surface form from different retrieved pas- sages. Intuitively, if a text span is considered as the answer multiple times in different passages, it is more likely to be the correct answer. Speciﬁcally, GAR calculates a normalized score p(Si[j]) for the j-th span in passage di during inference as follows: p(Si[j]) = softmax(D)[i]× softmax(Si)[j]. GAR then aggregates the scores of the spans with the same surface string among all the retrieved pas- sages as the collective passage-level score.5 4.2 Generative Reader For the generative setup, we use a seq2seq frame- work where the input is the concatenation of the question and top-retrieved passages and the target output is the desired answer. Such generative read- ers are adopted in recent methods such as SpanSe- 5We ﬁnd that the number of spans used for normalization in each passage does not have signiﬁcant impact on the ﬁnal performance (we takeN = 5) and using