approach that queries the EPG to retrieve a narrative prompt set about event e (Figure V-A). The set serves as a prompt to a Large Language Model (LLM) during report generation. There are three pri- mary steps to our approach: (1) Fine-Tuning an LLM with D + IR, (2) Data-to-text prefix-tuning, and (3) SPARQL template LLM prompting for report generation. Let V denote the vocabulary set of the report generation task. The desired target output is to generate report text denoted by Y utilizing the LLM where, Y = (w1, w2, ..., wj, ..., wT ) wj ϵ V is a single word in the generated report Y . To generate Y , we first fine-tune the LLM GPT-Neo [16] with our news intelligence corpus D + IR. Fine-tuning GPT-Neo using D + IR has two advantages. First, it augments the existing vocabulary of GPT-Neo ( VGP T −N eo) with the vocabulary of the news intelligence corpus ( VD+IR) which is equivalent to the vocabulary of FABULA’s EPG. After the fine-tuning report generation task, vocabulary V includes both VGP T −N eo and VD+IR. Second, fine-tuning with public intelligence reports IR provides the LLM with examples of the desired document structure for output Y . To generate the intelligence report Y , GPT-Neo takes as input a narrative prompt set, i.e. a set of narrative plot points about the event e stored in the subgraph G′ ϵ G. G′ = {(se, p, oe)|se, oe ϵ Ie, p ϵ R} where, Ie ϵ I and R denote the instances relevant to query e and relations stored in G (Equation (1). The determination of the narrative plot points in G′ retrieved from G, is imple- mented using the SPARQL Protocol and RDF Query Language templates executed on FABULA’s EPG G. The