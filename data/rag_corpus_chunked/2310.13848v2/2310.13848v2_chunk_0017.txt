model is a conditional probability of each word in the target text given the input and the previously generated words. We report a perplexity value (the exponential of the cross-entropy loss) of 11.14. Fig. 4. FABULA’s Retrieval-Augmented Generation (RAG) of Intelligence Report about event e. B. Prefix-tuning with Narrative Prompt Sets Traditionally, an autoregressive decoder like GPT-Neo re- quires a sentence based prompt to initiate generation. To modify this requirement and to enable prompting using a narrative prompt set for intelligence report Y generation, we require data-to-text prefix-tuning [27]. Prefix-tuning is a lightweight supplement to the fine-tuned GPT-Neo. This method keeps the GPT-Neo model parameters frozen and optimizes a sequence of continuous task-specific virtual vectors to the key and value matrices. When the tuning process is complete, the virtual tokens are stored in a lookup table, used during inference. We use the article to plot point mappings generated by the NPCE, described in Section IV-B and the HuggingFace Parameter-Efficient Fine-Tuning (PEFT) module for prefix-tuning. We use a beam decoding scheme and observed that adding more keywords provides increased supervision to the model and narrows the distribution of keyword context in the entire training dataset, leading to more accurate generation. More information on our evaluation can be found in Section VI. C. Large Language Model Prompting using FABULA’s EPG A FABULA generated intelligence report Y on event e should contain reliable and consistent information. Y should only contain plot points that are relevant to the input query event e, and should exclude non-event related details. Achiev- ing this criteria is not plausible by solely utilizing non- deterministic generative LLMs such as GPT-Neo, which are prone to output hallucinations [28]. We combat hallucinations and fulfill the above criteria by using a RAG-based approach by retrieving event narrative plot points stored in the