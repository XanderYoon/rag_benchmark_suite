Wikipedia reference descriptions help provide us with a lateral publicly available comparison for FABULA’s fragmented narrative construction. We use the Wikipedia Python library summaries endpoint [30] to extract event descriptors for 50 different public events we randomly selected from D + IR and calculate Rouge- 1 and Rouge-2. This provides term-based measures to quantify topic- level semantic relevance and syntactic quality [29]. Our results are displayed in Table VI-B. Rouge- 1 refers to overlap of unigrams between FABULA’s reports and Wikipedia’s event descriptors while Rouge- 2 refers to the overlap of bigrams. B. Human Evaluation (Qualitative) Study After evaluating the general efficacy of our model using quantitative metrics, we also conduct a human evaluation study to validate FABULA’s capability required specifically for the intelligence report generation task. Given the high cost of this evaluation, we task a group of 3 analysts to score reports across 5 randomly selected events with two aspects: factual correctness and language fluency. The first criterion evaluates how well the generated report conveyed the overall narrative of the event. The second criterion evaluates grammatically correctness and fluency of the generated intelligence report. The analysts were given a set of 5 articles per event (total 5 events), and were tasked to manually create a single report to convey the critical aspects across the set of 5 articles, for each separate event. This helped the analysts understand the narrative details for each of the 5 events. We then tasked the analysts to recommend IPP plot points from each of the 5 events. We compute Cohen’s kappa [31] to measure inter- annotator agreement for each of the recommended IPP plot points, keeping only the plot points that scored higher than 0.6. This helped us derive a gold standard set of 78 plot points that analysts want in the generated