over the single MaT index: ScoreMaT(q, i) = cos eq, ˜ei  ,rank by Score MaT. 3.2 Dual Encoders: Modular Integration Flattening metadata into the chunk text (Section 3.1) improves retrieval but is computationally expensive, since any metadata update requires re-embedding the full chunk index. To address this, we design dual-encoder approaches that embed content and metadata separately, making updates lighter and more mod- ular. Within this framework, we first present a unified single-index that merges both signals directly in embedding space, retaining the simplicity of serving while avoiding costly re-indexing. We then contrast it with a late-fusion dual encoder that combines scores at query time, and finally describe query-side strategies that surface metadata cues in the query. 6 Yousuf et al. Unified Single-Index via Weighted-Sum FusionLet the corpus beD= {(mi, ci)}N i=1, wherec i is chunk text andmi is a key–value metadata map (e.g., company,form,year,section). We encode content and metadata into the same d-dimensional space: etext i =f text θ (ci),e meta i =f meta θ (mi). We L2-normalize both vectors and form a convex combination to build a single fused index: ˆetext i = etext i ∥etext i ∥2 , ˆemeta i = emeta i ∥emeta i ∥2 , esum i (α) = α ˆetext i + (1−α) ˆemeta i ∥α ˆetext i + (1−α) ˆemeta i ∥2 , α∈[0,1].(1) At query time, we embed the query once with the text encoder and retrieve by cosine similarity against the fused index. Since document embeddings are already L2-normalized, leaving the query unnormalized does not affect ranking under cosine similarity: Scoresum(q, i;α) = cos etext q ,e sum i (α)  .(2) For inner-product distance, bothetext q ande sum i (α)should be L2-normalized to emulate cosine. Eqs. (1)–(2) yield a single index of dimensiond, without doubling as in concatenation. They