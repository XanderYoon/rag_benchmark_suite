form type).RAGMATE- 10Kis publicly available.3 We create 30 human-authored templates that instantiate into company- and year-specific questions, covering both general (e.g., business overview) and in- depth (e.g., risk factors) information needs. Excluding Apple filings from evalu- ation avoids contamination, leaving120test queries. Ground-truth answers are generated by constraining a language model to use only chunks from the target filing. The model must cite the supporting chunks, providing supervision for both retrieval accuracy and answer grounding. 8 Yousuf et al. Table 1: Flat metadata schema used in all experiments. Field Description Example company_nameFiling entity name Alphabet Inc. form_typeSEC form type 10-K sectionDocument section heading Item 1 - BUSINESS fiscal_year_endFiscal year end date 12-31 period_of_reportReporting period close date 2023-12-31 filed_dateSEC filing submission date 2024-01-31 exchange_listingsPublic exchange(s) listed on [NYSE] SIC_codeIndustry classification COMPUTER (a) Retrieval failure rate by category. (b) Average rank of first match. Fig.3: Comparative retrieval performance vs. plain baseline across query types using the Dual Encoder Unified Embedding approach. 4.2 Implementation Details We isolate metadata design effects by using a frozen text encoderfθ and a fixed retrieval pipeline. Each retrieval unit is a pair(mi, ci), wherec i is a chunk of document text andmi is a flat key–value metadata dictionary. Weevaluatetworepresentativeembeddingmodels:OpenAI’stext-embeddi- ng-3-small(dimension 1536) [2] and BAAI’sbge-m3(dimension 1024) [3], a strong open-source retriever optimized for multilingual and cross-domain re- trieval. Both encoders are used in frozen form without fine-tuning. Metadatam i is represented as a flat key–value dictionary, independent of the chunk text. The fields are serialized into a fixed-order header for text-based vari- ants, and passed verbatim to the metadata encoder for dual-encoder variants. We evaluate retrieval quality using cosine similarity with top-Ksearch, vary- ingK∈ {1, . . . ,10}. Performance is measured against ground-truth supporting chunks using four metrics: – Context@K: whether at least one retrieved chunk within the topKsup-