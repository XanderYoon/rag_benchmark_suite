for Better Retrieval-Augmented Generation 7 3.4 Embedding-Space Theory of Metadata Integration We consider a metadata-informed embedding˜e⋆ i that augments a chunk embed- dinge i =f θ(ci)with structured metadata, either through token-level prefixing (MaT) or vector-level fusion (Unified, see Sec. 3.2). Letd∈ Ddenote a doc- ument (e.g., a company–year SEC filing) andi∈da chunk belonging tod. The following propositions describe how such embeddings reshape the similarity landscape. Proposition 1 (Intra-document cohesion increases). Ei,j∈d[cos(˜ei, ˜ej)]>E i,j∈d[cos(ei,e j)]. Metadata anchors chunks to their document identity, pulling them closer in embedding space. Proposition 2 (Inter-document confusion decreases). Ei∈d1, j∈d2, d1̸=d2 [cos(˜ei, ˜ej)]<E i∈d1, j∈d2, d1̸=d2 [cos(ei,e j)]. Metadata provides discriminative cues (company, year, section) that reduce spu- rious similarity across different documents. Proposition 3 (Score variance increases). Var[cos(eq, ˜ei)]>Var[cos(e q,e i)], q∼typical queries. Unified embeddings interpolate between content-only and metadata-only signals viaaconvexweightα,andthereforeinherittheabovepropertieswheneverα <1. MaT achieves a similar effect through token-level injection, while Unified does so through vector-level fusion with tunable weighting. In effect, this creates a more structured space with clearer separation between relevant and irrelevant candidates. 4 Methodology 4.1 Dataset:RAGMA TE-10K We introduceRAGMATE-10K, a dataset of SEC 10-K filings designed to eval- uate metadata-aware retrieval. It consists of 25 filings from five U.S. technology companies (Apple, Alphabet, Adobe, Oracle, Nvidia), each segmented into non- overlapping 350-token chunks with 50-token overlap. This yieldsN= 4,490re- trieval units, each represented as a tuple(mi, ci)wherec i is the text content and mi its structured metadata (company, year, section, form type).RAGMATE- 10Kis publicly available.3 We create 30 human-authored templates that instantiate into company- and year-specific questions, covering both general (e.g., business overview) and in- depth (e.g., risk factors) information needs. Excluding Apple filings from evalu- ation avoids contamination, leaving120test queries. Ground-truth answers are generated by constraining a language model to use only chunks from the target filing. The model