1 (long and short papers). pp. 4171–4186 (2019) 13. Fu, J., Ng, S.K., Jiang, Z., Liu, P.: Gptscore: Evaluate as you desire. In: Proceed- ings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp. 6556–6576 (2024) 14. Gao, L., Ma, X., Lin, J., Callan, J.: Precise zero-shot dense retrieval without rel- evance labels. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 1762–1777 (2023) 15. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang, H., Wang, H.: Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.109972(1) (2023) 16. Hackl, V., Müller, A.E., Granitzer, M., Sailer, M.: Is gpt-4 a reliable rater? eval- uating consistency in gpt-4’s text ratings. In: Frontiers in Education. vol. 8, p. 1272229. Frontiers Media SA (2023) 17. Hu, Y., Koren, Y., Volinsky, C.: Collaborative filtering for implicit feedback datasets. In: 2008 Eighth IEEE international conference on data mining. pp. 263– 272. Ieee (2008) 18. Huang, H., Bu, X., Zhou, H., Qu, Y., Liu, J., Yang, M., Xu, B., Zhao, T.: An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge model is not a general substitute for gpt-4. In: Findings of the Association for Computational Linguistics: ACL 2025. pp. 5880–5895 (2025) 19. Islam, P., Kannappan, A., Kiela, D., Qian, R., Scherrer, N., Vidgen, B.: Fi- nancebench: A new benchmark for financial question answering. arXiv preprint arXiv:2311.11944 (2023) 20. Lai, V., Krumdick, M., Lovering, C., Reddy, V., Schmidt, C., Tanner, C.: Sec- qa: A systematic evaluation corpus for financial qa. In: Proceedings of The 10th Workshop on Financial Technology and Natural Language Processing. pp. 221–236 (2025) 21. Lee, C.T., Neeser, A., Xu, S., Katyan,