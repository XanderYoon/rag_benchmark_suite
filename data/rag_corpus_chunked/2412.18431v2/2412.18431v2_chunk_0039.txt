QA datasets: MuSiQue (Trivedi et al., 2022), HotpotQA (Yang et al., 2018), and 2Wiki- MultiHopQA (2Wiki) (Ho et al., 2020). Our dataset choices closely align with the multi-hop QA tasks, and are consistent with related studies in this space (Li et al., 2024; Fang et al., 2024; Gutierrez et al., 2024; Park et al., 2024). In order to explore the generalisability of GEAR in additional scenarios, we use the hand-picked case study data2 provided by Gutierrez et al.. These include four path-finding questions across four dif- ferent domains: books, movies, universities and biomedicine. We test GEAR against HippoRAG w/ IRCoT on these cases. Table 15 displays the results. In three out of the four cases, GEAR out- performs the competition in recall, successfully identifying more relevant passages, and misses the relevant passages in only one case. J Increasing the Number of Agent Iterations Figure 5 expands upon the analysis shown in Figure 2 by evaluating retrieval performance over 20 itera- tions, rather than the initial 4 iterations. The results 2https://github.com/OSU-NLP-Group/HippoRAG/ tree/main/data LLM MuSiQue 2Wiki HotpotQA R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 Closed-source GPT-4o mini 58.4 67 .6 71 .5 89 .1 95 .3 95 .9 93 .4 96.8 97 .3 Open-weight Llama-3.1-8B 52.4 62 .3 66 .7 81 .6 91 .0 93 .7 92 .2 97.4 98.1 Qwen-2.5-8B 53.7 63 .7 66 .7 85 .9 91 .6 93 .0 91 .7 96 .2 96 .9 Table 10: Retrieval performance of GEAR across different closed-source and open-weight models on MuSiQue, 2Wiki and HotpotQA. Results are reported using Recall@k (R@k) for k âˆˆ {5, 10, 15}, showing the percentage of questions for which the correct entries are found within the top-k retrieved passages. MuSiQue 2Wiki HotpotQA R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 GPT-4o mini w/ diversity