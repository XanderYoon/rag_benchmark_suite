IRCoT’s approach, we measure up to R@15 and include R@10 as an intermediate point, offering a comprehensive view of model performance across retrieval depths. Therefore, our evaluation employs recall at ranks 5, 10, and 15 (R@5, R@10, R@15). Choice of LLM Gutierrez et al. use gpt-3.5-turbo-1106 for their experiments, whereas in this paper we reproduce it with GPT-4o mini. GPT-4o mini was selected as a more capable alternative to GPT-3.5 Turbo (please refer to: https://openai.com/index/gpt-4 -mini-advancing-cost-efficient-intelli- gence). In order to alleviate any concerns regarding discrepancies with respect to the selected LLM, we also run experiments using gpt-3.5-turbo-1106. Table 8 shows the retrieval results of our proposed methods against Hip- poRAG w/ IRCoT. We observe a similar trend to that in Table 2— GEAR surpasses the performance of HippoRAG w/ IRCoT. E Why this graph construction method? We adopt an LLM-based triple extraction methodol- ogy, following the approach outlined in HippoRAG (Gutierrez et al., 2024). In their study, they evalu- ated the performance of various LLMs in OpenIE and compared these results with those of the end- to-end REBEL model (Huguet Cabot and Navigli, 2021). They reported substantial improvements in triple extraction when using LLMs in domains that deviate from conventional ClosedIE or OpenIE settings, which are respectively overly constrained and unconstrained in terms of named entities and pre-defined relations. Similar concerns about the generalisability and scalability of conventional KG construction approaches in open-domain scenarios are recognised by Wang et al., who sought to con- struct their graphs without relying on pre-existing ontologies, or KGs for named entity disambigua- tion. These findings resonate with the growing in- terest in recent literature towards applying such methodologies for automatic, schema-free knowl- edge graph construction (Li et al., 2024; Fang et al., 2024; Gutierrez et al., 2024; Park et al., 2024). As our primary