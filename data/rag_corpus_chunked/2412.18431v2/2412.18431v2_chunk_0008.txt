9: V ← [ ] 10: for t ∈ get_neighbours(T.last()) do 11: if exists(t, Bi−1) then 12: continue 13: s′ ← s+score(q, T ◦t) # concat 14: V.add(⟨s′, T ◦ t⟩) 15: sort(V, descending) 16: for n ∈ {0, . . . , V.length() − 1} do 17: ⟨s′, T ◦ t⟩ ← V [n] 18: s′ ← s′ × e− min(n,γ) γ 19: B.add(⟨s′, T ◦ t⟩) 20: Bi ← top(B, b) 21: return Bi and present a retrieval algorithm: Diverse Triple Beam Search (see Alg. 1). We maintain top-b sequences (beams) of triples and the scores at each step are determined by a scoring function. In this paper, we focus on lever- aging a dense embedding model to compute the cosine similarity between embeddings of the query and a candidate sequence of triples, leaving other implementations of the scoring function for future work (see Section 9). Considering all possible triple extensions at each step, in a Viterbi decoding fashion, would be in- tractable due to the size of T. Consequently, we define the neighbourhood of a triple as the set of triples with shared head or tail entities (i.e. get_neighbours in Alg. 1). During each expan- sion step, we only consider neighbours of the last triple in the sequence, and avoid selecting previ- ously visited triples (i.e. exists in Alg. 1) to further reduce the search space. While regular beam search can reduce the search space, it is prone to producing high-likelihood se- quences that differ only slightly from one another (Ippolito et al., 2019; Vijayakumar et al., 2018). Our algorithm increases the diversity across beams to improve the recall for retrieval. In detail, for each beam, we sort candidate sequences extended from that beam in descending order, and weight their scores based on their relative positions. Can-