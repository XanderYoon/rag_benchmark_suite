act as a more efficient alternative with respect to LLM token utilisation. We note that even for a single iteration, GEAR uses fewer tokens than HippoRAG w/ IRCoT. In contrast to ours, this trend exacerbates for the competition as the number of iterations increases. These findings also reiterate the value of SyncGE, which outper- forms a significantly more LLM-heavy solution on MuSiQue, using almost 2.9 million fewer tokens. Even in the case that HippoRAG w/ IRCoT runs for a single iteration it would require more than 0.7 million tokens that Hybrid + SyncGE, with a substantially lower R@15 of 51.7. 1 2 3 4 Number of Iterations (n) 1M 2M 3M 4MTokens Used (millions) Hybrid + SyncGE Input Hybrid + SyncGE Output GeAR Input GeAR Output HippoRAG w/ IRCoT Input HippoRAG w/ IRCoT Output Figure 4: Progressive accumulation of input and out- put LLM tokens across agent iterations on MuSiQue. The Hybrid + SyncGE method appears only to the left of Iteration 1 as it is a single-step approach. 9 Conclusion We propose GEAR, a novel framework that incor- porates a graph-based retriever within a multi-step retrieval agent to model the information-seeking process for multi-hop question answering. We showcase the synergy between our proposed graph retriever (i.e. SyncGE) and the LLM within the GEAR framework. SyncGE leverages the LLM to synchronise information from passages with triples and expands the graph by exploring diverse beams of triples that link multi-hop contexts. Our experiments reveal that this strategy improves over more naive implementations, demonstrating the LLMâ€™s capability to guide the exploration of initial nodes for graph expansion. Furthermore, GEAR utilises multi-hop contexts returned by SyncGE and constructs a gist memory which is used for effec- tively summarising information across iterations. GEAR achieves superior performance compared to other multi-step retrieval methods while requiring