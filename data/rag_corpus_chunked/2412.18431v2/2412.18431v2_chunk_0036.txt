main- tains highly competitive results and significantly outperforms alternative methods shown in Table 2. G Compatibility with Open-weight Models GEAR Results As shown in Table 10, we eval- uate GEAR using popular 7-8B parameter open- weight models, comparing them against a closed- source alternative. On HotpotQA, Llama-3.1-7B surpasses the closed-source alternative, achiev- ing higher recall rates at R@10 and R@15. For MuSiQue and 2Wiki, while the closed-source model maintains a slight superior edge in per- formance, the margin is narrow. Importantly, all tested open-weight models consistently out- perform the previous state-of-the-art, HippoRAG w/IRCoT. This decouples GEAR from the need to use closed-source models, suggesting that state-of- the-art multi-step retrieval can be achieved using more accessible models. Diverse Beam Search Results Expanding upon Table 4, Table 11 demonstrates that diverse beam search consistently improves retrieval performance across both closed-source and open-weight models when using our proposed Hybrid + SyncGE setup. This further confirms the broader applicability of this approach. H Robustness Studies We assess the robustness of our framework in re- trieving passages when triple extraction produces either limited or excessive triple content. Using the MuSiQue dataset, we group questions based on the average number of triples (i.e. triple density, œÅt) associated with their golden passages and evaluate R@15 performance across these ranges. Table 12 presents the results for both the single- and multi- step retrieval settings. The passage length remains R@k across Different Maximum Numbers of Iterations n = 1 n = 2 n = 3 n = 4 R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 Beam Search Length b = 2 58.1 66.0 69.5 59.2 68.9 71.3 57.9 68.0 71.5 58.4 67.6 71.5 b = 3 55.9 64.6 67.9 57.2 66.6 70.2 58.1 67.8 71.0 56.7 66.1 70.4 b = 4 54.9 62.9 67.3