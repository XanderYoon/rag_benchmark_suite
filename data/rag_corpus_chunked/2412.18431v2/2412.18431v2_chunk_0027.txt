question answering. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 2369–2380, Brussels, Belgium. Association for Com- putational Linguistics. A Dataset Choices and Statistics MuSiQue 2Wiki HotpotQA Split Source IRCoT IRCoT HippoRAG # Hops 2 − 4 2 2 # Documents 139, 416 430 , 225 9 , 221 # Test Queries 500 500 1 , 000 # Chunks (C) 148, 793 490 , 454 10 , 293 # Triples (T) 1, 521, 136 4 , 993, 637 122 , 492 Av. # T/C 10.2 10 .2 11 .9 Table 5: Dataset characteristics and preprocessing statis- tics, where triples are extracted from chunks, and Av. # T/C represents the average number of triples per chunk. Table 5 serves as a summary of various facts and statistics related to the employed datasets and the chunking and triple extraction process intro- duced in Section 3. Please note for all the evalu- ated datasets, we use their open-domain setting and answerable subset if applicable. Reasoning behind dataset split choices For MuSiQue and 2Wiki, we use the data provided by Trivedi et al., including the full corpus and sub- sampled test cases for each dataset. To limit the experimental cost for HotpotQA, we follow the set- ting by Gutierrez et al. where both the corpus and test split are smaller than IRCoT’s counterpart. B More Implementation Details B.1 Baselines Details We implement all proposed approaches using Elasticsearch1. For SBERT, we employ the all-mpnet-base-v2 model with approximate k- nearest neighbours and cosine similarity for vector comparisons. In IRCoT experiments, we evalu- ate both ColBERTv2 and BM25 retrievers — Col- BERTv2 for alignment with HippoRAG’s base- lines, and BM25 for consistency with the original IRCoT implementation. For all multi-step approaches, including ours, we follow Gutierrez et al. with respect to