QA experiments using the prompts provided in Appendix K.3. In addition to our proposed SyncGE, we consider a more naive implementation of GE (i.e. NaiveGE) to evaluate the performance when no LLM is in- volved and further demonstrate the effectiveness of synchronisation. In NaiveGE, we input all triples associated with C′ q (see Section 4) for diverse triple beam search. Comprehensive implementation de- tails are provided in Appendices B–D. Addition- ally, Appendix F provides more experiments evalu- 1 2 3 4 Number of Iterations (n) 40 45 50 55 60 65 70R@15 69.5 51.7 61.2 GeAR Hybrid + SyncGE HippoRAG w/ IRCoT IRCoT (BM25) IRCoT (ColBERTv2) Figure 2: R@15 over 4 iterations on MuSiQue. Re- call is computed at each iteration using the cumulative set of retrieved documents, with prior recall values car- ried forward for questions that terminated in earlier iterations. The horizontal line indicates the single-step performance of Hybrid + SyncGE. ating GEAR with varying configurations. 7 Results GEAR demonstrates state-of-the-art perfor- mance in multi-step retrieval The multi-step results in Table 2 show that our agent-based ap- proach to multi-step retrieval is highly effective, achieving state-of-the-art results across all datasets. While we see significant improvements on satu- rated datasets like 2Wiki and HotpotQA, GEAR especially excels on MuSiQue, delivering perfor- mance gains of over 10% compared to competitors. SyncGE contributes to state-of-the-art perfor- mance in single-step retrieval As shown in the single-step section of Table 2, our proposed Hy- brid + SyncGE method achieves state-of-the-art single-step retrieval performance on both MuSiQue and HotpotQA datasets. We observe consistent improvements using NaiveGE and SyncGE, out- performing HippoRAG in many setups regardless of the base retriever (i.e. sparse, dense or hybrid). Most notably, Hybrid + SyncGE surpasses Hip- poRAG by up to 9.8% at R@15 on MuSiQue. Higher recall leads to