Paul⟩ T3⟨Lund Cathedral, dedicated to, Saint Lawrence⟩ T4⟨Bremen, part of, Germany⟩ 6. Reasoner(see §5.2) After updatingG(n), we assess whether it contains sufficient evidence to answer the original question via an LLM reasoning step. Answerable:False Answer or reason:The provided facts do not contain information about the location of the basilica named for St. Peter, nor do they provide any details about when it became a country. The facts only mention the dedication of other cathedrals to different saints. 7. Rewriter(see §5.3) Given the originalq, the accumulated memoryG(n), and the reasoning outputr(n), an LLM is used to re-write the query. We return tostep 1and repeat. Next query:What is the location of the basilica dedicated to St. Peter, and when did that location become a country? Table 6: Visual walk-through example of the modules involved in offline index construction and online retrieval in GEAR. After query-rewriting, steps 1-7 are repeated until termination by the Reasoner, or reaching the maximum number of iterations. MuSiQue 2Wiki HotpotQA R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 HippoRAG original prompt 41.9 46.9 51.1 75.4 83.5 86.9 79.7 88.4 91.4 our prompt 41.0 47.0 51.4 75.1 83.2 86.4 79.8 89.0 92.4 HippoRAG w/ IRCoT original prompt 49.9 56.4 59.3 81.5 90.2 92.3 90.2 94.7 95.8 our prompt 48.8 54.5 58.9 82.9 90.6 93.0 90.1 94.7 95.9 Table 7: Retrieval performance comparison between HippoRAG’s sequential triple extraction method and our joint extraction approach across three datasets. LLM Retriever MuSiQue 2Wiki HotpotQA R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 GPT-3.5 turbo Hybrid + SyncGE 48.3 55.0 58.1 70.7 78.4 79.7 86.1 92.0 94.3 HippoRAG w/ IRCoT 45.5 51.0 54.8 80.0 87.9 89.8 87.4 92.6 94.6 GEAR 52.9 62.1 64.4 84.2 90.0 90.3 91.0 95.6 96.1 Table 8: Retrieval performance for our proposed retrievers and HippoRAG w/ IRCoT