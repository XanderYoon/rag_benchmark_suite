a similar synchronisation pro- cess that summarises retrieved passages in proximal triples to be stored in this multi-turn gist memory • determining if additional steps are needed for answering the original input question Within this multi-turn setting, the original input question q is iteratively decomposed into simpler queries: q(1), . . . ,q(n), where q(1) = q and n ∈ N represents the number of the current step. For each query q(n), we use the graph retrieval method introduced in Section 4 in order to retrieve relevant passages Cq(n). 5.1 Gist Memory Constructor To facilitate the multi-step capabilities of our agent, we introduce a gist memory, G(n), which is used for storing knowledge as an array of proximal triples. At the beginning of the first iteration, the gist mem- ory is empty. During the n-th iteration, similar to the knowledge synchronisation module explained in Section 4.1, we employ an LLM to read a collec- tion of retrieved paragraphs Cq(n) and summarise their content with proximal triples: TG q(n) =    read  Cq(n), q  , if n = 1 read  Cq(n), q, G(n−1)  , if n ≥ 2 (4) Apart from the first iteration where Eq. 1 and 4 are identical, the inclusion of the memory in the read operation differentiates the construction of proximal triples produced at the subsequent steps compared to the ones from Eq. 1. G(n) maintains the aggregated content of proximal triples s.t. G(n) = h TG q(1) ◦ · · · ◦ TG q(n) i , (5) where ◦ defines the concatenation operation. The triple memory serves as a concise representation of all the accumulated evidence, up to the n-th step. We believe the process introduced by the read step along with the information storage paradigm served by the gist