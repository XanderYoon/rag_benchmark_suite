67 .9 75 .5 76 .1 87 .0 92 .6 92 .9 IRCoT (ColBERTv2) 47.9 54 .3 56 .4 60 .3 86 .6 69 .7 86 .9 92 .5 92 .8 HippoRAG w/ IRCoT 48.8 54.5 58 .9 82.9 90.6 93.0 90.1 94.7 95.9 GEAR 58.4 67 .6 71 .5 89 .1 95 .3 95 .9 93 .4 96 .8 97 .3 Table 2: Retrieval performance for single- and multi-step retrievers on MuSiQue, 2Wiki, and HotpotQA. Results are reported using Recall@k (R@k) metrics for k ∈ {5, 10, 15}. 6.1 Baselines We evaluate GEAR against strong, multi-step base- lines, including IRCoT (Trivedi et al., 2023) and HippoRAG w / IRCoT (Gutierrez et al., 2024), which, similar to our framework, combines graph retrieval and a multi-step agent. To demonstrate our graph retriever’s (i.e. SyncGE) benefits, we evaluate it against several stand-alone, single-step retrievers: (i) BM25, (ii) Sentence-BERT (SBERT), (iii) a hybrid approach combining BM25 and SBERT through RRF and (iv) HippoRAG. Follow- ing Gutierrez et al., we refer to the single-step setup when multiple LLM iterations are not supported. 6.2 Implementation Details We reproduce HippoRAG and IRCoT using the code provided by Gutierrez et al.. To en- sure fair comparisons, we employ GPT-4o mini (gpt-4o-mini-2024-07-18) for all methods that require an LLM as well as their corresponding triple extraction. The temperature is set to 0. Our triple extraction prompt (in Appendix K.1) is in- spired by Gutierrez et al.. We run QA experiments using the prompts provided in Appendix K.3. In addition to our proposed SyncGE, we consider a more naive implementation of GE (i.e. NaiveGE) to evaluate the performance when no LLM is in- volved and further demonstrate the effectiveness of synchronisation. In NaiveGE, we input all triples associated with C′ q (see Section 4) for diverse triple beam search.