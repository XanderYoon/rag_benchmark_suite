prone to producing high-likelihood se- quences that differ only slightly from one another (Ippolito et al., 2019; Vijayakumar et al., 2018). Our algorithm increases the diversity across beams to improve the recall for retrieval. In detail, for each beam, we sort candidate sequences extended from that beam in descending order, and weight their scores based on their relative positions. Can- didate sequences that are ranked lower, within a beam, will receive smaller weights. Consequently, the resulting top-b beams at each step are less likely to share the same starting sequence. The top-b returned sequences are flattened in a breadth-first order. Each triple in the resulting list is then mapped to its source passage. This alignment between triples and passages is described in more detail in Section 3. Let eCq be the list of unique passages after alignment. The output of our graph expansion is then given by the Reciprocal Rank Fusion (RRF) (Cormack et al., 2009) of eCq and the initial C′ q list of passages : Cq = RRF  eCq, C′ q  . (3) We refer to this method for retrieving passages as Syncronised Graph Expansion (SyncGE). 5 Multi-step Extension We further present an agentic framework that models a human-like information-seeking process through multi-turn interactions with the graph- enhanced retriever. The resulting agent is referred to as GEAR. We focus on: • maintaining a gist memory of proximal knowl- edge obtained throughout the different steps • incorporating a similar synchronisation pro- cess that summarises retrieved passages in proximal triples to be stored in this multi-turn gist memory • determining if additional steps are needed for answering the original input question Within this multi-turn setting, the original input question q is iteratively decomposed into simpler queries: q(1), . . . ,q(n), where q(1) = q and n ∈