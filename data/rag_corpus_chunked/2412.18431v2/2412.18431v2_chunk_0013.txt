selected recall ranks in Appendix D. Following standard practices, QA performance is evaluated with Exact Match (EM) and F1 scores (Trivedi et al., 2023). Retriever MuSiQue 2Wiki HotpotQA R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 Single-step Retrieval ColBERTv2 39.4 44 .8 47 .7 59 .1 64 .3 66 .2 79 .3 87 .1 90 .1 HippoRAG 41.0 47 .0 51 .4 75.1 83 .2 86 .4 79.8 89 .0 92 .4 BM25 33.8 38 .5 41 .3 59 .5 62 .7 64 .1 74 .2 83 .6 86 .3 + NaiveGE 37.5 45 .5 48 .4 65 .0 70 .7 71 .8 79 .1 89 .1 91 .9 + SyncGE 44.7 52.6 57.4 70.5 76 .1 79 .3 87 .4 93.0 94.0 SBERT 31.1 37 .9 41 .6 41 .2 48 .1 51 .5 72 .1 79 .3 84 .0 + NaiveGE 32.2 41 .4 45 .4 45 .1 54 .0 57 .3 76 .1 84 .7 88 .8 + SyncGE 41.6 51 .3 54 .2 54 .8 64 .9 70 .7 84 .1 89 .6 92 .8 Hybrid 39.9 46 .3 49 .1 60 .0 65 .8 66 .6 77 .8 85 .8 89 .7 + NaiveGE 41.8 49 .4 53 .0 63 .0 70 .8 72 .6 80 .6 89 .4 92 .7 + SyncGE 48.7 57 .7 61 .2 72.6 80.9 82.4 87.4 93 .3 95 .2 Multi-step Retrieval IRCoT (BM25) 46.1 54 .9 57.9 67 .9 75 .5 76 .1 87 .0 92 .6 92 .9 IRCoT (ColBERTv2) 47.9 54 .3 56 .4 60 .3 86 .6 69 .7 86 .9 92 .5 92 .8 HippoRAG w/ IRCoT 48.8 54.5 58 .9 82.9 90.6 93.0 90.1 94.7 95.9 GEAR 58.4 67 .6 71 .5 89 .1 95 .3 95 .9 93 .4 96 .8