maintain consistent performance across both dense and sparse triple extraction out- comes. I Qualitative Analysis I.1 Positive Instances in MuSiQue Table 13 showcases some query instances where GEAR achieves perfect recall in a single iteration, while HippoRAG w / IRCoT achieves lower re- call and consumes all available iterations. The presented examples illustrate how GEAR’s Gist Memory G(n) precisely captures the essential infor- mation needed to answer MuSiQue’s queries, main- taining the appropriate level of granularity with- out including superfluous details. In contrast, Hip- poRAG w/ IRCoT struggles to retrieve crucial in- formation—whether due to limitations in its triple extraction step or retriever functionality—such as the exact population of Venice, which is necessary for accurate responses. Furthermore, the verbose nature of IRCoT’s thought process component con- trasts with GEAR’s streamlined approach. The lack of such verbose component in our approach contributes to the fact that GEAR requires fewer LLM tokens than the competition, as explained in subsection 8.3. I.2 Negative Instances in MuSiQue We manually assess 20 problematic cases in MuSiQue where GEAR did not achieve full re- call performance, and we identify the specific error types responsible for each. The findings, presented in Table 14 indicate that the majority of the errors are due to hallucinations of the LLM read steps, and only a very limited number of cases can be attributed to triple extraction. I.3 Beyond MuSiQuE, 2Wiki and HotpotQA We evaluate our framework on three open-domain multi-hop QA datasets: MuSiQue (Trivedi et al., 2022), HotpotQA (Yang et al., 2018), and 2Wiki- MultiHopQA (2Wiki) (Ho et al., 2020). Our dataset choices closely align with the multi-hop QA tasks, and are consistent with related studies in this space (Li et al., 2024; Fang et al., 2024; Gutierrez et al., 2024; Park et al., 2024). In order to explore the