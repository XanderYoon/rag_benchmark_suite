or degradation in subsequent iterations. While some minor fluctuations occur beyond this point, they are negligible. This performance plateau can be attributed to two key factors. First, the query re-writing mech- anisms in all investigated approaches struggle to generate effective subsequent queries. Second, our analysis has identified several cases of unanswer- able queries within MuSiQue’s answerable subset. A representative example is provided in Table 16. Error Category Count Example Base Retriever Limitations 2/20 (10%) Question:Who owns the record label where the singer of All Right records? Failure Explanation:Instead of retrieving passages about the song ‘All Right,’ the base retriever returns passages about the songs ‘All Right Now’ and ‘Who Owns My Heart,’ which are unrelated to the gold passage. Reasoner Hallucinations2/20 (10%) Question:What is the name of the castle in the city where the headquarters of the production company of A Cosmic Christmas is located? Failure Explanation:The LLM Reasoner concluded the question was answerable, even though key information related to ‘A Cosmic Christmas’ was still missing. Reader Hallucinations 7/20 (35%) Question:Who played the character in Willy Wonka and the Chocolate Factory that the performer of Victrola was named after? Failure Explanation:The triple⟨Victrola, named after, Violet Beauregardel⟩is hallucinated to complete the hop needed to reach the answer. Dataset Issues 5/20 (25%) Question:What other recognition did the Oscar winner for Best Actor in 2006 receive? Failure Explanation:The corpus has no mention that the ‘Oscars in 2006’ were also called the ‘78th academy awards’, preventing the system from finding the relevant information. Missing Details during Triple Extraction 1/20 (5%) Question:What was the form of the language that the last name Sylvester comes from, used in the era of the man crowned Roman Emperor in AD 800, later known as? Failure Explanation:No triple extracted about the last sentence in the gold paragraph (titled ‘Middle