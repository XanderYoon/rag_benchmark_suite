Retrieval performance of GEAR across different closed-source and open-weight models on MuSiQue, 2Wiki and HotpotQA. Results are reported using Recall@k (R@k) for k ∈ {5, 10, 15}, showing the percentage of questions for which the correct entries are found within the top-k retrieved passages. MuSiQue 2Wiki HotpotQA R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 GPT-4o mini w/ diversity 48.7 57.7 61.2 72.6 80.9 82.4 87.4 93.3 95.2 w/o diversity 47.0 53.9 58.4 68.2 76.0 77.4 85.0 92.2 94.3 Llama-3.1-8B-Instruct w/ diversity 46.2 54.3 57.4 69.1 78.1 81.6 87.3 92.8 95.1 w/o diversity 44.9 52 .7 55 .0 66 .9 75 .9 78 .2 85 .0 91 .7 94 .4 Table 11: Retrieval performance of the Hybrid + SyncGE method with different LLMs for the read step (see Eq. 1) w/ and w/o diversity for triple beam search. Results are reported using Recall@ k (R@k) for k ∈ { 5, 10, 15}, showing the percentage of questions for which the correct entries are found within the top-k retrieved passages. Retriever ρt < 9 9 ≤ ρt < 11 11 ≤ ρt < 13 13 ≤ ρt Single-step Retrieval Hybrid + NaiveGE 50.6 54 .6 54 .1 50 .0 Hybrid + SyncGE 62.8 (↑ 12.2%) 61.2 (↑ 6.6%) 59.8 (↑ 5.7%) 60.1 (↑ 10.1%) Multi-step Retrieval HippoRAG w/ IRCoT 64.4 65 .5 55 .0 52 .8 GEAR 73.6 (↑ 9.2%) 73.7 (↑ 8.2%) 69.3 (↑ 14.3%) 69.7 (↑ 16.9%) Table 12: Retrieval performance for single- and multi-step retrievers across different triple density measurements in MuSiQue. Results are reported using R@15. Triple densities (ρt) are calculated as the average number of triples associated with the gold documents for the questions within the MuSiQue’s test set. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16