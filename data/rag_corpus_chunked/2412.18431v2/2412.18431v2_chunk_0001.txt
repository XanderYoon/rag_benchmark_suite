3-hop reasoning chain starting from the question’s main entity (i.e. “Stephen Curry”). A base retriever cannot, by design, retrieve all neces- sary information in a single step. To address these complex reasoning require- ments, researchers have increasingly turned to graph-based approaches (Fang et al., 2024; Li et al., 2024; Edge et al., 2024; Gutierrez et al., 2024; †The authors contributed equally to this work. In what year did Stephen Curry’s father join the team from which he started his college basketball career? Table 1: A motivating example of a multi-hop question where a base retriever cannot, by design, retrieve all necessary information in a single step. Graph expansion (see §4.2), which we incorporate within GEAR, enables retrieval of subsequent hops and guides the system to- ward the correct answer without using an LLM. Liang et al., 2024). By extracting entities, atomic facts, or semantic triples (Li et al., 2024; Fang et al., 2024; Gutierrez et al., 2024), these graphs can es- tablish more direct pathways for multi-hop reason- ing. For instance, HippoRAG extracts triples from passages to form a knowledge graphs and employs a variant of PageRank for passage retrieval (Gutier- rez et al., 2024). GraphReader uses an LLM agent with graph-navigating operations to explore the re- sulting graph structure (Li et al., 2024). TRACE relies on an LLM to iteratively select triples for constructing reasoning chains, which then either ground answer generation, or filter irrelevant doc- uments from initially retrieved results (Fang et al., 2024). However, recursively prompting LLMs to traverse graphs remains computationally expensive, particularly as search spaces expand. In this paper, we present GEAR, a Graph- enhanced Agent for Retrieval-augmented gener- ation. During the offline stage, we align passages with their extracted triples to create interconnected indices. This alignment allows passages to be connected through graphs of