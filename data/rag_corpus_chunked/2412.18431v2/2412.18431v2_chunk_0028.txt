SBERT, we employ the all-mpnet-base-v2 model with approximate k- nearest neighbours and cosine similarity for vector comparisons. In IRCoT experiments, we evalu- ate both ColBERTv2 and BM25 retrievers — Col- BERTv2 for alignment with HippoRAG’s base- lines, and BM25 for consistency with the original IRCoT implementation. For all multi-step approaches, including ours, we follow Gutierrez et al. with respect to the maximum number of retrieval iterations, which vary based on the hop requirements of each dataset. Thus, we use a maximum of 4 iterations for MuSiQue and 2 iterations for HotpotQA and 2Wiki. B.2 G EAR Details GEAR involves several hyperparameters, such as the beam size inside graph expansion. We ran- 1https://www.elastic.co domly sampled 500 questions from the MuSiQue development set, which we ensure not to overlap with the relevant test set. We select our hyperpa- rameters based on this sample without performing a grid search across all possible configurations. Our goal is to demonstrate that our method is able to achieve state-of-the-art results without extensive parameter tuning. The initial retrieval phase utilises the chunks index C as the information source, while leaving the triple index T unused. Our graph expansion component implements beam search with length 2, width 10, and 100 neighbours per beam. The hyperparameter γ employed in diverse triple beam search is set to twice the beam search width. For the scoring function, we use the cosine similarity score and the SBERT embedding model. In Appendix F, we test the performance of GEAR across different beam search length values and maximum numbers of agent iterations. For the single-step configurations (i.e. any base retriever with NaiveGE or SyncGE), we set the base retriever’s maximum number of returned chunks to match our evaluation recall threshold. With the multi-step setup, we maintain a consistent maxi- mum of 10 retrieved chunks