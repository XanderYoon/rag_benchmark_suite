G(n) upon ter- mination, we link each proximal triple in G(n) to a list of passages as follows: Ctj = passageLink (tj, k) , (8) where j ∈  1, . . . ,|G(n)| . Similar to tripleLink, passageLink is implemented by re- trieving passages with a triple as the query (see Appendix B.2). The final list of passages returned by GEAR is the RRF of the resulting linked pas- sages and passages retrieved across iterations: C(n) q = RRF Ct1, . . . ,Ct|G(n)| , Cq(1), . . . ,Cq(n)  . (9) All relevant prompts for the read, reason and rewrite steps are provided in Appendix K.2. 6 Experimental Setup We evaluate our framework on three open-domain multi-hop QA datasets: MuSiQue (Trivedi et al., 2022), HotpotQA (Yang et al., 2018), and 2Wiki- MultiHopQA (2Wiki) (Ho et al., 2020). For MuSiQue and 2Wiki, we use the data provided in the IRCoT paper (Trivedi et al., 2023) which includes the full corpus, while for HotpotQA, we follow the same setting as HippoRAG (Gutierrez et al., 2024) to limit experimental costs. More de- tails are provided in Appendix A. We measure both retrieval and QA performance, with our primary contributions focused on the re- trieval component. For retrieval evaluation, we use Recall@k (R@k) for k ∈ {5, 10, 15}, showing the percentage of questions where the correct entries are found within the top-k retrieved passages. We include an analysis about the selected recall ranks in Appendix D. Following standard practices, QA performance is evaluated with Exact Match (EM) and F1 scores (Trivedi et al., 2023). Retriever MuSiQue 2Wiki HotpotQA R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 Single-step Retrieval ColBERTv2 39.4 44 .8 47 .7 59 .1 64 .3 66 .2 79 .3 87 .1 90 .1 HippoRAG 41.0 47