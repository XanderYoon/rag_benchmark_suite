47 .4 62 .3 50 .4 69 .4 Table 3: End-to-end QA performance using the top- 5 retrieved passages. The best model is in bold and second best is underlined. The top part shows the lower and upper bounds of QA performance, while the middle and bottom sections display scores for single-step and multi-step retrievers, respectively. 8 Discussion 8.1 What makes G EAR work? NaiveGE vs SyncGE As shown in Table 2, both graph expansion variants enhance every base re- triever’s performance across all datasets. The su- perior performance of SyncGE indicates the effec- tiveness of using LLMs for locating initial nodes. Notably, it surpasses HippoRAG w/ IRCoT’s on MuSiQue without multiple iterations. Metric Dataset w / Diversity w /o Diversity R@5 MuSiQue 48.7 47.0 2Wiki 72.6 68.2 HotpotQA 87.4 85.0 R@10 MuSiQue 57.7 53.9 2Wiki 80.9 76.0 HotpotQA 93.3 92.2 R@15 MuSiQue 61.2 58.4 2Wiki 82.4 77.4 HotpotQA 95.2 94.3 Table 4: Effects of beam search diversity on Hybrid + SyncGE across MuSiQue, 2Wiki and HotpotQA. Diverse Triple Beam Search improves perfor- mance As shown in Table 4, our diverse triple beam search consistently outperforms standard beam search across all datasets and recall ranks. By incorporating diversity weights into beam search, we align a language modelling-oriented solution with information retrieval objectives that involve satisfying multiple information needs underlying multi-hop queries (Drosou and Pitoura, 2010). GEAR mostly nails it the first time While GEAR supports multiple iterations, Figure 2 shows that GEAR achieves strong retrieval performance in a single iteration on MuSiQue. This differenti- ates it from IRCoT-oriented setups that require at least 2 iterations to reach maximum performance. This can be attributed to the fact that GEAR reads (Eq. 4) multi-hop contexts and associates proximal triples in gist memory with passages, establishing synergy between our graph retriever and the