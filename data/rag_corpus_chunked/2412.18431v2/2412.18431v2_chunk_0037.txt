n = 2 n = 3 n = 4 R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 Beam Search Length b = 2 58.1 66.0 69.5 59.2 68.9 71.3 57.9 68.0 71.5 58.4 67.6 71.5 b = 3 55.9 64.6 67.9 57.2 66.6 70.2 58.1 67.8 71.0 56.7 66.1 70.4 b = 4 54.9 62.9 67.3 56.6 66.3 69.3 58.1 67.9 71.0 56.1 66.1 69.9 Table 9: GEAR’s retrieval performance across different hyper-parameters in terms of maximum number of agent iterations (n) and graph expansion’s beam search length ( b). Results are reported using Recall@ k (R@k) for k ∈ {5, 10, 15} for the MuSiQue dataset. consistent across passages; hence, triple density serves as a proxy for the quality of triple extrac- tion. The results showcase that SyncGE and GEAR are more robust than the competition at retrieving suitable passages. NaiveGE’s performance tends to decline when the average number of triples as- sociated with the gold passages either falls below or exceeds a certain threshold (for MuSiQue, the average number of triples extracted from the gold passages is 11.71). A similar trend is observed for HippoRAG w / IRCoT in the case of golden passages associated with more than 11 triples. We believe that this trend can be partially attributed to the Personalised PageRank machinery that makes HippoRAG agnostic to the semantic relationships of the extracted triples. In contrast, SyncGE and GEAR are able to maintain consistent performance across both dense and sparse triple extraction out- comes. I Qualitative Analysis I.1 Positive Instances in MuSiQue Table 13 showcases some query instances where GEAR achieves perfect recall in a single iteration, while HippoRAG w / IRCoT achieves lower re- call and consumes all available iterations. The presented examples illustrate how GEAR’s Gist Memory G(n) precisely captures