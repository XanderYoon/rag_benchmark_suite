MuSiQue 2Wiki HotpotQA R@5 R@10 R@15 R@5 R@10 R@15 R@5 R@10 R@15 GPT-3.5 turbo Hybrid + SyncGE 48.3 55.0 58.1 70.7 78.4 79.7 86.1 92.0 94.3 HippoRAG w/ IRCoT 45.5 51.0 54.8 80.0 87.9 89.8 87.4 92.6 94.6 GEAR 52.9 62.1 64.4 84.2 90.0 90.3 91.0 95.6 96.1 Table 8: Retrieval performance for our proposed retrievers and HippoRAG w/ IRCoT across various datasets. We use gpt-3.5-turbo-1106 (temperature = 0) as the underlying LLM, to replicate HippoRAGâ€™s experimental setup. D Ensuring Fair Comparisons Although related studies often use common datasets, their experimental settings are frequently inconsistent. For instance, Gutierrez et al. used heavily sub-sampled corpora, drastically reducing the number of documents (e.g., from over 5M to 9k for HotpotQA) compared to the full datasets originally established for IRCoT (Trivedi et al., 2022). We believe such reductions significantly simplify the retrieval task. Therefore, in our paper, we reproduced HippoRAG on MuSiQue and 2Wiki using the same dataset settings (i.e., full corpus and identical evaluation split) as defined in the original IRCoT paper (Trivedi et al., 2022). On HotpotQA, however, processing the full corpus established by Trivedi et al. (2022) (over 5 million passages) was computationally prohibitive, so we follow the same setting as HippoRAG to limit the experimental cost. To ensure fairness in our comparisons, we ran all baselines using a consistent experimental setup. Additionally, for areas of potential discrepancy, and where possible, we report the retrieval performance of baselines in their original configuration to con- firm that the used experimental setup does not ad- versely affect performance. Below, we address any potential discrepancies in the following areas: (i) triple extraction methodology, (ii) retrieval met- rics, and (iii) LLMs. Choice of triple extraction methodology Hip- poRAG employs a sequential approach to triple ex- traction: it first identifies named entities from a text