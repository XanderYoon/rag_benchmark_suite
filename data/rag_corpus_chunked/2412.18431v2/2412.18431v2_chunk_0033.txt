original configuration to con- firm that the used experimental setup does not ad- versely affect performance. Below, we address any potential discrepancies in the following areas: (i) triple extraction methodology, (ii) retrieval met- rics, and (iii) LLMs. Choice of triple extraction methodology Hip- poRAG employs a sequential approach to triple ex- traction: it first identifies named entities from a text chunk, and then uses these entities to guide triple extraction in a second step. In contrast, our method extracts both entities and triples simultaneously. Table 7 shows that both approaches achieve com- parable retrieval performance across all datasets, with each method excelling in different scenarios. These results validate that joint entity and triple ex- traction can match the effectiveness of sequential extraction while reducing the number of required processing steps. Reasoning behind retrieval metrics The MuSiQue dataset contains 2-hop, 3-hop and 4-hop questions, where a k-hop question is defined as one that requires k pieces of evidence to reach the correct answer (Trivedi et al., 2022). This means 3- hop and 4-hop questions require more than 2 pieces of evidence. If we used Recall@2 to evaluate them, as previous works such as (Gutierrez et al., 2024) do, we would be misjudging these questions, since it assumes only two pieces of evidence are enough for perfect recall. Additionally, given modern LLMs’ expanding context length capabilities (Ding et al., 2024), examining recall beyond R@5 (HippoRAG’s highest evaluated rank) provides valuable insights. Following IRCoT’s approach, we measure up to R@15 and include R@10 as an intermediate point, offering a comprehensive view of model performance across retrieval depths. Therefore, our evaluation employs recall at ranks 5, 10, and 15 (R@5, R@10, R@15). Choice of LLM Gutierrez et al. use gpt-3.5-turbo-1106 for their experiments, whereas in this paper we reproduce it with GPT-4o mini. GPT-4o