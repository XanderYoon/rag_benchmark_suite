text = " The capital of France is Paris . " , score =0.9) , 8 Context ( id =2 , title = " Germany " , text = " Berlin is the capital of Germany . " , score =0.5) 9 ] 10 # Create a Document 11 doc = Document ( question = question , answers = answers , contexts = contexts ) 12 # Initialize Generator with In - Context RALM 13 generator = Generator ( method = " in - context - ralm " , model_name = ' meta - llama / Llama -3.1 -8 B ') 14 # Generate answer 15 g ene ra te d_ ans we rs = generator . generate ([ doc , doc ]) 16 print ( g en er at ed_ an sw er s ) Listing 7: Applying Retrieval-Augmented Generation (RAG) in Rankify. Rankify allows users to leverage large-scale language mod- els such as LLaMA [ 96], GPT-4 [ 5], and T5-based models [ 75] for retrieval-augmented generation. By supporting both encoder- decoder architectures (FiD [ 38]) and decoder-only models (e.g., GPT, LLaMA), the framework provides flexibility for optimizing generation quality based on task-specific requirements. 3.6 Evaluation Metrics Rankify provides evaluation metrics for retrieval, re-ranking, and retrieval-augmented generation (RAG). For retrieval and re-ranking, Top-k accuracy measures whether documents containing the cor- rect answer appear within the top-k results. This definition of rel- evance, while task-specific, aligns with the goal of downstream applications like question answering. Listing 8 demonstrates how to compute Top-k accuracy. 1 from rankify . metrics . metrics import Metrics 2 metrics = Metrics ( documents ) 3 # Compute Top - k accuracy before and after re - ranking 4 before_ranking = metrics . c a l c u l a t e _ r e