fundamental to many applications [56, 98], including question-answering [4, 43], search engines [20, 103], and knowledge-based generation [ 57]. These systems often rely on a two-stage pipeline: a retriever that efficiently identifies a set of candidate documents and a re-ranker that refines these results to maximize relevance to the query [66, 79, 109]. This approach has proven highly effective, with retrieval and re-ranking methods achieving state-of-the-art performance across diverse NLP benchmarks. Retrievers form the backbone of information retrieval systems by identifying a subset of documents relevant to a user’s query. The retrieval landscape includes sparse, dense, and hybrid methods. Sparse retrievers, such as BM25 [ 80], rely on exact term match- ing by representing queries and documents as high-dimensional sparse vectors. These methods are highly effective when query terms closely align with document content but they struggle to capture semantic relationships. Dense retrievers, including models arXiv:2502.02464v3 [cs.IR] 19 Feb 2025 SIGIR ’25, July 13–18, 2025, Padova, IT Abdallah et al. Table 1: Comparison of Retriever, Re-ranking, and RAG toolkits. Modular Design indicates if the toolkit uses modular components. Automatic Evaluation refers to the availability of built-in evaluation tools. Corpus shows whether the toolkit provides utilities for corpus processing, such as cleaning and chunking. The Combination column represents the total number of possible configurations, calculated as the product of the number of datasets, retrievers, re-rankers, and RAG methods (e.g., for Rankify: 40 × 7 × 24 × 3 = 20,160). Toolkit # Datasets # Retrievers # Re-rankers # RAG Methods Modular Design Automatic Evaluation Corpus Combination FlashRAG [39] 32 3 2 12 ✓ ✓ ✓ 2,304 FastRAG [2] 0 1 3 7 ✓ ✗ ✗ 21 AutoRAG [46] 4 2 7 1 ✓ ✓ ✗ 56 LocalRQA [111] 0 2 0 0 ✗ ✓ ✗ 2 Rerankers [19] 0 0 15