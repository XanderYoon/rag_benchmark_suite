integrates pre-retrieved datasets with structured annotations, enabling users to experiment with various retrieval and re-ranking methods with minimal code. The datasets in Rankify are categorized based on task type and size, as shown in Table 2. Each dataset follows a standardized schema with three main components: the Question, which represents the query requiring relevant information; the Answers, which are the expected correct responses to the query; and the Contexts (or Retrieved Documents), which provide a ranked list of candidate documents retrieved by a specific retriever. To support benchmarking and reproducibility,Rankify provides pre-retrieved datasets, each with 1,000 top-ranked documents from Wikipedia8, processed by various retrievers such as BM25, DPR, ANCE, BGE, Contriever, and ColBERT. These datasets cover diverse Re-Ranking and RAG tasks. Rankify allows users to load and explore datasets with minimal effort. Below is an example demonstrating how to download and inspect a dataset using Rankify â€™s API: 1 from rankify . dataset . dataset import Dataset 2 # Load the nq - test dataset retrieved with BM25 3 dataset = Dataset ( retriever = " bm25 " , dataset_name = " nq - test " ) 4 documents = dataset . download ( force_download = False ) Listing 2: Loading a dataset in Rankify and inspecting its structure. Beyond preloaded datasets, Rankify enables users to define their own datasets using the Dataset class. Below is an example demonstrating how to create a custom retrieval dataset: 1 from rankify . dataset . dataset import Dataset , Document , Question , Answer , Context 2 question = Question ( " What is the capital of France ? " ) 3 answer = Answer ([ " Paris " ]) 4 context = [ Context ( score =0.9 , has_answer = True , id =1 , title = " France " , text