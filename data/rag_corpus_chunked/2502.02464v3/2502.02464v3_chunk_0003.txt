# Datasets # Retrievers # Re-rankers # RAG Methods Modular Design Automatic Evaluation Corpus Combination FlashRAG [39] 32 3 2 12 ✓ ✓ ✓ 2,304 FastRAG [2] 0 1 3 7 ✓ ✗ ✗ 21 AutoRAG [46] 4 2 7 1 ✓ ✓ ✗ 56 LocalRQA [111] 0 2 0 0 ✗ ✓ ✗ 2 Rerankers [19] 0 0 15 0 ✓ ✓ ✗ 15 CHERCHE [88] 0 7 2 0 ✗ ✗ ✗ 14 Rankify 40 7 24 3 ✓ ✓ ✓ 20,160 like DPR [109], BPR [105], ColBERT [83], BGE [13], Contriever [36] and ANCE [ 104], overcome this limitation by encoding queries and documents into low-dimensional dense vectors using neural networks, enabling retrieval based on semantic similarity even when lexical overlap is minimal. However, dense retrieval requires significant computational resources for training and inference. Hy- brid retrieval methods [ 15, 106] integrate the strengths of both approaches, combining the precision of sparse retrievers with the semantic understanding of dense models to achieve a more balanced and robust retrieval system. Re-ranking techniques enhance the initial retrieval results by ensuring the most relevant documents appear at the top. These methods are categorized into three main approaches: (1) Pointwise reranking [3, 4, 66, 82] treats reranking as a regression or classifica- tion task, assigning independent relevance scores to each document. (2) Pairwise reranking [35, 73, 87] refines ranking by comparing document pairs and optimizing their relative order based on rele- vance. However, this approach treats all document pairs equally, sometimes improving lower-ranked results at the expense of top- ranked ones. (3) Listwise reranking [71, 90, 110] considers the en- tire document list, prompting LLMs with the query and a subset of candidates for reranking. Due to input length limitations, these methods tend to employ a sliding window strategy, progressively refining