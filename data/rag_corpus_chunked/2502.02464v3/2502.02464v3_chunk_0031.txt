generator module when integrated with Retrieval-Augmented Language Mod- eling (RALM) [77]. We evaluate the effectiveness of various retrieval models, including BM25, Contriever, and DPR, across three open- domain QA datasets: NQ, TriviaQA, and WebQ. The experiments utilize multiple LLMs: LLaMA V3 8B, LLaMA V3.1 8B, Gemma 2B/9B, LLaMA 2 13B, and Mistral 7B to assess the robustness of the retrieval and generation pipeline. We focus on the Exact Match (EM) metric, which measures the proportion of generated answers that exactly match the ground-truth references. Figure 4 provides a consolidated view of the EM scores for all datasets and models. From the figure, we observe the following key patterns: For the NQ dataset, DPR consistently outperforms other retrievers, achieving its peak performance with the LLaMA V3 8B model (28.08%). On the TriviaQA dataset, BM25 achieves the highest EM score (57.55%) when paired with the Gemma 2 9B model, outperforming both DPR and Contriever. For the WebQuestions (WebQ) dataset, DPR again exhibits strong performance, achieving its best EM score (19.83%) with the LLaMA V3 8B model. 5 Conclusion We introduced Rankify, a modular toolkit that unifies retrieval, re-ranking, and retrieval-augmented generation (RAG) within a cohesive framework. By integrating diverse retrievers, state-of-the- art re-rankers, and seamless RAG pipelines, Rankify streamlines experimentation and benchmarking in information retrieval. Our experiments demonstrate its effectiveness in enhancing ranking quality and retrieval-based text generation. Designed for extensibility, Rankify allows for easy integration of new models and evaluation methods. Future work will focus on improving retrieval efficiency, optimizing re-ranking strategies, and advancing RAG capabilities. The framework is open-source, ac- tively maintained, and freely available for the research community. Future work will focus on incorporating alternative retrieval evalu- ation metrics such as MAP, Precision, and NDCG, while expanding dataset coverage to include BEIR for broader benchmarking. The framework is open-source,