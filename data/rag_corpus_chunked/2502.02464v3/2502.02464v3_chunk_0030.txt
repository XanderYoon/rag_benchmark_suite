re-ranking improves document ranking quality across different models. Across all methods, re-ranking consistently improves retrieval effectiveness, with notable gains observed in Top-1 accuracy. Transformer-based models such as UPR (T5-based), RankGPT, FlashRank (MiniLM and TinyBERT), and RankT5 achieve substantial improvements over the BM25 base- line. In particular, RankT5-3B outperforms other models, achiev- ing a Top-1 accuracy of 47.17% on NQ-Test and 40.40% on WebQ, highlighting the effectiveness of T5-based models for large-scale retrieval. RankGPT, leveraging LLaMA-3.1-8B, also shows strong performance, particularly in WebQ, where it improves Top-1 accu- racy from 18.80% (BM25) to 38.77%. Among lighter-weight models, FlashRank (MiniLM and Tiny- BERT) and MonoBERT-large provide solid improvements while maintaining efficiency. MiniLM-L-12-v2 achieves 41.02% Top-1 ac- curacy on NQ-Test and 37.05% on WebQ, making it a strong candi- date for deployment scenarios where computational efficiency is critical. The MonoBERT-large model reaches 39.05% on NQ-Test and 34.99% on WebQ, reinforcing the effectiveness of BERT-based cross-encoders for document ranking. Models that integrate list- wise and contrastive ranking techniques, such as Twolar-xl and LiT5, show competitive performance. Twolar-xl achieves 46.84% Top-1 accuracy on NQ-Test and 41.68% on WebQ, while LiT5-Distill- xl reaches 47.81% on NQ-Test and 42.37% on WebQ, demonstrating that fine-tuned T5 models can significantly improve ranking quality. The sentence-transformer rerankers, particularly GTR-xxl, achieve 42.93% and 39.41% Top-1 accuracy on NQ-Test and WebQ, respec- tively, making them effective for diverse retrieval tasks. 4.5 Generator Results In this section, we present the performance of Rankifyâ€™s generator module when integrated with Retrieval-Augmented Language Mod- eling (RALM) [77]. We evaluate the effectiveness of various retrieval models, including BM25, Contriever, and DPR, across three open- domain QA datasets: NQ, TriviaQA, and WebQ. The experiments utilize multiple LLMs: LLaMA V3 8B, LLaMA V3.1 8B, Gemma 2B/9B, LLaMA 2 13B, and Mistral 7B to assess the robustness of the retrieval