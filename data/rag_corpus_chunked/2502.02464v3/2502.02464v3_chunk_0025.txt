Contriever on these datasets. Retrievers NQ TriviaQA WebQ DPR [109] 78.4 / 85.4 79.4 / 85.0 73.2 / 81.4 DPR_Pyserini [52] 79.5 / 86.8 79.7 / 85.1 75.1 / 82.9 DPR_Rankify 79.5 / 86.8 79.7 / 85.1 75.1 / 82.9 BM25 [82] 63.0 / 78.2 76.4 / 83.1 62.3 / 75.5 BM25_Pyserini [52] 64.8 / 79.7 77.3 / 83.8 63.3 / 76.4 BM25_Rankify 64.8 / 79.7 77.3 / 83.8 63.3 / 76.4 Contriever [37] 79.6 / 88.0 80.4 / 85.7 75.9 / 83.7 Contriever_Rankify 79.6 / 88.0 80.3 / 85.7 75.9 / 83.7 ColBert [83] 82.1 / 88.5 82.2 / 86.4 76.6 / 84.5 ColBert_Rankify 82.1 / 88.5 82.2 / 86.4 76.6 / 84.5 ANCE [104] 82.1 / 87.9 80.3 / 85.2 - / - ANCE_Rankify 82.5 / 88.5 80.0 / 85.2 76.6 / 84.25 4.2 Retrieval Results The retrieval performance of BM25 across multiple datasets is shown in Table 3. The results report Top-1, Top-5, Top-10, Top- 20, Top-50, and Top-100 retrieval accuracy for each dataset split. Performance varies significantly based on dataset characteristics, with high Top-k accuracy achieved on open-domain QA datasets such as TriviaQA and Natural Questions. On the other hand, more complex multi-hop datasets like 2WikiMultiHopQA and HotpotQA are characterized by lower retrieval rates, reflecting the challenges in retrieving supporting evidence across multiple documents. Tem- poral datasets like ArchivalQA and ChroniclingAmericaQA exhibit moderate retrieval performance, likely due to the temporal nature of their content. It is important to note that some datasets in Table 3 were either not officially recorded in previous studies or rely on different underlying corpora, making direct comparison difficult. Additionally, some datasets (e.g., ChroniclingAmericaQA) do not report retrieval accuracy as a standard Top-k metric, further compli- cating direct validation. Despite these variations, our results shown in Table 3 provide a comprehensive overview