[97]Train 6.23 13.70 18.54 24.39 33.77 40.81Test 19.54 42.86 53.44 63.25 72.34 76.43 Val 7.36 15.47 21.39 25.98 34.59 41.29 T-REx [23, 68] Val 45.48 64.34 70.60 76.04 80.42 82.98 TruthfulQA [53] Val 2.33 7.59 12.12 20.69 38.19 56.55 SIQA [84] Train 0.34 1.52 2.78 4.85 8.92 13.43 HotpotQA [108]Train 34.89 51.71 58.13 64.09 71.11 75.49Val 0.61 2.05 3.58 6.55 11.31 16.58 Val 28.13 45.02 51.92 57.79 65.44 70.67 StrategyQA [28] Train 4.19 15.72 26.29 39.87 52.88 57.86 PopQA [59] Test 25.00 38.77 44.70 49.74 56.82 62.36 WOW [22, 68] Train 0.26 0.38 0.41 0.45 0.49 0.52 ZSRC [50] Train 50.53 67.23 72.06 76.10 80.35 82.99Val 0.20 0.33 0.36 0.39 0.46 0.46 Val 52.26 70.86 76.29 81.44 85.39 87.73 Bamboogle [72] Test 5.60 12.80 17.60 24.80 39.20 44.00 EntityQuestions [85] Test 43.36 60.50 66.08 70.61 75.41 79.06 7 print ( " After Ranking : " , after_ranking ) Listing 8: Computing Top-k accuracy for retrieval and re- ranking in Rankify. For QA and RAG, Rankify supports Exact Match (EM), recall, precision, and containment to evaluate generated answers. Listing 9 shows how these metrics are computed. 1 from rankify . metrics . metrics import Metrics 2 qa_metrics = Metrics ( documents ) 3 qa_results = qa_metrics . c a l c u l a t e _ g e n e r a t i o n _ m e t r i c s ( ge ne ra te d_a ns we rs ) 4 print ( qa_results ) Listing 9: Computing QA and RAG evaluation metrics in Rankify. By providing a unified evaluation module, Rankify ensures con- sistent benchmarking across retrieval, re-ranking, and RAG tasks. 4 Experimental Result and Discussion 4.1 Experiment Setup Rankify enables researchers to benchmark retrieval, re-ranking, and RAG methods, evaluate their own approaches, and explore