Model LLama V3 8B LLama V3.1 8B Gemma-2-2b Gemma-2-9b Llama-2-13b-hf Mistral-7B-v0.1 BM25 Contriever DPR Retriever 42.10 36.25 45.88 40.13 35.29 43.62 43.28 33.05 33.05 57.55 50.93 50.93 57.90 42.69 51.07 52.85 42.69 42.69 TriviaQA - EM Comparison Language Model LLama V3 8B LLama V3.1 8B Gemma-2-2b Gemma-2-9b Llama-2-13b-hf Mistral-7B-v0.1 BM25 Contriever DPR Retriever 10.23 10.67 19.83 9.25 9.35 14.32 14.71 14.71 14.71 14.96 14.96 14.96 19.54 19.98 19.83 6.40 6.40 6.40 WebQ - EM Comparison Language Model LLama V3 8B LLama V3.1 8B Gemma-2-2b Gemma-2-9b Llama-2-13b-hf Mistral-7B-v0.1 Figure 4: Exact Match (EM) for BM25, Contriever, and DPR retrievers across three datasets (NQ, TriviaQA, WebQ) using various language models (LLaMA V3/V3.1, Gemma 2B/9B, LLaMA 2 13B, and Mistral 7B). results closely match the original implementations, demonstrating the correctness of our approach. Additionally, we observe improvements in the performance of BM25 and DPR compared to their original baselines. These improve- ments can be attributed to the optimizations present in Pyserini, such as better indexing techniques and parameter tuning. For Col- BERT and Contriever, we replicate the results reported in their respective papers by running their official GitHub implementations on our datasets, yielding identical outcomes. 4.4 Re-Ranking Results The performance of various re-ranking methods applied to BM25- retrieved documents is presented in Table 6. The table reports Top-1, Top-10, and Top-50 accuracy for the NQ-Test and WebQ datasets. The baseline BM25 retrieval scores serve as a reference, demonstrat- ing the extent to which re-ranking improves document ranking quality across different models. Across all methods, re-ranking consistently improves retrieval effectiveness, with notable gains observed in Top-1 accuracy. Transformer-based models such as UPR (T5-based), RankGPT, FlashRank (MiniLM and TinyBERT), and RankT5 achieve substantial improvements over the BM25 base- line. In particular, RankT5-3B outperforms other models, achiev- ing a Top-1 accuracy of 47.17% on NQ-Test and