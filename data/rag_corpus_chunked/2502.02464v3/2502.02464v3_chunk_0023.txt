ns we rs ) 4 print ( qa_results ) Listing 9: Computing QA and RAG evaluation metrics in Rankify. By providing a unified evaluation module, Rankify ensures con- sistent benchmarking across retrieval, re-ranking, and RAG tasks. 4 Experimental Result and Discussion 4.1 Experiment Setup Rankify enables researchers to benchmark retrieval, re-ranking, and RAG methods, evaluate their own approaches, and explore optimizations within these tasks. To demonstrate its capabilities, we conducted multiple experiments to provide reproducible bench- marks and performance insights. All main experiments were con- ducted on 2x NVIDIA A100 GPUs. Experiments were performed on a diverse set of datasets, cov- ering open-domain and multi-hop QA, fact verification, and tem- poral retrieval tasks. All the experiments in our study use prepro- cessed English Wikipedia dump from December 2018 as released by [109] as evidence passages. Each Wikipedia article is split into non-overlapping 100-word passages, with over 21 million passages in total. We evaluated retrieval performance on datasets such as Natu- ral Questions (NQ), TriviaQA, HotpotQA, 2WikiMultiHopQA, and ArchivalQA, while re-ranking performance was analyzed on MS- MARCO, WebQuestions, and PopQA. For retrieval evaluation, we measured Top-k accuracy at k=1, 5, 10, 20, 50, 100. Exact match (EM), Precision, Recall, Contains, and F1 were used as the primary evaluation metrics for QA tasks. We conducted experiments across all supported retrieval, re- ranking, and RAG methods. Retrieval models BM25, DPR, ANCE, BGE, and ColBERT. Re-ranking methods tested included MonoT5, RankT5, RankGPT, and various transformer-based re-rankers. For RAG, we evaluated zero-shot generation, Fusion-in-Decoder (FiD), in-context learning (RALM). These experiments highlight the adapt- ability of Rankify for benchmarking retrieval, ranking, and knowledge- grounded text generation. Table 4: Comparison of different retrieval models on NQ, WebQ, and TriviaQA. The table reports Top-1 to Top-100 accuracy for each retriever. Retriever Dataset Top-1 Top-5 Top-10 Top-20 Top-50 Top-100