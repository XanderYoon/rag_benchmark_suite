transformer-based re-rankers. For RAG, we evaluated zero-shot generation, Fusion-in-Decoder (FiD), in-context learning (RALM). These experiments highlight the adapt- ability of Rankify for benchmarking retrieval, ranking, and knowledge- grounded text generation. Table 4: Comparison of different retrieval models on NQ, WebQ, and TriviaQA. The table reports Top-1 to Top-100 accuracy for each retriever. Retriever Dataset Top-1 Top-5 Top-10 Top-20 Top-50 Top-100 Contriever [36] NQ 38.81 65.65 73.91 79.56 84.88 88.01 WebQ 35.97 63.83 69.78 75.94 81.64 83.76 TriviaQA 49.85 71.39 76.68 80.26 83.85 85.72 DPR [109] NQ 44.57 67.76 74.52 79.50 84.40 86.81 WebQ 44.64 63.98 70.52 75.05 80.51 82.97 TriviaQA 57.47 72.40 76.50 79.76 82.96 85.09 ColBert [83] NQ 42.99 68.78 76.12 82.08 86.15 88.59 WebQ 40.11 64.81 71.56 76.57 81.20 84.50 TriviaQA 57.36 75.90 79.49 82.18 84.74 86.41 Ance [104] NQ 50.80 71.86 78.12 82.52 86.23 88.28 WebQ 46.51 66.24 71.80 76.62 81.79 84.25 TriviaQA 56.46 72.14 76.65 80.04 83.37 85.22 BGE [13, 102] NQ 48.03 72.22 78.50 82.66 86.90 89.45 WebQ 42.67 65.45 72.54 79.04 82.78 85.88 TriviaQA 57.81 75.39 79.83 82.92 85.53 86.94 SIGIR ’25, July 13–18, 2025, Padova, IT Abdallah et al. Table 5: Top-20/Top-100 retrieval accuracy of different re- trievers on three open-domain QA datasets (NQ, TriviaQA, and WebQ). Our Rankify implementation achieves results identical to Pyserini across all retrievers, as we use Pyserini as the indexing backend. This validates the correctness of our implementation. We also match the official GitHub results for ColBERT and Contriever on these datasets. Retrievers NQ TriviaQA WebQ DPR [109] 78.4 / 85.4 79.4 / 85.0 73.2 / 81.4 DPR_Pyserini [52] 79.5 / 86.8 79.7 / 85.1 75.1 / 82.9 DPR_Rankify 79.5 / 86.8 79.7 / 85.1 75.1 / 82.9 BM25 [82] 63.0 / 78.2 76.4 / 83.1 62.3 / 75.5 BM25_Pyserini [52] 64.8 / 79.7 77.3 / 83.8 63.3