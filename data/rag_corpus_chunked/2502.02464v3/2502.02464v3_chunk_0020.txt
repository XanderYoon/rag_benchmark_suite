of downstream applications like question answering. Listing 8 demonstrates how to compute Top-k accuracy. 1 from rankify . metrics . metrics import Metrics 2 metrics = Metrics ( documents ) 3 # Compute Top - k accuracy before and after re - ranking 4 before_ranking = metrics . c a l c u l a t e _ r e t r i e v a l _ m e t r i c s ( ks =[1 ,5 ,10 ,20 ,50 ,100] , use_reordered = False ) 5 after_ranking = metrics . c a l c u l a t e _ r e t r i e v a l _ m e t r i c s ( ks =[1 ,5 ,10 ,20 ,50 ,100] , use_reordered = True ) 6 print ( " Before Ranking : " , before_ranking ) Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and RAG SIGIR ’25, July 13–18, 2025, Padova, IT Table 3: Retrieval performance of BM25 across several selected datasets. The table reports Top-1, Top-5, Top-10, Top-20, Top-50, and Top-100 retrieval accuracy for each dataset split. Datasets vary in complexity, including open-domain QA (e.g., TriviaQA, NQ), multi-hop reasoning (e.g., 2WikiMultiHopQA, HotpotQA), fact verification (e.g., TruthfulQA), and temporal retrieval (e.g., ArchivalQA, ChroniclingAmericaQA). Dataset Split Top-1 (%) Top-5 (%) Top-10 (%) Top-20 (%) Top-50 (%) Top-100 (%)Dataset Split Top-1 (%) Top-5 (%) Top-10 (%) Top-20 (%) Top-50 (%) Top-100 (%) 2WikiMultiHopQA [32]Train 8.85 19.78 28.15 38.16 51.16 57.70 ArchivalQA [99]Val 18.18 33.29 39.72 45.96 53.68 58.74Val 17.17 31.72 40.14 47.73 55.88 61.17 Test 17.68 32.55 39.26 45.36 52.76 58.08 ChroniclingAmericaQA [69]Val 4.26 11.80 16.42 21.50 28.19 33.15 AmbigQA [47, 61]Train 27.30 50.67 59.63 67.63 75.68 79.87Test 4.18 11.23 15.83 20.87 27.49 32.48 Val 30.22 54.25 65.58 72.98 80.47 84.92 ARC [18]