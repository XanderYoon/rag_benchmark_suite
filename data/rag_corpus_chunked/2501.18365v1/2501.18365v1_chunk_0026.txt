input documents. This smoother attention dis- tribution helps in two ways. First, it increases the model’s resistance against incorrect or irrelevant information by reducing excessive attention to and reliance on such content. Second, even when the input document does not directly contain the ground-truth answer, attending to more relevant information in the overall context may better activate the internal parametric knowledge and memory of the LLM, thereby facilitating more accurate responses. RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects Conference acronym ’XX, June 03–05, 2018, Woodstock, NY (a) The attention distribution over two input noisy documents of Vanilla RAG and RbFT. (b) The attention distribution over two input irrelevant documents of Vanilla RAG and RbFT. (c) The attention distribution over two input counterfactual documents of Vanilla RAG and RbFT. Figure 5: Case studies on the attention distribution over input documents of Vanilla RAG and RbFT under different retrieval defects. The greener a document token, the higher the attention it receives during the answer generation process. 6.4 Efficiency Analysis In Table 3, we assess the time efficiency of different methods during inference, reporting the average time required by each RAG system to process a single user query. It can be observed that RbFT, by only fine-tuning the LLMs, maintains an inference speed comparable to Vanilla RAG. In contrast, other robustness-oriented methods, except for InstructRAG, adopt more complex inference mechanisms that require multiple generation steps or rounds, leading to significantly higher time costs than Vanilla RAG and RbFT. This demonstrates that RbFT not only excels in performance but also offers a notable advantage in efficiency over other baseline models. On the other hand, RbFT is vertical to these methods and can be integrated with them to further enhance system robustness. Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Yiteng Tu, Weihang