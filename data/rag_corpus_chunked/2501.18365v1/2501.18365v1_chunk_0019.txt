improvements in both metrics mainly ranging between 15% and 20%. (3) In the Hard setting, RbFT continues to outperform all other methods and further widens the gap with the second- best approach, highlighting its exceptional performance and adapt- ability in extremely adverse retrieval environments. Traditional methods often exhibit instability when handling these complex issues in such high-difficulty scenarios, where the retrieval results consist entirely of defective documents. However, RbFT consis- tently maintains high performance, particularly in the Counter- factual scenario, where using the Llama model results in an EM metric improvement of over 70%. This significant advantage further validates RbFTâ€™s robustness and reliability in dealing with various complex retrieval scenarios. (4) RbFT demonstrates significant advantages in balancing effectiveness and robustness. We refer to the EM score under the Clean setting as a methodâ€™s organic capability in QA tasks (i.e., effectiveness), and refer to the EM score under the Hard+ Mix de- fective setting as the methodâ€™s ability to handle complex retrieval defects (i.e., robustness). Based on these two metrics, we plot the RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Table 1: The average evaluation results of each model on the three datasets under the Clean ( ğœ = 0), Normal ( ğœ = 0.4), and Hard (ğœ = 1.0) settings. "*" refers to a significant improvement compared to the Vanilla RAG baseline at ğ‘ < 0.05 level using the two-tailed pairwise t-test. The best and second-best methods are marked in bold and underlined, respectively. The improvement ratio of the best model over the second-best model is also reported. LLM Method Clean(ğœ =0) Normal(ğœ =0.4) Hard(ğœ =1.0) Noisy Irrelevant Counterfactual Mix Noisy Irrelevant Counterfactual Mix EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM F1