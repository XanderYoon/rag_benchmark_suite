significant advantages in knowledge-intensive tasks [2, 9, 14, 18, 20, 24, 39, 46, 47]. Traditional RAG typically fol- lows the "Retrieval-then-Read" framework [ 2, 14, 19, 24], where an external retriever [28, 34, 37, 38, 57] or a complex retrieval sys- tem [35, 40] is adopted to search for relevant documents from a large-scale external corpus based on the user’s query. The retrieved documents provide external knowledge that supplements the query, allowing the generative model to incorporate relevant information beyond its parametric knowledge when generating a response. To further enhance the retrieval effectiveness, researchers have intro- duced additional techniques such as query rewriting [ 8, 27] and re-ranking [1, 13] to refine the quality of retrieved documents before appending them to the generative model. Building upon the traditional RAG framework, various exten- sions have been proposed to enhance its effectiveness and efficiency. One such extension, Parametric RAG [43], directly injects the re- trieved documents into LLM parameters by offline parameterizing each document into independent plug-in parameters. During the inference process, the retrieved document’s parametric representa- tion is merged and integrated into the LLM, enabling knowledge injection without extending the input context. From another angle, GraphRAG [12, 16, 32] leverages pre-constructed knowledge graphs to retrieve graph elements with relational knowledge relevant to a given query. This approach has shown improved performance, es- pecially in tasks that rely on structured and relational information. Another research direction, dynamic RAG [20, 41, 42], dynamically triggers the retrieval module during the generation process when the LLM exhibits high uncertainty during the generation process. In contrast to existing works that focus on improving retrieval quality, refining retrieval pipelines (e.g., through query rewriting or re-ranking), or reorganizing knowledge representation, we propose a fundamentally different perspective: directly enabling the LLM itself to handle imperfect or even malicious retrieval