of producing the correct output text ğ‘¦: max Î˜ âˆ‘ï¸ (ğ‘¥,ğ‘¦ ) |ğ‘¦ |âˆ‘ï¸ ğ‘–=1 log  ğ‘Î¦0+Î”Î¦(Î˜) (ğ‘¦ğ‘– |ğ‘¦<ğ‘–, ğ‘¥)  (3) where Î¦0 and Î”Î¦(Î˜) denote the LLMâ€™s original parameters and the learned parameter adjustments during fine-tuning, respectively. 5 Experimental Settings 5.1 Datasets and Evaluation Metrics We conduct experiments on three widely used Question Answering (QA) datasets: Natural Questions (NQ) [23], HotpotQA (HQA) [56], and TriviaQA (TQA) [21], which cover both factoid QA and multi- hop QA tasks. Each data instance consists of a query and its corre- sponding ground truth answer. To create the training and validation sets, we randomly sample a total of 20,000 instances from the train- ing splits of these three datasets, where 10% of the training data is reserved for validation. For evaluation, to ensure efficient exper- iments, following [48, 53], we sample 1,000 queries from the test sets of NQ and TQA, as well as the HQA validation set (since HQA does not provide the test set), respectively (i.e., a total of 3,000 test queries). The e5-base-v2 [49] retriever is adopted to retrieve the top 100 most relevant documents for each query from the Wikipedia corpus 1. To assess the performance of different RAG systems un- der varying levels of retrieval defects, we employ the standard QA evaluation metrics: exact match (EM) and token-level F1 score (F1), which measure the precision of the generated answers. 5.2 Baselines Our RbFT is primarily compared with No RAG, Vanilla RAG, as well as four state-of-the-art robustness approaches for the RAG system: RobustRAG [53], CRAG [54], InstructRAG [51] and AstuteRAG [48]. RobustRAG leverages an "isolate-then-aggregate" strategy, where the LLM independently generates responses for each retrieved pas- sage and then aggregates these individual responses to produce the final output. CRAG introduces a lightweight retrieval evaluator that triggers