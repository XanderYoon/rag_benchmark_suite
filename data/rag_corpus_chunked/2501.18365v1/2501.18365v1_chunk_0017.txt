and concise, maintaining a language style similar to the original. Only give me the rewritten passage and do not output any other words. Original Document: { document } 5.4 Implementation Details We fine-tune two LLMs on the RbFT task, Llama-3.2-3B-Instruct [11] and Qwen2.5-3B-Instruct [55], to enhance their robustness against retrieval defects through the LLaMA-Factory toolkit 2. In the fol- lowing text, we refer to these two LLMs as Llama and Qwen for convenience. The fine-tuning is conducted for 2 epochs, with a learning rate of 1e-5, and a per-device batch size of 16, setting ğ‘™ğ‘œğ‘Ÿğ‘ _ğ‘Ÿğ‘ğ‘›ğ‘˜ = 16 and ğ‘™ğ‘œğ‘Ÿğ‘ _ğ‘ğ‘™ğ‘â„ğ‘ = 64. LLMs are fine-tuned on four types of defective data: Noisy, Irrelevant, Counterfactual, and Mix (randomly selected from the first three types), with the prob- ability of replacing original retrieval results with defective docu- ments (ğœ) selected from {0.2, 0.4, 0.6, 0.8, 1.0}. During the evaluation phase, the same ğœ values are used, with particular attention given to ğœ = 0.4 and ğœ = 1.0, referred to as the Normal and Hard settings, respectively, representing moderate and severe retrieval defects. Additionally, to compare the original performance, we also report their results with the original retrieval results (ğœ = 0), referred to as the Clean setting. The retrieval list size ğ‘˜ is set to 5. Our code and data are available at the URL 3. 6 Results and Analysis 6.1 Main Results Table 1 shows the performance of all methods under different levels of retrieval defects. We observe that: (1) In the Clean setting, RbFT is the only method that sur- passes Vanilla RAG. Unlike other approaches that experience performance degradation in defect-free environments, RbFT can 2https://github.com/hiyouga/LLaMA-Factory 3https://github.com/StibiumT16/Robust-Fine-tuning Figure 3: The effectiveness-robustness trade-off scatter dia- gram. The x-axis represents effectiveness measured by the EM scores of each model in