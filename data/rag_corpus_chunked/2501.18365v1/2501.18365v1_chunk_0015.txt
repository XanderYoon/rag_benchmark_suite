with No RAG, Vanilla RAG, as well as four state-of-the-art robustness approaches for the RAG system: RobustRAG [53], CRAG [54], InstructRAG [51] and AstuteRAG [48]. RobustRAG leverages an "isolate-then-aggregate" strategy, where the LLM independently generates responses for each retrieved pas- sage and then aggregates these individual responses to produce the final output. CRAG introduces a lightweight retrieval evaluator that triggers different knowledge retrieval actions based on evalu- ation results and enables knowledge refinement. To ensure a fair comparison of different RAG systems, we disable the module in CRAG that is responsible for large-scale web searches to acquire additional knowledge. InstructRAG instructs LLMs to explicitly denoise retrieved content by generating self-synthesized explana- tory rationales, which explain how the answer is derived from the retrieved documents. AstuteRAG, on the other hand, focuses on resolving conflicts between the internal knowledge of the LLM and the external knowledge provided by the retriever. It achieves this goal through an "iterative source-aware knowledge consolidation" process that integrates the two kinds of knowledge and handles knowledge conflicts. 5.3 Data Generation We simulate three types of defective documents using different ap- proaches. For noisy and irrelevant documents, inspired by the neg- ative sampling methods commonly used in training dense retrieval models [22, 33, 58], these two types of defective documents can be analogized to hard negatives and random negatives, respectively. Accordingly, noisy documents can be obtained by randomly sam- pling from lower-ranked retrieval results (e.g., documents ranked after 50 in the retrieval results), while irrelevant documents can be randomly sampled from the entire corpus. For counterfactual documents, we adopt a two-step generation strategy: first, given the query, the correct answer, and the original retrieval results, we use Llama-3.2-3B-Instruct [11] to generate a misleading incorrect answer. Then, we call the LLM again to rewrite all original docu- ments by