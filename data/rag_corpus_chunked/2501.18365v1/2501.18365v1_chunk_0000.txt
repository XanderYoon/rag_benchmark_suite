RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects Yiteng Tu DCST, Tsinghua University Beijing, China yitengtu16@gmail.com Weihang Su DCST, Tsinghua University Beijing, China swh22@mails.tsinghua.edu.cn Yujia Zhou DCST, Tsinghua University Beijing, China Yiqun Liu DCST, Tsinghua University Beijing, China Qingyao Ai∗ DCST, Tsinghua University Beijing, China aiqy@tsinghua.edu.cn Abstract Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally con- strained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading coun- terfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against re- trieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robust- ness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques. CCS Concepts •Information systems → Information retrieval; •Computing methodologies → Natural language generation. Keywords Retrieval-augmented Generation, Fine-tuning, Robust 1 Introduction Large Language Models (LLMs) have achieved exceptional perfor- mance across diverse natural language processing tasks [ 29, 50], yet they remain constrained by challenges such as hallucinations, outdated or incomplete knowledge, and limited adaptability to spe- cialized domains [19, 24]. Retrieval-augmented generation (RAG) has emerged as a key technique to address these limitations by integrating LLMs with external knowledge sources, enabling en- hanced factual accuracy, up-to-date information access, and im- proved domain-specific performance [17, 26, 44]. Due to its effec- tiveness, RAG has been widely adopted to provide LLMs with a flexible mechanism for knowledge augmentation, enhancing their performance in varying scenarios [5, 14]. Despite their popularity, existing RAG systems suffer from