answer the question. Doc 2 ...... RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY 4.2 Task II: Utility Extraction In the Utility Extraction task, we aim to train the LLM to extract as much useful information as possible from the defective retrieval result. The LLM can either directly utilize the extracted relevant information or leverage the relevant context to activate its internal parametric knowledge to generate the correct answer. Meanwhile, Utility Extraction training also enables the LLM to directly and efficiently handle low-quality or contaminated contexts without prior cleanup. Similarly, the original documents are replaced with defective documents at a probability ğœ, and the LLM is required to answer based on these defective retrieval results while producing correct outputs. The prompt used for this task is as follows: Input: Answer the question based on the given document. Only give me the answer and do not output any other words. The following are given documents. Doc 1: { document 1 } Doc 2: { document 2 } ...... Question: { question } Output: { answer } 4.3 Training Objective RbFT aims to directly fine-tune the LLM using the two afore- mentioned tasks, thereby enhancing its organic defense capability. To achieve efficient training while preserving the LLMâ€™s general- purpose capabilities, we adopt the Low-Rank Adaptation (LoRA) [15] technique for fine-tuning. Specifically, given the input text ğ‘¥, our goal is to maximize the probability of producing the correct output text ğ‘¦: max Î˜ âˆ‘ï¸ (ğ‘¥,ğ‘¦ ) |ğ‘¦ |âˆ‘ï¸ ğ‘–=1 log  ğ‘Î¦0+Î”Î¦(Î˜) (ğ‘¦ğ‘– |ğ‘¦<ğ‘–, ğ‘¥)  (3) where Î¦0 and Î”Î¦(Î˜) denote the LLMâ€™s original parameters and the learned parameter adjustments during fine-tuning, respectively. 5 Experimental Settings 5.1 Datasets and Evaluation Metrics We conduct experiments on three widely used Question Answering (QA) datasets: Natural