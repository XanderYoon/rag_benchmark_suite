triggers the retrieval module during the generation process when the LLM exhibits high uncertainty during the generation process. In contrast to existing works that focus on improving retrieval quality, refining retrieval pipelines (e.g., through query rewriting or re-ranking), or reorganizing knowledge representation, we propose a fundamentally different perspective: directly enabling the LLM itself to handle imperfect or even malicious retrieval results. Instead of assuming perfectly relevant or pre-filtered documents, we train the model to detect flaws in retrieved texts and extract only useful evidence, mitigating the impact of noisy, irrelevant, or incomplete information. This shift not only enhances accuracy but also fosters a more resilient and trustworthy RAG framework. 2.2 Robustness in RAG The robustness of RAG systems refers to the ability of LLMs to con- sistently extract and apply relevant knowledge, even when exposed to varying or defective retrieval inputs [60]. Existing works have found that misinformation and corruption retrieval inputs pose sig- nificant challenges to the robustness of RAG systems. Adversarial Addition and Modification [10] demonstrates the vulnerability of automated fact-checking systems when confronted with synthetic adversarial evidence. Pan et al. [30] and Pan et al. [31] explore the threat posed by misinformation (whether manually crafted or gen- erated by LLMs) to open-domain question-answering (ODQA) sys- tems, highlighting the vulnerability of these systems when exposed to misinformation corruption. By injecting malicious texts into the knowledge base, PoisonedRAG [61], GARAG [6] and Phantom [3] can manipulate LLMs into generating specific incorrect or harm- ful responses. To address these vulnerabilities, researchers have proposed strategies focusing on input optimization and knowledge integration. Weller et al. [52] conducts query augmentation and introduces a novel confidence method based on answer redundancy. RobustRAG [53] employs an isolate-then-aggregate strategy to en- sure the robustness of LLM responses against retrieval corruption attacks. By generating self-synthesized rationales,