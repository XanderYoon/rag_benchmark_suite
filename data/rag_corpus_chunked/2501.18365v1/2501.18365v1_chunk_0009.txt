of false or misleading information, often resulting from inaccu- racies or malicious manipulation within online content. They fail to answer the question and may even lead to misconceptions. For example, while the correct answer to the query is "The Dark Side of the Moon", a counterfactual document might falsely claim that the song "Time" appears on another album "Wish You Were Here" (Doc 4 in Figure 1). Such incorrect information undermines the reliability of the retrieval process and can mislead both users and LLMs. To simulate different levels of retrieval defects, we use varying probabilities ğœ to randomly replace the original retrieved documents with defective documents of different or identical types. Let the Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai Figure 2: Emperical study: the impact of different types of retrieval defects on Vanilla RAG. The average EM metric on NQ, HQA, and TQA datasets is reported. modified retrieval result be denoted as Dğ‘ ğœ , our goal is for the LLM to generate the correct answer ğ‘ even when provided with Dğ‘ ğœ . To validate the negative impact of these retrieval defects on RAG systems, we conduct preliminary experiments on three datasets: Natural Questions [23], HotpotQA [56], and TriviaQA [21], and report the average results. We select e5-base-v2 [ 49] as the re- triever, returning the top-5 retrieved results, while LLama-3.2-3B- Instruct [11] and Qwen2.5-3B-Instruct [55] are adopted to generate the final answers, which are measured using the exact match (EM) metric. The tests are conducted using the aforementioned three types of retrieval defects and their mixture (i.e., randomly selecting one type of defective document for replacement). The defect replace- ment probability ğœ is set to {0, 0.2, 0.4, 0.6, 0.8, 1.0}. As observed in Figure 2,