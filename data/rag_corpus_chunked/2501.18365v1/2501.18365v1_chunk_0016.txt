50 in the retrieval results), while irrelevant documents can be randomly sampled from the entire corpus. For counterfactual documents, we adopt a two-step generation strategy: first, given the query, the correct answer, and the original retrieval results, we use Llama-3.2-3B-Instruct [11] to generate a misleading incorrect answer. Then, we call the LLM again to rewrite all original docu- ments by replacing all information related to the correct answer with the misleading incorrect answer: 1https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz Conference acronym ’XX, June 03–05, 2018, Woodstock, NY Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai Step 1 Input: Based on a given question and its correct answer, generate a misleading wrong answer. You can refer to some relevant documents for inspiration. The wrong answer should belong to the same entity type as the correct answer (e.g., person, time, place, organization, data, etc.) to enhance its confusion. If the answer does not contain an entity, replace a key entity in the question and treat it as the wrong answer. Only give me the wrong answer and do not output any other words. The following are given documents. Doc 1: { document 1 } Doc 2: { document 2 } ...... Question: { question } Correct Answer: { answer } Step 2 Input: You are a writing AI. Rewrite the passage by replacing all content and information related to { correct answer } with { wrong answer }. Ensure that the rewritten passage is fluent and concise, maintaining a language style similar to the original. Only give me the rewritten passage and do not output any other words. Original Document: { document } 5.4 Implementation Details We fine-tune two LLMs on the RbFT task, Llama-3.2-3B-Instruct [11] and Qwen2.5-3B-Instruct [55], to enhance their robustness against retrieval defects through the LLaMA-Factory toolkit 2. In the fol- lowing