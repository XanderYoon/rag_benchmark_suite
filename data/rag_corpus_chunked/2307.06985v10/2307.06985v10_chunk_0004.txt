whole), social (e.g., business, family), socio-physical (e.g., ownership), affiliations etc. NLP scholars and practitioners have proposed several generic algorithms for entity and relation extraction. Conventional relation extraction algorithms stem from syntactic dependencies between a pair of tokens or phrases, also denoted as subject-object pairs. For example, Sun and Grishman (2022) learn relations between entities using the lexical dependency paths. Due to the efficacy of Language Models, recent algorithms adopt these for encoding entities and relationships. Zhong and Chen (2021) fine-tune BERT for classifying spans into entity categories. Given a pair of spans, they include start and end markers with entity category and fine-tune BERT for predicting relation classes. As a subject can be related to many objects, Ye et al. (2022) mark all objects tied to a subject and fine-tune BERT, ALBERT, and SciBERT for relation prediction. Liu et al. (2021) capture relationships among entities that are placed in different sentences in a document. They gather BERT-based token embeddings of entities and couple these using CNNs to predict relationships. Geng et al. (2023) map a sentence to a 2D representation and train Bi-TDNN to learn dependencies between entities (span of tokens). While sophisticated algorithms are continuously being proposed, stable ones are also being made available in NLP toolkits such as spaCy13, TextRazor14 that enable direct usage of entity and relation extraction models. 1.3. Engineering Design Knowledge. Engineering design literature reports several NLP applications and ontologies (Siddharth et al., 2022a) for entity and relation extraction. Trappey et al. (2014) use TF-IDF values to extract keyphrases from patent documents and research articles concerning dental implants. They (2014, p. 158) build a domain ontology by linking these using relationships such as “type of”, “consist of”, “the same as”, “has shape of” etc. Yang et al. (2018) compile 114,793 reports pertaining to the verification