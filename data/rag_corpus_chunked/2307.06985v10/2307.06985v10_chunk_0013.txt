Figure 2: Fine-tuning language models for A) Relation Identification and B) Relation Elicitation. In relation identification, as depicted in Figure 2A, we create training examples as tokens with labels (HEAD, REL, TAIL, and OTH) and fine-tune language models for token classification. o Since our entities are selected from noun phrases given by spaCy transformers36, we leverage the training module for the token tagger given by spaCy37. The module offers 3 choices of models as listed in Table 2 â€“ small, large, and transformer-based (uses roBERTa-base). o We also include the DistilBERT encoder that is commonly used for token classification tasks. Further, as listed in Table 2, we include BERT, ALBERT, and SciBERT that have been used in knowledge extraction algorithms (Ye et al., 2022; Zhong and Chen, 2021). In relation elicitation, as depicted in Figure 2B, we form training examples as marked sentences with relation outputs and fine-tine language models for a Seq2Seq task. The models that are suitable for this task are generally heavy and fall beyond the limits of our hardware accessibility. Among feasible ones, we include BART and T5 encoders as listed in Table 2. As mentioned earlier, we make the dataset4 and training infrastructure5 for these models accessible. Table 2: Performances of different encoders for the tasks of relation identification and elicitation. For all encoders, the dataset of 375,084 examples (187,200 positives, 187,884 negatives), we split training and testing sets by 9:1. The relation accuracy for an example is 1 if the model returns the exact relationship (if exists) or None (if not exists). Model Params (M) Model Loss Tagger Loss Rel. Accuracy Relation Identification (spaCy) spacy/en-core-web-sm - 17.186 1125.017 0.850 spacy/en-core-web-lg - 244.156 3815.050 0.895 spacy/en-core-web-trf - 193.181 2285.626 0.954 Encoder Params (M) Training Loss Validation Loss Rel. Accuracy distilbert/distilbert-base-uncased 67 0.004 0.004 0.997 Relation