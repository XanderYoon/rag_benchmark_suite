an example is 1 if the model returns the exact relationship (if exists) or None (if not exists). Model Params (M) Model Loss Tagger Loss Rel. Accuracy Relation Identification (spaCy) spacy/en-core-web-sm - 17.186 1125.017 0.850 spacy/en-core-web-lg - 244.156 3815.050 0.895 spacy/en-core-web-trf - 193.181 2285.626 0.954 Encoder Params (M) Training Loss Validation Loss Rel. Accuracy distilbert/distilbert-base-uncased 67 0.004 0.004 0.997 Relation Identification google-bert/bert-base-uncased 110 0.004 0.004 0.996 albert/albert-base-v2 11.8 0.003 0.003 0.995 albert/albert-large-v2 17.9 0.003 0.003 0.997 allenai/scibert-scivocab-uncased - 0.004 0.004 0.997 Relation Elicitation facebook/bart-base 139 0.076 0.067 0.948 google-t5/t5-small 60.5 0.144 0.0915 0.907 google-t5/t5-base 222 0.108 0.0743 0.891 As observed in Table 2, relation identification that adopts a token classification task offers significantly higher relation accuracy compared to the elicitation approach. Token classification task also constraints the model to identify relation from the sentence and makes it preferable for identifying explicit relations as intended. ALBERT (large) encoder offers the best performance despite having fewer parameters. Acknowledged as a limitation by Zhong and Chen (2021, p. 5), extracting knowledge by repeatedly inputting pairs of entities is a computational challenge. For this reason, we utilise the transformer model (en-core-web-trf) trained using spaCy for the application purposes in this paper, while making the fine- tuned version of the ALBERT large accessible6. The custom-trained spaCy components could be easily included in the patent processing pipeline, and these are optimized for scalability, while also accommodating GPU acceleration. In GitHub7, we facilitate the usage of the method by packaging a series of tasks from mining patent text to populating facts. 3.3. Alternative Approaches. Apart from relation identification and elicitation, we experimented link prediction approach (as depicted in Figure 3) for populating facts from a sentence. Prior knowledge extraction algorithms include link prediction as the approach to extracting relationships between entities (Sun and Grishman, 2022; Zhong and