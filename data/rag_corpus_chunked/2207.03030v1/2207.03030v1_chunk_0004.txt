returning a conﬁdence value, whether this passage is relevant or not. Only if the conﬁdence is higher than our set threshold t, do we include the passage in the set. From the view of a dataset creator, it is usually unfeasible to conduct annotations for all possible pairs, therefore, those pairings without annotations return a null conﬁdence for relatedness, even if it might be related. As dataset creation is a very costly operation, many works adapt and evolve existing datasets. When knowledge inten- sive datasets are evolved, the query-answer pair may stay the same while the conﬁdence values of the passage connec- tions change. Therefore, we hypothesise it is beneﬁcial not to include all possible training examples, rather only take into account query-answer pairs, where a higher conﬁdence threshold on the relevance label is set, to reduce noise. A low or no conﬁdence value might indicate a low quality query answer pair. 10 50 100 150 200 Word Count 0% 10% 20% 30% 40% 50% 60%Relative Occurence KILT Raw Orig-100 Passages Alt-200 Passages Figure 2. Statistics of the passage lengths of the raw KILT texts, its original chunking (Orig-100) and our alternative approach (Alt- 200). The word counts are binned to 10 words. Starting from the training set T , which includes all possible pairs (q, a), we deﬁne a ﬁltered versionˆTt as follows: ˆTt = { (q, a) ⏐⏐∃ p∈ P (q,a) t ,∀ (q, a)∈ T } (2) where we need to deﬁne a thresholdt as our sampling bound- ary. The boundary needs to be adapted to the properties of the applied task. In this work we apply our sampling approach on KILT which conducted a heuristic mapping process of passages for given query-answer pairs. We implement the conﬁdence mapping Φ as the BLEU score in their mapping