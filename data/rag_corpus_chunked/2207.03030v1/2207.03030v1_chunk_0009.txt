4. Results In this section we present and discuss our experimental results. An important note with every use of the KILT benchmark is that the numbers presented here are only com- parable to other works also based on the KILT benchmark and not the original versions of the individual tasks. This is due to the changed collection as well as changed query sets, as described by Petroni et al. (2021). The results on the KILT dev set are shown in Table 1. In the ﬁrst section (lines 1-4) we show related works, which also report KILT-based scores: RAG (Lewis et al., 2020), as evaluated by Petroni et al. (2021); DPR + FiD (Piktus et al., 2021); KGI (Glass et al., 2021); and Re2G (Anonymous, 2022). We present our results using the original passage units in the second (lines 5 & 6) and our alternative retrieval units in the third section (lines 7 & 8). In both sections we compare the random downsampling and our proposed relevance-label guided sampling strategy. Sampling strategies. First, we focus on the two strongly imbalanced tasks (TriviaQA and T-REx), which had their Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling Table 2. Comparing our models with related work on the KILT test set via the leaderboard (as of July 3rd 2022). Highest result in bold. Model Generator Open Domain QA Fact Slot Filling Dialog EM Acc. Accuracy F1 NQ HotpotQA TriviaQA FEVER T-REx zsRE WOW Top Leaderboard Entries 1 RAG (Petroni et al., 2021) BART-Large 44.4 27.0 71.3 86.3 59.2 44.7 13.1 2 DPR + FiD (Piktus et al., 2021) T5-Base 51.6 38.3 72.7 89.0 82.2 74.0 15.7 3 KGI (Glass et al., 2021) BART-Large 45.2 – 61.0 85.6 84.4 72.6 18.6 4 Re2G (Anonymous, 2022) BART-Large 51.7 – 76.3 89.6 87.7 – 18.9 5 Hindsight (Paranjape