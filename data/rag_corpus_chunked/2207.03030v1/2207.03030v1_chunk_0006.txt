combine and shufﬂe all tasks. By applying the relevance- label ﬁlter method ˆT we reduce our training set size by 25% (or 140K fewer examples) to 808K examples compared to our control. 1A T5-Base training run until convergence requires roughly one TPU month of compute with our downsample strategies. Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling Table 1. Comparing sampling strategies and model capacity scaling for multi-task training on the KILT dev set. Highest result in bold. Our results are averaged over the last 10 checkpoints with a 95% conﬁdence interval shown in gray. Model LM Open Domain QA Fact Slot Filling Dialog EM Accuracy Accuracy F1 NQ HotpotQA TriviaQA FEVER T-REx zsRE WOW Related Methods 1 RAG (Petroni et al., 2021) BART-L 44.4 27.0 71.3 86.3 59.2 44.7 13.1 2 DPR+FiD (Piktus et al., 2021) T5-Base 55.0 38.0 71.4 90.9 80.9 72.4 16.1 3 KGI (Glass et al., 2021) BART-L – – – – 84.0 71.3 – 4 Re2G (Anonymous, 2022) BART-L 46.7 – 74.0 91.1 86.6 – 19.4 Ours (DPR-100 passages) 5 GTR + FiD with control T T5-Base 54.1 ±.3 31.1 ±.2 65.0 ±.6 89.8 ±.2 78.0 ±.5 70.7 ±.4 19.8 ±.2 6 GTR + FiD with treatment ˆT T5-Base 54.4 ±.3 31.0 ±.2 78.1 ±.2 89.6 ±.4 82.9 ±.1 71.6 ±.3 19.5 ±.2 Ours (Alt-200 passages) 7 GTR + FiD with control T T5-Base 55.1 ±.3 31.6 ±.2 65.7 ±.6 89.8 ±.4 77.6 ±.3 70.2 ±.2 20.1 ±.2 8 GTR + FiD with treatment ˆT T5-Base 56.0 ±.3 31.8 ±.2 78.4 ±.2 89.6 ±.4 82.5 ±.1 71.4 ±.3 19.9 ±.2 9 T5-Large 60.7 ±.5 35.4 ±.2 81.8 ±.2 92.1 ±.2 82.9 ±.1 72.9 ±.4 19.9 ±.2 10 T5-XL 62.9 ±.4 39.0 ±.2 84.3 ±.2 92.8 ±.4 84.1 ±.2 75.2 ±.3 21.0 ±.3 Alternative retrievable units. The