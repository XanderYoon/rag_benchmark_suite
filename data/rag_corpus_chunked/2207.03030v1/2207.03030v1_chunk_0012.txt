ceiling on a total of ﬁve KILT tasks. We outperform the previous best methods by: NQ +7.4 EM, TriviaQA +6.4 EM, FEVER +2.7 Accuracy, ZS-RE +9 Ac- curacy, and WoW +0.05 F1. We only come in second place on HotpotQA (-1.4 EM) and T-REx (-2.5 Accuracy). This might be attributable to our handicapped zero shot retriever, as HotpotQA is challenging for retrieval models; and down- sampling of the T-REx training data, as the related methods are trained exclusively on the single task, without the need for training data adjustments. Overall, these results are a strong indicator for the viabil- ity and usefulness of our relevance-label sampling strategy considering that it has access to 140K fewer training exam- ples than the baseline. We want to emphasize that when we compare our already competitive results to related work our approach is handicapped in a few key areas: 1) we are not training the retriever (which is out of scope, but orthogonal to our work and should lead to further improve- ments); 2) we are training a single model, which gives us less chance to overﬁt on a single task;3) we do not employ multiple training loops, index updates, or knowledge dis- tillation. Therefore, we conclude that multi-task training is a viable option for the community to build upon going forward. Are we just gaming the benchmark? A valid concern we need to raise is whether we are really improving the quality of the model, or simply moving the training set construction closer to the way the tests sets have been con- 2The leaderboard is available at: https://eval.ai/web/challenges/challenge-page/689 Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling structed by Petroni et al. (2021). The KILT test sets ﬁlter an average 18% of queries compared to their original task versions. Petroni et al. (2021) removed a