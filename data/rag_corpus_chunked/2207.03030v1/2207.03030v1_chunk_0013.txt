the model, or simply moving the training set construction closer to the way the tests sets have been con- 2The leaderboard is available at: https://eval.ai/web/challenges/challenge-page/689 Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling structed by Petroni et al. (2021). The KILT test sets ﬁlter an average 18% of queries compared to their original task versions. Petroni et al. (2021) removed a query if not at least one of the answers could be mapped to a passage at least once. Crucially, if one of the answers is partially map- pable, all the other answers for this query were also kept as valid. Our analysis shows that, while the average ratio of mapped answers increases compared to the raw training data, especially exact mapped answers still only account for 10% to 67% of available answers. Therefore, we argue that we are not gaming the benchmark, as we exclusively select mapped query-answer pairs for our training, which differs from the test set construction. For a conclusive answer to this question future work should evaluate our training pro- cedure on other, independently created, evaluation tasks. A setup which is increasingly common in the neural retrieval community (Ni et al., 2021; Hofst¨atter et al., 2022). 5. Related Work Multi-task training. To the best of our knowledge, the multi-task focus of the KILT community so far has been on the retriever module and not the answer generator. The foundational retrieval augmented architectures FiD (Izacard & Grave, 2020), RAG (Lewis et al., 2020), and REALM (Guu et al., 2020) are trained on individual tasks. In their initial baseline setup Petroni et al. (2021) already studied the impact of multi-task retrieval training; Maillard et al. (2021) continued to study various conﬁgurations for KILT multi-task single-model retrieval. Lewis et al. (2021) trained the RePAQ-retriever system on multiple tasks, but for