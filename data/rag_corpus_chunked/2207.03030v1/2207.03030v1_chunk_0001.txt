more holistic view including retrieval performance (Zamani et al., 2022). The coverage sparsity of relevance judgements of large collections and the resulting reliability issues are well studied, yet still a timely problem in the re- trieval community (Zobel, 1998; V oorhees, 2001; Craswell et al., 2021; Hofst¨atter et al., 2021). This challenge is exacer- 1TU Wien, Austria (work conducted during an intern- ship at Google) 2Google, USA 3University of Massachusetts Amherst, USA. Correspondence to: Sebastian Hofst ¨atter <s.hofstaetter@tuwien.ac.at>. ICML workshop on Knowledge Retrieval and Language Mod- els, Baltimore, Maryland, USA, 2022. Copyright 2022 by the author(s). bated when tasks are retroactively expanded (Kwiatkowski et al., 2019), re-purposed (Bajaj et al., 2016) or adapt the collection (Petroni et al., 2021). We propose a simple yet effective approach for training retrieval-augmented models for knowledge-intensive tasks with noisy labels. We use a conﬁdence score for query- answer pairs and items in the knowledge base. This conﬁ- dence can be sourced from manually annotated, heuristic, or model generated aspects. We ﬁlter training examples via a threshold of conﬁdence on the relevance labels, whether a pair is answerable by the knowledge base or not. With this we aim to reduce noise in the training process, and produce better results with fewer training examples. To study our training approach, we use a ﬁxed T5-based dense retrieval module (Ni et al., 2021) and train a Fusion-in- Decoder (FiD) generator (Izacard & Grave, 2020) on multi- ple tasks of the KILT benchmark (Petroni et al., 2021). KILT aggregates and heuristically maps many different English Wikipedia-based generation tasks to a single Wikipedia snapshot, which introduces considerable noise in the label quality, due to the time-shifted nature of the task creations. We apply our conﬁdence threshold on relevance label ﬁl- tering to remove a training example if no knowledge item