J., Bulian, J., Garcia, X., Ni, J., Chen, A., Kenealy, K., Clark, J. H., Lee, S., Garrette, D., Lee-Thorp, J., Raf- fel, C., Shazeer, N., Ritter, M., Bosma, M., Passos, A., Maitin-Shepard, J., Fiedel, N., Omernick, M., Saeta, B., Sepassi, R., Spiridonov, A., Newlan, J., and Gesmundo, A. Scaling up models and data witht5x and seqio. arXiv preprint arXiv:2203.17189, 2022. Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In International Con- Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling ference on Machine Learning , pp. 4596–4604. PMLR, 2018. Singh, D., Reddy, S., Hamilton, W., Dyer, C., and Yogatama, D. End-to-end training of multi-document reader and retriever for open-domain question answering. Advances in Neural Information Processing Systems , 34, 2021. Thakur, N., Reimers, N., R ¨uckl´e, A., Srivastava, A., and Gurevych, I. Beir: A heterogenous benchmark for zero- shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021. Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A. Fever: a large-scale dataset for fact extraction and veriﬁcation. arXiv preprint arXiv:1803.05355, 2018. V oorhees, E. M. The philosophy of information retrieval evaluation. In Workshop of the cross-language evaluation forum for european languages , pp. 355–370. Springer, 2001. Yang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question an- swering. arXiv preprint arXiv:1809.09600, 2018. Zamani, H., Diaz, F., Dehghani, M., Metzler, D., and Ben- dersky, M. Retrieval-enhanced machine learning. arXiv preprint arXiv:2205.01230, 2022. Zobel, J. How reliable are the results of large-scale infor- mation retrieval experiments? In Proc. of SIGIR, 1998.