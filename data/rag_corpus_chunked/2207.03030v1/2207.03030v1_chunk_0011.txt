augmented generation are very similar. Our alternative approach is slightly better on NQ, HotpotQA, and WOW. Overall, we also notice, that our passage sampling strategy works slightly better on the alternative passages, resulting in the best overall results of our ablation. Therefore, we select this combination (line 8) for the following experiments. Scaling the generator capacity. In most NLP settings, increasing the capacity of a pre-trained model leads to ef- fectiveness gains, at the cost of efﬁciency. Given how the related methods use varying generator capacities (such as BART-Large (Lewis et al., 2019)), we want to understand and measure the implications of scaling up the generator for our alternative passage aggregation and relevance-label sampling strategy ˆT . We show these results in Table 1 for T5-Base (line 8), T5-Large (line 9), and T5-XL (line 10). We ﬁnd that scaling the model size and compute resource consistently improve results over all tasks. This is an ex- pected result. Nevertheless, we wanted to conﬁrm that our sampling improvement is not just beneﬁcial in a smaller setting. Leaderboard comparison. We submitted a T5-Base and T5-XL version of the FiD model with our relevance sam- pling to the ofﬁcial KILT leaderboard2 for a blind evaluation and present the results in Table 2. Compared to related meth- ods our FiD model with T5-Base is already state of the art on two tasks (TriviaQA and zsRE). Our T5-XL version sets a new state of the art ceiling on a total of ﬁve KILT tasks. We outperform the previous best methods by: NQ +7.4 EM, TriviaQA +6.4 EM, FEVER +2.7 Accuracy, ZS-RE +9 Ac- curacy, and WoW +0.05 F1. We only come in second place on HotpotQA (-1.4 EM) and T-REx (-2.5 Accuracy). This might be attributable to our handicapped zero shot retriever, as HotpotQA is challenging