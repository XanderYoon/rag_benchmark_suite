original chunking. In both cases the title of the page is added to all passages. Implementation. All our experiments are based on the T5X framework (Roberts et al., 2022). We use a ﬁxed GTR-Base dense retrieval model (Ni et al., 2021), which is pre-trained on the MSMARCO passage retrieval task (Bajaj et al., 2016) and has been shown to generalize well on the BEIR benchmark (Thakur et al., 2021). We train an FiD model (Izacard & Grave, 2020) using T5 v1.1 as language model backbone (Raffel et al., 2020) on TPUs. We attach task speciﬁc markers to the input for the multi- task training. We cap the input at 384 tokens (combined query and passage) and a maximum of 64 output tokens. For training we use a batch size of 128 with 50 retrieved passages, and a learning rate of 10 −3 with the Adafactor optimizer (Shazeer & Stern, 2018). We do not tune our models to a speciﬁc checkpoint, rather train them all for 50K steps. The only special case is T5-XL, which uses a learning rate of 5∗ 10−4 and is trained for 30K steps. We use beam search with a beam size of 4 for the decoding. Evaluation. To reduce the noise in our results, we present the mean and a 95% conﬁdence interval measured with a t-statistic of the last 10 checkpoints (every thousand steps from 40K to 50K training steps; 20K to 30K for T5-XL). 4. Results In this section we present and discuss our experimental results. An important note with every use of the KILT benchmark is that the numbers presented here are only com- parable to other works also based on the KILT benchmark and not the original versions of the individual tasks. This is due to the changed collection as well as