Generation with Relevance Sampling FEVER HotpotQA NQ TriviaQA zsRE T-REx WoW0K 50K 100K 150K 200KTraining Examples Control: T Treatment: ̂T Figure 1. Training examples per task and sampling method. Hatched bars indicate downsampling with potentially more train- ing data available. We ﬁnd that our training strategy signiﬁcantly improves the effectiveness on the two strongly imbalanced datasets: Triv- iaQA (+ 12.7 EM) and T-REx (+4.9 Accuracy). This leads to a new state-of-the-art in TriviaQA, and is competitive in T-REx compared to more specialized models. It also statistically signiﬁcant improves two out of the remaining ﬁve tasks, albeit at a smaller rate. When scaling up the T5 backbone of the FiD model with our multi-task sampling technique from T5-Base to T5-Large and T5-XL, we ob- serve expected quality gains across all our evaluated tasks and outperform the state-of-the-art on a total of ﬁve out of seven KILT tasks on the ofﬁcial leaderboard. 2. Relevance-Based Conﬁdence Sampling The main goal in retrieval-augmented generation is to gen- erate an answer string a given a query q; with a secondary goal of identifying a set of relevant passages P from a col- lection C, which are the source of the answer. In a dataset, the relevant passage set P (q,a) t using a threshold t, is: P (q,a) t = { p ⏐⏐ Φ(p, q, a) > t,∀ p∈ C } (1) where Φ is a mapping function between a passage, query and answer triple, returning a conﬁdence value, whether this passage is relevant or not. Only if the conﬁdence is higher than our set threshold t, do we include the passage in the set. From the view of a dataset creator, it is usually unfeasible to conduct annotations for all possible pairs, therefore, those pairings without annotations return a null conﬁdence for relatedness, even