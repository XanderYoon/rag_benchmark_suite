where we need to deﬁne a thresholdt as our sampling bound- ary. The boundary needs to be adapted to the properties of the applied task. In this work we apply our sampling approach on KILT which conducted a heuristic mapping process of passages for given query-answer pairs. We implement the conﬁdence mapping Φ as the BLEU score in their mapping and set the threshold t to be > 0, ﬁltering all pairs, where no overlap in the previ- ously annotated document was found. Our sampling is not limited to KILT and could be extended to other resources with a similar setup or by mining weakly-supervised rele- vance signals, as proposed by Asai et al. (2021), and ﬁltering for example based on the conﬁdence of the labelling model. 3. Experiment Design KILT multi-task training. We train a single generator model on multiple tasks of the KILT benchmark, most of which already provide training sets of similar magnitude (50 to 150 thousand), except for: TriviaQA (1.8 million) and T-REx (12.5 million), accounting for 96% of training exam- ples. FiD training requires considerable hardware resources, therefore we decided to downsample the oversized datasets, rather than upsample the others.1 Figure 1 shows the number of query-answer pairs available per sampling method: our control T and treatment ˆT . We apply our ﬁltering before downsampling oversized tasks to balance our multi-task training set. For our uniﬁed training we downsample oversized tasks to 200K examples, then combine and shufﬂe all tasks. By applying the relevance- label ﬁlter method ˆT we reduce our training set size by 25% (or 140K fewer examples) to 808K examples compared to our control. 1A T5-Base training run until convergence requires roughly one TPU month of compute with our downsample strategies. Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling Table 1. Comparing sampling