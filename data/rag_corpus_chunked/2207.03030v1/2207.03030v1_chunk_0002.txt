of the KILT benchmark (Petroni et al., 2021). KILT aggregates and heuristically maps many different English Wikipedia-based generation tasks to a single Wikipedia snapshot, which introduces considerable noise in the label quality, due to the time-shifted nature of the task creations. We apply our conﬁdence threshold on relevance label ﬁl- tering to remove a training example if no knowledge item could be identiﬁed as sufﬁciently relevant from the exist- ing labels. Because of the time shifted knowledge base, if an answer is not available in the new passage text any- more, we have a lower conﬁdence, that the query can be answered at all given the new passages. After this step, we apply downsampling on imbalanced tasks for a balanced multi-task training on all the seven tasks of KILT that have passage mappings; spanning open domain QA, slot ﬁlling, fact veriﬁcation, and dialogue categories: HotpotQA (Yang et al., 2018), TriviaQA (Joshi et al., 2017), Natural Ques- tions (NQ) (Kwiatkowski et al., 2019), T-REx (Elsahar et al., 2018), Zero Shot RE (zsRE) (Levy et al., 2017)), FEVER (Thorne et al., 2018), and Wizard of Wikipedia (WoW) (Di- nan et al., 2018). Furthermore, we demonstrate the robustness of our sam- pling strategy by creating an alternative to the prevalent original, aggregation method from Wikipedia paragraphs to retrievable units. Finally, we study the impact of our training method on increased capacities of the generator backbone. arXiv:2207.03030v1 [cs.CL] 7 Jul 2022 Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling FEVER HotpotQA NQ TriviaQA zsRE T-REx WoW0K 50K 100K 150K 200KTraining Examples Control: T Treatment: ̂T Figure 1. Training examples per task and sampling method. Hatched bars indicate downsampling with potentially more train- ing data available. We ﬁnd that our training strategy signiﬁcantly improves the effectiveness on the two strongly imbalanced datasets: Triv- iaQA (+ 12.7