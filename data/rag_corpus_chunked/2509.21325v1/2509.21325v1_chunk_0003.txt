novel chunk-transposed data struc- ture, which transforms the private retrieval into a single, high-performance homomorphic matrix-vector product. • We conduct a comprehensive comparative evaluation of PIR-RAG against our own imple- mentations of strong graph-based and Tiptoe-style baselines, providing a clear, data-driven analysis of the architectural trade-offs in terms of scalability, performance, and search quality. Our results establish PIR-RAG as a highly practical and efficient solution for building the next generation of privacy-preserving AI systems. Our main contributions are: • A New Architecture for Private RAG: We propose a ”cluster-and-fetch” architecture that uses coarse-grained semantic indexing to prune the search space, making PIR computa- tionally practical. • An Efficient System Implementation: We build our system using a fast, lattice-based homomorphic encryption scheme and a novel chunk-transposed data structure that transforms retrieval into a single, highly efficient matrix-vector product. • A Comprehensive Comparative Evaluation: We benchmark PIR-RAG against our own implementations of two powerful baseline architectures: a graph-based PIR system (Graph- PIR) and a Tiptoe-style private scoring system, providing a clear analysis of the trade-offs in the design space. 2 Our results demonstrate that PIR-RAG’s architecture provides a compelling balance of scala- bility, performance, and privacy, proving particularly effective for the end-to-end RAG task. 2 Related Work Our work lies at the intersection of Retrieval-Augmented Generation (RAG), Private Information Retrieval (PIR), and the system architectures that make their combination practical. We situate PIR-RAG by reviewing advances in each of these areas. 2.1 Retrieval-Augmented Generation and its Privacy Gap Retrieval-Augmented Generation (RAG) [5] has emerged as a dominant paradigm for enhancing Large Language Models (LLMs) with timely and factual knowledge from external sources. The standard RAG workflow involves embedding a user’s query, searching a vector database for relevant document chunks, and providing these chunks as context to an LLM. While highly effective at