quality evaluation and the SIFT1M dataset for scalability analysis. • Baselines: We implemented two baseline systems for direct comparison: 5 1. Graph-PIR: An architecture inspired by systems like PACMANN [10]. It constructs a k-NN similarity graph over document embeddings. Retrieval is performed via a private, multi-step graph traversal using the same LHE-based PIR primitive to fetch neighbors at each hop. 2. Tiptoe-style Private Scoring: An architecture based on the principles of Tiptoe [3]. It uses clustering to partition data. The server receives an encrypted query and homomorphically computes similarity scores for all documents in the target cluster, returning only the encrypted scores. A key limitation is that the server knows which cluster the user is querying, which may leak information. • Metrics: We measure Setup Time, Query Time, Communication, and Search Quality (NDCG, Precision, Recall). 4.2 Scalability Analysis We evaluate how the performance of each system scales with the number of documents. The results are presented in Figure 2. The setup time analysis in Figure 2(a) highlights the significant one-time cost of graph con- struction for Graph-PIR, which grows to nearly 20 seconds for 5,000 documents. In contrast, the cluster-based setups for PIR-RAG and the Tiptoe-style system are substantially faster. Figure 2(b) illustrates the query latency. Graph-PIR demonstrates excellent scalability, with its query time remaining relatively stable regardless of database size. Conversely, the query times for both PIR-RAG and the Tiptoe-style system show a clear linear growth, as their performance is tied to the increasing cluster size. The communication costs are shown in Figures 2(c) and 2(d). The downlink communication reveals the core architectural trade-offs: PIR-RAG incurs the highest cost by far (growing to nearly 475 MB), as it fetches the full plaintext content of an entire cluster. Graph-PIR and the Tiptoe- style system are far more conservative, transmitting only