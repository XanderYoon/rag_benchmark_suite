establish a foundation for safety in tool design and basic chatbot interactions, but little work has explored how real-time, context-specific warnings shape users’ ability to critically engage with AI-augmented content. This research aims to bridge that gap by investigating how tailored warning messages influence reasoning and trust in RAG-based educational settings, contributing to the broader goal of AI-augmented reasoning systems that foster reflective, well-informed decision-making. 2 Position Statement Current approaches to improving AI reliability have treated model refinement and user-oriented feedback as separate concerns. While technical advancements reduce hallucinations at the system level, user-facing interventions remain limited in their ability to guide effective human-AI reasoning. Baseline studies indicate that in-situ warning messages improve user detection of hallucinations, mitigating potential harms in AI-assisted decision-making. However, existing warnings are often generic and detached from the user’s context, limiting their effectiveness. This paper argues for the development of tailored warning systems in RAG-based AI tools that go beyond simple disclaimers by providing contextualized, actionable insights into AI biases and hallucinations. Unlike traditional warning mechanisms, tailored warnings act as cognitive scaffolds—helping users navigate the complexities of AI-generated content and engage in more reflective, informed decision-making. These warnings address errors at both the information retrieval and response generation levels, ensuring that users Manuscript submitted to ACM Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models 3 are aware of potential biases while actively guiding their reasoning process. Moving forward, this research will explore the optimal integration of hallucination detection mechanisms within user interfaces and evaluate their impact on users’ decision-making and trust calibration. 3 Study Our on-going research proposes a tailored warning system (figure 2), where a “fact-check” is being added at both the information retrieval and LLM output level. A warning message tailored to the specific problem is then outputted into