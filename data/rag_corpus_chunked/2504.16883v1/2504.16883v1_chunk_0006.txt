warnings) are statistically significant, and obtained a value of 0.006162 (PR(>F)). Despite the small sample size, this strongly suggests that the results are statistically significant, and thus, the following evaluation tend to assume similar results over a large sample size. Trust and User Experience. The evaluation of interface usability (Figure 4, scored from 1 (not easy to use) to 5 (very easy to use), shows that participants in the no warning group found the interface easiest to use, followed by the Manuscript submitted to ACM 4 Zhu et al. tailored warning group, and finally, the standard warning group. This might suggest further improvements can be made in the way warning messages were conveyed to the user; that said, it is certainly interesting how the tailored survey group had a easier time utilizing the interface than the standard. A possibility for this result may lie in that more useful/specific warning messages allows for the user to feel as if they understood the output better, thus feeling more at ease. Perhaps more importantly, Figure 3 expresses how tailored users tend to have more trust in the model than their other two counterparts, with a difference of 0.67 (out of a 5-point Likert scale). Fig. 2. The accuracy rate of the question & answer task under different levels of hallucination outputs, and under different warning conditions. 5 Discussion The nuanced relationship between user trust and warning systems reveals a complex psychological dynamic in human- AI interaction [17] [14]. Our findings suggest that transparency is not merely about presenting warnings, but about crafting them in a way that empowers users rather than intimidates them. The statistically significant difference in trust levels (0.67 on a 5-point scale) between the tailored warning group and other groups underscores the critical role of contextually relevant information [8]. While