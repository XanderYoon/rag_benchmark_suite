Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models XUYANG ZHU† , Stanford University, USA SEJOON CHANG∗, Stanford University, USA ANDREW KUIK∗, Stanford University, USA Fig. 1. Two layers of hallucination in question & answering task relevant to history subject. A tailored warning message is generated to augment user’s reasoning, based on the identified biases in the information retrieval and LLM generation layer. Retrieval-Augmented Generation (RAG) systems offer a powerful approach to enhancing large language model (LLM) outputs by incorporating fact-checked, contextually relevant information. However, fairness and reliability concerns persist, as hallucinations can emerge at both the retrieval and generation stages, affecting users’ reasoning and decision-making. Our research explores how tailored warning messages—whose content depends on the specific context of hallucination—shape user reasoning and actions in an educational quiz setting. Preliminary findings suggest that while warnings improve accuracy and awareness of high-level hallucinations, they may also introduce cognitive friction, leading to confusion and diminished trust in the system. By examining these interactions, this work contributes to the broader goal of AI-augmented reasoning: developing systems that actively support human reflection, critical thinking, and informed decision-making rather than passive information consumption. Additional Key Words and Phrases: Retrieval-Augmented Generation, Hallucination detection, User perception of AI warnings 1 Introduction Large Language Model (LLM)-based tools are increasingly proposed as cognitive assistants in domains requiring human reasoning, such as education. However, these systems are prone to generating plausible yet incorrect or biased information [3]. Retrieval-Augmented Generation (RAG) has emerged as a key approach to mitigating these hallucinations by grounding responses in external knowledge sources, thereby enhancing reliability and contextual relevance. Since the foundational RAG framework was introduced, research has made significant strides in improving retrieval accuracy and addressing misinformation-related challenges [12]. This paper was presented at the 2025 ACM Workshop on Human-AI Interaction