human- AI interaction [17] [14]. Our findings suggest that transparency is not merely about presenting warnings, but about crafting them in a way that empowers users rather than intimidates them. The statistically significant difference in trust levels (0.67 on a 5-point scale) between the tailored warning group and other groups underscores the critical role of contextually relevant information [8]. While this study focuses on educational contexts, the cognitive impact of tailored warnings extends to other AI- augmented reasoning applications, including healthcare, law, and finance. In these high-stakes fields, where users rely on AI for critical decision-making, effective warning systems play a crucial role in ensuring that AI serves as a reasoning aid rather than an unquestioned authority. Increasingly, users seek systems that not only provide information but also offer meaningful insights into its reliability [1] [13]. The tailored warning approach represents a shift from passive information consumption to active engagement with AI-generated content. By delivering specific, actionable context about potential hallucinations, these warnings empower users to critically evaluate information rather than passively accept it [12]. Moreover, the psychological impact of these warnings cannot be overstated. Traditional approaches often create a binary perception of AI systems as either entirely trustworthy or completely unreliable [15]. Our research demonstrates Manuscript submitted to ACM Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models 5 Fig. 3. The average trust (in scale of 1-5; 1 is the lowest, 5 is the highest) to the system reported by participants of the pilot study. Fig. 4. The average ease (in scale of 1-5; 1 is the lowest, 5 is the highest) to the system reported by participants of the pilot study. a more nuanced approach: users can be made aware of potential limitations while maintaining a constructive relationship with the technology. This approach aligns