to the system reported by participants of the pilot study. Fig. 4. The average ease (in scale of 1-5; 1 is the lowest, 5 is the highest) to the system reported by participants of the pilot study. a more nuanced approach: users can be made aware of potential limitations while maintaining a constructive relationship with the technology. This approach aligns with emerging research on responsible AI development, which emphasizes transparency and user empowerment [4]. During the qualitative portion of the study, one participant questions, "Why canâ€™t you just provide us the right answer if you know how to warn us?" Another participant mentions, "The warning Manuscript submitted to ACM 6 Zhu et al. messages confuse me. I just want the correct answer. " These reactions raise the question: under what context is pushing users to "think critically" with warning messages desirable? If users are naturally prone to clear-cut answers, what would be the best way to emphasize the biases and problems of AI system outputs? Future studies in this space also need to draw more from psychology and cognitive science research to inform the best system design that effectively fosters critical thinking. References [1] Jaeyeon Byun et al. 2024. Design and Implementation of an Interactive Question-Answering System with Retrieval-Augmented Generation for Personalized Databases. https://doi.org/10.3390/app14177995 (2024). [2] Jiawei Chen, Hongyu Lin, and Han. 2024. Benchmarking Large Language Models in Retrieval-Augmented Generation. Proceedings of the AAAI Conference on Artificial Intelligence 38, 16 (2024). doi:10.1609/aaai.v38i16.29728 [3] Sunhao Dai et al. 2024. Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era. arXiv preprint arXiv:2404.11457v2 (2024). [4] Shachi H Kumar, Saurav Sahay, Sahisnu Mazumder, Eda Okur, Ramesh Manuvinakurike, Nicole Beckage, Hsuan Su, Hung-yi Lee, and Lama Nachman. 2024. Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language