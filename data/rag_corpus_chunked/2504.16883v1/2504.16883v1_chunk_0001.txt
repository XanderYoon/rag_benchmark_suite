[3]. Retrieval-Augmented Generation (RAG) has emerged as a key approach to mitigating these hallucinations by grounding responses in external knowledge sources, thereby enhancing reliability and contextual relevance. Since the foundational RAG framework was introduced, research has made significant strides in improving retrieval accuracy and addressing misinformation-related challenges [12]. This paper was presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning (AIREASONING-2025-01). This is the authors’ version for arXiv. ∗All authors contributed equally to this research. Authors’ Contact Information: Xuyang Zhu, Stanford University, Stanford, California, USA, xuyang1@stanford.edu; Sejoon Chang, Stanford University, Stanford, California, USA, sejoon@stanford.edu; Andrew Kuik, Stanford University, Stanford, California, USA, akuik@stanford.edu. Manuscript submitted to ACM 1 arXiv:2504.16883v1 [cs.HC] 23 Apr 2025 2 Zhu et al. Despite these advancements, challenges remain in ensuring that RAG systems actively support human reasoning rather than passively presenting information. Initial RAG implementations occasionally propagated factually incorrect information due to unreliable retrieval sources, undermining trust [2]. To address this, novel approaches have introduced mechanisms to refine factual accuracy and content relevance. For instance, QA-RAG enhances response quality by restructuring retrieved content into a question-answer format and cross-referencing it with internal knowledge [7]. Similarly, RAGAR employs a "Chain of RAG" verification method to iteratively fact-check political content, reducing susceptibility to misinformation [5]. Domain-specific adaptations, such as RAG-end2end, further optimize accuracy by jointly training retrieval and generation on specialized datasets, as seen in COVID-19 research [10]. However, RAG systems do not solely suffer from retrieval-based limitations; they also introduce cognitive and epistemic challenges for users engaging with their outputs. Even when retrieval is accurate, biases and fairness issues can emerge at the response generation stage, where LLMs may prioritize certain documents, misinterpret retrieved facts, or be influenced by user queries. In educational contexts, retrieval-augmented content, such as historical textbooks, presents additional challenges due to