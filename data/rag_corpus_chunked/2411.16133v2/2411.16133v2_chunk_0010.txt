improves the efficiency in computing the distributions of the dataset, offering a more streamlined and scalable solution for high- dimensional data analysis. V. R ESULTS To evaluate the capabilities and performance of the Context Awareness Gate (CAG), we applied this architecture to the SQuAD dataset [25] and our proposed benchmark, CRSB. We implemented an open-domain question-answering pipeline to assess the outcomes of CAG under a specified scenario: â€¢ Setting CRSB as the local database while querying from SQuAD [25]. The pipeline should identify irrelevant queries to this local database and refrain from using RAG, instead generating a few-shot response using the LLM input prompt. In this scenario, we evaluate the pipeline outputs using two metrics from RAGAS: context relevancy and answer relevancy [17].Due to the absence of retrieved context for irrelevant queries to the dataset, we ask our model to generate a pseudo-context that answers the query and then calculate the context relevancy based on this generated context. Our question-answering base model was Qwen-2.5-7B [26] and we applied Llama-3.2-3B as our answer relevancy evaluator [27]. To demonstrate the effectiveness of the proposed pipeline, we compare the results of the classic RAG [1], HyDE [6], query transformation [7], Adaptive-RAG [16] and the proposed CAG. For the evaluation step, we applied both RAG and CAG. We set 95% density distribution as the Policy P and we set the threshold T to 0 as the Vector Candidates hyperparameters. Our experimental results shown in Table II clearly demon- strate that classic Retrieval-Augmented Generation (RAG) [1] TABLE II EVALUATION OF CONTEXT AWARENESS GATE (CAG) ON SQUAD AND CRSB Context Relevancy Answer Relevancy RAG [1] 0.016 0.206 Query Rewriting [7] 0.026 0.147 HyDE [6] 0.043 0.228 Adaptive-RAG [16] 0.334 0.613 CAG (Ours) 0.338 0.709 is unable to generalize effectively for open-domain question answering. Both Query Rewriting