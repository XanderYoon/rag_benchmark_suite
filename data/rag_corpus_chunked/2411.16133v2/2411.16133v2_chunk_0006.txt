the Vector Candidates method, which operates on a set of contexts with C contexts and N pipeline input requests, reveals a significant advantage. Specifically, the complexity of the Vector Candidates method is O(1), as it relies solely on the number of contexts, regardless of the number of input requests ( This happens when we disable the query transformation as one of the CAG steps ) . In contrast, the complexity of LLM supervision [16] scales linearly with the number of input requests, represented as O(N ). This indicates that while generating pseudo-queries may seem cumbersome, the overall efficiency of the Vector Candidates approach is superior in scenarios with multiple input requests. Alongside all the steps involved in the Vector Candidates approach, the process begins with transforming the userâ€™s input query into a more appropriate format to enhance the quality of semantic search. This query transformation is critical as it ensures that the input is optimized for better alignment with the embeddings used in the retrieval process. After this transformation, the Vector Candidates method is applied to assess the relevance of context retrieval. Following query transformation and Vector Candidates anal- ysis, the Context Awareness Gate (CAG) system dynamically adapts the input query into a suitable prompt. This involves determining whether context retrieval is necessary or if the LLM can answer the query based on its internal knowledge, utilizing techniques like Chain of Thought (CoT) reasoning [4], [5], agents, or other methods. B. Vector Candidates To address the issues of using a LLM for context su- pervision , we propose a statistical approach based on the distributions of emmbedings of contexts and pseudo-queries. Algorithm 1 Vector Candidates Algorithm Require: Contexts C, Pseudo Queries Q, Policy P , Threshold T , Input Query q Ensure: A classification (True or False) 1: Compute dataset