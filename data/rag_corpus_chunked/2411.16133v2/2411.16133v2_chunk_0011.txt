Table II clearly demon- strate that classic Retrieval-Augmented Generation (RAG) [1] TABLE II EVALUATION OF CONTEXT AWARENESS GATE (CAG) ON SQUAD AND CRSB Context Relevancy Answer Relevancy RAG [1] 0.016 0.206 Query Rewriting [7] 0.026 0.147 HyDE [6] 0.043 0.228 Adaptive-RAG [16] 0.334 0.613 CAG (Ours) 0.338 0.709 is unable to generalize effectively for open-domain question answering. Both Query Rewriting [7] and HyDE [6], while providing improvements in context retrieval, share the same limitation as classic RAG [1] in failing to address the broader challenges posed by open-domain queries. As illustrated in our results, the context relevancy scores for these methods are close to zero (0.016 for RAG [1], 0.026 for Query Rewriting [7], and 0.043 for HyDE [6]), indicating that they are unable to adapt the pipeline outputs in scenarios where the input query does not align with the local dataset. Consequently, their ability to generate accurate and relevant answers is also compromised, as reflected by their low answer relevancy scores (0.206 for RAG [1], 0.147 for Query Rewriting [7], and 0.228 for HyDE [6]). In contrast, Adaptive-RAG [16] and CAG (our proposed method) show a marked improvement in both context rele- vancy and answer relevancy. The results for Adaptive-RAG [16] ( context relevancy: 0.334, answer relevancy: 0.613) demonstrate its ability to adaptively decide when to trigger context retrieval based on query classification. Similarly, CAG achieves even higher performance (context relevancy: 0.338, answer relevancy: 0.709), suggesting that both methods suc- cessfully enhance the pipeline’s ability to dynamically adjust the input to the language model, either by incorporating external context or by relying solely on the internal knowledge of the model, depending on the query’s nature. However, a close examination of the computational costs associated with each approach reveals key differences. While Adaptive-RAG [16] relies on an additional large language