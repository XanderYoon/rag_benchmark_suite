issue of retrieving irrelevant data chunks for each input query, one solution is to ask a supervising large language model (LLM) to classify whether the query should prompt a retrieval-augmented generation (RAG) or a RAG-free response [16]. This involves determining whether the input query falls within the scope of the local database. However, a significant limitation of this approach is the high computational cost of using an LLM with billions of parame- ters for a relatively simple task like query classification. Even smaller language models come with their own challenges, such as hallucination and limited reasoning capabilities [11]. To mitigate these issues,we propose an efficient yet highly effective statistical approach, known as Vector Candidates. The key idea behind Vector Candidates is to generate pseudo- queries for each document in the set, then calculate the dis- tribution of embeddings and their similarities. By comparing the input query to this distribution, it is possible to estimate whether context retrieval is necessary with a certain level of probability. If the input query is far from this distribution, it is recommended not to retrieve any context and instead reformulate the LLM input prompt into a simpler few-shot question-answering task, rather than utilizing RAG. The over- all architecture of CAG is presented in figure 1 The limitation of this approach may appear to be the necessity of generating numerous pseudo-queries for a local dataset. However, when comparing it to LLM supervision [16], the complexity of the Vector Candidates method, which operates on a set of contexts with C contexts and N pipeline input requests, reveals a significant advantage. Specifically, the complexity of the Vector Candidates method is O(1), as it relies solely on the number of contexts, regardless of the number of input requests ( This happens when we disable the query transformation as one