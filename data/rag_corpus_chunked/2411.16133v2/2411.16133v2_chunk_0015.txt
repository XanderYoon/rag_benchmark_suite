, 2023. [10] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” arXiv preprint arXiv:2312.10997 , 2023. [11] F. Wang, X. Wan, R. Sun, J. Chen, and S. ¨O. Arık, “Astute rag: Overcoming imperfect retrieval augmentation and knowledge conflicts for large language models,” arXiv preprint arXiv:2410.07176 , 2024. [12] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval augmented language model pre-training,” in International conference on machine learning . PMLR, 2020, pp. 3929–3938. [13] Z. Jin, P. Cao, Y . Chen, K. Liu, X. Jiang, J. Xu, Q. Li, and J. Zhao, “Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models,” arXiv preprint arXiv:2402.14409, 2024. [14] A. Mallen, A. Asai, V . Zhong, R. Das, D. Khashabi, and H. Hajishirzi, “When not to trust language models: Investigating effectiveness of para- metric and non-parametric memories,” arXiv preprint arXiv:2212.10511, 2022. [15] X. Wang, Z. Wang, X. Gao, F. Zhang, Y . Wu, Z. Xu, T. Shi, Z. Wang, S. Li, Q. Qian et al., “Searching for best practices in retrieval-augmented generation,” arXiv preprint arXiv:2407.01219 , 2024. [16] S. Jeong, J. Baek, S. Cho, S. J. Hwang, and J. C. Park, “Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity,” arXiv preprint arXiv:2403.14403 , 2024. [17] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Au- tomated evaluation of retrieval augmented generation,” arXiv preprint arXiv:2309.15217, 2023. [18] L. Wang, N. Yang, and F. Wei, “Query2doc: Query expansion with large language models,” arXiv preprint arXiv:2303.07678 , 2023. [19] C.-M. Chan, C. Xu, R. Yuan, H. Luo, W. Xue, Y . Guo, and J. Fu, “Rq-rag: Learning to refine queries for retrieval augmented generation,”