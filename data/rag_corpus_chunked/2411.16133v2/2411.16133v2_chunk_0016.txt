S. Schockaert, “Ragas: Au- tomated evaluation of retrieval augmented generation,” arXiv preprint arXiv:2309.15217, 2023. [18] L. Wang, N. Yang, and F. Wei, “Query2doc: Query expansion with large language models,” arXiv preprint arXiv:2303.07678 , 2023. [19] C.-M. Chan, C. Xu, R. Yuan, H. Luo, W. Xue, Y . Guo, and J. Fu, “Rq-rag: Learning to refine queries for retrieval augmented generation,” arXiv preprint arXiv:2404.00610 , 2024. [20] Z. Jin, P. Cao, Y . Chen, K. Liu, X. Jiang, J. Xu, Q. Li, and J. Zhao, “Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models,” arXiv preprint arXiv:2402.14409, 2024. [21] Y . Yu, W. Ping, Z. Liu, B. Wang, J. You, C. Zhang, M. Shoeybi, and B. Catanzaro, “Rankrag: Unifying context ranking with retrieval- augmented generation in llms,” arXiv preprint arXiv:2407.02485, 2024. [22] G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ram ´e et al. , “Gemma 2: Improving open language models at a practical size,” arXiv preprint arXiv:2408.00118, 2024. [23] N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings using siamese bert-networks,” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 11 2019. [Online]. Available: https: //arxiv.org/abs/1908.10084 [24] A. Roberts, H. W. Chung, G. Mishra, A. Levskaya, J. Bradbury, D. Andor, S. Narang, B. Lester, C. Gaffney, A. Mohiuddin et al. , “Scaling up models and data with t5x and seqio,” Journal of Machine Learning Research, vol. 24, no. 377, pp. 1–8, 2023. [25] P. Rajpurkar, “Squad: 100,000+ questions for machine comprehension of text,” arXiv preprint arXiv:1606.05250 , 2016. [26] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang et al. , “Qwen2 technical report,” arXiv preprint