We use the widely adopted retrieval benchmark BEIR (Thakur et al., 2021). Since not all of the tasks are suitable for the RAG setting, we focused on the task of question-answering. Out of three datasets in the "Question-Answering" (QA) cat- egory of the benchmark, we chose NQ and Hot- potQA since these datasets have short answer la- bels in the form of a few keywords. This decision eases the evaluation process while enabling for a more stable robustness analysis. Moreover, we include BioASQ from the "Bio-Medical IR" cat- egory to see the effect of the perturbations on a domain-specific QA dataset. Similar to Hsia et al. (2024), we integrated datasets having different cor- pora (Wikipedia and biomedical), characteristics (multi-hop, single-hop) and sizes. 4.2 Models In order to assess the robustness of the RAG pipeline against query perturbations, we define our RAG pipeline to consist of a retriever and a gen- erator, as shown in Figure 1. In this system, the retriever is responsible of interacting with a knowl- edge base to retrieve most relevant documents con- ditioned on the given query, while the generator produces a final response using the initial query along with the retrieved context information. Retriever We employ three main retrievers in our system: BGE-base-en-v1.5 (Xiao et al., 2024), Contriever (Izacard et al., 2022) as dense retrievers, and BM25 (Robertson et al., 1995) as a sparse re- triever. For BM25, we adopt two variants: one that considers only the document content and another that uses a multi-field setup including both the doc- ument content and the “Title” field. We obtain the publicly available precomputed indices from the Pyserini framework (Lin et al., 2021). LLM Generator As generators, we used widely employed LLMs between 7-8B parameters in size: Llama-3.1-8B-Instruct (Grattafiori et al., 2024), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023)