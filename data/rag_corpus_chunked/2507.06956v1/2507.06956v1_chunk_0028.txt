consistency alignment. arXiv preprint arXiv:2403.14221. Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Shuaiqiang Wang, Chong Meng, Zhicong Cheng, Zhaochun Ren, and Dawei Yin. 2024c. Im- proving the robustness of large language models via consistency alignment. Preprint, arXiv:2403.14221. Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, and Xing Xie. 2024a. Promptrobust: Towards evaluating the robustness of large language models on adversarial prompts. Preprint, arXiv:2306.04528. Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, and Xing Xie. 2024b. Promptbench: A unified library for evaluation of large language models. Journal of Machine Learning Research, 25(254):1–22. Shengyao Zhuang and Guido Zuccon. 2022. Character- bert and self-teaching for improving the robustness of dense retrievers on queries with typos. In Proceed- ings of the 45th International ACM SIGIR Confer- ence on Research and Development in Information Retrieval, SIGIR ’22, page 1444–1454. ACM. A Appendix In this appendix we provide more details of the data preparation, perturbation and experiments runs. A.1 Datasets In this study, the experiments are performed on three datasets that are included from the BEIR benchmark: HotpotQA, Natural Questions and BioASQ. For all of the datasets, we incorporated the corpora defined within the BEIR benchmark and used samples from the test split of the datasets. HotpotQA dataset is multi-hop question answer- ing dataset that uses Wikipedia as knowledge base. This dataset requires system to retrieve all the ref- erence text passages and generators in the system to reason over them (Yang et al., 2018). Natural Questions (NQ) dataset is single-hop question answering dataset consisting of generic questions and named as "natural" since the col- lected questions are collected from the real user queries submitted to the Google Search Engine (Kwiatkowski et al., 2019a). BioASQ dataset is a biomedical question- answering dataset