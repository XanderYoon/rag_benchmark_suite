and generators in the system to reason over them (Yang et al., 2018). Natural Questions (NQ) dataset is single-hop question answering dataset consisting of generic questions and named as "natural" since the col- lected questions are collected from the real user queries submitted to the Google Search Engine (Kwiatkowski et al., 2019a). BioASQ dataset is a biomedical question- answering dataset in English that uses articles from PubMed as its corpus. BEIR benchmark uses the Training v.2020 data for task 9a as corpus while using the test data from the task 8b as queries. Fur- ther detail on the number of samples and corpus sizes as well dataset characteristics could be seen in Table 1 (Tsatsaronis et al., 2015). A.2 Automated Sample Generation Transforming textual input using large language models is a widely used technique in natural langueg processing community.For instance, the (Zhao et al., 2024b; Sun et al., 2023; Zhao et al., 2024c) used large language models to generate paraphrases of the textual inputs. Following the previous work, we also used GPT4o to automati- cally generate the perturbed samples. The prompts used to generate perturbed samples for redundancy, formal tone and ambiguity insertion cases can be found in Table 3. To assess the quality of the generated samples, we checked the perplexity and the semantic sim- ilarity values corresponding to different perturba- tion types which are shown in Figure 6. For the perplexity calculations, we used GPT2-Large (Rad- ford et al., 2019) calculate the perplexity values for each sample corresponding to a perturbation and reported the mean perplexity values. The average Redundancy """Paraphrase the input text {output_per_sample} times by inserting related redundant knowledge into the input text. Do not insert any information that will answer the question directly. Separate the output text samples by single \n be- tween them. Do