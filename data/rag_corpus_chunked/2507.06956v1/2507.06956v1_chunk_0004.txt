ommendations for improving the robustness of RAG systems. We will make our data and code publicly avail- able to support future work on evaluating the robust- ness of RAG systems to variations in user queries. 2 Related Work Existing studies on RAG robustness can be broadly grouped as focusing on the retriever, on the LLM as the final generator, or on the entire RAG pipeline. Retriever-Level Robustness Research in this category explores how retrievers maintain perfor- mance under various query perturbations (Liu et al., 2024; Zhuang and Zuccon, 2022; Sidiropoulos and Kanoulas, 2022; Penha et al., 2022; Liu et al., 2023; Arabzadeh et al., 2023). For example, Zhuang and Zuccon (2022) explores how BERT-based retriev- ers cope with spelling errors and proposes encoding strategies and training procedures to improve ro- bustness. In contrast, Liu et al. (2023) studies how generative retrievers handle query variants. Mean- while, Arabzadeh et al. (2023) evaluates retriever stability by perturbing queries’ dense representa- tions directly. LLM-Level Robustness Another body of work targets the LLM itself, examining how effectively the model filters out irrelevant or misleading con- text (Shi et al., 2023) and how it responds to per- turbations in the prompt at various granularity lev- els, from character-level to entire sentences (Zhu Dataset PERT Corpus NQ 1496 2.68M HotpotQA 1494 5.23M BioASQ 378 14.91M Table 1: Number of samples and the size of the corpora for each dataset used in the experiments: HotpotQA (Yang et al., 2018), NQ(Kwiatkowski et al., 2019a) and BioASQ(Tsatsaronis et al., 2015).(PERT: Number of perturbed samples for each perturbation type) et al., 2024a,b). These studies primarily aim to ensure that the model’s outputs remain accurate and consistent despite possible noise or adversarial modifications in the prompt. Pipeline-Level Robustness A further line of re- search adopts a holistic view of RAG, focusing