performance degradations do not stem from the naturalness of the samples to the large language models. Further, we calculated the semantic similarity of the samples to the original ones by embedding the samples into a vector space and calculating the average cosine similarity distance. To embed the samples we used the multilingual-e5-base (Wang et al., 2024). As the results show, the formal tone change and typo insertion at %10 percent result in the most semantically similar samples to the original ones. Moreover, redundant information Figure 6: Perplexity and semantic similarity of the generated samples for different perturbations and datasets. insertion causes the samples to be most distant to the original ones. As the ambiguity and typo (%25) inserted samples result in more performance drops then more redundant correpondents in many cases, we also show that the performances could not be entirely attributed to the semantic similarity changes. We used the widely employed TextAt- tack(Morris et al., 2020) library to generate the typos with typo-inserted perturbations. This library is frequently adopted across the literature. For example, (Zhu et al., 2024a) uses TextAttack to introduce character level and word level attacks to adversarial prompts, (Zhang et al., 2025) uses TextAttack to introduce stochastic perturbations to text on a character level to assess the performance of text-to-image-diffusion-models-and (Xie et al., 2024) uses TextAttack to generate typos to introduce a typos correction training for dense retrieval. A.3 Prompts used During the Experiments We follow (Rau et al., 2024) and use their prompts for question answering. From their benchmark, we used the following prompt for the experiments performed in a closed-book setting without any document insertion: system_without_docs: "You are a help- ful assistant. Answer the questions as briefly as possible." user_without_docs: f"Question:\ {question }" For the RAG experiments where the LLMs are expected to generate