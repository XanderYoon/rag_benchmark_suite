phrase a question can reflect differences in educational and cultural backgrounds. In this work, we systematically investigate the sensitivity of RAG systems to perturbations in their input queries. Specifically, we introduce transfor- mations and varying levels of typographical errors to queries across several benchmark datasets, mea- suring how such perturbations impact the perfor- mance of different components in a RAG system. Across 4 popular retrievers, we find consistent vari- ations in their performance in the face of our query perturbations. Moreover, we investigate the correlations be- tween the performances of each module and joint pipeline and provide insights on the decoupling of the case-specific sensitivities arising from each module. Motivated by these findings, we provide recommendations for improving RAG system ro- bustness to query variations and propose an evalua- tion framework. To our knowledge, this is the first work providing a framework to decouple each mod- uleâ€™s sensitivities in RAG pipelines for robustness research. To summarize, our contributions are as follows: 1. We introduce a framework for measuring the robustness of RAG systems to varying levels of typographical errors and frequently occur- ring prompt perturbation scenarios for input queries. 2. We conduct experiments using 4 different re- trievers and 3 different LLMs, evaluating 12 resulting question-answering pipelines in to- tal. Further, we cover datasets of different characteristics and domains to provide a com- prehensive analysis. 3. Based on our experimental results and addi- tional analyses, we provide insights and rec- ommendations for improving the robustness of RAG systems. We will make our data and code publicly avail- able to support future work on evaluating the robust- ness of RAG systems to variations in user queries. 2 Related Work Existing studies on RAG robustness can be broadly grouped as focusing on the retriever, on the LLM as the final generator, or