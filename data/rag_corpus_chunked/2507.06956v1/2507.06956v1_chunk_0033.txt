Remain- ing 378 questions are used and the "exact" asnwers provided are used as the golden answer of the sys- tem during the experiments. For the NQ dataset, the version contained within the BEIR benchmark is collected from the development set of the origi- nal Natural Questions (Kwiatkowski et al., 2019b) set. In order to match the labels to the queries, we collected the subset of samples in the NQ that has a corresponding answer label in the development set. A.5 Retriever Robustness For the remaining dataset and retriever combina- tions, the average retriever performances with dif- ferent Topk@k values can be seen on Figure 8 Topic Retriever Original Redundancy Formal Tone Ambiguity T%10 T%25 HotpotQA BGE Base 71.82↑ 66.92 69.34 64.45 62.94 47.75↓ HotpotQA Contriever 60.84↑ 59.12 59.34 56.42 53.37 39.06↓ HotpotQA BM25 Flat 60.81↑ 44.72 54.20 49.38 54.34 41.92↓ HotpotQA BM25 MF 58.0↑ 47.20 53.90 50.17 50.59 37.36↓ NQ BGE Base 64.59↑ 55.10 61.65 51.60 50.04 34.35↓ NQ Contriever 58.60↑ 52.11 56.58 47.33 45.39 30.95↓ NQ BM25 Flat 35.87↑ 23.64 32.80 23.55 25.87 17.12↓ NQ BM25 MF 38.91↑ 30.38 37.68 28.57 29.35 19.73↓ BioASQ BGE Base 36.06↑ 33.01 34.82 30.24 30.43 27.83↓ BioASQ Contriever 34.93↑ 30.57 31.60 28.87 28.12 25.16↓ BioASQ BM25 Flat 45.22↑ 25.01↓ 37.89 33.37 35.94 29.87 BioASQ BM25 MF 39.33↑ 25.34↓ 34.54 30.31 32.40 29.04 Table 4: The average retriever performances reported with metric Recall@5 (%). The up and down arrows define the maximum and minimum performing cases, respectively. (T%X: Typo insertion at %X level) while the average retriever results displayed on Fig- ure 4 and 9 are reported in Table 4. A.6 RAG Robustness In this section of the Appendix we report the results of the experiments and results of the analysis we perform to understand the importance of param- eter "k" selection and