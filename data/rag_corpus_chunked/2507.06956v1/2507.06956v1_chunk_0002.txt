By condition- ing generation on retrieved documents, new infor- mation can be incorporated into LLMs’ responses without additional training. Furthermore, RAG reduces the likelihood of hallucinations by ground- ing generation in documents which are a trusted source of truth and enables greater transparency by allowing end-users to inspect documents which were used to produce the response generated by an LLM. While RAG systems have achieved impressive performance, an essential question for their prac- tical application in downstream systems is how variations in a user’s query impact the relevancy of retrieved results. For instance, different users seek- ing the same information may phrase their queries differently or introduce typographical errors to the query. A desirable attribute of a RAG system is that the elements in the system (e.g., retrievers) arXiv:2507.06956v1 [cs.CL] 9 Jul 2025 Original Query Sample Typo Insertion • %10 – 25 Via Prompting • Redundant Information • Formal Tone • Ambiguity Perturbed Samples SampleOrg SamplePer1 SamplePer2 SamplePer5 . . . Retriever Knowledge Base AnswerOrg AnswerPer1 . . .RAG System AnswerPer2 AnswerPer5 Perturbation (a) (b) Generator Figure 1: Illustration of our approach to evaluating RAG robustness. (a) Perturbations are generated via prompting an LLM or random insertion of typographical errors. (b) Evaluation datasets are formed using five perturbed samples for each original example. (Org: Original, Per: Perturbed) are robust to such variations and perform similarly for all users. This is important for overall usabil- ity and fairness, as how humans phrase a question can reflect differences in educational and cultural backgrounds. In this work, we systematically investigate the sensitivity of RAG systems to perturbations in their input queries. Specifically, we introduce transfor- mations and varying levels of typographical errors to queries across several benchmark datasets, mea- suring how such perturbations impact the perfor- mance of different components in a RAG