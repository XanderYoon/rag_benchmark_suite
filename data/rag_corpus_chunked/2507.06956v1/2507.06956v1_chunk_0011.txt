define k as 5. To evaluate the LLM-generated content in the RAG pipeline, we adopted a surface matching metric from the BERGEN framework (Rau et al., 2024), called Match. This metric checks whether the generated output contains the answer span. Unlike recent trends that use an LLM for auto- mated evaluation, we opt for a model-free assess- ment to ensure robust and reproducible analysis and to avoid fluctuations caused by changes in the evaluating LLM. Moreover, it is intended that our evaluation framework avoid the computational cost associated with employing an LLM-based evalua- tor, thereby removing the need to choose a model that is parameter-efficient while ensuring evalua- tion quality. 5 Experiments In this section, we detail the steps in our analy- sis framework and describe our findings in the de- signed experiments. 5.1 Our Analysis Framework To understand the effect of each query perturba- tion on the RAG pipeline, we first perform isolated assessments on each module. For retrievers, we examine the changes in performances measured in Recall@k on the text passage retrieval task. For generators, we define two settings to cover two mechanisms that an LLM can rely on to generate answers. Then we move to the end-to-end pipeline and analyze the effect of each perturbation on the overall RAG performance. We further provide anal- ysis on correlations to individual module sensitivi- ties and changes in internal LLM representations. Details of each experiment are provided in the fol- lowing sections. 5.2 Retriever Robustness The analysis of the RAG pipeline begins with the retriever component, which interacts with a knowl- edge base to return a list of ranked elements con- ditioned on the input query. This knowledge base, consisting of text passages, will be referred to as "documents" in this study. To investigate the robustness of the retrievers, we