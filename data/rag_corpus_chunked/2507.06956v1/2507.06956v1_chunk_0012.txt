lowing sections. 5.2 Retriever Robustness The analysis of the RAG pipeline begins with the retriever component, which interacts with a knowl- edge base to return a list of ranked elements con- ditioned on the input query. This knowledge base, consisting of text passages, will be referred to as "documents" in this study. To investigate the robustness of the retrievers, we analyzed the performance changes observed with each perturbation. The resulting effects of perturbation types using different retrievers on the HotpotQA dataset are shown in Figure 2. We also provide results for the remaining retriever and dataset combinations in Figure 8. Our analysis and the recall curves show that the dense retrievers are more robust against the redun- dant information contained when compared to the sparse methods, however sparse methods perfor- mances are more robust against the typos intro- duced to the input queries. Formal tone change is the least effective perturbation types on the re- triever performances across both retrieval cate- gories. While increased typo levels i.e. %25 lead to least performance scores across all combinations in general, redundant information insertion leads to even worse performances for sparse retrievers when used for the domain specific BioASQ dataset. 5.3 Generator Robustness In order to assess how different generators handle different types of query perturbations, we exam- ine the performance changes caused by each per- turbation type. These performance changes are investigated in two settings representing the two abilities the LLMs can use in QA task to gener- ate an answer. First, they can use their parametric knowledge gained during pretraining to answer the input queries. Second, these models use their con- text utilization abilities to integrate the knowledge given in their context window into the generated answer. We refer to these settings as "closed-book" and "oracle" (respectively). Figure 3: LLM performances