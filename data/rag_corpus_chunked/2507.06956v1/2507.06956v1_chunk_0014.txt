are behaving differently. For exam- ple, while all perturbations result in performance reductions, the redundant information increases the performance of Llama 3.1-8B-Instruct in certain cases when parametric knowledge is incorporated. Similarly, the formal tone change causes perfor- mance decreases and increases based on the LLM and the dataset chosen. When perturbation types are individually as- sessed, ambiguity insertion decreases model per- formance in both settings across different datasets, posing a challenge for LLMs. While redundant in- formation has a low impact on performance for gen- eral datasets such as NQ and HotpotQA, it causes drastic performance drops on the domain-specific BioASQ dataset in both settings. Moreover, the typo insertions are particularly impactful in the closed-book setting, resulting in great performance decreases. In contrast, when the necessary knowledge is provided, the systems are mostly able to recover from these perturbations, especially at a level of 10%. This indicates that when combined with information, the query pertur- bations result in different impacts than the effects seen in closed book settings which are commonly used to evaluate LLM robustness. Lastly, the performance of the models in the closed book setting is not reflected in the oracle performances, which underlines the importance of the context utilization abilities and the retrieval incorporated to the RAG pipelines. For instance, although the parametric knowledge of Mistral-7B- Instruct-v0.2 varies across datasets, it is the best performing model in the oracle setting. 5.4 RAG Robustness Finally, we analyze the joint effect of combining different elements to form an end-to-end RAG sys- tem. This setting differs from the oracle experi- ments defined earlier in that the system includes a non-ideal retriever which can return irrelevant documents. Figure 4 and 9 display the average end-to-end results of the pipeline reported in "Match" metric. Each window incorporates a single retrieverâ€™s data while