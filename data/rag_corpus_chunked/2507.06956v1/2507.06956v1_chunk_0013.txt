use in QA task to gener- ate an answer. First, they can use their parametric knowledge gained during pretraining to answer the input queries. Second, these models use their con- text utilization abilities to integrate the knowledge given in their context window into the generated answer. We refer to these settings as "closed-book" and "oracle" (respectively). Figure 3: LLM performances under different pertur- bations using the "Match" metric in closed-book and oracle settings. Closed-book experiments require the generator LLM to answer the given queries without access- ing any external knowledge source and hence com- pletely relying on the knowledge stored in their parametric memory. In contrast, in oracle exper- iments, the existence of an "oracle" retrieval sys- tem is assumed to return only correct documents and nothing else. This setting establishes an upper bound for the system, as the models have access to only correct information and no other information. For each dataset, we report the experimental results in Figure 3, where each generator is differentiated by color and different settings are reflected by line styles. All results are reported in Match metric for the original and perturbed datasets. Our results show that the generator robustness is dependent on the nature of the dataset and the sensitivity of each LLM against difference pertur- bations. The LLMs tend to follow similar trends on a dataset and the perturbations result in perfor- mance drops in general. However, there are cases where LLMs are behaving differently. For exam- ple, while all perturbations result in performance reductions, the redundant information increases the performance of Llama 3.1-8B-Instruct in certain cases when parametric knowledge is incorporated. Similarly, the formal tone change causes perfor- mance decreases and increases based on the LLM and the dataset chosen. When perturbation types are individually as- sessed, ambiguity insertion decreases model