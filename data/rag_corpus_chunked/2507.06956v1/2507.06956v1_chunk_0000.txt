Investigating the Robustness of Retrieval-Augmented Generation at the Query Level Sezen Per√ßin1*, Xin Su 2, Qutub Sha Syed 2, Phillip Howard 3, Aleksei Kuvshinov 1, Leo Schwinn1, Kay-Ulrich Scholl 2 1Technical University of Munich, 2Intel Labs, 3Thoughtworks, {sezen.percin, aleksei.kuvshinov, l.schwinn}@tum.de, {xin.su, syed.qutub}@intel.com, phillip.howard@thoughtworks.com Abstract Large language models (LLMs) are very costly and inefficient to update with new information. To address this limitation, retrieval-augmented generation (RAG) has been proposed as a so- lution that dynamically incorporates external knowledge during inference, improving factual consistency and reducing hallucinations. De- spite its promise, RAG systems face practical challenges-most notably, a strong dependence on the quality of the input query for accurate retrieval. In this paper, we investigate the sen- sitivity of different components in the RAG pipeline to various types of query perturba- tions. Our analysis reveals that the performance of commonly used retrievers can degrade sig- nificantly even under minor query variations. We study each module in isolation as well as their combined effect in an end-to-end question answering setting, using both general-domain and domain-specific datasets. Additionally, we propose an evaluation framework to system- atically assess the query-level robustness of RAG pipelines and offer actionable recommen- dations for practitioners based on the results of more than 1092 experiments we performed. 1 Introduction Recent advancements in the capabilities of large language models (LLMs) have revolutionized the field of natural language processing (NLP) and have achieved impressive performance across a broad range of downstream applications. Their suc- cess can largely be attributed to the massive text datasets on which they are trained and their increas- ing size in terms of model parameters. However, these factors that have enabled their success also limit their practical implementation in downstream applications. For example, a business seeking to implement an LLM to answer questions about pro- prietary internal documents