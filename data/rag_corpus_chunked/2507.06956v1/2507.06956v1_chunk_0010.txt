only the document content and another that uses a multi-field setup including both the doc- ument content and the “Title” field. We obtain the publicly available precomputed indices from the Pyserini framework (Lin et al., 2021). LLM Generator As generators, we used widely employed LLMs between 7-8B parameters in size: Llama-3.1-8B-Instruct (Grattafiori et al., 2024), Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) and Qwen2.5-7B-Instruct (Team, 2024). Using the BERGEN framework (Rau et al., 2024), we set the maximum input length to 4096 tokens, the max- imum generated tokens to 128, and the temperature to 0. When generating the responses, we used greedy decoding. Following the setup provided by the framework, when incorporating the retrieved documents into the LLM’s input, we truncate each document to a maximum of 100 words. We use vLLM (Kwon et al., 2023) as our inference frame- work to run these models. All the experiments are performed on a NVIDIA GeForce RTX 3090 GPU. 4.3 Standard Evaluation Metrics To evaluate the performance of retrievers, we uti- lized a widely employed metric for assessing the information retrieval of dense and sparse retrievers, Figure 2: Recall@k results obtained with different retrievers on HotpotQA with respect to the changing "k" parameter as shown in axis Top@k. namely the Recall@k, where the parameter k de- fines the top “k” documents that are returned by the retriever. While investigating retriever robustness, we experimented with different k choices; how- ever, during the end-to-end experiments we define k as 5. To evaluate the LLM-generated content in the RAG pipeline, we adopted a surface matching metric from the BERGEN framework (Rau et al., 2024), called Match. This metric checks whether the generated output contains the answer span. Unlike recent trends that use an LLM for auto- mated evaluation, we opt for a model-free assess- ment to ensure