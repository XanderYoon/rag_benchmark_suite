al., 2019) calculate the perplexity values for each sample corresponding to a perturbation and reported the mean perplexity values. The average Redundancy """Paraphrase the input text {output_per_sample} times by inserting related redundant knowledge into the input text. Do not insert any information that will answer the question directly. Separate the output text samples by single \n be- tween them. Do not output anything else and do not answer the question but only paraphrase it. Input text: {input_str} Output:\n\n """ Formality """Paraphrase the input text {output_per_sample} times in a more formal tone. Separate the output text samples by single \n be- tween them. Do not output anything else and do not answer the question but only paraphrase it. Input text: {input_str} Output:\n\n """ Ambiguity """Paraphrase the text below {output_per_sample} times while making it unclear to answer by intro- ducing ambiguity to the text. Separate the output text samples by single \n be- tween them. Do not output anything else and do not answer the question but only paraphrase it. Input text: {input_str} Output:\n\n """ Table 3: Prompts used to generate perturbed samples. perplexity of the original, i.e. non-perturbed, sam- ples are reported with the dashed line on Figure 6 for each dataset. The results showed that the sam- ples perturbed via prompting have less perplexity when compared to the original samples while the typo insertions result in samples more perplexing to the models. Based on our analysis, we showed that the performance degradations do not stem from the naturalness of the samples to the large language models. Further, we calculated the semantic similarity of the samples to the original ones by embedding the samples into a vector space and calculating the average cosine similarity distance. To embed the samples we used the multilingual-e5-base (Wang et al., 2024). As the results show,