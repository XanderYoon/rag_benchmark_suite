Their suc- cess can largely be attributed to the massive text datasets on which they are trained and their increas- ing size in terms of model parameters. However, these factors that have enabled their success also limit their practical implementation in downstream applications. For example, a business seeking to implement an LLM to answer questions about pro- prietary internal documents may lack the compute *Work performed while at Intel Labs. resources and dataset scale needed to train an LLM with the necessary domain knowledge. Even when an LLM can be properly trained on domain-specific data, all existing models are prone to the well-known issue of hallucination (Huang et al., 2023). This phenomenon, where LLMs pro- duce confident-sounding, factually inaccurate re- sponses, is particularly problematic for applications in which downstream users may lack the necessary domain knowledge to identify and correct the in- accuracies. Further compounding this issue is the inherent lack of transparency in how LLMs arrive at their generated responses (Zhao et al., 2024a). Retrieval-augmented generation (RAG) has been proposed as a solution for mitigating the aforemen- tioned shortcomings of LLMs (Lewis et al., 2020; Ram et al., 2023). Specifically, a RAG system utilizes a text retriever to identify the documents in a text corpus most relevant to a given query via a semantic similarity measure. The most sim- ilar retrieved documents are then provided as ad- ditional context to the LLM, along with the query for context-augmented generation. By condition- ing generation on retrieved documents, new infor- mation can be incorporated into LLMsâ€™ responses without additional training. Furthermore, RAG reduces the likelihood of hallucinations by ground- ing generation in documents which are a trusted source of truth and enables greater transparency by allowing end-users to inspect documents which were used to produce the response generated by an LLM.