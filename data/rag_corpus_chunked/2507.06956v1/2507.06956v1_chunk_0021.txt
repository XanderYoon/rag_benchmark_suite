work, prompts will be further tuned to meet the characteristics of datasets better. Furthermore, we kept our pipeline simple to pro- vide researchers with a framework to evaluate their own system. With the same aim and to reflect the scenario where there is limited computational power and high-quality annotated data, we chose to use retrievers and LLMs directly without applying fine-tuning. This decision also entailed the exclu- sion of rerankers as these models also rely on the performance of retrievers to return document sets with larger set sizes. Our analysis shows that the perturbations are still effective on larger document set sizes as shown in Figure 2 and 8, therefore we leave the analysis of the rerankers to a future study. Moreover, the role of the ranking of the docu- ment sets returned by the retriever with respect to the perturbations is left to a future study. We hope that by integrating metrics that concentrate more on the ranking aspects of the retrieval (e.g. MRR and nDCG) into our analysis framework, practitioners can assess the sensitivity of their pipelines with a focus on this particular aspect. Lastly, potential mitigation strategies aiming to increase the robustness of the modules such as fine- tuning of the retrieval augmented generation com- ponents on the perturbed sample-answer pairs, or including perturbed samples into the end-to-end joint training of retrievers and LLMs for robust- ness aware question answering systems are not discussed within the scope of the analysis of this work. This also includes the analysis of another cat- egory of methods in relation to query perturbations, namely the query transformations. The robustness of these methods and their effect within the scope of RAG pipelines when faced with various input variations are not addressed in this study. Lastly, while the investigation of LLM internal represen-