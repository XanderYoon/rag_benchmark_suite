(Rau et al., 2024) and use their prompts for question answering. From their benchmark, we used the following prompt for the experiments performed in a closed-book setting without any document insertion: system_without_docs: "You are a help- ful assistant. Answer the questions as briefly as possible." user_without_docs: f"Question:\ {question }" For the RAG experiments where the LLMs are expected to generate an answer using the knowl- edge contained in the retrieved documents, we used the following prompt: system: "You are a helpful assistant. Your task is to extract relevant infor- mation from provided documents and to answer to questions as briefly as possible." user: f"Background:\n{docs}\n\nQuestion:\ {question}" For the BGE BASE model, we used the follow- ing prompt given in Pyserini regressions to encode the passages: "Represent this sentence for searching relevant passages:" A.4 Details of the Answer Label Matching BEIR benchmark does not originally incorporate the labels of the Question Answering to their eval- uation process. In order to use these dataset for the RAG setting, we collected the respective answer labels of the queries from various resources. For HotpotQA we collected the answer labels from the metadata information stored within the sample instances. Similarly for the BioASQ, we followed the instructions provided by the BEIR benchmark to form the corpus and the test set. Out of 500 test queries provided, the ones belonging to the cate- gory "Summary" are eliminated as these provide a free-form string as the reference answer. Remain- ing 378 questions are used and the "exact" asnwers provided are used as the golden answer of the sys- tem during the experiments. For the NQ dataset, the version contained within the BEIR benchmark is collected from the development set of the origi- nal Natural Questions (Kwiatkowski et al., 2019b) set. In order to match the labels to the