“tomrow” in- stead of “tomorrow”. Nevertheless, such typo- graphical errors may still affect retrieval and gener- ation in a token-based RAG pipeline. One potential solution is to run a dedicated spell checker before feeding the text into the RAG pipeline, but this introduces additional computational overhead and may be inaccurate for domain-specific terminology. For instance, the term “agentic,” recently popular- ized in AI discussions, often triggers false alarms in existing spell-check systems. To explore the effect of spelling errors, we use the TextAttack (Morris et al., 2020) library to sim- ulate minor typos by replacing characters in the query based on their proximity on a QWERTY key- board. We experiment with perturbing 10% and 25% of the words in each query to ensure the over- all intent remains understandable. In addition, we maintain a stop-word list that remains unaltered to preserve key semantic content. Example obtained with typo perturbations at 10% and 25% levels in respective order are provided below. "when does the cannes film festival take plac" "when does the cannes fjlm festival takr place" For each perturbation type, we take each sample from the original dataset and generate 5 new per- turbed samples based on the original sample. We present an overview of our approach in Figure 1. 4 Experiment Details In this section, we describe the elements used in these experiments, such as the datasets and models, to assess the robustness of the RAG systems. 4.1 Datasets We use the widely adopted retrieval benchmark BEIR (Thakur et al., 2021). Since not all of the tasks are suitable for the RAG setting, we focused on the task of question-answering. Out of three datasets in the "Question-Answering" (QA) cat- egory of the benchmark, we chose NQ and Hot- potQA since these datasets have short answer la- bels in the