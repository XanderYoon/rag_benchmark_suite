as screenshot images (Ma et al., 2024; Faysse et al., 2024), and VLMs process both textual ques- tions and these document images to generate an- swers (Riedler and Langer, 2024; Xia et al., 2024; Yu et al., 2024; Cho et al., 2024). Compared to traditional text-only RAG, vision-based RAG can leverage structured and visual information from documents, such as tables, graphs, and images, which are often challenging to extract through text- only pipelines. Our VISA attribution method proposed in this paper is a novel approach for vision-based RAG pipelines: directly drawing bounding boxes around the content in retrieved document screenshots that potentially supports the generated answers. This approach differs from existing attribution methods in two ways: (1) Granularity: Existing attribution methods often operate at the document level, re- quiring users to read entire documents to locate supportive content. In contrast, our method directly attributes the answer to specific content within the document, such as a passage, table, or image in the screenshot. (2) Presentation: Traditional attri- bution methods provide a list of textual citations, whereas our method uses bounding boxes, offering a visually-oriented form of attribution. This can help users quickly locate the relevant information. 2.2 Bounding Box Drawing with VLM Bounding box-based object detection is a well- established task in computer vision (CV) (Zhao et al., 2019; Zou et al., 2023). Traditional ap- proaches rely on convolutional neural networks (CNNs) (LeCun et al., 2015) or Vision Transform- ers (ViTs) (Dosovitskiy et al., 2021) to extract fea- tures and predict bounding boxes alongside object classification (Ren et al., 2015; Dai et al., 2016; Redmon et al., 2016; Carion et al., 2020). Recent vision-language models (VLMs) like GPT4o (OpenAI, 2024), QWen2-VL (Wang et al., 2024), and PaliGemma (Steiner et al., 2024) have shown the ability to generate bounding box coor- dinates