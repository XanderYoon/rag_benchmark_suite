bility to detect no-answer situations. After these operations, the data statistics are shown in Table 1. 4 Experiment Setup 4.1 Evaluation Evaluation metrics assessed both the generated an- swers and bounding box predictions. Relaxed exact match (EM) was used to measure generated answer accuracy, considering a generated answer correct if it shares a subsequence relationship with the golden answer and differs by no more than 20 characters. For bounding boxes, Intersection over Union (IoU) was calculated to determine localization precision, with an IoU threshold of 0.5 indicating a correct prediction. To analyze performance across varying content types, test samples were categorized by the modal- ity and location of the evidence. For Wiki-VISA, categories included first-page passages, passages beyond the first page, and non-passage content such as tables and figures. For Paper-VISA, since it is a single-page document, categories were divided into passage and non-passage content. The overall accuracy for each dataset was computed as a macro average across these categories. We evaluate the effectiveness of VISA in two dif- ferent settings: Single oracle candidate and Multi- candidate. Single oracle candidate setting solely evaluates the generation and visual attribution com- ponent. We conduct controlled experiments by training and testing the VLMs using only a single ground truth relevant document screenshot as input. In this setup, it is guaranteed that the answer can be found within the input document. The VLMs do not need to predict the relevant document identifier and can focus exclusively on answer generation and bounding box prediction. In a Multi-candidate setting, the model is evalu- ated on its ability to distinguish relevant documents from irrelevant ones, in addition to generating ac- curate answers and bounding boxes. This setup better reflects the RAG scenarios in which multiple candidate documents are retrieved, and the model must not only generate a