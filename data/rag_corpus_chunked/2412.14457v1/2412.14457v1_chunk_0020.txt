T rut h VIS A Output Figure 2: Type of errors in the evaluation of Wiki-VISA. Train Data Wiki-VISA Paper-VISA bbx ans bbx ans Crop, Absolute 54.2 65.2 27.8 36.2 No Random Crop 58.8 65.6 1.7 36.9 Normalized Value 56.4 64.4 0.1 37.2 No Bounding Box 0 67.6 0 35.2 Table 4: Impact of bounding box target representation and cropping strategies during training on Wiki-VISA in the single oracle candidate setting. 6.3 Bounding Box Target Table 4 shows the impact of different bounding box target representations and cropping strategies during training. Training with random cropping and absolute coordinate values achieves a balance between in-domain performance on Wiki-VISA (54.2%) and zero-shot generalization to Paper- VISA (27.8%) in bounding box accuracy. Remov- ing random cropping slightly improves Wiki per- formance but drastically reduces zero-shot general- ization, indicating that random cropping enhances the model’s robustness to varied input sizes. Nor- malizing coordinate values achieves moderate per- formance on Wiki-VISA but fails on Paper-VISA, suggesting that absolute bounding box values are better suited to our experiments. The “No Bounding Box” row represents a vanilla visual retrieval-augmented generation setup with- out visual source attribution, where models gen- erate answers without bounding box predictions. VISA enables visual source attribution capability while the effectiveness of answer generation is pre- served at about the same level of effectiveness. 6.4 Error Analysis We conducted an error analysis on 50 randomly sampled cases from Wiki-VISA to better under- stand the limitations of VISA. Errors were cate- gorized into three main types as demonstrated in Figure 2. The first type, wrong source attribution, occurred in 43 cases where the model attributed the source to an incorrect section of the document, failing to identify the precise region containing the evidence. The second type, position misalignment, was observed in 4 cases where the