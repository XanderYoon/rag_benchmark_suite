document together as the input for the multi-document VISA. The rea- son we did not directly take top- m documents as the retrieval candidate is that we do not want VISA biased on a specific retriever and position of the candidate docs. Generally, our VISA does not rely on the type of retriever. It can be either a traditional Dataset # Train # Test Wiki-VISA 87k 3,000 Paper-VISA 100k 2,160 Fineweb-VISA 60k - Table 1: Datasets statistics for train and test splits. text-based retriever that indexes the document with extracted text or a recent document screenshot re- triever that directly indexes the original document screenshot. However, integrating with those visual- based retrievers enables us to build an end-to-end RAG solution without the necessity of explicit doc- ument content processes such as HTML parsing or OCR. Thus, we leverage an off-the-shelf Doc- ument Screenshot Embedding (DSE) model (Ma et al., 2024) to serve as the retrieval component of the RAG system. When encoding queries and doc- uments, the model directly encodes textual queries and document screenshot images into single vector embeddings and performs cosine similarity search during inference. In this work, we set k = 20 and m = 3. Additionally, an RAG pipeline may have the chance of having no ground truth document re- turned from the retriever. We use a probability of 20% to randomly replace the ground truth docu- ment in the candidates, to access the modelâ€™s capa- bility to detect no-answer situations. After these operations, the data statistics are shown in Table 1. 4 Experiment Setup 4.1 Evaluation Evaluation metrics assessed both the generated an- swers and bounding box predictions. Relaxed exact match (EM) was used to measure generated answer accuracy, considering a generated answer correct if it shares a subsequence relationship with the golden answer and