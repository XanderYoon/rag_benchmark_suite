the answer, and the bounding box for the evidence. For cases where no relevant document was present (20% of the training samples), the model was trained to generate “No answer.” We used the Method Wiki-VISA Paper-VISA Average [<1] Passage [>1] Passage Non-PassageAverage Passage Non-Passage bbx ans bbx ans bbx ans bbx ans bbx ans bbx ans bbx ans Zeroshot Prompt QWen2-VL-72B1.5 60.4 3.4 58.5 0.1 54.9 0.9 67.9 1.5 43.1 0.5 40.2 2.5 45.9 Fine-tune, Single Oracle Candidates VISA-2B-single37.5 57.1 70.0 61.1 18.7 44.9 23.8 65.3 63.0 38.3 50.6 34.4 75.3 42.1 VISA-7B-single54.2 65.2 75.6 66.5 50.1 56.0 36.8 73.1 68.2 43.8 58.1 41.6 78.2 45.9 Fine-tune, Multi Candidates, Oracle in Candidates VISA-2B-multi22.5 37.9 46.5 46.1 6.4 27.2 14.6 40.5 51.3 33.8 41.1 30.1 61.4 37.4 VISA-7B-multi37.7 41.8 58.1 49.2 32.8 32.0 22.2 44.1 59.9 39.2 47.7 35.9 72.0 42.4 Fine-tune, Multi Candidates, Full VISA-2B-full 32.1 46.9 51.0 53.6 18.9 38.0 26.5 49.1 59.8 44.7 51.6 42.6 67.9 46.7 VISA-7B-full 41.6 51.1 56.6 57.1 34.4 43.2 33.9 53.1 66.8 50.3 57.1 47.5 76.5 53.0 Table 2: Effectiveness of VISA on Wiki-VISA and Paper-VISA datasets for bounding box accuracy (bbx) and answer accuracy (ans). Fine-tuned models are trained individually on in-domain data. The Multi-Candidate, Oracle in Candidates setting uses the same query set as the Single Oracle Candidates setting, allowing direct comparison. The full setting has an additional 20% queries with no ground truth documents in candidates. prompt template provided in Appendix A.3 to for- mat the model’s input and output. The training objective for both setups was next- token prediction with cross-entropy loss. We fine- tuned the models for two epochs in the single- candidate setting, using LoRA with a learning rate of 1e-4, a batch size of 64, and 4×H100 GPUs. For the multi-candidate setting, we initialized the mod-