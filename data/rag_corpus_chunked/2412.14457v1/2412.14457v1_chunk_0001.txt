for users to verify the sources and appropriately trust generated an- swers, given that models could produce halluci- nated content (Min et al., 2023; Malaviya et al., 2024). Recent works have introduced the genera- tion with citation paradigm (Gao et al., 2023; Ye * Equal contribution et al., 2024), prompting the model to not only gen- erate answers but also directly cite the identifiers of the source documents. Such source attribution approaches make it possible for users to check the reliability of the outputs (Asai et al., 2024). However, text-based generation with source attri- bution faces several issues: First, citing the source at the document level could impose a heavy cogni- tive burden on users (Foster, 1979; Sweller, 2011), where users often struggle to locate the core ev- idence at the section or passage level within the dense and multi-page document. Despite such granularity mismatch could be addressed through passage-citation-based generation methods — link- ing answers to specific text chunks, it requires non- trivial extra engineering efforts to match the chunk in the document source. Moreover, visually high- lighting text chunks in the source document is more intuitive for users, but it remains challenging as it requires control over document rendering, which is not always accessible, such as in PDF scenarios. Inspired by the recent document screenshot em- bedding retrieval paradigm — dropping the docu- ment processing module and directly using VLM to preserve the content integrity and encoding doc- ument screenshots for retrieval (Ma et al., 2024), we ask whether source attribution can also be in- tegrated into such a unified visual paradigm to es- tablish a fully visual, end-to-end verifiable RAG pipeline that is both user-friendly and effective? To this end, we propose Retrieval Augmented Generation with Visual Source Attribution (VISA). In our approach, a large vision-language model