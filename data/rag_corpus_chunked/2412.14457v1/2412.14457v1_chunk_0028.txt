Linguistics. Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Jun- hao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. 2024. Visrag: Vision-based retrieval-augmented gener- ation on multi-modality documents. Preprint, arXiv:2410.10594. Zhong-Qiu Zhao, Peng Zheng, Shou tao Xu, and Xin- dong Wu. 2019. Object detection with deep learning: A review. Preprint, arXiv:1807.05511. Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019. PubLayNet: Largest Dataset Ever for Docu- ment Layout Analysis . In 2019 International Con- ference on Document Analysis and Recognition (IC- DAR), pages 1015–1022, Los Alamitos, CA, USA. IEEE Computer Society. Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. 2023. Object detection in 20 years: A survey. Preprint, arXiv:1905.05055. Figure 3: An example of synthetic data from Paper-VISA. A Appendix A.1 Prompt for synthetic data generation The following prompt was used for prompting QWen2-VL-72B to generate synthetic questions and answers for Paper-VISA and Fineweb-VISA datasets. System: Ask a question that can be specifically answered by the content in the red bounding box area and give a short answer. The question can be a wh- question, a yes/no question, or a how question, that can be answered in a few words. Output format: Question: <question> Short Answer: <short answer> Or simply return ‘Empty’ if the bounding box area is not visible or informative. User: {image} Figure 3 shows an example of synthetic data from Paper-VISA. A.2 Prompt for Single Oracle candidate VISA The following prompt template was used to format the model’s inputs and outputs for training theSingle Oracle Candidate VISA. Model Input: System: Given a document image, your task is to answer the question and locate the source of the answer via a bounding box. User: {image} Image Size: {image.size} Question: {question} Model Output: Assistant: Answer: {answer} Bounding Box: {bounding_box} A.3