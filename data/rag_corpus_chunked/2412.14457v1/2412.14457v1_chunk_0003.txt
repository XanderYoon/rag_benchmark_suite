v el Figure 1: Comparison between (a) Text-based generation with source attribution in a RAG pipeline. and (b) Visual-based generation with source attribution in a V-RAG pipeline. VISA directly pinpoint the source evidence of the answer for user query in the original document with a bounding box. original context for the generated answer. VLMs are not restricted by document format or render- ing, making them more versatile for diverse use cases. Moreover, this task serves as a meaning- ful evaluation of VLMs, assessing their ability to provide self-explanations and accurately localize supporting information within their original input in an RAG paradigm. To the best of our knowledge, this is the first work to leverage a VLM for directly enabling visual source attribution in an RAG frame- work. To train and evaluate VISA, we curated two datasets: Wiki-VISA and Paper-VISA. Wiki- VISA is derived from the Natural Questions dataset (Kwiatkowski et al., 2019). It reconstructs the original Wikipedia webpages, using short an- swers as generation targets and corresponding long answer’s HTML bounding box as source attribution targets. This dataset supports the test of model’s ability to attribute sources across multi-document, multi-page, and multi-modal content. On the other hand, Paper-VISA, built from PubLayNet (Zhong et al., 2019) with synthetic query generation, fo- cuses on the biomedical domain by evaluating per- formance on multi-modal scientific paper PDFs. Together, they provide diverse and challenging benchmarks for assessing the granularity and ac- curacy of source attribution in RAG systems. Our experiments, spanning both in-domain training and zero-shot evaluation, revealed existing state-of-the- art models like QWen2-VL-72B (Wang et al., 2024) struggle with precise visual source attribution in zero-shot prompting. Fine-tuning VISA on our cu- rated datasets significantly improved model perfor- mance in visual attribution accuracy. Further anal- ysis highlights key areas for improvement, such as enhancing