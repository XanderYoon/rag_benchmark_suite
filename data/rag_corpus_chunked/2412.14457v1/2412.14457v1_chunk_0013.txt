focus exclusively on answer generation and bounding box prediction. In a Multi-candidate setting, the model is evalu- ated on its ability to distinguish relevant documents from irrelevant ones, in addition to generating ac- curate answers and bounding boxes. This setup better reflects the RAG scenarios in which multiple candidate documents are retrieved, and the model must not only generate a correct response but also attribute it to the correct source document. For the Multi-candidate evaluation, we assess two config- urations: Multi-candidate, Oracle in Candidates which has ground truth in candidates, this setting has the same query set as the single setting, hence directly comparable. Multi-candidate, Full con- tains the additional 20% cases where ground truth has no answer. 4.2 Training Details To train vision-language models (VLMs) for an- swer generation with VISA, we initialized the models using the open-source Qwen2-VL-2B and Qwen2-VL-7B (Wang et al., 2024), finetuning on the training datasets described in Section 3.3. We first trained the models in a single-candidate setup, where the input was limited to a single or- acle document image. In this setup, the model was trained to generate both the answer and its corresponding bounding box. We used the prompt template provided in Appendix A.2 to format the model’s input and output. Next, we trained the models in a multi-candidate setup. Here, the model received three document candidates and the task was to generate the iden- tifier of the relevant document (if present), the answer, and the bounding box for the evidence. For cases where no relevant document was present (20% of the training samples), the model was trained to generate “No answer.” We used the Method Wiki-VISA Paper-VISA Average [<1] Passage [>1] Passage Non-PassageAverage Passage Non-Passage bbx ans bbx ans bbx ans bbx ans bbx ans bbx ans bbx ans Zeroshot Prompt