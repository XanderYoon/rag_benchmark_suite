(Dosovitskiy et al., 2021) to extract fea- tures and predict bounding boxes alongside object classification (Ren et al., 2015; Dai et al., 2016; Redmon et al., 2016; Carion et al., 2020). Recent vision-language models (VLMs) like GPT4o (OpenAI, 2024), QWen2-VL (Wang et al., 2024), and PaliGemma (Steiner et al., 2024) have shown the ability to generate bounding box coor- dinates in an image-to-text manner, taking input images and generate the top-left and bottom-right coordinates of target objects. Unlike traditional object detection that focuses on natural images, our method applies bounding box drawing to text- intensive document screenshots. Additionally, grounding elements on screenshots has been explored in GUI agent systems (Cheng et al., 2024; Lin et al., 2024), where bounding boxes are used to localize UI elements like but- tons. While these approaches focus on GUI con- texts, our work targets visual source attribution in vision-based RAG processes, grounding bounding boxes to locate evidence within document images. 3 Method 3.1 Task Definition Our VISA is a novel source attribution method pri- marily designed for vision-based RAG systems. To formally define the task of RAG with visual-based source attribution: given a textual user query q as the RAG system input, the retrieval component of the system needs to retrieve a set of candidate doc- uments D = {d1, ..., dn} from corpus C. Then the generation component of the system needs to return three outputs: an answer a that answers the query q, the identifier i of the most relevant doc- ument d∗ in D, and a bounding box coordinates Bd∗ = [(x1, y1), (x2, y2)] within d∗ that highlight the content supporting the generated answer a. In a vision-based RAG setup, user queries are textual, while all documents in the corpus C are screenshots of documents (e.g., webpages or PDF pages)