attribution in RAG systems. Our experiments, spanning both in-domain training and zero-shot evaluation, revealed existing state-of-the- art models like QWen2-VL-72B (Wang et al., 2024) struggle with precise visual source attribution in zero-shot prompting. Fine-tuning VISA on our cu- rated datasets significantly improved model perfor- mance in visual attribution accuracy. Further anal- ysis highlights key areas for improvement, such as enhancing bounding box precision for long im- age documents, multi-documents, and zero-shot generalization capabilities. 2 Related Work 2.1 RAG attribution Open-domain question answering with LLMs often suffer from two key issues: hallucinations and out- dated internal knowledge. Retrieval-Augmented Generation (RAG) has been recognized as an ef- fective solution to these problems (Lewis et al., 2020; Gao et al., 2024; Ovadia et al., 2024). In RAG, relevant documents are first retrieved from an external database and then fed into LLMs along- side the question. This allows LLMs to reference the retrieved documents during answer generation. Furthermore, RAG can generate a list of citations attached to the generated answers, linking them to the retrieved documents so users can verify the accuracy of the output. This process is known as source attribution (Rashkin et al., 2023; Bohnet et al., 2023; Khalifa et al., 2024). Typically, RAG with source attribution follows a text-only pipeline where all inputs and outputs, such as questions, retrieved documents, generated answers, and citations, are in textual form. Re- cently, vision-based RAG pipelines have emerged, where the retrieved documents are represented as screenshot images (Ma et al., 2024; Faysse et al., 2024), and VLMs process both textual ques- tions and these document images to generate an- swers (Riedler and Langer, 2024; Xia et al., 2024; Yu et al., 2024; Cho et al., 2024). Compared to traditional text-only RAG, vision-based RAG can leverage structured and visual information from documents, such as tables,