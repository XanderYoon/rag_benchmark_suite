q, the identifier i of the most relevant doc- ument d∗ in D, and a bounding box coordinates Bd∗ = [(x1, y1), (x2, y2)] within d∗ that highlight the content supporting the generated answer a. In a vision-based RAG setup, user queries are textual, while all documents in the corpus C are screenshots of documents (e.g., webpages or PDF pages) provided as image inputs. 3.2 Generation with Visual Source Attribution This paper focuses on VISA within the generation component of vision-based RAG systems. As dis- cussed in the previous section, VISA must handle multimodal input. To achieve this, we leverage VLMs for implementing VISA. Specifically, for a given query and a set of retrieved candidate docu- ments (i.e., screenshots of documents), the system processes the inputs as follows: query tokens are directly input into the language model, while docu- ment screenshots are first processed by the image encoder to extract image representations, which are then fed into the language model. The language model subsequently generates the answer, the identifier of the relevant document, and the xy-coordinates of the bounding box’s top-left and bottom-right corner on the content that sup- ports the generated answer. Notably, this entire process can be framed as a next-token prediction task. Finally, the generated identifier and bounding box coordinates are used to draw the bounding box on the target document screenshot, which is pre- sented to the user along with the generated answer. Technically, existing instruction-tuned VLMs, such as Qwen2-VL-72B (Wang et al., 2024), can potentially be prompted to perform VISA in a zero- shot manner. However, we find that VISA remains a challenging task. Consequently, further super- vised fine-tuning on a dedicated VISA task dataset is necessary. In the next section, we introduce the datasets we crafted specifically for training and evaluating VISA. 3.3 Dataset