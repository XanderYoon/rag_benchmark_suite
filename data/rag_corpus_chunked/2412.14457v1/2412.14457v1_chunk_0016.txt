original documents with proper location and granularity. Fine-tuning on our crafted training data enables the model to effectively execute the task. In the single-candidate setup, where the model processes only the relevant document, fine-tuned models demonstrate substantial gains compared to zero- shot prompting a much larger model. On Wiki- VISA, the 7B variant achieves 54.2% bounding box accuracy and 65.2% answer accuracy, while on Paper-VISA, the corresponding scores reach 68.2% and 43.8%. Performance in the multi-candidate setting, which more closely mirrors real-world retrieval- augmented generation (RAG) systems, shows simi- lar trends. The 7B model achieves 41.6% bounding box accuracy and 51.1% answer accuracy when handling three candidate documents, including cases where no relevant document is present. This demonstrates the modelâ€™s capability to identify rel- evant sources among multiple documents while enabling fine-grained attribution. However, when comparing the multi-candidates, oracle in candi- dates setting, We can see the model facing chal- lenges when handling multiple candidates com- pared to just handling a single relevant document. E.g. on Wiki-VISA, bounding box accuracy for 7B model is 37.7% on average which is 17 points lower than the corresponding single candidate set- ting. Showing that visual source attribution among multi-candidates is much harder than just locating the source element in a single one. It further demonstrates that the effectiveness Train Data Wiki-VISA Paper-VISA Average [<1] Passage [>1] Passage Non-PassageAverage Passage Non-Passage bbx ans bbx ans bbx ans bbx ans bbx ans bbx ans bbx ans Wiki 54.2 65.2 75.6 66.5 50.1 56.0 36.8 73.1 27.8 36.2 20.5 32.6 35.1 39.7 Paper 0.2 42.6 0 46.3 0.4 33.5 0.1 48.1 68.2 43.8 58.1 41.6 78.2 45.9 FineWeb 37.6 50.2 48.9 45.1 57.3 52.3 6.6 53.1 22.0 43.3 26.5 41.7 17.4 44.9 Wiki+Fineweb 58.2 65.3 68.7 66.6 61.7 57.1 44.1 72.1 21.0 43.1 18.5 42.2 23.4