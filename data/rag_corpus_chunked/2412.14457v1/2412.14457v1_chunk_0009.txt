answer is derived. We then draw a bounding box around this element to obtain thecoordinates. Notably, the answers in this dataset can come from various elements, such as passages, tables, lists, or images within the webpage. Since the ques- tions and answers in Wiki-VISA are human-judged, we consider this dataset a high-quality, supervised dataset and evaluation for VISA on general knowl- edge, with Wikipedia webpage. Paper-VISA is derived from PubLayNet (Zhong et al., 2019), a dataset originally designed for doc- ument layout analysis of single page PubMed PDF documents. PubLayNet provides bounding box coordinates and class labels (e.g., title, text, table, figure, etc.) for each element in a paperâ€™s PDF screenshot. However, the dataset does not include queries or answers associated with each document. To address this limitation, we leverage instruction- tuned VLMs (e.g. Qwen2-VL-72B) to syntheti- 1https://pypi.org/project/selenium/ cally generate queries and answers. Specifically, for each paper screenshot sample in the PubLayNet training data, we select a bounding box within the sample and overlay it on the screenshot. The mod- ified screenshot is then input to the VLM with a prompt designed to instruct the model to generate a question and a short answer based on the content within the bounding box. See Appendix A.1 for the prompt details and generation example. By aug- menting the original PubLayNet in this way, we create synthetic queries and answers, enabling it to support VISA training. We consider the result- ing Paper-VISA dataset as synthetic training and evaluation for scientific paper PDFs in the medical domain. FineWeb-VISA is based on the FineWeb-edu corpus (Penedo et al., 2024), a high-quality text corpus of crawled webpages. We sampled 60k web- page URLs and used Selenium to capture screen- shots of diverse, content-rich webpages. A passage containing more than 50 words was randomly se- lected as