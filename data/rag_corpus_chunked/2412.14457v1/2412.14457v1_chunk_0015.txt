Appendix A.3 to for- mat the model’s input and output. The training objective for both setups was next- token prediction with cross-entropy loss. We fine- tuned the models for two epochs in the single- candidate setting, using LoRA with a learning rate of 1e-4, a batch size of 64, and 4×H100 GPUs. For the multi-candidate setting, we initialized the mod- els with weights from the single-candidate setup and trained for one epoch with the same learning rate. We froze the image encoder to reduce GPU memory usage in the multi-candidate setting. During the training, random cropping was ap- plied outside of the bounding box. This augmen- tation exposed the model to varying input sizes, which enhanced its zero-shot effectiveness on un- seen document layouts. Bounding box targets were represented using absolute coordinate values. We also explored normalizing the scale of bounding box coordinates to values in the range[0-1]. Details can be found in Section 6.3. 5 Experimental Results Table 2 presents the performance of VISA on the Wiki-VISA and Paper-VISA datasets across dif- ferent experimental settings. Zero-shot prompting results reveal the difficulty of directly applying state-of-the-art VLMs to the visual source attribu- tion task. QWen2-VL-72B achieves a reasonable answer generation accuracy of 60.4% on average on Wiki-VISA but fails to deliver effective bound- ing box predictions, with only 1.5% accuracy. This observation is consistent on Paper-VISA. These highlight the limitations of existing VLMs in pin- pointing the source evidence in original documents with proper location and granularity. Fine-tuning on our crafted training data enables the model to effectively execute the task. In the single-candidate setup, where the model processes only the relevant document, fine-tuned models demonstrate substantial gains compared to zero- shot prompting a much larger model. On Wiki- VISA, the 7B variant achieves 54.2% bounding box accuracy and 65.2%