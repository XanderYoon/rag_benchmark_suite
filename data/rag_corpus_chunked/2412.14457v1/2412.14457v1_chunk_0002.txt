ument screenshots for retrieval (Ma et al., 2024), we ask whether source attribution can also be in- tegrated into such a unified visual paradigm to es- tablish a fully visual, end-to-end verifiable RAG pipeline that is both user-friendly and effective? To this end, we propose Retrieval Augmented Generation with Visual Source Attribution (VISA). In our approach, a large vision-language model (VLM) processes single or multiple retrieved docu- ment images and not only generates an answer to the user query but also returns the bounding box of the relevant region within the evidence document. As illustrated in Figure 1, this method enables di- rect attribution by visually pinpointing the exact position within the document, allowing users to quickly check the supporting evidence within the arXiv:2412.14457v1 [cs.IR] 19 Dec 2024 HTML parser , OCR, et c. R etrie v ed Candidat es Extr act ed T e xt R etrie v ed Candidat es Gener at e Gener at e LLMT e xt R etrie v er R ef er ence Answ er [1] VLM R ef er ence Answ er Document Scr eenshot R etrie v al T e xtual Sour ce A ttribution Visual Sour ce A ttribution (VIS A) (x, y) Quer y Visual R etrie v er Quer y T e xt R etrie v ala b losing cont e xt dir ect & user -friendly OR har d t o locat e document -le v el passage-le v el Figure 1: Comparison between (a) Text-based generation with source attribution in a RAG pipeline. and (b) Visual-based generation with source attribution in a V-RAG pipeline. VISA directly pinpoint the source evidence of the answer for user query in the original document with a bounding box. original context for the generated answer. VLMs are not restricted by document format