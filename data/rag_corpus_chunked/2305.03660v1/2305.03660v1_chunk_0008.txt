to modulate the report generation outputs per user requirements to enable a wide variety of usage patterns across diﬀerent clinical settings. We also measure the hallucinations from RAG to pivot the application of the proposed approach in a real-world clinical setting. 3. Methods In this paper we propose radiology report generation as a data augmented generation task using large language models like text-davinci-003, gpt-3.5-turbo and gpt-4. Our hy- pothesis is that it is not required to have domain speciﬁc generative models, but domain speciﬁc retrievers. We can leverage embeddings from domain speciﬁc encoders for the data retrieval task and use the retrieved data for augmentation using a general domain generative model. We build on top of the work by CXR-RePaiR [Endo et al. (2021)] and CXR-ReDonE [Ramesh et al. (2022)] by considering the problem of radiology report generation as retrieval task but adding a generative step using the retrieved impressions. We use the contrastively pretrained vision language model ALBEF [Li et al. (2021)] from CXR-ReDonE to generate the image and text embeddings from the radiology images and reports. For selecting the top K records for data augmentations, we tried both report level corpus R = r1, ..., rn and sentence level corpus S = s1, ..., sn as in CXR-RePaiR. We use the ALBEF [Li et al. (2021)] model from CXR-ReDonE to generate contrastively aligned text embeddings for the radiology report text corpus using the CXR-PRO dataset 5 CXR-RePaiR-Gen [Ramesh et al.] and index it in the vector database. We use the same model for generating the contrastively aligned image embedding for the input radiology image x and use the embedding to retrieve the top-K records from the reports or sentences corpus based on dot- product similarity. The top-K reports or sentences that have the highest similarity to the in-