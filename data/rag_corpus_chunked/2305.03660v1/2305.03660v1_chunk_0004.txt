contrastively pretrained ALBEF model [Li et al. (2021)] from CXR-ReDonE to generate the vision-language aligned embeddings for a database of radiology reports. The same model is used to generate the embedding for an input radiology image. As the image and text embeddings were aligned during the contrastive pre-training, the most relevant text radiology text (reports or sentences) is re- trieved for an input x-ray image based on the similarity of the input image embeddings to the radiology report embeddings. A consolidated radiology report impression is generated from the ﬁltered set of records using the OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 models. RAG based approach not only makes the radiology report generations grounded on the relevant radiology text retrieved from the radiology text corpus but also allows the user to inject user intents as instructions and few shot examples as part of the generation process via prompt engineering to generate content in the required format applicable for the clinical setting. Generalizable Insights about Machine Learning in the Context of Healthcare Our approach brings the below key insights for ML in healthcare: • Retrieval augmented generation can help bridge the advantages of various domain speciﬁc healthcare encoder models with that of the general domain generative models leveraging the best of both models. We show this improves the clinical metrics. • We also measure the radiology report generations for hallucinations by comparing the LLM generated response with the retrieved radiology text from the radiology reports or sentences corpus. This can help in decision making when planning to practically use these systems in a real clinical setting. • Our paper also shows how we can leverage prompt engineering in LLM to inject user intents and requirements to produce radiology reports in diﬀerent output formats relevant for the downstream application with few-shot learning. Our approach achieves