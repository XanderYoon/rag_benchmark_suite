which can generate relevant content based on instructive prompts in a zero-shot or few-shot setting for a wide variety of downstream tasks, it would be useful to explore how they can be leveraged for the work of radiology report generation to assist the radiologists. These models, however, lack the up-to-date information or domain speciﬁc information required speciﬁcally in a medical domain setting. Updated and relevant domain speciﬁc content when available for these models during the time of generation can allow to extend the capabilities of these large language models to do tasks with data, they were not exposed to during the training phase. 2 CXR-RePaiR-Gen In-addition we can leverage the instructions following capabilities of these models to elicit the required responses we require from these models. This additional context available to the LLMs for generations makes them hallucinate less. We are motivated by the advantages of Retrieval Augmented Generation (RAG) experimented in the work by Lewis et al. (2020) which showed that the generations from RAG are more strongly grounded in real factual knowledge causing less hallucinations and its broad application for various downstream tasks called out in the “Broader Impact” section of the paper. We propose Contrastive X-ray-Report Pair Retrieval based Generation (CXR-RePaiR- Gen) as a RAG based methodology for radiology report generation extending on top of the work by CXR-RepaiR [Endo et al. (2021)] and CXR-ReDonE [Ramesh et al. (2022)]. As illustrated in Figure 1, we leverage the contrastively pretrained ALBEF model [Li et al. (2021)] from CXR-ReDonE to generate the vision-language aligned embeddings for a database of radiology reports. The same model is used to generate the embedding for an input radiology image. As the image and text embeddings were aligned during the contrastive pre-training, the most relevant text radiology text (reports or sentences) is re- trieved