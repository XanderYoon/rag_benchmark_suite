may be required to give precise and concise phrases about the radiology image. 4.2. Evaluation Approach We evaluate the free text radiology impressions generated using the LLMs from the retrieved records of the report level corpus and sentence level corpus. For sentence level corpus we evaluate the impressions generated from top K sentence retrievals with K= 1, 2, 3. Our baselines are the impressions retrieved from CXR-ReDonE. We evaluate on the two semantic metrics – BERTScore [Zhang et al. (2019)] andSemb [Endo et al. (2021)] to see the similarity of the generation to the ground truth impression. We see this more meaningful as in the medical context phrases like lung collapse can represent atelectasis though the exact word may not be in the sentence. BERTScore computes a similarity score for each token in the predicted impressions with each token in the ground truth impressions. Token level similarity is computed using contextual embeddings instead of direct token matches. Semb uses CheXbert model [Smit et al. (2020)] to calculate the cosine similarity between the embeddings from the ﬁnal hidden state representations. To evaluate the overlap in clinical entities included in both the generated and ground truth reports we use RadGraph F1, a metric proposed by Yu et al. (2022) that makes use of a RadGraph model [Jain et al. (2021)] to evaluate the overlap in clinical entities. We also generate the radiology report impressions in a structured json format to evaluate if we can format the output the generated impression as per user requirements. At this point in time these are not quantitatively measured but we show qualitative outputs for structured generation. 4.3. Measuring Hallucinations We also qualitatively and quantitatively evaluate if the report impression summary hallu- cinates from the top K sentences corpus given to the LLM as context. We use