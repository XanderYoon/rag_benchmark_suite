ALBEF [Li et al. (2021)] and set the current SOTA for the radiology report generation task. They also used the RadGraph F1 [Yu et al. (2022)] score as an additional metric to measure the completeness and accuracy of the clinical entities available in the predicted report using the RadGraph model [Jain et al. (2021)] With the rise of the LLMs, Retrieval Augmented Generation (RAG) was introduced in the work by Lewis et al. (2020) which brought some key advantages of leveraging external knowledge sources to augment the knowledge of LLMs to do a task. LLMs generations are also strongly grounded in real factual knowledge which makes it “hallucinate” less and produce generations that are more factual. The broarder impact statement from the paper mentioned its application in a wide variety of scenarios, for example by endowing it with a medical index. We in this work endow the LLMs with the index of radiology report text and use it as a knowledge base to allow LLMs generate a radiology report impression for an input radiology image. To enable the multimodal retrieval of Images and Text, we use the constrastively pretrained vision language model from CXR-ReDonE [Ramesh et al. (2022)] which improved the work of CXR-RepaiR [Endo et al. (2021)] for multimodal retrievals to see if augmented generation on top of these retrievals can further push the report generation benchmark. We also see if we can use the general capabilities of LLMs to modulate the report generation outputs per user requirements to enable a wide variety of usage patterns across diﬀerent clinical settings. We also measure the hallucinations from RAG to pivot the application of the proposed approach in a real-world clinical setting. 3. Methods In this paper we propose radiology report generation as a data augmented generation task using large language