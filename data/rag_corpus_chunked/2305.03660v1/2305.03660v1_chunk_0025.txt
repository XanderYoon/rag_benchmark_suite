details or dupli- cates and use the template database for performing the retrievals for better quality report generation. We see that RAG based generation can address this limitation via eﬀective prompt engineering and also using approaches like structured content extraction with few shot prompts to extract only the attributes of interest and produce a more concise, precise, and complete impression summary from the retrieved records. 5.1. Limitations We note that as RAG impression generation eﬃcacy is sensitive to the retrieved sentences from the corpus which is based on the embeddings from the constrastively pretrained model meaning that the clinical entities generated by RAG is limited by the clinical entities from the retrieval unless we allow it to use the general knowledge it has. In our experiments we restricted the model to use the context while making the generations via prompt instructions to alleviate hallucinations. So it is imperative that the retrieval model is able to bring in the all the relevant clinical entities for the generation. 5.2. Future Work We see that RAG based report generations can beneﬁt from more advanced contrastive models that are more sensitive to ﬁne details of the radiology image such as severity, size, position, pathology, and other attributes of interest from the radiology image. Advances in prompt engineering for medical text is another area to explore so that we can elicit the LLM more eﬃciently for a speciﬁc downstream task. 18 CXR-RePaiR-Gen Table 9: Examples of cases where the Semb scores were less between the retrieved context sentence corpus and the generations from the LLM model. Semb uses CheXbert model to calculate the cosine similarity between the embeddings from the ﬁnal hidden state repre- sentations and can give an indication of hallucination. We see there are no hallucinations. Semb Context Records (K=3) RAG Impression 0.1508