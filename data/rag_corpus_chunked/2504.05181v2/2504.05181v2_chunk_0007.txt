Direct Document Relevance Optimization for Generative Information Retrieval SIGIR â€™25, July 13â€“18, 2025, Padua, Italy 3 Preliminaries and Motivations 3.1 Generative Information Retrieval (GenIR) GenIR models build on large pre-trained language models, such as T5 [41] and BART [27], and are designed to take a query string and generate a ranked list of document identifiers (docids) based on their generation probabilities, ordered in descending sequence. Each document ğ‘‘ is assigned a unique identifier docid = (docid1, docid2, . . . ,docidğ¿), where ğ¿ is the length of the identifier, and the model processes the query ğ‘ to autoregressively generate the corresponding docid using a scoring function defined as: ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ (docid | ğ‘) = ğ¿Ã– ğ‘–=1 ğ‘ğœƒ (docidğ‘– | docid1:ğ‘– âˆ’1, ğ‘), (1) where ğ‘ğœƒ denotes the generative retrieval model parameterized by ğœƒ, and docidğ‘– is the ğ‘–-th token of the docid for document ğ‘‘. The generation continues until a special end-of-sequence (EOS) token is decoded. Training is performed using a multi-task setup that combines indexing and fine-tuning, which yields better results than training these tasks separately [60]. During indexing, the model memorizes the document collection and maps each documentâ€™s text to its corresponding docid. Fine-tuning then refines this mapping by optimizing query-to-docid associations. The model is optimized via the following loss ğ¿ğœƒ ğ·ğ‘†ğ¼ with teacher forcing [66]: âˆ‘ï¸ ğ‘‘ğ‘– âˆˆğ· log ğ‘ƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ ğ‘– | ğ‘‡ 5ğœƒ (ğ‘‘ğ‘– )) + âˆ‘ï¸ ğ‘ ğ‘— âˆˆğ‘„ log ğ‘ƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ ğ‘– | ğ‘‡ 5ğœƒ (ğ‘ ğ‘— )), (2) where ğ· represents the document set, and ğ‘„ denotes the query set. This loss function enables parameter updates during both indexing and fine-tuning, enhancing the modelâ€™s ability to generate the most relevant docid for a given query or document. The retrieval phase employs a (constrained) beam search algorithm to decode the most probable docid, with