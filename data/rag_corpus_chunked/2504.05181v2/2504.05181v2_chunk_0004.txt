relevance, aligning the model more closely with the query. The SFT phase serves as a pretraining step, aligning the model with initial relevance signals from training data and providing a strong foundation for the pairwise ranker to further fine-tune document ranking effectiveness. Experiments conducted on the MS MARCO document rank- ing [3] and Natural Questions (NQ) [23] benchmarks demonstrate the effectiveness of DDRO in improving retrieval accuracy, outper- forming multiple baselines. Moreover, DDRO maintains compet- itive performance with established baselines on broader metrics like R@10, demonstrating robustness across evaluation criteria. An ablation study further highlights the contributions of pairwise ranking optimization to the observed performance improvements. Main contributions. We introduce direct document relevance optimization (DDRO), a pairwise ranking approach that aligns do- cid generation with document-level relevance judgments. This ap- proach ensures that docids are generated not only based on token- level likelihood but also according to their relevance to the userâ€™s query. DDRO unifies training objectives within a single framework, optimizing directly for document-level relevance. Experimental re- sults demonstrate improvements in retrieval accuracy, highlighting the effectiveness of the proposed approach in enhancing generative retrieval models for relevance-based ranking. Reproducibility. To promote reproducibility in GenIR, we open- source our codebase and make checkpoints publicly available.1 2 Related Work Reward modeling. Recent efforts in GenIR have sought to bridge the gap between token-level optimization and document-level rele- vance. GenRRL [76] addresses this issue using RLRF to align docid generation with query relevance. While effective, this approach requires a robust reward model training and reinforcement learn- ing fine-tuning, both of which are resource-intensive and prone to instability. Developing a reliable reward model demands substan- tial labeled data, and reinforcement learning fine-tuning involves 1https://github.com/kidist-amde/DDRO-Direct-Document-Relevance-Optimization/ tree/main extensive hyperparameter tuning [40], contributing to training in- stability and scalability challenges for large-scale applications. In contrast, we