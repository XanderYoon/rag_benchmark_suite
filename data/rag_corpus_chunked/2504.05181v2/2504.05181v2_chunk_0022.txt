sequences. 6 Experimental Evaluation and Results Our evaluation of DDRO focuses on the following questions: RQ1 How does DDRO compare to RLRF-based methods, such as GenRRL, in terms of retrieval performance while avoiding the complexities of reward modeling and reinforcement learning? RQ2 How does DDRO perform relative to established baselines in terms of retrieval accuracy and ranking consistency on benchmark datasets? RQ3 What is the impact of pairwise ranking optimization on the performance of DDRO? RQ4 How robust is DDRO across datasets with varying character- istics? RQ5 How does DDRO balance relevance across the ranked list in generative retrieval models, and what impact does it have on overall ranking quality? 6.1 Comparison with Reinforcement Learning-Based Methods To address RQ1, we compare DDRO with GenRRL [76] on both datasets. Results are presented in Table 1 and Table 2. We observe the following: (i) On MS MARCO,DDRO (TU) achieves the highest scores in early precision-focused metrics. Specifically, it outperforms GenRRL(Sum) by 15.06% in R@1 and 7.4% in MRR@10, highlighting its effectiveness in ranking the most relevant docu- ment at the top. While GenRRL (Sum) performs better in the R@10 metric, DDRO (TU) achieves comparable results with a simplified optimization process, avoiding the need for a reward model and re- inforcement learning. (ii) On NQ,DDRO (PQ) outperforms GenRRL (Sum) by 34.69% in R@1 and 19.87% in MRR@10, further validat- ing its effectiveness in retrieving relevant documents within the top ranks. GenRRL variants achieve higher R@10 scores, likely benefiting from multi-signal learning and listwise optimization strategies, potentially improving broader document retrieval. Sum- mary-based docids may aid performance on knowledge-intensive queries. (iii) DDRO (TU) and DDRO (PQ) show different trends across datasets: TU performs better on MS300K, while PQ excels on NQ. This disparity likely stems from higher document quality in NQ, where the rich