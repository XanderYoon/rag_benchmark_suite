optimize generative models for alignment with document-level relevance. RLRF optimizes rewards while ensuring alignment with human preferences using a KL divergence constraint [2, 14, 30, 35, 39, 47, 76]. This method refines model predictions using a learned reward function, as formalized in prior research [20, 21]: maxğœ‹ğœƒ Eğ‘¥âˆ¼D ,ğ‘¦âˆ¼ğœ‹ğœƒ (ğ‘¦ |ğ‘¥ )  ğ‘Ÿğœ™ (ğ‘¥, ğ‘¦)  âˆ’ ğ›½DKL  ğœ‹ğœƒ (ğ‘¦|ğ‘¥) âˆ¥ğœ‹ref(ğ‘¦|ğ‘¥)  , (3) where ğ›½ is a parameter controlling deviation from the base refer- ence policy ğœ‹ref, which is typically the initial supervised fine-tuned model. This constraint prevents the model from straying too far from the data distribution used to train the reward function, pre- serving output diversity and avoiding overfitting to high-reward responses. Since language generation is non-differentiable, rein- forcement learning (RL) techniques are widely used. A widely rec- ognized approach [39, 47, 81] optimizes the reward function using Proximal Policy Optimization (PPO) [46], and defines the reward function as: ğ‘Ÿ (ğ‘¥, ğ‘¦) = ğ‘Ÿğœ™ (ğ‘¥, ğ‘¦) âˆ’ ğ›½ (log ğœ‹ğœƒ (ğ‘¦|ğ‘¥) âˆ’ log ğœ‹ref(ğ‘¦|ğ‘¥)) . (4) GenRRL [76] trains a reward model using relevance-annotated data derived from BM25 [44], DPR [22], and LLaMA-13b [61]. This re- ward model guides reinforcement learning to optimize the language modelâ€™s policy for generating high-reward outputs, with a KL diver- gence constraint ensuring alignment with the original supervised fine-tuned model or the reference policyğœ‹ref. The optimization pro- cess involves supervised fine-tuning using negative log-likelihood, pairwise ranking loss for the reward model, and reinforcement learning techniques incorporating pointwise, pairwise, and list- wise approaches to enhance ranking performance. While effective, this approach introduces considerable complexity, requiring the training of multiple models and sampling from the policy during training, which substantially increases computational costs. SIGIR â€™25, July 13â€“18, 2025, Padua, Italy Kidist Amde Mekonnen et al. 2 3 6 â€¦â€¦ Model ğœ‹!"#$ Pseudo-query