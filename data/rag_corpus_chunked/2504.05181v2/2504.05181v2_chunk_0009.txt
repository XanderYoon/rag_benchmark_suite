such as RankNet [5] and LambdaRank [7], in that it optimizes a margin between relevant and non-relevant documents. It differs in that the ranking signal is used to supervise the generation of structured docid sequences via a generative decoder. Unlike typical L2R approaches that score docu- ments retrieved by an external system, DDRO learns to produce do- cids directly, making it end-to-end generative. This integration of se- quence modeling and pairwise supervision is a key distinction from prior L2R pipelines. (iii) List-wise approaches [24, 67], optimize the entire ranked list:ğ¿list = Ã ğ‘ L (Softmax( Ë†ğ‘  (ğ‘, Ë†ğœ‹)), Softmax(ğ‘  (ğ‘, ğœ‹))), where Ë†ğœ‹ and ğœ‹ are the predicted and ground-truth lists, respectively. In GenIR, Tang et al. [53] introduces a position-aware list-level ob- jective to learn the relevance. As we focus on pair-wise approaches, comparisons with list-wise methods are left for future work. A fundamental challenge in GenIR stems from the inherent mis- alignment between the optimization objectives of autoregressive models and the overarching objectives of document ranking tasks. Training GenIR models solely to generate docids can be treat as the point-wise approach, which is often insufficient for achieving effective ranking. Addressing this challenge necessitates the de- velopment of a robust framework that enables GenIR models to directly learn to rank [29, 76, 77]. 3.3 Reinforcement Learning from Relevance Feedback (RLRF) To address the aforementioned limitations, Zhou et al. [76] propose GenRRL, a generative retrieval model based on RLRF to optimize generative models for alignment with document-level relevance. RLRF optimizes rewards while ensuring alignment with human preferences using a KL divergence constraint [2, 14, 30, 35, 39, 47, 76]. This method refines model predictions using a learned reward function, as formalized in prior research [20, 21]: maxğœ‹ğœƒ Eğ‘¥âˆ¼D ,ğ‘¦âˆ¼ğœ‹ğœƒ (ğ‘¦ |ğ‘¥ )  ğ‘Ÿğœ™ (ğ‘¥, ğ‘¦)  âˆ’ ğ›½DKL 