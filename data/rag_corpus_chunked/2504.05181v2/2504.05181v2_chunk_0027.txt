4: Effect of KL constraint strength (ğ›½) on DDRO perfor- mance. A moderate value ( ğ›½ = 0.4) yields the best MRR@10, while under- or over-regularization degrades performance. Table 4: Ablation study evaluating the impact of pairwise ranking optimization on DDRO performance across the MS300K and NQ320K datasets. Statistical significance is as- sessed using a paired t-test with a significance threshold of ğ‘ < 0.05. Statistically significant improvements ( ğ‘ < 0.05) are marked with a dagger symbol ( â€ ), while non-significant improvements are underlined. Abbreviations: PQ â€“ Product Quantization; TU â€“ Title + URL. MS MARCO doc Model R@1 R@5 R@10 MRR@10 DDRO (PQ) 32.92 64.36â€  73.02â€  45.76 w/o pairwise ranking 32.18 62.62 71.29 44.79 DDRO (TU) 38.24 66.46â€  74.01â€  50.07 w/o pairwise ranking 38.12 64.60 72.90 49.18 Natural Questions Model R@1 R@5 R@10 MRR@10 DDRO (PQ) 48.92 â€  64.10â€  67.31â€  55.51â€  w/o pairwise ranking 44.19 58.44 62.23 50.48 DDRO (TU) 40.86 â€  53.12â€  55.98â€  45.99â€  w/o pairwise ranking 39.58 50.50 53.53 44.32 show lower early ranking performance, but remain competitive at broader levels. (ii) Dense retrieval baselines, including DPR and ANCE, improve early ranking metrics over term-based methods. (iii) Generative retrieval baselines, such as ROGER-Ultron (TU) and LTRGR, perform well across all metrics. The proposed DDRO achieves the highest overall performance. Specifically, DDRO (PQ) surpasses the best-performing baseline, ROGER-Ultron (TU), by 36.27% in R@1 and 23.58% in MRR@10. Impact of Dataset Characteristics and Docid Selection. The docid design is a critical factor influencing performance in GenIR, we further analyze the differences in performance among various de- signs within our proposedDDRO. Our analysis underscores the crit- ical impact of docid design on retrieval performance across datasets. DDRO (TU) excels on MS MARCO, where shorter, keyword-driven queries align well with title and URL-based docids that capture Lightweight and Direct Document