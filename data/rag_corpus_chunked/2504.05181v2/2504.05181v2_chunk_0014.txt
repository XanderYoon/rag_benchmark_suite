to achieve alignment with the ground truth. Indexing task. To memorize the corpus, the indexing task learns associations between documents and docids, making the document input format a crucial factor. Inspired by the indexing strategy pro- posed in [79], we generate self-supervised learning signals directly from the document corpus. Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval SIGIR â€™25, July 13â€“18, 2025, Padua, Italy âˆ‡ğœƒ LDDRO  ğœ‹ğœƒ ; ğœ‹ref  = âˆ’ğ›½E(ğ‘,ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ +,ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ’ )âˆ¼D [ ğœ Ë†ğ‘Ÿğœƒ (ğ‘, ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ’) âˆ’ Ë†ğ‘Ÿğœƒ ğ‘, ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ + | {z } higher weight when reward estimate is wrong Ã— [ âˆ‡ ğœƒ log ğœ‹ ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ + | ğ‘ | {z } increase likelihood of ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ + âˆ’ âˆ‡ ğœƒ log ğœ‹ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ’ | ğ‘) | {z } decrease likelihood of ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ’ ]], (7) Figure 2: Gradient for direct learning-to-rank optimization using relevance feedback. Text segments are mapped to their corresponding docids, en- abling the model to link document passages with their broader context [8, 79]. Each document is divided into fixed-size passages, paired with the documentâ€™s docid to create passage-to-docid pairs. For a document containing ğ‘ terms, {ğ‘¤ 1, ğ‘¤2, . . . , ğ‘¤ğ‘ }, multiple passage-to-docid pairs are generated as follows: passage : {ğ‘¤ğ‘–, ğ‘¤ğ‘–+1, . . . , ğ‘¤ğ‘–+ğ‘šâˆ’1} â†’ ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘, (7) where ğ‘– is the starting term of a passage, and ğ‘š is the fixed passage length. To emphasize a documentâ€™s core semantic content, terms are prioritized by their ğ‘¡ ğ‘“ -ğ‘–ğ‘‘ ğ‘“ scores [44], with a subset of high- scoring terms selected to form a compressed representation, which is then mapped to the documentâ€™s docid: terms : {ğ‘¤ ğ‘, ğ‘¤ğ‘, ğ‘¤ğ‘ } â†’ ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘, (8) where ğ‘¤ ğ‘, ğ‘¤ğ‘, ğ‘¤ğ‘ are key terms selected based on their ğ‘¡ ğ‘“ -ğ‘–ğ‘‘ ğ‘“ scores.