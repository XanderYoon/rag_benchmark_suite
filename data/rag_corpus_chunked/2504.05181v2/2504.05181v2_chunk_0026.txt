59.02 68.78 42.02 ROGER-Ultron (TU) [77] 33.07 63.93 75.13 46.35 MINDER (SI) 29.98 58.37 71.92 42.51 LTRGR (SI) 32.69 64.37 72.43 47.85 Ours DDRO (PQ) 32.92 64.36 73.02 45.76 DDRO (TU) 38.24â€  66.46â€  74.01 50.07â€  between the DDRO policy ğœ‹ğœƒ and the reference policy ğœ‹ ref , on retrieval performance. Figure 4 shows results for different ğ›½ values on MS MARCO and Natural Questions (NQ). A moderate setting (ğ›½ = 0.4) consistently yields the best MRR@10 across both datasets. Smaller values (e.g., ğ›½ = 0.2) lead to under-regularization, resulting in unstable and suboptimal learning. In contrast, larger values (e.g., ğ›½ = 0.6) impose excessive regularization, restricting the modelâ€™s ability to adapt, and thus degrading performance. These results highlight the sensitivity of DDRO to the KL constraint, suggest- ing the importance of tuning ğ›½ to balance learning flexibility and regularization. 6.4 Robustness Analysis Across Datasets To address RQ4, additional experiments were conducted NQ dataset to evaluate the robustness of DDRO across datasets with varying characteristics. The analysis focuses on two aspects: (i) comparing DDRO retrieval performance with baseline models across different categories Table 5, and (ii) examining the impact of various docid design choices on retrieval effectiveness. Comparison to baseline retrieval models. The performance comparison on the NQ320k across different retrieval baselines is as follow: (i) Term-based baselines, such as BM25 and DocT5Query, 0.20 0.40 0.49 0.60 35 40 45 50 55MRR@10 MSMARCO (PQ) MSMARCO (TU) NQ (PQ) NQ (TU) Figure 4: Effect of KL constraint strength (ğ›½) on DDRO perfor- mance. A moderate value ( ğ›½ = 0.4) yields the best MRR@10, while under- or over-regularization degrades performance. Table 4: Ablation study evaluating the impact of pairwise ranking optimization on DDRO performance across the MS300K and NQ320K datasets. Statistical significance is as- sessed using a paired t-test with a significance