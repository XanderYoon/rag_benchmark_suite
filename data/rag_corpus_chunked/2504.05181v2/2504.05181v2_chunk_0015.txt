content, terms are prioritized by their ğ‘¡ ğ‘“ -ğ‘–ğ‘‘ ğ‘“ scores [44], with a subset of high- scoring terms selected to form a compressed representation, which is then mapped to the documentâ€™s docid: terms : {ğ‘¤ ğ‘, ğ‘¤ğ‘, ğ‘¤ğ‘ } â†’ ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘, (8) where ğ‘¤ ğ‘, ğ‘¤ğ‘, ğ‘¤ğ‘ are key terms selected based on their ğ‘¡ ğ‘“ -ğ‘–ğ‘‘ ğ‘“ scores. Retrieval task. During retrieval, a pre-trained language model is fine-tuned with supervised query-docid pairs to learn seman- tic mappings between queries and their corresponding docids. A key challenge in this process is the scarcity of labeled click data, which limits the ability to establish effective query-to-docid asso- ciations. To mitigate this, pseudo-queries are generated directly from the document corpus [ 64, 80]. Specifically, the docTTTTT- query [38] model is fine-tuned using supervised click data from the MS MARCO document and NQ datasets. For each document, an initial passage serves as input, and the model produces ğ‘˜ predicted queries using beam search, denoted as ğ‘„ = {ğ‘1, . . . , ğ‘ğ‘˜ }. These diverse datasets, including passage-to-docid pairs, super- vised query-docid pairs (derived from real-world relevance judg- ments), and synthetic pseudo-queries, are collectively used to train the ğ…ref ğœƒ model. Using these comprehensive and diverse training data, the SFT model acquires a robust understanding of query- to-document mappings and learns to generate relevant docids by optimizing token-level generation probabilities for a given query. Training objective. The model is trained with a sequence-to- sequence objective, aiming to maximize the likelihood of the target sequence through teacher forcing [66]. Given an input sequence ğ‘ , which can be any of the document formats or queries described above, the objective is defined as: ğ¿ğœƒ SFT = arg max ğœƒ log ğ‘ƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ— | ğ‘ , ğœ‹ ref ğœƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ )), (9) where ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ—