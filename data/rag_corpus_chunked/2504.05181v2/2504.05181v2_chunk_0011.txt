model, and reinforcement learning techniques incorporating pointwise, pairwise, and list- wise approaches to enhance ranking performance. While effective, this approach introduces considerable complexity, requiring the training of multiple models and sampling from the policy during training, which substantially increases computational costs. SIGIR â€™25, July 13â€“18, 2025, Padua, Italy Kidist Amde Mekonnen et al. 2 3 6 â€¦â€¦ Model ğœ‹!"#$ Pseudo-query Model ğœ‹!"#$ DocIDDocIDDocID Labeled query Model ğœ‹!"#$ DocID Query generation model Model ğœ‹!"#$Model ğœ‹! Labeled queryLabeled query DocID+DocID-DocID+DocID- Eq. (10) Eq. (7) Score(DocID+)Score(DocIDâˆ’)Score(DocID+)Score(DocIDâˆ’)Trainable parametersFrozen parameters DocumentDocument Corpus URL /title+domainProduct quantization code DocID Product quantization Step 1: DocIDconstructionStep 2: Supervised fine-tuningStep 3: Direct L2R optimization using relevance feedback Gradient updates Loss computation Passagetf-idfscore Figure 1: The proposed workflow comprises three key stages: (1) Construction of document identifiers (docids) including URL/title, domain, and product quantization codes; (2) Supervised fine-tuning of the retrieval model ğœ‹ ref ğœƒ using diverse data pairs; and (3) Freezing the trained reference policy model ğœ‹ ref ğœƒ and performing direct learning-to-rank (L2R) optimization on a policy model ğœ‹ğœƒ . Inspired by the work of Rafailov et al. [40], we propose direct doc- ument relevance optimization (DDRO), a streamlined approach de- signed to enhance the ranking capabilities ofGenIR models. DDRO directly optimizes GenIR models to learn document-level rank- ing without relying on explicit reward modeling or reinforcement learning. While our optimization is inspired by the DPO frame- work [40], its adaptation to GenIR is non-trivial. Unlike preference alignment for open-ended generation, our task involves optimizing structured docid generation under beam decoding constraints. Ad- ditionally, our method differs in both architecture (encoder-decoder vs. decoder-only) and objective (document ranking vs. preference alignment), requiring novel integration into GenIR pipelines. To the best of our knowledge, DDRO is the first method to apply preference-style optimization directly to generative document re- trieval by extending