relevance. While effective, this approach requires a robust reward model training and reinforcement learn- ing fine-tuning, both of which are resource-intensive and prone to instability. Developing a reliable reward model demands substan- tial labeled data, and reinforcement learning fine-tuning involves 1https://github.com/kidist-amde/DDRO-Direct-Document-Relevance-Optimization/ tree/main extensive hyperparameter tuning [40], contributing to training in- stability and scalability challenges for large-scale applications. In contrast, we proposeDDRO, a direct document-level relevance opti- mization method that eliminates the need for explicit reward model training and reinforcement learning fine-tuning, thereby reducing computational overhead and improving optimization efficiency. Dense-generative integration. Ranking-oriented generative re- trieval (ROGER) [77] combines dense and generative retrieval by using dense retrievers as intermediaries to provide relevance signals, bridging the gap between document ranking and docid generation. ROGER employs knowledge distillation from dense retrievers to enhance the generative model’s ranking capabilities, combining the strong relative ranking signals of dense retrieval with the flexibility of generative models. However, it relies on external dense retrievers and does not directly optimize for document-level relevance within the generative model’s training objectives. In contrast, DDRO elim- inates this dependency by incorporating pairwise ranking directly into the generative model’s optimization pipeline, ensuring align- ment with document-level relevance. Learning to rank in generative retrieval models. Similarly, LTRGR [29] incorporates a learning-to-rank (LTR) framework to address the gap between docid generation and document ranking. It introduces an additional training phase where the model is op- timized using a margin-based ranking loss, eliminating the need for a separate ranking step during inference. However, LTRGR focuses on optimizing passage ranking during the second phase, treating docid generation as a step toward this goal rather than fully integrating document-level relevance into the generative pro- cess. Consequently, the challenge of embedding document-level relevance directly into docid generation remains unaddressed. In contrast, DDRO integrates pairwise ranking directly into the