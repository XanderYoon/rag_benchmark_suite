the URL reverses to prioritize seman- tically meaningful segments. When a URL lacks descriptive content (e.g., uses numeric IDs or generic paths), we fall back to a combi- nation of the documentâ€™s title and domain name as an alternative identifier. Formally, this docid variant is defined as: ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ TU = ( reverse(URL), if the URL is semantically rich, title + domain, otherwise. (5) Product quantization codes (PQ). Building on prior work [10, 42, 70, 79], we adopt product quantization (PQ) to reduce the di- mensionality of document representations while maintaining their semantic integrity. PQ compresses document vectors into latent semantic tokens by employing K-means clustering to partition the latent vector space into clusters. Each document is then represented by the corresponding cluster center, forming a compact identifier that preserves the documentâ€™s core semantic features. The resulting docid is defined as: ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ ğ‘ƒğ‘„ = ğ‘ƒğ‘„ (Encoder(ğ‘‘)), (6) where the encoder is based on a pre-trained T5 model [36]. The clus- tering process generates ğ’Œ cluster centers across ğ’ groups, expand- ing the vocabulary by ğ’ Ã— ğ’Œ new tokens. This approach produces a semantically rich and efficient representation of each document. 4.2 Supervised Fine-tuning Supervised fine-tuning (SFT) enhances the retrieval capabilities of pre-trained language models by aligning them with task-specific data [1, 39, 47]. Based on the two basic operations of DSI [60], i.e., indexing and retrieval tasks, diverse data pairs are curated and optimized using a teacher forcing policy [66] to achieve alignment with the ground truth. Indexing task. To memorize the corpus, the indexing task learns associations between documents and docids, making the document input format a crucial factor. Inspired by the indexing strategy pro- posed in [79], we generate self-supervised learning signals directly from the document corpus. Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval SIGIR