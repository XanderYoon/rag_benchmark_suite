higher R@10 scores, likely benefiting from multi-signal learning and listwise optimization strategies, potentially improving broader document retrieval. Sum- mary-based docids may aid performance on knowledge-intensive queries. (iii) DDRO (TU) and DDRO (PQ) show different trends across datasets: TU performs better on MS300K, while PQ excels on NQ. This disparity likely stems from higher document quality in NQ, where the rich semantic content allows the generated PQs to convey more meaningful information. In contrast, MS300K, which is derived from web search logs, contains noisy content such as ads, Table 2: Performance comparison of GenRRL and DDRO on the NQ320K dataset. The best results are in bold. Results for cited models are sourced from their original papers. Abbre- viations: PQ – Product Quantization; TU – Title + URL; Sum – document summary. Model R@1 R@5 R@10 MRR@10 GenRRL (TU) [76] 35.79 56.49 70.96 45.73 GenRRL (Sum) [76] 36.32 57.42 71.49 46.31 DDRO (TU) 40.86 53.12 55.98 45.99 DDRO (PQ) 48.92 64.10 67.31 55.51 resulting in lower-quality PQs. Consequently, TU, which prioritizes keyword-based features, effectively captures the core content of MS300K, where the shorter, keyword-focused queries align well with surface-level signals such as titles and URLs. 6.2 Comparison with Established Baselines To address RQ2, Table 3 presents a comprehensive comparison of DDRO with baselines on the MS300K dataset. We can observe the followings: (i) The performance of dense retrieval baselines is gen- erally better than that of sparse retrieval baselines, likely because the former uses dense vectors to capture richer semantic infor- mation, which is consistent with earlier findings [31, 32]. (ii) The best-performing dense retrieval baseline, ANCE, outperforms oth- ers such as SEAL, NCI, and DSI-QG. This could be due to the fact that these generative retrieval baselines rely solely on maximum likelihood estimation (MLE) to learn relevance, which may not fully