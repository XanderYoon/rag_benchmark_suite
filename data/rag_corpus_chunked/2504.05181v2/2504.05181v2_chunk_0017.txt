while improving relevance-based rank- ing. The DDRO update guides the model toward producing outputs better aligned with relevance by utilizing pairwise comparisons, of- fering a simplified alternative to RL-based approaches. The gradient w.r.t. the model parametersğœƒ is defined as in Eq. 7, where Ë†ğ‘Ÿğœƒ (ğ‘, ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ ) = ğ›½ log ğœ‹ğœƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ | ğ‘) ğœ‹ref(ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ | ğ‘) (11) is the reward implicitly defined by the model ğœ‹ğœƒ and the reference model ğœ‹ref. The examples are weighted according to how much the implicit reward model Ë†ğ‘Ÿğœƒ overestimates the ranking of the non- relevant docid compared to the relevant docid. This weighting is scaled by ğ›½, reflecting the degree of misjudgment while considering the strength of the KL divergence constraint. The sigmoid function ğœ (Â·) ensures smooth optimization, and the model parameters ğœƒ are adjusted to increase the likelihood of the relevant docids over non-relevant ones. This reparameterization streamlines the train- ing process by eliminating the need for an explicit reward model and iterative fine-tuning, providing a more stable and efficient framework for aligning the retrieval model with document-level relevance objectives. 5 Experimental Settings 5.1 Datasets and Evaluation Metrics Datasets. We conduct our experiments using two widely recog- nized benchmarks: the MS MARCO Document Ranking dataset2 [3] and theNatural Questions (NQ)dataset3 [23]. The MS MARCO document ranking dataset is widely used for document ranking tasks and contains a large collection of queries and web pages. Fol- lowing prior work [65, 76, 78, 79], we use a subset with 320k docu- ments and 360k query-document pairs for training. The NQ dataset, 2https://microsoft.github.io/msmarco/Datasets.html#document-ranking-dataset 3https://ai.google.com/research/NaturalQuestions/download SIGIR â€™25, July 13â€“18, 2025, Padua, Italy Kidist Amde Mekonnen et al. Figure 3: Architecture of the DDRO model, which fine-tunes the retrieval model through direct learning-to-rank (L2R) optimization using relevance feedback. Unlike GenRRL [ 76], DDRO directly optimizes with