generated tokens. The generation process is controlled through (constrained) beam search [ 33, 60, 64, 70, 71, 80]. Docids could be predefined and remain static dur- ing training, making their careful design crucial for optimal re- trieval performance [58]. We classify docids into two categories based on their generation methodology and abstraction level. The first category, referred to as content-derived docids, includes iden- tifiers that are extracted directly from document elements such as titles [12, 13, 16, 25, 51, 56, 57, 59], n-grams [4, 11, 28, 29, 65], URLs [43, 76, 79, 82], and key terms [ 75]. These docids preserve surface-level textual characteristics and are closely tied to the orig- inal document content. In contrast, the second category, termed computationally-generated are derived using techniques like quan- tization [10, 42, 70, 71, 79] or hierarchical clustering algorithms [33, 48, 60, 64] to encode deeper semantics by abstracting raw doc- ument content into conceptual features. During training, GenIR models learn to associate document text with corresponding docids, embedding semantic information di- rectly into its parameters. During retrieval, docids are sequentially generated based on learned representations. By unifying indexing and retrieval within a transformer-based architecture, these models optimize both processes simultaneously [34, 60]. Challenges in GenIR. Despite recent advancements, GenIR mod- els face key limitations that hinder their effectiveness. These models typically rely on an auto-regressive loss function that optimizes the generation of individual docid tokens. However, this token- level optimization approach does not align with the broader goal of ranking tasks, which requires assessing the overall relevance of a document to the query. As a result, this misalignment often leads to suboptimal ranking performance. To address these challenges, it is crucial to align token generation with document-level relevance estimation to ensure more accurate, well-rounded retrieval outcomes. arXiv:2504.05181v2 [cs.IR] 24 Apr 2025