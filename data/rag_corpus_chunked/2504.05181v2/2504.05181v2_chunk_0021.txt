and lower (501‚Äì1000) ranks. Negatives were randomly sampled in roughly equal proportions, with 8 per query for NQ and 16 for MS MARCO. DDRO. The DDRO model was initialized with the pre-trained au- toregressive SFT model (see Section 4.2) and fine-tuned using the proposed direct learning-to-rank framework (see Section 4.3). Train- ing was performed using a modified Hugging Face TRLDPOTrainer [62], adapted for encoder-decoder models. A cosine learning rate scheduler with 1000 warm-up steps and early stopping was applied. The learning rate was set to 5e-6 for PQ-based docids and 1e-5 for URL-based docids, with a batch size of 64 and a regularization parameter ùõΩ of 0.4 to balance chosen and rejected responses. All experiments were conducted on a single NVIDIA A100 GPU. Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval SIGIR ‚Äô25, July 13‚Äì18, 2025, Padua, Italy Table 1: Performance comparison of GenRRL and DDRO on the MS MARCO document ranking (MS300K) dataset. The best results are in bold. Results for cited models are sourced from their original papers. Abbreviations: PQ ‚Äì Prod- uct Quantization; TU ‚Äì Title + URL; Sum ‚Äì document sum- mary. Model R@1 R@5 R@10 MRR@10 GenRRL (TU) [76] 33.01 63.62 74.91 45.93 GenRRL (Sum) [76] 33.23 64.48 75.80 46.62 DDRO (PQ) 32.92 64.36 73.02 45.76 DDRO (TU) 38.24 66.46 74.01 50.07 Constrained beam search. During inference, constrained beam search generates valid docids by using a prefix tree [60] to enforce valid token sequences. 6 Experimental Evaluation and Results Our evaluation of DDRO focuses on the following questions: RQ1 How does DDRO compare to RLRF-based methods, such as GenRRL, in terms of retrieval performance while avoiding the complexities of reward modeling and reinforcement learning? RQ2 How does DDRO perform relative to established baselines in terms of retrieval accuracy and ranking consistency on benchmark