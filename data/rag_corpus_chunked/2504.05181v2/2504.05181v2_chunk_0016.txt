a sequence-to- sequence objective, aiming to maximize the likelihood of the target sequence through teacher forcing [66]. Given an input sequence ğ‘ , which can be any of the document formats or queries described above, the objective is defined as: ğ¿ğœƒ SFT = arg max ğœƒ log ğ‘ƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ— | ğ‘ , ğœ‹ ref ğœƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ )), (9) where ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ— represents the ground truth sequence, ğœ‹ref ğœƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ ) denotes the sequence generated by the SFT model, and ğ‘ƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ— | ğ‘ , ğœ‹ ref ğœƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ )) corresponds to the conditional probability of the ground truth given the input sequence and the modelâ€™s generated sequence. 4.3 Direct L2R Optimization Using Relevance Feedback DDRO simplifies the complexities associated with reward modeling and reinforcement learning used in RLRF approaches. Instead, it directly optimizes the likelihood that relevant docids (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ +) are assigned higher scores over non-relevant ones (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ’) for a given query, as illustrated in Figure (3). The corresponding optimization objective is formulated as: LDDRO (ğœ‹ğœƒ ; ğœ‹ref) = âˆ’ E(ğ‘,ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ +,ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ’ )âˆ¼ğ·  log ğœ  ğ›½ log ğœ‹ğœƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ + | ğ‘) ğœ‹ref(ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ + | ğ‘) âˆ’ ğ›½ log ğœ‹ğœƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ’ | ğ‘) ğœ‹ref(ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ âˆ’ | ğ‘)  , (10) where ğœ‹ğœƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ |ğ‘) is the policy that is being optimized, while ğœ‹ref(ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ |ğ‘) is the reference policy, typically the fine-tuned model (SFT). This formulation ensures that the optimized model remains close to the reference policy while improving relevance-based rank- ing. The DDRO update guides the model toward producing outputs better aligned with relevance by utilizing pairwise comparisons, of- fering a simplified alternative to RL-based approaches. The gradient w.r.t. the model parametersğœƒ is defined as in Eq. 7, where Ë†ğ‘Ÿğœƒ (ğ‘, ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ ) = ğ›½ log ğœ‹ğœƒ (ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ | ğ‘) ğœ‹ref(ğ‘‘ğ‘œğ‘ğ‘–ğ‘‘ | ğ‘) (11) is the