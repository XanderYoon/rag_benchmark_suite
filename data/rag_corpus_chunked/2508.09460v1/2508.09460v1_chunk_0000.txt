Towards Self-cognitive Exploration: Metacognitive Knowledge Graph Retrieval Augmented Generation Xujie Yuan Sun Yat-sen University Zhuhai, China Shimin Di Southeast University Nanjing, China Jielong Tang Sun Yat-sen University Zhuhai, China Libin Zheng Sun Yat-sen University Zhuhai, China Jian Yin Sun Yat-sen University Zhuhai, China ABSTRACT Knowledge Graph-based Retrieval-Augmented Generation (KG- RAG) significantly enhances the reasoning capabilities of Large Language Models by leveraging structured knowledge. However, existing KG-RAG frameworks typically operate as open-loop sys- tems, suffering from cognitive blindness, an inability to recognize their exploration deficiencies. This leads to relevance drift and incomplete evidence, which existing self-refinement methods, de- signed for unstructured text-based RAG, cannot effectively resolve due to the path-dependent nature of graph exploration. To ad- dress this challenge, We propose Metacognitive Knowledge Graph Retrieval Augmented Generation (MetaKGRAG), a novel frame- work inspired by human metacognition process, which introduces a Perceive-Evaluate-Adjust cycle to enable path-aware, closed-loop refinement. This cycle empowers the system to self-assess explo- ration quality, identify deficiencies in coverage or relevance, and perform trajectory-connected corrections from precise pivot points. Extensive experiments across five datasets in the medical, legal, and commonsense reasoning domains demonstrate that MetaKGRAG consistently outperforms strong KG-RAG and self-refinement base- lines. Our results validates the superiority of our approach and highlights the critical need for path-aware refinement in structured knowledge retrieval. KEYWORDS Large Language Models, Knowledge Graph, Retrieval-Augmented Generation, Metacognition 1 INTRODUCTION Large Language Models (LLMs) have demonstrated remarkable capabilities [2, 28], yet their reliability is often limited by hallu- cinations and outdated internal knowledge [18, 20, 36]. Retrieval- Augmented Generation (RAG) mitigates these issues by grounding LLMs in external knowledge [6, 8]. While standard RAG uses un- structured text, Knowledge Graph RAG (KG-RAG) [5, 7, 33] lever- ages explicit, structured relationships. This enables a more verifi- able reasoning process, making it particularly powerful for complex queries