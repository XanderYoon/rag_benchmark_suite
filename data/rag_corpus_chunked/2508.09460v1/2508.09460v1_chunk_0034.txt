covering var- ious clinical departments including internal medicine, surgery, gynecology, and pediatrics, with answers provided by doctors and experienced users. The dataset has been preprocessed to remove web tags, links, and garbled characters, retaining only Chinese and English characters, numbers, and punctuation. • JEC-QA [35] is a legal domain dataset collected from the National Judicial Examination of China. It serves as a comprehensive eval- uation of professional skills required for legal practitioners. The dataset is particularly challenging as it requires logical reason- ing abilities to retrieve relevant materials and answer questions correctly. A.2 Implementation details Our framework is built on LangChain1. The local open-source LLMs are deployed based on the llama.cpp2 project. Except for the context window size, which is adjusted according to the dataset, all other pa- rameters use default configurations, such as temperature is 0.8. Both LangChain and llama.cpp are open-source projects, providing good transparency and reproducibility. For computational resources, all experiments were conducted on a cluster of 8 NVIDIA RTX 3090 GPUs. Due to computational constraints and to ensure fair com- parison across different model scales, we applied 4-bit quantization to locally deployed LLMs. For the evaluation, we employed Bert Score metrics using “bert-base-chinese [3]” model, while ROUGE Score version 0.1.2 was utilized. Due to resource constraints, G-Eval assessments were conducted using locally deployed Qwen2.5-72B. B KNOWLEDGE GRAPH CONSTRUCTION We employed a consistent KG construction method for all datasets, utilizing LLMs to extract knowledge triples from the datasets to build specialized KGs. The prompt example is shown in Tab. 8. All KGs were deployed using Neo4j3. C COMPLEMENTARY EXPERIMENTAL RESULTS C.1 Analysis of Generative Quality In addition to evaluating answer accuracy, we assessed the qual- ity of the generated explanations for tasks requiring free-form responses (ExplainCPE and webMedQA). We used ROUGE-L to measure lexical overlap with reference