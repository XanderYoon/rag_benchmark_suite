by testing values from 0.1 to 0.5. The results, depicted in Figure 5 (b), demonstrate that the framework exhibits low sensitivity to the specific value of ùõø. Performance on both datasets forms a plateau, with optimal results achieved for ùõø values between 0.2 and 0.3. The minimal variation in accuracy suggests that as long as the adjustment provides a clear directional signal, its exact magnitude is not a critical factor. 0.1 0.2 0.3 0.4 0.5 Concept Relevance Threshold ( c) 76 78 80 82 84 86 88Accuracy (%) (a) Concept Relevance Threshold ExplainCPE JEC-QA 0.1 0.2 0.3 0.4 0.5 Weight Adjustment ( ) 76 78 80 82 84 86 88 (b) Weight Adjustment ExplainCPE JEC-QA Figure 5: Evaluation of other hyperparameters on Ex- plainCPE and JEC-QA. C.4 Case Study We conduct a case study on the ExplainCPE, . To visually demon- strate the operational difference between MetaKGRAG and baseline methods when handling complex queries, we present a represen- tative case from our medical dataset. As shown in Tab. 10, this case illustrates the ‚ÄúCognitive Blindness‚Äù issue in traditional KG- RAG, the path-dependency failure of self-refinement methods, and showcases how our metacognitive cycle effectively corrects the evidence-gathering path. 11 Xujie et al. Table 7: Prompt Example for Answer Generation prompt = f"""Your task is to accurately understand the question requirements and provide reasonable answers and explanations based on the provided reference content. Input Question: {question} You have the following medical evidence knowledge: {evidence_text} What is the answer to this multiple-choice question? Answer the question by referring to the provided medical evidence knowledge. First, choose the answer from (A\B\C\D\E), output the answer option, then explain the reasoning. """ Table 8: Prompt Example for Knowledge Graph Construction prompt = f"""As a professional knowledge extraction assistant, your task is to extract knowledge triples from