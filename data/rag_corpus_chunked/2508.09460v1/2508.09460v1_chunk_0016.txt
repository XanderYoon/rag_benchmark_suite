• Large Language Models. This group serves as a fundamental baseline to evaluate the capabilities of LLMs themselves without any external knowledge retrieval. We select leading commercial models including GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, and OpenAI o1-mini. For open-source models, we useQwen2.5- 7B and Qwen2.5-72B [31] for Chinese tasks, and Llama-3-8B and Llama-3-70B [4] for English tasks. • KG-RAG Approaches. We compare against several representa- tive KG-RAG approaches. Vanilla KGRAG [23] serves as the fundamental implementation, performing direct entity similarity- based retrieval and presenting facts to LLMs. ToG (Think-on- Graph) [24] guides LLMs to explore multiple reasoning paths within knowledge graphs for multi-hop reasoning.MindMap [30] constructs structured representations that integrate knowledge from subgraphs to enhance interpretability. KGGPT [11] han- dles complex queries by decomposing them into simpler clauses and constructing evidence graphs through separate retrievals. These baselines providing comprehensive coverage of existing retrieval paradigms from simple similarity matching to sophisti- cated multi-hop strategies. • Self-Refinement Methods. As discussed in Sec. 5, we compare sev- eral Self-Refinement methods. Chain-of-Thought (CoT) [29] encourages more thoughtful reasoning by generating intermedi- ate steps. Metacognitive Prompting [27] employs a metacog- nitive prompt to guide the model to self-critique and refine its reasoning. For retrieval-augmented scenarios, we construct base- lines by combining a standard KG-RAG retriever with frame- works like FLARE [10], which performs active retrieval when generation confidence is low, and ReAct [32], which synergizes reasoning and acting to iteratively search for information. We also adapt Meta RAG [37], which enhances RAG by implement- ing a three-step metacognitive process of monitoring, evaluating, and planning to enable the model to introspectively identify and rectify its own knowledge gaps and reasoning errors. These base- lines represent a straightforward “stacking” approach that adds a refinement mechanism in KG-RAG. 4.1.4 Implementation Details. Our MetaKGRAG framework is im-