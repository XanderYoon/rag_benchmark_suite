For medical domains, we employ three datasets, CMB-Exam [26] with 2,000 sampled questions from Chi- nese medical professional examinations (including Nursing, Phar- macy, Postgraduate, and Professional), ExplainCPE [13] containing pharmaceutical questions from the National Licensed Pharmacist Examination with both answers and explanations, and webMedQA [9] featuring real-world patient-doctor conversations from online med- ical platforms. For legal domain, we use JEC-QA [35] from China’s National Judicial Examination, which requires logical reasoning to apply legal materials to specific case scenarios. For more detailed information on these datasets, please refer to Appendix A.1. 4.1.2 Evaluation Metrics. For evaluation, we adopt a variety of different metrics. Correct (Accuracy), Wrong, Fail are used for those with ground truth (e.g., CommonsenseQA, CMB-Exam, ExplainCPE), where Fail indicates the model fails to generate any answer. For tasks requiring generative answers (ExplainCPE, webMedQA), we use ROUGE-L [15] to measure lexical overlap with reference an- swers and BERTScore [34] to assess semantic similarity. To further evaluate the overall quality of the generated responses, we uti- lize G-Eval [17], a framework that leverages LLMs for evaluation, assessing the generated answers based on four key dimensions, Coherence, Consistency, Fluency, and Relevance. For all results, the best results are in bold and the second best results are underlined. 4.1.3 Baselines. To comprehensively evaluate the performance of our proposed MetaKGRAG framework, we select a diverse range of baselines, which can be categorized into three groups: Large Lan- guage Models, KG-RAG approaches, and self-refinement methods. • Large Language Models. This group serves as a fundamental baseline to evaluate the capabilities of LLMs themselves without any external knowledge retrieval. We select leading commercial models including GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, and OpenAI o1-mini. For open-source models, we useQwen2.5- 7B and Qwen2.5-72B [31] for Chinese tasks, and Llama-3-8B and Llama-3-70B [4] for English tasks. •