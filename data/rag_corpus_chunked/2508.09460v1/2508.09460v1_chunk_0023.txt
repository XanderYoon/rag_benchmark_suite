cepts) and the maximum iteration numberğ‘ğ‘šğ‘ğ‘¥ . As shown in Fig. 3 (a), ğœğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ performs optimally around 0.6 across both datasets. This is because a threshold set too low risks triggering unneces- sary adjustments for irrelevant concepts, while one set too high may fail to identify and correct important evidence gaps. Forğ‘ğ‘šğ‘ğ‘¥ , as shown in Fig. 3 (b), its performance peaks at 3 iterations and then stabilizes, suggesting that most path deficiencies can be ef- fectively resolved within three cycles, with further iterations offer- ing diminishing returns. The consistency of these optimal values across different domains validates our default parameter choices and demonstrates the frameworkâ€™s robustness. Multi-start Retrieval Analysis. To quantify the importance of the multi-start retrieval strategy, we analyze the impact of the num- ber of starting entities on final performance. As illustrated in Fig. 3 7 Xujie et al. 0.4 0.5 0.6 0.7 0.8 Coverage Threshold 78 80 82 84 86Accuracy (%) (a) Coverage Threshold ExplainCPE JEC-QA 1 2 3 4 5 Max Iterations 76 78 80 82 84 86 (b) Max Iterations ExplainCPE JEC-QA 1 2 3 4 5 6 7 Number of Start Entities 70 72 74 76 78 80 82 84 86 (c) Start Entities ExplainCPE JEC-QA Figure 3: Evaluation of different hyperparameters on ExplainCPE and JEC-QA. (c), accuracy on both datasets improves significantly as the number of starting entities increases, reaching an optimal point at 5 before plateauing. This is because complex questions often require multi- ple perspectives to construct a complete evidence subgraph, and starting from just one entity frequently misses critical information. Notably, medical questions (ExplainCPE) benefit more from addi- tional starting points than legal questions (JEC-QA), likely because medical problems often involve more interconnected concepts that necessitate exploration from diverse angles. This result echoes our ablation study conclusions, further demonstrating