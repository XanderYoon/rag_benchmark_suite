We also adapt Meta RAG [37], which enhances RAG by implement- ing a three-step metacognitive process of monitoring, evaluating, and planning to enable the model to introspectively identify and rectify its own knowledge gaps and reasoning errors. These base- lines represent a straightforward “stacking” approach that adds a refinement mechanism in KG-RAG. 4.1.4 Implementation Details. Our MetaKGRAG framework is im- plemented using both large and small-scale open-source models as its backbone. Specifically, we utilize the Qwen2.5 series (7B, 72B) for Chinese tasks and the Llama-3 series (8B, 70B) for English tasks. 5 Xujie et al. Table 2: Performance Comparison on ExplainCPE and JEC- QA using Accuracy. Type Method ExplainCPE JEC-QA Without Retrieval LLM Only Qwen2.5-7B 69.76 65.06 Qwen2.5-72B 81.82 80.13 GPT4o 79.64 78.51 o1-mini 75.10 70.15 Claude3.5-Sonnet 76.88 75.33 Gemini1.5-Pro 69.37 76.18 Self-Refine Chain-of-Thought 82.53 81.02 Meta Prompting 83.11 81.67 With Retrieval KG-RAG KGRAG 78.53 73.88 ToG 78.85 74.90 MindMap 78.41 71.55 KGGPT 78.86 71.83 Self-Refine FLARE 80.23 75.81 ReAct 81.51 76.92 Meta Prompting 80.88 76.25 Meta RAG 81.93 77.31 Ours MetaKGRAG (Qwen2.5-7B) 85.97 77.10 MetaKGRAG (Qwen2.5-72B) 91.70 88.49 For all semantic similarity calculations, such as matching concepts to entities and assessing path coverage, we employ the distiluse- base-multilingual-cased-v1 [19] embedding model due to its strong multilingual capabilities. The specific knowledge graphs for each dataset were custom-built to ensure relevance and quality. The detailed methodologies for knowledge graph construction, the spe- cific prompt templates used for different stages of the framework (e.g., concept extraction, answer generation), and the experimental environment setup are all detailed in the Appendix A for repro- ducibility. The core parameters of MetaKGRAG will be analyzed in detail in the subsequent analysis section. 4.2 Results and Analysis 4.2.1 Main Results. The main experimental results are presented in Tab. 2, 3, 4, and 5. The results across all