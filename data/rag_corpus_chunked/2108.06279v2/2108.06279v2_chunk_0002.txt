several works have proposed investigating whether BERT-based systems are able to identify the relevant passages among all passages in a collection, rather than just among a query-dependent sample; these systems represent a new type of retrieval approaches called dense retrieval. In dense retrieval, passages are represented by real-valued vectors, while the query-document similarity is computed by deploying efficient nearest neighbour techniques over specialised indexes, such as those provided by the FAISS toolkit [10]. Thus far, two different families of dense retrieval approaches have emerged recently, based onsingle representation and multiple representation. In particular, DPR [11] and ANCE [12] use a single representation, by indexing only the embedding of BERTâ€™s [CLS] token, and therefore this is assumed to represent the meaning of an entire passage within that single embedding. At retrieval time, the [CLS] embedding of the query is then used to retrieve passages by identifying nearest neighbours using a FAISS index. In contrast, ColBERT [3], which uses multiple representation, indexes an embedding for each token in each document. At retrieval time, a set of the nearest document embeddings to each query embedding is retrieved, by identifying the approximate nearest neighbours from a FAISS index. These passages must then be exactly scored, based on the maximal similarity between the query and the passage embeddings, to obtain the final ranking. These are two markedly different families of dense retrieval approaches. Indeed, as ColBERT records one embedding for every token, this makes for a large index of embeddings, which may allow a richer semantic representation of the content. On the other hand, DPR and ANCE rely on a single embedding sufficiently representing the content of each passage. However, at the time of writing, no systematic study has compared these two families of dense retrieval approaches. For this reason, this work contributes a first