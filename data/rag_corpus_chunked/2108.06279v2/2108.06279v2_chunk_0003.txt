a large index of embeddings, which may allow a richer semantic representation of the content. On the other hand, DPR and ANCE rely on a single embedding sufficiently representing the content of each passage. However, at the time of writing, no systematic study has compared these two families of dense retrieval approaches. For this reason, this work contributes a first investigation into the effectiveness of single and multiple representation embeddings for dense retrieval, as exemplified by ANCE and ColBERT, respectively. We perform experiments in a controlled environment using the same collection and query sets, and we report several effectiveness metrics, together with a detailed comparison of the results obtained for the two representation families w.r.t. a common baseline, namely BM25. To derive further insights, we also provide a per-query analysis of the effectiveness of single and multiple representations. We observe that, while ANCE is more efficient than ColBERT in terms of response time and memory usage, multiple representations are statistically more effective than the single representations for MAP and MRR@10. We also show that multiple representations obtain better improvements than single representations for queries that are the hardest for BM25, as well as for definitional queries, and those with complex information needs. 2. Problem Statement Embeddings. Contextualized language models such as BERT have been trained on a large corpus for language understanding, and then fine-tuned on smaller, more specific textual collections targeting a particular IR task. Through this fine-tuning, BERT learns how to map texts, either queries or documents, into a multi-dimensional space of one or more vectors, called embeddings. Both queries and documents are tokenised into terms according to a predefined vocabulary; BERT learns a function mapping tokens in a query into multiple query embeddings, one per query term, and another potentially different function mapping tokens in a document