over the entire query sets as reported in Table 2. To perform this analysis, we define the difficulty of 5This is the metric recommended by the track organisers for this query set. Table 2 Effectiveness metrics of BM25, ANCE and ColBERT on different query sets. Points marked with △ and ▲ denote a significant increase in effectiveness compared to BM25 and ANCE, respectively, according to a paired t-test with Bonferroni correction (p-value < 0.05). TREC 2019 MSMARCO Dev MAP NDCG@10 MRR@10 MRR@10 BM25 0.2864 0.4795 0.6410 0.1836 ANCE 0.3715 △ 0.6537△ 0.8574△ 0.3292△ ColBERT 0.4309 ▲△ 0.6934△ 0.8527△ 0.3519△▲ a query according to an effectiveness metric on the BM25 baseline, following Mothe et al. [18]. Due to the sparsity of the relevance judgements and the official evaluation metrics of the two query sets, we adopt a different query difficulty classification for TREC 2019 and MSMARCO Dev. For the TREC 2019 query set, a query is considered hard, resp. easy, for the BM25 baseline system if the NDCG@10 (the official TREC metric in [19]) value is in the first quartile, resp. in the fourth quartile, and medium otherwise. For the MSMARCO query set, the official metric MRR@10 per query is too sparse to allow percentile computations. Hence we consider a Dev query to be hard if its MRR@10 is lesser than or equal to 0.1, and easy otherwise. We partition the queries in each query set according to the corresponding difficulty classifi- cation, and compute for how many queries the effectiveness of ANCE and ColBERT is higher (denoted by W(in)) or lower (denoted with L(oss)) than BM25. For each partition, we also compute the average reward and risk associated with the W and L queries, following [17]. Table 3 reports the observed results. For the TREC 2019 queries, both ANCE and