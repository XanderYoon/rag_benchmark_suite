palsy’ aspect, omitting the dysarthria aspect of the query. Indeed, the Precision @10 of ANCE for this query was 3 10, compared to 6 10 for ColBERT. This suggests that ANCE’s compression of a complex information need into one embedding has caused an information loss, with the model focusing on only a single aspect of the query, resulting in low effectiveness. On the other hand, for query 1063750 (‘why did the us volunterilay enter ww1’), ANCE identified a relevant passage, but ColBERT identified a passage (1300452) focusing entirely on the wrong World War (‘ww2’ rather than ‘ww1’). At least some of the reason for the conflation of meanings is that neither ‘ww1’ nor ‘ww2’ do not appear in BERT’s fixed vocabulary, e.g., the latter is tokenised into word pieces as ‘w’, ‘##w’, ‘##2’. Hence distinguishing between ‘ww1’ and ‘ww2’ information needs require context to be distributed across the three embeddings. To analyse this passage further, Figure 2 shows the ColBERT interaction between the query and document embeddings for this passage and query6. In the figure, the darker shading in the matrix is indicative of higher similarity; the highest similarity that is selected for a given query embedding by the max-sim operator is indicated by a × symbol; the histogram at the top of the 6This figure can be reproduced using the explain_text() function within our PyTerrier_ColBERT library. 0.0 0.5 [CLS][Q]whydidtheusvoluntarilyenterw##w##1[SEP][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK] [CLS][D]themaineventthatledtheustoenteringw##w##2wasjapanbombingpearlharbor.thedayafterthebombingu.s X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Figure 2: ColBERT interaction between query and document embeddings for query 106375 and passage 1300452 (see Table 4). In the interaction matrix, darker shading is indicative of higher similarity; the document embedding (row) with highest similarity for