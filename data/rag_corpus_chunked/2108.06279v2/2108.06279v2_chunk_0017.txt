that in ANCE, the use of a single embedding representation risks misinterpreting complex queries with multiple aspects as shown by results in the previous subsection; For ColBERT, the max similarity operator can overly focus on highly similar embeddings at the risk of mis-interpreting a query. 4. Conclusions Despite their recency, dense passage retrieval approaches have the effectiveness potential to supplant the traditional inverted index data structure. Yet, different families of dense retrieval are emerging, for which the comparative advantages and disadvantages are not yet clear. In this work, we made a systematic study of single vs. multiple representation dense retrieval approaches, namely ANCE and ColBERT. We found that while both significantly outperformed BM25 baselines across various metrics, ColBERT significantly outperformed ANCE for MAP on TREC2019 and MRR@10 on the MSMARCO Dev query set, was more effective for queries that BM25 found hard, and was better at definitional queries as well as queries that had complex information needs. On the other hand, ANCE has desirable qualities in terms of mean response time and memory occupancy (see Table 1). We postulate that research should be directed toward hybrid solutions, either reducing the size of the ColBERT embedding index, e.g., through adaptations to static pruning, or through using multiple embeddings within ANCE for complex queries/passages. Acknowledgements Nicola Tonellotto was partially supported by the Italian Ministry of Education and Research (MIUR) in the framework of the CrossLab project (Departments of Excellence). Craig Macdon- ald and Iadh Ounis acknowledge EPSRC grant EP/ R018634/1: Closed-Loop Data Science for Complex, Computationally- & Data-Intensive Analytics. References [1] J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, in: Proc. NAACL, 2019. [2] J. Lin, R. Nogueira, A. Yates, Pretrained transformers for text ranking: BERT and beyond, 2020. arXiv:2010.06467. [3] O. Khattab,