questions. 3. Experiments In the following, we report our experimental setup, followed by analyses for RQs 1-3. 3.1. Setup Our experiments use the MSMARCO passage ranking dataset, a dataset of 8.8M passages and build upon our PyTerrier IR experimentation platform [ 14, 15]. We adapt the ANCE implementation2 and the ColBERT implementation3 provided by their respective authors, using integrations with PyTerrier4. We use the provided ANCE model for the MSMARCO passage ranking dataset. We train ColBERT using the same MSMARCO passage ranking training triples file for 44,500 batches. In particular, we follow [ 12] and [ 3] for the settings of ANCE and ColBERT, as summarised in Table 1. Of note, while ColBERT fine-tunes the bert-base-uncased BERT model, ANCE fine-tunes a RoBERTa model [16] (specifically roberta-base), which is reported to apply more refined pre-training than BERT. To try to eliminate model choice as a confounding factory, we also trained a version of ColBERT by fine-tuning roberta-base. We found that even after training for 300k batches (6Ã— longer than we trained ColBERT using bert-base-uncased), this latter model could had relative performance 25% less than the BERT-based ColBERT model (around NDCG@10 of 0.533). Hence we discarded the RoBERTa-based ColBERT model from further consideration. On the other hand, all of the released ANCE models use RoBERTa; training ANCE requires multiple GPUs, e.g., 16, and has not, to the best of our knowledge, been reproduced. Hence, we argue that as the RoBERTa-based ANCE and BERT-based ColBERT are individually shown to be effective by their respective authors, the comparison of these representative models still allows for interesting observations. We index the corpus using the code provided by the authors. Table 1 reports the statistics of the resulting indices. In particular, the ANCE document index is stored in FAISS using the uncompressed IndexFlatIP format. The ColBERT