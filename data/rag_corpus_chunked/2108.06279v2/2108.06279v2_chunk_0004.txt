learns how to map texts, either queries or documents, into a multi-dimensional space of one or more vectors, called embeddings. Both queries and documents are tokenised into terms according to a predefined vocabulary; BERT learns a function mapping tokens in a query into multiple query embeddings, one per query term, and another potentially different function mapping tokens in a document into a document embedding per term. BERT and its derived models also make use of special tokens, such as the [CLS] (classification) token, the [SEP] (separator) token, and the [MASK] (masked) token. In particular, [CLS] is always placed at the beginning of any text given as input to BERT, both at training and inference time, and is used to let BERT learn a global representation of the input text as a single embedding. In more detail, a text composed byùëö terms given as input to BERT will produce ùëö + 1 embeddings, one per input term plus one additional embedding for [CLS]. In single representation models, such as ANCE, the embedding corresponding to [CLS] is assumed to encode all possible information about the input text, including the possible semantic context of the composing terms. In contrast, in multiple representation models, such as ColBERT, each input term‚Äôs embedding encodes its specific semantic information within the context of the entire input text. Dense Retrieval. The embeddings produced by the BERT models have recently demonstrated their promise in being a suitable basis for dense retrieval. In dense retrieval, documents and queries are represented using embeddings. The embeddings from the documents in a collection can be pre-computed through the application of the BERT learned mapping and stored into an index data structure for embeddings supporting nearest neighbour similarity search, as exemplified by the FAISS toolkit [10]. Depending on the number and dimensions of the