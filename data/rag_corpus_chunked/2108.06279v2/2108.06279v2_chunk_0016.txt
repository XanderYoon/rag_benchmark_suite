X X X X X X X X X X X X X X X X X X X X X X X Figure 2: ColBERT interaction between query and document embeddings for query 106375 and passage 1300452 (see Table 4). In the interaction matrix, darker shading is indicative of higher similarity; the document embedding (row) with highest similarity for each query embedding (column) is indicated with a × symbol. The histogram at the top portrays the contribution of each query embedding to the final score of the passage, with shading also indicative of the magnitude of contribution. figure indicates the contribution of each query embedding to the final passage score. Indeed, on inspection of the max similarities for this passage shows that the highest contributions to the passage’s score comes from the ‘##w’ token, with ‘##1’ query embedding being highly similar to the ‘##2’ document embedding. This suggests that the embeddings for ‘##1’ and ‘##2’ are not sufficiently contextualised when following ‘##w’, or that ColBERT’s max similarity computation could be adapted to better address proximity. In contrast, ANCE retrieved passage 1300452 at rank 155, showing that the single representations for the passages sufficiently distinguish between World War 1 vs. World War 2. In summary, in addressing RQ3, we observed that there exists some large differences between ANCE and ColBERT for some queries. Our analysis found that ColBERT perfoms better than ANCE for definitional type queries. Moreover, our analysis suggests that in ANCE, the use of a single embedding representation risks misinterpreting complex queries with multiple aspects as shown by results in the previous subsection; For ColBERT, the max similarity operator can overly focus on highly similar embeddings at the risk of mis-interpreting a query. 4. Conclusions Despite their recency, dense passage retrieval approaches have the effectiveness potential to supplant