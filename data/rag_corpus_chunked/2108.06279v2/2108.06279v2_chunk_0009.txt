for each query and each effectiveness metric, we examine the comparative reward (improvement) and risk (degradation) over a BM25 baseline (following [17]), as well as the number of wins & losses (improved and degraded queries). 3.2. Overall Comparison Table 2 reports the effectiveness metrics of BM25, ANCE and ColBERT computed on the TREC 2019 and the sample of the MSMARCO Dev query sets. As expected, both the ANCE and ColBERT dense retrieval approaches are significantly better than BM25 for the NDCG@10 and MRR@10 metrics on both query sets. Comparing the two dense retrieval approaches, for MAP, ColBERT significantly outperforms ANCE; for NDCG@10, ColBERT enhances ANCE by 6% (0.6537→0.6934), but not significantly so; for MRR@10, ANCE is slightly (but not significantly) better than ColBERT on the TREC2019 query set while ColBERT is statistically better than ANCE on MSMARCO Dev by +7%. Overall, for RQ1, we conclude that multiple representations, employed by ColBERT, experimentally obtain better effectiveness than single representations (as employed by ANCE), exhibiting significant boost in effectiveness for MAP (TREC 2019) and MRR@10 (Dev). Among the most striking differences is that for MAP on TREC 2019, where ColBERT markedly outperforms ANCE (and BM25); this observation suggests that the single representation is not sufficiently good at attaining high recall. 3.3. Comparison using a Common Baseline Next, we investigate the comparative effectiveness of ANCE and ColBERT from the perspective of using BM25 as the reference point, going further than reporting average performances over the entire query sets as reported in Table 2. To perform this analysis, we define the difficulty of 5This is the metric recommended by the track organisers for this query set. Table 2 Effectiveness metrics of BM25, ANCE and ColBERT on different query sets. Points marked with △ and ▲ denote a significant increase in effectiveness compared to BM25