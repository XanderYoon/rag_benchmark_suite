BERT-based ColBERT are individually shown to be effective by their respective authors, the comparison of these representative models still allows for interesting observations. We index the corpus using the code provided by the authors. Table 1 reports the statistics of the resulting indices. In particular, the ANCE document index is stored in FAISS using the uncompressed IndexFlatIP format. The ColBERT document index is stored in FAISS using the compressed and quantised IndexIVFPQ format, which is trained on a random 5% sample of the document embeddings. Mean response times for both ANCE and ColBERT, and their memory consumption, are also shown in Table 1. 2https://github.com/microsoft/ANCE 3https://github.com/stanford-futuredata/ColBERT/tree/v0.2 4See https://github.com/terrierteam/pyterrier_ance and https://github.com/terrierteam/pyterrier_colbert. Table 1 Salient statistics of the ANCE and ColBERT setups. ANCE ColBERT Representation single multiple Base model roberta-base bert-base-uncased # emb. per query 1 32 # emb. per passage 1 up to 180 Emb. dimensions 768 128 FAISS index size 26GB 16GB Embedding index size – 176GB Mean Response Time 211ms 635ms For evaluating effectiveness, we use the publicly available query sets with relevance assess- ments: 5000 queries sampled from the MSMARCO Dev set – which contain on average 1.1 judgements per query – as well as the TREC 2019 query set, which contains 43 queries with an average of 215.3 judgements per query. To measure effectiveness, we employ MRR@10 for the MSMARCO Dev set5, and the MRR@10, NDCG@10 and MAP for the TREC query set. To examine gains and losses, for each query and each effectiveness metric, we examine the comparative reward (improvement) and risk (degradation) over a BM25 baseline (following [17]), as well as the number of wins & losses (improved and degraded queries). 3.2. Overall Comparison Table 2 reports the effectiveness metrics of BM25, ANCE and ColBERT computed on the TREC 2019 and the sample of the MSMARCO