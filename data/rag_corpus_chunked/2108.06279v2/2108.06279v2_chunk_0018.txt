and Iadh Ounis acknowledge EPSRC grant EP/ R018634/1: Closed-Loop Data Science for Complex, Computationally- & Data-Intensive Analytics. References [1] J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, in: Proc. NAACL, 2019. [2] J. Lin, R. Nogueira, A. Yates, Pretrained transformers for text ranking: BERT and beyond, 2020. arXiv:2010.06467. [3] O. Khattab, M. Zaharia, ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT, in: Proc. SIGIR, 2020, p. 39–48. [4] S. Hofstätter, A. Hanbury, Let’s measure run time! Extending the IR replicability infras- tructure to include performance aspects, in: OSIRRC@SIGIR, 2019. [5] H. Zamani, M. Dehghani, W. B. Croft, E. Learned-Miller, J. Kamps, From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing, in: Proc. CIKM, 2018, pp. 497–506. [6] S. MacAvaney, A. Yates, A. Cohan, N. Goharian, CEDR: Contextualized embeddings for document ranking, in: Proc. SIGIR, 2019, pp. 1101–1104. [7] S. MacAvaney, F. M. Nardini, R. Perego, N. Tonellotto, N. Goharian, O. Frieder, Efficient document re-ranking for transformers by precomputing term representations, in: Proc. SIGIR, 2020, pp. 49–58. [8] S. MacAvaney, F. M. Nardini, R. Perego, N. Tonellotto, N. Goharian, O. Frieder, Expansion via prediction of importance with contextualization, in: Proc. SIGIR, 2020, p. 1573–1576. [9] Z. Dai, J. Callan, Deeper text understanding for IR with contextual neural language modeling, in: Proc. SIGIR, 2019, pp. 985–988. [10] J. Johnson, M. Douze, H. Jégou, Billion-scale similarity search with GPUs, 2017. arXiv:1702.08734. [11] V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, W.-t. Yih, Dense passage retrieval for open-domain question answering, in: Proc. EMNLP, 2020, pp. 6769–6781. [12] L. Xiong, C. Xiong, Y. Li, K.-F. Tang, J. Liu, P. Bennett, J. Ahmed, A. Overwijk, Approximate nearest neighbor negative contrastive learning for dense