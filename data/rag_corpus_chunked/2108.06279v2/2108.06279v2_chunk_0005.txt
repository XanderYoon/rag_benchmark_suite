dense retrieval. In dense retrieval, documents and queries are represented using embeddings. The embeddings from the documents in a collection can be pre-computed through the application of the BERT learned mapping and stored into an index data structure for embeddings supporting nearest neighbour similarity search, as exemplified by the FAISS toolkit [10]. Depending on the number and dimensions of the em- beddings stored into the index, advanced compression strategies, together with suitable nearest neighbour search algorithms, can be employed. In order to reduce the time required to identify the most similar document embeddings to a given input embedding, it is possible to shift from exact nearest neighbour search to approximate nearest neighbour search. While ANCE stores embeddings in an uncompressed format supporting exact search, ColBERT, given the larger number of document embeddings it has to store, resorts to compressed and quantised embed- dings supporting approximate search. However, the approximate similarity scores produced by approximate search are not used by the ColBERT implementation to compute the final top documents to return for a given query [13]1. Hence, ColBERT uses approximate search over compressed embeddings to identify a candidate set of documents, which are then re-scored using an index with direct lookup for retrieving the candidate documentsâ€™ embeddings, to obtain the final ranking of documents returned to the user. Research Questions. In this work, we aim to compare the single and the multiple embedding representations leveraging the ANCE and ColBERT implementations. Indeed, there was not an effectiveness comparison with ColBERT in the ANCE paper [12]. ANCE embodies a recent single representation approach, where we have a single large embedding per query/document, which can be processed with exact similarity search in a single stage. In contrast, in the multiple representation approach (ColBERT), we have a smaller sized embedding for each term in the