amid vast information. To mitigate cognitive burdens in decision-making, Recommender System (RecSys) has emerged as an essential tool and is prevalently incorporated into many online services. RecSys effectively filters irrelevant information and delivers tailored recommendations. This way, it not only helps users better find their desired targets but also facilitates the operation of platforms. Hence, RecSys has attracted much attention and progressed actively over the past few decades [2, 3]. As users increasingly rely on recommendations for high-stakes decisions (e.g., financial investments and healthcare choices), mere predictive accuracy proves insufficient without explainable justification for recommendations This challenge has catalyzed the emergence of Explainable Recommender Sys- tem (ExRec) [4]. ExRec explains the recommendation results to users and enhances the transparency of the decision-making process. Therefore, it increases users’ trust in RecSys, further boosting the operation of online services. Prior works on ExRec mainly study how to generate interpretable explanations for user-item interac- tions [5]. For instance, some works adopt deep learning techniques like RNNs [ 6], GNNs [7] and Transformer [8] for capturing intricate patterns in user behaviors and item attributes, enabling the Preprint. Under review. arXiv:2507.09188v1 [cs.IR] 12 Jul 2025 generation of persuasive explanations. Although these methods are effective in some cases, they naturally suffer from limited language competence as they are trained over the restricted recom- mendation data. Hence, a branch of work on ExRec opts to use language models (e.g., BERT [9]) pre-trained on vast, diverse textual corpora to generate human-like explanations. This paradigm shift has gained momentum with the rise of large language models (LLMs), whose extensive world knowledge and nuanced language understanding enable the generation of human-like, contextually grounded explanations [10–12]. XRec is one representative LLM-based ExRec [11]. It enables LLMs to better understand complex user-item interaction patterns and offer explanations via integrating collaborative filtering (CF)