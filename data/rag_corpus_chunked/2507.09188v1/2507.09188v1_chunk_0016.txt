p grows. When p > 4, BERTPrecision Score keeps increases on Google-reviews. When p = 8 REXHA achieves best perfor- mance on Amazon-books. We notice that BERTrecall score remains relatively insensitive to changes of p, indicating that the profile already contains the essential information required for explanation generation. 3.5 Analysis of Retrieval Efficiency We evaluate the preprocessing time required byREXHA and G-Refer on three datasets. Across all three datasets, the retrieval time during the inference stage for REXHA is consistently under 1 second, significantly faster than G-Refer, which requires over 4 minutes. This highlights the superior inference efficiency of REXHA . Unlike G-Refer, REXHA does not perform any retrieval during the training phase. However, the profile preprocessing step in REXHA involves the Hierarchical Aggregation 8 (HA) module, which incurs a relatively high computational cost—taking up to 20 hours. Despite this, it is still considerably more efficient compared to G-Refer, whose total retrieval time during training exceeds 40 hours across all three datasets. These results demonstrate that the HA module, while computationally intensive, remains more efficient than G-Refer’s training-time retrieval operations. 4 Related Work 4.1 Explainable Recommender Systems (ExRec) With the rise of LLMs, many recent works leverage their world knowledge to generate more fluent and informative explanations. Explainable recommendation (ExRec) enhances user trust by revealing the rationale behind recommendations [4, 5], and has received increasing attention. Existing methods fall into three categories: generation-based, extraction-based, and hybrid approaches [5]. Generation- based methods (e.g., NRT [ 6], PETER [ 8], SEQUER [ 24]) produce explanations word-by-word from user/item representations. Extraction-based methods (e.g., ESCOFILT [ 9], GREENer [ 7]) select sentences directly from reviews. Hybrid methods (e.g., ERRA [25], ExBERT [26]) integrate both paradigms using retrieval-augmented generation. With the advent of LLMs, recent works utilize their world knowledge for more fluent and informative