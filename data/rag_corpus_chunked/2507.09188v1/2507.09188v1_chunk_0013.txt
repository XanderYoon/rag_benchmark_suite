74.91 0.3573 0.4264 0.3922 10.88 0.1050 0.0952 0.0862 REXHA-P 76.25 0.4879 0.3604 0.4237 10.64 0.1033 0.1153 0.0933 REXHA-L 74.32 0.5005 0.3603 0.4298 10.40 0.0994 0.1160 0.0938 Google-reviews NRT 58.27 0.3509 0.3495 0.3496 19.16 0.2176 0.1267 0.1571 Att2Seq 61.31 0.3619 0.3653 0.3636 17.47 0.1855 0.1247 0.1403 PETER 65.16 0.3892 0.3905 0.3881 17.00 0.2819 0.1356 0.2005 PEPLER 61.58 0.3373 0.3711 0.3546 17.17 0.1134 0.1161 0.0999 XRec 69.12 0.4546 0.4069 0.4311 14.24 0.0972 0.1163 0.0938 G-Refer (7B) 71.47 0.4253 0.4873 0.4566 13.46 0.1184 0.0872 0.0921 REXHA-P 70.35 0.4565 0.4200 0.4385 14.52 0.1060 0.1130 0.0940 REXHA-L 69.91 0.4884 0.4259 0.4573 14.23 0.1001 0.1179 0.0957 3.1.3 Baselines We compare our method with the following state-of-the-art methods, including NRT [6], Att2Seq [20], PETER [21], PEPLER [22], XRec [11] and G-Refer [12]. The details of baseline can be found in Appendix C. 3.1.4 Implementation Details For the collaborative signal extractor module, we train LigntGCN with a learning rate of 1e-3 and a batch size of 1024. For hierarchical aggregation (HA) module, we set 4 reviews as a set to be summarized. For the retrieval module, we use llm-embedder model [23] to generate the embeddings of the reviews. For the generation module, we use the LLaMA-2-7B model as the base model. We set the learning rate, epochs, and batch size as 8e-4, 2 and 12, respectively. For inference, we set the temperature to 0 and the max output tokens as 128. We run the experiments on a machine with 8 NVIDIA A800 GPUs. 3.2 Overall Performance Tab. 1 provides the overall results. It can be observed that REXHA achieves outstanding performance in explanation quality across evaluation metrics based on GPT and BERT. XRec and G-Refer take different approaches: the former leverages graph neural networks to capture collaborative filtering information and integrates it into large models, while the latter