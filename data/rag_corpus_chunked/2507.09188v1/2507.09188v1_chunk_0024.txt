translating collaborative filtering signals from user-item graphs into human-readable text, enabling large language models to generate personalized explanations through retrieval-augmented fine-tuning. D Analysis of Hierarchical Aggregation To validate the effectiveness of the Hierarchical Aggregation (HA) method, we compare it with the “Direct” method, where the LLM is invoked only once to generate the profile based on all reviews at once. We also compare the performance of Qwen2.5-7B with Qwen2.5-7B-1M, which supports a 1M-token context length. Tab. 4 reports the results. We observe that Qwen2.5-7B fails to generate profiles using the Direct method since the input exceeds its context length. In contrast, the HA method outperforms the Direct method, achieving improvements of 1.57%, 0.17%, and 1.02% on BERTP score, BERTR score, and BERTF1 score, respectively. Additionally, we evaluate an alternative approach in which second-layer summaries are directly concatenated to form the profiles. This method achieves slightly higher BERT precision than HA (an increase of 0.08%), but results in a slight drop in recall (a decrease of 0.19%). Table 4: Contrast study for Hierarchical Aggregation module. We evaluate the performance without Review Retrieval module. In this table, “Directly” denotes that for all reviews of user/item, LLMs are invoked once to generate profiles, and “Second Layer” denotes that sub-nodes of the final node are used as profiles. Models Type BERTPscore BERTRscore BERTF1score Qwen2.5-7B-1M HA 0.4712 0.3576 0.4142 Qwen2.5-7B-1M Directly 0.4639 0.3570 0.4100 Qwen2.5-7B-1MSecond Layer 0.4716 0.3557 0.4134 Qwen2.5-7B Directly — — — Qwen2.5-7B HA 0.4930 0.3580 0.4250 E Comparison of Retrieval Results between Latent Representation Query and Profile Query To examine the differences between the latent representation query and the profile query, we sampled a set of user-item pairs and calculated the pairwise similarity among the reviews retrieved by each query type. The results are shown in Fig. 4 and Fig. 5. As