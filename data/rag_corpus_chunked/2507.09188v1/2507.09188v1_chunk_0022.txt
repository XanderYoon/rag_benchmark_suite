and bars) are considered as items. The dataset provides rich information about local businesses, including user review text and ratings under multiple categories. • Google-reviews comes from user reviews of businesses (such as restaurants, stores, attractions, etc.) on Google, including ratings, review text and timestamps, as well as business metadata. B Metrics • GPTscore [18] utilizes LLMs to evaluate text quality. We regard GPT-3.5-Turbo ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. • BAR Tscore [19] assesses the similarity between each token in the text by using the contextual embeddings generated by the pre-trained BERT model and calculating the sum of the cosine similarities of the token embeddings between two sentences. Given a reference sentence x = ⟨x1, x2, . . . , xn⟩ and a candidate sentence ˆx = ⟨ˆx1, ˆx2, . . . , ˆxm⟩. Bert computes word embeddings sequence for the reference sentence and the candidate sentence as follows: x = ⟨x1, x2, . . . ,xn⟩ = BERT(x = ⟨x1, x2, . . . , xn⟩) (11) ˆx = ⟨ˆx1, ˆx2, . . . ,ˆxn⟩ = BERT(x = ⟨ˆx1, ˆx2, . . . , ˆxn⟩) (12) The cosine similarity of a reference token xi and a candidate token ˆxj is x⊤ i ˆ xj ∥xi∥∥ˆ xj ∥. As both embeddings are pre-normalized, the formula is reduced to the inner product x⊤ i ˆ xj. Then the BERTPrecision score , BERTRecall score and BERTF1 score are computed as follows: BERTPrecision score = 1 |x| X xi∈x max ˆxj ∈ˆx x⊤ i ˆ xj, (13) BERTRecall score = 1 |ˆx| X ˆxj ∈ˆx max xi∈x x⊤ i ˆ xj, (14) BERTF1 score = 2 PBERT · RBERT PBERT + RBERT