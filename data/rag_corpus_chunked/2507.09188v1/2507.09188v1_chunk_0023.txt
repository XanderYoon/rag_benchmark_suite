inner product x⊤ i ˆ xj. Then the BERTPrecision score , BERTRecall score and BERTF1 score are computed as follows: BERTPrecision score = 1 |x| X xi∈x max ˆxj ∈ˆx x⊤ i ˆ xj, (13) BERTRecall score = 1 |ˆx| X ˆxj ∈ˆx max xi∈x x⊤ i ˆ xj, (14) BERTF1 score = 2 PBERT · RBERT PBERT + RBERT . (15) C Baselines • NRT [6] tackles both rating prediction and tip generation by learning shared representations from user and item IDs through a joint optimization approach. It utilizes a GRU to produce concise, abstractive tips. 12 • Att2Seq [20] builds upon an attribute-to-sequence framework where an attention mechanism helps the model focus on relevant input features. A stacked LSTM is employed for decoding the review text. • PETER [21] introduces a Transformer-based approach that personalizes review generation by linking ID-based representations of users and items with natural language output. Due to dataset limitations, the basic version without auxiliary word features is applied. • PEPLER [22] leverages a pretrained language model to generate textual explanations. It refines this process with techniques like sequential adaptation and regularization to narrow the gap between the prompt structure and the language model’s expectations. • XRec [11] enhances text generation by injecting collaborative filtering signals from GNN-based user and item encoders into every layer of a language model, enabling more contextually relevant and personalized content. • G-Refer [12] strengthens explainable recommendation by retrieving and translating collaborative filtering signals from user-item graphs into human-readable text, enabling large language models to generate personalized explanations through retrieval-augmented fine-tuning. D Analysis of Hierarchical Aggregation To validate the effectiveness of the Hierarchical Aggregation (HA) method, we compare it with the “Direct” method, where the LLM is invoked only once to generate the profile based on all reviews at once.