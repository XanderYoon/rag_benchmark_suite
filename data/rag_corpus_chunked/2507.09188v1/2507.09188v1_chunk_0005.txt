X u∈Ni 1p |Ni| p |Nu| e(l) u (1) where e(l) u and e(l) i denote the embedding of user u and item i after l-layer propagation, respectively. Nu denotes the set of users who have interacted with item i, and Ni indicates the set of items interacted by user u. The user and item embeddings are extracted by averaging their embeddings from all GCN layers: eu = LX l=0 1 L + 1 e(l) u , ei = KX l=0 1 L + 1 e(l) i (2) 3 where L denotes the number of layers. To align with the dimensionality of LLM’s representation space, we further employ a three-layer feedforward neural network to project user and item embeddings. We utilize the negative log-likelihood (NLL) as our training loss, while the parameters of LLM are frozen. L = − 1 N NX i=1 CiX c=1 yi,c log(ˆyi,c). (3) Here N denotes the total number of user-item pairs to be explained, and C denotes the token count in each explanations. yi,c and ˆyi,c correspond to the ground-truth and predicted tokens at position c of i-th sample, respectively. 2.2 Hierarchical Aggregation Based Profiling Item textual attributes (e.g., item title, item category, etc) contain important features for capturing item characteristics. Nevertheless, as many items share similar textual attributes, there is insufficient information for constructing distinguishing profiles, making later explanation generation lack useful supporting inputs. Instead, reviews written by humans reflect user preferences and item characteristics. Therefore, incorporating and summarizing fragmented human-written reviews using natural language as user and item profiles can enhance the richness and usefulness of the constructed profiles, providing better supporting inputs for LLM to generate persuasive and comprehensive recommendation explanations. Existing LLM-based ExRec methods XRec and G-Refer utilize LLM as the summarizer. Since processing all the reviews of a popular