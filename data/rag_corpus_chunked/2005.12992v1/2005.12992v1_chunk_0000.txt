arXiv:2005.12992v1 [cs.IR] 21 May 2020 Evaluating Information Retrieval Systems for Kids Ashlee Milton ashleemilton@u.boisestate.edu PIReT – People and Information Research Team Boise State University, Boise, Idaho, USA Maria Soledad Pera solepera@boisestate.edu PIReT – People and Information Research Team Boise State University, Boise, Idaho, USA ABSTRACT Evaluation of information retrieval systems (IRS) is a prom inent topic among information retrieval researchers–mainly dir ected at a general population. Children require unique IRS and by ext en- sion diﬀerent ways to evaluate these systems, but as a large p op- ulation that use IRS have largely been ignored on the evaluat ion front. In this position paper, we explore many perspectives that must be considered when evaluating IRS; we specially discuss prob- lems faced by researchers who work with children IRS, includ ing lack of evaluation frameworks, limitations of data, and lack of user judgment understanding. CCS CONCEPTS • Social and professional topics → Children; • Information systems → Information retrieval ; Evaluation of retrieval results ; • Human-centered computing → User studies. KEYWORDS Information Retrieval, Evaluation, Children 1 THE ISSUE Evaluation in information retrieval (IR) remains a core research in- terest. Unlike most disciplines, evaluation of informatio n retrieval systems (IRS), including popular ones like recommender systems and search engines, is not limited to eﬀectiveness, as IR str ives to give users what makes them happy not necessarily what they asked for [3]. IR evaluation is an ongoing topic of interest a mong researchers and practitioners, with the focus being on metr ics and their applicability for diﬀerent tasks (e.g., multilingua l IR), diﬀer- ent domain (e.g., medical and legal), and applicability concepts like bias and fairness [4, 14, 16, 21, 22]. The target users of thes e explo- rations on evaluation have been a traditional user [3]. Thus , the frameworks