arXiv preprint arXiv:2410.11414 , 2024. [9] S. Patel, H. Kane, and R. Patel, “Building domain-specific llms faithful to the islamic worldview: Mirage or technical possibility?” arXiv preprint arXiv:2312.06652, 2023. www.ijacsa.thesai.org 10 | P a g e (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 16, No. 2, 2025 [10] Y . B. I. Radi, Al-Bitaqat: Chapters of the Noble Quran Explored in 114 Cards. Dakwah Corner Bookstore (M) Sdn. Bhd, 2023. [11] C. Njeh, H. Nakouri, and F. Jaafar, “Enhancing rag-retrieval to improve llms robustness and resilience to hallucinations,” in International Con- ference on Hybrid Artificial Intelligence Systems . Springer, 2024, pp. 201–213. [12] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval- augmented generation for knowledge-intensive nlp tasks,” Advances in Neural Information Processing Systems , vol. 33, pp. 9459–9474, 2020. [13] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod- els are few-shot learners,” Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020. [14] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” arXiv preprint arXiv:2312.10997 , 2023. [15] E. Kamalloo, N. Dziri, C. L. Clarke, and D. Rafiei, “Evaluating open- domain question answering in the era of large language models,” arXiv preprint arXiv:2305.06984, 2023. [16] Y . Zhou, Y . Liu, X. Li, J. Jin, H. Qian, Z. Liu, C. Li, Z. Dou, T.-Y . Ho, and P. S. Yu, “Trustworthiness in retrieval-augmented generation systems: A survey,” arXiv preprint arXiv:2409.10102 , 2024. [17] S. Patel, H. Kane, and