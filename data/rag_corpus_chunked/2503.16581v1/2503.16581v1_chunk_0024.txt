To www.ijacsa.thesai.org 8 | P a g e (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 16, No. 2, 2025 compute this metric, we examined the percentage of instances where both evaluators assigned identical evaluation scores independently. This measure enabled us to evaluate the degree of concordance between evaluators and assess their consistency in evaluation. TABLE I. T HE KAPPA VALUES FOR THE EVALUATION Creator Model Evaluator Meta Llama 3.3 70B 0.80 Meta Llama 3.2 3B 0.90 Meta Llama 3.2 1B 0.82 Meta Llama 3.1 70B 0.82 Meta Llama 3.1 8B 0.85 Meta Llama 3 70B 0.92 Meta Llama 3 8B 0.80 Google Gemma 2 27B 0.90 Google Gemma 2 9B 0.92 Google Gemma 2 2B 0.83 Microsoft Phi 3.5 3.8B 0.83 Microsoft Phi 3.3 3.8B 0.93 Alibaba QwQ 32B 0.89 In Table I, we delve into the inter-annotator agreement analysis, which measures the level of agreement between human evaluators across different models using Fleiss’ Kappa. This indicates a similar level of agreement between the eval- uators. IV. D ISCUSSION This section discusses the experimental findings, analyzing the performance of various LLMs categorized into large, medium, and small models across the three evaluation metrics: context relevance, answer faithfulness, and answer relevance [14]. This study examines the relationship among model size, response quality, and computational trade-offs, as well as the behavior of models within the Retrieval-Augmented Genera- tion (RAG) framework. A. Performance Insights Based on Model Size The experimental results show that the quality of the responses across all three evaluation criteria is significantly affected by the model size. • Large Models (Red) : Large models, including Llama3:70b, Llama3.1:70b, Llama3.3:70b, Gemma2:27b, and QwQ:32b, consistently demonstrate superior performance compared to medium and small models. The models demonstrated superior context relevance due to their larger parameter sizes [39], which facilitate a