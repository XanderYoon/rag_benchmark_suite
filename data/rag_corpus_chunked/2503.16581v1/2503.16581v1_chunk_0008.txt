Search and Retrieval: Upon a user’s query submission, the system does a semantic similarity search over the vectorized dataset obtained from Qur’anic surah descriptions [19]. This procedure de- termines the most contextually pertinent entries from the dataset to respond to the query. • Response Generation: The retrieved descriptions are submitted to the LLMs, which produce a compre- hensive response [14]. This response integrates the retrieved information and provides explanatory content to address the query. • Citations and Contextualization: Each generated response includes references to the original dataset entries (e.g., surah descriptions or specific virtues), allowing users to trace the information back to its source [25]. 2) Evaluation Guidelines: To assess the quality of the responses generated by the system, human evaluators followed a structured set of evaluation guidelines. These guidelines provided a consistent framework for scoring responses across three key dimensions: Context Relevance, Answer Faithful- ness, and Answer Relevance [14]. Each dimension is explained below, along with its calculation method and examples. • Context Relevance evaluates how precisely the re- trieved and generated responses align with the user query while avoiding irrelevant or extraneous infor- mation. The relevance score is calculated using the precision@k metric, where k represents the number of top retrieved results considered as shown in Equa- tion 1. Precision@k = No. of relevant results in the top-k responses k (1) Example: ◦ Query: “What is the reason for Surah Al- Fatihah being named Umm Al-Kitab?” ◦ Retrieved Information: 1) Surah Al-Fatihah is named Umm Al- Kitab because it summarizes the essence of the Qur’an (relevant). 2) It is recited in every unit of prayer (rel- evant). 3) Surah Al-Baqarah discusses laws and stories (irrelevant). 4) Surah Al-Fatihah has seven verses (rele- vant). 5) Surah An-Nas is the last chapter of the Qur’an (irrelevant). If k = 5