the limitations of standalone models. The comparative analysis focuses on assessing the relevance, faithfulness, and contextual accuracy of the responses generated by the LLMs within this framework. A. Experimental Results The experimental evaluation assessed the capacity of a variety of large language models (LLMs) to respond to queries related to quranic studies. The models were assessed based on three critical metrics: context relevance, answer faithfulness, and answer relevance. The models were categorized into three categories based on their parameter sizes: large models (marked in red), medium models (marked in yellow), and small models (marked in green). Therefore, the results were analyzed. The following is a comprehensive analysis of the performance of each category. 1) Context Relevance: Context relevance as shown in Fig- ure 1 evaluates how well the generated responses align with the query’s context. • Large Models (Red): The large models outperformed other categories, with Llama3.3:70b achieving the highest score of 0.583, followed by Llama3.2:3b Fig. 1. Context Relevance by the 13 LLMs. (0.508), despite being categorized as a small model. Both Llama3.1:70b and Gemma2:27b achieved com- petitive scores of 0.492 and 0.429, respectively, while QwQ:32b recorded 0.397. These models excel at re- trieving relevant information and aligning responses with the query intent due to their larger parameter size. • Medium Models (Yellow): Among the medium mod- els, Gemma2:9b performed best with a score of 0.492, comparable to some large models. Llama3:8b and Llama3.1:8b followed with scores of 0.381 and 0.317, respectively. These models demonstrated decent performance but lagged behind the large models in handling complex or nuanced queries. • Small Models (Green) : The small models strug- gled overall, with Llama3.2:1b achieving the lowest score of 0.254. Phi3.5:3.8b and Phi3:3.8b performed moderately with scores of 0.333, while Gemma2:2b achieved 0.413. Notably, Llama3.2:3b outperformed all expectations with a score