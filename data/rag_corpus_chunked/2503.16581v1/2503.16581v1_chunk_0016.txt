robust measure of agreement across evaluators, validating the credibility of the evaluation process and the results. IEA measures the level of consistency between evaluators when scoring responses based on predefined criteria: context relevance, answer faithfulness, and answer relevance. An el- evated IEA score signifies uniform application of evaluation criteria by assessors, whereas a diminished score reveals inconsistencies that may necessitate recalibration. Fleiss’ Kappa, as presented in Equation 2, was employed to calculate IEA due to its appropriateness for evaluating www.ijacsa.thesai.org 5 | P a g e (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 16, No. 2, 2025 agreement among multiple evaluators concurrently [33]. The consistent monitoring of IEA scores allowed the research team to detect discrepancies promptly and facilitate recalibration sessions as needed, ensuring evaluators’ comprehension and interpretation of the scoring guidelines were aligned. This methodical approach guaranteed that the evaluation process was dependable, replicable, and aligned with the study’s goals. κ = ¯P − ¯Pe 1 − ¯Pe (2) where: ¯P = 1 N NX i=1 Pi and ¯Pe = KX k=1 p2 k (3) with: Pi = 1 n(n − 1) KX k=1 nik(nik − 1), p k = PN i=1 nik N n (4) where: • κ: Fleiss’ Kappa value. • ¯P : Average observed agreement across all items. • ¯Pe: Expected agreement based on chance. • Pi: Proportion of agreement for item i. • pk: Proportion of ratings in category k. • n: Total number of ratings per item. • nik: Number of raters who assigned category k to item i. • N: Total number of items. • K: Total number of categories. F . Large Language Models The efficacy of numerous large language models (LLMs) in responding to inquiries regarding quranic studies is assessed in this study using a Retrieval-Augmented Generation (RAG)