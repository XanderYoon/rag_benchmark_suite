research emphasizes both the difficulties and potential benefits of utilizing general-purpose LLMs for specialized tasks, including responding to inquiries about quranic studies. Although large models excelled in aligning responses with the given dataset, their effectiveness is significantly dependent on the quality and organization of the retrieved content. This re- search demonstrates how important it is to add domain-specific knowledge [4], like curated descriptive datasets, to improve the models’ abilities and lower the risk of hallucinations. The RAG framework proved to be an effective technique for ensuring that responses were contextually correct and based on credible sources. The experimental results offer important insights into the performance of LLMs of different sizes. Large models pro- vide exceptional accuracy and relevance, though they require significant resources, whereas medium models offer a practi- cal compromise between performance and efficiency. Smaller models, while typically less powerful, demonstrated surprising potential, especially Llama3.2:3b, which excelled across var- ious metrics. The use of the RAG framework enhanced the models’ performance by reducing hallucinations and ground- ing responses in reliable data. These findings highlight the importance of model selection, optimization strategies, and retrieval mechanisms when applying LLMs to domain-specific tasks. Future work will include conducting an ablation study to compare the performance of models with and without the RAG framework, evaluating additional large language models (LLMs), and leveraging LLMs for automatic evaluation of answer faithfulness and answer relevance metrics. Additional fine-tuning strategies and assessments in other specialized domains can also be explored. V. C ONCLUSIONS This study evaluated multiple large language models (LLMs) of different sizes in responding to Quranic studies- related queries using a Retrieval-Augmented Generation (RAG) framework. The findings indicate that large mod- els, such as Llama3:70b, Llama3.1:70b, and Gemma2:27b, consistently delivered superior performance in context rele- vance, answer faithfulness, and answer relevance. However, their computational demands