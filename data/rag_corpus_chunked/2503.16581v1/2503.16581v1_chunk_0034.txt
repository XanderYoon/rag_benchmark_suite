generative large language models,” arXiv preprint arXiv:2405.18638 , 2024. [32] K. Feng, K. Ding, K. Ma, Z. Wang, Q. Zhang, and H. Chen, “Sample- efficient human evaluation of large language models via maximum discrepancy competition,” arXiv preprint arXiv:2404.08008 , 2024. [33] F. Moons and E. Vandervieren, “Measuring agreement among sev- eral raters classifying subjects into one-or-more (hierarchical) nom- inal categories. a generalisation of fleiss’ kappa,” arXiv preprint arXiv:2303.12502, 2023. [34] S. B. Islam, M. A. Rahman, K. Hossain, E. Hoque, S. Joty, and M. R. Parvez, “Open-rag: Enhanced retrieval-augmented reasoning with open- source large language models,” arXiv preprint arXiv:2410.01782, 2024. [35] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. , “Llama: Open and efficient foundation language models,”arXiv preprint arXiv:2302.13971, 2023. [36] G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ram ´e et al. , “Gemma 2: Improving open language models at a practical size,” arXiv preprint arXiv:2408.00118, 2024. [37] J. Bai, S. Bai, Y . Chu, Z. Cui, K. Dang, X. Deng, Y . Fan, W. Ge, Y . Han, F. Huang et al. , “Qwen technical report,” arXiv preprint arXiv:2309.16609, 2023. [38] M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach, A. Bahree, A. Bakhtiari, J. Bao, H. Behl et al., “Phi-3 technical report: A highly capable language model locally on your phone,”arXiv preprint arXiv:2404.14219, 2024. [39] S. Badshah and H. Sajjad, “Quantifying the capabilities of llms across scale and precision,” arXiv preprint arXiv:2405.03146 , 2024. [40] S. Tonmoy, S. Zaman, V . Jain, A. Rani, V . Rawte, A. Chadha, and A. Das, “A comprehensive survey of hallucination mitigation techniques in large language models,” arXiv preprint arXiv:2401.01313