(IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 16, No. 2, 2025 Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models Zahra Khalila ∗, Arbi Haza Nasution ∗, Winda Monika §, Aytug Onan ¶, Yohei Murakami ∥, Yasir Bin Ismail Radi ∗∗, Noor Mohammad Osmani x ∗Department of Informatics Engineering, Universitas Islam Riau, Pekanbaru 28284, Indonesia §Department of Library Information, Universitas Lancang Kuning, Riau 28266, Indonesia ¶Department of Computer Engineering, College of Engineering and Architecture, Izmir Katip Celebi University, Izmir, 35620 Turkey ∥Faculty of Information Science and Engineering, Ritsumeikan University, Kusatsu, Shiga 525-8577, Japan ∗∗Faculty of Al-Quran & Sunnah, Universiti Islam Antarabangsa Tuanku Syed Sirajuddin (UniSIRAJ), Kuala Perlis, Perlis 02000, Malaysia x Department Of Qur’an And Sunnah Studies, Ahas Kirkhs, International Islamic University Malaysia, Malaysia Email: arbi@eng.uir.ac.id Abstract—Accurate and contextually faithful responses are critical when applying large language models (LLMs) to sen- sitive and domain-specific tasks, such as answering queries related to quranic studies. General-purpose LLMs often struggle with hallucinations, where generated responses deviate from authoritative sources, raising concerns about their reliability in religious contexts. This challenge highlights the need for systems that can integrate domain-specific knowledge while maintaining response accuracy, relevance, and faithfulness. In this study, we investigate 13 open-source LLMs categorized into large (e.g., Llama3:70b, Gemma2:27b, QwQ:32b), medium (e.g., Gemma2:9b, Llama3:8b), and small (e.g., Llama3.2:3b, Phi3:3.8b). A Retrieval-Augmented Generation (RAG) is used to make up for the problems that come with using separate models. This research utilizes a descriptive dataset of Quranic surahs including the meanings, historical context, and qualities of the 114 surahs, allowing the model to gather relevant knowledge before responding. The models are evaluated using three key metrics set by human evaluators: context relevance, answer faithfulness, and answer relevance. The findings reveal that large models consistently outperform smaller