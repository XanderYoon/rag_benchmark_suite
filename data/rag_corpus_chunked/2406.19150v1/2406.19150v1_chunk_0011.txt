answers through retrieving relevant knowledge from a large exter- nal non-overlapping database with the fine-tuning datasets. Our experiments show clear benefits of our approach compared to non-retrieval baselines. Furthermore, the performance is competitive with similarly sized models, and even exceeds the per- formance of existing widely used captioning and VQA models several magnitudes larger. 4.1 Training Setup 4.1.1 Data We make use of an external memory and task spe- cific fine-tuning datasets in our implementation. For captioning, we use the MSCOCO 2014 Karpa- thy Splits for fine-tuning and NoCaps for a zero- shot evaluation. For VQA, we use the VQA v2 dataset augmented with VG-QA questions during fine-tuning. We use Laion-5B index as our exter- nal memory and map down to Laion-COCO 600M subset to retrieve image-caption pairs. The datasets are summarized in Table 1 and 2. Notably, unlike prior work, we ensure the fine-tuning datasets and external memory do not have any overlap, to real- ize the true benefits of retrieval augmentation in practical settings. Missing Samples: Retrieved data can be missing for 2 reasons: (1) lack of matches of the Laion- 5B retrieved samples with the Laion-COCO 600M DatasetSplit# of images(original)# of images(caption)# of images(caption + image)Size - w or w/oretrieval MSCOCOKarpathySplit (2014) train 113287 108780 107800 37G / 64Gval 5000 4776 4725 330M / 573Mtest 5000 4817 4778 329M / 576M NoCapsval 4500 4275 4239 295M / 512M Table 1: Captioning dataset summary Split# of samples# of images(original)# of images(caption)# of images(caption + image)Size - w or w/oretrieval train 1,358,769 121,277 116,439 115,387 106G / 151Gval 10,402 2,000 1,924 1,906 653M / 1.2Gtest-dev107,394 36,807 35,107 34,760 28G / 50Gtest-std447,793 81,434 77,856 77,098 28G / 50G Table 2: VQA v2 dataset summary subset, and (2) raw image download failure. For captioning, we only work on the subset of