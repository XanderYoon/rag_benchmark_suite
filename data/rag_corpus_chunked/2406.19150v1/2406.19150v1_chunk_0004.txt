perform other vision-and- language tasks. Earlier works in this direction unified multiple tasks like image captioning, im- age classification etc. using a simple sequence-to- sequence framework. Some notable examples in- clude OFA (Wang et al., 2022b), GIT (Wang et al., 2022a), SimVLM (Wang et al., 2021). Recent vision-language models (Biten et al., 2022) aug- ment pre-trained large language models with visual encoder. For example, Frozen (Tsimpoukelli et al., 2021), Flamingo (Alayrac et al., 2022), BLIP (Li et al., 2022), InstructBLIP (Dai et al., 2023), LLaV A (Liu et al., 2023), MiniGPT-4 (Zhu et al., 2023), Kosmos-1 (Huang et al., 2023), Pali (Chen et al., 2022c). In this work, we use OFA (Wang et al., 2022b) as the baseline rather than using VLMs augmented with pretrained LLMs. This choice allows us to remove the effects of in-context learning abilities of the pretrained language mod- els from the resulting enhancement brought by retrieval-augmented vision-language modeling. 2.2 Retrieval Augmented Generation in NLP Retrieval augmentation has become an important technique for improving natural language process- ing models. One of the first works in this area was kNN-LM by Khandelwal et al. (Khandelwal et al., 2020) who showed how interpolating over nearest neighbors from any text collection could improve generalization. This was followed by RETRO (Borgeaud et al., 2021), which scaled up the retrieval corpus to trillions of tokens. Another line of work has focused on integrating Wikipedia passages directly into models like REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard and Grave, 2021). By retrieving and conditioning on relevant Wikipedia passages, these models can better perform knowledge-intensive downstream tasks like question answering. Over- all, retrieval augmentation has proven to be a highly effective way of injecting knowledge into language models to improve their capabilities. The techniques have progressed from