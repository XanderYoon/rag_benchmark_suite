128.0 102.8 - - 252M BLIPCapFilt-L(Li et al., 2022) 0.404 136.7 113.2 - - 172M VL-T5 (Cho et al., 2021) 0.346 116.1 4.4 - - 1.4B SimVLM (huge, (Wang et al., 2021)) 0.406 143.3 110.3 - - 5.1B GIT2 (current SOTA (Wang et al., 2022a)) 0.432 146.4 126.9 *Gain with respect to the non retrieved baseline is comparable to the only prior work which reported it for the MSCOCO captioning task (Sarto et al., 2022) Table 3: Fine-tuning evaluation results using cross-entropy optimization on MSCOCO, and NoCaps benchmarks, compared with different image captioning baselines. For NoCaps, we finetune on MSCOCO karpathy train following prior works (Li et al., 2022), and perform zero-shot evaluation. We use the Laion-5B image index mapped down to the Laion-COCO 600M subset as our external memory. We report BLEU@4 and CIDer scores for different methods and show the gain in the best performing models compared to the non-retrieved baseline. Test-Dev Accuracy %# of Parameters Ablation Description number other yes/no overall Our Approach (Text Retrieval) 182M no retrieved samples 58.55 67.47 90.12 75.89 182M alttext 61.10 67.94 90.10 76.29 182M alttext + all captions 57.84 67.92 90.46 76.06 182M top caption + all captions 61.33* (+ 2.78%) 68.27*(+ 0.80%) 90.54* (+0.42%) 76.75* (+0.86%) VQA Baselines (Fine-tuning) 122M UnifiedVLP (Zhou et al., 2020) 52.10 60.30 87.20 70.50 252M BLIPCapFilt-L(Li et al., 2022) - - - 78.25 1.4B SimVLM (huge, (Wang et al., 2021)) - - - 80.30 80B Flamingo (Alayrac et al., 2022) - - - 82.00 55B PaLI-X (2023) - current SOTA (Chen et al., 2022c)- - - 86.10 *Gain with respect to the non retrieved baseline surpasses that of the only prior work which reported it for the VQA v2 task (Gur et al., 2021) Table 4: Finetuning evaluation results on VQA v2 benchmarks, compared with