external database for zero and in-context few- shot image-to-text generations. Recently, Iscen et al. (Iscen et al., 2023) proposed to equip con- trastive vision-text models with the ability to refine their embedding with cross-modal retrieved infor- mation from a memory at inference time, which greatly improved their zero-shot predictions. Hu et al. (Hu et al., 2023) presented REVEAL that learns to encode world knowledge into a large- scale memory, and to retrieve from it to answer knowledge-intensive queries, and achieves state-of- the-art results on visual question answering and im- age captioning. In text-to-image generation, Chen et al. (Chen et al., 2022b) presented Re-Imagen that uses retrieved information to produce high-fidelity and faithful images, even for rare or unseen entities. RA-CM3 is the first multimodal model that can retrieve and generate mixtures of text and images and exhibits novel capabilities such as knowledge- intensive image generation and multi-modal in- context learning (Yasunaga et al., 2023). Our multitask framework, RA VEN, extends be- yond RA-CM3 by supporting both captioning and VQA, and it diverges from REVEAL (Hu et al., 2023) by attaining retrieval capabilities solely through fine-tuning, eliminating the need for pre- training and additional retrieval-specific parame- ters; and is adaptable to any base VLM. 3 Proposed Approach 3.1 RA VEN Framework Our framework, RA VEN, is illustrated in Figure 1. At a high level, given a multimodal input con- sisting of images and text, we use a retriever to retrieve relevant image-text pairs from a large ex- ternal memory. Subsequently, we use a pretrained multitask encoder-decoder VLM which refers to the retrieved context in addition to the multimodal query and generates a textual output. Importantly, we demonstrate that through short, but efficient, task specific fine-tuning of the base VLM, with concatenated retrieval augmented samples and no additional retrieval-specific trainable parameters, the