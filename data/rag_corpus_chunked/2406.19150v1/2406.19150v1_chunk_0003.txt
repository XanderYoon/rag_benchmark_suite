contributions are as follows: 1. We are the first to design a multitask re- trieval augmented VLM framework (RA VEN), which relies on only fine-tuning, no retrieval specific trainable parameters and is adaptable to any multitask base VLM. 2. Our method allows for comprehensive abla- tions which examine the trade-offs between retrieval modalities and their advantages rel- ative to non-retrieval baselines while using a non-overlapping and larger external memory. 3. We demonstrate the benefits and limitations of our approach on Image Captioning and VQA through quantitative and qualitative anal- ysis. Our results achieve a new state-of- the-art performance improvement compared to non retrieved baselines: +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps (using mag- nitudes of fewer parameters than prior works), and nearly a +3% accuracy on specific VQA question types. Broadly, our work expands the empirical knowl- edge on RAG techniques and contributes to the rapidly growing body of work focusing on their applications to multitask VLMs. Ultimately, this work establishes a clearer understanding of the role of retrieval augmentation in VLMs, paving the way for more efficient and sustainable approaches in the field. 2 Related Work 2.1 Vision Language Models Vision language models are an emerging type of multi-modal AI system that can process both vi- sual and textual data (Appalaraju et al., 2024, 2021) They build upon recent advances in com- puter vision and natural language processing to generate textual descriptions of images, answer visual questions, and perform other vision-and- language tasks. Earlier works in this direction unified multiple tasks like image captioning, im- age classification etc. using a simple sequence-to- sequence framework. Some notable examples in- clude OFA (Wang et al., 2022b), GIT (Wang et al., 2022a), SimVLM (Wang et al., 2021). Recent vision-language models (Biten et al., 2022) aug- ment pre-trained large language models with