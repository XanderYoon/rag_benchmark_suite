images are less informative (Gur et al., 2021). In captioning, additional textual context resembles few-shot inference (Yasunaga et al., 2023). Reasons for OFA(Wang et al., 2022b) as a VLM backbone. We list 4 reasons for choosing OFA rather than alternates like Beit-3 (Wang et al., 2023) and Open Flamingo (Awadalla et al., 2023): First, OFA is naturally suited to our approach as it unifies multiple modalities and tasks into a single Seq2Seq model; the multitask backbone is a delib- erate design choice that underscores the versatility of our approach and is a foundational element cru- cial to our modelâ€™s architecture. Second, we can easily endow the model retrieval augmented capa- bilities through short, but efficient, task specific fine-tuning with no additional trainable parameters. Moreover, we intentionally avoided recent MLLM models like LLaVa or Flamingo which contain an LM to not add additional trainable parameters, re- move their in-context learning ability and isolate re- trieval capabilities within an encoder-decoder back- bone, a first in the field.Third, the codebase is open source, modular and easy to extend. Finally, the base OFA model is not very large (182M param- eters) given our compute and finance limitations, but sufficient to demonstrate the benefits of our framework. 4 Experiments In this section, we evaluate the performance of our approach under the fine-tuning setting on various image captioning and VQA benchmarks. We aim to demonstrate the benefits of retrieval augmenta- tion on the generated captions and answers through retrieving relevant knowledge from a large exter- nal non-overlapping database with the fine-tuning datasets. Our experiments show clear benefits of our approach compared to non-retrieval baselines. Furthermore, the performance is competitive with similarly sized models, and even exceeds the per- formance of existing widely used captioning and VQA models several magnitudes larger. 4.1 Training Setup 4.1.1 Data We