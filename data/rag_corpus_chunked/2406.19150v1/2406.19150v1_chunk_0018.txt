Flamingo (Alayrac et al., 2022) - - - 82.00 55B PaLI-X (2023) - current SOTA (Chen et al., 2022c)- - - 86.10 *Gain with respect to the non retrieved baseline surpasses that of the only prior work which reported it for the VQA v2 task (Gur et al., 2021) Table 4: Finetuning evaluation results on VQA v2 benchmarks, compared with the non retrieval VQA baseline. We finetune our method on VQA v2 train split using a subset of the OFA dataset. We report Test-Dev accuracy % from the eval.ai server for different methods. 5 Results 5.1 Quantitative Analysis Captioning. The results for image captioning, presented in Table 3, reveal notable insights. Base- line comparisons indicate that both the retrieval- only and zero-shot in-context retrieval fall short of the no-retrieved samples baseline, underscor- ing the value of fine-tuning on the target dataset. The absence of zero-shot in-context retrieval ca- pabilities may be attributed to the absence of a language model in the transformer-based encoder- decoder VLM architecture. In the text-only abla- tion, concatenating with the top caption and/or all captions yields optimal performance, demonstrat- ing a gain of nearly 1 CIDEr point on MSCOCO and up to 4 CIDEr points on zero-shot NoCaps. The gain with respect to the non retrieved base- line is comparable to the only prior work which reported it (+1.2 CIDEr score) for the MSCOCO Figure 2: Examples of the retriever output given a query image. captioning task (Sarto et al., 2022). This empha- sizes the valuable contextual information provided by retrieved captions. However, concatenating with alt text proves less effective due to its inherent noise. Both image-only and combined image and text concatenation exhibit performance below the non-retrieved baseline, suggesting that retrieved im- ages and naive concatenation introduce noise rather than relevant context. In fine-tuning settings,