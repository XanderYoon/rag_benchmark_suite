images(original)# of images(caption)# of images(caption + image)Size - w or w/oretrieval train 1,358,769 121,277 116,439 115,387 106G / 151Gval 10,402 2,000 1,924 1,906 653M / 1.2Gtest-dev107,394 36,807 35,107 34,760 28G / 50Gtest-std447,793 81,434 77,856 77,098 28G / 50G Table 2: VQA v2 dataset summary subset, and (2) raw image download failure. For captioning, we only work on the subset of samples which have both retrieved captions and images. We validate that augmentation with images is not use- ful, and subsequently decide to only use retrieved captions for augmentation. For VQA, we retain the original dataset, and missing captions are handled with an empty string. This allows us to evaluate our results on the VQA evaluation server. Importantly, the model learns to be robust to samples which may not have corresponding retrieved context at inference; a scenario common in practice. 4.1.2 Implementation Our retriever uses the off-the-shelf CLIP image en- coder (Radford et al., 2021) for both the query and memory encoders. We use FAISS (Douze et al., 2017) to index the external Laion-5B image-based memory and perform MIPS-based top-50 retrieval. We then map down to the Laion-COCO 600M sub- set ensuring to select, when it exists, the top-1 im- age (excluding exact or near duplicates), and all associated metadata, including the top caption, all captions and alt text. The retrieved samples are con- catenated with the original samples in the TSV file provided as input during the fine-tuning process. We ensure our fine-tuning process is able to op- erate in resource constrained settings. We use a lightweight OFA-base (Wang et al., 2022b) model checkpoint of 182M parameters as our multitask VLM. The maximum sequence length is 1024. We fine-tune the model for 8-12 hours, upto 10 epochs, on 4 V100 32GB GPUâ€™s. Our implementation is in PyTorch. We increase the