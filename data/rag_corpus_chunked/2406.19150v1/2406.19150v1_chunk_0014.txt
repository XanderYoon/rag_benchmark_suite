In-Context Retrieval: During inference, this baseline directly concatenates the retrieved top cap- tion and all captions with the prompt. The objec- tive is to evaluate the model’s capacity to leverage retrieved context without any pretraining or fine- tuning. (3) No Retrieved Samples: In this scenario, the model undergoes fine-tuning solely on the tar- get dataset without incorporating any retrieved con- text. This baseline helps establish a performance reference point. VQA. No Retrieved Samples: Similar to the captioning task, this baseline involves fine-tuning the model exclusively on the target dataset without incorporating any retrieved context. In all cases, we report performance gains rela- tive to the “No Retrieved Samples” baselines to highlight the efficacy of our proposed approach. Notably, most prior work fail to report this base- line making it challenging to assess the benefits of retrieval augmentation. Additionally, we provide a comparative analysis by reporting recent baselines and the current State- of-the-Art (SOTA) for both captioning and VQA tasks. This comparative assessment considers per- formance metrics and the number of parameters, offering a comprehensive view of the landscape and positioning our model within the current state- of-the-art research. 4.2.2 Metrics In evaluating the performance of RA VEN for cap- tioning, we employ two key metrics: BLEU@4 and CIDEr. BLEU@4 measures the quality of gen- erated captions by assessing the overlap of n-grams (in this case, four-grams) between the generated caption and reference captions. Meanwhile, the CIDEr metric gauges the diversity and distinctive- ness of generated captions by considering consen- sus across multiple reference captions. For the VQA task, we utilize accuracy as the evaluation metric. This measure is computed using the Eval.ai server. 4.2.3 Ablations We explore three distinct sets of ablations for both captioning and VQA: text-only, image-only, and combined image and text. To the best of our knowl-