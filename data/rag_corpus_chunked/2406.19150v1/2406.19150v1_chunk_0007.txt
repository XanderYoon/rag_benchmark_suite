image-text pairs from a large ex- ternal memory. Subsequently, we use a pretrained multitask encoder-decoder VLM which refers to the retrieved context in addition to the multimodal query and generates a textual output. Importantly, we demonstrate that through short, but efficient, task specific fine-tuning of the base VLM, with concatenated retrieval augmented samples and no additional retrieval-specific trainable parameters, the model acquires retrieval properties which gen- eralizes to multiple tasks. We now describe both these components in detail. 3.2 Multimodal Retriever Our semantic search based retrieval system, relies on the Facebook AI Similarity Search (FAISS) li- brary (Douze et al., 2017). FAISS enables high- dimensional vector indexing within an external memory and facilitates efficient search through an approximate nearest neighbor approach based on a specified similarity measure, such as dot-product similarity. We utilize the publicly available Laion- 5B (Schuhmann et al., 2022) image-based index which consists of 5 billion images and correspond- ing alt text. To describe the retrieval steps in detail, we first encode the query image using a CLIP-based image encoder (Radford et al., 2021) into a dense vector. Next, we follow the Dense Retrieval method out- lined in Karpukhin et al. (2020) to retrieve the top ‘k’ (k can be specified by the user) image-text pairs by scoring the query (image) and memory data as follows: score(query, memory) = E(query)T E(memory) (1) where E is the CLIP-based image encoder. Fi- nally, we perform Maximum Inner Product Search (MIPS) over the memory to obtain the top ‘k’ candi- date image-text pairs sorted according to the score. Our retrieval approach ensures that the retrieved samples, which are provided as additional con- text to the model, along with the query image, are relevant, diverse and in the style of our tar- get datasets. Relevance is easily ensured through sampling based