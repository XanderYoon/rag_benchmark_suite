and distinctive- ness of generated captions by considering consen- sus across multiple reference captions. For the VQA task, we utilize accuracy as the evaluation metric. This measure is computed using the Eval.ai server. 4.2.3 Ablations We explore three distinct sets of ablations for both captioning and VQA: text-only, image-only, and combined image and text. To the best of our knowl- edge, we are the first to comprehensively discern the impact of text and image modalities in retrieval augmented VLMs, providing valuable insights to model practitioners. Captioning. For the text-only ablation, we ex- periment with various combinations, concatenating one or more of the top caption, all captions, and image alt text. This helps us discern the impact of textual information in isolation. In the image-only ablation, we alter the patch size, doubling it, and employ a horizontal concatenation strategy. If a re- trieved image is present, we concatenate it with the query image. In cases where the retrieved image is absent, we duplicate the query image. This analysis provides valuable insights into the modelâ€™s reliance on visual information alone. For the combined im- age and text ablation, we adopt a similar approach to the image-only case for processing images. Si- multaneously, we concatenate the top caption and all captions to the text prompt. This exploration allows us to understand the synergistic effects of both modalities. VQA. Building on insights gained from the cap- tioning task, where naive image fusion through concatenation proved less useful (see Table 3), we hypothesize that captions serve as good aux- iliary information in image-to-text tasks, while similar/retrieved images are less informative, since the content of the image and the objects contained is often very correlated with the question and an- swer. Therefore, in the VQA ablations, we exclu- sively consider text concatenation scenarios. This involves combining