task (Sarto et al., 2022). This empha- sizes the valuable contextual information provided by retrieved captions. However, concatenating with alt text proves less effective due to its inherent noise. Both image-only and combined image and text concatenation exhibit performance below the non-retrieved baseline, suggesting that retrieved im- ages and naive concatenation introduce noise rather than relevant context. In fine-tuning settings, our model performs competitively with similar-sized models such as BLIP. Notably, in the zero-shot set- ting on NoCaps, our model surpasses SimVLM (1.4B vs 182M parameters), achieving a CIDEr score of 111.0 compared to 110.3. VQA. Given the limited efficacy observed in the use of retrieved image for captioning (see Ta- ble 3), we exclusively explore text augmentation strategies for VQA. The results, presented in Table 4, align with the captioning outcomes, affirming the efficacy of text-only augmentation. Notably, across all question categories, text-only augmenta- tion yields improvements in accuracy ranging from 0.42% to 2.78%. The gain with respect to the non retrieved baseline surpasses that of the only prior work which reported it (+0.36% accuracy) for the VQA v2 task (Gur et al., 2021). The highest perfor- mance is achieved through concatenating the top caption and all captions with the question, while the addition of alt text introduces noise, resulting in lower performance. The overall performance of our model in VQA remains competitive and com- parable to similar-sized models, underscoring its robustness in leveraging textual information for accurate question answering. 5.2 Qualitative Analysis In this section, we present qualitative examples that elucidate the efficacy and limitations of our approach. Figure 3: Examples where RA VEN succeeds in generat- ing the correct answer. Retriever Output. Figure 2 illustrates the out- put of the retriever for a given query image. The retrieved images align with the query image, em- phasizing relevance.