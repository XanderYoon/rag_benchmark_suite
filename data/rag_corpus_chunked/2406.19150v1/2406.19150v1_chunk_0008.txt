(MIPS) over the memory to obtain the top ‘k’ candi- date image-text pairs sorted according to the score. Our retrieval approach ensures that the retrieved samples, which are provided as additional con- text to the model, along with the query image, are relevant, diverse and in the style of our tar- get datasets. Relevance is easily ensured through sampling based on the top similarity score. How- ever, simply sampling based on relevance score can result in exact or near duplicates resulting in poor performance. To avoid this redundancy and enhance diversity, we exclude near duplicate im- ages. Finally, to use COCO-style captions rather than the noisy image alt text in Laion-5B, we map the retrieved samples from Laion-5B down to the Laion-COCO 600M 1 subset, whose captions are synthetically generated using a BLIP model trained on COCO-style captions. This can result in some missing data due to lack of matches with LAION- COCO 600M and also due to failure of LAION- COCO 600M raw image downloads. Our approach is robust to these missing samples. 3.3 Base Vision-Language Model (VLM) RA VEN relies on a multitask, multimodal encoder- decoder base VLM which can easily leverage addi- 1https://laion.ai/blog/laion-coco/ tional multimodal context from an external mem- ory. Architecture. For image encoding, we use a ResNet, and for text encoding we use a byte-pair encoding (BPE) to convert the text sequence into a subword sequences, and then embed them into features. We adopt a unified vocabulary encom- passing linguistic and visual tokens, incorporating subwords, image codes, and location tokens. The base architecture is the transformer; this serves as the backbone for the encoder-decoder framework. To enhance stability and hasten convergence, the model uses head scaling for self-attention, post- attention layer normalization (LN), and LN fol- lowing the first layer of FFN. For positional infor-