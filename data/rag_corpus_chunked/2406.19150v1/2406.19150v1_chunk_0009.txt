unified vocabulary encom- passing linguistic and visual tokens, incorporating subwords, image codes, and location tokens. The base architecture is the transformer; this serves as the backbone for the encoder-decoder framework. To enhance stability and hasten convergence, the model uses head scaling for self-attention, post- attention layer normalization (LN), and LN fol- lowing the first layer of FFN. For positional infor- mation, separate absolute position embeddings are used for text and images. Notably, we decouple position correlation from token embeddings and patch embeddings, while employing 1D relative position bias for text and 2D relative position bias for images. VL Tasks. All cross-modal tasks are cast as Seq2Seq generation. We focus on 2 popular image-to-text tasks, image captioning and visual question answering (VQA). For image captioning, the model adeptly adopts the Seq2Seq format, gen- erating captions based on both the provided image and the input textual prompt, “What does the image describe?”. For VQA, the model takes in the im- age and the question as inputs, learning to generate accurate responses. Need for Retrieval in VL tasks. Retrieval can benefit performance in VL tasks as contextual information can be crucial for guiding models to ac- curate answers. Moreover, the retrieval mechanism can mitigate bias by sourcing information from di- verse datasets, countering the influence of biased training data. Specifically, in VQA, image con- tent, such as object attributes, strongly correlates with questions and answers, making captions valu- able auxiliary information while similar/retrieved images are less informative (Gur et al., 2021). In captioning, additional textual context resembles few-shot inference (Yasunaga et al., 2023). Reasons for OFA(Wang et al., 2022b) as a VLM backbone. We list 4 reasons for choosing OFA rather than alternates like Beit-3 (Wang et al., 2023) and Open Flamingo (Awadalla et al., 2023): First, OFA is naturally suited to our