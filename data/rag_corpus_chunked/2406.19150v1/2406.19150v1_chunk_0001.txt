billion parameters (Radford et al., 2019) to GPT-3’s 175 billion (Brown et al., 2020), and further to over a trillion in GPT-4 (OpenAI, 2023), is a source of increasing concern. This trend re- quires more data and computational power, lead- *Work conducted during an internship at Amazon. †Work done while at Amazon. ‡Correspondence to: srikara@amazon.com ing to higher carbon emissions and presenting sig- nificant obstacles for less-resourced researchers (Strubell et al., 2019). In response, the field is pivoting to approaches like Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), which in- corporates external non-parametric world knowl- edge into a pretrained language model, removing the necessity of encoding all information directly into the model’s parameters. However, this strat- egy is not yet widely applied in vision-language models (VLMs) (Li et al., 2022; Wang et al., 2021; Alayrac et al., 2022; Chen et al., 2022c; Radford et al., 2021; Wang et al., 2022b), which process both image and textual data, and are typically more resource-intensive. Moreover, VLMs often rely on massive datasets like LAION-5B (Schuhmann et al., 2022), presenting a significant opportunity for performance gains through retrieval augmenta- tion. The scant prior work exploring retrieval augmen- tation applied to VLMs, although promising, is beset with several limitations. Most importantly, they rely on pretraining with retrieval specific pa- rameters (Hu et al., 2023; Ramos et al., 2023b; Yang et al., 2023); as a result the performance im- provement over non-retrieval baselines cannot be established and the benefit due to retrieval augmen- tation cannot be independently discerned. Next, model architectures are suited to only a single task, and therefore, experimental evaluation is also only presented on a single task e.g. on image caption- ing (Ramos et al., 2023b,a; Yasunaga et al., 2023); other image-to-text tasks like VQA are ignored. Further, the decision on which modality