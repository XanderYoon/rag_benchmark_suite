ensure our fine-tuning process is able to op- erate in resource constrained settings. We use a lightweight OFA-base (Wang et al., 2022b) model checkpoint of 182M parameters as our multitask VLM. The maximum sequence length is 1024. We fine-tune the model for 8-12 hours, upto 10 epochs, on 4 V100 32GB GPU’s. Our implementation is in PyTorch. We increase the max source length from 80 upto 600 to account for the retrieved samples. Otherwise, we rely on the task-specific default hy- perparameters in the OFA-base run scripts. Following the OFA implementation, we optimize the model with the standard cross-entropy loss. Given an input image i, a prompt t, and an output y, we minimize the loss L = − P|y| j=i log Pθ(yj|y < j, i, t) where θ refers to the model parameters. For inference, we decode using beam search, to en- hance the quality of generation. For the VQA task, we employ a trie-based search to only search over a bounded set of vocabulary (top 3129 VQA v2 answers) to prevent labels out of the closed label set during inference. 4.2 Evaluation Setup 4.2.1 Baselines We establish baselines to gauge the performance of RA VEN in comparison to various configurations: Captioning. (1) Retrieval Only: This baseline involves using the top caption retrieved from the memory as the generated output. It serves as a benchmark to assess the additional benefits gained through fine-tuning the OFA-base model. (2) Zero Shot In-Context Retrieval: During inference, this baseline directly concatenates the retrieved top cap- tion and all captions with the prompt. The objec- tive is to evaluate the model’s capacity to leverage retrieved context without any pretraining or fine- tuning. (3) No Retrieved Samples: In this scenario, the model undergoes fine-tuning solely on the tar- get dataset without incorporating any retrieved con-