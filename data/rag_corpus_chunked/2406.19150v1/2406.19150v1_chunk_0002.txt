and the benefit due to retrieval augmen- tation cannot be independently discerned. Next, model architectures are suited to only a single task, and therefore, experimental evaluation is also only presented on a single task e.g. on image caption- ing (Ramos et al., 2023b,a; Yasunaga et al., 2023); other image-to-text tasks like VQA are ignored. Further, the decision on which modality to prior- itize during retrieval - textual, visual, or a combi- nation of both - is not established. Some works (Yasunaga et al., 2023; Chen et al., 2022a) retrieve and concatenate both image and text, while others (Ramos et al., 2023a,b; Yang et al., 2023) only re- trieve text, even though they all evaluate on image- to-text tasks. Finally, we also observe that overlaps arXiv:2406.19150v1 [cs.CV] 27 Jun 2024 between the retrieval and pre-training/fine-tuning datasets exist; for example, Ramos et al. (2023a,b) pretrain and retrieve from MSCOCO. This can confound the benefits attributed to the RAG ap- proach, underscoring the need for a larger and non- overlapping external memory. In this paper, we present RA VEN (see Figure 1), a multitask retrieval augmented framework adapt- able to any multitask base VLM. The framework does not rely on pretraining with retrieval specific parameters, and is suitable to a variety of tasks. Importantly, the design of RA VEN allows for a comprehensive investigation of the performance benefits over non-retrieval baselines, and implica- tions of retrieving and using different modalities. Specifically, our key contributions are as follows: 1. We are the first to design a multitask re- trieval augmented VLM framework (RA VEN), which relies on only fine-tuning, no retrieval specific trainable parameters and is adaptable to any multitask base VLM. 2. Our method allows for comprehensive abla- tions which examine the trade-offs between retrieval modalities and their advantages rel- ative to non-retrieval