answering. 5.2 Qualitative Analysis In this section, we present qualitative examples that elucidate the efficacy and limitations of our approach. Figure 3: Examples where RA VEN succeeds in generat- ing the correct answer. Retriever Output. Figure 2 illustrates the out- put of the retriever for a given query image. The retrieved images align with the query image, em- phasizing relevance. However, Laion-5B’s image alt text is observed to be noisy and differs from the required COCO-style captions. Mapping down to synthetically generated BLIP captions from the LAION-COCO 600M subset, mitigates the style issue by mimicking the COCO caption style, and offers more valuable context to the model. Incorporating World Knowledge. Figure 3 demonstrates VQA outputs leveraging world knowledge. The model adeptly utilizes entity- rich captions from the retriever to disambiguate between entities, as seen in the bear image distin- guishing logs from rocks. Additionally, the model accurately identifies nuanced details, such as a boy squatting while playing baseball, by leveraging relevant context in the captions, such as the term “crouches." Retriever Failures. Despite successes, retrieved context may not consistently contribute to specific questions, particularly when inquiries concern en- tities not prominently featured in the image. This issue is more pronounced in tasks such as VQA, rather than in captioning, where general knowledge about the image is often sufficient to generate high quality and diverse captions. Illustrated in Figure 4, failure cases for VQA depict relevant but insuf- ficiently informative captions. For instance, cap- tions for an elephant image focus on the foreground elephant, neglecting details about the background mountains and forest. Similarly, captions for a cake image lack information about the cake lifter in the corner. Figure 4: Examples where RA VEN fails in generating the correct answer. Multimodal Query Embedding. Considering scenarios where retrieved context may lack speci- ficity,