or text-to-video, which only generate a single modality, but rather scenarios where the answers includes text and at least one other modality of data, such as text-image output, or image-video output.In the basic VQA task, MIMOQA[328] was the first to propose the concept of multimodal output, which achieved the capability of multimodal output by transforming questions into an image-text matching task. It constructed a dual-tower model called MExBERT. The text stream, based on BERT, takes in the query and related documents to output the final text answer. The visual stream, based on VGG-19, receives images related to the query and documents, outputting a relevance score between the image and text. The final insertion of the image is determined by this relevance score. Its groundbreaking introduction to multimodal output research has, however, certain limitations: 1) It is necessary to screen out images related to the question. The model only needs to select and output images from the small number of screened ones. The task is relatively simple. 2) Multimodality is , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 33 still limited to the image modality. 3) To simplify the issue, it is still limited to scenarios where the input images must include at least one relevant image. Based on the aforementioned limitations, the latest research has made corresponding improvements[67, 259, 401, 465, 466, 508]. A common workflow paradigm for implementing multimodal output is to first conduct position identification after generating a text answer to determine where to insert multimodal data. Subse- quently, based on the surrounding context of the corresponding positions, candidate multimodal data is retrieved. Finally, a relevance matching model is utilized to determine the final data to be inserted. InternLM-XComposer [ 465] achieves multimodal output of text and images. After