transformers via masked auto-encoder. arXiv: 2205.12035 (2022). [492] Xiaoyang Zheng, Zilong Wang, Sen Li, Ke Xu, Tao Zhuang, Qingwen Liu, and Xiaoyi Zeng. 2023. Make: Vision- language pre-training based product retrieval in taobao search. In Companion Proceedings of the ACM Web Conference 2023. 356â€“360. [493] Chenyu Zhou, Mengdan Zhang, Peixian Chen, Chaoyou Fu, Yunhang Shen, Xiawu Zheng, Xing Sun, and Rongrong Ji. 2024. VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models. arXiv preprint arXiv:2406.10228 (2024). [494] Dong Zhou, Fang Lei, Lin Li, Yongmei Zhou, and Aimin Yang. 2024. Cross-Modal Interaction via Reinforcement Feedback for Audio-Lyrics Retrieval. IEEE/ACM Transactions on Audio, Speech, and Language Processing (2024). , Vol. 1, No. 1, Article . Publication date: April 2018. 80 Trovato et al. [495] Junjie Zhou, Zheng Liu, Shitao Xiao, Bo Zhao, and Yongping Xiong. 2024. VISTA: visualized text embedding for universal multi-modal retrieval. arXiv preprint arXiv:2406.04292 (2024). [496] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. 2024. MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding. arXiv preprint arXiv:2406.04264 (2024). [497] Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong Wen, Nan Duan, et al. 2022. Simans: Simple ambiguous negatives sampling for dense text retrieval. arXiv preprint arXiv:2210.11773 (2022). [498] Tianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu. 2023. MARVEL: unlocking the multi-modal capability of dense retrieval via visual module plugin. arXiv preprint arXiv:2310.14037 (2023). [499] Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan. 2023. Efficient prompting via dynamic in-context learning. arXiv preprint arXiv:2305.11170 (2023). [500] Yujia Zhou, Zhicheng Dou, and Ji-Rong Wen. 2023. Enhancing generative retrieval with reinforcement learning from relevance feedback. In Proceedings of the 2023