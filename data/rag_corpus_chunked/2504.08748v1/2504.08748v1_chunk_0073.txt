is to first conduct position identification after generating a text answer to determine where to insert multimodal data. Subse- quently, based on the surrounding context of the corresponding positions, candidate multimodal data is retrieved. Finally, a relevance matching model is utilized to determine the final data to be inserted. InternLM-XComposer [ 465] achieves multimodal output of text and images. After generating each paragraph of text, it calls a model to determine whether to insert an image. If it is determined that an image needs to be inserted, it will generate a caption of the image to be inserted and search the web for candidate images, eventually allowing the model to select the most relevant image from candidate set for insertion. InternLM-XComposer2 and 2.5 [ 67, 466] allow users to directly input a set of candidate images on the basis of the above. MuRAR [508] has also implemented multi - modal output in RAG scenarios based on this paradigm, but it has innovated the methods of position identification and candidate set recall in RAG scenarios. It uses source attribution to confirm the correspondence between the generated snippet and the retrieved snippet from the large model input, thereby determining the insertion point, and the candidate set directly uses the multimodal data associated with the retrieved snippet, simplifying the recall operation. In addition, it has expanded the multimodal data from images to include tables and videos. ğ‘€ 2ğ‘…ğ´ğº [259] employs an alternative paradigm to achieve multimodal output in the RAG scenario. It uses the userâ€™s query to simultaneously recall associated text elements and images. Then, based on the associations of the images and text elements in the original document, they are refined. Subse- quently, MLLMs are employed to vectorize the images or convert them into descriptions, which are input into the generative model