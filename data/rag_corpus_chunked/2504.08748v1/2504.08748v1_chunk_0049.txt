a multi-stage training algorithm. E5-V [ 150] fine-tunes MLLMs on single-text or vision- centric relevance data, outperforming traditional image-text pair training. VLM2VEC [152] proposes a contrastive training framework to convert vision-language models into embedding models using the MMEB dataset [152]. To address modality imbalance, GME [476] trains an MLLM-based dense retriever on the large-scale UMRB dataset[ 476]. Ovis [244] aligns visual and textual embeddings by integrating a learnable visual embedding table, enabling probabilistic combinations of indexed embeddings for rich visual semantics. ColPali [ 79] leverages Vision Language Models and the ViDoRe benchmark [79] to index documents from their visual features, facilitating efficient query matching with late interaction mechanisms. CREAM [462] employs a coarse-to-fine retrieval and ranking approach, combining similarity calculations with large language model-based grouping and attention pooling for MLLM- based multi-page document processing. DSE [254] fine-tunes a large vision-language model on 1.3 million Wikipedia web page screenshots, enabling direct encoding of document screenshots into dense representations. • Generative Structure: Traditional information retrieval (IR) methods, which rely on similarity matching to return ranked lists of documents, have long been a cornerstone of information acquisition, dominating the field for decades. However, with the advent of pre-trained language models, generative retrieval (GR) has emerged as a novel paradigm, garnering increasing attention in recent years. GR primarily consists of two fundamental components: model training and document identifier. Model Training aims to train generative models to effectively index and retrieve documents, while enhancing the model’s capacity to memorize information from the document corpus. This is typically achieved through sequence-to-sequence (seq2seq) training, where the model learns to map queries to their corresponding Document Identifiers (DocIDs). The training process emphasizes optimizing the model’s understanding of semantic relationships between queries and documents, thereby improving retrieval accuracy. Document Identifiers (DocIDs) serve as the target output for the generative