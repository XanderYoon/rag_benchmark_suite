multimodal data is often converted into text for utilization, as seen in models like TableNet [286] for tables and UniChart [265] for charts. This necessitates distinct models for extracting captions from different modalities. With the advancement of MLLMs, there is a trend toward unifying these models into a single MLLM framework, leveraging their robust representation capabilities [173, 227]. Further developments in MLLMs enable the direct retention and input of original multimodal data during generation [312, 437, 474]. 3.1.2 Representation-based. Although extractive-based methods have been widely adopted, they suffer from several inherent limitations: (1) The parsing process is time-consuming, involves multiple steps, and requires different models for different document types; (2) Critical information, such as document structure, may be lost during extraction; and (3) Parsing errors can propagate to downstream tasks. Recent advancements in MLLMs [6, 20, 218] have enabled a novel approach that directly uses document screenshots as primary data for metadata indexing, addressing these issues [78, 170, 253, 342, 463]. To capture both global and local information, DSE [253] processes the document screenshot along with its sub-images through a unified encoding framework. Additionally, a late interaction mechanism, inspired by ColBERT [163], has been introduced to improve recall efficiency [78]. However, page-level document splitting may hinder the modelâ€™s ability to capture full context and inter-part relationships. To address this problem, a holistic document representation method has been proposed [170], which segments large documents into passages within the token limit of MLLMs. Empirical studies reveal a performance gap between multimodal and text-only retrieval, highlighting differences in effectiveness when using raw multimodal data versus text or combined modalities [312, 463]. Consequently, a new paradigm has emerged that leverages OCR for text indexing, document screenshots for multimodal indexing, and executes textual and visual RAG in parallel. The results from both streams are then