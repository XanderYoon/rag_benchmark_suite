the Siberian Husky . How to make steamed egg custard? Fig. 5. Multimodal output in QA scenarios can be categorized into three distinct types. In sub-scenario I, the user’s query can be fully addressed using only images or videos, without requiring supplementary textual information. Sub-scenario II involves a step-by-step explanation that combines text and images to ensure clarity and precision; omitting the images may lead to user confusion at specific steps. In sub-scenario III, supplementary images enrich the information conveyed in the answer, but their removal does not compromise the answer’s accuracy. – Native MLLM-Based Output: In this task, the generation of multimodal data is entirely model- driven, eliminating the need for external data sources to supplement the model responses. The most straightforward approach involves using a unified MLLM to produce the desired multimodal output in a single step, ensuring seamless integration of diverse data types, such as text, images, or audio, within a cohesive framework. – Augmented Multimodal Output: This method utilizes pre-existing multimodal data to enhance textual responses. After generating the text, the system executes three sequential subtasks to create the final multimodal output: 1) Position Identification: The system deter- mines optimal insertion points within the text where multimodal elements (e.g., images, videos, graphs) can be integrated to complement or clarify the content. This step ensures that the mul- timodal data aligns contextually with the text. 2) Candidate Set Retrieval: Relevant multimodal data is retrieved from external sources, such as the web or a knowledge base, by querying and filtering potential candidates that best match the text’s context and intent. 3) Matching and Insertion: The system selects the most appropriate multimodal element from the retrieved candidate set based on relevance, quality, and coherence. The chosen data is then seamlessly integrated into the identified positions, producing a cohesive and