DocID-based methods introduce learnable document representations, where DocIDs are optimized during training to better capture document semantics and improve retrieval performance. GenRet [340] employs a discrete autoencoder to encode documents into compact DocIDs, minimizing reconstruction error. MINDER [200] enhances document representations using multi-view identifiers, including pseudo-queries, titles, and sub-strings. NOVO [389] introduces learnable continuous N-gram DocIDs, refining embeddings through query denoising and retrieval tasks. LMIndexer [153] generates neural sequential discrete IDs via progressive training and contrastive learning, addressing semantic mismatches. ASI [420] automates DocID learning, assigning similar IDs to semantically close documents and optimizing end-to-end retrieval using an generative model. RIPOR [ 449] improves relevance scoring during sequential DocID generation using dense encoding and Residual Quantization [264]. GLEN [175] employs a dynamic lexical identifier with a two-phase index learning strategy. Firstly, the keyword-based DocID are defined by extracting keywords from documents using self-supervised signals. Secondly, dynamic DocIDs are refined by integrating query-document relevance, enabling efficient inference. The field of generative text retrieval is evolving from static, pre-defined DocIDs to dynamic, learnable DocIDs that better capture document semantics and relationships. Learnable DocIDs, combined with advanced techniques like reinforcement learning, knowledge distillation, and contrastive learning, are driving improvements in retrieval performance. â€“ Retrieval for Cross-modal. Similarly, MLLMs are considered to memorize and retrieve multimodal content, such as images and videos, within their parameters. When presented with a user query for visual content, the MLLM is expected to "recall" the relevant image from its parameters as a response. Achieving this capability presents significant challenges, particularly in developing effective visual memory and recall mechanisms within MLLMs. IRGen [479] employs a seq2seq model to predict discrete visual tokens (image identifiers) from , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 25 query images. Its key innovation is