30 Trovato et al. QA retrieval. QGC [25] retained key information under high compression using query-guided dynamic strategies. UniICL [91] unified demonstration selection, compression, and generation within a single frozen LLM, projecting demonstrations and inputs into virtual tokens for semantic-based processing. Recent advancements in prompt compression for LLMs focus on encoding hard prompts into reusable soft prompts to enhance efficiency and generalization across tasks. Early work by Wingate et al. [396] distilled complex hard prompts into concise soft prompts by minimizing output distribution differences, reducing inference costs. A series of works aim to enhance generalization across diverse prompts. Gist [279] used meta-learning to encode multi-task in- structions into gist tokens, while Gist-COCO [193] employed an encoder-decoder architecture to compresses original prompts into shorter gist prompts, via the Minimum Description Length principle. UltraGist [467] optimized cross-attention for compressing ultra-long contexts into near-lossless UltraGist tokens. AutoCompressor [49] iteratively compressed contexts segments into summary vectors using a Recurrent Memory Transformer, reducing computational load. Other approaches, like ICAE [99] and 500xCompressor [203], fine-tuned LoRA-adapted LLMs for context encoding and prompt compression. For LLM-based recommendations, POD [190] distilled discrete prompt templates into continuous prompt vectors with an whole-word em- bedding to integrate the item ID, while RDRec [379] synthesizes training data and internalizes rationales into a smaller model. SelfCP [90] balances training cost, inference efficiency, and generation quality by compressing over-limit prompts asynchronously using frozen LLMs as the compressor and generator and trainable linear layers to project hidden states into LLM-acceptable memory tokens. â€“ Refining for Cross-Model: PromptMM [393] tackles overfitting and side information inac- curacies in multi-modal recommenders by using Multi-modal Knowledge Distillation with prompt-tuning. It compresses models by distilling user-item relationships and multi-modal content from complex teacher models to lightweight student models, eliminating extra pa- rameters. Soft prompt-tuning bridges the semantic gap between