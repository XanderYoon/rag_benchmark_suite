to project hidden states into LLM-acceptable memory tokens. â€“ Refining for Cross-Model: PromptMM [393] tackles overfitting and side information inac- curacies in multi-modal recommenders by using Multi-modal Knowledge Distillation with prompt-tuning. It compresses models by distilling user-item relationships and multi-modal content from complex teacher models to lightweight student models, eliminating extra pa- rameters. Soft prompt-tuning bridges the semantic gap between multi-modal context and collaborative signals, enhancing robustness. Additionally, a disentangled multi-modal list-wise distillation with modality-aware re-weighting addresses multimedia data inaccuracies. RACC [395] compresses and aggregates retrieved knowledge for image-question pairs, generating a compact Key-Value (KV) cache modulation to adapt downstream frozen MLLMs for efficient inference. VTC-CLS [364] uses the prior knowledge of the association between the [CLS] token and visual tokens in the visual encoder to evaluate visual token importance, enabling Visual Token Compression and shortening visual context. VisToG [128] introduces a grouping mecha- nism using pretrained vision encoders to group similar image segments without segmentation masks. Semantic tokens represent image segments after linear projection and before input into the vision encoder. Isolated attention identifies and eliminates redundant visual tokens, reducing computational demands. However, as dataset size increases, so do the computational resource requirements. Additionally, soft prompts are less interpretable than hard prompts, as their continuous vectors are not directly readable or explainable by humans. 3.4 Multimodal Generation Multimodal generation based on Multimodal Large Language Models (MLLMs) represents a sig- nificant advancement, enabling the generation of content across multiple modalities such as text, images, audio, and video. These models leverage the strengths of large language models (LLMs) and extend them to handle and integrate diverse data types, creating rich, coherent, and contextually , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 31 Multimodal Generation Modality Augmentation InternLM-XComposer [465], InternLM-XComposer2 [ 67], InternLM-XComposer-2.5