to generate compact video-level representations, prioritizing efficiency , Vol. 1, No. 1, Article . Publication date: April 2018. 22 Trovato et al. over granularity. Later advancements incorporate additional modalities, such as audio and motion, to enhance video semantics [273]. For text encoding, simpler methods like Word2Vec, LSTMs, or GRUs are commonly used [274, 276, 441], with evidence suggesting that combining multiple text encoding strategies improves retrieval performance [64]. Transformer-based methods [89, 224] utilize self-attention mechanisms to jointly encode videos and texts, enabling cross-modal interaction. MMT [ 89] employs mutual attention between video and text modalities, integrating temporal information to enhance feature representation. Inspired by MoCo [ 116], HiT [ 224] introduces hierarchical cross-modal contrastive matching at both feature and semantic levels. Additionally, these methods [89, 224] encode diverse modalities in video data, such as audio and motion, further enriching video representations. Recently, VLP-based models [131, 247, 356, 371, 402, 421] utilize pretrained models like CLIP [305] to enhance text-video tasks in text-video retrieval tasks by capturing cross-modal and temporal dependencies. CLIP4Clip [247] adapt CLIP for text-video retrieval and caption- ing, analyzing temporal dependencies. VoP [131] introduces prompt tuning and fine-tunes CLIP to model spatiotemporal video aspects, while Cap4Video [ 402] leverages zero-shot captioning with CLIP and GPT-2 [306] for auxiliary captions. DGL [421] proposes dynamic global-local prompt tuning, emphasizing intermodal interaction and global video informa- tion through shared latent spaces and attention mechanisms. TeachCLIP [ 356] improves CLIP4Clip by integrating fine-grained cross-modal knowledge from advanced models, and re- fining text-video similarity with an frame-feature aggregation block. T-MASS [371] addresses dataset limitations by enriching text embeddings with stochastic text modeling. ∗ Text–Audio Retrieval. Text-audio retrieval involves matching textual queries with corre- sponding audio content, requiring alignment of semantic text information with dynamic acoustic patterns in speech, music, or environmental sounds. The