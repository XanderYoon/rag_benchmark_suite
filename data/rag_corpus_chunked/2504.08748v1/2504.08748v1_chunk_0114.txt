the inherent complexity of integrating and retrieving information across diverse data modalities such as text, images, audio, and video. Below, we outline the key challenges in this field: • Heterogeneity of Cross-Modal Data : The heterogeneity of data across modalities poses a significant challenge in multimodal retrieval and representation learning. Text, being sequential and discrete, relies on syntactic and semantic structures best captured by language models, while images, being spatial and continuous, require convolutional or transformer-based architectures to extract hierarchical visual features. This structural divergence complicates cross-modal alignment and comparison, as each modality demands specialized feature extraction techniques tailored to its unique characteristics. Extracting meaningful and comparable features from each modality is non-trivial, requiring domain-specific expertise and sophisticated models capable of capturing nuanced data properties. For instance, while transformers excel in processing sequential data like text, their adaptation to spatial data like images often necessitates architectural modifications, such as vision transformers (ViTs), to handle pixel arrays. Aligning these features into a unified representation space that preserves cross-modal semantic relationships remains a major challenge. Current approaches, including cross-modal transformers and MLLMs, often fail to create a common embedding space that adequately captures the semantic richness of each modality while ensuring inter-modal consistency. • Cross-modal components (reranker, refiner): While the dual-tower architecture has made significant strides in first-stage retrieval by efficiently encoding and aligning multimodal data (e.g., text and images), developing advanced reranking models that enable fine-grained multimodal interaction remains a challenge. Additionally, refining external multimodal knowledge post- retrieval and reranking remains underexplored, despite its potential to enhance result accuracy and relevance. Addressing these gaps requires innovative methodologies that leverage MLLMs and LLMs to enable sophisticated cross-modal understanding and reasoning. 6.4 Generation The multimodal module in MRAG achieves human-aligned sensory representation through diver- sified modality integration, which significantly enhances user experience and