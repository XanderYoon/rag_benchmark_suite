Large Language Models through Post-Processing. arXiv preprint arXiv:2404.11791 (2024). [417] Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, and Li Erran Li. 2024. Vigor: Improving visual grounding of large vision language models with fine-grained reward modeling. In European Conference on Computer Vision. Springer, 37–53. [418] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. 2024. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning. [419] Song Yang, Qiang Li, Wenhui Li, Xuanya Li, and An-An Liu. 2022. Dual-level representation enhancement on characteristic and context for image-text retrieval. IEEE Transactions on Circuits and Systems for Video Technology 32, 11 (2022), 8037–8050. [420] Tianchi Yang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, and Qi Zhang. 2023. Auto search indexer for end-to-end document retrieval. arXiv preprint arXiv:2310.12455 (2023). [421] Xiangpeng Yang, Linchao Zhu, Xiaohan Wang, and Yi Yang. 2024. DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 38. 6540–6548. [422] Yang Yang, Chubing Zhang, Yi-Chu Xu, Dianhai Yu, De-Chuan Zhan, and Jian Yang. 2021. Rethinking Label-Wise Cross-Modal Retrieval from A Semantic Sharing Perspective.. In IJCAI. 3300–3306. [423] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 (2023). [424] Zhen Yang, Yingxue Zhang, Fandong Meng, and Jie Zhou. 2023. Teal: Tokenize and embed all for multi-modal large language models. arXiv preprint arXiv:2311.04589 (2023). [425] Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. 2024. Deco: Decoupling token compression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985 (2024). [426] Jiabo Ye, Anwen Hu, Haiyang