to achieve multimodal output in the RAG scenario. It uses the user’s query to simultaneously recall associated text elements and images. Then, based on the associations of the images and text elements in the original document, they are refined. Subse- quently, MLLMs are employed to vectorize the images or convert them into descriptions, which are input into the generative model in the form of placeholders. The output generates answer text and a simple description placeholder for the associated image. Finally, through a chain-of-thought(COT) process, the placeholders are converted into actual images. NExT-GPT [401] employs an entirely different and novel paradigm. It directly trains a unified multimodal large model, unifying the reasoning and generation process, and directly generates multimodal data including text, images, videos, etc., through the model [401]. 4 Dataset for MRAG To evaluate the general capabilities of MRAG systems in real-world multimodal understanding and knowledge-based question-answering tasks, we curated a collection of existing datasets designed to comprehensively evaluate the MRAG pipeline. These datasets are categorized into two classes: (1) Retrieval & Generation-Joint Components, which evaluate the synergy of retrieval and generation by requiring systems to retrieve external knowledge and generate accurate responses; and (2) Generation, focusing solely on the model’s ability to produce contextually accurate outputs without external retrieval. This categorization enables a detailed evaluation of MRAG systems’ strengths and limitations in diverse scenarios. 4.1 Dataset for Retrieval & Generation Datasets for Retrieval & Generation in MRAG are designed to evaluate end-to-end systems capable of retrieving relevant knowledge from multimodal sources (e.g., text, images, videos) and generating accurate responses. These datasets evaluate the synergistic integration of retrieval and generation, focusing on the system’s ability to dynamically utilize external knowledge to improve response quality and relevance. In this section, we introduce key benchmarks designed for diverse evaluation of Retrieval &