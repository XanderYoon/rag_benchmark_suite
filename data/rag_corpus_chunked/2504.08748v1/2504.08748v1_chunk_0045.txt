the strong representational capabilities [88, 146, 234, 240, 250, 380, 405, 477, 492] of VLP models to significantly enhance cross-modal retrieval performance. Single-stream models like TEAM [405] align multimodal token embeddings for token-level matching, while dual-stream approaches such as COTS [240] integrate contrastive learning with token- and task-level interactions. Methods like CSIC [234] and LAPS [88] improve mul- timodal alignment by quantifying semantic significance and associating patch features with words, respectively. AGREE [380] fine-tunes and reranks cross-modal entities to harmonize their alignment. IRRA [146] employs text-specific mask mechanism to capture fine-grained intra- and inter-modal relationships. USER [477], EI-CLIP [250], and MAKE [492] leverage CLIP [305] or ALIGN [144] to integrate contrastive learning and keyword enhancement for enriching representations. Overall, VLP models, through strategies such as fine-tuning, reranking, and follow-up training, have become essential for improving cross-modal align- ment and interaction. ∗ Text–Video Retrieval. Text-video retrieval involves matching textual descriptions with corresponding videos, requiring spatiotemporal representations to address temporal dy- namics, scene transitions, and precise text-video alignment. This task is more complex than text-image retrieval due to the need to model both visual and sequential information effectively. Early CNN/RNN-based methods [64, 273, 274, 276, 361, 441] encode videos and texts into a shared latent space for similarity measurement. LLVE [361] employs CNNs and LSTMs to extract latent features from images and texts, with LSTMs further capturing temporal relationships between video frames. Subsequent studies [274, 276] apply mean/max pooling to frame sequences to generate compact video-level representations, prioritizing efficiency , Vol. 1, No. 1, Article . Publication date: April 2018. 22 Trovato et al. over granularity. Later advancements incorporate additional modalities, such as audio and motion, to enhance video semantics [273]. For text encoding, simpler methods like Word2Vec, LSTMs, or GRUs are commonly used [274, 276, 441], with evidence suggesting that combining