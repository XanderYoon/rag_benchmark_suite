single-vector bi-encoders struggle to model fine-grained semantic interactions between queries and documents. To address this limitation, multi-vector representation en- hance text representation and semantic interaction by employing multiple-representation bi-encoders. The Poly-encoder [137] generates multiple context codes to capture text se- mantics from multiple views. ME-BERT [246] produces ùëö representations for a candidate text using the contextualized embeddings of the first ùëö tokens. ColBERT [163] maintains per-token contextualized embeddings with a late interaction mechanism. ColBERTer [120] extends ColBERT by combining single- (‚Äú[CLS]‚Äù) and multi-representation (per-token) mechanisms for better performance. MVR [471] introduces multiple ‚Äú[VIEW]‚Äù tokens to learn diverse representations, with a local loss to identify the best-matched view. MADRM [166] learns multiple aspect embeddings for queries and texts, supervised by explicit aspect annotations. For training method, to achieve optimal retrieval performance, dense retrieval mod- els are typically trained using two key techniques: negative sampling and pretraining. Negative sampling focuses on selecting high-quality negatives to compute the negative log-likelihood loss used for training dense retrieval models. Basic methods include ran- dom sampling [127] and in-batch negatives [118, 161, 304], which increase the number of negatives within memory limits but do not guarantee the inclusion of hard negatives, i.e., irrelevant texts with high semantic similarity to the query. Hard negatives are critical for improving the model‚Äôs ability to distinguish relevant from irrelevant texts. Various approaches have been proposed to incorporate hard negatives. BM25-retrieved documents are used as static hard negatives [ 95, 161]. STAR [454] combines static hard negatives with random negatives, while ANCE [ 409] retrieves hard negatives using a warm-up dense retrieval model and refreshes the document index during training. ADORE [454] employs an adaptive query encoder to retrieve top-ranked texts as hard negatives, keeping the text encoder and document index fixed. However, hard negatives may include false negatives, introducing noise