features while suppressing irrelevant ones. DSVEL [72] employs spatial-aware pooling to align image regions with text, while CRAN [299] and CAAN [468] improve global-local alignment through relation alignment and context-aware selection. RANet [ 408] refines attention mechanisms with reference attention to reduce incorrect scores and adaptive aggregation to amplify relevant information and minimize redundancy. Transformer-based methods [18, 57, 87, 215, 333, 350, 419, 422, 451] leverage multi-head self-attention to encode multimodal relationships and optimize modality-specific encoders, demonstrating superior performance in multimodal modeling and cross-modal retrieval tasks. Recent advancements in multimodal representation learning have focused on enhancing Transformer architectures and feature alignment. PVSE [333] integrates self-attention and residual learning, while PCME [57] uses probabilistic embeddings to model one-to-many and many-to-many correlations. RLCMR [422] tokenizes multimodal data and trains with a unified Transformer encoder for cross-modal semantic correlation. DREN [419] refines feature representation through character-level and context-driven augmentation. TGDT [215] unifies coarse- and fine-grained learning with multimodal contrastive loss for feature alignment. HREM [87] improves image-text matching by capturing multi-level intra- and inter-modal relationships. TransTPS [ 18] extends Transformers with cross-modal multi- granularity matching and contrastive loss for better feature distinction. IEFT [350] models text-image pairs as unified entities to model their intrinsic correlation. With the rapid advancement of pretraining paradigms, Vision-Language Pretraining (VLP) models [46, 62, 68, 107, 136, 144, 183, 194, 305], including both single- and dual-stream architectures, have leveraged large-scale visual-linguistic datasets for joint pretraining. Re- searchers have utilized the strong representational capabilities [88, 146, 234, 240, 250, 380, 405, 477, 492] of VLP models to significantly enhance cross-modal retrieval performance. Single-stream models like TEAM [405] align multimodal token embeddings for token-level matching, while dual-stream approaches such as COTS [240] integrate contrastive learning with token- and task-level interactions. Methods like CSIC [234] and LAPS [88] improve mul- timodal alignment