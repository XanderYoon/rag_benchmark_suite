manual annotations, to capture semantic correlations. Existing methods can be categorized into three groups: CNN/RNN-based approaches, Transformer-based techniques, and Vision-Language Pretraining (VLP) model-based methods. Early CNN/RNN-based methods [75, 110, 174, 231, 370, 390] extract features from each modality separately using MLP, CNN, and RNN, enforcing cross-modal constraints through positive/negative sample construction. MSDS [370] uses CNN with a maximum likelihood- based scheme for image-text relevance. VSE++ [ 75] combines CNN and RNN with hard sample mining in ranking loss. Advances include residual learning [ 231], character-level convolution [390], and disentangled representation [110] for improved feature mapping and retrieval. DSCMR [489] maps multimodal data into a shared space using modality-specific networks and fully connected layers, leveraging label constraints and pairwise loss for discriminant learning. Recent CNN/RNN-based methods improve image-text matching by addressing key challenges. DRCE [384] enhances rare content representation and association using a dual-path structure, adaptive fusion, and reranking to mitigate long-tail issues. ESSE [386] tackles one-to-many correspondence by projecting data as sectors with uncertainty apertures. SDCMR [388] employs diverse CNNs for multimodal feature extraction and a dual adversarial mechanism to isolate semantic-shared features, ensuring retrieval consistency. , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 21 These methods collectively advance cross-modal retrieval robustness and accuracy. Spatial attention [72, 135, 299, 408, 468] is widely used in CNN/RNN-based cross-modal retrieval to uncover fine-grained associations by generating weighted masks for local regions, enhancing key features while suppressing irrelevant ones. DSVEL [72] employs spatial-aware pooling to align image regions with text, while CRAN [299] and CAAN [468] improve global-local alignment through relation alignment and context-aware selection. RANet [ 408] refines attention mechanisms with reference attention to reduce incorrect scores and adaptive aggregation to amplify relevant information and minimize redundancy. Transformer-based methods [18, 57, 87, 215,