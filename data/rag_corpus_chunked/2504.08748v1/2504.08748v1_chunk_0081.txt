which focuses on multiple-choice questions. SEED- Bench-2 [179] expanded the scope to 24K QA pairs, including the evaluation of both text and image generation. MMT-Bench [431] further scaled up to 31K QA pairs across diverse scenarios. Common findings reveal that model performance improves with scale, but challenges persist in fine-grained perception tasks (e.g., spatial localization), chart and visual mathematics comprehension, and interleaved image-text understanding. Open-source MLLMs have shown rapid progress, often matching or surpassing closed-source models. Real-world usage scenarios are critical for evaluating model performance in practical applications. Benchmarks like RealWorldQA [4] evaluates spatial understanding capabilities sourced from real- life scenarios, while BLINK [86] highlights tasks such as visual correspondence and multi-view reasoning that challenge current MLLMs despite being intuitive for humans. WV-Bench [245] and Visit-Bench [21] emphasize human preferences and instruction-following capabilities, whereas V*-Bench [399] evaluates high-resolution image processing and correct visual details through attribute recognition and spatial reasoning tasks. MME-RealWorld [ 480] enhances quality and difficulty with extensive annotated QA pairs and high-resolution images. These benchmarks reveal that fine-grained perception tasks remain challenging for models, while artistic style recognition and relative depth perception are relatively stronger. Although closed-source models like GPT-4o outperform others, human performance still surpasses general models significantly. Many studies simplify evaluation into binary or multi-choice problems for easier quantification, but this approach overlooks the importance of the reasoning process, which is critical for under- standing model capabilities. To address this, some works use open-ended generation and LLM-based evaluators, though these face challenges with inaccurate LLM scoring. For instance, MM-Vet [439] employs diverse question formats to assess integrated vision-language capabilities, while Touch- stone [17] emphasizes real-world dialogue evaluation, arguing that multiple-choice questions are insufficient for evaluating multimodal dialogue capabilities. InfiMM-Eval [113] evaluates models on , Vol. 1, No. 1, Article . Publication date: April 2018. A