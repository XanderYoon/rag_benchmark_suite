multimodal retrieval, is designed to re-rank a multimodal document list initially retrieved by a first-stage retriever. It achieves this by employing advanced relevance scoring mechanisms, such as cross-attention models, which enable more contextual interactions between queries and documents. Based on the utilization of large models, including LLMs and MLLMs, existing reranking methods can be categorized into two primary paradigms: fine-tuning-as-reranker and prompting-as-reranker. • Fine-tuning-as-Reranker: The fine-tuning-as-reranker paradigm adapts PLMs to domain- specific reranking tasks through supervised fine-tuning on domain-specific datasets, addressing their inherent lack of ranking awareness and inability to effectively measure query-document relevance. – Reranking for Text-Modal : According the development of large models’ architecture, reranker can be divided to three categories: encoder-only, encoder-decoder, and decoder- only. Encoder-only rerankers have advanced document ranking by fine-tuning PLMs (e.g., BERT [62]) to achieve precise relevance estimation. Key examples include Nogueira and Cho [282] and monoBERT [285], which format query-document pairs as query-document sequences. The relevance score is derived from the “[CLS]” token’s representation via a linear layer, with optimization achieved through negative sampling and cross-entropy loss. Existing research on encoder-decoder rerankers primarily formulates document ranking as a generation task [157, 283, 296, 510], fine-tuning models like T5 to generate classification tokens (e.g., “true” or “false”) for query-document pairs, with relevance scores derived from token logits [283]. Extensions include multi-view learning approaches [157] that simultaneously generate classification tokens for query-document pairs and queries conditioned on documents, and DuoT5 [296], which compares the classification tokens of document pairs to determine relative relevance. Beyond these approaches, studies have explored alternative training losses and architectures. Contrast with previous methods that rely on text generation losses, RankT5 [510] directly produces numerical relevance scores for each query-document pair, optimizing with ranking losses instead of generation losses. ListT5 [433] further advances this by processing multiple documents simultaneously, directly