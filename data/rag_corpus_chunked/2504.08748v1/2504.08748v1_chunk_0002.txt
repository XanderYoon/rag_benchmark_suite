full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym ’XX, Woodstock, NY © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX , Vol. 1, No. 1, Article . Publication date: April 2018. arXiv:2504.08748v1 [cs.IR] 26 Mar 2025 2 Trovato et al. 1 Introduction Large language models (LLMs), especially the Transformer-based variants, have achieved extraor- dinary success in many language-related tasks. Through pre-training on extensive, high-quality instruction datasets, LLMs can learn a wide range of language patterns, structures, and factual knowledge. These pre-trained LLMs can generate human-like text with high degrees of fluency and coherence, and attain strong performance on question-answering tasks, which demonstrates their ability to understand and respond to a wide range of queries. However, despite their impressive capabilities, LLMs still face significant limitations. One of the primary challenges lies in their performance within specific domains or knowledge-intensive tasks. While these models are often trained on diverse and extensive datasets, such datasets may not cover the depth of knowledge required for highly specialized fields or real-time information updates. This can be particularly problematic in areas like medicine, law, finance, and other technical fields where precision and up-to-date knowledge are to be prioritized. When handling queries that extend beyond the scope of their training knowledge or require the most current information, LLMs may generate responses that are speculative or based on patterns they have learned, rather than on verified facts. This can result in misleading, incorrect, or even entirely fabricated answers, a phenomenon known as "hallucination". Minimizing the incidence