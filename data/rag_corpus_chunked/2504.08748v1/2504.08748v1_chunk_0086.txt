continue to face challenges in reasoning tasks and long-context document comprehension, particularly in interpreting extended multimodal contexts, which remains a critical limitation. 4.2.4 Mathematics. Visual math problem-solving is key to evaluating MLLMs, leading to the development of specialized benchmarks. MathVista [242] pioneered this effort by aggregating 28 existing datasets and introducing 3 new ones, featuring diverse tasks like logical, algebraic, and scientific reasoning with various visual inputs. Subsequent benchmarks, such as Math-Vision [372] and OlympiadBench [115], introduced more complex tasks and fine-grained evaluation methods. We-Math [300] decomposes problems into sub-problems to assess fundamental understanding, while MathVerse [470] further evaluates MLLMsâ€™ comprehension of math diagrams by transforming problems into six versions with varying proportions of visual and textual content. Despite promising results from MLLMs, significant challenges remain. existing MLLMs often struggle with interpreting complex diagrams, rely heavily on textual cues, and address composite problems through memorization rather than underlying reasoning. These limitations highlight the need for further development in MLLM capabilities. 4.2.5 Video Understanding. Traditional video-QA benchmarks like TGIF-QA [141] and ActivityNet- QA [442] are domain-specific, focusing on tasks related to human activities. With advancements in MLLMs, new benchmarks have emerged to address more complex video understanding challenges. Video-MME [85] explores diverse video domains with multimodal inputs and manual annotations, while MVBench [188] reannotates existing datasets using ChatGPT. MMBench-Video [77] features , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 39 free-form questions for short to medium-length videos. Benchmarks like MLVU [496], LVBench [376], Event-Bench [71], and VNBench [488] emphasize long-video understanding, testing models on extended multimodal contexts. VNBench [488] introduces a synthetic framework for evaluating tasks like retrieval and ordering, by inserting irrelevant images or text into videos. Specialized benchmarks like EgoSchema [ 262] focus on egocentric videos. TempCompass [ 232] evaluates