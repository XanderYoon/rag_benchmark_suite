with high similarity and establishes a key token set to retain vision-critical tokens. In the prefilling stage, it further compresses vision tokens guided by text semantics using a dual-attention filtering strategy. In the decoding stage, an output-aware cache policy reduces the size of the KV cache. By employing tailored strategies across these stages, MustDrop achieves an optimal balance between performance and efficiency. G-Search [484] proposes a greedy search algorithm to determine the minimum number of vision tokens to retain at each layer, from shallow to deep. Based on this strategy, a parametric sigmoid function (P-Sigmoid) is designed to guide token reduction at each layer of the MLLM, with parameters optimized using Bayesian Optimization. G-Prune [151] introduces a graph-based method for training-free visual token pruning. It treats visual tokens as nodes and constructs connections based on semantic similarities. Information flow is propagated through weighted links, and the most important tokens are retained for MLLMs after iterations. Although interpretable and transparent, the inherent ambiguity of hard prompts often hinders the precise expression of intent, limiting their effectiveness in diverse or complex scenarios. Crafting accurate and impactful hard prompts demands significant human effort and may require model-based refinement or optimization. Moreover, even minor variations in hard prompts can lead to inconsistent LLM performance for identical tasks. • Soft Prompt Refiner: Soft prompts are trainable, continuous vectors that match the dimension- ality of token embeddings in LLM’s vocabulary. Unlike hard prompts, which rely on discrete tokens from a predefined vocabulary, soft prompts are optimized through training to capture nuanced meanings that discrete tokens cannot express. When fine-tuned on diverse datasets, soft prompts enhance the LLM’s performance across various tasks. – Refining for Text-Model: Language models convert text prompts into vectors for denser representation, enabling compression of discrete text into continuous vectors within the model.