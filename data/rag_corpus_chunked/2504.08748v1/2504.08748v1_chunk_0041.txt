STAR [454] combines static hard negatives with random negatives, while ANCE [ 409] retrieves hard negatives using a warm-up dense retrieval model and refreshes the document index during training. ADORE [454] employs an adaptive query encoder to retrieve top-ranked texts as hard negatives, keeping the text encoder and document index fixed. However, hard negatives may include false negatives, introducing noise that can degrade performance. RocketQA [ 304] addresses this by using a cross-encoder to filter out likely false negatives. AR2 [ 459] integrates a dual-encoder retriever with a cross-encoder ranker, jointly optimized through a minimax adversarial objective to produce harder negatives and improve the retriever. SimANS [497] introduces the concept of sampling ambiguous negatives, i.e., texts ranked near positives , Vol. 1, No. 1, Article . Publication date: April 2018. 20 Trovato et al. with moderate similarity to the query. These negatives are more informative and less likely to be false negatives, further enhancing model performance. Pretraining aims to learn universal semantic representations that generalize to down- stream dense retrieval tasks. To enhance the modeling capacity of PLMs, self-supervised pretraining tasks, such as those proposed by Lee et al. [172] (selecting random sentences as queries) and Chang et al. [27] (leveraging hyperlinks for constructing query-passage pairs), mimic retrieval objectives. Prop [251] and B-PROP [252] use document language models (e.g., unigram, BERT) to sample word sets, training PLMs to predict pairwise preferences. To enhance dense retrieval models, studies focus on improving the “[CLS]” token embedding. Condenser [92] aggregates global text information for masked token recovery, while co-Condenser [93] adds a query-agnostic contrastive loss to cluster related text segments while distancing unrelated ones. Contriever [139] generates positive pairs by sampling two spans from the same text and negatives using in-batch and cross-batch texts. Following with an unbalanced architecture (strong encoder, simple decoder),