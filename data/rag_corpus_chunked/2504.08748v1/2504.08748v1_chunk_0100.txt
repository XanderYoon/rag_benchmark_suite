2018. A Survey on Multimodal Retrieval-Augmented Generation 43 MMDU [235] 2024 comprises 110 multi-image multi-turn dialogues with more than 1600 questions, each accompanied by detailed long-form answers. The questions in MMDU involve 2 to 20 images, with an average image&text token length of 8.2k tokens, a maximum turn length of 27, and a maximum image&text length reaching 18K tokens. Multidisciplinary ScienceQA [243] 2022 multiple-choice science question dataset containing 21,208 examples. It covers diverse topics across three subjects: natural science, social science, and language science. MMMU [445] 2024 includes 11.5K multimodal questions from college exams, quizzes, and textbooks, covering 6 core disciplines. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types. CMMU [117] 2024 It consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank. CMMMU [457] 2024 A Chinese Multi-discipline multimodal Understanding, including 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering 6 core disciplines. These questions span 30 subjects and comprise 39 highly heterogeneous image typesbenchmark. MMMU-Pro [446] 2024 3460 questions in total (1730 samples are in the standard format and the other 1730 are in the screenshot or photo form) 5 Evaluation Metrics of MRAG Multimodal RAG systems generally consist of four core components: document parsing, search planning, retrieval, and generation, which collectively influence their end-to-end performance. Accurate and comprehensive evaluation of these components is essential, leveraging available multimodal benchmarks. In practice, three common evaluation strategies are typically employed: human evaluation, rule-based evaluation, and LLM/MLLM-based evaluation. Each strategy offers distinct advantages and disadvantages in calculating evaluation metrics. â€¢ Human evaluation: Human evaluation is widely regarded as the gold standard for assessing MRAG systems, as their effectiveness is ultimately determined by human users. This method