task-specific evaluations for MLLMs are insufficient, particularly in commercially relevant domains like invoice recognition, multimodal knowledge base comprehension, and UI understanding and industry. Finally, while existing multimodal benchmarks primarily focus on image and video modalities, there is a notable deficit in assessing capabilities related to audio and 3D representations. Addressing these challenges is essential for developing more robust and comprehensive evaluation methodologies for MLLMs in the future. Despite rapid advancements, current evaluations of MLLMs remain insufficiently comprehensive, primarily focusing on perception and reasoning abilities through objective questions. This creates a significant gap between evaluation methodologies and real-world applications. Moreover, optimiz- ing models based on objective assessments often leads developers to prioritize objective question corpora during instruction tuning, potentially degrading the quality of dialogue experiences. Al- though subjective multimodal evaluation platforms like WildVision and OpenCompass MultiModal Arena have emerged, further research is needed to develop assessment methods that better align with practical usage scenarios. Current evaluation strategies predominantly rely on curated or crafted questions to assess specific capabilities, yet complex multimodal tasks typically require the integration of multiple skills. For instance, a chart-related question may involve OCR, spatial relationship recognition, reasoning, and calculations. The absence of decoupled assessments for these distinct capabilities represents a major limitation in existing frameworks. Additionally, crucial abilities such as instruction following remain under-evaluated. Multiturn dialogue, the primary mode of human interaction with multimodal models, remains a weakness for most models, and corresponding evaluations, are still in their infancy. In the realm of complex multimodal reasoning, current evaluations predominantly focus on mathematical and examination problems, necessitating , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 51 improvements in both difficulty and relevance to everyday use cases. Notably, the evaluation of multimodal creative tasks, a key application area for these