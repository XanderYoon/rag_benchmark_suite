components: text recognition, scene text-centric VQA, document-oriented VQA, key information extraction, and handwritten mathematical expression recognition. VCR [473] 2024 It comprise 2.11M English and 346K Chinese entities, featuring captions in both languages across ‘easy’ and ‘hard’ difficulty levels. SEED-Bench-2-Plus [178] 2024 comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs. Structural Document FigureQA [160] 2017 its training set contains 100,000 images with 1.3 million questions; the validation and test sets each contain 20,000 images with over 250, 000 questions. The images are synthetic, scientific-style figures from five classes: line plots, dot-line plots, vertical and horizontal bar graphs, and pie charts. DocVQA [268] 2021 contains 50,000 question-answer pairs with 12,767 document images sourced from documents in UCSF Industry Documents Library. VisualMRC [348] 2021 It contains 30562 pairs of a question and an abstractive answer for 10,197 document images sourced from multiple domains of webpages. ChartQA [266] 2022 consists of 20,882 charts curated from four different online sources. It covers 9,608 human-written questions focusing on logical and visual reasoning questions, and generates another 23,111 questions automatically from human-written chart summaries. InfographicVQA [267] 2022 comprises 30,035 questions over 5,485 images. Questions in the dataset include questions grounded on tables, figures and visualizations and questions that require combining multiple cues. ChartBench [414] 2023 includes over 68k charts and more than 600k high-quality instruction data, covering 9 major categories and 42 subcategories of charts. 5 chart question-answering tasks to assess the models’ cognitive and perceptual abilities. SciGraphQA [192] 2023 generate 295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs. As context, it provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the graph, and rich text contextual data from the graph itself, obtaining dialogues with an average 2.23 question-answer turns for each graph. MMC-Benchmark [217] 2023