on standardized evaluation tools, enabling objective, reproducible assessments with minimal human intervention. Compared to subjective human evaluations, deterministic metrics offer significant advantages, including reduced time consumption, lower susceptibility to bias, and greater consistency across multiple assessments. Such consistency is particularly crucial for large-scale evaluations or when comparing different systems or model iterations. • LLM/MLLM-based evaluation: For evaluation of MRAG systems, LLMs/MLLMs are employed to compare reference answers with generated outputs or to directly score responses. For example, MM-Vet [439] uses GPT-4 to automate evaluation, generating scores for each sample based on the input question, ground truth, and model output. Similarly, TouchStone [17] and LLaVA-bench [219] leverage GPT-4 to directly compare generated answers with reference answers, simplifying , Vol. 1, No. 1, Article . Publication date: April 2018. 44 Trovato et al. the evaluation process. While integrating LLMs/MLLMs in evaluation reduces human effort, it has limitations. This approach is prone to systematic biases, such as sensitivity to the order of response presentation. Additionally, evaluation outcomes are heavily influenced by the inherent capabilities and limitations of the LLMs/MLLMs themselves, leading to potential inconsistencies, as different models may produce divergent results for the same task. These challenges underscore the need for careful model selection and evaluation design to mitigate biases and ensure reliable assessments. 5.1 Metrics of Retrieval & Generation The evaluation of MRAG systems is essential for ensuring their effectiveness and reliability in processing complex, multimodal data. Evaluation metrics can be broadly classified into rule-based and LLM/MLLM-based approaches. • Rule-based Metrics: Rule-based metrics evaluate the performance of MRAG systems using predefined criteria and heuristics. These metrics are generally interpretable, transparent, and computationally efficient, making them well-suited for tasks with well-defined benchmarks. Examples of common rule-based metrics include: – Exact Match (EM): This metric evaluates whether the model’s output exactly matches the ground