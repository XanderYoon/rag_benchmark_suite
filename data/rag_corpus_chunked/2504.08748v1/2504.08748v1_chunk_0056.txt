the relevance score, as reranking have already shown its importance in various knowledge-intensive tasks. Wen et al . [394] fine-tunes a pretrained MLLM to facilitate cross-item interaction between questions and knowledge items. The reranker is trained on the same dataset as the answer generator, using distant supervision by checking whether answer candidates appear in the knowledge text. RagVL RETRIEVAL et al. [311] introduces a novel framework featuring knowledge-enhanced reranking and noise- injected training. The approach involves instruction-tuning the MLLM with a simple yet effective template to enhance its ranking capability, enabling it to serve as a reranker for accurately filtering the top-ùëò retrieved images. In summary, these approaches leverages the representational capacity of large models while optimizing them for task-specific relevance signals, often achieving high reranking accuracy. However, it requires substantial computational resources and labeled training data, resulting in increased costs. ‚Ä¢ Prompting-as-Reranker: In contrast, the prompting-as-reranker paradigm leverages large models in a zero-shot or few-shot manner by designing prompts that direct the model to gen- erate relevance scores or rankings directly. This approach exploits the inherent knowledge and reasoning capabilities of large models, eliminating the need for extensive fine-tuning and offering greater flexibility and resource efficiency. Researchers have explored prompting LLMs and MLLMs to perform ranking tasks on multimodal documents, with prompting strategies generally categorized into three types: point-wise, pair-wise, and list-wise methods. ‚Äì Reranking for Text-Model: LLMs are increasingly employed in text-modal reranking tasks, leveraging their advanced capabilities to optimize the ranking of textual documents. Point-wise methods evaluate the relevance between a query and individual documents, reranking them based on relevance scores.. Zhuang et al. [509] integrates fine-grained rele- vance labels into prompts for better document distinction. MCRanker [109] addresses biases in existing point-wise rerankers by generating relevance scores based on multi-perspective criteria. UPR [318] re-scores retrieved passages