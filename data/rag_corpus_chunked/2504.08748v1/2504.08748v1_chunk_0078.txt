linguistic context. ViQuAE [ 177] focuses on answering questions about named entities grounded in a visual context using a Knowledge Base. InfoSeek [42] and Encyclopedic-VQA [272] target knowledge-based questions beyond common sense knowledge, with Encyclopedic- VQA using model-generated annotations. MMSearch [147] evaluates MLLMs as multimodal search engines, focusing on image-to-image retrieval. Compared with previous works, MRAG-bench [126] evaluates MLLMs in utilizing vision-centric retrieval-augmented knowledge, identifying scenarios where visual knowledge outperforms textual knowledge. MRAMG-Bench [435] evaluates answers combining text and images, leveraging multimodal data within a corpus. Additionally, VCR [448] and VisualCOMET [291], derived from movie scenes, evaluate Visual Commonsense Reasoning. KnowIT VQA [97] and SOK-Bench [365] focus on video understanding and reasoning task, combining visual, textual, and temporal reasoning with knowledge-based questions. Ads [138] proposes an automatic advertisement understanding task, featuring rich annotations on topics, sentiments, and persuasive reasoning. 4.2 Dataset for Generation The Generation category evaluates a model’s intrinsic capacity to generate contextually accurate outputs based solely on its pre-trained knowledge and internal reasoning, without external retrieval. This evaluation isolates the generation component, providing insights into the model’s foundational language understanding capabilities. It enables a detailed analysis of MRAG systems’ strengths and limitations across diverse scenarios. In this section, we provide an overview of representative benchmarks developed for various evaluation of Generation tasks. The existing benchmarks are systematically organized in Figure 10, and the statistics of selected representative benchmarks are summarized in Table 2. 4.2.1 Comprehensive. To rigorously evaluate the capabilities of MLLMs, a diverse range of evalua- tion benchmarks has been developed. These benchmarks are designed to test various dimensions of model performance. By utilizing these benchmarks, researchers can systematically quantify the strengths and limitations of MLLMs, ensuring their alignment with real-world applications and user expectations. This evaluation framework not only supports the iterative improvement of MLLMs but