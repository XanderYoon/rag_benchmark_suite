a set of four multiple-choice options. Visual Advertisement Ads [138] 2017 202,090 questions from 64,832 image ads and 3,477 video ads. Early knowledge-based datasets include KB-VQA [375] and FVQA [374], which rely on closed knowledge. FVQA, for instance, uses a fixed knowledge graph, making questions straightforward once the knowledge is known, with minimal reasoning required. KVQA [322] focuses on images in Wikipedia articles, primarily testing named entity recognition and Wikipedia knowledge retrieval rather than commonsense reasoning. OK-VQA [263] and A-OKVQA [321] evaluate multimodal reasoning using external knowledge, with A-OKVQA introducing "rationale" annotations to better evaluate knowledge acquisition and reasoning. S3VQA [140] extends OK-VQA by requiring object detection and web queries, but like OK-VQA, it often reduces to single retrieval tasks rather than complex reasoning. MultiModalQA [345] pioneers complex questions requiring reasoning across snippets, tables, and images, focusing on cross-modal knowledge extraction. However, its template- based questions simplify the task to filling in blanks with modality-specific answering mechanisms. , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 35 ManyModalQA [114] also uses snippets, images, and tables but emphasizes answer modality choice over knowledge aggregation. MIMOQA [329] introduces “Multimodal Input Multimodal Output”, requiring both text and image selections to enhance understanding. WebQA [ 28] is a manually crafted, multi-hop multimodal QA dataset that retrieves visual content but provides only textual answers, relying solely on MLLMs for reasoning, making it unsuitable for models dependent on linguistic context. ViQuAE [ 177] focuses on answering questions about named entities grounded in a visual context using a Knowledge Base. InfoSeek [42] and Encyclopedic-VQA [272] target knowledge-based questions beyond common sense knowledge, with Encyclopedic- VQA using model-generated annotations. MMSearch [147] evaluates MLLMs as multimodal search engines, focusing on image-to-image retrieval. Compared with previous works, MRAG-bench [126] evaluates MLLMs in