in low-dimensional Euclidean spaces for modeling semantic relationships between queries and documents. These embeddings enable relevance measurement through Euclidean dis- tances or inner products. Dense retrieval methods have demonstrated strong performance , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 19 across various information retrieval tasks [161, 163, 270]. Additionally, Approximate Near- est Neighbor Search (ANNS) algorithms [98, 142, 156], particularly quantization-based methods [98, 142] and their retrieval-oriented variants [415, 453, 455, 461], enable efficient retrieval of top-ranked documents from large collections using precomputed ANNS indices. Dense retrieval techniques primarily focus on two key aspects: model architecture and training methods. For model architecture, dense retrieval methods employ a two-tower architecture to balance retrieval efficiency and effectiveness by modeling semantic interactions between queries and documents through their representations. These methods vary in represen- tation granularity, primarily falling into two categories: single-vector and multi-vector representations. Then, the relevance scores are computed using similarity functions (e.g., cosine similarity, inner product) between these embeddings. A common technique in- volves placing a special token (e.g., ‚Äú[CLS]‚Äù) at the beginning of a text sequence, with its learned representation capturing the overall semantics. The existing dense retrieval models learn the query and document representations by fine-tuning PLMs like BERT [62], RoBERTa [229], or Mamba Gu and Dao [106], Zhang et al. [458], or large language models (LLMs) like RepLLaMA [255] on annotated datasets (e.g., MSMARCO [281], BEIR [354]). However, single-vector bi-encoders struggle to model fine-grained semantic interactions between queries and documents. To address this limitation, multi-vector representation en- hance text representation and semantic interaction by employing multiple-representation bi-encoders. The Poly-encoder [137] generates multiple context codes to capture text se- mantics from multiple views. ME-BERT [246] produces ùëö representations for a candidate text using the contextualized embeddings of the first