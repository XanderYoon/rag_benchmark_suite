A Survey on Multimodal Retrieval-Augmented Generation LANG MEI, Huawei Cloud BU, China SIYU MO, Huawei Cloud BU, China ZHIHAN YANG, Huawei Cloud BU, China CHONG CHENâˆ—, Huawei Cloud BU, China Multimodal Retrieval-Augmented Generation (MRAG) represents a significant advancement in enhancing the capabilities of large language models (LLMs) by integrating multimodal data, such as text, images, and videos, into the retrieval and generation processes. Traditional Retrieval-Augmented Generation (RAG) systems, which primarily rely on textual data, have shown promise in reducing hallucinations and improving response accuracy by dynamically incorporating external knowledge. However, these systems are limited by their reliance on text-only modalities, which restricts their ability to leverage the rich, contextual information available in multimodal data. MRAG addresses this limitation by extending the RAG framework to include multimodal retrieval and generation, thereby enabling more comprehensive and contextually relevant responses. In MRAG, the retrieval step involves locating and integrating relevant knowledge from diverse modalities, while the generation step utilizes multimodal large language models (MLLMs) to produce answers that incorporate information from multiple data types. This approach not only enhances the quality of question-answering systems but also significantly reduces the incidence of hallucinations by grounding responses in factual, multimodal knowledge. Recent research has demonstrated that MRAG outperforms traditional text-modal RAG, particularly in scenarios where visual and textual information are both critical for understanding and responding to queries. This survey systematically reviews the current state of MRAG research, focusing on four key aspects: essential components and technologies, datasets, evaluation methods and metrics, and existing limitations. By analyzing these dimensions, we aim to provide a comprehensive understanding of how MRAG can be effectively constructed and improved. Additionally, we highlight current challenges and propose future research directions, encouraging further exploration into this promising paradigm. Our work underscores the potential of MRAG to revolutionize multimodal information retrieval and