is essential, leveraging available multimodal benchmarks. In practice, three common evaluation strategies are typically employed: human evaluation, rule-based evaluation, and LLM/MLLM-based evaluation. Each strategy offers distinct advantages and disadvantages in calculating evaluation metrics. • Human evaluation: Human evaluation is widely regarded as the gold standard for assessing MRAG systems, as their effectiveness is ultimately determined by human users. This method is extensively used in research to ensure the reliability and relevance of model outputs. For instance, Bingo [58] employs human annotators to assess the accuracy of GPT-4V’s responses, with a focus on identifying and analyzing model biases. In hallucination detection, M-HalDetect [108] demon- strates that human evaluation outperforms model-based methods in detecting subtle inaccuracies, highlighting its precision. Additionally, WV-Arena [245] uses a human voting system combined with Elo ratings to rank and compare multiple models, providing a robust benchmarking frame- work. However, human evaluation presents challenges, including increased time and labor costs, which limit its scalability for large-scale assessments. The reliability of results can also be affected by the limited number of evaluators, as individual biases may influence outcomes. To address these issues, some studies employ diverse evaluator pools and cross-validation techniques to enhance the balance and representativeness of assessments. Nonetheless, the trade-off between evaluation accuracy and resource expenditure remains a critical consideration in designing RAG model evaluation methodologies. • Rule-based evaluation: Rule-based evaluation metrics [41, 430, 473] are essential for assessing the performance of MRAG systems. These metrics rely on standardized evaluation tools, enabling objective, reproducible assessments with minimal human intervention. Compared to subjective human evaluations, deterministic metrics offer significant advantages, including reduced time consumption, lower susceptibility to bias, and greater consistency across multiple assessments. Such consistency is particularly crucial for large-scale evaluations or when comparing different systems or model iterations. • LLM/MLLM-based evaluation: For evaluation of MRAG systems,