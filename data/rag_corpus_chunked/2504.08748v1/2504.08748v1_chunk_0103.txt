classified into rule-based and LLM/MLLM-based approaches. • Rule-based Metrics: Rule-based metrics evaluate the performance of MRAG systems using predefined criteria and heuristics. These metrics are generally interpretable, transparent, and computationally efficient, making them well-suited for tasks with well-defined benchmarks. Examples of common rule-based metrics include: – Exact Match (EM): This metric evaluates whether the model’s output exactly matches the ground truth, offering a clear and unambiguous performance measure. It is especially valuable in tasks requiring high accuracy and fidelity to reference data, such as question answering, fact verification, and information retrieval. While exact match (EM) provides a straightforward and interpretable evaluation, it may fall short in scenarios where semantically equivalent but lexically divergent responses are acceptable. – ROUGE-N (N-gram Recall): The ROUGE metric is a widely used framework for evaluating text summarization and generation tasks. ROUGE-N measures the overlap of N-grams (con- tiguous sequences of N words) between generated text and one or more reference texts, with a strong emphasis on recall. This metric assesses how well the generated text captures the essential content of the reference. For example, ROUGE-1 evaluates unigram overlap, ROUGE-2 focuses on bigrams, and higher-order N-grams (e.g., ROUGE-3) capture more complex linguis- tic structures. While ROUGE-N provides a quantitative measure of lexical similarity, it is often supplemented by other metrics to account for semantic coherence, fluency, and relevance, particularly in multimodal contexts where textual and non-textual data interact. – BLEU: BLEU is a widely used metric in NLP for evaluating the quality of machine-generated text by assessing its similarity to one or more reference texts. Initially designed for machine translation, BLEU has been adapted to various NLP tasks, including multimodal generation. In multimodal settings, BLEU can evaluate the alignment between generated text and associated modalities (e.g., images, videos) by comparing the output to reference descriptions or