been driven by the integration of pre-trained language models (PLMs). While these approaches leverage PLMs, they remain fundamentally rooted in lexical matching, enabling the reuse of traditional sparse index structures by incorporating auxiliary information such as contextualized embeddings [94, 208] and extended tokens [83, 84, 284]. This research domain focuses on two main approaches: term weighting and term expansion. Term weighting enhances relevance estimation by leveraging context-specific token representations. DeepCT [60] and HDCT [59] use learned token representations to estimate the context-specific importance of terms within passages, while COIL [94] and uniCOIL [208] employ contextualized token repre- sentations of exact matching terms to compute relevance via dot products and summed similarity scores. Term expansion mitigates vocabulary mismatch by expanding queries or documents using PLMs. For instance, DocTTTTTquery [ 284] predicts relevant queries for documents to enrich the document’s content, while SPLADE [84] and SPLADEv2 [83] project terms onto vocabulary-sized weight vectors derived from masked language model logits. These vectors, aggregated via methods like summing or max pooling, effectively expand content by incorporating absent terms. Sparsity regularization ensures efficient sparse representations for inverted index usage. In summary, sparse retrieval models achieve an optimal balance in cross-domain transfer, retrieval efficiency, and overall effectiveness. · Dense Text Retrieval. Recent advancements in deep learning [5, 50, 51, 105, 119, 167, 169], particularly pre-trained language models (PLMs) [23, 62, 229] based on the Trans- former architecture [ 80, 362], have increasingly adopted dense vector embeddings in low-dimensional Euclidean spaces for modeling semantic relationships between queries and documents. These embeddings enable relevance measurement through Euclidean dis- tances or inner products. Dense retrieval methods have demonstrated strong performance , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 19 across various information retrieval tasks [161, 163, 270]. Additionally, Approximate Near-