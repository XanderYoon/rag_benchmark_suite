state-of-the-art models—to encode disparate modalities into a shared embedding space. This interoperability enhances downstream tasks, including information extraction, question answering, and summarization. A promising approach involves hybrid strategies that combine raw multimodal data with textual representations. For instance, raw images can support visual tasks (e.g., object detection or layout analysis), while textual captions or OCR-derived text can improve text generation tasks (e.g., summarization or translation). This dual methodology leverages the strengths of each modality, ensuring both accuracy and efficiency in processing complex documents, as demonstrated by recent research. Additionally, integrating LLMs and MLLMs with Optical Character Recognition (OCR) systems can enhance the parsing of scanned or image-based documents. By aligning OCR outputs with multimodal embeddings, these systems improve the handling of noisy or unstructured data, enabling more accurate interpretation and contextual understanding. – Advanced Captioning and Description Generation: To improve multimodal data inte- gration, particularly in document parsing, enhancing automated captioning and description generation for non-textual elements like images, tables, and charts is critical. Leveraging state- of-the-art vision-language models (VLMs) and MLLMs can boost the accuracy and contextual relevance of textual descriptions. These models bridge the gap between visual and textual data, enabling more comprehensive document understanding. Integrating domain-specific knowledge into captioning models is essential for generating accurate and contextually tai- lored descriptions. This can be achieved by fine-tuning pre-trained models on domain-specific datasets or incorporating external knowledge bases. Such an approach ensures that descrip- tions align with the document’s content, enhancing the utility of multimodal data integration. • Leveraging LLMs and MLLMs for Enhanced Parsing: – LLM/MLLM-Driven Parsing and Indexing: LLMs and MLLMs can be fine-tuned on domain- specific corpora to improve their ability to parse and interpret complex document structures. Leveraging their advanced multimodal understanding, these models can accurately identify and extract key information—such as legal clauses, scientific