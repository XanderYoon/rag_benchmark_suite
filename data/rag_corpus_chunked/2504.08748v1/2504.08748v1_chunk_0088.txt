text-image interleaving with two rounds and four images per instance. SciGraphQA [192] constructs multi-turn QA con- versations based on scientific graphs from Arxiv papers, emphasizing complex scientific discourse. ConvBench [225] assesses perception, reasoning, and creation capabilities across individual rounds and overall conversations, revealing that MLLMs’ reasoning and creation failures often stem from inadequate fine-grained perception. MMDU [235] engages models in multi-turn, multi-image con- versations, with up to 20 images and 27 turns, highlighting that the performance gap between open-source and closed-source models is largely due to limited conversational instruction tuning data. These benchmarks collectively enhance the evaluation of MLLMs in complex, real-world interaction scenarios. 4.2.8 Multidisciplinary. The mastery of multidisciplinary knowledge is a key indicator of a model’s expertise, and several benchmarks have been developed to evaluate this capability. ScienceQA [243] comprises scientific questions annotated with lectures and explanations, designed to facilitate chain-of-thought evaluation. It spans grade-level knowledge across diverse domains. MMMU [445] presents a more challenging college-level benchmark across diverse subjects, including engineering, art and design, business, science, humanities, social science, and medicine. Its question format extends beyond single image-text pairs to include interleaved text and images. Similarly, CMMU [117] and CMMMU [457] provide Chinese domain-specific benchmarks for grade-level and college- level knowledge, respectively. MMMU-Pro [446] enhances MMMU with a more robust version for advanced evaluation. Table 2. Summary of dataset for generation. Dataset Time Statistics Comprehensive VQA v2 [104] 2017 contains more than 443K training, 214K validation and 453K test image-question pairs. , Vol. 1, No. 1, Article . Publication date: April 2018. 40 Trovato et al. NLVR2 [336] 2018 contains 107,292 examples of English sentences paired with web photographs, including 29,680 unique sentences and 127,502 images. The task is to determine whether a natural language caption is true about a pair of photographs. VizWiz [112] 2018 contains