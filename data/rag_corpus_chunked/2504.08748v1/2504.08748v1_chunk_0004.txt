are not only based on static, pre-trained knowledge but are continuously updated with current and relevant data. By retrieving and referencing external knowledge, RAG grounds the generated responses in factual information, thereby significantly reducing the occurrence of hallucinations. However, previous research on RAG systems has primarily focused on knowledge bases built from plain text and LLMs pre-trained on plain text, ignoring other rich sources of knowledge available for query responses in the real world, such as videos and images, referred to as "multimodal data". Multimodal data refers to data that comes from multiple sources or formats. This can include text, images, audio, video, and other types of data. In real-world scenarios, humans naturally interact with multimodal data, such as browsing web pages that combine text, images, and videos in mixed layouts. By analyzing images or videos alongside text, the user can better understand the context of the content, and thus improve the satisfaction with the quality of the answers. For example, if a passenger inquires about how to store luggage while flying, it will be clearer that the system provides relevant graphic guides or instructional videos. However, transferring the capabilities of LLMs to the domain of multimodal text and images remains an active area of research, as plain-text LLMs are typically trained only on textual corpora and lack perceptual abilities for visual signals. How to effectively incorporate multimodal data is important to enhance the capability of , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 3 LLMs. In recent years, the development of multimodal generative models has showcased additional application possibilities. Apart from textual generative models, multimodal generative models have been increasingly applied in fields such as human-computer interaction, robot control, image search, and speech generation. Similarly, based on multimodal generative models