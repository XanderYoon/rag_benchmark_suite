453K test image-question pairs. , Vol. 1, No. 1, Article . Publication date: April 2018. 40 Trovato et al. NLVR2 [336] 2018 contains 107,292 examples of English sentences paired with web photographs, including 29,680 unique sentences and 127,502 images. The task is to determine whether a natural language caption is true about a pair of photographs. VizWiz [112] 2018 contains 20,000 training, 3,173 validation, and 8,000 test sets of visual questions originating from blind people. MME [29] 2023 measures both perception and cognition abilities on a total of 14 subtasks Visit-Bench [21] 2023 comprising 592 instances and 1,159 public images. The instances are either from 45 newly assembled instruction families or reformatted from 25 existing datasets. 10 instruction families cater to multi-image query scenarios. Touchstone [17] 2023 908 questions covering 27 subtasks. The highest proportion of questions pertains to recognition, ac- counting for about 44.1%, followed by comprehension questions at 29.6%. The proportions of the other categories are 15.3% for basic descriptive ability, 7.4% for visual storytelling ability, and 3.6% for multi-image analysis ability. MM-Vet [439] 2023 defines 6 core vision-language capabilities and examines the 16 integrations of interest derived from their combinations. It contains 200 images, and 218 questions (samples), all paired with their respective ground truths. InfiMM-Eval [113] 2023 It consists of 279 manually curated reasoning questions, associated with a total of 342 images. The dataset is categorized into three reasoning paradigms: deductive, abductive, and analogical reasoning. 49 questions pertain to abductive reasoning, 181 require deductive reasoning, and 49 involve analogical reasoning. Furthermore, the dataset is divided into two folds based on reasoning complexity, with 108 classified as “High” reasoning complexity and 171 as “Moderate” reasoning complexity. Q-Bench [398] 2023 consists of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. Seed-Bench [180]