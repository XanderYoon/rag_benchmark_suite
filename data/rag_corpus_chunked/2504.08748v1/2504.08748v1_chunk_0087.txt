for short to medium-length videos. Benchmarks like MLVU [496], LVBench [376], Event-Bench [71], and VNBench [488] emphasize long-video understanding, testing models on extended multimodal contexts. VNBench [488] introduces a synthetic framework for evaluating tasks like retrieval and ordering, by inserting irrelevant images or text into videos. Specialized benchmarks like EgoSchema [ 262] focus on egocentric videos. TempCompass [ 232] evaluates fine-grained temporal perception, and MovieChat [ 332] targets long videos but often reduces tasks to short-video problems. Current MLLMs, especially open-source ones, face challenges with long-context processing and temporal perception, underscoring the need for improved capabilities in these areas. 4.2.6 Industry. The absence of a comprehensive benchmark for evaluating MLLMs across diverse industry verticals has limited understanding of their applicability in specialized real-world scenarios. To address this gap, MME-Industry [429] was developed specifically for industrial applications, covering over 21 industrial sectors such as power generation, electronics manufacturing, textile production, steel, and chemical processing. Domain experts from each sector meticulously annotated and validated test cases, ensuring the benchmark’s reliability, accuracy, and practical relevance. MME-Industry thus serves as a robust tool for assessing MLLMs in industrial contexts. 4.2.7 Conversational QA. Current MLLMs are primarily designed for multi-round chatbot inter- actions, yet most benchmarks focus on single-round QA tasks. To better align with real-world conversational scenarios, multi-round QA benchmarks have been developed to simulate human-AI interactions with extended contextual histories. SparklesDialogue [134] evaluates conversational proficiency across multiple images and dialogue turns, featuring flexible text-image interleaving with two rounds and four images per instance. SciGraphQA [192] constructs multi-turn QA con- versations based on scientific graphs from Arxiv papers, emphasizing complex scientific discourse. ConvBench [225] assesses perception, reasoning, and creation capabilities across individual rounds and overall conversations, revealing that MLLMs’ reasoning and creation failures often stem from inadequate fine-grained perception. MMDU [235] engages models in