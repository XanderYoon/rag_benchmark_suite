classification tokens of document pairs to determine relative relevance. Beyond these approaches, studies have explored alternative training losses and architectures. Contrast with previous methods that rely on text generation losses, RankT5 [510] directly produces numerical relevance scores for each query-document pair, optimizing with ranking losses instead of generation losses. ListT5 [433] further advances this by processing multiple documents simultaneously, directly generating reranked lists using the Fusion-in- Decoder architecture. Recent studies [ 222, 255, 294, 464, 475] have explored fine-tuning decoder-only models like LLaMA for document reranking. RankLLaMA [255] formats query-document pairs into , Vol. 1, No. 1, Article . Publication date: April 2018. 26 Trovato et al. prompts and uses the last token representation for relevance scoring. TSARankLLM [ 464] employs a two-stage training approach: continuous pretraining on web-sourced relevant text pairs to align LLMs with ranking tasks, followed by fine-tuning with supervised data and tailored loss functions. Q-PEFT [294] introduces query-dependent parameter-efficient fine- tuning to generate accurate queries from documents. In contrast, listwise approaches like those in [475] and PE-Rank [222] focus on directly outputting reranked document lists. Zhang et al. [475] highlight the limitations of point-wise datasets with binary labels, and instead use ranking outputs from existing systems as gold standards to train a listwise reranker. PE-Rank [222] compresses documents into single embeddings, reducing input length and improving reranking efficiency. â€“ Reranking for Cross-Model : The multi-modal reranking uses the multi-modal question and multi-modal knowledge items to obtain the relevance score, as reranking have already shown its importance in various knowledge-intensive tasks. Wen et al . [394] fine-tunes a pretrained MLLM to facilitate cross-item interaction between questions and knowledge items. The reranker is trained on the same dataset as the answer generator, using distant supervision by checking whether answer candidates appear in the knowledge text. RagVL RETRIEVAL et