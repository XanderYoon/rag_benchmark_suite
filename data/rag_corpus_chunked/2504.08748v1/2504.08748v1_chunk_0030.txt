[ 357] employs vision-language models to convert the visual component of a query into textual descriptions, followed by text- based retrieval planning. This strategy simplifies the multimodal problem into a traditional text-based RAG pipeline, leveraging established multi-stage query processing techniques from conventional IR systems. However, this approach often introduces a semantic gap between the user’s original intent and the generated textual descriptions. The conversion of visual queries to text may fail to precisely capture the user’s specific information needs, leading to the retrieval of irrelevant or noisy documents that diverge from the query’s focus. – Image-centric planning strategies rely solely on image-based retrieval regardless of the query characteristics. Systems such as Wiki-LLaVA [24] demonstrate this paradigm by consistently triggering image retrieval from knowledge bases for multimodal queries. While this approach ensures visual information preservation, it presents practical limitations. Recent empirical studies [125] highlight that compulsive image retrieval can be counterproductive, particularly when textual information suffices or when retrieved images introduce misleading visual contexts, impairing MLLM performance. The inflexibility of single-modality planning strategies highlights their inherent limitations: they cannot adapt to the diverse information needs of real-world scenarios. For example, while a text-centric approach may be suitable for queries referencing visual content but focused on factual information, an image-centric strategy is more effective for queries requiring detailed visual comparisons. • Planning for Multimodal Retrieval. Recent studies have begun investigating the use of multimodal information retrieval to enhance the performance of MRAG systems. Unlike single- modality approaches, these methods integrate both textual and visual knowledge sources, albeit through fixed processing pipelines. For instance, MMSearch [147] employs a rigid multimodal planning pipeline, mandating Google Lens image searches for all image-containing queries. This is followed by a "Requery" phase, where LLMs reformulate the search query using the original query, image, and Google Lens results.