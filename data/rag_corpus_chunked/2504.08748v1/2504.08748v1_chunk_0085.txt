visual chart understanding tasks like chart information extraction, reasoning, and classification. Web2Code [447] introduces a webpage-to-code dataset for instruction tuning and an evaluation framework to assess MLLMsâ€™ webpage understanding and HTML code translation capabilities. VisualWebBench [221] evaluates MLLMs on various web tasks at website, element, and action levels. Many charts lack data point annotations, necessitating MLLMs to infer values using chart elements. ComTQA [ 486] introduces a table VQA benchmark for perception and comprehension tasks, while DocVQA [268] focuses on document image QA with an emphasis on information extraction tasks. InfographicVQA [267] targets understanding infographics images, which are designed to present information concisely. Infographics exhibit diverse layouts and structures, requiring basic reasoning and arithmetic skills. As MLLMs advance, benchmarks now focus on complex chart and document understanding. For instance, DocGenome [403] analyzes scientific papers, covering tasks like information extraction, layout detection, VQA, and code generation. CharXiv [387] targets challenging charts from scientific papers, while MP-DocVQA [358] extends DocVQA to multi-page scenario, where questions are constructed based on multi- page documents instead of single page. MMLongBench-Doc [ 258] focuses on long document understanding, averaging 47.5 pages. SciGraphQA [ 192] is a synthetic dataset with 295K QA dialogues about academic graphs, generated using Palm-2 from CS/ML ArXiv papers. SciFIBench [313] benchmarks scientific figure interpretation, using adversarial filtering for negative examples and human verification for quality assurance. Despite advancements, a performance gap persists between proprietary and open-source models on conventional benchmarks. Current MLLMs continue to face challenges in reasoning tasks and long-context document comprehension, particularly in interpreting extended multimodal contexts, which remains a critical limitation. 4.2.4 Mathematics. Visual math problem-solving is key to evaluating MLLMs, leading to the development of specialized benchmarks. MathVista [242] pioneered this effort by aggregating 28 existing datasets and introducing 3 new ones, featuring diverse tasks like logical, algebraic,