fine-grained cross-modal knowledge from advanced models, and re- fining text-video similarity with an frame-feature aggregation block. T-MASS [371] addresses dataset limitations by enriching text embeddings with stochastic text modeling. ∗ Text–Audio Retrieval. Text-audio retrieval involves matching textual queries with corre- sponding audio content, requiring alignment of semantic text information with dynamic acoustic patterns in speech, music, or environmental sounds. The challenge lies in bridging the gap between discrete text and continuous audio signals. Early CNN/RNN-based approaches [ 239, 271, 485] focus on encoding text and audio separately and aligning them in a shared space for similarity measurement. ATR [239] uses pretrained CNN-based audio networks with NetRVLAD pooling [143] to aggregate features into a unified representation. OML [271] employs CNNs for robust audio feature extraction and metric learning to enhance audio-text alignment. MGRL [ 485] leverages CNNs for localized audio features and introduces adaptive aggregation to handle varying text–audio granularities. Furthermore, Transformer-based methods [63, 406, 494] utilize multi-head attention mech- anisms and fine-tuning to enhance cross-modal interactions. TAP-PMR [406] employs scaled dot-product attention to enable text to focus on relevant audio frames, reducing mislead- ing information, while its prior matrix revised loss optimizes dual matching by addressing similarity inconsistencies. CMRF [494] enhances audio-lyrics retrieval through directional cross-modal attention and reinforcement learning to refine multimodal embeddings and interactions. TTMR++ [63] integrates fine-tuned LLMs and rich metadata to generate detailed text descriptions, improving retrieval by addressing musical attributes and user preferences. ∗ Unified-Modal Retrieval. Unified-Modal Retrieval aims to process diverse hybrid-modal data (e.g., text, images, videos) within a unified model architecture, such as transformer-based PLMs, to encode all modalities into a shared feature space. This enables efficient cross-modal retrieval between any pairwise combination of hybrid-modal data. With the growing demand for multimodal applications, there is an increasing need for unified multimodal retrieval models