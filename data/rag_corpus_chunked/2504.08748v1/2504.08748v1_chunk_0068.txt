LISA [ 168], GILL [164], Kosmos-2 [ 293], DreamLLM [ 65], MiniGPT-5 [490], Kosmos-G [ 287], VisCPM [ 124], CM3Leon [ 434], LaVIT [155], GLaMM [ 307], RPG [ 418], Vary-toy [392], CogCoM [ 298], SPHINX-X [ 216], LLaVA-Plus [223], PixelLM [310], VL-GPT [506], CLOVA [96], Emu-2 [337], MM-Interleaved [355], DiffusionGPT [301] • Video ⊕ Text → Text Video-ChatGPT [260], VideoChat [187], Dolphins [257] • Video ⊕ Text → Video ⊕ Text Video-LaVIT [154] • Unified → Text Flamingo [10], X-LLM [ 30], LanguageBind [ 503], InstructBLIP [ 219], MM-REACT [423], X-InstructBLIP [ 289], EmbodiedGPT [280], Video-LLaMA [460], Lynx [450], LLaMA-VID [198], InternVL [47], AnyMAL [278] • Unified → Image ⊕ Text BuboGPT [487], Emu [338], GroundingGPT [204] • Unified → Unified TEAL [424], GPT-4 [7], Gemini [ 353], HuggingGPT [ 325], CoDi-2 [ 351], AudioGPT [129], ModaVerse [382], MLLM-Tool [366], ControlLLM [236], NExT-GPT [400] Fig. 8. Taxonomy of recent advancements in multimodal generation research. relevant outputs. We classify MLLMs from generative perspectives of inputs and outputs, and summarize the related researches in Figure 8. 3.4.1 MODALITY INPUT. With the rapid advancement of large language models in the domain of textual knowledge comprehension and question-answering, researchers try to explore how to enable these models to understand and process inputs from a broader range of modalities, thereby facilitating more extensive multimodal question-answering tasks. Initial efforts focused on incorporating image modality into the input of large models. For instance, Blip-2 [185] proposes a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language