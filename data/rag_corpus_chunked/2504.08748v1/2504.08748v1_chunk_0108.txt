it also ensures alignment with multimodal retrieved content, including textual, visual, and auditory elements, maintaining consistency across modalities. – Hallucination: This metric measures the proportion of generated outputs containing hal- lucinated content, such as unsupported claims, fabricated information, or inaccuracies not substantiated by the retrieved data. It is essential for evaluating the reliability and factual consistency of the model’s responses. LLM/MLLM-based metrics are highly effective at capturing complex semantic relationships and contextual nuances, making them particularly suitable for multimodal RAG systems. However, they may inherit biases from the underlying models and demand substantial computational resources. • Metric Calculation: When evaluating multimodal retrieval-augmented generation systems, implementation methods for the same metric can vary significantly, primarily categorized into coarse-grained and fine-grained approaches. These methodologies differ in their granularity and the depth of analysis applied to assess the quality of model-generated responses against reference answers. – Coarse-Grained Evaluation: Coarse-grained evaluation utilizes LLMs or MLLMs to compare model-generated responses with reference answers. This method involves inputting both the generated output and the reference into the LLM/MLLM, which evaluates the overall semantic alignment, coherence, and relevance between the two. The model assesses whether the generated content captures the core meaning and intent of the reference, providing a holistic score or qualitative feedback. This approach is computationally efficient and scalable, making it suitable for rapid benchmarking and high-level quality checks in large-scale applications. However, its broad focus may overlook fine-grained inaccuracies, such as subtle factual errors, logical inconsistencies, or nuanced contextual mismatches. Consequently, coarse- grained evaluation is best used as an initial screening tool or in scenarios where high-level semantic fidelity is prioritized over detailed precision. For more rigorous evaluation, it is often supplemented by fine-grained metrics that address specific aspects of content quality. In summary, coarse-grained evaluation offers a pragmatic balance between efficiency and effectiveness,