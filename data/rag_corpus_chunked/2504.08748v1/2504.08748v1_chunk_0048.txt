Retrieval. Unified-Modal Retrieval aims to process diverse hybrid-modal data (e.g., text, images, videos) within a unified model architecture, such as transformer-based PLMs, to encode all modalities into a shared feature space. This enables efficient cross-modal retrieval between any pairwise combination of hybrid-modal data. With the growing demand for multimodal applications, there is an increasing need for unified multimodal retrieval models tailored to complex scenarios. Current approaches leverage pre-trained models like CLIP [305], BLIP [186], and ALIGN [144] for multimodal embedding. For instance, FLAVA , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 23 [326] integrates multiple modalities into a unified framework, leveraging joint pretraining on multimodal data with cross-modal alignment and fusion objectives. Similarly, UniVL-DR [237] encodes queries and multimodal resources into a shared embedding space, employing a universal embedding optimization strategy with modality-balanced hard negatives and an image verbalization method to bridge the gap between images and texts. MARVEL [ 498] addresses the modality gap between images and texts by incorporating visual features into the encoding process. FLMR [211] enhances image representations by using a visual model aligned with existing text-based retrievers to supplement the image representation of image-to-text transforms. UniIR [391] introduces a unified instruction-guided multimodal retriever, achieving robust generalization through instruction tuning on diverse multimodal- IR tasks. VISTA [495] extends image understanding capability by integrating visual token embeddings into a text encoder, supported by high-quality composed image-text data and a multi-stage training algorithm. E5-V [ 150] fine-tunes MLLMs on single-text or vision- centric relevance data, outperforming traditional image-text pair training. VLM2VEC [152] proposes a contrastive training framework to convert vision-language models into embedding models using the MMEB dataset [152]. To address modality imbalance, GME [476] trains an MLLM-based dense retriever on the large-scale UMRB dataset[ 476]. Ovis [244] aligns