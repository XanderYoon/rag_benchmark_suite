the “[CLS]” token embedding. Condenser [92] aggregates global text information for masked token recovery, while co-Condenser [93] adds a query-agnostic contrastive loss to cluster related text segments while distancing unrelated ones. Contriever [139] generates positive pairs by sampling two spans from the same text and negatives using in-batch and cross-batch texts. Following with an unbalanced architecture (strong encoder, simple decoder), SimLM [373] pretrains the encoder and decoder with replaced language modeling, recovering original tokens after replacement. It further optimizes the retriever through hard negative training and cross-encoder distillation. RetroMAE [491] utilizes a high masking ratio for the decoder and a standard ratio for the encoder, incorporating an enhanced decoding mechanism with two-stream and position-specific attention masks. Liu and Yang [214] introduces a two-stage pretraining approach, combining general-corpus pretraining with domain-specific continual pretraining, achieving strong benchmark performance. However, single-modal retrieval is inherently limited by its inability to capture cross-modal relationships, which underscores the importance of integrating multimodal retrieval strategies to bridge textual and visual semantics for more comprehensive information retrieval and generation. – Retrieval for Cross-modal. Cross-modal retrieval enables the identification of relevant data in one modality (e.g., images) using a query from another (e.g., text). It enhances MRAG systems by facilitating the retrieval and generation of information across diverse modalities, including text, images, audio, and video. ∗ Text–Image Retrieval. Text–Image Retrieval aims to match images with corresponding textual queries by leveraging multimodal data co-occurrence, such as paired text-image instances or manual annotations, to capture semantic correlations. Existing methods can be categorized into three groups: CNN/RNN-based approaches, Transformer-based techniques, and Vision-Language Pretraining (VLP) model-based methods. Early CNN/RNN-based methods [75, 110, 174, 231, 370, 390] extract features from each modality separately using MLP, CNN, and RNN, enforcing cross-modal constraints through positive/negative sample construction. MSDS [370] uses CNN with a maximum likelihood- based