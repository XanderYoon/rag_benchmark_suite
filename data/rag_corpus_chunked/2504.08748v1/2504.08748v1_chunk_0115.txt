Additionally, refining external multimodal knowledge post- retrieval and reranking remains underexplored, despite its potential to enhance result accuracy and relevance. Addressing these gaps requires innovative methodologies that leverage MLLMs and LLMs to enable sophisticated cross-modal understanding and reasoning. 6.4 Generation The multimodal module in MRAG achieves human-aligned sensory representation through diver- sified modality integration, which significantly enhances user experience and system usability. However, achieving these enhancement objectives entails addressing shared challenges across both QA systems and multimodal generation: • Multimodal Input: Multimodal systems face the challenge of integrating diverse data structures and representations across modalities such as text, images, audio, and video. As multimodal models evolve, they are increasingly required to process arbitrary combinations of modalities (e.g., text+image, text+video, image+audio). This necessitates a highly flexible and adaptive framework capable of dynamically accommodating diverse input configurations. Such frameworks must be modality-agnostic, enabling seamless integration of any input combination without predefined structures or extensive retraining. Achieving this flexibility involves designing architectures that generalize across modalities, extract relevant features, and fuse them meaningfully, regardless of input composition. , Vol. 1, No. 1, Article . Publication date: April 2018. 50 Trovato et al. • Multimodal Output: – Coherent and Contextually Relevant Generation : Ensuring consistency across different modalities in the output presents a significant challenge. For instance, in a text-image pair, the image must accurately reflect the scene or object described in the text, while the text should precisely convey the visual content. – Positioning and Integration of Multimodal Elements : In multimodal outputs, such as text with embedded images or videos, the model must intelligently determine where to integrate non-textual elements. This requires an understanding of the narrative flow and the identification of optimal insertion points to enhance coherence and readability. Additionally, the model should dynamically generate or retrieve relevant multimodal content based