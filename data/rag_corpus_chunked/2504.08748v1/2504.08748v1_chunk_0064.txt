discrete tokens from a predefined vocabulary, soft prompts are optimized through training to capture nuanced meanings that discrete tokens cannot express. When fine-tuned on diverse datasets, soft prompts enhance the LLM’s performance across various tasks. – Refining for Text-Model: Language models convert text prompts into vectors for denser representation, enabling compression of discrete text into continuous vectors within the model. These vectors can serve as internal parameters (internalization) or additional soft prompts (encoding). Such compression extends the context window and enhances inference speed, particularly with repeated prompt usage. Early work focused on system prompt internalization. Askell et al. [13] used Knowledge Distillation to align models with human values, while Choi et al. [53] introduced Pseudo-Input Generation, generating pseudo-inputs from prompts and distilling knowledge between teacher and student models to avoid redundant inference computations. Later research compressed user prompt contexts. Snell et al. [331] distilled abstract instructions, reasoning, and examples into prompts with distinct distribution differences, enabling task execution without explicit prompts. Sun et al. [339] internalized ranking techniques for zero-shot relevance tasks, while Distilling Step-by-Step [122] improved reasoning tasks by distilling rationales as additional supervision. In retrieval-augmented generation, xRAG [48] integrated compressed document embeddings via a plug-and-play projector, using self-distillation for robustness. For context compression, COCOM [308] reduced long contexts to few embeddings, balancing trade-offs between decoding time and answer quality. LLoCO [346] learned offline compressed representations for efficient , Vol. 1, No. 1, Article . Publication date: April 2018. 30 Trovato et al. QA retrieval. QGC [25] retained key information under high compression using query-guided dynamic strategies. UniICL [91] unified demonstration selection, compression, and generation within a single frozen LLM, projecting demonstrations and inputs into virtual tokens for semantic-based processing. Recent advancements in prompt compression for LLMs focus on encoding hard prompts into reusable soft prompts to enhance efficiency