each question, it requires the correct answer to be selected between five given options based on a three-minute-long video clip. Video-MME [85] 2024 It contains a annotated set of 2,700 high-quality multiple-choice questions (3 per video) from 900 videos, 744 subtitles and 900 audio files across various scenarios. For diversity in video types, it spans 6 visual domains, with 30 subfields. For duration in temporal dimension, it encompasses both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour. MVBench [188] 2024 covers 20 video temporal understanding tasks that cannot be effectively solved with a single frame. Each task produces 200 multiple-choice QA pairs by leveraging ChatGPT to automatically reannotate existing video datasets with their original annotations. MMBench-Video [77] 2024 incorporates approximately 600 web videos from YouTube, spanning 16 major categories. Each video ranges in duration from 30 seconds to 6 minutes. The benchmark includes roughly 2,000 original question-answer pairs, contributed by volunteers, covering a total of 26 fine-grained capabilities. MLVU [496] 2024 consists of 3,102 questions across 9 categories with 2,593 questions for dev set and 509 questions for test set. It is made up of videos of diversified lengths, spanning from 3 min to more than 2 hours. Besides, each video is further partitioned as incremental segments, e.g., the first 3 min, the first 6 min, and the entire video. LVBench [376] 2024 gathers an initial collection of 500 videos, each with a minimum duration of 30 minutes. Finally, these videos is annotated to select a subset of 103 videos. Event-Bench [71] 2024 includes 6 event-related tasks and 2,190 test instances. VNBench [488] 2024 1,350 samples with 9 sub-tasks. TempCompass [232] 2024 collects a total of 410 videos and 500 pieces of meta-information, with 9 content categories. MovieChat [332] 2024 1K long videos and 13K manual