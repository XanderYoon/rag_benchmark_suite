achieved through two primary approaches: hard prompt methods and soft prompt methods. Hard prompt methods involve filtering out unnecessary or low-information content, still using natural language tokens and resulting in less fluent but generalizable prompts , Vol. 1, No. 1, Article . Publication date: April 2018. 28 Trovato et al. that can be used across LLMs with different embedding configurations. Soft prompt methods, in contrast, encode prompt information into continuous representations, producing latent vectors (special tokens) that are not human-readable but optimized for model performance. • Hard Prompt Refiner: Hard prompts consist of natural language tokens from the LLM/MLLM’s vocabulary, representing specific words or sub-words, and can be generated by humans or models. – Refining for Text-Model: Recent advancements in prompt compression and context distil- lation aim to optimize the efficiency of LLMs. DynaICL [499] employs a meta controller to dynamically allocate in-context demonstrations based on input complexity and computational constraints. FILCO [385] distills retrieved documents using lexical and information-theoretic methods—String Inclusion, Lexical Overlap, and CXMI—training both context filtering and generation models for RAG tasks. CPC [213] preserves semantic integrity by using a context- aware encoder to remove irrelevant sentences, while AdaComp [ 469] dynamically selects optimal documents via a compression-rate predictor. LLMLingua [148] introduces a coarse- to-fine approach, compressing prompt components (instructions, questions, demonstrations) using a small language model (SLM) to measure token informativeness via perplexity (PPL). LongLLMLingua [149] extends this to long documents, employing a linear scheduler, reorder- ing mechanism, and contrastive perplexity to retain question-relevant tokens while ensuring key information integrity. CoT-Influx [132] compresses GPT-4-generated Chain-of-Thought (CoT) prompts using a shot-pruner and token-pruner, both implemented as MLPs trained via reinforcement learning. These methods collectively improve performance while reducing useless CoT examples and redundant tokens. Selective Context [196] evaluates lexical unit in- formativeness using a causal language model