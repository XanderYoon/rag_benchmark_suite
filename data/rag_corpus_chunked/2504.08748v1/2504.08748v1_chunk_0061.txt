ing mechanism, and contrastive perplexity to retain question-relevant tokens while ensuring key information integrity. CoT-Influx [132] compresses GPT-4-generated Chain-of-Thought (CoT) prompts using a shot-pruner and token-pruner, both implemented as MLPs trained via reinforcement learning. These methods collectively improve performance while reducing useless CoT examples and redundant tokens. Selective Context [196] evaluates lexical unit in- formativeness using a causal language model and a percentile-based filtering method to remove redundancy. It calculates token self-information by predicting next-token probabilities, aggre- gating these at phrase and sentence levels. Prompt-SAW [11] preserves syntactic and semantic structures by extracting key tokens via relation-aware graphs, integrating them into com- pressed prompts. PCRL [159] treats prompt compression as a binary classification task, using a frozen pre-trained policy language model with trainable MLP layers. The compression policy labels tokens as include or exclude, optimizing a reward function that balances faithfulness and prompt length reduction. LLMLingua-2 [288] employs a bidirectional encoder-only model with a linear classification layer for compression, determining token retention or removal. RECOMP [410] employs extractive and abstractive compressors to generate query-focused summaries, leveraging contrastive learning and knowledge distillation. Nano-Capsulator [56] optimizes compression using reward feedback from response differences and enforces strict length con- straints. MEMWALKER [32] uses interactive prompting to build and navigate a memory tree for context summarization. CompAct [432] sequentially compresses document segments for long-context question-answering, achieving high compression rates. Style-Compress [297] iteratively refines prompts using diverse styles and task-specific examples, evaluated by larger LLMs. TCRA-LLM [220] combines summarization and semantic compression to reduce token size. TACO-RL [323] employs reinforcement learning for task-aware prompt compression, ensuring low latency. FaviComp [ 158] enhances evidence familiarity by combining token probabilities from compression and target models, reducing perplexity. â€“ Refining for Cross-Model: Recent advancements in visual token compression for MLLMs focus on enhancing efficiency without significant performance loss.