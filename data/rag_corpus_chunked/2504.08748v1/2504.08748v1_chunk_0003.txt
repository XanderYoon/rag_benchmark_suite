are to be prioritized. When handling queries that extend beyond the scope of their training knowledge or require the most current information, LLMs may generate responses that are speculative or based on patterns they have learned, rather than on verified facts. This can result in misleading, incorrect, or even entirely fabricated answers, a phenomenon known as "hallucination". Minimizing the incidence of hallucinations is important for enhancing the reliability of LLMs in providing accurate and context-relevant information across different domains. Recently, Retrieval-Augmented Generation (RAG) has emerged as an effective solution to mitigate hallucinations, by enhancing the generation capabilities of large language models (LLMs) through the retrieval of relevant external knowledge. Existing RAG systems typically operate through a two-step process: retrieval and generation. In the retrieval step, the goal is to quickly locate relevant knowledge that is semantically similar to the query from a large-scale document collection. Since the relevant knowledge is often scattered across various parts of documents, each document is pre-processed into multiple chunks. Additional chunks may be created through manual or automated methods. This process, known as document chunkerization, ensures that fine-grained knowledge can be retrieved more efficiently. In the generation step, the retrieved document chunks are combined with the query to form an augmented input. This augmented input provides the LLM with context that includes external knowledge. Furthermore, RAG allows LLMs to dynamically integrate the latest information during the inference stage. This capability ensures that the modelâ€™s responses are not only based on static, pre-trained knowledge but are continuously updated with current and relevant data. By retrieving and referencing external knowledge, RAG grounds the generated responses in factual information, thereby significantly reducing the occurrence of hallucinations. However, previous research on RAG systems has primarily focused on knowledge bases built from plain text and LLMs pre-trained on plain text,