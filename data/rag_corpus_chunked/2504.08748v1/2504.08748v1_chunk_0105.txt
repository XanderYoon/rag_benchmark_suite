between generated and reference texts. – SPICE (Semantic Propositional Image Caption Evaluation) : The evaluation of MRAG systems frequently utilizes the SPICE metric to assess the quality of generated captions. SPICE prioritizes semantic fidelity by parsing captions into structured scene graphs, which depict objects, attributes, and relationships within the text. These generated scene graphs are subsequently compared to reference graphs derived from ground-truth captions. By emphasiz- ing semantic similarity over lexical overlap, SPICE offers a robust measure of how well the generated content aligns with the intended meaning. This makes it particularly well-suited for evaluating multimodal systems that integrate visual and textual information, ensuring a nuanced and contextually accurate assessment of MRAG outputs. – BERTScore : Evaluation of MRAG focuses on assessing the quality and relevance of outputs in contexts integrating both textual and non-textual data (e.g., images, audio). A key metric for evaluating textual components is BERTScore, which utilizes contextual embeddings from BERT to measure semantic similarity between generated and reference texts. Unlike traditional metrics such as BLEU or ROUGE, which depend on exact word matches or n-gram overlap, BERTScore captures deeper semantic relationships by aligning tokens based on their contextual embeddings. – Perplexity: It measures the model’s ability to predict the next word in a sequence, with lower perplexity values indicating greater confidence and accuracy in predictions. This reflects a stronger understanding of the underlying data distribution. Rule-based metrics offer objective and reproducible outcomes but frequently lack the adaptability needed to capture nuanced semantic or contextual understanding, especially in multimodal environments where text, images, and other data types interact. • LLM/MLLM-based Metrics: The emergence of LLMs and MLLMs has transformed evalua- tion paradigms, enabling the use of their advanced reasoning and comprehension capabilities. LLM/MLLM-based metrics now provide more holistic and context-aware assessments of MRAG systems, with key approaches