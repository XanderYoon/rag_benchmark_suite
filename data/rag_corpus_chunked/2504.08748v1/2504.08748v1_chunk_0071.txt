conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains. NExT-GPT [400] present an end-to-end general-purpose any-to- any MM-LLM system. NExT-GPT connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, image, video, and audio. By leveraging the existing well-trained high-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training but also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. 3.4.2 MODALITY OUTPUT. With the explosive growth in the capabilities of MLLMs, the ability to answer questions based on multimodal inputs and generate multimodal outputs has also seen a qualitative improvement. There is also increasing attention from researchers on VQA scenarios that shift from generating text results to generating multimodal results that include text.In this section, we are discussing multimodal outputs that are not scenarios like text-to-image or text-to-video, which only generate a single modality, but rather scenarios where the answers includes text and at least one other modality of data, such as text-image output, or image-video output.In the basic VQA task, MIMOQA[328] was the first to propose the concept of multimodal output, which achieved the capability of multimodal output by transforming questions into an image-text matching