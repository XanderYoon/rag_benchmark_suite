models. Performance varies significantly by data type, with knowledge graphs and maps posing greater challenges than simpler formats like charts, suggesting potential improvements through data-specific optimization or dedicated OCR integration. 4.2.3 Structural Document. Structural documents, including charts, HTML web content, and vari- ous document formats, play a critical role in practical applications due to their ability to efficiently convey complex information. These data types are characterized by their highly structured nature and information density, distinguishing them from natural images. Unlike images, which rely on visual patterns and textures, structural documents require models to comprehend intricate layouts, spatial relationships, and semantic connections between embedded elements such as text, tables, and graphical components. To advance models capable of understanding and reasoning with such data, several benchmarks have been proposed for different types of structural documents. Early dataset FigureQA [ 160] , Vol. 1, No. 1, Article . Publication date: April 2018. 38 Trovato et al. introduces a visual reasoning corpus with synthetic images and scientific-style figures, focusing on relationships between plot elements. ChartQA [266] emphasizes VQA with charts, ranging from tasks that require both data extraction and math reasoning. ChartX [404] collects a comprehensive dataset with 22 topics, 18 chart types, and 7 tasks, incorporating multiple modalities. VisualMRC [348] targets visual machine reading comprehension, emphasizing natural language understanding and generation. ChartBench [ 414] evaluates chart comprehension and data reliability through complex reasoning. MMC-Benchmark [217] provides a human-annotated benchmark to assess MLLMs on visual chart understanding tasks like chart information extraction, reasoning, and classification. Web2Code [447] introduces a webpage-to-code dataset for instruction tuning and an evaluation framework to assess MLLMsâ€™ webpage understanding and HTML code translation capabilities. VisualWebBench [221] evaluates MLLMs on various web tasks at website, element, and action levels. Many charts lack data point annotations, necessitating MLLMs to infer values using