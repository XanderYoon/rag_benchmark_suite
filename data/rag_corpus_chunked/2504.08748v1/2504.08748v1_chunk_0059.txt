image, or interleaved text-image formats. In summary, these approaches leverages the pre-existing knowledge and reasoning capabilities of LLMs, reducing the need for extensive task-specific fine-tuning. Consequently, it provides greater flexibility and resource efficiency, particularly in scenarios with limited labeled data or computational resources. However, its effectiveness depends heavily on the quality and design of the prompts, as well as the model’s ability to generalize its pre-trained knowledge to the specific demands of the target task. 3.3.3 REFINER. Theoretically, LLMs improves with more comprehensive task-relevant knowledge in the retrieved and reranked context. However, unlimited input length poses practical deployment challenges: (1) Limited Context Window: LLMs have a fixed input length determined during pre- training, and any text exceeding this limit is truncated, leading to loss of contextual semantics. (2) Catastrophic Forgetting: Insufficient cache space can cause LLMs to forget previously learned knowledge when processing long sequences. (3) Slow Inference Speed. Consequently, refined prompts are crucial for optimizing LLM performance. The refiner is an optional yet highly impactful component that optimizes retrieved and reranked information before its utilization by the LLM. It performs advanced processing tasks, such as summarization, distillation, or contextualization, to condense and refine content into a more digestible and actionable format. By extracting key insights, eliminating redundancies, and aligning information with the query’s context, the refiner enhances the utility of the retrieved data, enabling the LLM to generate more coherent, accurate, and contextually relevant responses. Prompt refinement can be achieved through two primary approaches: hard prompt methods and soft prompt methods. Hard prompt methods involve filtering out unnecessary or low-information content, still using natural language tokens and resulting in less fluent but generalizable prompts , Vol. 1, No. 1, Article . Publication date: April 2018. 28 Trovato et al. that can be used across LLMs with different embedding configurations.