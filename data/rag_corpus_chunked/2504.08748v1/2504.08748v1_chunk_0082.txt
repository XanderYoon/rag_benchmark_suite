and LLM-based evaluators, though these face challenges with inaccurate LLM scoring. For instance, MM-Vet [439] employs diverse question formats to assess integrated vision-language capabilities, while Touch- stone [17] emphasizes real-world dialogue evaluation, arguing that multiple-choice questions are insufficient for evaluating multimodal dialogue capabilities. InfiMM-Eval [113] evaluates models on , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 37 deductive, abductive, and analogical reasoning across tasks, including intermediate reasoning steps, aligning with practical scenarios like mathematical problem-solving. These benchmarks highlight the strengths and limitations of MLLMs in complex tasks. Closed-source models excel in reasoning but struggle with complex localization, structural relationships, charts, and visual mathematics. High-resolution data improves recognition of small objects, dense text, and fine-grained details. While Chain-of-Thought (CoT) strategies significantly boost reasoning in closed-source models, their impact on open-source models remains limited. The development of multimodal benchmarks emphasizes continuous refinement to accurately as- sess model capabilities. MMStar [38] addresses data leakage by curating 1.5K visually-dependent QA pairs, while CV-Bench [359] tackles the scarcity of vision-centric benchmarks with 2.6K manually- inspected samples for 2D/3D understanding. FOCI [101] evaluates MLLMs using domain-specific subsets and supplementary classification datasets, revealing challenges in fine-grained perception. MMVP [360] identifies 9 distinct patterns in CLIP-based models, showing MLLMs’ struggles with visual details, with only Gemini and GPT-4V performing above random guessing. Q-Bench [398] evaluates low-level attribute perception, highlighting GPT-4V’s near-human performance. Visual- COT [324] introduces visual chain-of-thought prompts to enhance MLLMs’ focus on specific image regions. To further upgrading vision capabilities on multiple image understanding, Mementos [381] evaluates sequential image understanding, while MIRB [482] focuses on multi-image reasoning across perception, visual knowledge, and multi-hop reasoning tasks. ReMI [162] designs 13 tasks with diverse image relationships and input formats, and MuirBench [368] includes 12 multi-image understanding tasks with unanswerable