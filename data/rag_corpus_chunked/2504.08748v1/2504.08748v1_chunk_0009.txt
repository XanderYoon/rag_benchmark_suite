images, videos, and other non-textual elements. These chunks and captions are encoded into vector representations using an embedding model and stored in a vector database. The choice of embedding model is crucial, as it significantly impacts the performance and effectiveness of downstream retrieval tasks. ‚Ä¢ Retrieval: This component processes user queries by encoding them into vector representations using the same embedding model applied during indexing. The query vectors are then utilized to retrieve the top- ùëò most relevant chunks and captions from the vector database, typically employing cosine similarity as the relevance metric. Duplicate or overlapping information from chunks and captions is merged to create a consolidated set of external knowledge, which is subsequently integrated into the prompt for the generation phase. This ensures the system retrieves contextually relevant information to deliver accurate and informed responses. ‚Ä¢ Generation: In the Generation phase, the MRAG system synthesizes the user‚Äôs query and retrieved documents into a coherent prompt. A large language model (LLM) generates a response , Vol. 1, No. 1, Article . Publication date: April 2018. A Survey on Multimodal Retrieval-Augmented Generation 5 Documents Parsing (Extractive-Based, Plain Text) Table Captions Generation Retrieval Indexing Documents ‚Ä¶‚Ä¶ Text Text Parsing Model Image Caption Model Multimodal Data Image Table ‚Ä¶‚Ä¶ Table Parsing Model Text Chunks Image Captions Text Embedding Model Text Vector DB Relevant Text Chunks Relevant Image Captions Prompt LLMs Query + History ÔºàText OnlyÔºâ OCR-Based Model Text Embedding Model Answer ÔºàText OnlyÔºâ Fig. 1. The architecture of MRAG1.0, often termed "pseudo-MRAG", closely resembles traditional RAG, consisting of three modules: Document Parsing and Indexing, Retrieval, and Generation. While the overall process remains largely unchanged, the key distinction lies in the Document Parsing stage. In this stage, specialized models are employed to convert diverse modal data into modality-specific captions. These captions are then stored