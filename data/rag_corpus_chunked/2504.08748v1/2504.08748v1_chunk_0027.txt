text and layout information across scanned document images. LayoutLMv2 [413] and LayoutLMv3 [133] further propose a new single multimodal framework to model the interaction among text, layout, and image. DocFormer [12] based on the multimodal transformer architecture proposes a novel multimodal attention layer to fuse text, vision, and spatial features in a document, thereby achieving end-to-end document parsing. â€¢ Multimodal Extraction. In this phase, the original format of multimodal data is preserved during extraction, allowing downstream tasks to autonomously determine subsequent operations. For semi-structured documents, extraction can be performed similarly using rule-based methods. Relevant multimodal data is identified through specific tags, such as extracting images from , Vol. 1, No. 1, Article . Publication date: April 2018. 14 Trovato et al. HTML files using the "<img>" tag. However, this approach faces similar challenges to plain text extraction. The pipeline for multimodal document parsing based on OCR consists of three steps: page segmentation, text recognition, and text parsing. Page segmentation, similar to text detection in plain text extraction, locates and extracts target regions while annotating them with semantic labels (e.g., title, table, footnote). This subtask of semantic segmentation commonly employs CNN- based methods, categorized into region-based, FCN-based, and weakly supervised approaches [111]. Text recognition, similar to plain text extraction, focuses on parsing text data such as titles and page text. Text parsing involves layout analysis and other operations, processing multimodal data according to downstream task requirements. In the era of LLMs, multimodal data is often converted into text for utilization, as seen in models like TableNet [286] for tables and UniChart [265] for charts. This necessitates distinct models for extracting captions from different modalities. With the advancement of MLLMs, there is a trend toward unifying these models into a single MLLM framework, leveraging their robust representation capabilities [173, 227]. Further developments