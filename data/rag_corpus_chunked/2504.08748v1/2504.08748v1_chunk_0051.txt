formats, or structured identifiers to represent documents. GENRE [61] generates entity names via constrained beam search using a prefix tree, with document titles serving as DocIDs. DSI [352] introduces numeric DocID formats, including unstructured, naively structured, and semantically structured identifiers, trained through indexing and retrieval strategies. DynamicRetriever [502] uses unstructured atomic DocIDs and enhances memorization with pseudo queries. SEAL [19] representing documents with N-gram sub-string identifiers, leveraging FM-Index [82] for retrieval. DSI-QG [512] represents documents with generated queries, re-ranked by a cross-encoder. NCI [383] generates document identifiers using a seq2seq network with a prefix-aware decoder. It is trained on both labeled and augmented pseudo query-document pairs. Ultron [501] combines URLs and titles as DocIDs to uniquely identify web documents. It encodes documents into a latent semantic space using BERT [62] and compresses vectors via Product Quantization (PQ) [ 98, 142], with PQ codes serving as semantic identifiers. Additional digits ensure DocID uniqueness. LTRGR [ 201] focuses on learning to rank passages directly using generative retrieval models, optimizing autoregressive models via rank loss. GenRRL [500] integrates reinforcement learning for aligning token-level DocID generation with document-level relevance estimation. DGR [202] enhances generative retrieval through knowledge distillation, using a cross-encoder as a teacher model to provide fine-grained ranking supervision. Despite these innovations, most approaches rely on static DocIDs, which are not optimized for retrieval tasks, limiting their ability to capture document semantics and relationships, thereby hindering retrieval performance. To address this limitation, Learnable DocID-based methods introduce learnable document representations, where DocIDs are optimized during training to better capture document semantics and improve retrieval performance. GenRet [340] employs a discrete autoencoder to encode documents into compact DocIDs, minimizing reconstruction error. MINDER [200] enhances document representations using multi-view identifiers, including pseudo-queries, titles, and sub-strings. NOVO [389] introduces learnable continuous N-gram DocIDs, refining embeddings through query