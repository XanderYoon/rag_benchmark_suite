a balance between computational complexity and retrieval quality. Our experimental results illustrates significant enhancement not only in retrieval but also in the accuracy of the final output generated by multi-modal RAG systems. V. N UMERICAL RESULTS We use RS and CS model developed in [9] for eval- uation. The performance of RS and CS has shown 88% and 91% alignment with human evaluation, respectively, as reported in [9] over 5000 human evaluated samples of RAG queries/responses built on a set of 1281 images from COCO dataset. Here, our evaluation relies on the reported accuracy of the corresponding RS and CS models. Here, we first compare the retrieval relevancy performance of CLIP-score with directly using RS and re-ranking with l = 10 , 20. We randomly select 1,000 questions from the COCO-QA dataset and use the same set of 1,281 images from the COCO dataset. Fig. 6 illustrates the average relevancy score (RS) for the top 5 selected entries where the horizontal axis is the order of the selected entry for each method. We observe that different CLIP models (large, base-16, and base- 32) perform similarly, while using RS directly, the relevancy is boosted almost by a factor of 2. However, this boost comes at a cost of 35 times slower retrieval process. The re-ranking mechanism with l = 20 and l = 10 provide a significant boost in relevancy performance of about 85% and 71%, at cost of 1.55 and 1.27 times slower retrieval process, respectively. Next, we compare the generated output performance of different RAG implementations, i.e., four RAG schemes with combinations of different VLMs and LLMs as well as a RAG scheme directly based on GPT-4o that processes multi-modal context, e.g., multiple images, and text query simultaneously. For each RAG implementation, we use 4 retrieval methods: top-k selection