provides a quantitative value that can be used to explicitly rank entries based on their relevancy as well as eliminating the entries due to irrelevancy. In this work, we address the shortcomings of CLIP-based retrieval by proposing the use of previously developed RS model for re-ranking retrieved entries in multi-modal RAG arXiv:2501.04695v1 [cs.LG] 8 Jan 2025 systems [9]. Unlike CLIP, the RS model is specifically trained to assess query-specific relevance while penalizing irrelevant entries effectively. For example, given the query “a doctor holding a medical instrument”, RS can prioritize images depicting this exact context over generic depictions of doctors or medical tools, which CLIP might incorrectly rank highly. By replacing the initial CLIP-based top- k selection process with RS-based re-ranking, our method ensures that retrieved data aligns more closely with the user’s intent. Our evaluation, conducted on a dataset derived from the COCO benchmark, demonstrates that RS-based re-ranking significantly improves the quality of selected images, leading to more accurate and factually grounded responses as measured by our correctness score (CS). Furthermore, we highlight the broader implications of using advanced relevance metrics in retrieval refinement for multi-modal RAG systems, showcasing their potential to mitigate hallucinations and enhance response reliability. The design, training, and evaluation of both RS and CS models are available in [9]. II. M ULTI-MODAL RAG In RAG, the knowledge-base is pre-processed by generating embeddings for each piece of information which is stored in a vector database enabling fast similarity-based retrieval. In multi-modal RAG systems, embeddings for different data types, such as text and images, are derived using modality- specific encoders which share the same embedding space that is used for embedding the query. For instance, CLIP embeddings can be used, where text and image data are encoded using the CLIP text and vision encoders, respectively, and relevance is