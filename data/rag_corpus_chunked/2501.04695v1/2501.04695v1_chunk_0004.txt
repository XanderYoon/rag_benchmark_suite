similarity-based retrieval. In multi-modal RAG systems, embeddings for different data types, such as text and images, are derived using modality- specific encoders which share the same embedding space that is used for embedding the query. For instance, CLIP embeddings can be used, where text and image data are encoded using the CLIP text and vision encoders, respectively, and relevance is determined through cosine similarity. It is noted that even though by using CLIP for both image and text, the embedding spaces are the same, the comparison is not seamless, i.e., the range of similarity between text-text pair is quite different with that of text-image pair. This is yet another challenge in selecting the relevant pieces of information for the scenarios where both image and texts are selected from the vector database. The retrieved top- k entries in the first phase of the RAG, forming the raw context , are passed to the next phase of the RAG, i.e., the generation module, to produce the final response. In multi-modal scenarios, RAG systems may utilize different approaches for the generation module. One approach involves using vision language models (VLMs) [10], [11] to convert the retrieved image context into text-based descrip- tions, which are then combined with the query to generate the final response. Alternatively, MLLMs can directly process the retrieved images along with the user query to generate the response. III. RS SCORE VS CLIP FOR RAG SELECTION The RS (please see details in [9]) is a specialized metric designed to evaluate the relevancy of a piece of information from the vector database to the user query in multi-modal RAG systems. The RS model is designed by using a VLM with specific fine-tuned head that learns the semantic relationship between a user query and a retrieved entry, such as an image or