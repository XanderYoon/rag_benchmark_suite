suboptimal retrieval, and consequently, hallucinations in downstream tasks. In multi-modal RAG systems, where accuracy depends on precise alignment between the retrieved data and the user query, such limitations can significantly degrade the system’s reliability. Re-ranking techniques offer a promising solution to refine initial retrieval results by re-evaluating the relevance of re- trieved entries before passing them to the response generation phase. Recent works in multimodal retrieval have introduced advanced re-ranking methods to enhance relevance estimation [4]–[8]. For example, the work in [5] reconstructs data samples based on top-ranked intra- and inter-modal neighbors (referred to as “pillars”) to improve retrieval accuracy. The work in [4] employs multi-modal large language models (MLLMs) with knowledge-enhanced re-ranking and noise-injected training to refine retrieval results. Also, the work in [6] demonstrates the potential of vision-language models for relevance evaluation, but it highlights these models’ tendency to over-rely on semantic similarity, often failing in cases that require precise contextual understanding. A common limitation of most re- ranking approaches is their reduced ability to detect irrelevant information, as MLLMs are primarily trained on datasets containing relevant image-query pairs. Moreover, using VLM or MLLM in prior art for re-ranking is limited to a binary decision, i.e., “YES/NO”. In contrast, our RS model addresses these challenges by being explicitly trained to assess context- specific relevance, effectively filtering out irrelevant content and ensuring that retrieved entries align with the user’s in- tent. Moreover, analogous to similarity score, the RS model provides a quantitative value that can be used to explicitly rank entries based on their relevancy as well as eliminating the entries due to irrelevancy. In this work, we address the shortcomings of CLIP-based retrieval by proposing the use of previously developed RS model for re-ranking retrieved entries in multi-modal RAG arXiv:2501.04695v1 [cs.LG] 8 Jan 2025 systems [9]. Unlike CLIP,