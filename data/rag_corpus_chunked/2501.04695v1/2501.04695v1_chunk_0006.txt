RS model minimizes a modified version of RLHF loss function that penalizes mismatched query-context pairs while rewarding alignment with the most relevant entries. This enables the RS model to differentiate between truly relevant data and entries that might exhibit superficial similarity to the query. Compared to CLIP-based methods, which rely on cosine similarity between embeddings to rank image-text pairs, RS is designed to excel in detecting both relevance and irrelevance. While CLIP is effective at capturing general semantic simi- larity, it often assigns high scores to visually or conceptually generic data, even if it is not contextually relevant to the query. For instance, given a query like “a person reading a book in a park”, CLIP may retrieve images of people in a park or people reading indoors, failing to prioritize those that align with the full context of the query. In contrast, RS is specifically tuned to penalize such irrelevant data, enabling more precise selection of relevant entries. We evaluate both CLIP and our RS model by using 2000 pairs of images and positive statements as well as 2000 pairs of the same images and negative statements drawn from evalu- ation dataset. Fig. 2 shows the histograms of similarity scores based on CLIP for 2000 evaluation dataset. The histogram for the similarity scores between the CLIP embeddings of the positive (negative) statements and the image are depicted by blue (orange) color and labeled as relevant (irrelevant) state- ments. Let us define CLIP-score as the similarity score based on CLIP embeddings. From the histogram, it is evident that (i) the CLIP-score between image-text pairs has limited range, e.g., about (0.13, 0.35), and (ii) the distribution of the CLIP- score for relevant and irrelevant statements has considerable overlap which means that such similarity score struggles to adequately distinguish between relevant and