metrics are shown in Table 8. Table 8: LambdaMART final hyperparameters and metrics for v5 and v6. v5 (grid) v6 (random) Learning Rate 0.2 0.2 Max Tree Depth 6 8 Min Child Weight 5 1 Data Sampling Ratio 1.0 0.8 Feature Sampling Ratio 1.0 0.8 Split Loss Threshold 0 0.5 Train NDCG 0.892 0.917 Validation NDCG 0.869 0.867 While v6 achieved higher training NDCG (0.917 vs 0.892), it did not generalize as well to unseen dev and synthetic datasets. The validation NDCG scores were similar (0.869 vs 0.867), but v6’s more aggressive hyperparameters (deeper trees, lower sampling ratios) led to overfitting. Additionally, the simpler parameter configuration of v5 (higher sampling ratios, shallower trees) makes it more robust and interpretable. Therefore, v5 was selected as the final model for all main results. Figure 7 shows the tree depth distribution comparison between v5 and v6. Wenxin Zhou, Ritesh Mehta, and Anthony Miyaguchi Figure 7: Tree depth distribution for LambdaMART v5 and v6 models. DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking B Prompt for Open Domain Query Generation Let’s do a role play. You are now a person who vaguely remembers something called {ToTObject}. You are trying to recall the name by posting a verbose post in an online forum like Reddit describing it. Generate a post of around 200 words about {ToTObject}. Your post must describe a vague memory without revealing its exact name. People on the forum must have a hard time figuring out what you are looking for. The answer should be difficult to find in search engines, so avoid using obvious keywords. I will provide you with some basic information, and you must follow the guidelines to create a post. Information about {ToTObject}: {Psg} Guidelines: MUST FOLLOW: 1. Reflect the imperfect nature