multi-hop reasoning, expressions of uncertainty, and even false memories, which makes it difficult for traditional keyword- based search systems to handle effectively. The primary task of the ToT track is to develop systems that can successfully identify the "known-item" from these elaborate and often imprecise descriptions across various domains. In our paper, we address the ToT challenge using two-stage re- trieval, as shown in Figure 1. In the first stage, we use a hybrid retrieval method that combines LLM-based, sparse, and dense re- trieval results. We explore topic-aware multi-index dense retrieval that leverages semantic document organization to improve search efficiency. For this method, the Wikipedia corpus is partitioned into 24 topical domains, enabling query-specific routing to relevant document subsets. In the second stage, we experiment with an LLM-based reranker and a learned LambdaMART reranker. The learned reranker lever- ages various features such as dense and sparse retrieval scores, page view count, and network-theoretic measures (i.e., PageRank). For data augmentation, we use LLMs to generate 5000 synthetic open-domain ToT queries to train the learned reranker. Our findings reveal that the hybrid retrieval approach signifi- cantly enhances recall compared to individual retrieval methods. The LLM reranker achieves the best overall performance. The Lamb- daMART reranker shows notable improvements over baseline meth- ods in terms of recall. The topic-aware routing approach demon- strates a trade-off between efficiency and effectiveness, achieving competitive recall with a reduced search space. The implementation can be found at https://github.com/dsgt-arc/trec-tot-2025. Figure 1: The two-stage retrieval architecture for ToT search. 2 Methodology 2.1 Baseline Methods For sparse retrieval, we use the PyTerrier BM25 baseline results provided by the ToT track organizers. For dense retrieval, we use the BGE-M3 model [2] to generate query embeddings, and perform dot product similarity search on pre-computed Wikipedia paragraph embeddings from the June 2024 Wikipedia