terms such as "object" and "name. " Second, it asks the LLM to return the query inside a code block, which helps extract the query from the LLM response. The LLM models used are gemini-2.5-flash-lite [4], o4-mini [7] and gpt-4o [6]. Next, we sample approximately 5,000 entities from the corpus. We use Wikipedia article pageview and infobox template informa- tion to guide the sampling process to ensure diversity and popular- ity of the selected entities. We aggregate user pageview counts from October 2022 to Oc- tober 2023. To ensure relevance, we retain only pages that rank in the top 20% by total views. Additionally, we filter out stub and short articles by keeping only those with word counts exceeding the 25th percentile. We also exclude articles with more than 5,000 words to avoid extremely long pages. We only use pages that have infobox template information, which is inspired by He et al. [ 5]. We organized entities into infobox template categories and applied the following sampling strategy for each category: • For categories with more than 1000 entities, select 5 entities. •For categories with 500-1000 entities, select 4 entities. •For categories with 100-500 entities, select 3 entities. •For categories with 10-100 entities, select 1-2 entities. This process selects 5,000 entities, and we use the gemini-2.5- flash-lite model to generate synthetic queries for them. This aug- mented dataset is then split into train, dev, and test sets with 75%, 15%, and 10%, respectively. 2.4 First-Stage Retrieval 2.4.1 LLM Retrieval.We use LLMs as one of the methods for first- stage retrieval. In the prompt, we ask the LLM to return a single or up to 20 Wikipedia article titles that match the tip-of-the-tongue query, with relevance scores from 1-5. We request the LLM to return results in JSON format to make extracting