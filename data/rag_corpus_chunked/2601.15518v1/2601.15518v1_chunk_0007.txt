2.5 Reranking 2.5.1 LLM Reranker.We use an LLM to perform listwise reranking. The implementation is based on the RankLLM library[10], where a sliding window algorithm is used to split a large set of docu- ments into groups so that the context can fit within the LLM’s context window. We tested various context settings, such as title only, first paragraph, first sentence, and full (first 1500 characters), and adjusted the sliding window size and stride accordingly. We also experimented with various models such as Gemma and Gemini. 2.5.2 LambdaMART Reranker.We trained a LambdaMART reranker [13] using the xgboost library [3], and explored multiple sampling strategies and feature sets for training. The final LambdaMART model uses five features: dense retrieval score (from the BGE-M3 model), PyTerrier BM25 score for each document-query pair, nor- malized pageview count and pagerank score for the document, and the query’s word count. For sampling, we adopted a pseudo-relevance approach. Each query’s golden document is assigned a score of 2. Five pseudo- relevant documents are randomly selected from the top-10 results of either dense or sparse retrieval and assigned a score of 1. Ten irrelevant documents are randomly sampled from ranks 10 to 100 in the retrieval results and given a score of 0. This ensures that each query in the training set is associated with one golden document, five pseudo-relevant documents, and ten irrelevant documents, providing a balanced distribution to mitigate class imbalance and reduce overfitting. The training and validation data consist of official train set (143 queries) and the first 200 queries of dev3 set, and the training set of our generated queries (around 4000 queries). Among those, 80% are used for training and 20% for validation. 3 Results 3.1 Synthetic Query Correlation Study To validate that our adapted LLM query generation method pro- duces queries