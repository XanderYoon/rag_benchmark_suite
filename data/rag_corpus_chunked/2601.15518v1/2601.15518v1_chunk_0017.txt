regarding the lower relative difficulty of the NIST-sourced queries. 5 Conclusions This work addresses the TREC Tip-of-the-Tongue task through a comprehensive two-stage retrieval pipeline that strategically com- bines multiple retrieval methods with both learned and LLM-based reranking approaches. Our work includes: (1) building a hybrid first-stage retrieval strat- egy that effectively merges results from LLM, sparse, and dense retrieval methods, (2) developing a topic-aware multi-index dense retrieval approach that improves computational efficiency while maintaining competitive recall through semantic document parti- tioning, and (3) training a learned LambdaMART reranker on both official and synthetic data with features from retrieval scores and document popularity metrics. Our best system achieves substantial improvements over base- line methods by merging hybrid retrieval with strong LLM-based reranking. The results demonstrate that leveraging multiple re- trieval methods is crucial for capturing complementary aspects of relevance. While LLM-based reranking achieves superior perfor- mance, using a learned LambdaMART reranker with a weaker LLM DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking provides a computationally efficient alternative with competitive results. Acknowledgements We thank the Data Science at Georgia Tech Applied Research Com- petition group (DS@GT ARC) for their support. This research was supported in part through research cyberinfrastructure resources and services provided by the Partnership for an Advanced Com- puting Environment (PACE) at the Georgia Institute of Technology, Atlanta, Georgia, USA [8]. References [1] Jaime Arguello, Fernando Diaz, Maik Fr√∂ebe, To Eun Kim, and Bhaskar Mitra. 2025. OVERVIEW OF THE TREC 2025 TIP-OF-THE-TONGUE TRACK.The Thirty-Fourth Text REtrieval Conference (TREC 2025) Proceedings(2025). [2] Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. InFindings of the As- sociation for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association