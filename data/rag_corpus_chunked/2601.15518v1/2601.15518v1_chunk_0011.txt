the baseline of 0.3038). For dense retrieval, Gemma-3-27B-QAT achieves an RR of 0.3393 (120% im- provement over the baseline of 0.1545). Larger models consistently outperform smaller variants. All models with 8B+ parameters show strong performance. 3.5 LambdaMART Reranker We train LambdaMART rerankers using xgboost with both grid search and random search for hyperparameter tuning. The training details can be found in Appendix A. The models are trained with pairwise loss over 100 rounds using NDCG as the evaluation met- ric. We select the grid search variant (v5) as our final model, as it generalizes better to the unseen dev and synthetic datasets than the random search variant (v6). DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking Table 5 presents recall improvements across all datasets on sparse (PyTerrier BM25) and dense (BGE-M3) baseline retrieval. Figure 6 shows the feature importance weight of the v5 model. Table 5: LambdaMART v5 Reranked Recall@10 and Re- call@100. The arrow ( →) indicates improvement from base- line to reranked results. Synthetic refers to the dataset from our generated synthetic queries. Dataset Baseline Recall@10 Recall@100 Train Sparse 0.10→0.15 0.24→0.29 Train Dense 0.08→0.12 0.20→0.29 Dev1 Sparse 0.11→0.14 0.22→0.25 Dev1 Dense 0.10→0.14 0.23→0.27 Dev2 Sparse 0.15→0.15 0.23→0.26 Dev2 Dense 0.11→0.16 0.27→0.27 Dev3 Sparse 0.43→0.49 0.60→0.66 Dev3 Dense 0.21→0.43 0.40→0.58 Synthetic-dev Sparse 0.15→0.21 0.24→0.34 Synthetic-dev Dense 0.12→0.20 0.27→0.38 Figure 6: Feature importance for LambdaMART v5 model. Across all datasets, LambdaMART v5 consistently improves re- call@10 and recall@100 over baseline BM25 and dense retrieval. For example, on the dev3 set, reranking increases recall@100 from 0.60 to 0.66 for sparse and from 0.40 to 0.58 for dense retrieval. Based on the feature importance gain for the LambdaMART v5 model, the dense and sparse retrieval scores are the most influential features (16.39 and 14.93, respectively), while