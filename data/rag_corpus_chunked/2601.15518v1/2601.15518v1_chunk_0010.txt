0.1429 0.4507 0.4789 0.5000 4.4% Dev2 0.1727 0.4545 0.5105 0.5664 11.0% Dev3 0.6355 0.7705 0.6474 0.8302 7.8% Table 3 summarizes recall@1000 across all datasets. The hybrid approach improves over the best individual method on each dataset. Dev3 shows the strongest absolute performance, while Dev2 demon- strates the largest relative improvement at 11%. Figure 5 illustrates the recall comparison at various cutoffs for the dev3 set. These results show that the round-robin merging strategy effec- tively combines the complementary strengths of different retrieval methods, providing notable recall improvements. 3.4 LLM Reranking Results We evaluate LLM-based listwise reranking on the first 100 queries of the dev3 set using full document context with sliding window size 20 and stride 10. Table 4 compares the reranking performance Figure 5: Recall comparison of individual and hybrid re- trieval methods on dev3 set. of various models on sparse (PyTerrier BM25) and dense (BGE-M3) retrieval baselines with recall@1000 of 0.81 and 0.64, respectively. Table 4: LLM Reranking Results on dev3 Set (first 100 queries) Baseline Reranker Model RR NDCG@1000 Sparse Gemma-3-27B-QAT 0.5104 0.5534 Sparse Gemma-3-12B-QAT 0.4832 0.5307 Sparse Mistral-Nemo-Instruct-2407 0.4485 0.5018 Sparse Llama-3.1-8B-Instruct 0.4411 0.4911 Sparse Gemma-3-4B-QAT 0.3098 0.3820 Sparse None (baseline) 0.3038 0.3894 Sparse Gemma-3-1B-QAT 0.0868 0.1759 Dense Gemma-3-27B-QAT 0.3393 0.3774 Dense Gemma-3-12B-QAT 0.3318 0.3717 Dense Gemma-3-12B 0.3227 0.3647 Dense None (baseline) 0.1545 0.2315 Most LLM rerankers outperform the baseline in both retrieval settings. For sparse retrieval, Gemma-3-27B-QAT achieves an RR of 0.5104 (68% improvement over the baseline of 0.3038). For dense retrieval, Gemma-3-27B-QAT achieves an RR of 0.3393 (120% im- provement over the baseline of 0.1545). Larger models consistently outperform smaller variants. All models with 8B+ parameters show strong performance. 3.5 LambdaMART Reranker We train LambdaMART rerankers using xgboost with both grid search and random search for hyperparameter tuning. The training details can be found