brid retrieval results on the test set. The hybrid results contained an average of 1,829 candidates per query. Following reranking, recall@1000 improved from 0.5700 to 0.6109, which shows that the model effectively surfaces relevant documents into the top 1,000. However, the Reciprocal Rank fell significantly from 0.2838 to 0.0601. This indicates that while LambdaMART excels at iden- tifying relevant documents, it often displaces those originally at the top to lower positions. This explains the low NDCG observed for the lambdamart-rerank system. This result is different from what we observed in the training and development sets, where the precision of the result does not decrease. This shows that the model does not generalize well on the test set. However, we can still use LamdbaMART as a high-recall filter in this pipeline and the subsequent application of LLM reranking helps to recover the precision of the final result. 4.4 Performance Analysis on Data Source The test dataset comprises queries from three sources [1]: (1) human- elicited queries from NIST assessors; (2) synthetic queries generated from GPT-4o and Llama-3.1-8B-Instruct; and (3) the MS-ToT Dataset. The performance of our best system (gmn-rerank-500) varies signif- icantly across these subsets. The NIST assessor queries achieved a recall@1000 of 0.89 and NDCG@1000 of 0.65. In contrast, the MS-ToT and synthetic datasets proved more challenging, with re- call@1000 reaching 0.59 and 0.57, and NDCG@1000 at 0.32 and 0.37, respectively. These results align with the track coordinatorsâ€™ obser- vations regarding the lower relative difficulty of the NIST-sourced queries. 5 Conclusions This work addresses the TREC Tip-of-the-Tongue task through a comprehensive two-stage retrieval pipeline that strategically com- bines multiple retrieval methods with both learned and LLM-based reranking approaches. Our work includes: (1) building a hybrid first-stage retrieval strat- egy that effectively merges results from LLM, sparse, and dense retrieval methods,