re- call@10 and recall@100 over baseline BM25 and dense retrieval. For example, on the dev3 set, reranking increases recall@100 from 0.60 to 0.66 for sparse and from 0.40 to 0.58 for dense retrieval. Based on the feature importance gain for the LambdaMART v5 model, the dense and sparse retrieval scores are the most influential features (16.39 and 14.93, respectively), while pageview count (5.80) moderately contributes to reranking. This indicates that the learned reranker primarily relies on quality signals from the initial retrieval stage rather than document popularity. 3.6 End-to-End System Results We submitted 7 systems to the TREC Tip-of-the-Tongue 2025 chal- lenge. Table 6 summarizes their performance on the official test set: • bge-m3:Dense retrieval baseline using BGE-M3 embed- dings with cosine similarity search on Wikipedia passages from the June 2024 dump. • top_model_dense:First-stage dense retrieval enhanced with topic-aware routing, partitioning the corpus into 24 topic-specific FAISS indexes with query classification and multi-variant query expansion. • gemini-retrieval:LLM retrieval using Gemini-2.5-flash to generate up to 20 Wikipedia page titles per query with relevance scores (1-5), including title matching with redirect name resolution. • lambdamart-rerank:Hybrid first-stage retrieval (combin- ing LLM, Pyterrier BM25, and BGE-M3 results) reranked with a trained LambdaMART model using dense/sparse scores, pageview counts, and PageRank as features. • gmn-rerank-500:Hybrid first-stage retrieval (20 LLM re- sults + top-500 BM25 + top-500 BGE-M3) reranked with Gemini-2.5-flash listwise reranking on all 1020 documents per query. • gm27q-LMART-1000:Two-stage approach applying Lamb- daMART reranking to hybrid retrieval results (top-1000), followed by Gemma-27B-quantized reranking on top-500 documents. • gm27q-comb-500:Limited hybrid retrieval (LLM results + top-200 BM25 + top-200 BGE-M3) reranked with Gemma- 27B-quantized model. Table 6: End-to-End System Results on Test Set System R@10 R@1000 NDCG@1000 gmn-rerank-500 0.4341 0.6559 0.4106 gm27q-LMART-1000 0.3617 0.6109 0.3339 gm27q-comb-500 0.4196 0.5884 0.3848 gemini-retrieval 0.3457 0.3505 0.2962 top_model_dense 0.1720 0.5096