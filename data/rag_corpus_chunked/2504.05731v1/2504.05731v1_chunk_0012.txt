the masked documents and ğœ‚ğ‘š is a hyper-parameter that controls the mask ratio. The masked documents are replaced with a special token [mask]. The history after masking is as follows: H mask ğ‘¢ = [ Ë†ğ‘‘1, Ë†ğ‘‘2, . . . , Ë†ğ‘‘ğ‘ ], Ë†ğ‘‘ğ‘– = ( ğ‘‘ğ‘–, ğ‘– âˆ‰ Imask, [mask], ğ‘– âˆˆ Imask. â€¦ ğ‘‘1 ğ‘‘ğ‘ ğ’œâ€² â€¦ ğ‘‘1 â€² ğ‘‘ğ‘ â€² Emb Trm ğ’œâ€²â€² â€¦ ğ‘‘1 â€²â€² ğ‘‘ğ‘ â€²â€² Emb Trm Encoderğ‘¢ Contrastive Learning Data Augmentation Figure 3: Contrastive learning for user embedding training. Document Reorder. We randomly select a sub-sequence [ğ‘‘ğ‘Ÿ , ğ‘‘ğ‘Ÿ +1, . . . , ğ‘‘ğ‘Ÿ +ğ¿ğ‘Ÿ âˆ’1] of length ğ¿ğ‘Ÿ = âŒŠğœ‚ğ‘Ÿ ğ‘ âŒ‹ from Hğ‘¢, where ğœ‚ğ‘Ÿ is a hyper-parameter controlling the reorder ratio, and then randomly shuffle the order of the documents within the sub-sequence to obtain [ Ë†ğ‘‘ğ‘Ÿ , Ë†ğ‘‘ğ‘Ÿ +1, . . . , Ë†ğ‘‘ğ‘Ÿ +ğ¿ğ‘Ÿ âˆ’1]. The history after reordering is as follows: H reorder ğ‘¢ = [ğ‘‘1, ğ‘‘2, . . . , Ë†ğ‘‘ğ‘Ÿ , . . . , Ë†ğ‘‘ğ‘Ÿ +ğ¿ğ‘Ÿ âˆ’1, . . . , ğ‘‘ğ‘ ]. 4.1.3 Contrastive Loss. Each time, we randomly select two data augmentation methods Aâ€² and Aâ€²â€² to generate two different views of Hğ‘¢, denoted as H â€²ğ‘¢ and H â€²â€²ğ‘¢ . Then, using the encoder described in Section 4.1.1, we obtain the user embeddings eâ€²ğ‘¢ and eâ€²â€²ğ‘¢ cor- responding to the different views. Since eâ€²ğ‘¢ and eâ€²â€²ğ‘¢ are obtained through data augmentation of Hğ‘¢, they are more similar to each other. Therefore, we treat them as positive samples for each other and use the views generated from the augmented histories of other users in the same batch as negative samples. We then perform contrastive learning using the InfoNCE [28] loss as follows: LCL = âˆ’ " log exp(cos(eâ€²ğ‘¢, eâ€²â€²ğ‘¢ )/ğœ1)Ã ğ‘¢