augmentation of Hğ‘¢, they are more similar to each other. Therefore, we treat them as positive samples for each other and use the views generated from the augmented histories of other users in the same batch as negative samples. We then perform contrastive learning using the InfoNCE [28] loss as follows: LCL = âˆ’ " log exp(cos(eâ€²ğ‘¢, eâ€²â€²ğ‘¢ )/ğœ1)Ã ğ‘¢ âˆ’ âˆˆ Uneg exp(cos(eâ€²ğ‘¢, eâ€²â€²ğ‘¢ âˆ’ )/ğœ1) + log exp(cos(eâ€²ğ‘¢, eâ€²â€²ğ‘¢ )/ğœ1)Ã ğ‘¢ âˆ’ âˆˆ Uneg exp(cos(eâ€²ğ‘¢ âˆ’ , eâ€²â€²ğ‘¢ )/ğœ1) # , (2) where ğœ1 is the temperature coefficient, Uneg are the set of ran- domly sampled in-batch negative samples, and cos(Â·) denotes the cosine similarity. 4.1.4 Top- ğ‘š User Retrieval. After training with contrastive learn- ing, we can use the encoder from Section 4.1.1 to obtain the user embedding eğ‘¢. We then calculate the cosine similarity between each pair of user embeddings and retrieve the top-ğ‘š most similar users Uretrieved = {ğ‘¢1, ğ‘¢2, . . . , ğ‘¢ğ‘š } for user ğ‘¢. Subsequently, the histories of these ğ‘š users will be used for further document retrieval. 4.2 Document Retrieval After retrieving the top-ğ‘š users, we design a personalized retriever to retrieve the top-ğ‘˜ documents from each userâ€™s history, result- ing in a total of ğ‘š Ã— ğ‘˜ candidate documents Dretrieved = {ğ‘‘ğ‘–,ğ‘— |ğ‘– âˆˆ {1, . . . , ğ‘š}, ğ‘— âˆˆ { 1, . . . , ğ‘˜}}. This section introduces how the re- triever is designed and how itâ€™s trained to retrieve documents that better align with the requirements of personalized LLM generation. 4.2.1 Retriever. First, we use a pre-trained dense retrieval model (such as BGE retriever [ 45]) to compute the semantic relevance Retrieval Augmented Generation with Collaborative Filtering for Personalized Text Generation SIGIR â€™25, July 13â€“18, 2025, Padua, Italy. â€¦ ğ‘‘11 ğ‘‘1ğ‘˜ â€¦ ğ‘‘ğ‘š1