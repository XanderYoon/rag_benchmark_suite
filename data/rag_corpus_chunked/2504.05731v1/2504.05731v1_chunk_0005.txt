needs [25, 35]. To address the above challenges, this paper proposes a method named CFRAG which adapts Collaborative Filtering to personal- ized Retrieval Augmented Generation. Firstly, to address Challenge 1, since there are no explicit user similarity labels, we use contrastive learning [15, 44] to train user embeddings for retrieving similar users to introduce collaborative information. Specifically, we apply different data augmentation methods to the user‚Äôs history to obtain different views, and then treat different views of the same user‚Äôs history as positive samples for each other. Then we use contrastive learning on different views to train the user embeddings. Secondly, for Challenge 2, we designed a personalized retriever and reranker to retrieve the top-ùëò documents from the histories of the retrieved users. In both retrieval and reranking, in addition to the semantic relevance between the query and documents, we also considered the user‚Äôs preferences for different documents to enable personal- ized retrieval. Additionally, we further fine-tune the retriever and reranker based on the feedback from the LLM to ensure that the retrieved documents better support the personalized LLM genera- tion. Finally, the top-ùëò documents are concatenated with the user‚Äôs input query to form a prompt, which is then fed into the LLM for personalized generation. The major contributions of the paper are summarized as follows: ‚Ä¢ We analyzed the necessity of introducing collaborative filtering into RAG for LLM personalization and identified the challenges: how to introduce collaborative information and how to retrieve documents that support personalized LLM generation. ‚Ä¢ We proposed a method called CFRAG, which uses contrastive learning to train user embeddings for retrieving similar users and incorporating collaborative information. It leverages LLM feedback to train the personalized retriever and reranker, enabling them to retrieve documents that support personalized LLM generation. ‚Ä¢ Experimental results on the Language Model