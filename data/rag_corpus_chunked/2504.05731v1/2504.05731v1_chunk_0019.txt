experiments on the Language Model Personalization (LaMP) [32] benchmark, which consists of seven personalized text generation tasks. We excluded LaMP-6 because its data is not publicly available. The remaining tasks include: LaMP-1 (Personalized Citation Identification); LaMP-2 (Person- alized Movie Tagging); LaMP-3 (Personalized Product Rating); LaMP-4 (Personalized News Headline Generation); LaMP-5 (Per- sonalized Scholarly Title Generation);LaMP-7 (Personalized Tweet Paraphrasing). We used the time-based split provided by LaMP to divide the data into training, validation, and test sets. The statistics of these datasets are shown in Table 1. 5.1.2 Evaluation Metrics. Following previous works [31, 32], we evaluate Accuracy and F-1 score for LaMP-1 and LaMP-2, mean absolute error (MAE) and root mean squared error (RMSE) for LaMP- 3, ROUGE-1 and ROUGE-L [24] for LaMP-4, LaMP-5 and LaMP-7. 1https://github.com/TengShi-RUC/CFRAG 5.1.3 Baselines. In this work, we compare CFRAG with the follow- ing methods. No Personalization: We directly input the userâ€™s query into the LLM without retrieving from user history, using this as the non-personalized baseline. We refer to this method as Zero Shot. Personalized Baselines: We compared CFRAG with methods that personalize by retrieving from user history using different retrieval models, including: (1) Random selects ğ‘˜ items randomly from the userâ€™s history; (2)Recency selects the most recentğ‘˜ items from the userâ€™s history; (3)BM25 [30] retrieves top-ğ‘˜ items from the userâ€™s history using BM25; (4)BGE [45] retrieves top-ğ‘˜ items from the userâ€™s history using BGE retriever; (5)ROPG [31] optimizes the dense retrieval model based on the results generated by the LLM. 5.1.4 Implementation Details. We conducted experiments on two LLMs: Llama3-8B-Instruct [1] and Qwen2-7B-Instruct [47]. In this paper, we do not fine-tune the LLM because fine-tuning is costly and could cause the LLM to retain user information, potentially compromising user privacy. To ensure a fair comparison, we use greedy search for text generation. The