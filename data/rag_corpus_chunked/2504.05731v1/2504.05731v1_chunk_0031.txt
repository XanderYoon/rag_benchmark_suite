introducing col- laborative filtering, but it also indicates that excessive introduction can lead to decreased effectiveness. 5.4.5 Impact of the Number of Retrieved Documents. We also ana- lyzed the impact of the number of retrieved documents, ùëò, on the results, as shown in Figure 9. It can be observed that as the number of retrieved documents increases, performance improves, indicating the importance of retrieving user history to reflect user preferences for enhancing LLM-generated results. Since more documents lead to longer prompts and slower LLM generation, we chose ùëò = 5 for our experiments. 6 Conclusion In this paper, we propose CFRAG, which adapts collaborative fil- tering into RAG to personalize LLMs. To introduce collaborative information without explicit user labels and retrieve documents that support personalized LLM generation, we first train user em- beddings through contrastive learning to retrieve similar users. Then, we design the personalized retriever and reranker that con- siders user preferences during retrieval and fine-tune them using LLM feedback. The results on the Language Model Personalization (LaMP) benchmark validate the effectiveness of CFRAG. The ex- perimental analysis also confirms the effectiveness of each module within CFRAG. A Appendix: Prompts We provide detailed formats for the inputs, outputs, and user histo- ries for the LLM across different datasets, as shown in Table 4. SIGIR ‚Äô25, July 13‚Äì18, 2025, Padua, Italy. Teng Shi et al. References [1] AI@Meta. 2024. Llama 3 Model Card. (2024). https://github.com/meta-llama/ llama3/blob/main/MODEL_CARD.md [2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. [n. d.]. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations . [3] Sebastian Borgeaud, Arthur Mensch, et al. 2022. Improving language models by retrieving from trillions of tokens. InInternational conference on machine learning. PMLR, 2206‚Äì2240. [4] Jin Chen, Zheng Liu, et al. 2024.