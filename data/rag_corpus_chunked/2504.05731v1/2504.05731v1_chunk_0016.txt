calculate the distribution of these candidate documents as follows: ğ‘LLM (ğ‘‘ğ‘–,ğ‘— |ğ‘, ğ‘¦) = exp(eval(ğ‘¦, ğ‘‚ğ‘,ğ‘‘ğ‘–,ğ‘— )) Ãğ‘š ğ‘–=1 Ãğ‘˜ ğ‘—=1 exp(eval(ğ‘¦, ğ‘‚ğ‘,ğ‘‘ğ‘–,ğ‘— )) , (6) where eval(Â·) measures the difference between the target output ğ‘¦ and the LLMâ€™s output, using metrics such as ROUGE [24] score. A larger value returned by eval(Â·) indicates a better-generated result. Similarly, we can also calculate the score distribution of the candidate documents by the retrieval model based on ğ‘†retriever ğ‘¢,ğ‘,ğ‘‘ in Eq. (5): ğ‘retriever(ğ‘‘ğ‘–,ğ‘— |ğ‘, ğ‘¢) = exp(ğ‘†retriever ğ‘¢,ğ‘,ğ‘‘ğ‘–,ğ‘— ) Ãğ‘š ğ‘–=1 Ãğ‘˜ ğ‘—=1 exp(ğ‘†retriever ğ‘¢,ğ‘,ğ‘‘ğ‘–,ğ‘— ) . (7) We aim for the retrieval model to retrieve documents that lead to better LLM-generated results, which means making the distribution ğ‘retriever(ğ‘‘ |ğ‘, ğ‘¢) in Eq. (7) closer to the distribution ğ‘LLM (ğ‘‘ |ğ‘, ğ‘¦) in Eq (6). Therefore, we compute the KL divergence between the two distributions as the loss to optimize the retriever: Lretriever = KL(ğ‘retriever(ğ‘‘ |ğ‘, ğ‘¢) || ğ‘LLM (ğ‘‘ |ğ‘, ğ‘¦)) . (8) 4.3 Document Rerank After retrieving Dretrieved through the retriever, in this section, we further refine the results by reranking Dretrieved to obtain the final top-ğ‘˜ ranked results Dreranked = {ğ‘‘ğ‘– |ğ‘– âˆˆ { 1, . . . , ğ‘˜}}. 4.3.1 Reranker. We use a pre-trained cross-encoder (such as the BGE reranker [45]) to encode the query and document, obtaining the hidden state corresponding to the [CLS] token from the last layer: hğ‘,ğ‘‘ = CrossEncoder(ğ‘, ğ‘‘), (9) where hğ‘,ğ‘‘ âˆˆ Rğ‘‘. Similarly, when reranking, in addition to consid- ering the semantic relevance between query and document, we also take into account the userâ€™s personalized preferences. However, since the cross-encoder does not encode documents separately, it cannot compute the cosine similarity between users and documents as shown in Eq. (4) to express the user preference score. Therefore, we directly concatenate