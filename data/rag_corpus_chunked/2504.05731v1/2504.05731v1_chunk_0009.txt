. . , ğ‘‘ğ‘ ] which includes all her historical documents, where ğ‘ is the number of documents in the history. The personalized text generation dataset is D = {(ğ‘¢, ğ‘, ğ‘¦)ğ‘– } | D | ğ‘–=1 . For each instance, ğ‘ is the query input by the user ğ‘¢ to the LLM, and ğ‘¦ is the target output. Our goal is first to introduce collaborative information by retrieving the top-ğ‘š most similar users for user ğ‘¢: Uretrieved = {ğ‘¢1, ğ‘¢2, . . . , ğ‘¢ğ‘š }. Then, we use a retriever to retrieve the top-ğ‘˜ documents from each of the ğ‘š usersâ€™ histories, resulting in a total ofğ‘š Ã— ğ‘˜ documents. Dretrieved = {ğ‘‘ğ‘–,ğ‘— |ğ‘– âˆˆ { 1, . . . , ğ‘š}, ğ‘— âˆˆ { 1, . . . , ğ‘˜}}. Finally, we use a reranker to rerank these ğ‘š Ã— ğ‘˜ documents and obtain the final top-ğ‘˜ documents: Dreranked = {ğ‘‘ğ‘– |ğ‘– âˆˆ { 1, . . . , ğ‘˜}}. These top-ğ‘˜ documents will be concatenated with the userâ€™s query ğ‘ as a prompt and input into the LLM, enabling it to generate a response that aligns with the target output ğ‘¦. This paper primarily focuses on how to retrieve Uretrieved to introduce collaborative information, and how to train the retriever and reranker so that they can effectively retrieve documents that support the personalized LLM generation. 4 Our Approach This section introduces our method CFRAG. CFRAGâ€™s overall archi- tecture is shown in Figure 2. As mentioned in Section 1, to address SIGIR â€™25, July 13â€“18, 2025, Padua, Italy. Teng Shi et al. Challenge 1, i.e., how to introduce collaborative information, we first train user embeddings using contrastive learning to retrieve the top-ğ‘š most similar users (see Section 4.1). For Challenge 2, which involves retrieving documents that support