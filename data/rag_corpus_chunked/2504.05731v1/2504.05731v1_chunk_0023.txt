as the user embedding. â€œw/oâ€ indicates the corresponding module in CFRAG is removed. Variants LaMP-1 LaMP-2 LaMP-3 LaMP-4 LaMP-5 LaMP-7 # Model Accuracy â†‘ F1â†‘ Accuracyâ†‘ F1â†‘ MAEâ†“ RMSEâ†“ ROUGE-1â†‘ ROUGE-Lâ†‘ ROUGE-1â†‘ ROUGE-Lâ†‘ ROUGE-1â†‘ ROUGE-Lâ†‘ (0) CFRAG 0.6533 0.3267 0.5340 0.0356 0.2812 0.5997 0.1957 0.1745 0.4810 0.4153 0.3752 0.3055 (1) w/o User Retrieval 0.6400 0.3200 0.4936 0.0329 0.3444 0.6925 0.1914 0.1689 0.4642 0.3963 0.3566 0.2903 (2) User Retrieval (MEAN) 0.6420 0.3210 0.5064 0.0338 0.3412 0.6867 0.1847 0.1639 0.4779 0.4113 0.3722 0.3022 (3) w/o Retriever Tuning 0.6453 0.3227 0.4979 0.0332 0.2852 0.6070 0.1916 0.1704 0.4742 0.4048 0.3599 0.2940 (4) w/oğ‘†retrieverğ‘¢,ğ‘‘ in Eq. (5) 0.6333 0.3167 0.5113 0.0341 0.3324 0.6861 0.1895 0.1696 0.4750 0.4088 0.3732 0.3039 (5) w/o Reranker Tuning 0.6307 0.3153 0.4695 0.0313 0.3696 0.7392 0.1766 0.1550 0.4714 0.4068 0.3432 0.2775 (6) w/oeğ‘¢ in Eq. (10) 0.6313 0.3157 0.4993 0.0333 0.3420 0.6925 0.1887 0.1672 0.4772 0.4123 0.3731 0.3030 hand, fine-tuning the retriever based on LLM feedback to ensure it can retrieve the documents that meet the personalized generation needs of LLM is crucial. 5.3 Ablation Study We conducted an ablation study to investigate the effectiveness of different modules in CFRAG, as shown in Table 3. CFRAG consists of three modules: User Retrieval, Document Retrieval, and Document Rerank. We removed different modules from CFRAG one by one to verify the effectiveness of each module. 5.3.1 User Retrieval. First, we validated the effectiveness of intro- ducing collaborative information by retrieving similar users, as shown in row (1) of Table 3. It can be seen that without retrieving similar users and only retrieving from the current userâ€™s history, the performance is worse than that of CFRAG, highlighting the importance of collaborative information. We also validated the effectiveness of training user embeddings using contrastive learning. For comparison, we directly averaged the document embeddings from the