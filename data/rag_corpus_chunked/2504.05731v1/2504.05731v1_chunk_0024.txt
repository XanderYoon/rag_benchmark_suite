shown in row (1) of Table 3. It can be seen that without retrieving similar users and only retrieving from the current userâ€™s history, the performance is worse than that of CFRAG, highlighting the importance of collaborative information. We also validated the effectiveness of training user embeddings using contrastive learning. For comparison, we directly averaged the document embeddings from the userâ€™s history to create user embeddings for retrieval, as shown in row (2) of Table 3. It can be seen that CFRAG, which uses user embeddings trained with con- trastive learning, achieves better results. This is because contrastive learning constructs user similarity labels through data augmenta- tion and uses the InfoNCE loss to help the embeddings learn which users are similar. In contrast, using mean pooling directly cannot capture user similarity. Accuracy F10.600 0.616 0.632 0.648 0.664 0.680Accuracy random top-(m-2m) top-m 0.300 0.306 0.312 0.318 0.324 0.330 F1 (a) LaMP-1 ROUGE-1 ROUGE-L0.450 0.462 0.474 0.486 0.498 0.510ROUGE-1 random top-(m-2m) top-m 0.390 0.396 0.402 0.408 0.414 0.420 ROUGE-L (b) LaMP-5 Figure 5: Results of using different methods to select users for introducing collaborative information. â€œrandomâ€ indicates randomly selectingğ‘š users; â€œtop-(ğ‘š-2ğ‘š)â€ represents selecting users whose similarity to the current user ranks between ğ‘š and 2ğ‘š; â€œtop-ğ‘šâ€ indicates selecting the most similar ğ‘š users. 5.3.2 Document Retrieval. We also validated the effectiveness of the personalized retriever we designed, as shown in Table 3, rows (3) and (4). First, in row (3), we can see that without fine-tuning based on LLM feedback, using a pre-trained dense retrieval model leads to worse performance. This indicates that retrieval cannot be based solely on semantic relevance, ensuring that the retrieved documents support personalized LLM generation is crucial. Addi- tionally, we analyzed the impact of removing ğ‘†retriever ğ‘¢,ğ‘‘ from Eq. (4) and only using ğ‘†retriever ğ‘,ğ‘‘ from Eq.