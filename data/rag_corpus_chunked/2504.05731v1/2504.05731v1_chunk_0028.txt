we fine-tune the retriever and reranker using feedback from the con- tent generated by the LLM, enabling them to retrieve documents that better meet personalized LLM generation needs. To validate its effectiveness, we compared the results with those using retrievers and rerankers without LLM feedback fine-tuning, as well as using BM25 as the retriever and reranker, as shown in Figure 6. It can be observed that CFRAG performs the best, highlighting the impor- tance of fine-tuning with LLM feedback rather than relying solely on semantic relevance. 5.4.3 Impact of the Number of Documents from the Current User. To further validate that CFRAG enhances personalization by incor- porating collaborative information, we observed the impact of the number of documents from the current user in the final top-ğ‘˜ doc- uments on the results, as shown in Figure 7. We varied the number of documents retrieved from the current userâ€™s history in the top-ğ‘˜ documents from 0 to 5, with the remaining documents retrieved from similar usersâ€™ histories. The results indicate that retrieving only from the current userâ€™s history leads to poor performance, while appropriately retrieving documents from similar usersâ€™ histo- ries significantly improves the results. This verifies the importance of incorporating collaborative information. Retrieval Augmented Generation with Collaborative Filtering for Personalized Text Generation SIGIR â€™25, July 13â€“18, 2025, Padua, Italy. Table 4: The format of input, output, and user history for different datasets in the LaMP [ 32] benchmark. In the input, {historyğ‘– } will be replaced by the retrieved ğ‘–-th history, and each history is represented as shown in the â€œUser Historyâ€ column. The other italicized text in the input is replaced with the userâ€™s input. For text generation tasks, to ensure that the LLM does not generate irrelevant information, we instruct the LLM in the input to generate in JSON