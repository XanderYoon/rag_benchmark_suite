on the results generated by the LLM. 5.1.4 Implementation Details. We conducted experiments on two LLMs: Llama3-8B-Instruct [1] and Qwen2-7B-Instruct [47]. In this paper, we do not fine-tune the LLM because fine-tuning is costly and could cause the LLM to retain user information, potentially compromising user privacy. To ensure a fair comparison, we use greedy search for text generation. The dense retrieval model used in all methods is bge-base-en-v1.52 [45]. The cross-encoder used for reranker in Section 4.3.1 is bge-reranker-base3 [45]. All hyper- parameters for the baselines are searched according to the set- tings in the original papers. The embedding dimension ğ‘‘ is set to 768. The number of retrieved documents ğ‘˜ is set to 5, and the number of retrieved users ğ‘š is tuned among {2, 3, 4, 5, 6}. The Trm(Â·) encoder in Eq. (1) has 1 layer and 2 heads. The hyper- parameters ğ¿ğ‘, ğ¿ğ‘š, and ğ¿ğ‘Ÿ used for data augmentation in Sec- tion 4.1.2 are set to 0.7, 0.3, and 0.3, respectively. The temperature parameters ğœ1 in Eq. (2) is tuned among {0.01, 0.1, 1}. The weight ğ›¼ in Eq. (5) is tuned among [0.01, 1.0]. The learning rate is tuned among {1ğ‘’-3, 1ğ‘’-4, 1ğ‘’-5}. Adam [18] is used to conduct the optimiza- tion. The data input and output formats are provided in Appendix A. 5.2 Experimental Results Experimental results are shown in Table 2. From the results, we can find that: â€¢ Firstly, compared to existing methods, CFRAG achieved the best results across six datasets in the LaMP benchmark. This demon- strates the effectiveness of introducing collaborative information between users into RAG and using LLM feedback to tune the re- triever and reranker to ensure that they can retrieve the documents that support the personalized LLM generation. â€¢ Secondly, we can observe that even randomly