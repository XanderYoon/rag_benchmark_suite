âˆˆ Rğ‘‘. Similarly, when reranking, in addition to consid- ering the semantic relevance between query and document, we also take into account the userâ€™s personalized preferences. However, since the cross-encoder does not encode documents separately, it cannot compute the cosine similarity between users and documents as shown in Eq. (4) to express the user preference score. Therefore, we directly concatenate the user embeddings to the output of the cross-encoder to account for the influence of user preferences. The overall score used for reranking is calculated as follows: ğ‘†reranker ğ‘¢,ğ‘,ğ‘‘ = MLP3 (CONCAT(hğ‘,ğ‘‘, MLP2 (eğ‘¢ ))) , (10) where MLP2 : Rğ‘‘ â†’ Rğ‘‘ and MLP3 : R2ğ‘‘ â†’ R are two multi-layer perceptions. CONCAT(Â·) denotes the concatenation operation. 4.3.2 Training. Similar to the retrieverâ€™s training in Section 4.2.2, we also want the reranker to assign higher scores to the documents that lead to better LLM-generated results. Therefore, we train the reranker using a similar approach. We use the trained retrieval model from Section 4.2.2 to retrieve top-ğ‘˜ documents from the history of each of the ğ‘š users, result- ing in a total of ğ‘š Ã— ğ‘˜ candidate documents. These documents are concatenated with the query ğ‘ and used as prompts for the LLM, producing ğ‘š Ã— ğ‘˜ outputs. Similar to Eq.(6), we can obtain the distribution ğ‘LLM (ğ‘‘ |ğ‘, ğ‘¦) of these candidate documents. Based on SIGIR â€™25, July 13â€“18, 2025, Padua, Italy. Teng Shi et al. Table 1: Statistics of the datasets used in this paper. Dataset LaMP-1 LaMP-2 LaMP-3 LaMP-4 LaMP-5 LaMP-7 #Users 6,542 929 20,000 1,643 14,682 13,437 #Train 6,542 5,073 20,000 12,500 14,682 13,437 #Dev 1,500 1,410 2,500 1,500 1,500 1,498 #Test 1,500 1,557 2,500 1,800 1,500 1,500 ğ‘†reranker ğ‘¢,ğ‘,ğ‘‘ in Eq. (10), we can also get the score distribution of the candidate documents by the