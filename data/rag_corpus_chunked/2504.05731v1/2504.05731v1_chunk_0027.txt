approach, we compared it with randomly selecting ğ‘š users and selecting users from top-ğ‘š to 2ğ‘š, as shown in Figure 5. First, we can see that randomly selecting 1 2 3 4 5 6 top-m 0.635 0.638 0.641 0.644 0.647 0.650Accuracy Accuracy 0.315 0.317 0.319 0.321 0.323 0.325 F1 F1 (a) LaMP-1 1 2 3 4 5 6 top-m 0.460 0.466 0.472 0.478 0.484 0.490ROUGE-1 ROUGE-1 0.395 0.400 0.405 0.410 0.415 0.420 ROUGE-L ROUGE-L (b) LaMP-5 Figure 8: Performance under different numbers of retrieved users. The performance is the worst since no collaborative information is introduced when ğ‘š = 1. 1 2 3 4 5 top-k 0.590 0.604 0.618 0.632 0.646 0.660Accuracy Accuracy 0.295 0.303 0.311 0.319 0.327 0.335 F1 F1 (a) LaMP-1 1 2 3 4 5 top-k 0.470 0.473 0.476 0.479 0.482 0.485ROUGE-1 ROUGE-1 0.400 0.404 0.408 0.412 0.416 0.420 ROUGE-L ROUGE-L (b) LaMP-5 Figure 9: Performance under different numbers of retrieved documents per user. users yields the worst performance, indicating that collaborative information cannot be introduced indiscriminately. Secondly, the results show that retrieving users from the range of top-ğ‘š to 2ğ‘š performs worse than using the top-ğ‘š users, suggesting that infor- mation from users who are more similar to the current user ğ‘¢ is more important. These highlight the importance of retrieving the most similar top-ğ‘š users 5.4.2 Effectiveness of Document Retrieval using LLM Feedback (Chal- lenge 2). As mentioned in Section 1, to address Challenge 2, we fine-tune the retriever and reranker using feedback from the con- tent generated by the LLM, enabling them to retrieve documents that better meet personalized LLM generation needs. To validate its effectiveness, we compared the results with those using retrievers and rerankers without LLM feedback fine-tuning, as well as using BM25 as the retriever and reranker, as shown in Figure