and has been proven effective. It assumes that users who have interacted with similar items share similar preferences, and recommending items from similar users to the current user can meet their needs. Some works [11, 46] learn the collaborative information between users and items through matrix factorization [19], while others [10, 40] further explore higher-order collaborative information between users and items using graph neural networks. The application of collaborative filtering in LLM personalization remains under-explored. Retrieval Augmented Generation. Retrieval Augmented Gen- eration [7, 8] introduces external knowledge through document retrieval, alleviating issues such as LLM hallucinations [53], and en- hancing LLMsâ€™ capabilities in knowledge-intensive tasks [17] such as open-domain question answering [14, 20]. Some works [3, 13] encode retrieved documents using separate encoders, and then fuse the results with the language model using cross-attention. A more common approach is to directly include the retrieved documents in the prompt of the LLM [ 2, 9, 20, 25, 35]. In recent years, this in-context RAG framework has also been applied to LLM person- alization, which is personalized by retrieving documents from the userâ€™s history [31, 32, 57]. This paper introduces collaborative filter- ing by retrieving similar usersâ€™ histories for better personalization. 3 Problem Formulation Let U = {ğ‘¢1, ğ‘¢2, . . . , ğ‘¢ğ‘€ } denotes the set of all users, whereğ‘€ is the number of users. Each user ğ‘¢ âˆˆ U has a chronologically ordered history Hğ‘¢ = [ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘ ] which includes all her historical documents, where ğ‘ is the number of documents in the history. The personalized text generation dataset is D = {(ğ‘¢, ğ‘, ğ‘¦)ğ‘– } | D | ğ‘–=1 . For each instance, ğ‘ is the query input by the user ğ‘¢ to the LLM, and ğ‘¦ is the target output. Our