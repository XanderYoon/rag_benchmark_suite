of the datasets used in this paper. Dataset LaMP-1 LaMP-2 LaMP-3 LaMP-4 LaMP-5 LaMP-7 #Users 6,542 929 20,000 1,643 14,682 13,437 #Train 6,542 5,073 20,000 12,500 14,682 13,437 #Dev 1,500 1,410 2,500 1,500 1,500 1,498 #Test 1,500 1,557 2,500 1,800 1,500 1,500 ğ‘†reranker ğ‘¢,ğ‘,ğ‘‘ in Eq. (10), we can also get the score distribution of the candidate documents by the reranker: ğ‘reranker(ğ‘‘ğ‘–,ğ‘— |ğ‘, ğ‘¢) = exp(ğ‘†reranker ğ‘¢,ğ‘,ğ‘‘ğ‘–,ğ‘— ) Ãğ‘š ğ‘–=1 Ãğ‘˜ ğ‘—=1 exp(ğ‘†reranker ğ‘¢,ğ‘,ğ‘‘ğ‘–,ğ‘— ) . (11) We compute the KL divergence between distributionsğ‘reranker(ğ‘‘ |ğ‘, ğ‘¢) and ğ‘LLM (ğ‘‘ |ğ‘, ğ‘¦) as the loss to optimize the reranker: Lreranker = KL(ğ‘reranker(ğ‘‘ |ğ‘, ğ‘¢) || ğ‘LLM (ğ‘‘ |ğ‘, ğ‘¦)) . (12) The loss allows the reranker to assign higher scores to documents that enable better personalized generation by the LLM. 4.4 Discussion Computational Efficiency.CFRAG comprises three modules. The User Encoder is a lightweight, single-layer Transformer with inputs derived from a frozen BGE embedding (dimension 768), resulting in minimal parameter overhead. The retriever and reranker are com- parable in size to BERT (approximately 100M parameters). Overall, the training cost is low due to the modest parameter size. During inference, user and document embeddings can be precomputed, requiring only similarity calculations for retrieval, ensuring min- imal computational cost. This efficiency enables our method to generalize quickly to new datasets. 5 Experiments We conducted experiments to evaluate the performance of CFRAG. The source code is available. 1 5.1 Experimental Setup 5.1.1 Dataset. We conducted experiments on the Language Model Personalization (LaMP) [32] benchmark, which consists of seven personalized text generation tasks. We excluded LaMP-6 because its data is not publicly available. The remaining tasks include: LaMP-1 (Personalized Citation Identification); LaMP-2 (Person- alized Movie Tagging); LaMP-3 (Personalized Product Rating); LaMP-4 (Personalized News Headline Generation); LaMP-5 (Per- sonalized Scholarly Title Generation);LaMP-7 (Personalized Tweet Paraphrasing). We used