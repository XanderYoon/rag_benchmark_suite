is computed. eğ‘¢ is the embedding obtained in Section 4.1.1. The total score for retrieval is computed as follows: ğ‘†retriever ğ‘¢,ğ‘,ğ‘‘ = (1 âˆ’ ğ›¼)ğ‘†retriever ğ‘,ğ‘‘ + ğ›¼ğ‘† retriever ğ‘¢,ğ‘‘ , (5) where ğ›¼ is a hyper-parameter that controls the weight of personal- ization. 4.2.2 Training. Since the pre-trained dense retrieval model is not fine-tuned for our specific task, the retrieved results may not nec- essarily lead to LLM responses that better match the target output ğ‘¦ [25, 35]. However, there is no ground truth indicating which doc- uments are better. Therefore, we evaluate the difference between the LLMâ€™s output and the target outputğ‘¦, using this as a label to train the retrieval model. Figure 4 shows the process of training the retriever using LLM feedback. Specifically, we first use the pre-trained retrieval model to re- trieve the top- ğ‘˜ documents from each of the ğ‘š usersâ€™ histories based on ğ‘†retriever ğ‘,ğ‘‘ in Eq. (3), resulting in a total of ğ‘š Ã— ğ‘˜ candidate documents. These documents are then concatenated with the query one by one and used as prompts for the LLM, producing ğ‘š Ã— ğ‘˜ outputs: {ğ‘‚ğ‘,ğ‘‘ğ‘–,ğ‘— = LLM(ğ‘, ğ‘‘ğ‘–,ğ‘— )|ğ‘– âˆˆ { 1, . . . , ğ‘š}, ğ‘— âˆˆ { 1, . . . , ğ‘˜}}, where LLM(ğ‘, ğ‘‘ğ‘–,ğ‘— ) represents the output generated by inputting the concatenated query ğ‘ and document ğ‘‘ğ‘–,ğ‘— into the LLM. Then, based on the quality of these outputs, we can calculate the distribution of these candidate documents as follows: ğ‘LLM (ğ‘‘ğ‘–,ğ‘— |ğ‘, ğ‘¦) = exp(eval(ğ‘¦, ğ‘‚ğ‘,ğ‘‘ğ‘–,ğ‘— )) Ãğ‘š ğ‘–=1 Ãğ‘˜ ğ‘—=1 exp(eval(ğ‘¦, ğ‘‚ğ‘,ğ‘‘ğ‘–,ğ‘— )) , (6) where eval(Â·) measures the difference between the target output ğ‘¦ and the LLMâ€™s output, using metrics such as ROUGE [24] score. A larger value returned by eval(Â·) indicates a better-generated result. Similarly, we can