and how itâ€™s trained to retrieve documents that better align with the requirements of personalized LLM generation. 4.2.1 Retriever. First, we use a pre-trained dense retrieval model (such as BGE retriever [ 45]) to compute the semantic relevance Retrieval Augmented Generation with Collaborative Filtering for Personalized Text Generation SIGIR â€™25, July 13â€“18, 2025, Padua, Italy. â€¦ ğ‘‘11 ğ‘‘1ğ‘˜ â€¦ ğ‘‘ğ‘š1 ğ‘‘ğ‘šğ‘˜ â€¦ Candidate Documents Retriever/ Reranker â€¦ ğ‘‘11 ğ‘‘1ğ‘˜ â€¦ ğ‘‘ğ‘š1 ğ‘‘ğ‘šğ‘˜ â€¦ Candidate Documents â€¦ â€¦ Query ğ‘User ğ‘¢ â€¦ â€¦ Query ğ‘ Evaluated Score (eg. ROUGE) KL Divergence Rank Score â€¦ â€¦ LLM Figure 4: The method of training the retriever and reranker using LLM feedback. between the query and the candidate documents: ğ‘†retriever ğ‘,ğ‘‘ = cos(Encoderğ‘ (ğ‘), Encoderğ‘‘ (ğ‘‘)), (3) where Encoderğ‘ (Â·) â†’ Rğ‘‘ and Encoderğ‘‘ (Â·) â†’ Rğ‘‘ are the encoders for the query and the document in the retrieval model, respectively. Pre-trained retrieval models typically use ğ‘†retriever ğ‘,ğ‘‘ directly for re- trieval. However, ğ‘†retriever ğ‘,ğ‘‘ only considers the semantic relevance between the query and the document. Since different users might input the same query but expect different outputs due to their vary- ing preferences, we further account for user personalization by calculating the preference score of the user for the document as follows: ğ‘†retriever ğ‘¢,ğ‘‘ = cos(MLP1 (eğ‘¢ ), Encoderğ‘‘ (ğ‘‘)), (4) where MLP1 : Rğ‘‘ â†’ Rğ‘‘ is a multi-layer perceptron that maps the user embedding to the space where the cosine similarity is computed. eğ‘¢ is the embedding obtained in Section 4.1.1. The total score for retrieval is computed as follows: ğ‘†retriever ğ‘¢,ğ‘,ğ‘‘ = (1 âˆ’ ğ›¼)ğ‘†retriever ğ‘,ğ‘‘ + ğ›¼ğ‘† retriever ğ‘¢,ğ‘‘ , (5) where ğ›¼ is a hyper-parameter that controls the weight of personal- ization. 4.2.2 Training. Since the pre-trained dense retrieval model is not fine-tuned for our specific task, the