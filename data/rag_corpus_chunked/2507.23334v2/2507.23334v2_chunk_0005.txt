ther by jointly embedding text with the audio modality. By leveraging contrastive learning between audio and text, they can serve as more domain-specialized text embedding models for music-related tasks. 3.1.2 Retrieval Formally, the retriever R is defined as a function: R : (q, D) → c where q is the input question, D is the entire database of text passages, and c ⊂ D is the filtered context consist- ing of the top- k passages, such that |c| = k ≪ | D|. Each passage 1 p ∈ D is scored based on its similarity to the input question using cosine similarity between their em- beddings: sim(q, p) = E(q) · E(p) ∥E(q)∥∥E(p)∥ Here, E(·) denotes an embedding function that maps both questions and passages into a shared vector space. The retriever ranks all passages in D by their similarity scores and selects the top-k passages to form c, which serve as the external context for the generation step. 3.1.3 Generation The retrieved context c is provided to a generator LLM, which produces an output sequence using next-token pre- diction. Each token xi is generated conditioned on the input query q, the retrieved context c, and the previously generated tokens x<i: p(x1, . . . , xn | q, c) = nY i=1 pθ (xi | [q, c; x<i]) This structure enables the model to dynamically incorpo- rate external knowledge during inference, improving fac- tual accuracy and adaptability without retraining. 3.2 RAG vs. Fine-tuning LLMs often struggle with specialized tasks such as MQA due to limited exposure to domain-specific knowledge dur- ing pretraining. To address this, two primary domain adap- tation strategies are commonly used: fine-tuning and RAG. Fine-tuning is akin to a closed-book exam: the model in- ternalizes domain knowledge during training and must rely solely on that knowledge