due to the relatively small amount of music-specific knowl- edge in their training data. To effectively deploy general LLMs in music-related domains such as music recommen- dation systems and chatbots, a deep understanding of Mu- sic Question Answering (MQA) in text-only settings is es- sential. Mastering text-based MQA would enable LLMs to provide more accurate and contextually aware responses to user questions about music, ultimately enhancing the user experience in music-related applications. Developing a ro- bust text-only music QA framework is therefore a key step 1 ejmj63@kaist.ac.kr 2 seungheondoh@kaist.ac.kr 3 juhan.nam@kaist.ac.kr toward improving the adaptability of LLMs in the music domain. Traditionally, domain adaptation of LLMs has often been achieved by fine-tuning them on domain-specific data [1â€“3]. However, this approach faces challenges in securing high-quality training data, and as model size in- creases, the training time and cost also rise significantly. Additionally, continuously updating the model with new knowledge remains a persistent challenge. In this paper, we propose MusT-RAG, a framework that leverages Retrieval Augmented Generation (RAG) [4] techniques to enhance general-purpose LLMs for music- specific tasks. The core idea behind MusT-RAG is to augment LLMs with external knowledge retrieval mech- anisms. Specifically, the model retrieves relevant external knowledge from a pre-constructed, comprehensive music- specific vector database in order to answer input questions. For music-domain specific retrieval, we introduce MusWikiDB, which, to our knowledge, is the first com- prehensively curated vector database designed specifically for music-related content. We explore various design choices for optimizing retrieval performance, including embedding models and chunking strategies. By incorpo- rating this retrieval process, MusT-RAG enables LLMs to efficiently generate contextually relevant responses, draw- ing on specialized music knowledge to enhance perfor- mance on music-related tasks, all without requiring addi- tional training. Furthermore, we extend the application of RAG beyond inference by incorporating contextual infor- mation