RAG Inference By utilizing RAG inference without addi- tional training, we were able to address the low factual per- formance that was an issue with previous LLMs. It demon- strated a 14.6% higher factual performance compared to GPT-4o [35]. Contextual performance improved by 3.6% compared to zero-shot, but was still 4.2% lower than GPT- 4o. RAG Fine-tuning The model fine-tuned on the RAG- style dataset showed improvements in both types of ques- tions. Compared to RAG inference, factual performance improved by 0.4%, and contextual performance improved by 3.2%. This demonstrates that by learning to leverage context, the model not only improves its memory of in- formation present in the training data but also enhances its overall contextual understanding ability. It exhibited a remarkable 15.0% higher factual performance compared to GPT-4o, and only 1.0% lower contextual performance, which is nearly equivalent. Considering factors such as the model size, amount of training data, and the extent of training, this is an exceptionally high performance. Embedding Passage Size Factual Contextual Gold (Upper Bound) 97.8 97.0 BM25 [16] 512 82.0 88.8 256 82.8 88.0 128 82.2 89.0 Contriever [19] 512 46.6 81.0 256 55.6 84.2 128 58.2 86.6 CLAP [20] 512 41.2 79.6 256 41.0 84.0 128 41.8 84.0 Table 4: Llama 3.1 8B Instruct [34] RAG performance on ArtistMus, by different passage size and embeddings. 6.2 Out-of-domain Performance To validate the effectiveness of the MusT-RAG in out-of- domain scenarios, we conducted experiments by changing the benchmark from ArtistMus to TrustMus [13], using the same framework with in-domain evaluation. The results are presented in Table 3. Zero-shot Baselines A similar trend was observed in the zero-shot evaluation for the in-domain setting. MuL- LaMA [36] and ChatMusician [12] performed worse than the random baseline (25%), which is due to incorrect an- swers being counted when