(CH), each comprising 100 questions. All evaluations use a multiple-choice QA format. 5.2 Models We compare zero-shot and QA fine-tuned models with our proposed RAG inference and RAG fine-tuned models to evaluate MQA performance. Following [11], we consider a response incorrect if it deviates from the expected format. Zero-shot Baselines We evaluated GPT-4o [35] (API- based), Llama 3.1 8B Instruct [34] (open-source), and two music-specific models: MuLLaMA [36] and ChatMusi- cian [12]. MuLLaMA is designed to handle audio based question answering. ChatMusician specializes in music understanding and generation with ABC notation. QA Fine-tuning We fine-tune the Llama 3.1 8B In- struct [34] on 8K multiple-choice QA pairs that were gen- erated from MusWikiDB. RAG Inference We use Llama 3.1 8B Instruct [34] as our base model and implement RAG at inference-time using MusWikiDB as the retrieval database. RAG Fine-tuning We performed RAG fine-tuning us- ing a dataset in the form of (context, question, answer), by augmenting the original QA fine-tuning dataset with addi- tional context. The target model and all other training set- tings were kept identical to those used in QA fine-tuning. 5.3 Training Configurations The models are trained for one epoch using LoRA [37] with 8-bit quantization with the following hyperparameter settings: batch size = 2, gradient accumulation steps = 4, learning rate = 3e-5, weight decay = 0.005, warmup ratio = 0.1, cosine scheduler [38], AdamW [39] optimizer, r = 16, alpha = 16, and dropout = 0.1. For the ArtistMus dataset, half of the artists were included in the training data (Seen), while the other half were excluded (Unseen). 5.4 Retriever Configurations To select the optimal retriever configuration MusWikiDB, we performed an ablation study using the ArtistMus Factual Contextual Model Params Seen Unseen All Seen Unseen All Baseline Models (zero-shot) GPT-4o [35] N/A 70.0 64.8 67.4