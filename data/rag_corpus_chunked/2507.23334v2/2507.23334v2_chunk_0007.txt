external context that might be available. As a result, the model may struggle to utilize background infor- mation effectively, especially when answering questions that require specialized or up-to-date knowledge. To address this limitation, we adopt a RAG-style fine- tuning approach using a dataset consisting of (context, question, answer) triples. Unlike standard QA fine-tuning, which relies solely on the question, our method introduces an external relevant passagep for the input questionq. This enables the model to learn how to incorporate relevant con- textual information during answer generation. Both ap- proaches share the same next-token prediction objective, but differ in the input they condition on. In standard fine- tuning, the model is trained as follows: LQA Fine-tuning = − nX i=1 log pθ(xi | [q; x<i]), where the model predicts each answer token xi based only on the question and the previously generated tokens. In contrast, RAG-style fine-tuning conditions the generation not only on the question but also on the relevant passages as context: LRAG Fine-tuning = − nX i=1 log pθ(xi | [q, c; x<i]), where c is the relevant passage retrieved from an external corpus. By incorporating c as an additional context, the model is encouraged to utilize external knowledge when generating answers. This strategy improves the model’s ability to ground its responses in retrieved evidence, lead- ing to more accurate and contextually appropriate answers. During RAG fine-tuning, we used gold passages with high relevance to the answers, ensuring the model learns to ef- fectively utilize contextual information. 4. DATASET 4.1 MusWikiDB To address the lack of a music-specific vector database for RAG in MQA, we developed MusWikiDB. We be- gan by collecting music-related content from Wikipedia across seven categories: artists, genres, instruments, his- tory, technology, theory, and forms. These categories were selected to cover a broad spectrum of music