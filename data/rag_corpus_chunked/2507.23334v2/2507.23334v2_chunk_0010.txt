try library. Then, we select a diverse range of 500 artists based on topic, genre, and country. Country was set as the highest priority, with a preference for artists from minor countries. Subsequently, popular genres and topics were replaced with less common ones. We generated one fac- tual and one contextual question for each artist to evalu- ate the LLM’s factuality and contextual understanding. To construct these questions, we provided GPT-4o [35] with the corresponding section text. Factual questions focus on verifiable details such as dates, names, or events, whereas contextual questions require reasoning or synthesis across multiple pieces of information within the passage. We validate the generated questions based on two cri- teria: Music Relevance and Faithfulness. For Music Rel- evance, questions that did not pertain to musical aspects were excluded except important details such as the artist’s birthplace. For Faithfulness, GPT-4o was asked to verify whether the question and answer could be derived from the provided text. Finally, 1,000 multiple-choice questions passing human validation were generated. We randomly reassigned the correct answers, ensuring an even distribu- tion by assigning 250 correct answers to each option. 5. EXPERIMENTS 5.1 Benchmarks For evaluation, we used two datasets: ArtistMus (in- domain) and TrustMus (out-of-domain). Performance on factual and contextual questions was separately measured on the ArtistMus. For TrustMus, evaluation was conducted across four categories: People (Ppl), Instrument & Tech- nology (IT), Genre, Forms, and Theory (GFT), and Cul- ture & History (CH), each comprising 100 questions. All evaluations use a multiple-choice QA format. 5.2 Models We compare zero-shot and QA fine-tuned models with our proposed RAG inference and RAG fine-tuned models to evaluate MQA performance. Following [11], we consider a response incorrect if it deviates from the expected format. Zero-shot Baselines We evaluated GPT-4o [35] (API- based), Llama 3.1 8B Instruct