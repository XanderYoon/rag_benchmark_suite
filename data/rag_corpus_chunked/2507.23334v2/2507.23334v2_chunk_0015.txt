changing the benchmark from ArtistMus to TrustMus [13], using the same framework with in-domain evaluation. The results are presented in Table 3. Zero-shot Baselines A similar trend was observed in the zero-shot evaluation for the in-domain setting. MuL- LaMA [36] and ChatMusician [12] performed worse than the random baseline (25%), which is due to incorrect an- swers being counted when the models failed to follow in- structions. Given that the overall zero-shot performance closely aligns with the factual scores from the in-domain evaluation, we infer that TrustMus mostly consists of fac- tual questions. The Llama 3.1 8B Instruct [34] model scored 17.2% lower than GPT-4o [35]. QA Fine-tuning The QA fine-tuned model showed a 3.8% decrease in performance compared to zero-shot, which can be attributed to the fact that models trained on artist data tend to forget information about out-of-domain topics, such as Instrument and Genre. RAG Inference RAG inference led to an 5.0% per- formance improvement over zero-shot, demonstrating that MusT-RAG framework is also helpful for out-of-domain data, such as The Grove Dictionary Online [15], which is the basis for the TrustMus benchmark. RAG Fine-tuning The RAG fine-tuned model showed a 0.7% improvement over RAG inference, even with the same artist data used for QA fine-tuning. This supports the fact that the RAG fine-tuning method, which incorporates context, enhances the modelâ€™s robustness in contextual un- derstanding, even for out-of-domain data. 6.3 Ablation Study: Retriever Configurations Table 4 shows the results of the RAG inference for the Llama 3.1 8B Instruct [34] with various passage sizes and embeddings, evaluated under the same total computation budget for fair comparison. The performance on contex- tual questions tended to improve as the passage size de- creased across all embedding models. In contrast, for Figure 2 : RAG performance and retrieval time for Wikipedia