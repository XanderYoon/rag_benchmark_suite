3.2 RAG vs. Fine-tuning LLMs often struggle with specialized tasks such as MQA due to limited exposure to domain-specific knowledge dur- ing pretraining. To address this, two primary domain adap- tation strategies are commonly used: fine-tuning and RAG. Fine-tuning is akin to a closed-book exam: the model in- ternalizes domain knowledge during training and must rely solely on that knowledge at inference. While effective for learning structured formats or stylistic patterns [26, 27], it is resource-intensive and inflexible when adapting to new or frequently changing knowledge. In contrast, RAG is like an open-book exam: the model dynamically retrieves relevant information from an external knowledge source during inference. This enables LLMs to access up-to- date and specialized information without retraining. Prior studies [28, 29] show that RAG improves factual accuracy, mitigates hallucinations, and provides greater transparency by allowing source verification. It is also more scalable and economically efficient, as it does not require updat- ing model parameters [27]. These benefits are especially useful in rapidly evolving domains like music, where new artists, compositions, and styles continuously emerge. 3.3 RAG with Fine-tuning While fine-tuning typically relies on question-answer pairs, it does not always emphasize learning to extract rel- evant information from the context provided alongside the question. In standard fine-tuning, the model is trained to 1 A passage refers to a portion of a document relevant to a query [25]. directly map a question to its answer without fully lever- aging any external context that might be available. As a result, the model may struggle to utilize background infor- mation effectively, especially when answering questions that require specialized or up-to-date knowledge. To address this limitation, we adopt a RAG-style fine- tuning approach using a dataset consisting of (context, question, answer) triples. Unlike standard QA fine-tuning, which relies solely on the question, our