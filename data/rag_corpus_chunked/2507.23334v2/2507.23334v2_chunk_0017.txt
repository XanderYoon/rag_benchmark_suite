commonly observed in LLMs. As a result, our method achieves substantial im- provements over GPT-4o [35], particularly in factual ac- curacy. Beyond simple retrieval, we further demonstrated that RAG-style fine-tuning outperforms traditional QA fine-tuning by improving both factual and contextual per- formance. Our final model achieves a 15.0% gain in fac- tual performance over GPT-4o while maintaining compa- rable performance in contextual tasks. Importantly, MusT- RAG shows strong generalization capabilities. On the out- of-domain benchmark TrustMus [13], it delivers a 5.7% performance improvement over the zero-shot baseline, un- derscoring its robustness across diverse music-related QA scenarios. To facilitate future work in this underexplored domain, we release two key resources: MusWikiDB, a music-specific retrieval corpus, and ArtistMus, a bench- mark focused on artist-level musical knowledge. We hope these contributions will drive further progress in develop- ing accurate and domain-aware LLMs for music under- standing and beyond. 8. REFERENCES [1] C. Jeong, “Fine-tuning and utilization meth- ods of domain-specific llms,” arXiv preprint arXiv:2401.02981, 2024. [2] S. S. Sahoo, J. M. Plasek, H. Xu, Ö. Uzuner, T. Cohen, M. Yetisgen, H. Liu, S. Meystre, and Y . Wang, “Large language models for biomedicine: foundations, oppor- tunities, challenges, and best practices,” Journal of the American Medical Informatics Association , vol. 31, no. 9, pp. 2114–2124, 2024. [3] N. Satterfield, P. Holbrooka, and T. Wilcoxa, “Fine- tuning llama with case law data to improve legal do- main performance,” OSF Preprints, 2024. [4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. tau Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval- augmented generation for knowledge-intensive nlp tasks,” 2021. [Online]. Available: https://arxiv.org/abs/ 2005.11401 [5] Z. Zhao, E. Monti, J. Lehmann, and H. Assem, “En- hancing contextual understanding in large language models through contrastive decoding,” arXiv preprint arXiv:2405.02750,