preprint arXiv:2408.01337 , 2024. [12] R. Yuan, H. Lin, Y . Wang, Z. Tian, S. Wu, T. Shen, G. Zhang, Y . Wu, C. Liu, Z. Zhou et al. , “Chatmusi- cian: Understanding and generating music intrinsically with llm,” arXiv preprint arXiv:2402.16153, 2024. [13] P. Ramoneda, E. Parada-Cabaleiro, B. Weck, and X. Serra, “The role of large language models in musicology: Are we ready to trust the machines?” 2024. [Online]. Available: https://arxiv.org/abs/2409. 01864 [14] J. Li, L. Yang, M. Tang, C. Chen, Z. Li, P. Wang, and H. Zhao, “The music maestro or the musi- cally challenged, a massive music evaluation bench- mark for large language models,” arXiv preprint arXiv:2406.15885, 2024. [15] S. Sadie and J. Tyrrell, The New Grove Dic- tionary of Music and Musicians, 2nd edition , D. Root, Ed. London: Macmillan Publishers, 2001, accessed 05-05-2024. [Online]. Available: http://www.oxfordmusiconline.com [16] S. E. Robertson and S. Walker, “Some simple effec- tive approximations to the 2-poisson model for proba- bilistic weighted retrieval,” in SIGIR’94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Informa- tion Retrieval, organised by Dublin City University . Springer, 1994, pp. 232–241. [17] P. BehnamGhader, V . Adlakha, M. Mosbach, D. Bah- danau, N. Chapados, and S. Reddy, “Llm2vec: Large language models are secretly powerful text encoders,” arXiv preprint arXiv:2404.05961, 2024. [18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transform- ers for language understanding,” in Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human lan- guage technologies, volume 1 (long and short papers) , 2019, pp. 4171–4186. [19] G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bo- janowski, A. Joulin, and E. Grave, “Unsupervised dense information retrieval with contrastive learning,” arXiv preprint arXiv:2112.09118, 2021. [20] Y