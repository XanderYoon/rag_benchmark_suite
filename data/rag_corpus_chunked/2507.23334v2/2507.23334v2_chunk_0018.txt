Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. tau Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval- augmented generation for knowledge-intensive nlp tasks,” 2021. [Online]. Available: https://arxiv.org/abs/ 2005.11401 [5] Z. Zhao, E. Monti, J. Lehmann, and H. Assem, “En- hancing contextual understanding in large language models through contrastive decoding,” arXiv preprint arXiv:2405.02750, 2024. [6] A. M. N. Allam and M. H. Haggag, “The question an- swering systems: A survey,” International Journal of Research and Reviews in Information Sciences (IJR- RIS), vol. 2, no. 3, 2012. [7] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+ questions for machine comprehen- sion of text,” arXiv preprint arXiv:1606.05250, 2016. [8] V . Karpukhin, B. Oguz, S. Min, P. S. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih, “Dense pas- sage retrieval for open-domain question answering.” in EMNLP (1), 2020, pp. 6769–6781. [9] A. Pal, L. K. Umapathi, and M. Sankarasubbu, “Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering,” in Conference on health, inference, and learning . PMLR, 2022, pp. 248–260. [10] I. Chalkidis, A. Jana, D. Hartung, M. Bommar- ito, I. Androutsopoulos, D. M. Katz, and N. Ale- tras, “Lexglue: A benchmark dataset for legal language understanding in english,” arXiv preprint arXiv:2110.00976, 2021. [11] B. Weck, I. Manco, E. Benetos, E. Quinton, G. Fazekas, and D. Bogdanov, “Muchomusic: Eval- uating music understanding in multimodal audio- language models,” arXiv preprint arXiv:2408.01337 , 2024. [12] R. Yuan, H. Lin, Y . Wang, Z. Tian, S. Wu, T. Shen, G. Zhang, Y . Wu, C. Liu, Z. Zhou et al. , “Chatmusi- cian: Understanding and generating music intrinsically with llm,” arXiv preprint arXiv:2402.16153, 2024. [13] P. Ramoneda, E. Parada-Cabaleiro, B. Weck, and X. Serra, “The role of large language models in