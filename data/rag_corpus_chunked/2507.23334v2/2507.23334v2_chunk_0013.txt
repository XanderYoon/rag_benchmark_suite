38.0 46.0 49.0 41.5 Table 3: Performance on out-of-domain (OOD) TrustMus benchmark. Four categories are: People (Ppl), Instrument & Technology (IT), Genre, Forms, and Theory (GFT), and Culture & History (CH). benchmark. We varied the passage size (128, 256, 512 to- kens) and embedding models (BM25 [16], Contriever [19], CLAP [20]). For CLAP, we increased the token limit with- out additional training. To ensure a fair comparison, we constrained the total token budget to 1024 by adjusting the number of retrieved passages: top-8 for 128-token pas- sages, top-4 for 256, and top-2 for 512. 6. RESULT 6.1 In-domain Performance Zero-shot Baselines As shown in Table 2, all models performed significantly worse on factual questions than on contextual ones, indicating challenges in recalling concrete information such as names or dates. GPT-4o [35] out- performed Llama [34] by 28.4% in factual performance, though the gap narrowed to 7.8% for contextual under- standing. Despite being music-specific, both ChatMusi- cian [12] and MuLLaMA [36] showed relatively low per- formance. ChatMusician slightly underperformed com- pared to Llama, while MuLLaMA exhibited the lowest scores, likely due to its lack of training on the MQA task and poor instruction-following capabilities. QA Fine-tuning Comparing QA fine-tuning with zero- shot performance, factual performance improved by 1.0%, but contextual performance decreased by 5.5%. This sug- gests that while QA fine-tuning is effective in helping the model retain information from the training data, it may also reduce the overall inference capability. RAG Inference By utilizing RAG inference without addi- tional training, we were able to address the low factual per- formance that was an issue with previous LLMs. It demon- strated a 14.6% higher factual performance compared to GPT-4o [35]. Contextual performance improved by 3.6% compared to zero-shot, but was still 4.2% lower than GPT- 4o. RAG Fine-tuning The model fine-tuned