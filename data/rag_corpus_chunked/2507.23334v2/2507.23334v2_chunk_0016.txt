results of the RAG inference for the Llama 3.1 8B Instruct [34] with various passage sizes and embeddings, evaluated under the same total computation budget for fair comparison. The performance on contex- tual questions tended to improve as the passage size de- creased across all embedding models. In contrast, for Figure 2 : RAG performance and retrieval time for Wikipedia Corpus [8] and MusWikiDB. factual questions, only Contriever [19] showed clear im- provements with shorter passages, while BM25 [16] and CLAP [20] showed little to no change in performance across different passage lengths. For factual questions, there was a significant performance gap between BM25 and the other two dense embeddings. This is likely because ArtistMus places high importance on music entities such as artist and albums. Overall, the best performance was achieved using BM25 with a passage size of 128. When compared to the gold context, the factual performance was 15.6% lower, and the contextual performance was 8.0% lower. In Figure 2, we compare the RAG inference perfor- mance using the Wikipedia corpus [8] and MusWikiDB. The results show that MusWikiDB achieves a 10x faster retrieval speed and 5.9% higher performance. 7. CONCLUSION In this paper, we presented MusT-RAG, a retrieval- augmented framework that enhances text-only Music Question Answering (MQA) by adapting general-purpose LLMs to the music domain. By retrieving relevant pas- sages from a music-specific database and incorporating them into the generation context, MusT-RAG effectively mitigates the factuality limitations commonly observed in LLMs. As a result, our method achieves substantial im- provements over GPT-4o [35], particularly in factual ac- curacy. Beyond simple retrieval, we further demonstrated that RAG-style fine-tuning outperforms traditional QA fine-tuning by improving both factual and contextual per- formance. Our final model achieves a 15.0% gain in fac- tual performance over GPT-4o while maintaining compa- rable performance