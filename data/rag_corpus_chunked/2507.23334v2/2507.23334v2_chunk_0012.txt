0.1. For the ArtistMus dataset, half of the artists were included in the training data (Seen), while the other half were excluded (Unseen). 5.4 Retriever Configurations To select the optimal retriever configuration MusWikiDB, we performed an ablation study using the ArtistMus Factual Contextual Model Params Seen Unseen All Seen Unseen All Baseline Models (zero-shot) GPT-4o [35] N/A 70.0 64.8 67.4 93.2 92.8 93.0 ChatMusician [12] 7B 28.0 25.2 26.6 78.8 67.6 73.2 MuLLaMA [36] 7B 27.2 25.2 26.2 38.4 40.0 39.2 Llama 3.1 8B Instruct [34] 8B 40.0 38.0 39.0 87.6 82.8 85.2 Domain Adaptation Models (Llama 3.1 8B Instruct) QA Fine-tuning 8B 41.2 38.8 40.0 81.6 78.8 79.7 RAG Inference (Ours) 8B 81.2 82.8 82.0 89.6 88.0 88.8 RAG Fine-tuning (Ours) 8B 81.6 83.2 82.4 92.4 91.6 92.0 Table 2: Performance on the ArtistMus benchmark. Seen refers to data with artists present in training data, while Unseen contains new artists. This distinction applies only to domain adaptation models. For baseline models, all data is unseen. Model Params Ppl IT GFT CH All Baseline Models (zero-shot) GPT-4o [35] N/A 48.0 47.0 57.0 60.0 53.0 ChatMusician [12] 7B 18.0 20.0 26.0 24.0 20.0 MuLLaMA [36] 7B 25.0 15.0 18.0 21.0 19.8 Llama 3.1 8B Instruct [34] 8B 36.0 24.0 41.0 42.0 35.8 Domain Adaptation Models (Llama 3.1 8B Instruct) QA Fine-tuning 8B 32.0 21.0 39.0 36.0 32.0 RAG Inference (Ours) 8B 33.0 40.0 44.0 46.0 40.8 RAG Fine-tuning (Ours) 8B 33.0 38.0 46.0 49.0 41.5 Table 3: Performance on out-of-domain (OOD) TrustMus benchmark. Four categories are: People (Ppl), Instrument & Technology (IT), Genre, Forms, and Theory (GFT), and Culture & History (CH). benchmark. We varied the passage size (128, 256, 512 to- kens) and embedding models (BM25 [16], Contriever [19], CLAP [20]). For CLAP, we increased the token limit with- out