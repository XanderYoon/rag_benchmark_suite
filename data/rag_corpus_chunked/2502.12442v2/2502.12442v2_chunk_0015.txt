51.57 63.78 21.75 16 37.50 51.44 16.47 60.00 67.52 15.02 59.50 76.45 21.75 52.33 65.14 17.75 20 39.10 53.00 13.89 61.60 68.93 12.51 61.30 78.34 18.48 54.00 66.76 14.96 Table 3: We test the robustness w.r.t hyperparametertopk on HopRAG using GPT-3.5-turbo on multiple datasets. We vary topk from 2 to 20 and report both the answer and retrieval metrics, where the best score is in bold and the second best is underlined. MuSiQue 2Wiki HotpotQA Average nhop Retrieval F1 LLM Cost Retrieval F1 LLM Cost Retrieval F1 LLM Cost Retrieval F1 LLM Cost 1 8.78 20.00 8.68 19.86 6.78 19.91 8.08 19.92 2 11.86 30.32 11.42 31.52 15.13 29.39 12.80 30.41 3 12.67 37.28 11.97 37.15 16.76 33.35 13.80 35.93 4 13.89 40.32 12.51 40.12 18.48 35.14 14.96 38.53 Table 4: We test the effect of hyperparameter nhop on HopRAG using GPT-3.5-turbo on multiple datasets with top 20 passages. We vary nhop from 1 to 4 and report both the answer and retrieval metrics in Table 8, and report the retrieval metrics here. For retrieval metrics, we calculate the retrieval F1 score and also the average number of calling LLM during traversal to measure the cost (the lower, the better). The best score is in bold and the second best is underlined. ing (BGE), 9.94% higher than RAPTOR, 3.08% higher than HippoRAG, 1.11% higher than SiR- eRAG. This illustrates the strengths of HopRAG in capturing both textual similarity and logical rela- tions for handling multi-hop QA. Specifically, BM25, BGE and BGE with query decomposition yield unsatisfactory results since they rely solely on similarity, and BGE with rerank- ing cannot capture logical relevance among can- didates. Since GraphRAG considers relevance among entities instead of similarity for graph search, and RAPTOR focuses on the hierarchical logical relations among passages but cannot cap-