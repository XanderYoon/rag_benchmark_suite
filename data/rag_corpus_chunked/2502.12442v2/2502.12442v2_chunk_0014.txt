rics in the ablations and discussion on HopRAG. See Appendix A.3 for more metric details. Settings We use BGE embedding model for se- mantic vectors at 768 dimensions. To avoid the loss of semantic information caused by chunking at a fixed size, we adopt the same chunking methods uti- lized in the original datasets respectively. GPT-4o- mini serves as both the model generating in-coming and out-coming questions when constructing the graph index, and the reasoning model for graph traversal. We use two reader models GPT-4o and GPT-3.5-turbo to generate the response given the context with 20 retrieval candidates and nhop = 4. See Appendix A.4 for more setting details. 4.2 Main Results The main results are presented in Table 1 and 2. We observe that almost in all the settings HopRAG gives the best performance, with exceptions on HotpotQA when compared against SiReRAG and 2WikiMultiHopQA against HippoRAG. Overall, HopRAG achieves approximately 76.78% higher than dense retriever (BGE), 48.62% higher than query decomposition, 36.25% higher than rerank- MuSiQue 2Wiki HotpotQA Average Answer Retrieval Answer Retrieval Answer Retrieval Answer Retrieval topk EM F1 F1 EM F1 F1 EM F1 F1 EM F1 F1 2 32.50 46.31 37.83 47.80 53.91 36.77 52.00 67.78 50.23 44.10 56.00 41.61 4 36.50 49.53 35.02 54.50 59.35 33.22 55.60 71.10 46.45 48.87 59.99 38.23 8 38.50 50.81 26.36 56.10 61.81 23.90 58.20 75.05 34.14 50.93 62.56 28.13 12 37.50 51.47 20.38 57.70 64.33 18.54 59.50 75.54 26.34 51.57 63.78 21.75 16 37.50 51.44 16.47 60.00 67.52 15.02 59.50 76.45 21.75 52.33 65.14 17.75 20 39.10 53.00 13.89 61.60 68.93 12.51 61.30 78.34 18.48 54.00 66.76 14.96 Table 3: We test the robustness w.r.t hyperparametertopk on HopRAG using GPT-3.5-turbo on multiple datasets. We vary topk from 2 to 20 and report both the answer and retrieval metrics, where