faster hop from indirectly relevant passages to truly rel- evant ones, we test the robustness by evaluating both the QA and retrieval performance on GPT-3.5- turbo with smaller topk, as is shown in Table 3. From the results, we find that even with top 12 can- didates, the QA performance of HopRAG is still comparable to that of HippoRAG or RAPTOR with 20 candidates, which highlights the effectiveness of our graph traversal design in efficiently retrieving more information within a limited context length. Meanwhile, we also observe that as topk increases, the retrieval F1 score gradually decreases due to MuSiQue 2Wiki HotpotQA Average Answer Retrieval Answer Retrieval Answer Retrieval Answer Retrieval Method (Traversal Model)EM F1 F1 EM F1 F1 EM F1 F1 EM F1 F1 BM25 5.80 11.00 5.79 27.00 31.55 9.25 33.40 44.30 8.75 22.07 28.95 7.93 BGE 11.80 18.60 8.76 27.90 30.80 7.60 38.40 50.56 11.10 26.03 33.32 9.16 HopRAG (non-LLM)19.00 27.68 8.27 42.20 46.72 8.09 46.90 61.17 11.73 36.04 45.19 9.36 HopRAG (Qwen2.5-1.5B-Instruct)38.0046.73 11.91 58.4064.78 11.82 58.2074.74 18.22 51.5362.08 13.98 HopRAG (GPT-4o-mini)39.10 53.00 13.89 61.60 68.93 12.51 61.30 78.34 18.48 54.00 66.76 14.96 Table 5: We conduct an ablation study on the reasoning model during traversal with GPT-3.5-turbo as the inference model and top 20 passages. We compare 5 scenarios including sparse retriever (BM25), dense retriever (BGE), HopRAG (non-LLM), HopRAG (Qwen2.5-1.5B-Instruct) and HopRAG (GPT-4o-mini) and report both the answer and the retrieval metrics, where the best score is in bold and the second best is underlined. the inclusion of excessive redundant information. Conversely, the answer quality generally improves, attributed to GPT-3.5-turboâ€™s strong capability in processing and reasoning over extended contexts, with only one exception in the MuSiQue dataset. Effects of nhop To assess the effects of the hy- perparameter nhop on reasoning-augmented graph traversal, we vary nhop