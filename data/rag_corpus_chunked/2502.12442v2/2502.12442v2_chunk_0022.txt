opportunities with large language models. arXiv preprint arXiv:2404.11457. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024a. From Local to Global: A Graph RAG Approach to Query-Focused Summa- rization. arXiv preprint. ArXiv:2404.16130 [cs]. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024b. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130. Masoomali Fatehkia, Ji Kim Lucas, and Sanjay Chawla. 2024. T-rag: Lessons from the llm trenches. Preprint, arXiv:2402.07483. Thibault Févry, Livio Baldini Soares, et al. 2020. En- tities as experts: Sparse memory access with entity supervision. In EMNLP. John Guare. 2016. Six degrees of separation. In The Contemporary Monologue: Men, pages 89–93. Rout- ledge. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In- centivizing reasoning capability in llms via reinforce- ment learning. arXiv preprint arXiv:2501.12948. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. LightRAG: Simple and Fast Retrieval-Augmented Generation. arXiv preprint. ArXiv:2410.05779. Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michi- hiro Yasunaga, and Yu Su. 2025. Hipporag: Neu- robiologically inspired long-term memory for large language models. Preprint, arXiv:2405.14831. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020a. Retrieval augmented language model pre-training. In International confer- ence on machine learning, pages 3929–3938. PMLR. Kelvin Guu, Kenton Lee, Zora Tung, et al. 2020b. REALM: retrieval-augmented language model pre- training. ICML. Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V . Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval- Augmented Generation for Textual Graph Under- standing and Question Answering. arXiv preprint. ArXiv:2402.07630. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.