F1 score is then computed as: F 1 = 2 · P · R P + R In our ablation study, we also report the retrieval F1 score to test the sensitivity of HopRAG, which is calculated as follows. The Precision (P) and Recall (R) for retrieval are computed as: P = |Ret ∩ Rel| |Ret| , R = |Ret ∩ Rel| |Rel| where Ret represents the set of passages retrieved during retrieval, and Rel denotes the set of relevant passages that support the ground truth answer. The Retrieval F1 score is then calculated as the harmonic mean of precision and recall: F 1retrieval = 2 · P · R P + R A.4 Settings To avoid semantic loss by chunking the documents at a fixed size, we chunk each document in a way corresponding to the supporting facts of each dataset. Specifically, we chunk each document in HotpotQA and 2WikiMultiHopQA by sentence since the smallest unit of these two datasets’ sup- porting facts is a sentence. To get embedding rep- resentation for each chunk, we use bge-base model. To extract keywords, we use the part-of-speech tag- ging function of Python package PaddleNLP to extract and filter entities. In our method, we use the Neo4j graph database to store vertices and build edges. When building edges we employ prompt engineering technique to instruct the LLM to gen- erate an appropriate number of questions for each vertex to cover its information, with a minimum requirement of at least 2 in-coming questions and 4 out-coming questions. To prevent the graph struc- ture from becoming overly complex and dense, we retain only O(n · log(n)) edges, where n is the number of vertices. We use GPT-4o-mini for reasoning-augmented graph traversal, GPT-4o and GPT-3.5-turbo for inference with 2048 max tokens and 0.1 temperature in