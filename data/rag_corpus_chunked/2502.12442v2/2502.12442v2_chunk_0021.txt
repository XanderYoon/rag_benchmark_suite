siderations. First, we utilized AI assistant to en- hance the writing process of our paper and code. We ensure that the AI assistant was used as a tool to improve clarity and conciseness, while the final content and ideas were developed and reviewed by human authors. Second, we employed multi- ple open source datasets and one open source tool Neo4j Community Edition (Webber, 2012) under GPL v3 license in our experiments. We are trans- parent about their origin and limitations, and we respect data ownership and user privacy. References Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Bin- bin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. 2022. Towards tracing knowledge in language models back to the training data. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2429–2446, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Akari Asai, Zeqiu Wu, Yizhong Wang, et al. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arxiv:2310.11511. Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. arXiv preprint. ArXiv:2310.05029. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 38, pages 17754–17762. Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhen- hua Dong, and Jun Xu. 2024. Unifying bias and unfairness in information retrieval: A survey of chal- lenges and opportunities with large language models. arXiv preprint arXiv:2404.11457. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024a. From Local to Global: A Graph RAG Approach to Query-Focused Summa- rization. arXiv preprint. ArXiv:2404.16130 [cs]. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024b.