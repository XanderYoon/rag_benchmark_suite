the LLM as an interactive agent walk- ing on the tree of summarization. SiReRAG (Zhang et al., 2024) explicitly considers both similar and related information by constructing both similar- ity tree and relatedness tree. PG-RAG (Liang et al., 2024) prompts LLMs to organize docu- ment knowledge into mindmaps, and unifies them for multiple documents. GNN-RAG (Mavroma- tis and Karypis, 2024) reasons over dense KG subgraphs with learned GNNs to retrieve answer candidates. For query-focused summarization, GraphRAG (Edge et al., 2024b) builds a hierarchi- cal graph index with knowledge graph construction and recursive summarization. Despite advance- ments, tree-structured RAG only focuses on the hierarchical logic within a single document; graph- structured RAG is costly, time-consuming, and re- turns triplets instead of plain text. In contrast, Ho- pRAG offers a more lightweight and downstream task friendly alternative, with flexible logical mod- eling, cross-document organization, efficient con- struction and updating. 3 Method In this section, we introduce our logic-aware RAG system, named HopRAG. An overview of this sys- tem is illustrated in Figure 3. 3.1 Problem Formulation Given a passage corpus P = {p1, p2, ..., pN } and a query q which requires the information from multiple passages in P , the task is to design (1) a graph-structured RAG knowledge base that not only stores all the passages in corpus P but also models the similarity and logic between passages; (2) a corresponding retrieval strategy that can hop from indirectly relevant passages to truly relevant passages for better retrieval. Finally, with the query q and k passages as context C = {pi1, pi2, ..., pik }, the LLM generates the response O âˆ¼ P (O|q, C). 3.2 Graph-Structured Index We construct a graph-structured index G = (V, E) where the vertex set V consists of vertices storing all the passages and