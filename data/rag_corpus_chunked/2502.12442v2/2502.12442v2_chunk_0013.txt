60.36 36.17 45.14 Query Decomposition 29.00 38.50 55.70 60.57 52.80 68.67 47.46 55.91 Reranking 32.00 40.29 53.70 58.44 55.40 70.03 48.61 56.25 GraphRAG 12.10 20.22 22.50 27.49 31.70 42.74 22.10 30.15 RAPTOR 36.40 49.09 53.80 61.45 58.00 73.08 49.40 61.21 SiReRAG 40.50 53.08 59.60 67.94 61.70 76.48 53.93 65.83 HopRAG 42.20 54.90 61.10 68.26 62.00 76.06 55.10 66.40 Table 2: We report the QA performance metrics EM and F1 score with GPT-4o and top 20 passages here, where the best score is in bold and the second best is underlined. (Nogueira and Cho, 2020) (5) tree-structured RAG - RAPTOR (Sarthi et al., 2024) (6) tree-structured RAG - SiReRAG (Zhang et al., 2024) (7) graph- structured RAG - GraphRAG (Edge et al., 2024a) with the local search function (8) graph-structured RAG - HippoRAG (Guti√©rrez et al., 2025). For structured RAG baselines, we follow the same set- ting as previous work (Zhang et al., 2024). Metrics To measure the answer quality of dif- ferent methods, we adopt exact match (EM) and F1 score which focus on the accuracy between a generated answer and the corresponding ground truth. We also use retrieval metrics to compare graph-based methods. Since tree-based methods like SiReRAG (Zhang et al., 2024) and RAPTOR (Sarthi et al., 2024) create new candidates (e.g., summary nodes) in the retrieval pool, it would be unfair to use retrieval metrics to compare them with others. We report both the answer and retrieval met- rics in the ablations and discussion on HopRAG. See Appendix A.3 for more metric details. Settings We use BGE embedding model for se- mantic vectors at 768 dimensions. To avoid the loss of semantic information caused by chunking at a fixed size, we adopt the same chunking methods uti- lized in the original datasets respectively. GPT-4o- mini serves as both