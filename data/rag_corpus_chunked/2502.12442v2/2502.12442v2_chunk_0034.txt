innovations. As shown in Figure 7 , the main differences between HopRAG and GraphRAG are listed as follows. • Vertices. GraphRAG uses LLM to summarize the information from text chunks and then cre- ates additional vertices (e.g., event, organiza- tion and person) in the graph, while HopRAG directly stores the original chunks in the ver- tices and thus avoids LLM hallucination dur- ing summarization, information loss during entity extraction and overly dense graph struc- ture from redundant vertices. • Edges. GraphRAG connects vertices with pre-defined relationships like "part of" or "related", while HopRAG flexibly stores in- coming questions on the edges along with their keywords and embeddings, which can not only guide reasoning-augmented graph traversal but also facilitate edge retrieval. • Index. GraphRAG creates and stores em- beddings for the summarizations from LLM, while HopRAG creates sparse and dense in- dexes for both the vertices and the edges, which leads to more precise and efficient in- formation retrieval. In summary, the graph structure of HopRAG not only excavates logical relationships without creat- ing additional vertices, but also paves the way for reasoning-driven knowledge retrieval. A.7 Retrieval Efficiency We supplement more comprehensive analysis of retrieval efficiency, along with optimization strate- gies for further speedup. The main latency in Ho- pRAG’s retrieval comes from the LLM inference time during retrieval-augmented graph traversal. Since HopRAG with locally deployed Qwen2.5- 1.5B-Instruct as the traversal model also showcases competitive performances, we focus on the retrieval efficiency in this scenario. Following the hyper- Figure 5: Prompt for generating out-coming questions. parameters nhop = 4 and topk = 20 from our main experiments, each question requires calling LLM 38.53 times, where each LLM call involves selecting one edge from an average of 5.87 edges, with an input of around 500 tokens and an output of 20 tokens.