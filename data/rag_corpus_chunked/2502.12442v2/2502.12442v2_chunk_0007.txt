passages to truly relevant passages for better retrieval. Finally, with the query q and k passages as context C = {pi1, pi2, ..., pik }, the LLM generates the response O ∼ P (O|q, C). 3.2 Graph-Structured Index We construct a graph-structured index G = (V, E) where the vertex set V consists of vertices storing all the passages and the directed edge set E = {⟨vi, ei,j, vj⟩|vi, vj ∈ V} ⊂ V × V is established based on the logical relations between passages for multi-hop reasoning. To establish G, we utilize Query Simulation to identify the logical relations and leverage textual similarity for efficient Edge Merging. Figure 3: The workflow of HopRAG. Left: At indexing time, we first utilize Query Simulation to generate pseudo- queries for each passage and then apply Edge Merging to connect passages with directed logical edges. Right: At retrieval time, we employ a Retrieve-Reason-Prune pipeline. We first retrieve through purely similarity-based retrieval, then run reasoning-augmented graph traversal to explore the neighborhood, and finally prune the search by a novel metric Helpfulness considering both textual similarity and logical importance. Query Simulation To identify the logical rela- tions between passages, we generate a series of pseudo-queries for each passage, and use them to explore the passage’s relations with the others and bridge the inherent gap between user-queries and passages (Wang et al., 2024c). Specifically, we adopt LLM to generate two groups of pseudo- queries for each passage pi: (1) m out-coming questions Q+ i = S 1≤j≤m{q+ i,j} that originate from this passage but cannot be answered by itself; (2) n in-coming questions Q− i = S 1≤j≤n{q− i,j} whose answers are within the passage. As demonstrated in Figure 2, for the toy passage "Rose is the princess in the story The Frog Prince", one in-coming