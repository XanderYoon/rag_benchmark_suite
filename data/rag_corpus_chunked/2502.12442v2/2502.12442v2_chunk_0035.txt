efficiency in this scenario. Following the hyper- Figure 5: Prompt for generating out-coming questions. parameters nhop = 4 and topk = 20 from our main experiments, each question requires calling LLM 38.53 times, where each LLM call involves selecting one edge from an average of 5.87 edges, with an input of around 500 tokens and an output of 20 tokens. According to (Qwen Development Team, 2025), the output token speed for Qwen2.5- 1.5B-Instruct is about 40.86 token/s using BF16 and Transformer. Therefore, the additional latency for each question from LLM inference will be 38.53∗20/40.86 = 18 .86 seconds. However, there are many optimization strategies to improve the re- trieval efficiency. Using vLLM and GPTQ-Int4 techniques, the additional latency for each question can be reduced to 38.53 ∗ 20/174.04 = 4 .43 sec- onds. Moreover, parallelism techniques like mul- tithreading can further reduce the total execution time for all the queries. A.8 Discussion Results on nhop In the discussion we notice that as the hyper- parameter nhop varies from 1 to 4, the answer and retrieval performance both increase, along with the retrieval cost of calling LLM during traversal. Since the average queue length in the fifth hop is only as small as 1.23, we believe 4 is the idealnhop. The overall results are shown in Table 8. (a) Demonstration of one edge between two vertices. (b) Demonstration of reasoning- augmented graph traversal. Figure 6: Demonstration of the traversal in the graph structure (a) Demonstration of HopRAG’s graph. (b) Demonstration of GraphRAG’s graph. Figure 7: Visualizations of HopRAG and GraphRAG MuSiQue 2Wiki HotpotQA Average Answer Retrieval Answer Retrieval Answer Retrieval Answer Retrieval nhop EM F1 F1 Cost EM F1 F1 Cost EM F1 F1 Cost EM F1 F1 Cost 1 21.50 30.77 8.78 20.00 48.60 52.44 8.68 19.86 47.90 62.92 6.78