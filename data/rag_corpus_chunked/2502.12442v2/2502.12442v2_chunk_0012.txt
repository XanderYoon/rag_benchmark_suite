of these three datasets. See Appendix A.2 for details. Baselines We compare HopRAG with a variety of baselines: (1) unstructured RAG - sparse re- triever BM25 (Robertson and Zaragoza, 2009a) (2) unstructured RAG - dense retriever BGE (Xiao et al., 2024; Karpukhin et al., 2020) (3) unstruc- tured RAG - dense retriever BGE with query decomposition (Min et al., 2019) (4) unstruc- tured RAG - dense retriever BGE with reranking MuSiQue 2Wiki HotpotQA Average Method EM F1 EM F1 EM F1 EM F1 BM25 5.80 11.00 27.00 31.55 33.40 44.30 22.07 28.95 BGE 11.80 18.60 27.90 30.80 38.40 50.56 26.03 33.32 Query Decomposition 21.50 31.40 43.90 47.06 43.60 58.94 31.10 40.01 Reranking 24.50 34.53 46.70 50.89 47.70 62.95 34.67 43.60 HippoRAG 32.60 43.78 66.40 74.01 59.90 74.29 52.97 64.03 RAPTOR 35.30 47.47 54.90 61.20 58.10 72.48 49.43 60.38 SiReRAG 38.90 52.08 60.40 68.20 62.50 77.36 53.93 65.88 HopRAG 39.10 53.00 61.60 68.93 61.30 78.34 54.00 66.76 Table 1: We test our HopRAG against a series of baselines on multiple datasets using GPT-4o and GPT-3.5-turbo as the inference model with top 20 passages. We report the QA performance metrics EM and F1 score with GPT-3.5-turbo here and GPT-4o in Table 2, where the best score is inbold and the second best is underlined. MuSiQue 2Wiki HotpotQA Average Method EM F1 EM F1 EM F1 EM F1 BM25 13.80 21.50 40.30 44.83 41.20 53.23 31.77 39.85 BGE 20.80 30.10 40.10 44.96 47.60 60.36 36.17 45.14 Query Decomposition 29.00 38.50 55.70 60.57 52.80 68.67 47.46 55.91 Reranking 32.00 40.29 53.70 58.44 55.40 70.03 48.61 56.25 GraphRAG 12.10 20.22 22.50 27.49 31.70 42.74 22.10 30.15 RAPTOR 36.40 49.09 53.80 61.45 58.00 73.08 49.40 61.21 SiReRAG 40.50 53.08 59.60 67.94 61.70 76.48 53.93 65.83 HopRAG 42.20 54.90 61.10 68.26 62.00 76.06 55.10 66.40 Table 2: