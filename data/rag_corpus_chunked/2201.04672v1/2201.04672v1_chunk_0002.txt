results from diﬀerent graph models, we provide insights towards GNN model design for textual retrieval, with the hope to prompt more discussions on the emerging areas such as IR with GNNs. 2 GNNs for Document Retrieval 2.1 Overview In this section, we describe the process of GNN-based document retrieval. As is shown in Fig. 1, concept maps G ={V, E} are ﬁrst constructed for documents. Each node vi ∈ V is a concept (usually a word or phrase) in the document, associated with a frequency fi and an initial feature vectorai from the pretrained model. The edges in E denote the interactions between concepts. GNNs are then applied to each individual concept map, where node representation hi∈ Rd is updated through neighborhood transformation and aggregation. The graph-level embedding hG∈ Rd is summarized over all nodes with a read-out function. For the training of GNN models, the widely-used triplet loss in retrieval tasks [22,37,42] is adopted. Given a triplet ( Q, Gp, Gn) composed by a relevant document Gp (denoted as positive) and an irrelevant document Gn (denoted as negative) to the query Q, the loss function is deﬁned as: L(Q, Gp, Gn) = max{S(Gn| Q)− S(Gp| Q) + margin, 0} . (1) The relevance score S (G| Q) is calculated as hG·hQ ∥hG∥∥hQ∥, where hG is the learned graph representation from GNN models and hQ is the query representation from a pretrained model. In the training process, the embeddings of relevant documents are pulled towards the query representation, whereas those of the irrelevant ones are pushed away. For retrieval in the testing phrase, documents are ranked according to the learned relevance score S(G| Q). Graph Neural Networks for Document Retrieval 3 2.2 Concept Maps and Their Generation Concept map generation, which aims to distill structured information hidden un- der unstructured