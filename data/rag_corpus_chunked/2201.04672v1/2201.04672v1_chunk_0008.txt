on sketchy candidates. Therefore, two widely-used and simple models, the fore- mentioned BM25 and Anserini3, are adopted as baselines, instead of the heavier language models such as BERT-based [8, 9, 41] and learning to rank (LTR)- based [2,35] ones. The retrieval performance are shown in Table 2. All the values are reported as the averaged results of ﬁve runs under the best settings. For the structure-oriented GIN and GAT, diﬀerent read-out functions includ- ing mean, sum, max and a novel proposed tf-idf (i.e., weight the nodes using the tf-idf scores) are experimented, and tf-idf achieves the best performance. It is shown that GIN constantly fails to distinguish relevant documents while GAT is relatively better. However, they both fail to improve the baselines. This perfor- mance deviation may arise from the major inductive bias on complex structures, 3 https://git.uwaterloo.ca/jimmylin/covidex-trec-covid-runs/-/tree/master/round5, which is recognized by the competition organizers as a baseline result. 6 H. Cui et al. GIN GAT N-Pool E-Pool RW-Pool Graph Models 0.0 0.1 0.2 0.3 0.4 0.5NDCG@20 (a) Stability comparison 0 100 200 300 400 500 Training Epochs 0.0 0.1 0.2 0.3 0.4 0.5NDCG@20 GIN GAT N-Pool E-Pool RW-Pool (b) Eﬃciency comparison Fig. 2. Stability and eﬃciency comparison of diﬀerent graph models. which makes limited contribution to document retrieval and is easily misled by noises. In contrast, our three proposed semantics-oriented graph functions yield signiﬁcant and consistent improvements over both baselines and structure- oriented GNNs. Notably, E-Pool and RW-Pool improve the document retrieval from the initial candidates of BM25 by 11.4% and 12.0% on NDCG@20, respec- tively. Such results demonstrate the potential of designing semantics-oriented GNNs for textual reasoning tasks such as classiﬁcation, retrieval, etc. 3.4 Stability and Eﬃciency We further examine the stability and eﬃciency of diﬀerent models across runs. As is shown in Fig. 2(a), GIN and GAT