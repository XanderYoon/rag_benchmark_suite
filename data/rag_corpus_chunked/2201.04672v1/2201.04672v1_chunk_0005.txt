aj on its two ends to encode ﬁrst-order interactions, i.e., hG = READOUT ( {cat (MLP(ai), MLP(aj)) | eij ∈ E} ) . (3) 4 H. Cui et al. Table 1. The similarity of diﬀerent concept map pairs. Pair Type # PairsNCR (%) NCR+ (%) ECR (%) ECR+ (%) Pos-Pos 762,084 4.96 19.19 0.60 0.78 Pos-Neg 1,518,617 4.12 11.75 0.39 0.52 (t-score) - (187.041) (487.078) (83.569) (105.034) Pos-BM 140,640 3.80 14.98 0.37 0.43 (t-score) - (126.977) (108.808) (35.870) (56.981) – RW-Pool: for each sampled random walk pi = (v1, v2, . . . , vm) that encode higher-order interactions among concepts ( m = 2, 3, 4 in our experiments), the embedding is computed by the sum of all node embeddings on it, i.e., hG = READOUT ( {sum (MLP(a1), MLP(a2), . . . ,MLP(am)) | pi ∈ P } ) . (4) All of the three proposed graph functions are easier to train and generalize. They preserve the message passing mechanism of complex GNNs [11], which is essentially permutation invariant [15, 24, 25], meaning that the results of GNNs are not inﬂuenced by the orders of nodes or edges in the graph; while focusing on the basic semantic units and diﬀerent level of interactions between them. 3 Experiments 3.1 Experimental Setup Dataset We adopt a large scale multi-discipline dataset from the TREC-COVID1 challenge [29] based on the CORD-19 2 collection [33]. The raw data includes a corpus of 192,509 documents from broad research areas, 50 queries about the pandemic that interest people, and 46,167 query-document relevance labels. Experimental settings and metrics We follow the common two-step prac- tice for the large-scale document retrieval task [7, 19,28]. The initial retrieval is performed on the whole corpus with full texts through BM25 [30], a traditional yet widely-used baseline. In the second