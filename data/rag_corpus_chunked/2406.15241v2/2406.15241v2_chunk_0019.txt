(Contriever) represents the intervention of QZero using the dense retriever, and QZero BM25 represents the intervention of QZero using the sparse retriever. Models TagMyNews (%) AG News (%) Ohsumed (%) Yummly (%) DBpedia (%) Yahoo (%) Base +QZero BM25 +QZero CTV Base +QZero BM25 +QZero CTV Base +QZero BM25 +QZero CTV Base +QZero BM25 +QZero CTV Base +QZero BM25 +QZero CTV Base +QZero BM25 +QZero CTV Word2Vec 46.37 +11.75 +13.00 65.24 +9.84 +11.06 18.35 +5.00 +3.22 4.36 +38.23 +21.41 69.00 -0.72 +1.37 42.63 +5.09 +6.17 GloVe 38.84 +17.52 +18.06 60.59 +9.46 +10.78 12.64 +9.37 +5.61 16.62 +16.30 +4.08 61.72 +12.46 +14.96 46.72 -0.98 +0.30 FastText 44.30 +14.73 +15.04 69.96 +8.33 +8.93 8.66 +12.23 +11.23 8.26 +26.89 +17.69 70.86 -0.59 +1.33 37.36 +9.19 +11.18 All-mpnet 49.85 +8.86 +9.21 76.28 +4.65 +4.61 46.27 +0.38 -2.42 27.10 +17.54 +6.84 78.02 +2.03 +4.05 52.65 -6.55 -6.55 TE-3-small 50.10 +10.60 +12.23 72.90 +4.57 +4.17 34.55 +0.15 -2.69 37.47 -2.75 +0.28 74.16 -3.02 -0.04 50.49 -5.94 -7.63 TE-3-large 55.81 +4.85 +6.61 76.93 -1.07 -1.57 37.45 +5.53 +2.59 54.36 -9.26 -13.32 78.50 -5.13 -3.30 53.28 -3.54 -3.44 for All-mpnet-base-v210. Similarly, GPT-3 models are trained on massive-scale datasets that might already contain the information encoded in the reformulated queries. As a result, applying QZero for such datasets might introduce redundancy or contradict exist- ing knowledge within these larger models, leading to performance drops. 5.2 Eﬀect of Dense vs. Sparse Retrieval Models The /f_indings presented in Table 4 highlight QZero’s robustness, showcasing its compatibility with dense and sparse retrievers. The BM25 (sparse) retriever performs better on the Ohsumed dataset, which contains lengthy documents that most dense neural retriev- ers might struggle with. Interestingly, the BM25 retriever also out- performs the Contriever (dense) retriever on the Yummly dataset. This could be because Contriever’s training data was not well- suited for