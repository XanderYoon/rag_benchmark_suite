the Red Sox apparently bitter about the way he was treated by management, " aiding in interpreting the model’s predictions. 10training datasets can be found: via:https://huggingface.co/sentence-transformers/all- mpnet-base-v2 Our analysis of incorrect predictions reveals a few key sources of error. In some cases, the predicted topic and the annotated ground truth might diﬀer, but both could be valid interpretations of the query’s meaning. Alternatively, the ground truth itself could be inaccurate. For example, in Table 5, the query "Why do I not walk correctly when I have sinusitis? Your equilibrium could be ‘oﬀ’ due to your sinusitis, which could cause problems with your inner ear" clearly has no connection to the ground truth, "Business & Finance. " Other errors stem from retrieving irrelevant categories or limitations of the embedding model itself. By understanding these nuances, we can leverage the model’s capabilities to interpret its predictions, re/f_ine our evaluation methods, and ultimately enhance model accuracy. 5.4 Eﬀect of Number of Retrieved Documents Figure 2 shows how QZero’s performance changes with respect to the number of documents retrieved for reformulating queries. This applies to both GloVe and All-mpnet embedding models. We see a trend across all datasets: as more documents are retrieved initially, QZero’s performance improves. However, once the number of retrieved documents exceeds 50, the accuracy plateaus in the case of the All-mpnet model due to the /f_ixed number of maximum tokens the model can take as input. On the other hand, the accuracy of the GloVe embedding model starts to decline. This suggests that there’s a point of diminishing return where retrieving more documents starts to include irrelevant ones. Retrieving too many documents dilutes the pool of relevant categories that appropriately describe the input query, ultimately reducing QZero’s eﬀectiveness. 6 CONCLUSIONS AND FUTURE WORK Embedding models present an eﬀective solution for