computation. This has necessitated the widespread adoption of zero-shot text classi/f_ication. Generative Large Language Models (LLMs) have revolutionized the /f_ield of natural language processing [3, 23], especially for zero- shot learning. Their remarkable zero-shot abilities enable them to tackle diverse tasks with impressive eﬃciency and adaptability. However, applying generative LLMs directly for zero-shot text clas- si/f_ication tasks presents challenges due to their massive size and computational demands. These models require signi/f_icant compu- tational resources for inference, which can render them impractical in resource-constrained settings. In addition, these models tend to make predictions that are independent of user-speci/f_ied classes [35]. For instance, when classifying cuisine as Chinese or Mexican, a gen- erative LLM model might incorrectly predict Italian, even though it wasn’t an option. This lack of user control over the classi/f_ication process poses a signi/f_icant limitation to achieving targeted results. A straightforward and eﬃcient alternative for zero-shot text classi/f_ication involves assigning a class (or label) to a query (or text) by comparing the embeddings of the query and potential classes using a distance metric such as cosine similarity [15, 27]. This rela- tively cheap approach removes the need for retraining or additional data labeling and allows for control over the classi/f_ication process. However, when applied to queries that do not explicitly re/f_lect the context of the class or align well with the model’s training data, the accuracy of this technique decreases. For example, consider Table 1, which illustrates the contrast between explicit and implicit queries for a text input whose ground truth class is Technology. In the explicit example, keywords like "Arti/f_icial Intelligence" di- rectly suggest Technology as the correct class. In contrast, the implicit example does clearly indicate the ground truth class. Here, a model’s ability to infer Technology heavily relies on prior knowl- edge stored within its representations.