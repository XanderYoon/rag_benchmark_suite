keywords, we assign weights /u1D464to them based on their frequency across the reformulated query, facilitating eﬀec- tive measurement of each keyword’s importance (since keywords will be repeated across the reformulated query). This results in a re- /f_ined query where each keyword is paired with its weight, forming a structured representation as a list of tuples expressed as: /u1D465ĭ Ď = ((/u1D43E1, /u1D4641), (/u1D43E2, /u1D4642), . . . , (/u1D43EĤ , /u1D464Ĥ )) In the equation, /u1D465ĭ Ď represents the reformulated word represen- tation that will be used as input into the static word embedding model, and (K, w) is the keyword and its corresponding weight. 3.6 Zero-shot Text Classi/f_ication We explore zero-shot text classi/f_ication using both contextual em- bedding and static word embedding models (Table 3) to leverage the distinct advantages of each embedding type. Contextual em- beddings capture the overall meaning of the reformulated query, while static word embeddings allow for /f_iner-grained analysis of the individual keywords in the reformulated query. Both approaches are explained as follows: Retrieval Augmented Zero-Shot Text Classification ICTIR ’24, July 13, 2024, Washington DC, DC, USA • Contextual Embedding Models: To perform zero-shot classi/f_ication, we use the contextual embedding models to obtain embeddings for the reformulated query /u1D465ĩ Ď and each potential class label. Next, we compute the cosine similarity (a measure of closeness between embeddings) between /u1D465ĩ Ď and each class. The class with the highest cosine similarity is assigned to /u1D465ĩ Ď (See Figure 1). • Static Word Embedding Models: First, we compute the embedding of each class label. For class labels with single words, we obtain their representation directly from the em- bedding model, while we average the representations of the constituent words within the label if it is a phrase. We also obtain the representation for each keyword /u1D43Ein the