ensure a thorough examination of textual nuances across diverse domains, we evaluated QZero on six distinct publicly available text clas- si/f_ication datasets. These include the AG News articles [36] and the DBPedia factual knowledge base [17], which contains a sum- mary of Wikipedia extracts. We also utilize the Yahoo! Answers community-driven knowledge exchange [36], and the Yummly dataset [13] of recipes from various regional cuisines, obtained from Kaggle’s What’s cooking challenge 4. Additionally, we em- ployed the TagMyNews dataset [13] 5, containing news from RSS feeds as adopted by [18], and the Ohsumed corpus [10] 6, a collec- tion of medical abstracts about various diseases. We used labels similar to [15, 7, 13, 18], which we show in Table 2. We report the classi/f_ication accuracy averaged over three runs on the test sets. We summarize all dataset statistics in Table 2. 4.2 Zero Shot Models and Baselines We evaluated the impact of the QZero pipeline on embedding mod- els by comparing their performance utilizing the original input versus reformulated queries. Six diﬀerent static word and contex- tual embedding models (See Table 3) were tested. • Zero-shot classi/f_ication via Contextual Embeddings: The ap- proach is similar to the contextual embedding classi/f_ication method described in Section 3.6. The only diﬀerence is that instead of the reformulated query, we compare the cosine similarity between the embeddings of the original text input and class labels to achieve zero-shot classi/f_ication. • Zero-shot classi/f_ication via Static Word Embeddings: The key diﬀerence between the baseline static word embedding approach in Section 3.6 and this baseline approach is the absence of weights in the input to be classi/f_ied. This means we compared each class label to the average vector represen- tation of words in the original input text. We accessed the text-embedding-3 small and large models through the