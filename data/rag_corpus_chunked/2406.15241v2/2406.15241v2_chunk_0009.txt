reformulated query becomes the con- catenation of the retrieved categories. Subsequently, the embedding of the reformulated query is compared to the embeddings of the potential classes using cosine similarity. The class with the highest cosine similarity becomes the model’s prediction. In this section, we describe the components of the QZero pipeline in detail. 3.2 Knowledge Corpus We used Wikipedia 2 as our knowledge corpus across all exper- iments. Wikipedia articles oﬀer concise information on a wide range of subjects, organized into categories located at the bottom of the page. These categories serve as topics and keywords associated with an article. We indexed English Wikipedia articles, focusing on article content and categories. Articles with fewer than 20 words and those with no assigned categories were excluded, resulting in 5.85 million documents with at least one category. 3.3 The Retrieval System The QZero scheme can be used with arbitrary retrieval systems. To build our retrieval system, we constructed a one-time index of the selected Wikipedia articles, leveraging it for subsequent retrieval of relevant article categories. Each article, represented as a unique document, contains attributes such as content and categories. For classi/f_ication tasks, the retrieval process focuses solely on returning the categories of the best-matched articles, ensuring a concise presentation of information. 3.4 Keyword Extraction Wikipedia category names are usually lengthy and more detailed than conventional label names, which poses a challenge for static word embedding models. For such models, we used diﬀerent key- word extraction strategies to harness the extensive information encapsulated in the Wikipedia category as described below: • SpaCy’s POS tagging 3: it identi/f_ies keywords within a sen- tence by breaking down the sentence into tokens then, it employs a trained statistical model to analyze each token and assign it a Part-of-Speech (POS) tag. We selected only Noun tokens as