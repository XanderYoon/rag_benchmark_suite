[29] contextual text-embedding-3-small (TE-3-small) contextual text-embedding-3-large (TE-3-large) contextual dataset, the smallest model (Word2vec) experienced a signi/f_icant 13.00% boost in accuracy, while even the largest model (TE-3-large) saw a 6.61% increase. Similarly, in the AG News dataset, all models achieved a minimum accuracy gain of 4.17%, except for TE-3-large, which exhibited a slight 1.57% drop. This drop in TE-3-large’s per- formance suggests potential noise introduced by uninformative categories in the reformulated query. QZero enhances smaller models to achieve performance com- parable to larger models without QZero. In TagMyNews, a QZero- enhanced Word2vec outperformed TE-3-large and TE-3-small (with original input) by substantial margins of 3.56% and 9.27%, respec- tively. Similarly, in the AG News dataset, Word2vec outperformed TE-3-small by 3.4% while achieving similar accuracy with TE-3- large. This is particularly valuable for scenarios with limited com- putational resources or tight /f_inancial constraints, where utilizing OpenAI’s expensive embeddings might not be feasible. Furthermore, QZero enriches the original input with useful con- text information that may be outside of the model’s training data. For example, in the Ohsumed disease topic dataset, TE-3-large and Word2vec (a model with limited medical knowledge) achieved a minimum of 5.00% increase in accuracy. On the Yummyly recipe datasets, the static word embedding models achieved a boost as high as approximately 38.00%. In addition, even the All-mpnet-base-v2 model, also lacking training data in the culinary domain, improved by 17.54% on the Yummyly recipes. This is impressive considering the limited medical and culinary domain information present in the general Wikipedia corpus, which was the only Knowledge corpus for QZero. These results highlight the promising potential of QZero as a cost-eﬀective solution for domain adaptation challenges. Our results also demonstrate that retrieval-augmented query re- formulation is eﬀective for classifying topics outside of the model’s training data. By transforming the input query into