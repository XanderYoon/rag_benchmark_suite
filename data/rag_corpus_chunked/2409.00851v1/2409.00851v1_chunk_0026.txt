obtain a bigger drop on the ğ‘Ÿğ‘’ğ‘£ and ğ‘Ÿğ‘’ğ‘ test subsets. This indicates that although Clotho contains considerably more simultaneous actions and the number of future and past temporal cues is greatly dispro- portionate, the loss still helps the model better focus on temporal cues when present. This is observed in Tab. 4. 6 Conclusion In this work, we dissected the temporal understanding capabilities of a current state-of-the-art text-audio model. We first performed an in-depth analysis of the AudioCaps and Clotho text-audio datasets. We concluded that these datasets are not well-suited for obtaining and evaluating temporal understanding capabilities in a text-audio retrieval model. As a result, we proposed a variant of the Audio- Caps dataset, namely AudioCapsğ‘¢ğ‘›ğ‘– , that contains a more uniform distribution of different temporal cues. We then showed that using AudioCapsğ‘¢ğ‘›ğ‘– reduces biases learnt by the model and improves performance on the AudioCapsğ‘¢ğ‘›ğ‘– test set. Furthermore, we intro- duced a synthetic dataset (SynCaps), showing that indeed models fail to use the temporal cues even in a controlled data setting. Lastly, we proposed a simple loss that results in better text-to-audio re- trieval results on SynCaps, whilst also putting more emphasis on the temporal content of the audio and text data in all datasets analysed. Acknowledgments This work was supported by an EPSRC DTA Studentship, by the Royal Academy of Engineering (RF\201819\18\163), by the DFG: SFB 1233, project number: 276693517, and by the DFG EXC number 2064/1 â€“ project number 390727645. We are very grateful to Samuel Albanie and Bruno Korbar for helpful feedback and suggestions. Dissecting Temporal Understanding in Text-to-Audio Retrieval MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia References [1] Max Bain, Arsha Nagrani, GÃ¼l Varol, and Andrew Zisserman. 2021. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. In IEEE International