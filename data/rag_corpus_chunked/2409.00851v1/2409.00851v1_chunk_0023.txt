This leads to 485 test examples of 10 second long audio clips. For training and validation, we allow the same 5 seconds sound component to appear on average five times. We apply five different types of augmentation that include time shifting, volume adjustment, pitch shift, time stretch, and the MM ‚Äô24, October 28-November 1, 2024, Melbourne, VIC, Australia Oncescu, et al. addition of noise. We also allow for an overlap between the files of up to one second. This results in a total of 4400 training samples, 485 validation samples, and 485 test samples. 4.2 SynCaps experiments We analyse the temporal understanding of the text-audio model in the controlled setting of the SynCaps dataset. For this, we take the same pre-trained model from [24] and finetune it on SynCaps using the original Lùë°ùëé loss. We observe that evaluating on the ‚Äòreversed‚Äô(rev) and ‚Äòreplaced‚Äô (rep) datasets gives almost the same results as using the correct (original) test data (left half of Tab. 6). This shows that the model indeed does not understand temporal cues even on a simple dataset. 5 Text-text contrastive loss We propose a loss function Lùë°ùë° that aims to enhance the under- standing of temporal information. It is formulated as a text-text contrastive loss, which relies on pairs of positive examples (that have the same temporal significance as the original sentence) and negative text examples (that have the opposite temporal meaning). Concretely, given the original descriptionBird sings followed by dog barks, one positive example is Bird sings before dog barks and one negative example would be Bird sings after dog barks. We provide the model with two positive text examples and two negative text examples for each text description containing the previously defined future and past temporal textual cues. Positive and negative text examples can be generated once,