proven beneficial on the text-to-audio retrieval benchmarks [24, 35, 43]. As the annotation of audio files with descriptions is time consuming, some of the text-audio pairs collected by [ 35] and [ 24] contain short audio labels instead of descriptions. To overcome this, [35] employed the T5 [29] model to generate descriptions starting from audio labels, whilst [24] used ChatGPT [27]. [24] also used ChatGPT to clean audio descriptions from datasets such as BBC Sound Effects 1 by removing vision- based content. Another line of works considered metric learning objectives for text-to-audio retrieval [ 23, 37]. Other concurrent research pushed the text-to-audio retrieval results even further by training models with additional modalities, such as video and speech [6, 32]. Recently, [25] introduced new text-to-audio retrieval benchmarks on egocentric video data. Text-audio grounding. [38] proposes a new set of data annota- tions for a subset of the AudioCaps dataset [ 16] with the aim of grounding each sound to a time interval. For this, annotators la- belled the start and end times of all relevant sounds in each audio clip. [39, 40] investigated the task of weakly supervised text-to- audio grounding. The audio grounded dataset has also been used for learning to align sounds and text in an unsupervised manner [36]. [3] used the grounded sounds to introduce new metrics for audio 1https://sound-effects.bbcrewind.co.uk/ Dissecting Temporal Understanding in Text-to-Audio Retrieval MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia captioning. In this work, we use this subset to provide an empiri- cal evaluation of the quality of existing AudioCaps captions. More specifically, we give the grounded sounds and their corresponding AudioCaps descriptions to an LLM and ask it to evaluate if the descriptions are correct and complete. Temporal understanding in text-audio models. [34] show that text-audio models do not pay attention to