temporal understanding in current state-of-the-art models is caused by the training data or by the model architecture. To determine whether commonly used text-audio datasets, such as AudioCaps [16] and Clotho [ 9], are suitable for training and evaluating the ability of current models to comprehend time, we examine the relative distribution of audio descriptions that con- tain temporal cues. In particular, we plot the frequency of specific temporal cues in relation to the total number of descriptions. Our analysis shows that both the AudioCaps and Clotho datasets suffer from biases caused by the way humans describe events. That is, we tend to describe events in the order they appear. When first hearing the sounds of a dog barking and then the sound of a human speaking, we describe this as â€˜A dog barking followed by a human speakingâ€™ rather than â€˜A dog barking before a human speaksâ€™. To try to address the lack of some temporal examples, in [ 34], the authors generate new text-audio pairs that enhance the existing text-audio data. They concatenate the audio files in a specific order and then generate a description that reflects that. For instance, if the generated sound isğ‘†ğ‘œğ‘¢ğ‘›ğ‘‘ 1, ğ‘†ğ‘œğ‘¢ğ‘›ğ‘‘2, the description is â€˜<Original description of ğ‘†ğ‘œğ‘¢ğ‘›ğ‘‘ 1> before <Original description of ğ‘†ğ‘œğ‘¢ğ‘›ğ‘‘ 2>â€™. This increases the size of the training data by 40%. Different to [34], we rephrase existing text descriptions to obtain a more uniform distribution of textual temporal cues whilst preserving the content (AudioCapsğ‘¢ğ‘›ğ‘– ). Furthermore, we investigate the impact of more uniform training data on the text-to-audio retrieval performance, reporting results on the original test data and on rephrased test data (TempTestğ‘Ÿğ‘’ğ‘£ and TempTestğ‘Ÿğ‘’ğ‘ ). The rephrasing of text descriptions is illustrated in Fig. 1. Furthermore, we present an empirical evaluation of the correct- ness and completeness of