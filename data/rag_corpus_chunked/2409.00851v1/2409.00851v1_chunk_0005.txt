better analysis of temporal understanding in existing models. This uniform version of AudioCaps keeps the audios intact and only requires changing the text descriptions. We provide benchmarks and an analysis of the behaviour of one current state-of-the art model on the original and more uniform versions of this dataset. (iii) We propose a synthetic dataset and use it to evaluate the modelâ€™s understanding of time. (iv) We investigate an additional loss term to encourage the model to focus on text-based temporal cues. 2 Related work Text-to-audio retrieval.Text-to-audio retrieval involves matching a textual query with its most relevant audio file in a database of audio samples. The task of searching through audio databases can be approached in multiple ways. One simple approach is to match the text query with the title or the metadata of the audio file, provided it exists. However, for unlabelled databases, the aim is to find an audio file that has the content specified by the user through a text query. This is called semantic search. For many years, text-audio semantic retrieval has used audio class labels that consist of individual or few words as text queries [11, 12, 30, 33]. More recently, [18, 26] proposed new benchmarks where the text query is a free-form text description rather than a pre-defined class label, allowing for more control over the retrieved audio content. Collecting new text- audio pairs for training and using state-of-the-art transformer- based audio encoders has proven beneficial on the text-to-audio retrieval benchmarks [24, 35, 43]. As the annotation of audio files with descriptions is time consuming, some of the text-audio pairs collected by [ 35] and [ 24] contain short audio labels instead of descriptions. To overcome this, [35] employed the T5 [29] model to generate descriptions starting from audio labels, whilst [24] used ChatGPT