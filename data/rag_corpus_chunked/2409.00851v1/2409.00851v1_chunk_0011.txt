empirically evaluate the completeness and quality of the de- scriptions in AudioCaps with grounded sound sources, we use an LLM, specifically GPT-4 [27]. We provide the LLM with the Audio- Caps description, the grounded sources and their time intervals. We use one-shot prompting to give the model an example, such that it better understands the task. We then task the LLM to decide if the AudioCaps description is ‘correct’, ‘incomplete’, or ‘wrong’ based on the sound source information. Details for our prompt are shown in Tab. 1. To check how reliable the GPT-4 outputs are for this task, we manually checked 40 randomly selected descriptions, their grounded sounds and the GPT-4 evaluation. We found that the GPT-4 evaluation was correct 85% of the time, with the major- ity of the mistakes being in favour of AudioCaps, i.e. evaluating a sentence as correct when in fact it was incomplete. We show the resulting proportions of correct, incomplete, and wrong descrip- tions as identified by the LLM in the subset of AudioCaps in Tab. 2. On average, 23% of the descriptions are incomplete or wrong. This percentage increases for descriptions containing future and past temporal cues. The use of future and past refers to the fact that if ‘Sound 1’ and ‘Sound 2’ are connected by afuture temporal cue, then that means that ‘Sound 1’ comes first and is followed by ‘Sound 2’. If a past cue is used, then ‘Sound 1’ comes after ‘Sound 2’, e.g. ‘Bird sings after dog barks’.Future cues include ‘Followed by’, ‘Before’ and ‘Then’, e.g. ‘Bird sings before dog barks’. Forpast, we consider ‘Preceded by’ and ‘After’. Based on the significant proportion of incomplete or wrong descriptions, and the distribution of temporal textual cues, we conclude that AudioCaps is not well-suited for analysing if text-audio models understand