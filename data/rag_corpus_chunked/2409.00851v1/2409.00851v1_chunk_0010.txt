et al. ‘Before’ and ‘After’. However, in [34], this distribution describes their training and test data which combines multiple datasets in- cluding AudioCaps [16] and Clotho [9]. Here, we consider all words in the AudioCaps descriptions that represent temporal ordering. Given the distribution in Fig. 2, expecting a model trained on this data to understand the meaning of reverse temporal prepositions is unreasonable. At the same time, the test data also suffers from the same problem, therefore, using AudioCaps benchmarks for deciding if models understand temporal ordering is not optimal either. Next, we empirically evaluate the correctness and completeness of AudioCaps descriptions by using the grounded sound time inter- vals provided by [38]. Through manual inspection, we notice that many AudioCaps descriptions are composed of multiple sounds. For instance, a 10 second audio file with a bird singing from second 0 to second 6 and a dog barking from second 4 to second 10 can be described as ‘Bird singing and/as dog barks’. Alternatively, this could be described as ‘Bird singing followed by dog barking’. Both descriptions are correct, however, a more complete version of these descriptions would be, for example, ‘Bird singing, soon joined by a dog barking. Their sounds overlap briefly before the bird stops, while the dog continues barking. ’. If the description is not complete, however, how could a model learn the difference between ‘as’ and ‘followed by’ when they describe the same audio clip? To empirically evaluate the completeness and quality of the de- scriptions in AudioCaps with grounded sound sources, we use an LLM, specifically GPT-4 [27]. We provide the LLM with the Audio- Caps description, the grounded sources and their time intervals. We use one-shot prompting to give the model an example, such that it better understands the task. We then task the