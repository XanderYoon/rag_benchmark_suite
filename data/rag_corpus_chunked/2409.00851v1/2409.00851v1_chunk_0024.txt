by dog barks, one positive example is Bird sings before dog barks and one negative example would be Bird sings after dog barks. We provide the model with two positive text examples and two negative text examples for each text description containing the previously defined future and past temporal textual cues. Positive and negative text examples can be generated once, before training the model. We searched for the temporal cues we are interested in and automatically generated multiple positives and negatives by changing the temporal cues and/or the ordering of the sounds. The contrastive loss for each query and a margin ğ›¼ is: Lğ‘¡ğ‘¡ = 1 2ğ‘ ğ‘âˆ‘ï¸ ğ‘›=1 2âˆ‘ï¸ ğ‘˜=1 max(0, ğ›¼ âˆ’ ğ‘ ğ‘¡ğ‘›ğ‘¡ğ‘˜,ğ‘ğ‘œğ‘  + ğ‘ ğ‘¡ğ‘›ğ‘¡ğ‘˜,ğ‘›ğ‘’ğ‘” ), (3) where ğ‘ ğ‘¡ğ‘›ğ‘¡ğ‘˜,ğ‘ğ‘œğ‘  is the cosine similarity (see Eq. 1) between the ğ‘›- th text query and its ğ‘˜-th positive example, ğ‘ ğ‘¡ğ‘›ğ‘¡ğ‘˜,ğ‘›ğ‘’ğ‘” is the cosine similarity between the ğ‘›-th query and its ğ‘˜-th negative example. Our full loss then becomes: L = Lğ‘¡ğ‘ + ğœ†Lğ‘¡ğ‘¡ . (4) In our experiments that use the text-text contrastive loss, we set ğœ† = 10, and ğ›¼ = 0.2. 5.1 Performance using text-text contrastive loss We evaluate the same model pre-trained on the joint WavCaps+ AudioCaps + Clotho, and finetuned on SynCaps, AudioCapsğ‘¢ğ‘›ğ‘– and Clotho using our additional text-text contrastive loss. For SynCaps, we observe in Tab. 6, that the model performs better on the origi- nal test set when the additional text-text contrastive loss is used, whilst at the same time showing a big drop in performance on the â€˜reversedâ€™ and â€˜replacedâ€™ data. This shows that employing a simple additional loss can help the model better understand time, at least in the controlled setting of the SynCaps dataset. When finetuning the WavCaps-pretrained model on AudioCapsğ‘¢ğ‘›ğ‘– with the text-text contrastive loss, we see in the right-hand