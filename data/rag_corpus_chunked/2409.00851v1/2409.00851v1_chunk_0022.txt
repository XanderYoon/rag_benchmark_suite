and ğ‘Ÿğ‘’ğ‘ compared to TempTest). Subset Loss Tâ†’A A â†’T Loss Tâ†’A A â†’T R@1 R@1 R@1 R@1 Test Lğ‘¡ğ‘ 43.52 53.40 +ğœ†Lğ‘¡ğ‘¡ 42.00 52.25 TempTest Lğ‘¡ğ‘ 50.71 62.41 +ğœ†Lğ‘¡ğ‘¡ 48.32 59.37 TempTestğ‘Ÿğ‘’ğ‘£ Lğ‘¡ğ‘ 46.82 59.43 +ğœ†Lğ‘¡ğ‘¡ 39.38 52.43 TempTestğ‘Ÿğ‘’ğ‘ Lğ‘¡ğ‘ 46.99 58.58 +ğœ†Lğ‘¡ğ‘¡ 38.42 51.52 4.1 Data generation We use the ESC-50 [28] environmental sound classification dataset to generate a synthetic dataset for text-to-audio retrieval with a focus on temporal understanding capabilities. ESC-50 is a dataset of 2000 audio samples from 50 classes. As this dataset is clean and contains â€˜atomicâ€™ sounds (i.e. 5 second audios containing only one sound), we use it for synthetic data generation. We first task an LLM to take the sound labels from ESC-50 and generate textual descriptions in the style of AudioCaps (e.g. â€˜dogâ€™â†’ â€˜dog barkingâ€™). To generate the text-audio pairs, we take two sounds and their LLM-generated labels and concatenate them based on an arbitrarily selected temporal order (see Fig. 1 in the middle). We call this dataset SynCaps. To avoid any confusion, we only use future and past temporal cues. This is because synchronous temporal cues such as â€˜asâ€™ or â€˜dur- ingâ€™ are ambiguous, especially in a noisily labelled dataset. They can be used for sounds that completely overlap, or for partial overlaps of sounds, ignoring the actual order in which the sounds appear. The test set contains unique sound components that are not used in the training and validation sets. This leads to 485 test examples of 10 second long audio clips. For training and validation, we allow the same 5 seconds sound component to appear on average five times. We apply five different types of augmentation that include time shifting, volume adjustment, pitch shift, time stretch, and the MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Oncescu, et