a controlled evaluation setting. Right: Our text-text contrastive loss improves temporal understanding using positive descriptions (green) for the same sound ordering and negative examples (red) for the opposite temporal meaning. are incomplete or incorrect. This can contribute to models trained on AudioCaps not being able to understand temporal ordering. To gain further insights into the temporal understanding capa- bilities, we propose a synthetic dataset that provides a controlled setting for analysing text-audio models. This dataset contains 10 second long audios to be consistent with the general setting that current models have been trained on. We show that the considered model struggles to use temporal cues in the synthetic dataset, too, confirming the findings from [34] in a controlled setting. This al- lows us to decouple the bad temporal performance of the model from the data not being suited for the task. Lastly, we propose a simple text-based contrastive loss function (see Fig. 1) and show that it results in the model paying more attention to the temporal ordering of events. This gives improvements in the overall retrieval results on the synthetic dataset. In summary, we make the following contributions: (i) We show why existing text-to-audio retrieval datasets are not good indicators of a text-audio modelâ€™s ability to understand temporal ordering, (ii) We propose a more uniform version of AudioCaps that is better suited for obtaining temporal understanding in models trained on this data. Additionally, its test subset allows for a better analysis of temporal understanding in existing models. This uniform version of AudioCaps keeps the audios intact and only requires changing the text descriptions. We provide benchmarks and an analysis of the behaviour of one current state-of-the art model on the original and more uniform versions of this dataset. (iii) We propose a synthetic dataset and use it to evaluate