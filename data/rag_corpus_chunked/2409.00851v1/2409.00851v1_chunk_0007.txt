work, we use this subset to provide an empiri- cal evaluation of the quality of existing AudioCaps captions. More specifically, we give the grounded sounds and their corresponding AudioCaps descriptions to an LLM and ask it to evaluate if the descriptions are correct and complete. Temporal understanding in text-audio models. [34] show that text-audio models do not pay attention to temporal cues in text queries, such as ‘followed by’, or ‘after’. One example of an experi- ment in [34] is replacing temporal cues with words that represent a wrong ordering, e.g. replacing ‘then’ with ‘as’. Then, the model’s performance on the ‘wrong’ descriptions is evaluated, revealing that this performance is similar to when the temporal ordering in the text queries is correct. In their study, [34] utilize CNNs for audio process- ing and identify a critical limitation of CNN-based models: applying temporal pooling across all embeddings can result in the loss of tem- poral information. To mitigate this issue, they augment the CNN architecture with several transformer layers to preserve temporal dynamics. In contrast, contemporary models built on transformers inherently incorporate mechanisms to handle temporal data more effectively. Different to [34], we investigate the temporal under- standing of a transformer-based state-of-the-art audio-text retrieval model. In particular, we analyse if a transformer-based model also ignores temporal cues. Additionally, the approach proposed by [34] for helping models better understand time does not improve the overall performance on downstream retrieval benchmarks. In this work, we investigate an alternative for guiding the model to fo- cus on temporal cues. Furthermore, we present a detailed analysis of descriptions in text-audio datasets in the context of temporal understanding. Concurrent work [41] claims that commonly used text-audio datasets only contain simple audio descriptions that lack information about temporal cues, the number of times a sound can be