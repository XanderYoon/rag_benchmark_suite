work, we investigate an alternative for guiding the model to fo- cus on temporal cues. Furthermore, we present a detailed analysis of descriptions in text-audio datasets in the context of temporal understanding. Concurrent work [41] claims that commonly used text-audio datasets only contain simple audio descriptions that lack information about temporal cues, the number of times a sound can be heard, or details about sounds overlapping. To address this, [41] introduce a synthetic text-audio datasets by merging ‘atomic’ sounds in a controlled way. Differently from [41] that focuses on more varied audio details such as loudness, or number of times a sound can be heard, we generate audio files and descriptions that showcase clear temporal relations between the composed sounds. In addition to that, [41] uses an LLM to generate descriptions which are varied but could include hallucinated content in contrast to the descriptions in our synthetic dataset which are rule-based. 3 Analysis of temporal understanding in text-to-audio retrieval In this section, we take a close look at the AudioCaps and Clotho datasets in the context of understanding temporal information. We present a detailed evaluation of text-audio retrieval models on those datasets. 3.1 AudioCaps dataset The AudioCaps [16] dataset contains paired audio clips and text descriptions. The training set consists of one text description for each audio file. The validation and test sets contain five descriptions for each audio file. In the AudioCaps evaluation setting, if at least one of the five text descriptions matches the audio clip, it counts as 100% retrieval accuracy. Figure 2: Distribution of temporal conjunctions and prepo- sitions in the full AudioCaps [ 16] dataset. Most temporal sentences contain future temporal cues, such as ‘Followed by’. There is only a small proportion ofpast cues, e.g. ‘Before’. Figure 3: Distribution of temporal conjunctions and preposi- tions