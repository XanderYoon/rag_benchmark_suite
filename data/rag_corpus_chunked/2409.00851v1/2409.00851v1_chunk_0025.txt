used, whilst at the same time showing a big drop in performance on the â€˜reversedâ€™ and â€˜replacedâ€™ data. This shows that employing a simple additional loss can help the model better understand time, at least in the controlled setting of the SynCaps dataset. When finetuning the WavCaps-pretrained model on AudioCapsğ‘¢ğ‘›ğ‘– with the text-text contrastive loss, we see in the right-hand side of Tab. 5 that the drop is larger between the TempTest and theğ‘Ÿğ‘’ğ‘£ Table 6: Text-to-audio and audio-to-text retrieval on the Syn- Caps dataset for the model fine-tuned on SynCaps with the text-audio loss Lğ‘¡ğ‘ , and with the text-text loss Lğ‘¡ğ‘¡ . When using only Lğ‘¡ğ‘ , there is almost no drop in performance on the wrongly ordered test sets ğ‘Ÿğ‘’ğ‘£ , ğ‘Ÿğ‘’ğ‘ . With the additional Lğ‘¡ğ‘¡ , the drop becomes much more significant, confirming that the model understands temporal ordering. Subset Loss T â†’A A â†’T Loss T â†’A A â†’T R@1 R@1 R@1 R@1 Test Lğ‘¡ğ‘ 67.22 65.23 +ğœ†Lğ‘¡ğ‘¡ 69.35 69.90 Testğ‘Ÿğ‘’ğ‘£ Lğ‘¡ğ‘ 67.35 65.84 +ğœ†Lğ‘¡ğ‘¡ 40.55 41.03 Testğ‘Ÿğ‘’ğ‘ Lğ‘¡ğ‘ 66.94 63.92 +ğœ†Lğ‘¡ğ‘¡ 44.33 45.43 and ğ‘Ÿğ‘’ğ‘ test sets than when just using the text-audio original loss function. At the same time, the performance on the full Testğ‘¢ğ‘›ğ‘– and TempTest remains competitive when using the additional loss as compared to only using the original one. Lastly, we show that on the original form of the Clotho dataset, when finetuning using the text-text contrastive loss, we obtain a bigger drop on the ğ‘Ÿğ‘’ğ‘£ and ğ‘Ÿğ‘’ğ‘ test subsets. This indicates that although Clotho contains considerably more simultaneous actions and the number of future and past temporal cues is greatly dispro- portionate, the loss still helps the model better focus on temporal cues when present. This is observed in Tab. 4. 6 Conclusion In this work, we dissected