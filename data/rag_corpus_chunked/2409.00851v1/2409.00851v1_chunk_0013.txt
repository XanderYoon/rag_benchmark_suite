Model performance on AudioCaps and Clotho In this section, we investigate the performance of a state-of-the-art model for text-to-audio retrieval on AudioCaps and Clotho in detail. 3.3.1 Evaluation metrics. Throughout all experiments, we use the standard evaluation metrics for retrieval: recall at rank ğ‘˜ (R@ğ‘˜). This measures the percentage of targets retrieved within the top ğ‘˜ ranked results. Higher numbers are better. We report results for text-to-audio (T â†’ A) and audio-to-text retrieval (A â†’ T). We report the mean of three runs that use different random seeds. 3.3.2 Model. We employ the state-of-the-art text-audio model by [24], utilising an HTS-AT audio encoder [4], and a pre-trained BERT encoder for text. After encoding audio and text inputs, an MLP projects the embeddings into the same space. We use the model variant pre-trained on the joint dataset of WavCaps [ 24] + AudioCaps [16] + Clotho [9]. In our experiments, we finetune the model for 40 epochs on the dataset of interest (e.g. AudioCaps, Clotho) and use the same setup as [24]. The best model is selected using the highest average validation retrieval accuracy R@1 on the dataset used for experiments. We run these experiments on A6000s. 3.3.3 Loss function. We use the same loss as [24] - a normalised temperature scaled bidirectional cross-entropy loss (NT-Xent) [7]. We refer to this as Lğ‘ğ‘¡ with ğ‘ ğ‘ğ‘–ğ‘¡ ğ‘— = ğ‘“ (ğ‘ğ‘– ) Â· ğ‘”(ğ‘¡ ğ‘— ) âˆ¥ğ‘“ (ğ‘ğ‘– ) âˆ¥2 âˆ¥ğ‘”(ğ‘¡ ğ‘— ) âˆ¥2 , (1) Lğ‘¡ğ‘ = âˆ’ 1 2ğµ Ãğµ ğ‘–=1  log  exp(ğ‘ ğ‘ğ‘–ğ‘¡ğ‘– /ğœ)Ãğµ ğ‘—=1 exp(ğ‘ ğ‘ğ‘–ğ‘¡ğ‘— /ğœ)  + log  exp(ğ‘ ğ‘ğ‘–ğ‘¡ğ‘– /ğœ)Ãğµ ğ‘—=1 exp(ğ‘ ğ‘ğ‘— ğ‘¡ğ‘– /ğœ)  . (2) Here ğ‘“ (Â·) is the audio encoder and ğ‘”(Â·) the text encoder. ğ‘ ğ‘ğ‘–ğ‘¡ ğ‘— is the cosine similarity, ğ‘ğ‘– the audio input, ğ‘¡ ğ‘— the text input, ğµ the