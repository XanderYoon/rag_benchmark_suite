learning a dual- encoder for embedding the questions and passages [ 6]. A dual- encoder model consists of two separate neural networks optimized to score relevant (i.e., positive) question-passage pairs higher than irrelevant (i.e., negative) ones. At inference time, the score of a question-passage pair is computed as the inner product of the cor- responding question and passage embeddings. Due to their high efficiency, dual-encoders are popular first-stage rankers in large- scale settings (in contrast to cross-encoders where even though they can achieve higher performance, they are not indexable and therefore are used as re-rankers [4, 15]). The whole corpus can be encoded and indexed offline, while at inference time, high-scoring passages with respect to a question can be found using efficient maximum inner product search [5]. So far, dense retrieval models have been evaluated on clean and curated datasets. However, these models will encounter user- generated noisy questions when deployed in real-life applications. Questions can include typos because of users mistyping words, such as keyboard typos (additional/missing character and character substitution), phonetic typing errors due to the close pronunciation, and misspellings. How these typos affect the encoding of questions and whether dense dual-encoder retrieval models are robust to them is not studied yet. Works on text classification have shown that deep neural lan- guage models such as BERT are not robust against typos [13, 16], even though they apply the WordPiece tokenization. Ma et al. [10] and Zhuang et al. [17] showed that typos can confuse even advanced BERT-based cross-encoders for re-ranking [2, 4, 12] and proposed data augmentation training for building typo-robust re-rankers. Additionally, Ma et al. [10] showed that bringing closer in the latent space the representations of the positive question-passage pairs of different questions while being far apart from negative ones can increase robustness. While the