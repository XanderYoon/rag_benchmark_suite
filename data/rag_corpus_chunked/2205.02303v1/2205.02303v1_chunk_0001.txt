Misspellings. In Proceed- ings of the 45th International ACM SIGIR Conference on Research and Devel- opment in Information Retrieval (SIGIR ’22), July 11–15, 2022, Madrid, Spain. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3477495.3531818 1 INTRODUCTION With the advances in neural language modeling [3], learning dense representations for text has become a vital component in many in- formation retrieval (IR) tasks. In passage ranking and open-domain question answering, dense retrieval has become a new paradigm to Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’22, July 11–15, 2022, Madrid, Spain © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8732-3/22/07. . . $15.00 https://doi.org/10.1145/3477495.3531818 retrieve relevant passages [6, 7, 9]. In contrast to traditional term- based IR models (TF-IDF and BM25) that fail to capture beyond lexical matching, dense retrieval learns dense representations of questions and passages for semantic matching. A typical approach for dense retrieval involves learning a dual- encoder for embedding the questions and passages [ 6]. A dual- encoder model consists of two separate neural networks optimized to score relevant (i.e., positive) question-passage pairs higher than irrelevant (i.e., negative) ones. At inference time, the score of a question-passage pair is computed as the inner product of the cor- responding question and passage embeddings. Due