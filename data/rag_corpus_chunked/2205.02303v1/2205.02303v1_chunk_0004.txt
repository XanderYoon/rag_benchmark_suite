their com- bination improve the robustness of dense retrieval to typos? RQ2 Do certain typoed words affect the robustness of the question encoding more than others? RQ3 Do the proposed method improve the robustness of the ques- tion encoding by ways other than simply learning to ignore the typoed word? Our main contributions are the following: (i) we propose an approach for robustifying dense retrievers towards typos in user questions that combines data augmentation with contrastive learn- ing and performs better than applying each component separately, (ii) we perform a thorough analysis on the robustness of dense retrieval, and (iii) show that typos in various words influence per- formance differently. 1 2 EXPERIMENTAL SETUP In this section, we discuss the datasets, the metrics, and the robust- ness methods we experiment with to answer our research questions. 2.1 Datasets We conduct our experiments on two large-scale datasets, namely, MS MARCO passage ranking [11], and Natural Questions [8]. In MS MARCO passage ranking, the goal is to rank passages based on their relevance to a question (i.e., the probability of including the answer). The data collection consists of 8.8 million passages; the questions were selected from Bing search logs. Natural Questions is a large-scale dataset for open-domain QA over Wikipedia, and its questions were selected from Google search logs. Table 1 shows the statistics of the two datasets. Table 1: Number of questions in each dataset, and the aver- age length of question. Train Dev Test Avg. q length MS MARCO 502,939 6,980 6,837 5.94 Natural Questions 79,168 8,757 3,610 9.20 2.2 Metrics To measure the retrieval performance on MS MARCO, we use the official metric MRR (@10) alongside the commonly reported Recall (R) at top-k ranks [ 7, 14].2 Following previous work on Natural 1https://github.com/GSidiropoulos/dense-retrieval-against-misspellings. 2Similar to previous works, we report