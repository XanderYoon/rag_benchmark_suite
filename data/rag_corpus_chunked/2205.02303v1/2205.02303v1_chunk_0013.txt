can see from Figure 2, it consistently outperforms the baseline. Furthermore, Figure 2 highlights that when the importance of typoed words is low, simply ignoring them is a highly competitive approach. On the other hand, as the importance of the typoed words increases, the effectiveness of just ignoring these words decreases dramatically, to the extent that keeping the typoed words performs better. That can be attributed to the application of the WordPiece tokenizer (by the underlying BERT model) that allows DR to recover from some typos, such as when the character n-gram splits remain intact despite the typos. For instance originalrobustness and typoed robustnessd will be split into [robust, ##ness] and [robust, ##ness, ##d] respectively. 4We define a wordâ€™s relevant importance as the ratio of its IDF to the sum of the IDFs of every word in the question. [0-0.1) [0.1-0.2) [0.2-0.3) [0.3-0.4) [0.4-0.5) [0.5-0.6) [0.6-0.7] Importance of typoed words 0.0 0.2 0.4 0.6 0.8 1.0 R@50 DR DR (+ Delete words with typos) DR + Data augm. + CL (ours) Figure 2: Retrieval results w.r.t the relevant importance of the typoed words; on MS MARCO (Dev). Questions are split into bins w.r.t the relevant importance their typoed words. 4 CONCLUSIONS In this work, we provided insights on the robustness of dual-encoders to typos of user questions for dense retrieval. We proposed an ap- proach for robustifying dual-encoders that combines data augmen- tation with contrastive learning. Our experimental results showed that our proposed method not only improves robustness but also performs better than separately applying data augmentation or contrastive learning. Analysis of the methods we explored showed that typos in various words do not influence performance equally. In particular, typos on words that are less frequent on the training set and more important for a question are harder to address.