data augmentation combined with contrastive learning do not harm the performance. Moreover, all the approaches for robustifying DR are performing significantly better compared to the original DR, on questions with typos. That indicates that they successfully robustify the underlying dual-encoder model. That said, our proposed data augmentation combined with contrastive learning approach holds the best performance. Following previous works, we randomly introduce typos to ques- tions. However, we want to investigate if the performance of the approaches we consider remains the same irrespectively of the word in which the typo appears. To answer RQ2, we create two addi- tional test settings for the case of questions with typos. Specifically, we create (i) a setting where typos appear only in non-stopwords, and (ii) a setting where typos appear only in utterances with lexical match with the relevant passage.3 In detail, we consider the overlap- ping consecutive words between the ground-truth passage and the question (e.g., “Who was the president of the united states during wwi?”, and “Woodrow Wilson, a leader of the Progressive Move- ment, was the 28th President of the United States (1913-1921). After a policy of neutrality at the outbreak of World War I, he led Amer- ica into war. ” mark the “president of the united states” as available utterance to introduce typos). The highly discriminative utterances obtained through this heuristic are typically entity mentions. As we can see in Table 3 and by comparing the numbers with the results in Table 2, the effectiveness of the methods varies across the three settings. Particularly, robustness deteriorates when typos do not appear randomly. In detail, the most significant losses occur when typos appear on discriminative utterances. To this extent, our proposed data augmentation combined with contrastive learning approach remains the best performing one across all setting. To better understand