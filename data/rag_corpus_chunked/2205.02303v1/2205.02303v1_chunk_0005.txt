question. Train Dev Test Avg. q length MS MARCO 502,939 6,980 6,837 5.94 Natural Questions 79,168 8,757 3,610 9.20 2.2 Metrics To measure the retrieval performance on MS MARCO, we use the official metric MRR (@10) alongside the commonly reported Recall (R) at top-k ranks [ 7, 14].2 Following previous work on Natural 1https://github.com/GSidiropoulos/dense-retrieval-against-misspellings. 2Similar to previous works, we report the metrics on MSMARCO (Dev) since the correct answers for the test set are not available to the public. Questions, we use answer recall (AR) at the top-K retrieved passages [6, 14]. Answer recall measures whether at least one of the top-k retrieved passages contains the ground-truth answer. 2.3 Methods In this section, we describe the dual-encoder model we use for our experiments. Moreover, we present the three approaches we apply as extensions to this model in order to increase robustness. Dense Retriever (DR) is a dual-encoder BERT-based model used for scoring question-passage pairs [6]. Given a questionğ‘, a rel- evant passage ğ‘+ and a set of irrelevant passages {ğ‘âˆ’ 1 , ğ‘âˆ’ 2 , . . . , ğ‘âˆ’ğ‘› }, the model learns to rank ğ‘+ higher than the negative passages via the optimization of the negative log-likelihood of the relevant passage: L1 (ğ‘ğ‘–, ğ‘+ ğ‘– , ğ‘âˆ’ ğ‘–,1, Â· Â· Â·, ğ‘âˆ’ ğ‘–,ğ‘›) (1) = âˆ’ log ğ‘’sim(ğ‘ğ‘–,ğ‘+ ğ‘– ) ğ‘’sim(ğ‘ğ‘–,ğ‘+ ğ‘– ) + Ãğ‘› ğ‘—=1 ğ‘’sim(ğ‘ğ‘–,ğ‘âˆ’ ğ‘–,ğ‘— ) . Data Augmentation (DR + Data augm.) is one of the tradi- tional approaches for robustifying neural models. By exposing DR on questions with and without typos, the model learns to be in- variant to typos. Similar to Zhuang et al. [17], for each original correctly written question, on training time, we draw an unbiased coin. If the result is heads, we use the original question for training. If