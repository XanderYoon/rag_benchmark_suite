al. [17] showed that typos can confuse even advanced BERT-based cross-encoders for re-ranking [2, 4, 12] and proposed data augmentation training for building typo-robust re-rankers. Additionally, Ma et al. [10] showed that bringing closer in the latent space the representations of the positive question-passage pairs of different questions while being far apart from negative ones can increase robustness. While the aforementioned works studied robustness for the case of re-ranking, improving the robustness of dense retrieval for first- stage ranking has not been explored in-depth yet. Intuitively, if typos cause an inferior first-stage ranking, that will already nega- tively affect the performance of the re-ranker. Therefore, robustness for first-stage ranking is crucial for the overall performance. To the best of our knowledge, Zhuanget al. [17] is the only work that stud- ied the first-stage ranking and used data augmentation to improve the robustness of a BERT-based Siamese encoder. arXiv:2205.02303v1 [cs.IR] 4 May 2022 In this paper, we study in-depth the robustness of dense retrieval for the case of dual-encoder architecture. We propose an approach that combines data augmentation with contrastive learning for robustifying dual-encoders against questions with typos. In detail, alongside augmenting questions with typos, we propose to use a contrastive loss that brings the representation of a question close to its typoed variations in the latent space while keeping it distant from other distinct questions. We aim to answer the following research questions: RQ1 Can data augmentation, contrastive learning, and their com- bination improve the robustness of dense retrieval to typos? RQ2 Do certain typoed words affect the robustness of the question encoding more than others? RQ3 Do the proposed method improve the robustness of the ques- tion encoding by ways other than simply learning to ignore the typoed word? Our main contributions are the following: (i) we propose an