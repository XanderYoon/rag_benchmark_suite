that our proposed method not only improves robustness but also performs better than separately applying data augmentation or contrastive learning. Analysis of the methods we explored showed that typos in various words do not influence performance equally. In particular, typos on words that are less frequent on the training set and more important for a question are harder to address. Our proposed technique remains the best performing one in these set- tings, however the performance deteriorates significantly compared to a clean question. There is a significant question that has risen throughout our study: What could a dual-encoder actually learn to fix the problem? The WordPiece tokenizer, applied by BERT, allows models to recover from some typos. However, it would be ideal if embeddings could be learned at the character n-gram level to allow recovery from typical character substitution, deletion, etc. Further- more, word-to-word interactions during training (e.g., through a late interaction model [7]) could also allow implicitly to learn the “correct spelling” of a typoed word during training. We leave these directions as future work. ACKNOWLEDGMENTS This research was supported by the NWO Innovational Research Incentives Scheme Vidi (016.Vidi.189.039), the NWO Smart Culture - Big Data / Digital Humanities (314-99-301), the H2020-EU.3.4. - SOCIETAL CHALLENGES - Smart, Green And Integrated Transport (814961). All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors. REFERENCES [1] Joanna Bitton and Zoe Papakipos. 2021. AugLy: A data augmentations library for audio, image, text, and video. https://github.com/facebookresearch/AugLy. https://doi.org/10.5281/zenodo.5014032 [2] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 , Benjamin Piwowarski, Max Chevalier, Éric