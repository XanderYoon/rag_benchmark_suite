a random character; e.g., committee ‚Üí { copmmittee, commttee, comimt- tee, commitlee}. ‚Ä¢ Keyboard: Swaps a random character with those close to each other on the QWERTY keyboard; e.g., committee ‚Üí comnittee. ‚Ä¢ Common misspellings: Replaces words with misspelled ones, defined in a dictionary of common user-generated misspellings; e.g., committee ‚Üí comittee. 2.5 Implementation Details The DR model used in our experiments is trained using the in-batch negative setting described in [6]. The question and passage BERT encoders are trained for 50ùêæ steps, with a batch size of 48. The learning rate is set to 2ùëí-5 using Adam, and the rate of the linear scheduling with a warm-up is set to0.1. Moreover, we use the same hyper-parameters for the three robustifying methods described in Section 2.3, in order to ensure a fair comparison. For generating typos in the training phase as well as building the typo-robustness test set, we use the open-source Aug library [1]. Each word in a question gets transformed with a probability of 0.2, and the transformation type (Section 2.4) gets chosen at random. 3 RESULTS In this section, we present our experimental results that answer our research questions. We aim to answer RQ1 by comparing the retrieval performance of the methods we consider (Section 2.3) for two settings: clean questions and questions with typos. In Table 2, we obverse that on clean questions, data augmentation as well as our two proposed approaches, namely, contrastive learning and data augmentation combined with contrastive learning do not harm the performance. Moreover, all the approaches for robustifying DR are performing significantly better compared to the original DR, on questions with typos. That indicates that they successfully robustify the underlying dual-encoder model. That said, our proposed data augmentation combined with contrastive learning approach holds the best performance. Following previous works, we