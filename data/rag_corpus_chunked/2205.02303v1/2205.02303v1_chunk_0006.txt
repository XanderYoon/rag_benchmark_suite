of the tradi- tional approaches for robustifying neural models. By exposing DR on questions with and without typos, the model learns to be in- variant to typos. Similar to Zhuang et al. [17], for each original correctly written question, on training time, we draw an unbiased coin. If the result is heads, we use the original question for training. If the result is tails, we use one of its typoed variations. Contrastive learning (DR + CL) of representations works by maximizing the agreement between differently augmented views of the same object. We propose a contrastive loss that compares the similarity between a question and its typoed variations and other distinct questions. In contrast with data augmentation, which explicitly trains on typoed question-passage pairs, here such pairs are seen implicitly only. In detail, in addition to Equation 1, we introduce a loss that enforces that a question, ğ‘ and its typoed variations ğ‘+ are close together in the latent space while being far apart from other distinct questions {ğ‘âˆ’ 1 , ğ‘âˆ’ 2 , . . . , ğ‘âˆ’ğ‘› }: L2 (ğ‘ğ‘–, ğ‘+ ğ‘– , ğ‘âˆ’ ğ‘–,1, Â· Â· Â·, ğ‘âˆ’ ğ‘–,ğ‘›) (2) = âˆ’ log ğ‘’sim(ğ‘ğ‘–,ğ‘+ ğ‘– ) ğ‘’sim(ğ‘ğ‘–,ğ‘+ ğ‘– ) + Ãğ‘› ğ‘—=1 ğ‘’sim(ğ‘ğ‘–,ğ‘âˆ’ ğ‘–,ğ‘— ) . The final loss is a weighted average of the two losses: L = ğ‘¤ 1 Â· L1 + ğ‘¤ 2 Â· L2 (3) The weights ğ‘¤ 1 and ğ‘¤ 2 are hyper-parameters and therefore need to be defined. Giving equal weights to the two losses is an effective and straightforward combination method which we used in our experiments. Combination (DR + Data augm. + CL) method consists of data augmentation combined with contrastive learning. Specifically, we propose alongside augmenting questions with typos to use the contrastive loss of Equation 2 that brings