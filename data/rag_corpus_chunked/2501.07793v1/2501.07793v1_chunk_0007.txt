label for q: π = Metrics(q, DOC⋆, rm, r⋆), (4) where the details of used metrics are shown in § 4.3. By comparing Equation 1 with Equation 4, it can be found that through a series of transformations, we use upper-bound multi-sourced r⋆ instead of the manually annotated rgold, thus achieving the goal of unsupervised query routing. 4 Methodology The overall framework of our method is shown in Figure 1. Our method mainly consists of four steps: (1) Single-sourced Response Generation which generates single-sourced responses for each search tool; (2) Upper-bound Construction which constructs the multi-sourced responses; (3) Label Construction which utilizes the single-sourced re- sponse and its corresponding multi-sourced upper- bound to create training labels; and (4) Model Training where we train our routing model. 4.1 Single-sourced Response Generation As stated in Equation 2, for each toolTm, we search q within Tm to retrieve DOCm which consists of k documents. Next, we compile the query q and the retrieved document DOCm into a prompt, which is then fed into a LLM to generate the single-sourced response rm. The prompt is shown in Figure 5. For the LLM, we employ Qwen2-max 2 (Yang et al., 2024) to generate responses. This process is re- peated for each search tool Tm, resulting in a col- lection of single-sourced responses {rm}M m=1. 4.2 Upper-bound Construction Given q and {Tm}M m=1, we can obtain DOCm for each Tm (Equation 2), respectively. The question is how to retain k documents from k ∗ M documents to achieve a fair comparison to § 4.1. Previous works generally utilize the text reranker (Li et al., 2023; Chen et al., 2023a) to retain top-ranked doc- uments. However, these rerankers require extra training and may not generalize well to black-box search engines. Therefore, we opt to forgo the re-ranking