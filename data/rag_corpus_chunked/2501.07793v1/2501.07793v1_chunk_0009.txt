et al., 2019) to evaluate the similarity between rm and its upper-bound r⋆: sBert m = BertScore(rm, r⋆). (5) The intuition is that the higher the value of sBert m , the better the quality of rm. Coherence Score Besides Similarity, we also care about the coherence between the query and the single-sourced response, i.e., whether rm can effectively solve q. Therefore, we use a LLM, e.g., Qwen2-max, to determine which one better solves q in a pair of single-sourced responses. With- out loss of generality, we define the paired single- sourced responses as ra and rb. Taken q, the multi- sourced documents DOC⋆, ra, and rb as inputs, the LLM determines the one with better quality3: ra or rb ← −LLM(q, DOC⋆, ra, rb). (6) 3The prompt is shown in Appendix A, Figure 6. Through M ∗(M −1) 2 comparisons, we can obtain the overall ranking relationship (in ascending order) among {rm}M m=1 4. We define the coherence score sCoh m as the index of rm in the overall ranking. The larger the index, the higher the score. Overall Score Having obtained sBert m and sCoh m of rm, we devise a normalization schema to get the overall score of rm, which helps to mitigate the impact of varying dimensions of multiple metrics. For example, we calculate the mean µBert and stan- dard deviation σBert of the similarity score across all examples. Then, the normalized similarity score and coherence score are calculated as follows: sBertm = sBert m − µBert σBert , sCohm = sCoh m − µCoh σCoh . (7) The overall score of rm is calculated as: sm = sBertm + sCohm 2 . (8) Converting to Training Labels After obtaining {sm}M m=1, we sort them in ascending order. We use the indices of {sm}M m=1 in