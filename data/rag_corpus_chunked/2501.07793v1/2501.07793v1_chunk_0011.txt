engine Quark 4.833 3.38 2.258 3.691 4.484 4.321 2.657 2.735 3.73 4.42 Bing 4.79 3.309 2.151 3.661 4.437 4.168 2.461 2.737 3.559 4.33 Google 4.797 3.205 2.104 3.745 4.395 4.225 2.557 2.644 3.79 4.312 Using two search engines Quark+Bing 4.823 3.428 2.274 3.829 4.546 4.323 2.669 2.755 3.877 4.445 Quark+Google 4.822 3.418 2.261 3.917 4.487 4.285 2.682 2.701 3.925 4.412 Bing+Google 4.815 3.363 2.214 3.858 4.458 4.24 2.61 2.765 3.893 4.36 Using three search engines Quark+Bing+Google 4.808 3.445 2.352 3.993 4.584 4.285 2.705 2.796 3.962 4.45 Table 2: The Correctness results under different combinations of LLM and search tools. Scores with bold denote the best result. Values are calculated by averaging 3 random experiments. 2009) to optimize model parameters: L = ListMLE(p, Ï€). (10) During inference, our method only requires the input q to predict the scores of using each search tool, without actually calling any search tool. 5 Experiment 5.1 Experimental Settings Datasets We use four public open-domain QA datasets, including NLPCC-MH (Wang and Zhang, 2018), CDQA (Xu et al., 2024), WebQA (Chen et al., 2019), SogouQA5, and a private QA dataset, named PrivateQA, for evaluation. The statistics of the dataset and the details of the construction of PrivateQA are shown in Appendix B. Search Tools and LLMs We consider 3 search tools, i.e., Quark, Bing, and Google, for document retrieval. For each query, we retrieve k = 6 docu- ments. That is, in the upper-bound construction pro- cess, we only retain the top-ranked 2 documents for each search tool. We use Qwen2-max and GPT4 for response generation. The Evaluation Metric Each test example is in the form of (query, answer). For each query, we use the different combinations of LLMs and search tools to obtain the retrieval-augmented responses. We adopt the Correctness score implemented by Llama-index (Liu, 2022)