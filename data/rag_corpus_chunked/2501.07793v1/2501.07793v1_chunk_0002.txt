effectively to actual user queries, it is essential to train them directly on real user queries. Neverthe- less, real user queries often lack gold-standard an- swers, presenting a major challenge in determining the appropriate search engine assignment, particu- larly when no definitive answer is available. To address this challenge, we must transform our problem into a labelling-free form. Since query routing fundamentally involves identifying the op- arXiv:2501.07793v1 [cs.IR] 14 Jan 2025 timal assignment within the limited computing re- sources, such as inference time and budget, we can envision an ideal situation in which we pos- sess infinite computing resources. In this case, an intuitive and obvious strategy is to search a query across all search engines, then merge multi-sourced documents for RAG1. Due to the complementar- ity of different search engines (Mu et al., 2024), the merged multi-sourced documents should have a better recall than the single-sourced documents. That is, multi-sourced responses are likely to ex- hibit higher quality than single-source responses. To support this assertion, we calculate the RAG results across various search strategies and differ- ent LLMs, as illustrated in Table 1. We find that utilizing more search engines consistently leads to improved results. This trend is observed across multiple LLMs. Based on this prior knowledge, we can view the multi-sourced responses as an up- per bound for unsupervised query routing. Now, let us return to the practical situation of limited computing resources. To evaluate the quality of a single-sourced response, we can compare it to the corresponding multi-sourced response, which serves as an upper bound. By assessing the simi- larity between the single-sourced response and its upper bound, we are able to determine its quality. If the single-sourced response shows a high level of similarity to its upper bound, this suggests that the query should be assigned