on CDQA and evaluate the model on WebQA. Therefore, "CDQA → CDQA" and "WebQA → WebQA" are actually in an i.i.d. setting. proportion of training data, we present the average result of five random experiments. We consider the following baselines for comparison: (1) Random-1 which randomly selects a tool from Quark6, Bing, or Google for RAG; (2) Random-2 which ran- domly selects a combination from Quark+Bing, Bing+Google, or Quark+Google for RAG; (3) Best1Outof3 which is the most effective one among Quark, Bing, or Google; (4) Best2Outof3 which is the most effective one among Quark+Bing, Bing+Google, or Quark+Google. It should be noted that Best1Outof3 and Best2Outof3 may correspond to different specific retrieval strategies on different datasets. Corre- spondingly, for each query, our method will predict the score of each tool. Therefore, we consider Top1-Predictedand Top2-Predicted, where Top1- 6https://www.quark.cn/ Predicted indicates that we choose the tool with the highest predicted score, and Top2-Predicted indicates that we choose the two tools with the highest predicted scores. Ultimately, we compare Top1-Predicted with Best1Outof3, and compare Top2-Predicted with Best2Outof3. Result The result is shown in Figure 2. The ran- dom baselines show an unsatisfactory performance. In addition, we have the following observations. • As the volume of training data increased, our method has demonstrated consistent and obvious performance improvements. Since our method eliminates the necessity of manual annotation, the sustained enhancement in performance high- lights the effectiveness of leveraging a larger amount of real user queries, ultimately, it is promising to obtain more refined and satisfied outcomes in diverse applications. • The effectiveness of our method is observed consistently across various LLMs and multiple datasets, underscoring the robustness and stabil- ity of our approach. The key reason is that a wide variety of real user queries are directly uti- lized for the learning of query