Finally, by constructing training data automatically, we train our routing model. Our method operates without the need for manual annotation, enabling the automation of large-scale processing of real- world user queries and facilitating the construction of training data. Consequently, our approach can be effectively scaled and has a strong generalization capability for real user queries. In summary, our contributions are as follows. (1) We introduce a novel unsupervised query routing method capable of automatically processing large- scale, unlabeled real user data for model training. This simple yet effective approach significantly re- duces annotation costs and offers excellent scalabil- ity potential. (2) As far as we know, we are the first to explore unsupervised query routing within the RAG scenario. Our method serves as a universal framework, holding significant promise for provid- ing valuable insights into the field of tool learning. (3) We validate our method across five datasets in a non-i.i.d. setting. The experiments demonstrate that our approach exhibits i) outstanding gener- alization ability, ii) excellent scalability, and iii) consistent effectiveness across multiple LLMs. 2 Related Work Query Routing for LLMs Nearly monthly re- leased LLMs (Qwen Team, 2023; Dubey et al., 2024) are trained with various data, resulting in a variety of strengths and weaknesses in versatile downstream tasks (Jiang et al., 2023). Therefore, researchers (Narayanan Hari and Thomson, 2023; Lu et al., 2023; Liu et al., 2024; Shnitzer et al., 2023; Chen et al., 2023b; Å akota et al., 2024) are de- voted to learning the optimal assignment of queries to different LLMs. The basic idea is to leverage the complementarity of various LLMs to achieve su- perior performance compared to any single model across a range of tasks. Notably, these methods typically require substantial amounts of annotated training data, leading to high costs for label collec- tion. In