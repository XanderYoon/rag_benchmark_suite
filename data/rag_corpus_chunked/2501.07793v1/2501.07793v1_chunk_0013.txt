decayed to zero with a 10% warmup ratio over 10 training epochs. Our model is eval- uated through 5 randomized experiments, and we report the average results. All experiments are con- ducted using the PyTorch toolkit on an Ubuntu server equipped with four V100 (32GB) GPUs. 5.2 The Evaluation of Scalability Our method, which eliminates the need for manual annotation, is designed to exhibit strong scalabil- ity. This means that as we increase the volume of training data, we are expected to witness a corre- sponding improvement in the performance of our model. In this section, we conduct a comprehensive evaluation of the scalability of our approach. Setting We alter the proportions of training data to assess the scalability of our method. For each (a) The data scaling result when using Qwen2-max for response generation. (b) The data scaling result when using GPT4 for response generation. Figure 2: The data scaling result of our method. For Best1Outof3 and Best2Outof3, we explicitly indicate the specific search strategy in parentheses. The values are obtained by averaging 5 random experiments. Figure 3: The evaluation result of the generalization abilities of different methods. We report the Top1-Predicted result. Values are calculated by averaging 5 randomized experiments. We try three different types of training examples on five test sets. "A → B" means we train the model on A and evaluate the model on B. For example, "CDQA → WebQA" means we train the model on CDQA and evaluate the model on WebQA. Therefore, "CDQA → CDQA" and "WebQA → WebQA" are actually in an i.i.d. setting. proportion of training data, we present the average result of five random experiments. We consider the following baselines for comparison: (1) Random-1 which randomly selects a tool from Quark6, Bing, or Google for RAG; (2) Random-2 which ran-