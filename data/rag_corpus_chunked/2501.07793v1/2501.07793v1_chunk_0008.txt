how to retain k documents from k ∗ M documents to achieve a fair comparison to § 4.1. Previous works generally utilize the text reranker (Li et al., 2023; Chen et al., 2023a) to retain top-ranked doc- uments. However, these rerankers require extra training and may not generalize well to black-box search engines. Therefore, we opt to forgo the re-ranking process and instead utilize the inher- ent sorting algorithm of the search engine directly. 2https://dashscope.aliyuncs.com/compatible-mode/v1 Figure 1: The overall framework of our method, which mainly consists of four steps. We automatically construct supervision information for training our routing model. Specifically, for DOCm that contains the ordered k documents, we only use the top-ranked k/m documents. That is, for {DOCm}M m=1, we finally reserve the multi-sourced documents DOC⋆ which contains k documents. Then, we pass q and DOC⋆ to LLMs to generate the multi-sourced response r⋆ (Equation 3). Notably, we consider multiple LLMs, including Qwen2-max and GPT4, for generatingr⋆ to improve the robustness of r⋆. This is motivated by the previous works of text generation (Bahdanau et al., 2014; Sutskever et al., 2014) in which the gold references usually contain multiple texts. 4.3 Label Construction We devise two metrics, i.e., similarity and coher- ence scores, to assess the quality of{rm}M m=1. Con- sidering multiple scores for each rm, we adopt a normalization schema to get the overall score, so that obtain the training labels. Similarity Score We first use BertScore (Zhang et al., 2019) to evaluate the similarity between rm and its upper-bound r⋆: sBert m = BertScore(rm, r⋆). (5) The intuition is that the higher the value of sBert m , the better the quality of rm. Coherence Score Besides Similarity, we also care about the coherence between the query and the single-sourced response, i.e., whether rm can effectively solve