to generalize effectively in non-i.i.d. environments. Such findings highlight a critical limitation in existing approaches. That is anno- tated data is difficult to obtain, which greatly limits the diversity of data and thus restricts their generalization ability. • Our method demonstrates a pronounced data scal- ing trend across all datasets, although the trend on NLPCC-MH is not as obvious as the trend on other test sets. This is primarily due to the fact that the data scale utilized by our approach sig- nificantly surpasses that of traditional supervised methods. Such an expansive data scale effec- tively enhances the diversity of the training data. This characteristic underscores the innovation of our approach, which is capable of automat- ically curating a wide array of diverse training data, eliminating the need for both time and cost consuming human annotation. 5.4 Further Discussion Metrics CDQA WebQA SogouQA PrivateQA Full (ours) 3.736 4.827 4.446 3.378 w/o Similarity 3.718 4.806 4.438 3.286 w/o Coherence 3.698 4.796 4.422 3.272 Table 3: The impact of different automatic metrics in the label construction step. Values are calculated by averaging 5 randomized trials. The Impact of Different Evaluation Metrics We develop the following two ablated variants to validate the effectiveness of different metrics in the label construction step: (1) "w/ Similarity" means we only use the similarity score for label construc- tion; (2) "w/ Coherence" means we only use the co- herence score for experiments. The result is shown in Table 3. Both "w/ Similarity" and "w/ Coher- ence" show a performance drop. In addition, we have the following observations. • The performance of "w/ Similarity" surpasses that of "w/ Coherence," indicating that the sim- ilarity metric is more effective. It is important to note that the similarity metric employs multi- sourced responses as the anchor, whose reliabil- ity has