Table 3. Both "w/ Similarity" and "w/ Coher- ence" show a performance drop. In addition, we have the following observations. • The performance of "w/ Similarity" surpasses that of "w/ Coherence," indicating that the sim- ilarity metric is more effective. It is important to note that the similarity metric employs multi- sourced responses as the anchor, whose reliabil- ity has been validated by our experiments. This suggests that the similarity metric is a more ob- jective measure, resulting in labels that are more robust. In contrast, the coherence metric relies solely on the knowledge within the LLM itself. However, due to inherent knowledge limitations in LLMs, it can generate incorrect information, leading to reduced effectiveness compared to the similarity metric. As evidence, we observe that LLMs may generate illegal ranking outputs, such as A > B, B > C , but C > A , in the coherence score calculation. These illegal outputs account for approximately 9% of the population. • Although not as effective as the Similarity metric, the Coherence metric contributes to our method. The best result is achieved when combining two metrics. One possible explanation is that Simi- larity and Coherence assess different aspects of the generated single-sourced responses. Specifi- cally, similarity measures how closely a single- sourced response aligns with its upper-bound, while Coherence evaluates the rationality of a single-sourced response in relation to the input query. As a result, these two metrics can effec- tively complement one another. Backbone CDQA WebQA SogouQA PrivateQA GTE-large (ours) 3.736 4.827 4.446 3.378 w/ Roberta-large 3.716 4.826 4.453 3.343 w/ Qwen-0.5B 3.727 4.808 4.419 3.290 Table 4: The result under different backbone models. Values are calculated by averaging 5 randomized trials. The Result under Different Backbone Models We also investigate the impact of different back- bone models. Besides GTE-large,