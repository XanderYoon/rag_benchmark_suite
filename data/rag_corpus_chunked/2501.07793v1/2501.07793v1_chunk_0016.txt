model theoretically has good generalization ability. The underlying principle is that by collect- ing sufficient real user queries, data diversity can be ensured, enabling the model to effectively gener- alize to unseen data. Consequently, in this section, we compare our approach with existing supervised methods to assess its generalization ability. Setting Our method uses real user queries for training. As a comparison, we reproduce existing supervised methods (Shnitzer et al., 2023; Mu et al., 2024) that utilize annotated (query, answer) data for model training. More specifically, we use the training splits of CDQA and WebQA which contain 10k and 20k examples, respectively, to construct training examples (§ 3.1). Then, we use the ob- tained examples to train the routing models and evaluate them on the 5 test sets. By altering the types of training data, we observe the scaling effect of the learned models on the 5 test sets. Result The result is shown in Figure 3. We have the following observations. • In the independent and identically distributed (i.i.d.) settings, specifically in the "CDQA → CDQA" and "WebQA → WebQA" scenarios, supervised methods demonstrate a pronounced scaling trend. This indicates that augmenting the volume of training data correlates with sustained performance improvements, which aligns with human intuitive understanding. • Conversely, in the other non-i.i.d. scenarios of supervised methods, the addition of more train- ing data fails to yield similar performance gains. This suggests that traditional supervised meth- ods struggle to generalize effectively in non-i.i.d. environments. Such findings highlight a critical limitation in existing approaches. That is anno- tated data is difficult to obtain, which greatly limits the diversity of data and thus restricts their generalization ability. • Our method demonstrates a pronounced data scal- ing trend across all datasets, although the trend on NLPCC-MH is not as obvious as