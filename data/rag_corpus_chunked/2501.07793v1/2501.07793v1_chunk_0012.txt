we only retain the top-ranked 2 documents for each search tool. We use Qwen2-max and GPT4 for response generation. The Evaluation Metric Each test example is in the form of (query, answer). For each query, we use the different combinations of LLMs and search tools to obtain the retrieval-augmented responses. We adopt the Correctness score implemented by Llama-index (Liu, 2022) as the metric to evaluate the quality of generated responses. For each query, we repeatedly generate three times and calculate the average correctness score. Since we have con- sidered 3 search engines and 2 LLMs, we have 16 5https://aistudio.baidu.com/datasetdetail/17514 RAG strategies. The result is shown in Table 2. In the vast majority of strategies, using more search tools will lead to better results. Data Source of Training Queries We utilize our in-house data to construct the training examples. After filtering out illegal samples (ยง 4.3), we finally obtain around 110k training examples. It should be noted that there is a considerable distribution discrepancy between the training and test examples. As a result, the evaluation of our method can be regarded as in a non-i.i.d. setting. Model Training Details We initialize our rout- ing model as GTE-large (Li et al., 2023). Since our method is model-agnostic, we have also tried other backbone models, as shown in ยง 5.4. We utilize the AdamW optimizer with an initial learning rate of 5e-5 and a batch size of 32. The learning rate is linearly decayed to zero with a 10% warmup ratio over 10 training epochs. Our model is eval- uated through 5 randomized experiments, and we report the average results. All experiments are con- ducted using the PyTorch toolkit on an Ubuntu server equipped with four V100 (32GB) GPUs. 5.2 The Evaluation of Scalability Our method, which eliminates the need for manual annotation,