complement one another. Backbone CDQA WebQA SogouQA PrivateQA GTE-large (ours) 3.736 4.827 4.446 3.378 w/ Roberta-large 3.716 4.826 4.453 3.343 w/ Qwen-0.5B 3.727 4.808 4.419 3.290 Table 4: The result under different backbone models. Values are calculated by averaging 5 randomized trials. The Result under Different Backbone Models We also investigate the impact of different back- bone models. Besides GTE-large, we have also evaluated Roberta-large and Qwen2-0.5B, as they possess a comparable number of parameters to GTE-large. The result is shown in Table 4. We observe a minor result difference between GTE- large and Roberta-large. This is because they be- long to the same type of models, and after training, they will all converge towards the distribution of the training data. However, Qwen2-0.5B shows a large performance drop compared to GTE-large and Roberta-large. We speculate that such decoder- only small models are less effective for regression tasks. Due to time and computational resource lim- itations, we are unable to try larger models, such as Qwen2-4 and Qwen2-7B, for experiments. Finally, we choose GTE-large as the backbone of our work. The Quality of Generated Training Datasets We additionally assess the quality of the gener- ated training datasets through manual review. We randomly sampled 200 automatically constructed examples, and manually evaluate the average ac- curacy of the labels generated by different metrics. The result is shown in Table 5. Metrics Accuracy (%) Similarity 83 Coherence 79 Overall 86 Table 5: The average accuracy of automatically labelled training datasets. The Similarity metric provides more accurate labels compared to the Coherence metric, as confirmed by our experiment (ยง 5.4). By integrating both met- rics, our weighted overall metric yields the highest accuracy in labeling. Given the automated nature of our method, we consider an 86% accuracy rate to be satisfactory. Visualization We conduct a