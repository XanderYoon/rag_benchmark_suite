form (q, π), where π represents the search engine index indicating which search engine should be selected to address the query. 3.1 Briefing of Previous Supervised Methods Existing supervised methods (Shnitzer et al., 2023; Mu et al., 2024) rely heavily on annotated (query, answer) paired data. Specifically, using each search tool Tm, they are able to obtain the retrieval- augmented response rm, respectively. By calculat- ing the similarity between rm and the gold answer rgold, they can obtain the labeled index: π = arg max m similarity(rm, rgold). (1) However, as mentioned earlier, public datasets tend to be relatively small in scale, lack diversity, and significantly differ from actual user queries, which restricts their scalability and generalization ability. 3.2 Problem Formulation We devise an unsupervised method that relies solely on user queries and search tools to generate training data. Formally, we first obtain the single-sourced responses using every search tool Tm: DOCm = Tm(q) rm = LLM(q, DOCm), (2) where DOCm denotes the single-sourced docu- ments retrieved from Tm. Then, by aggregating retrieved documents from multiple search tools, we obtain the upper-bound multi-sourced response: DOC⋆ = T1(q)|| · · · ||TM (q) r⋆ = LLMs(q, DOC⋆), (3) where ·||· denotes the merging operation,DOC⋆ de- notes the multi-sourced documents, and r⋆ denotes the upper-bound multi-sourced response. Finally, by using the upper-bound r⋆ as the anchor, we eval- uate the quality of every single-sourced response, which allows us to obtain the training label for q: π = Metrics(q, DOC⋆, rm, r⋆), (4) where the details of used metrics are shown in § 4.3. By comparing Equation 1 with Equation 4, it can be found that through a series of transformations, we use upper-bound multi-sourced r⋆ instead of the manually annotated rgold, thus achieving the goal of unsupervised query routing. 4 Methodology The