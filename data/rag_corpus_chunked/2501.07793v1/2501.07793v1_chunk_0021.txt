work, we present an unsupervised query routing approach. Given the insight that multi- sourced retrieval yields superior performance, we ingeniously design an automated process to assess the quality of single-sourced responses, thereby generating training labels without manual annota- tion. Experimental results demonstrate that our method has excellent scalability and generalization ability. We are confident that our approach will inspire new advancements within the community. Limitations Due to the high costs associated with large-scale data processing, we currently have access to only approximately 110k training samples. It is im- portant to note that query routing among general- purpose search engines presents a particularly chal- lenging problem, and it is likely that 110k exam- ples are still insufficient for our needs. In the future, given a larger budget, we plan to conduct exper- iments using a greater number of examples. For our methodology, we have chosen smaller models instead of larger Qwen2-7B or Qwen2-14B as the backbone for two primary reasons: (1) Query rout- ing demands low latency, whereas larger models tend to operate at slower speeds, which does not meet our requirements. (2) Tool selection necessi- tates that the model accurately predicts the score for each tool in relation to each query, which ef- fectively constitutes a numerical prediction task. However, auto-regressive LLMs often exhibit sig- nificant challenges in numerical prediction. References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learn- ing to align and translate. CoRR, abs/1409.0473. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2023a. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Preprint, arXiv:2309.07597. Lingjiao Chen, Matei Zaharia, and James Zou. 2023b. Frugalgpt: How to use large language models while re- ducing cost and improving performance. arXiv preprint arXiv:2305.05176. Yu Chen, Lingfei Wu, and Mohammed