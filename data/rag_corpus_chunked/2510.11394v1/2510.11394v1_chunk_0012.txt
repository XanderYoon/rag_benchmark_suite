forum. Characterized by complexhow,why, andwhatqueries, this dataset presents significant challenges in generating logically coherent, information-rich long-form explanations. For multi-hop reasoning evaluation, we employ HotpotQA [ 40] and MuSiQue [ 36]. Hot- potQA features curated factual questions requiring cross-document 2https://www.reddit.com/r/explainlikeimfive VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via Rigorous Verification SIGIR-AP 2025, December 7‚Äì10, 2025, Xi‚Äôan, China evidence integration through manually designed multi-step reason- ing. Conversely, MuSiQue contains synthetically generated factual questions formed by composing single-hop queries, typically de- manding 2-4 inference steps. This automated composition process yields linguistically structured questions that present heightened analytical difficulty relative to conventional benchmarks. The ASQA and ELI5 datasets are subsets released by ALCE [8], while HotpotQA and MusiQue are subsets released by IRCOT [37]. Each dataset is evaluated in terms of answer correctness and ci- tation quality. Among these, we use the EM (Exact Match) Recall metric to evaluate the answer correctness for the ASQA, HotpotQA, and MusiQue datasets, use Claim Recall to evaluate the answer correctness for the ELI5 dataset, and use Citation F1 to evaluate the citation quality for all datasets. Dataset details are summarized in Table 1. Experiments were conducted on five open-source LLMs: Llama3- 8B-Instruct [3], Gemma-2-9B-it [34], GLM-4-9B-Chat [10], Qwen2.5- 7B-Instruct [39], and Qwen2.5-14B-Instruct. 4.2 Baselines For baseline comparisons, we selected four established approaches: ‚Ä¢ Vanilla[ 8]: The query and top- ùëò retrieved passages are concatenated to form the model input. Task-specific instruc- tions coupled with in-context learning mechanisms guide the generation of answers with integrated citations. This approach represents the foundational methodology for at- tribution generation, processing retrieved passages without additional refinement. ‚Ä¢ Summary[ 8]: Retrieved passages undergo summarization- based compression prior to model input. These summarized compressions are concatenated with the original query and processed through identical task-specific instructions and in-context learning mechanisms to guide the generation