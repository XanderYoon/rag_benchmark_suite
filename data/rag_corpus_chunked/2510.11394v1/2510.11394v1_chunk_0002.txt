challenges for LLMs. For example, if irrelevant information is retrieved and used as a reference, the LLM may incorporate this noise and generate incorrect answers, exacerbating the hallucination issue[6, 15, 35]. Therefore, enabling LLMs to generate attributable responses is vital for ensuring trustworthiness and mitigating misinformation. An effective strategy to enhance the reliability of LLM responses is through citation mechanisms, whereby each statement is explicitly anchored to relevant source materials [5, 8, 17]. This approach not only establishes traceability by allowing users to independently verify the accuracy of responses, but also facilitates error diagnosis and promotes transparency in human-AI collaboration. arXiv:2510.11394v1 [cs.IR] 13 Oct 2025 SIGIR-AP 2025, December 7–10, 2025, Xi’an, China Haosheng et al. Current approaches for generating answers with citations can be broadly classified into two paradigms [14]. The first category, classified as “intrinsic attribution”, operates synchronously with text generation. These approaches typically treat citations as reg- ular tokens and enable LLM to directly generate citations within answers through fine-tuning or in-context learning [8, 20, 23]. Nev- ertheless, intrinsic integration approaches face several practical constraints: (1) Fine-tuning demands extensive domain-specific an- notation and significant computational resources; (2) In-context learning is highly sensitive to the input examples, leading to poor generalization performance. The other category can be classified as “extrinsic attribution”, which initially generates a draft answer and subsequently employs post-processing approaches to match retrieved passages with state- ments in the answer. Common matching methods include utilizing sentence similarity metrics such as BLEU [ 24] and ROUGE [ 18], or employing Natural Language Inference (NLI) classifiers to eval- uate entailment relationships [ 7]. Classic similarity metrics are computationally efficient, but their effectiveness is constrained by the challenge of determining thresholds. Conversely, although NLI models deliver higher accuracy, yet fundamentally struggle to han- dle cases where a single statement requires