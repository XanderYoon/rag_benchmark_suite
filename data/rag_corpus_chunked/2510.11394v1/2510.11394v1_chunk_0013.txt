guide the generation of answers with integrated citations. This approach represents the foundational methodology for at- tribution generation, processing retrieved passages without additional refinement. • Summary[ 8]: Retrieved passages undergo summarization- based compression prior to model input. These summarized compressions are concatenated with the original query and processed through identical task-specific instructions and in-context learning mechanisms to guide the generation of answers with integrated citations. This approach intention- ally mitigates textual redundancy in model inputs, enhancing focus on salient information. • Snippet[ 8]: Contrasting with the Summary approach, this methodology employs extractive summarization for model input. This methodology preserves exact expressions from retrieved passages, thereby circumventing potential seman- tic distortion inherent in abstractive summarization. • APO[ 17]: Automatic Preference Optimization framework enhances model performance through a dual-phase approach: supervised fine-tuning followed by preference optimization. During the preference optimization phase, a novel loss func- tion is implemented to enable fine-grained sentence-level rewards, facilitating more efficient model parameter updates. 4.3 Implementation Settings In the experiment, the top-5retrieved passages are provided for each query, and each method is given two few-shot examples for in- context learning. In the VeriCite method, we use TRUE [12] as the NLI model for citation verification. To ensure the reproducibility of the experiment, allLLMs generate responses using greedy decoding. 4.4 Main Results Our experimental results, as shown in Table 2. On the ASQA dataset, VeriCite exhibits a clear advantage in an- swer correctness across all five models, outperforming all baseline methods. Notably, the GLM-4 model delivers the most substantial improvement with a 4.54% increase in correctness over the best performing Vanilla baseline. Regarding citation quality, Llama3 and Qwen2.5 models achieve significant enhancements in citation F1, surpassing the strongest baseline. In contrast, Gemma-2 and GLM-4 perform marginally below their respective optimal baselines in this metric. For the ELI5 dataset,