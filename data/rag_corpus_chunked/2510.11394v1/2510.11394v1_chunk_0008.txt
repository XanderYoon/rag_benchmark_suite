confronted with complex problems. To enhance answer reliability, we implement a rigorous verifi- cation and filtering mechanism. During initial answer validation, unsupported content must be systematically eliminated, retaining exclusively evidence-substantiated answer statements. To facili- tate granular reliability verification, the initial answer ğ‘–ğ‘›ğ‘–ğ‘¡_ğ‘ğ‘›ğ‘  is decomposed into a set of statements {ğ‘ 1, ğ‘ 2, . . .} , where each statement ğ‘ ğ‘– is associated with a potentially empty set of cita- tions {ğ‘ğ‘–1, ğ‘ğ‘–2, . . .} . The verification process employs a NLI model ğœ™ trained for Recognizing Textual Entailment (RTE) tasks [2, 41], which predicts whether a hypothesis is entailed by, contradicts, or is neutral to given premises. Specifically, for each statementğ‘ ğ‘–, we validate whether the corresponding retrieval passages {ğ‘ğ‘ğ‘– ğ‘— } (premises) entail the statementğ‘  ğ‘– (hypothesis). ğ‘ ğ‘¢ğ‘ğ‘– =ğœ™(ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘ ğ‘ğ‘– ğ‘— ), ğ‘ ğ‘– )(2) The verification outcome ğ‘ ğ‘¢ğ‘ğ‘– is binary-valued: when the model determines that the answer statement ğ‘ ğ‘– is entailed by (a combina- tion of) the retrieved passages, ğ‘ ğ‘¢ğ‘ğ‘– is assignedTrue; otherwise, it returnsFalse, indicating an unsupported statement likely contain- ing hallucinated content that consequently fails verification and must be discarded. 3.3 Supporting Evidence Selection Irrelevant information can interfere with theLLMâ€™s response gener- ation, potentially causing critical relevant details to be overlooked. While conventional RAG approaches generate answers based on coarsely aggregated retrieval results, our methodology addition- ally incorporates fine-grained evidence extraction. This necessity SIGIR-AP 2025, December 7â€“10, 2025, Xiâ€™an, China Haosheng et al. - Settings ASQA ELI5 HotpotQA MuSiQue Dataset statistics Task Long-form QA Long-form QA Multihop QA Multihop QA Question Type Factoid How/Why/What Factoid Factoid # Examples 948 1000 500 500 Evaluation metrics Correctness EM Recall Claim Recall EM Recall EM Recall Citation Quality Citation Recall, Citation Precision, Citation F1 Table 1: Statistics of different datasets. arises from the fundamental misalignment between retriever and generator objectives in