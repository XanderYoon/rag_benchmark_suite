verified statements and citations as foun- dational input material, executing three critical transformations: restructuring logical flow and sentence sequencing to enhance co- herence, eliminating redundant content to improve conciseness, and strategically consolidating citations to optimize referential clarity. ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™_ğ‘ğ‘›ğ‘ =ğ‘…ğ‘’ ğ‘“ ğ‘–ğ‘›ğ‘’(ğ‘, ğ‘ƒ, ğ‘  1, ğ‘1, ğ‘ 2, ğ‘2, . . .)(6) To mitigate potential referential ambiguity and ensure contex- tual fidelity, the original retrieved passages are incorporated into the modelâ€™s input stream. This architectural choice provides es- sential grounding context, enabling more accurate interpretation of statement semantics and preventing summarization errors aris- ing from ambiguous references. Furthermore, explicit instructional constraints mandate that the model preserve the original semantic content of input statements without modification while simultane- ously ensuring the final output maintains both informational com- pleteness and fluent logical progression. The model must achieve this dual objective through careful rhetorical reorganization rather than content alteration. The instruction template at this stage is shown in Appendix A.4. 4 Experiments 4.1 Datasets and Models To comprehensively evaluate our methodâ€™s effectiveness across diverse question types, we conduct experiments on four bench- mark datasets. The long-form QA datasets include ASQA [31], an ambiguity-aware factual dataset distinguished from conventional benchmarks by its exclusive focus on ambiguous questions sourced from AmbigQA [22]. Each query admits multiple valid interpre- tations, necessitating models to recognize inherent ambiguities and synthesize comprehensive responses using evidentiary support. Complementing this, ELI5 [4] comprises predominantly non-factual questions originating from Redditâ€™s â€œExplain Like Iâ€™m Fiveâ€2 forum. Characterized by complexhow,why, andwhatqueries, this dataset presents significant challenges in generating logically coherent, information-rich long-form explanations. For multi-hop reasoning evaluation, we employ HotpotQA [ 40] and MuSiQue [ 36]. Hot- potQA features curated factual questions requiring cross-document 2https://www.reddit.com/r/explainlikeimfive VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via Rigorous Verification SIGIR-AP 2025, December 7â€“10, 2025, Xiâ€™an, China evidence integration through manually