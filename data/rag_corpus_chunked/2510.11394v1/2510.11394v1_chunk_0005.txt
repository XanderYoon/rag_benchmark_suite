yielding robust attribution capabilities. APO [17] advances training methodology by formulating attribu- tion as preference learning and introducing a progressive optimiza- tion framework with sentence-level rewards that enhances align- ment efficiency. Distinctively, LongCite [42] tackles fine-grained citation in long-context QA through its Coarse to Fine (CoF) data construction scheme, enabling precise sentence-level attribution with superior traceability relative to passage-level alternatives. Un- like approaches requiring fine-tuning, alternative approaches em- ploy prompting to instruct models to incorporate citations during answer generation. ALCE [ 8] systematically evaluated multiple few-shot citation generation strategies, including Vanilla, Sum- mary, and Snippet. Alternative research efforts have designed more sophisticated reasoning pipelines dedicated to enabling models to perform proactive verification and citation refinement during generation. For instance, VTG [32] introduces a document storage mechanism with long short-term memory, implements an active retrieval component that generates diversified queries, and incor- porates a hierarchical verification module featuring an evidence finder to validate relationships between generated answers and their citations. Contrastingly, “extrinsic attribution” methods incorporate cita- tions during post-processing. These methods first generate an initial answer (with or without citations) using a generative model, then establish correspondences between the generated text and retrieved passages through text matching techniques, and finally insert appro- priate citations [11]. This strategy enables attribution even for mod- els lacking inherent citation capabilities. For instance, WebGLM’s automated citation annotation pipeline utilized the ROUGE-1 [18] similarity metric to evaluate citation correctness, filtering higher- quality training data. Beyond text similarity metrics, alternative approaches leverage NLI models to determine entailment relation- ships between answer sentences and retrieved passages, assigning citations based on classification results. ALCE implemented this NLI approach as a representative post-processing baseline method. Effective evaluation methodologies are indispensable for advanc- ing citation generation research, with established approaches en- compassing both human assessment and automated metrics [25, 38]. VeriCite: