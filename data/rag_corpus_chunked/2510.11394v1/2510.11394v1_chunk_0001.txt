response based on all available contexts and has its claims verified through the NLI model; 2) thesupporting evidence selectionassesses the utility of each document and extracts useful supporting evidences; 3) thefinal answer refinementintegrates the initial response and collected evidences to produce the final, refined answer. We conduct experiments across five open-sourceLLMs and four datasets, demonstrating that VeriCite can significantly improve citation quality while maintaining the correctness of the answers. 1 ∗Corresponding author. 1Our code is publicly available at https://github.com/QianHaosheng/VeriCite This work is licensed under a Creative Commons Attribution 4.0 International License. SIGIR-AP 2025, Xi’an, China ©2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2218-9/2025/12 https://doi.org/10.1145/3767695.3769505 CCS Concepts •Information systems → Information systems applications. Keywords Large Language Model, Retrieval-Augmented Generation, Response Attribution ACM Reference Format: Haosheng Qian, Yixing Fan, Jiafeng Guo, Ruqing Zhang, Qi Chen, Dawei Yin, and Xueqi Cheng. 2025. VeriCite: Towards Reliable Citations in Retrieval- Augmented Generation via Rigorous Verification. InProceedings of the 2025 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (SIGIR-AP 2025), December 7–10, 2025, Xi’an, China.ACM, New York, NY, USA, 8 pages. https://doi.org/ 10.1145/3767695.3769505 1 Introduction Retrieval-Augmented Generation (RAG) [9, 16, 30] plays a crucial role in enabling large language models (LLMs) [ 1, 19] to tackle challenges such as real-time news queries and domain-specific issues, thereby expanding the capabilities and application scope of LLMs. However, as retrieval technology is not always flawless, it simultaneously introduces new challenges for LLMs. For example, if irrelevant information is retrieved and used as a reference, the LLM may incorporate this noise and generate incorrect answers, exacerbating the hallucination issue[6, 15, 35]. Therefore, enabling LLMs to generate attributable responses is vital for ensuring trustworthiness and mitigating misinformation. An effective strategy to enhance the reliability of LLM responses is through citation mechanisms,