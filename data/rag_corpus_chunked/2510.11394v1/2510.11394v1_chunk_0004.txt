to pre-screen the content within retrieved passages that is genuinely valuable for answer generation, pre-attributing citations to these high-quality segments to ensure source traceabil- ity. This preprocessing approach helps eliminate noise from the input, significantly alleviating the cognitive load on LLMs when extracting key information from long contexts. Furthermore, the strategy of pre-attributing citations reduces the model’s attribution difficulty, enabling the generator to more seamlessly and accurately reuse existing citations within the answer. Extensive experiments conducted across multiple datasets and LLMs demonstrate that while achieving answer accuracy on par with baselines, VeriCite yields a significant improvement in citation generation quality. 2 Related Work In retrieval-augmented question answering, methods for generating attributed answers typically fall into two primary categories. The first category employs “intrinsic attribution”, leveraging generative models’ inherent attribution capabilities. This approach typically utilizes supervised fine-tuning or in-context learning to enable models to produce answers with integrated citations. Among seminal implementations, WebGPT [23] enhances open- domain question answering (QA) accuracy by simulating human web browsing behavior. Built upon GPT-3 [1], this system extracts relevant webpage passages as supporting evidence and inserts ci- tations as commandS within answers. The authors trained reward models on extensive human preference annotations, optimizing answer quality through Proximal Policy Optimization [ 29]. Sub- sequent innovations include WebGLM [20], which integrates an LLM-augmented retriever, bootstrapped generator, and a human preference-aware scorer. Its automated annotation pipeline enabled large-scale training data generation, with supervised fine-tuning on citation-annotated QA data yielding robust attribution capabilities. APO [17] advances training methodology by formulating attribu- tion as preference learning and introducing a progressive optimiza- tion framework with sentence-level rewards that enhances align- ment efficiency. Distinctively, LongCite [42] tackles fine-grained citation in long-context QA through its Coarse to Fine (CoF) data construction scheme, enabling precise sentence-level attribution with superior traceability relative to passage-level alternatives.