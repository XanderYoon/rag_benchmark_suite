28.8 34.379.0 12.0 27.180.7 27.5 41.9RGFT Ablationvariants of RuleRAG-FTRG-DPR RGFT-LLAMA2_7B24.2 13.3 37.76.6 13.9 25.622.6 14.7 30.529.9 21.6 36.726.5 15.4 34.9RGFT-DPR RG-LLAMA2_7B45.1 14.2 33.155.7 33.9 36.549.9 38.7 43.495.1 33.5 41.992.5 37.2 47.6RuleRAG-CoK RG-DPR RG-LLAMA2_7B- 5.1 17.9- 2.6 14.5- 8.4 32.2- 11.8 26.1- 9.2 25.7 Table 2: Performance comparison of RuleRAG-ICL, RuleRAG-FT and the variant of RuleRAG, RuleRAG-CoK. RG-DPR and RG-LLAMA2_7B represent rule-guided DPR and rule-guided LLAMA2_7B in RuleRAG-ICL. RGFT represents rule-guided fine-tuning in RuleRAG-FT. SSFT represents standard supervised fine-tuning. Standard Prompting does not have a retrieval stage, VE and CoK involve multiple search objects, which change several times, so there is no R@10. The best performance of RuleRAG-ICL and RuleRAG-FT are in bold. tion, we initialize the knowledge sources as our corpus D and use 3-shot CoT prompts. Moreover, since RuleRAG relies solely on rule guidance in- stead of other sophisticated techniques like reflec- tion or interleave, we also focus on the performance comparison of RuleRAG with and without rules. 4.3 Evaluation Metrics For the retrieval stage, the quality of retrieved documents is critical for downstream queries and is usually measured by Recall@k (Karpukhin et al., 2020), indicating whether the top-k blocks contain targeted information. For our task, we calculate Recall@k ( R@k,%) by checking whether the correct answer to the given query is contained in the retrieved top-k documents. The higher R@k, the more potentially useful retrievers are for generators. For the generation stage, the quality of answers is measured by Exact Match (EM,%) and Token F1 (T-F1,%), which are widely recognized in QA performance evaluation (Zhu et al., 2021). For EM, an answer is deemed correct if its normalized form corresponds to any acceptable answer in the provided ground truth lists. T-F1 treats the answers and ground truths as bags of tokens and computes the average token-level overlap between