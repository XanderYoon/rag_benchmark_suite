the closed-source GPT-3.5-Turbo to replace RGFT due to its unpublished parameters. D Implementation Details Generator fine-tuning. We fine-tune the ChatGLM2_6B, Mistral_7B_v0.2, LLAMA2_7B, LLAMA2_13B models using 2, 2, 4 and 8 V100 32G GPUs, respectively. We use LORA (Hu et al., 2022) with 4-bit, a parameter-efficient fine-tuning (PEFT) adaptation method, to deal with the enor- mous computation costs and hardware require- ments in training LLM. Hyper-parameter N is 3 and Î¸ is 0.7. The fine-tuning hyperparameters are detailed in Table 6. Similar to Lin et al. (2024), we find that the best generalization performance on the dev set can be achieved using a small number of fine-tuning epochs. We evaluate the models every 3 epochs and select the best checkpoint based on the average dev set performance. Retriever fine-tuning. We fine-tune DPR and SimCSE on 4 V100 32G GPUs using their public codes with a lr of 1e-5, a batch size of 32, and a temperature of 0.01. The base models are down- loaded from their GitHub website. E The Robustness of RuleRAG In the inferring process, since we can not know the content of the queries in advance, we may match some relevant rules for the queries regardless of whether the queries need the guidance of rules or not. In our preliminary experiments, we also find that, in some cases, retrieving information for some queries can directly match relevant documents. Therefore, in this section, we verify the robust- ness of our proposed method RuleRAG on queries which may not need the guidance of rules. We want to know if our introduced rules will interfere with the performance of retrieval and generation of such queries. Specifically, for each query in the benchmark, we degenerate it into a new relevant query by using the previously matched rules ( [Entity 1, 14 LLM