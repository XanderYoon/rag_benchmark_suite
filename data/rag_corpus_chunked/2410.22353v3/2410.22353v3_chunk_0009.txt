RuleQA-I 557 77,508 6,594 7,440 1,559 ICEWS14RuleQA-Y 99 243,633 28,153 22,765 1,864 Y AGORuleQA-W 78 584,364 50,996 62,375 2,065 WIKIStaticRuleQA-F 367 49,088 8,082 9,645 1,233 FB15K-237RuleQA-N 234 18,177 4,351 4,764 815 NELL-995 Table 1: The statistics of our constructed benchmarks RuleQA. |R|, |D|, |FR|, |FG| and |Q| are the numbers of rules, documents in corpus, retriever fine-tuning pairs, generator fine-tuning pairs and test queries, respectively. parameters. Specifically, we randomly select three ((q, Dq, Rq), a ) pairs as fixed examples in the prompts, making up the in-context augmentation. 4 Experimental Settings 4.1 Benchmarks and Setup of RuleRAG The construction process of our constructed five rule-aware benchmarks RuleQA are in Appendix A. The statistics of RuleQA are in Table 1. For RuleRAG-ICL, in addition to adding rule guidance to both retrievers and generators (RG-retriever + RG-generator), we also add rule guidance only to the retrieval stage (RG-retriever + generator), try- ing to prove that introducing rules in two stages can both contribute to the performance. For RuleRAG- FT, the complete method involves retrievers and generators with RGFT. The ablation study shows both of them are individually beneficial to the results. To emphasize the contribution of rules, we introduce several variants of RuleRAG-FT. The SSFT in Table 2 represents the standard supervised fine-tuning following the vanilla manner, where the fine-tuning instruction consists only of the queries and retrieved documents without rules. Whether the inputs are added with rules during inference is consistent with how the models are fine-tuned. 4.2 Baselines Given that LLMs have lots of world knowledge, we report the performance of directly using LLMs as answer reasoners without retrieval (Standard Prompting in Table 2). Additionally, we com- pare RuleRAG with three baselines based on RAG. We instantiate the widespread RAG framework us- ing off-the-shelf LLMs and retrievers with queries as