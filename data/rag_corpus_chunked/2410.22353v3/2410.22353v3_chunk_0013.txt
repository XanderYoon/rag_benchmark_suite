correctness of LLMs. From their results, it is evident that although they happen to succeed in modifying some answers by using ra- tionales, they still fail to capture the logical relation- ships between the broader set of facts. The Stan- dard RAG framework has better performance than the above non-retrieval or self-verifying methods, highlighting the importance of retrieved documents for knowledge-intensive queries. However, their low performance is still unsatisfactory, suggesting that their principles of retrieval and generation are weak and leave much to be desired. In the experi- ments, we illustrate that the performance can be fur- 5 top-1 top-2 top-5 top-10 top-k retrieved documents 2 6 10 14 18 22 26 30 34 38 42 46Recall@k(%) RuleRAG-FT (DPR + RGFT-LLAMA2 7B) 6.6 10.1 17.9 24.2 14.2 20.4 29.2 38.4 22.9 29.1 36.7 45.1 DPR (EM) SSFT-DPR (EM) RGFT-DPR (EM) DPR (Recall@k) SSFT-DPR (Recall@k) RGFT-DPR (Recall@k) top-1 top-2 top-5 top-10 top-k retrieved documents 4 7 10 13 16 19 22 25 28 31 34 37Recall@k(%) RuleRAG-FT (SimCSE + RGFT-LLAMA2 7B) 8.0 10.2 15.1 19.9 10.4 14.9 20.3 25.5 15.3 20.6 28.9 36.1 SimCSE (EM) SSFT-SimCSE (EM) RGFT-SimCSE (EM) SimCSE (Recall@k) SSFT-SimCSE (Recall@k) RGFT-SimCSE (Recall@k) top-1 top-2 top-5 top-10 top-50 top-k retrieved documents 2 5 8 11 14 17 20 23 26 29 32 35 38Recall@k(%) RuleRAG-FT (BM25 + RGFT-LLAMA2 7B) 12.1 22.3 36.2 3.0 18.0 BM25 (EM) BM25 (Recall@k) 12.0 12.4 12.8 13.3 13.9 15.5 18.3 18.7 16.8 18.5 20.2 20.5 11 12 13 14 15 16 17 18 19 20 21 22 EM(%) 8 9 10 11 12 13 14 15 16 17 18 19 20 EM(%) 8.9 9.1 13.8 13.9 10.8 12.7 14.8 15.1 14.6 15.2 18.2 18.7 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 EM(%) 7.9 8.7 8.1 7.9 10.1 Figure 3: The Reacll@k