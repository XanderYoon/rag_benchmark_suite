parts, RuleRAG-FT which requires training and RuleRAG-ICL which does not. They can also be used in combination with different LLMs: small-scale LLMs (6B, 7B, 13B in our paper) and a closed-source LLM (GPT-3.5-Turbo in our paper). For different usage scenarios and requirements, we are free to choose different combinations. Sum- marizing all the results shown in this paper, we give the following heuristic decision criteria and corresponding reasons. Typically, the base performance of small-scale LLMs (the baseline Standard RAG) is low and the performance improvement of both RuleRAG-ICL and RuleRAG-FT with small-scale LLMs is very significant. Therefore, we can use the RuleRAG- ICL to get good results locally when hardware re- sources are limited. Otherwise, we recommend fine-tuning LLMs for better results. For our bench- marks, the inference time is 3-8 hours and the time for fine-tuning with the full data is 1-3 days. If users need to get inference results quickly in a short time, we recommend calling APIs of closed-source LLMs. In this combination, our methodsâ€™ abso- lute performance and performance improvement are still very high (even optimal in some cases). For our benchmarks, their inference time is 0.5-2 hours. G The EM performance Trend of LLAMA2_7B and LLAMA2_13B To make a stronger argument that dataset RuleQA-I is fairly difficult, we give in Figure 6 how the EM performance of two different LLMs varies with the amount of fine-tuning dataset. From the figure, we find that the larger LLM ends up with better results (The result of LLAMA2_13B is better than LLAMA2_7B in the end), which is intuitive. LLAMA2_13B also experiences performance fluctuations, which illustrates the general chal- lenging nature of RuleQA-I for multiple LLMs. 15 Architecture RuleQA-I RuleQA-Y RuleQA-W RuleQA-F RuleQA-NRetriever GeneratorR@10 EM T-F1R@10 EM T-F1R@10 EM T-F1R@10 EM T-F1R@10 EM T-F1Standard RAG DPR LLAMA2_7B4.6 10.7 34.92.7