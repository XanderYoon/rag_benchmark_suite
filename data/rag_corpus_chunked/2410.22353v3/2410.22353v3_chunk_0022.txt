of rules. Furthermore, the comparison results with and without rules in RuleQA for RuleRAG and CoK on existing RAG datasets also attest to the effectiveness of rules in broader scenarios. In the future, we will explore how to adapt rules in more complex RAG frame- works and use custom rules for more QA tasks. 8 Limitations Since existing RAG datasets do not have adapted rules, which have been widely used for knowledge- intensive reasoning tasks, we use mature KG rule mining algorithms to match rules for our con- structed benchmarks RuleQA. Although the ex- periments on four existing RAG datasets, includ- ing ASQA, PopQA, HotpotQA and NQ, initially demonstrated that the guideline of rules in RuleQA can be generalized to them and yielded perfor- mance gains, the gains were limited because the rules were not customized for them. Therefore, we plan to match rules for more RAG datasets and vali- date the rules on more RAG models to demonstrate the generic usefulness, since all RAG-based meth- ods involve the two basic processes of retrieval and generation. References Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511. Parishad BehnamGhader, Santiago Miret, and Siva Reddy. 2023. Can retriever-augmented language models reason? the blame game between the re- triever and the language model. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15492â€“15509, Singapore. Association for Computational Linguistics. Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Milli- can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Mag- giore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan,