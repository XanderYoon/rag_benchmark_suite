by Exact Match (EM,%) and Token F1 (T-F1,%), which are widely recognized in QA performance evaluation (Zhu et al., 2021). For EM, an answer is deemed correct if its normalized form corresponds to any acceptable answer in the provided ground truth lists. T-F1 treats the answers and ground truths as bags of tokens and computes the average token-level overlap between them (Li et al., 2023b). 5 Experimental Results 5.1 Main Results Table 2 shows the overall experimental results in the five rule-aware QA benchmarks detailedly and provides a comprehensive comparison between our proposed RuleRAG-ICL, RuleRAG-FT, the variant of RuleRAG, RuleRAG-CoK, and all the baselines, under the instantiation of DPR (Karpukhin et al., 2020) and LLAMA2_7B (Touvron et al., 2023) as retrievers and generators. As a baseline without retrieval, LLAMA2_7B using standard prompting can only refer to the knowledge it acquired during pre-training. Unsurprisingly, we notice that Stan- dard Prompting (LLAMA2_7B) yields the worst relative and absolute results in all the five bench- marks, revealing that parametric knowledge in LLMs makes it hard to answer our factual queries. Furthermore, the results of Standard Prompting avoid the concern that the performance improve- ment of subsequent experiments comes from in- trinsic knowledge in LLMs. This also gives a side note to the challenges of our constructed five bench- marks and motivates the introduction of rules. The CoT-based methods, VE and CoK, use the rationales corrected by the retrieved knowledge to enhance the factual correctness of LLMs. From their results, it is evident that although they happen to succeed in modifying some answers by using ra- tionales, they still fail to capture the logical relation- ships between the broader set of facts. The Stan- dard RAG framework has better performance than the above non-retrieval or self-verifying methods, highlighting the importance of retrieved documents for