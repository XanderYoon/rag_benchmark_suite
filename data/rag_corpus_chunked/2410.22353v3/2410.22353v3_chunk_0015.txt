and +15.3 in T-F1 compared to the best performance of RuleRAG-ICL). In addi- tion to Standard RAG-based RuleRAG-ICL and RuleRAG-FT, RuleRAG can also be applied to many advanced RAG-based models. As a variant of RuleRAG, RuleRAG-CoK introduces the idea of rule-guided RAG into CoK. The performance improvement achieved is attributed to our proposal. To further corroborate that these gains are due to the introduced rules, we first isolate the key component, rules, from fine-tuning data for RGFT, to form the standard supervised fine-tuning (SSFT) (Rule Ablation in Table 2) and then isolate the impact of the fine-tuned generator from the fine- tuned retriever in RuleRAG-FT (RGFT Ablation in Table 2). RGFT Ablation shows both RGFT-DPR and RGFT-LLAMA2_7B are beneficial when used individually, implicitly suggesting that the two phases do not depend on each other. Moreover, Rule Ablation shows when we no longer leverage rules to explicitly inform the retrievers of the retrieval directions (SSFT-DPR) or how LLMs should correctly utilise the retrieved documents while fine-tuning (SSFT-LLAMA2_7B), our recall and generation performances show varying degrees of degradation compared to RuleRAG-FT. This further clarifies the great assistance of rules on the ability to answer knowledge-intensive queries. 5.2 Further Analysis on Retrievers Retrievers in RuleRAG-FT In Figure 3, we initialize RuleRAG-FT with more retrievers: dense retrievers DPR (Karpukhin et al., 2020), SimCSE (Gao et al., 2021) and training-free sparse retriever BM25 (Robertson and Zaragoza, 2009), and we use several retrieval configurations: retrievers without fine-tuning or with SSFT/RGFT while recalling different numbers of top-scored documents. Before fine- tuning, the Recall@k and EM performance of the three retrievers are comparable and each has their own performance characteristic, with no obvious advantages or disadvantages. For instance, DPR has the best Recall@10 and SimCSE has the best EM under top-10 documents before fine-tuning. After fine-tuning, DPR consistently