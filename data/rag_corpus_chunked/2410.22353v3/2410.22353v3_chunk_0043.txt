LLAMA2_13B has more parameters, leading to fluctuating more and ultimately performing better. H The Difference of RuleQA and Existing RAG Datasets Most existing RAG datasets for QA only pro- vide questions and corpora when construction and do not match suitable rules for them. In this paper, we construct five rule-aware QA bench- marks RuleQA, where many high-quality rules are mined from KGs to guide the retrieval and reasoning. Meanwhile, we experimentally show that both the introduced rules and our proposed model RuleRAG are effective in four existing RAG datasets, ASQA (Stelmakh et al., 2022) (long- form QA), PopQA (Mallen et al., 2023)(short-form QA), HotpotQA (Yang et al., 2018)(multi-hop QA) and Natural Questions (NQ) (Kwiatkowski et al., 2019). In addition, although they are widely leveraged in evaluating the QA performance of LMs, we find that all these datasets are primarily focused on multi-hop and comparison-type questions and pay less attention to queries that require logical thinking to reason. As we know, many queries in the real world are not justified by relevance alone, because in many cases the lexical level of rele- vance is not the information that can support the answer to the query, and even introduces a lot of noise instead. Therefore, in this paper, we construct five rule-aware QA benchmarks RuleQA based on five popular static KGs or temporal KGs to empha- size the importance of rules in the QA task. It is worth noting that our described construction way in Section A is general and easy to reproduce. For newly defined rule patterns, we can quickly con- struct corresponding benchmarks using the above construction way, showing its better scalability. Moreover, our constructed RuleQA also provide corresponding fine-tuning datasets, which aim to improve the retrieval and generation ability of LMs. Currently, obtaining high-quality and plentiful su- pervised data