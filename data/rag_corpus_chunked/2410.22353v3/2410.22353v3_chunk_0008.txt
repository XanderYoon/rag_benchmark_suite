fashion with the following loss function Lr q: Lr q = − log es(d+ i ,q◦r) es(d+ i ,q◦r) + P d− j ϵB/Do es(d− j ,q◦r) , (3) where B is the documents for all the queries in one training batch. Do is oracle documents for the query and B/Do is its in-batch negative exam- ples. The final training goal of RGFT-retriever is to minimize the overall loss L = P ((q,r),Do)∈FRLr q. Rule-guided generator fine-tuning (RGFT- generator). To obtain greater model efficiency, we fine-tune the generators in RuleRAG-FT, en- hancing the proficiency to reason accurate answers following rules. Formally, the designed instruction contains three parts: the relevant facts Dq retrieved by retrievers fine-tuned above, the rulesRq guiding attributable retrieval logics and the original queryq. In practice, for open-source LLMs, we utilize the few-shot instruction fine-tuning strategy considering the following two aspects. First, our introduced rules reform the data-centric training to the alignment of task-centric abilities, i.e., it can be viewed as a reasoning task based on the guidance of rules (Zhou et al., 2023) and our training aim is to learn to use them. Secondly, tuning all the data is prohibitive. We randomly select a fixed number of samples to conduct few-shot tuning (2048 samples in our practice). For closed-source LLMs, we per- form 3-shot prompts as an empirical substitute of fine-tuning (Dai et al., 2023) due to the unavailable Benchmarks|R| |D| |FR| |FG| |Q| Source KG Temporal RuleQA-I 557 77,508 6,594 7,440 1,559 ICEWS14RuleQA-Y 99 243,633 28,153 22,765 1,864 Y AGORuleQA-W 78 584,364 50,996 62,375 2,065 WIKIStaticRuleQA-F 367 49,088 8,082 9,645 1,233 FB15K-237RuleQA-N 234 18,177 4,351 4,764 815 NELL-995 Table 1: The statistics of our constructed benchmarks RuleQA. |R|, |D|, |FR|, |FG| and |Q| are the numbers of rules, documents in corpus, retriever fine-tuning pairs, generator fine-tuning