and reason in RAG. We also introduce RuleRAG into an advanced model CoK to empha- size the paradigm of Rule-guided RAG is suitable for different RAG methods. Moreover, we conduct experiments on four existing RAG datasets. As a result, RuleRAG is beneficial for our constructed RuleQA and for introducing rules to existing RAG datasets. RuleRAG-CoK shows the attribution of the advanced RAG-based variant of RuleRAG un- der RuleQA and existing RAG methods. 2 Related Work 2.1 Retrieval-Augmented Generation In RAG, the retrieval module explicitly augments the generation module with external knowledge banks (Guu et al., 2020). Retrieval approaches include sparse retrievers based on sparse bag-of- words, dense retrievers based on dense vectors and more complex hybrid search algorithms (Li et al., 2023a). RAG is widely adopted to complement the LLM parametric knowledge along different stages (Gao et al., 2024), including pre-training stage (RETRO (Borgeaud et al., 2022), COG (Lan et al., 2023), Atlas (Izacard et al., 2024)), fine- tuning stage (SURGE (Kang et al., 2023), Self- RAG (Asai et al., 2023), CoN (Yu et al., 2023)) and inference stage (KnowledGPT (Wang et al., 2023), DSP (Khattab et al., 2023), RoG (Luo et al., 2024)). 2.2 Knowledge-intensive QA In the realm of QA, queries are viewed as knowledge-intensive if we need access to exter- nal corpora (Thorne et al., 2018). Assuming that documents in the corpora include the exact answers, 2 Figure 2: The framework of our proposed RuleRAG. RuleRAG-ICL relies on in-context learning with the guidance of rules. RuleRAG-FT involves fine-tuning retrievers and generators ahead. (a) The unified RuleRAG inference process. (b) Rule-guided retriever fine-tuning (RGFT-retriever). (c) Rule-guided generator fine-tuning (RGFT-generator). RAFT (Zhang et al., 2024a) and RA-DIT (Lin et al., 2024) fine-tune LLMs by concatenating documents and queries as prompts. However, many answers to factual queries are hidden in