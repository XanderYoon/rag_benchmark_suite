our proposed method RuleRAG on queries which may not need the guidance of rules. We want to know if our introduced rules will interfere with the performance of retrieval and generation of such queries. Specifically, for each query in the benchmark, we degenerate it into a new relevant query by using the previously matched rules ( [Entity 1, 14 LLM lr lora r lora alpha lora dropout warm-up batch size epochs model parallel seq len ChatGLM2_6B 3e-5 4 16 0.05 5 8 50 1 5120 Mistral_7B_v0.2 3e-5 4 16 0.05 5 8 50 1 5120 LLAMA2_7B 3e-4 8 32 0.05 5 8 50 2 5120 LLAMA2_13B 3e-4 16 32 0.05 10 4 50 4 5120 Table 6: Hyperparameters for RGFT-Generators. r1, Entity 2] leads to [Entity 1, r2, Entity 2] ) and ensure that the answer is unchanged and that the relevant documents can be retrieved directly from the corpus. Meanwhile, according to the principle of performance comparison, we try to minimize interference with the original queries. For instance, the original query is What is the nationality of Jean -Luc Godard? and the rule is that “ [Entity 1, born in, En- tity 2] leads to [Entity 1, has nationality, Entity 2] ”. Then, we convert the query into Where is Jean-Luc Godard born? . In this way, these queries can theoretically be successfully retrieved with related documents and correctly answered without the guidance of rules. In order to test the robustness of our rule-guided approach RuleRAG to such queries, we first conduct the Standard RAG on them as a baseline and then test the performance of RuleRAG by adding our previously matched rules. Hence, the only difference in the input of LMs between the main experiment and this experiment is the queries. The others, including rules and answers, remain