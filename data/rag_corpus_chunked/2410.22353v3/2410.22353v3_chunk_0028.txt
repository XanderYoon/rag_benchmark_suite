large lan- guage models via dynamic knowledge adapting over heterogeneous sources. In The Twelfth International Conference on Learning Representations. Ruotong Liao, Xu Jia, Yangzhe Li, Yunpu Ma, and V olker Tresp. 2024. GenTKG: Generative forecast- ing on temporal knowledge graph with large language models. In Findings of the Association for Computa- tional Linguistics: NAACL 2024, pages 4303–4317, 10 Mexico City, Mexico. Association for Computational Linguistics. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2024. RA-DIT: Retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Repre- sentations. Yushan Liu, Yunpu Ma, Marcel Hildebrandt, Mitchell Joblin, and V olker Tresp. 2022. Tlogic: Temporal logical rules for explainable link forecasting on tem- poral knowledge graphs. Proceedings of the AAAI Conference on Artificial Intelligence , 36(4):4120– 4127. LINHAO Luo, Yuan-Fang Li, Reza Haf, and Shirui Pan. 2024. Reasoning on graphs: Faithful and in- terpretable large language model reasoning. In The Twelfth International Conference on Learning Repre- sentations. Farzaneh Mahdisoltani, Joanna Biega, and Fabian M. Suchanek. 2013. Yago3: A knowledge base from multilingual wikipedias. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers) , pages 9802–9822, Toronto, Canada. Association for Computational Linguistics. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in