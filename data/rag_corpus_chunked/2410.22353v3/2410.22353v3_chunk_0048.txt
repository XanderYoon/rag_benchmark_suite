performance of our proposed meth- ods. Moreover, in the answering stage, Standard RAG naturally obtains a wrong answer based on low-quality retrieval results. However, RuleRAG- ICL and RuleRAG-FT attribute the correct answer through in-context learning and fine-tuning under the guidance of the rules. J Error Analysis We further analyzed the detailed performance of our proposed model on 60*5 incorrectly answered queries from the five benchmarks. There were three main classes of errors: (a) Rule Failure (5%): In the real world, rules can reflect the logical workings of most events. However, we cannot claim that absolutely no ex- ceptions occur. Among the incorrect responses we sampled, we found that the answers to some ques- tions did not follow the general rules of reasoning, which in turn resulted in response failures. Future work could address such special cases separately. (b) Retrieval Error (55%): In this section, we as- sume that a retrieval is considered correct as long as the correct answer is included in the top 10 recalled documents, and a retrieval is considered incorrect otherwise. Due to the very large size of the cor- pus and the large number of documents that are semantically similar but do not support the answer, even a fine-tuned retriever may not recall relevant facts for the correct answer. In almost all cases, the question can not be answered correctly if the retrieved documents are wrong. (c) Attribution Error (40%): Due to the complex logical relationships between events, when the re- trieved documents contain the correct answer, the generator may still fail to follow the rules and then come up with an incorrect answer. Generally, the more documents in the top 10 retrieved information that are related to the correct answer, the higher the probability that the generator will answer correctly. The problem of attribution error