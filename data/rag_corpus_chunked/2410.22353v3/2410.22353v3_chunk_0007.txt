2 (b) and (c). For rule-guided retriever fine-tuning (RGFT-retriever), we update the LM encoders in a contrastive learning objective (Chen et al., 2020) and train over supervised fine-tuning data provided in our constructed benchmarks , where inputs are the queries plus rules and supervised labels are heuristic oracle documents. For rule-guided gen- erator fine-tuning (RGFT-generator), we adopt the supervised instruction-tuning objective (Iyer et al., 2023) while combining query q with two compo- nents: retrieved documents Dq and the set of rules 3 Rq consistent with the retrieval phase. The rules introduced in the RGFT-generator train LLMs to optimally reason from the retrieved context into answers via attributable rules, making RuleRAG leverage our fine-tuned retrievers more rationally. Rule-guided retriever fine-tuning (RGFT- retriever). We utilize two main types of retrievers: sparse and dense retrievers. As the sparse retriever, we use Pyserini to implement the standard training- free BM25 (Robertson and Zaragoza, 2009), which relies on word-level frequencies. As the dense re- trievers, we adopt the dual-encoder based retriever architecture, such as DPR and SimCSE. We freeze the document encoder and tune the query encoder for high retrieval efficiency (Lewis et al., 2020). Given a ((q, r) , Do) pair in the fine-tuning data, where Do serve as the oracle documents, each d+ i ∈ D o is a positive learning example while each in-batch d− j ̸∈ D o is a negative example. We train the retrievers in an in-batch contrastive training fashion with the following loss function Lr q: Lr q = − log es(d+ i ,q◦r) es(d+ i ,q◦r) + P d− j ϵB/Do es(d− j ,q◦r) , (3) where B is the documents for all the queries in one training batch. Do is oracle documents for the query and B/Do is its in-batch negative exam- ples. The final training