rules with high confidence (Liao et al., 2024) and we only transform these high-confidence rules to the above text string form, comprising our rule bank R, which will be consistently leveraged in the training and inferring process of RuleRAG. Test dataset Q. To avoid skewed entity distribu- tion, we include links with both popular and long- tail entities in KG test sets and adjust their numbers to achieve balance. The remaining links are con- verted into queries with tail entities in these links as ground truths. Different from PopQA (Mallen et al., 2023) with more low-popularity entities from Wiki- data, our benchmarks consider entities in uniform distribution from five knowledge bases, aiming to show the more general effectiveness of our method. Corpus D and fine-tuning datasets, FR and FG. Different from EntityQuestions (Sciavolino et al., 2021), we linearize the links in KG training sets into documents by concatenating entity, relation and time, forming concise and distinct factoids in D, which serves as the retrieval source of RuleRAG. For RGFT, we split valid sets of KGs into two disjoint parts and convert the KG links of both parts into queries: one part is for queries in the fine-tuning datasets FR for retrievers and the other part is for queries in the fine-tuning datasets FG for generators. Specifically, we search the corresponding oracle document examples from D for each query-rule pair by entity name and relation-matching heuristics and take them as the golden training labels of the retrievers. Subsequently, we leverage the fine-tuned retrievers to retrieve relevant documents for each query in FG and create fine-tuning instructions for generators by combining retrieval results, rules and queries, with golden answers as supervision. Benchmarks with temporal queries, named RuleQA-I, RuleQA-Y and RuleQA-W, are constructed based on three temporal KGs, ICEWS14 (García-Durán et al., 2018),