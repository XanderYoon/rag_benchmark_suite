LLM ends up with better results (The result of LLAMA2_13B is better than LLAMA2_7B in the end), which is intuitive. LLAMA2_13B also experiences performance fluctuations, which illustrates the general chal- lenging nature of RuleQA-I for multiple LLMs. 15 Architecture RuleQA-I RuleQA-Y RuleQA-W RuleQA-F RuleQA-NRetriever GeneratorR@10 EM T-F1R@10 EM T-F1R@10 EM T-F1R@10 EM T-F1R@10 EM T-F1Standard RAG DPR LLAMA2_7B4.6 10.7 34.92.7 3.6 19.70.6 2.3 30.215.2 11.0 27.620.6 12.8 25.9RuleRAG-ICL RG-DPR RG-LLAMA2_7B11.8 11.9 35.55.3 9.5 23.45.9 2.4 32.526.0 17.0 39.924.9 17.6 36.3RuleRAG-FT RGFT-DPR RGFT-LLAMA2_7B39.8 16.6 36.346.8 28.7 33.834.9 15.9 34.194.1 35.9 48.933.7 20.4 37.5 Table 7: The performance of RuleRAG-ICL and RuleRAG-FT for queries which may not need the guidance of rules to retrieve or generate. The results reflect the robustness of our methods. 1/8 2/8 3/8 4/8 5/8 6/8 7/8 1 Fine-tuning data ratio 13 14 15 16 17 18 19 20 21 22EM performance (%) LLAMA2 7B LLAMA2 13B Figure 6: The EM performance of RuleRAG-FT in RuleQA-I with RGFT-LLAMA2_7B and RGFT- LLAMA2_13B under increasing fine-tuning data ratio. The retriever is kept as RGFT-DPR. In addition, we observe that in the second half of the fine-tuning process (the ratio from 4/8 to 1), both LLMs have similar change curves (up, then down, then up again), and the magnitude of change was greater for LLAMA2_13B than for LLAMA2_7B. We speculate that this is because both LLMs have similar model architectures, and thus the learning processes during fine-tuning are similarly guided; whereas, LLAMA2_13B has more parameters, leading to fluctuating more and ultimately performing better. H The Difference of RuleQA and Existing RAG Datasets Most existing RAG datasets for QA only pro- vide questions and corpora when construction and do not match suitable rules for them. In this paper, we construct five rule-aware QA bench- marks RuleQA, where many high-quality rules are mined