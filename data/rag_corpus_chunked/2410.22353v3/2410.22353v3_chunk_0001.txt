ICL improves the retrieval quality of +89.2% in Recall@10 and answer accuracy of +103.1% in Exact Match, and RuleRAG-FT yields more en- hancement. In addition, experiments on four ex- isting RAG datasets show RuleRAG is also ef- fective by offering rules in RuleQA to them, fur- ther proving the generalization of rule guidance in RuleRAG. Code and RuleQA are at https: //anonymous.4open.science/r/RuleRAG. 1 Introduction Large language models (LLMs) have achieved the impressive capability of language generation and knowledge learning (Brown et al., 2020; Ouyang et al., 2022). Despite the success, the full-parametric knowledge in LLMs struggles to precisely manipulate fine-grained queries, espe- cially in knowledge-intensive tasks (Jiang et al., 2023c; Shao et al., 2023). As complementary, RAG shows superior performance in many NLP tasks, such as open-domain QA (Trivedi et al., 2023) and natural language inference (Qin et al., 2023). However, two high-level issues exist in the cur- rent RAG. First, in the retrieval phase, the retriev- ers rely on word-level matching, and thus can not guarantee that the recalled information is always pertinent to the query answering. The reason is many retrievers are trained on unsupervised text or trained end-to-end, leading to their insufficiency in retrieving the necessary statements for reason- ing (BehnamGhader et al., 2023). Secondly, in the generation phase, the LLMs in the current RAG are not specifically informed of how to exploit noisy retrieved content properly, since relationships be- tween a wide range of facts are rarely explicitly “pointed out” and “supervised” in the pre-training corpora of LLMs. Even if answered correctly, they still lead to implicit attribution processes that are difficult to explain and verify. Therefore, the cur- rent RAG is neither inherently trained to retrieve along reasonable retrieval directions nor organi- cally attribute retrieved content to answers. While answering knowledge-intensive queries, a priori rules