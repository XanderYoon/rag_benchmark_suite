Dr q from all rules in Rq are assembled to produce the final retrieval results Dq, and RuleRAG-ICL conditions on the query q, rules Rq and documents Dq to reason the answer a. Rule-guided retriever (RG-retriever). The re- triever calculates a relevant score s(di, q ◦ r) be- tween (q,r) and every document di ∈ D: s(di, q ◦ r) = Ed(di) · Eq (q ◦ r), where ◦ is sequence con- catenation, · is dot product, Ed is document en- coder and Eq is query encoder. We select the top-k documents, Dr q, and combine all Dr q as the final retrieval results Dq. This process is formalized as: Dq = S r∈Rq Dr q; Dr q = arg top-k di∈D s (di, q ◦ r) . (1) Rule-guided generator (RG-generator). After recalling Dq, we construct an instruction to prompt LLMs to reason answers. Different from implicit case-based prompts (Wei et al., 2024), we directly inform LLMs of Rq as the attribution mechanisms and reason with the guidance of Rq. The probabil- ity of outputting answer a can be approximated as: P (a | q, R, D) ≈ PLLM (a | INS (q, Rq, Dq)), (2) where PLLM () is the LLM generation probability. INS () is instruction prompt, whose simplified form is in Figure 2 (c) and details are in Appendix K. 3.2 RuleRAG-FT The overview of our proposed rule-guided retriever and generator fine-tuning is shown in Figure 2 (b) and (c). For rule-guided retriever fine-tuning (RGFT-retriever), we update the LM encoders in a contrastive learning objective (Chen et al., 2020) and train over supervised fine-tuning data provided in our constructed benchmarks , where inputs are the queries plus rules and supervised labels are heuristic oracle documents. For rule-guided gen- erator fine-tuning (RGFT-generator), we adopt the supervised instruction-tuning