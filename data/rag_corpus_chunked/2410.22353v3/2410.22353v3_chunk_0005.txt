on in-context learning with the guidance of rules. RuleRAG-FT involves fine-tuning retrievers and generators ahead. (a) The unified RuleRAG inference process. (b) Rule-guided retriever fine-tuning (RGFT-retriever). (c) Rule-guided generator fine-tuning (RGFT-generator). RAFT (Zhang et al., 2024a) and RA-DIT (Lin et al., 2024) fine-tune LLMs by concatenating documents and queries as prompts. However, many answers to factual queries are hidden in semantically dissim- ilar but logically related documents, which need to be retrieved and reasoned with the guidance of rules. Our constructed RuleQA simulates this cir- cumstance, while most existing RAG datasets lack rules. Recently, Wu et al. (2024) investigates miti- gating misleading irrelevant interference; Sun et al. (2024) only discusses the rule-following abilities of LLMs without retrieval and ignores how to obtain rules. In contrast, our proposed RuleRAG involves a more comprehensive consideration of mining rules, retrieving documents and reasoning answers. 3 Proposed Method: RuleRAG 3.1 RuleRAG-ICL Figure 2 (a) illustrates the inference flow of RuleRAG-ICL. Given a queryq ∈ Q, we first lever- age Sentence-BERT (Reimers and Gurevych, 2019) to capture the semantic similarity between q and candidate rules. The highest N rules among those whose scores exceed a certain threshold θ are taken as guiding rules Rq, where N and θ are hyper- parameters. Then, we append q with one rule r ∈ Rq once at a time to avoid conflict and conduct rule- guided retrieval in the corpus D to obtain the top-k documents Dr q. Finally, Dr q from all rules in Rq are assembled to produce the final retrieval results Dq, and RuleRAG-ICL conditions on the query q, rules Rq and documents Dq to reason the answer a. Rule-guided retriever (RG-retriever). The re- triever calculates a relevant score s(di, q ◦ r) be- tween (q,r) and every document di ∈ D: s(di, q ◦ r)