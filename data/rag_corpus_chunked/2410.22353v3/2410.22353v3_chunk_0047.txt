the query in the form of “Time {time} what does {subject} {relation} ?”, we provide a collection of text consisting of multiple documents in the form of “Time {time} {subject} {relation} {object}.” Your response should directly generate the missing {object}. # Retrieved documents: Documents related to the Query. Time 2014-06-23 Abdullah Abdullah Expel or withdraw peacekeepers Election Commission (Afghanistan). Time 2014-02-20 Abdullah Abdullah Make a visit Afghanistan. · · · Time 2014-07-16 Abdullah Abdullah Make a visit Ashraf Ghani Ahmadzai. · · · Time 2014-09-20 Abdullah Abdullah Make a visit Foreign Affairs (United States). # Rules: Use the following Two rules to answer the given Query. Rule One: [Entity1, Abduct, hijack, or take hostage, Entity2] leads to [Entity1, Make a visit, Entity2]. Rule Two: [Entity1, Make a visit, Entity2] leads to [Entity1, Make a visit, Entity2]. # Query: Time 2014-12-01 what does Abdullah Abdullah Make a visit ? # Answer: Afghanistan. Table 9: Instruct prompt. only one out of the top 10 documents contains the correct answer “Citizen (Nigeria)”. RG-DPR’s re- trieval results are more relevant to the query entity and semantically support the answer. Meanwhile, 5 of the top 10 documents contain the correct answer. The retrieval quality of the fine-tuned RGFT-DPR is the best. All the retrieved documents are strongly supportive while answering the query through the given rules. In addition, 8 out of the top 10 doc- uments contain correct answers, which further re- flects the strong performance of our proposed meth- ods. Moreover, in the answering stage, Standard RAG naturally obtains a wrong answer based on low-quality retrieval results. However, RuleRAG- ICL and RuleRAG-FT attribute the correct answer through in-context learning and fine-tuning under the guidance of the rules. J Error Analysis We further analyzed the detailed performance of our proposed model on 60*5 incorrectly answered