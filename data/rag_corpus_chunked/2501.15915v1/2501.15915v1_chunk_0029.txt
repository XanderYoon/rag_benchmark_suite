of the rewriting and question-answer (QA) generation steps in the document augmen- tation process, we conduct a series of ablation experiments by removing (1) both rewriting and QA, (2) QA alone, and (3) rewriting alone. The experimental results are shown in Figure 3, and we have the following observations: (1) When neither rewriting nor QA generation is employed, the performance consistently degrades significantly across all evaluated tasks and models. This reduction suggests that simply training the LLM on the selected document via the next token prediction task without any form of data augmenta- tion leads to insufficient internalization of facts by the model. (2) 9All the training code and data are publicly available at our anonymous GitHub repository: https://github.com/oneal2000/PRAG Parametric Retrieval Augmented Generation Conference, Under Review, LLaMA Qwen 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 2WQA LLaMA Qwen 0.14 0.16 0.18 0.20 0.22 HQA LLaMA Qwen 0.00 0.05 0.10 0.15 0.20 PQA LLaMA Qwen 0.20 0.23 0.25 0.28 0.30 0.33 0.35 CWQ w/o Both w/o QA w/o Rewrite with Both Figure 3: Ablation study on the impact of the document aug- mentation stage. LLaMA indicates LLaMA-3.2-1B, and Qwen indicates Qwen-2.5-1.5B. The metric used is the F1 Score. Removing either QA or rewriting alone yields better results than removing both, indicating that each step offers distinct benefits. However, we notice that removing QA leads to a more significant performance decline than removing rewriting. This observation suggests that QA pair generation is more crucial for pushing the model to recall and apply factual information while rewriting offers valuable diversity in phrasing and structure and benefits the overall performance. (3) Incorporating both rewriting and QA results in the strongest overall performance on most of the evaluated tasks and models. These findings reinforce that rewriting and QA generation play complementary roles. In general,