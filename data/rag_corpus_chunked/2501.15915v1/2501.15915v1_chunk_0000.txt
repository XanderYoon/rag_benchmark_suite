Parametric Retrieval Augmented Generation Weihang Su swh22@mails.tsinghua.edu.cn DCST, Tsinghua University Beijing 100084, China Yichen Tang∗ DCST, Tsinghua University Beijing 100084, China Qingyao Ai† aiqy@tsinghua.edu.cn DCST, Tsinghua University Beijing 100084, China Junxi Yan DCST, Tsinghua University Beijing 100084, China Changyue Wang DCST, Tsinghua University Beijing 100084, China Hongning Wang DCST, Tsinghua University Beijing 100084, China Ziyi Ye DCST, Tsinghua University Beijing 100084, China Yujia Zhou DCST, Tsinghua University Beijing 100084, China Yiqun Liu DCST, Tsinghua University Beijing 100084, China Abstract Retrieval-augmented generation (RAG) techniques have emerged as a promising solution to enhance the reliability of large language models (LLMs) by addressing issues like hallucinations, outdated knowledge, and domain adaptation. In particular, existing RAG methods append relevant documents retrieved from external corpus or databases to the input of LLMs to guide their generation process, which we refer to as the in-context knowledge injection method. While this approach is simple and often effective, it has inherent limitations. Firstly, increasing the context length and number of relevant documents can lead to higher computational overhead and degraded performance, especially in complex reasoning tasks. More importantly, in-context knowledge injection operates primarily at the input level, but LLMs store their internal knowledge in their pa- rameters. This gap fundamentally limits the capacity of in-context methods. To this end, we introduce Parametric retrieval-augmented generation (Parametric RAG), a new RAG paradigm that integrates external knowledge directly into the parameters of feed-forward networks (FFN) of an LLM through document parameterization. This approach not only saves online computational costs by elimi- nating the need to inject multiple documents into the LLMs’ input context, but also deepens the integration of external knowledge into the parametric knowledge space of the LLM. Experimental results demonstrate that Parametric RAG substantially enhances both the effectiveness and efficiency of knowledge augmentation in LLMs. Also, it can be