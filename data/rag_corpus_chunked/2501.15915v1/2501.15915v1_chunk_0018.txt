depends on the number of in- put and output tokens. For simplicity, we focus on input tokens and ignore the variance in output tokens since they vary signifi- cantly from tasks and LLMs. Let |ğ‘| represent the length of input prompt/question ğ‘, and ğ‘¡ be the number of retrieved documents. Conference, Under Review, Su, et al. Since the time needed to load the LoRA parameters for ğ‘¡ docu- ments is neglectable, the inference time complexity of our method is O (|ğ‘|2â„ + |ğ‘|â„2). In contrast, the time complexity of in-context RAG methods is O (ğ‘¡ |ğ‘‘ | + | ğ‘|) 2â„ + ( ğ‘¡ |ğ‘‘ | + | ğ‘|)â„2, which means that our method can save O ğ‘¡ 2|ğ‘‘ |2â„ + ğ‘¡ |ğ‘‘ ||ğ‘|â„ + ğ‘¡ |ğ‘‘ |â„2 time for online inference. Empirically, suppose that the lengths of ğ‘ and ğ‘‘ are approximately the same and significantly smaller than the hidden dimension of the LLM (e.g., about 4096 for LLaMA-8B), and we retrieve ğ‘¡ = 6 documents for each ğ‘, then our method can roughly save 6|ğ‘‘ | tokens in inference. Compared to its offline cost, this means that Parametric RAG is more cost-friendly than in-context RAG when the number of queries is more than twice that of documents in the life cycle of the service. In summary, while the offline preprocessing step in Parametric RAG introduces additional computational overhead compared to traditional RAG, our analysis demonstrates that, when the system handles a large number of queries, Parametric RAG can provide a more carbon-efficient solution for large-scale RAG systems. 3.4.2 Storage Overhead. In Parametric RAG, storage overhead comes from the Parametric Representation of each document, which consists of low-rank matrices from the FFN layer. Letğ‘Ÿ be the LoRA rank, ğ‘› be the number of Transformer layers, â„ be the hidden