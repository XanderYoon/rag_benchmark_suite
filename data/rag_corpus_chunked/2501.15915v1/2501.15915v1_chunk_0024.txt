NVIDIA A100 GPUs with 40GB of memory. Preprocessing and Parameterization. Consistent with prior works [19, 20, 44, 45], we utilize Wikipedia dumps as our external knowledge corpus, specifically adopting the dataset7 proposed by DPR [20]. For document augmentation, each document is rewritten once, and three QA pairs are generated based on the document 8 (using the downstream LLM, if not mentioned explicitly). In the LoRA fine-tuning process, the learning rate was set to3 Ã— 10âˆ’4, and the training epoch was set to1. The LoRA modules were exclusively integrated into the feed-forward network (FFN) matrices, excluding the query, key, and value (ğ‘„ğ¾ğ‘‰ ) matrices. The scaling factor ğ›¼ was configured to 32, LoRA rank ğ‘Ÿ was set to 2, and no dropout was applied during training to ensure stability and full utilization of 6All the prompt templates used in this paper are available in our GitHub repository: https://github.com/oneal2000/PRAG/blob/main/all_prompt.md 7https://github.com/facebookresearch/DPR/tree/main 8The detailed prompt template for document augmentation is publicly available on our official GitHub repository. the parameter updates. The LoRA weight is randomly initialized following the setting of the original LoRA paper [14]. Retrieval Module. Recent studies on retrieval-augmented gen- eration (RAG) [33] reveal that BM25 performs on par with, or even outperforms, state-of-the-art dense models in some scenarios. Given its strong performance, simplicity, and low computational cost, we adopt BM25 as the retriever for our approach. We use Elasticsearch as the backend for implementing BM25, with detailed configuration settings and instructions available on our official GitHub repository. Generation Configuration. All experiments are conducted us- ing the publicly released Hugging Face implementations of LLaMA and Qwen. We adopt the default hyperparameters and chat tem- plate provided in the official Huggingface repository, with the only modification being the use of greedy decoding to ensure the repro- ducibility of our reported results. 5 Experiments