categories and HQA into two. To comprehensively compare the performance of P-RAG and other RAG baselines across different reasoning tasks, our main experi- mental table (Table 1) presents the performance for each sub-task separately, using the first 300 questions from each sub-dataset. Ta- ble 1 also presents the overall performance of each RAG baseline on the two datasets in the “Total” column. Since the original datasets contain uneven distributions of question types, the “Total” column is not a simple average of the sub-dataset performances. 4.2 Baselines We choose the following RAG baselines for comparison: • Standard RAG. This RAG method directly appends the top re- trieved documents to the LLM’s input prompt. The prompt ex- plicitly instructs the LLM to refer to the provided documents when answering the question and also includes instructions on the output format expected from the model. • DA-RAG incorporates the augmented documents and QA pairs using the Data Augmentation method introduced in § 3.2.1. This baseline aims to demonstrate that the performance improvement observed in Parametric RAG does not stem from the data aug- mentation phase but from the in-parameter knowledge injection. • FLARE [19] is a multi-round retrieval augmentation method that triggers retrieval each time it encounters an uncertain to- ken. When the retrieval module is triggered, the last generated sentence without the uncertain tokens is defined as the query. • DRAGIN [45] is a multi-round retrieval augmentation method. It triggers retrieval when an uncertain token has semantic meaning and also has a strong influence on the following tokens. When the retrieval module is triggered, it formulates the query based on the model’s internal state and preceding context. • P-RAG directly injects relevant documents into the LLM’s pa- rameters through document parameterization, enabling efficient RAG without increasing the input context length. • Combine Both