parametric document representations. For example, we could pre-train the random LoRA with a couple of few-shot examples following the same method described with Eq. (5) and save the LoRA weight ğ‘Šğ‘¤ğ‘ğ‘Ÿğ‘š âˆ’ğ‘¢ğ‘ to initialize the training of each docu- mentâ€™s LoRA (i.e., the documentâ€™s parametric representation). Our experiment (Section Â§ 5.2) demonstrates that this warm-up process can significantly improve the performance compared to random initialization on RAG tasks, indicating that a task-aware initializa- tion can further enhance the effectiveness of parametric knowledge injection for specific downstream tasks. Yet, we use random ini- tialization for LoRA if not mentioned explicitly for simplicity and broad applicability across various tasks. 3.3 Online Inference In the previous stage (Â§3.2.2), we generated a set of document- specific low-rank parameters for each document in the corpus ğ¾. This section describes how these parameters are utilized for RAG pipelines. Given a user query ğ‘, our proposed Parametric RAG pipeline proceeds through three steps:Retrieve, Update, and Generate. The following section details each step and illustrates the underlying mathematical operations. 3.3.1 Retrieve. We first use a retrieverğ‘… to calculate a relevance score ğ‘†ğ‘‘ğ‘– for each document ğ‘‘ğ‘– âˆˆ ğ¾ to the query ğ‘. We then select the top-ğ‘˜ documents with the highest relevance scores, denoted 5The loss is computed not only on the answer but also across the entire concatenated sequence, including the documents and the question as {ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘˜ } âŠ† ğ¾ as the relevant external knowledge. Each retrieved document ğ‘‘ğ‘– has a corresponding parametric representa- tion, i.e., a pair of low-rank matrices ğ´ğ‘–, ğµğ‘– , previously obtained by the procedure described in Â§3.2.2. 3.3.2 Update. After retrieval, we merge the low-rank matrices from the top-ğ‘˜ retrieved documents to form a single plug-in module for the LLM. Following the setting of LoRA [14]