of our proposed Parametric RAG pipeline. Consider an LLM (denoted as L) with base parameters ğœƒ. Given a user query ğ‘, we aim to generate an accurate response using an external corpus ğ¾. Formally, the corpus ğ¾ is defined as: ğ¾ = {ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘ }, where each ğ‘‘ğ‘– represents a text chunk, such as documents, Wikipedia articles, or passages (for convenience, we refer to each ğ‘‘ğ‘– as â€˜documentâ€™ in the following sections). The system contains a retrieval module ğ‘… that calculates the relevance score of each document {ğ‘†ğ‘‘1, ğ‘†ğ‘‘2, . . . , ğ‘†ğ‘‘ğ‘ } corresponding to the query ğ‘. Traditional RAG paradigms select the top ğ‘˜ documents with the highest relevance scores as relevant external knowledge and append them to the input context of the L. This process is typically guided by a prompt template that instructs L to generate the response based on the provided knowledge. In contrast to the in-context RAG paradigm that injects relevant documents into the LLMâ€™s input context, in Parametric RAG, we propose to insert documents directly into the parameters of L. To achieve this, the Parametric RAG framework is designed with two stages: an offline document parameterization stage and an online inference stage with a Retrieve-Update-Generate workflow. Offline document Parameterization. In this step (illustrated in Figure 2), we offline transform each document inğ¾ into a parametric representation, thereby forming a set of parameters known as the Parametric Corpus ğ¾ğ‘ƒ . Specifically, we define: ğ¾ğ‘ƒ = {ğ‘ğ‘– | ğ‘ğ‘– = ğ‘“ğœ™ (ğ‘‘ğ‘– ), ğ‘– = 1, 2, . . . , ğ‘}, (1) where ğ‘“ğœ™ is a mapping function that converts each document ğ‘‘ğ‘– into its corresponding parametric representation ğ‘ğ‘–. We define parametric representations ğ‘ğ‘– to possess the following properties: (1) The parameters ğ‘ğ‘– can be plugged into the