system handles a large number of queries, Parametric RAG can provide a more carbon-efficient solution for large-scale RAG systems. 3.4.2 Storage Overhead. In Parametric RAG, storage overhead comes from the Parametric Representation of each document, which consists of low-rank matrices from the FFN layer. Letğ‘Ÿ be the LoRA rank, ğ‘› be the number of Transformer layers, â„ be the hidden size, and ğ‘™ the intermediate size of FFN, then the number of parameters in the Parametric Representation of a document is 2ğ‘›ğ‘Ÿ (â„ + ğ‘™). For example, with the LLaMA3-8B model (32 layers, hidden size 4096, intermediate size 14336), we need to store approximately 2.36M extra parameters (with ğ‘Ÿ = 2 as used in our experiments). Stored at 16-bit precision, this requires around 4.72MB per document. While the storage requirements for Parametric RAG may seem substantial compared to the raw documents, there are multiple methods to reduce its cost in practice. For example, previous studies find that the access of information in real user traffic follows a long- tail distribution [38]. Taking Google as an example, about 96.55% of Web pages receive zero traffic, and only 1.94% get one to ten visits per month [39]. Therefore, creating parametric representations for a tiny set of head documents can serve the majority of user requests, which significantly reduces the storage cost of Parametric RAG. Also, as shown in our experiments, Parametric RAG can be used with in-context RAG together for downstream tasks. Thus, it can serve as a natural boost to existing RAG methods without breaking their system pipelines. 4 Experimental Setup In this section, we detail the experimental framework used to evalu- ate Parametric RAG. We begin with the introduction of our selected benchmark datasets(Â§4.1). Next, we introduce our selected baseline methods (Â§4.2) and implementation. Finally, we provide implemen- tation details