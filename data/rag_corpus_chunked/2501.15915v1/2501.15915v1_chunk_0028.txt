(P-RAG Warm.) consistently out- performs random initialization (P-RAG Rand.). This demonstrates the effectiveness of task-aware pretraining in enhancing the Para- metric RAG pipeline. Furthermore, the observed improvements across varying model sizes confirm the scalability and generality of this approach. The superior performance of the warm-up ap- proach in downstream tasks can be attributed to two key factors. First, it effectively aligns the additional LoRA parameters with the base LLM before document parameterizing, ensuring a smoother integration of knowledge. Second, it facilitates the incorporation of task-relevant knowledge, including output formats and generation patterns, which are critical for enhancing the quality of response in certain tasks. This finding suggests that in practical Parametric RAG applications where the downstream task is fixed, warming up the LoRA parameters for the task offers a promising approach to boost effectiveness. It is important to note that our main ex- periments (as well as all other experiments in this paper) were conducted using random initialization without any task-specific optimizations or dataset-specific tuning. This further highlights the strong generalization capability of our proposed Parametric RAG paradigm. These findings also highlight a broader insight: embedding few- shot examples either in the modelâ€™s context or directly into its parameters leads to improved downstream task performance. In- terestingly, our proposed parametric information representation method offers compatibility with few-shot in-context learning, en- abling a combination of parametric and in-context knowledge aug- mentation. 5.3 Impact of Document Augmentation To investigate the individual contributions of the rewriting and question-answer (QA) generation steps in the document augmen- tation process, we conduct a series of ablation experiments by removing (1) both rewriting and QA, (2) QA alone, and (3) rewriting alone. The experimental results are shown in Figure 3, and we have the following observations: (1) When neither rewriting nor QA generation is employed, the performance