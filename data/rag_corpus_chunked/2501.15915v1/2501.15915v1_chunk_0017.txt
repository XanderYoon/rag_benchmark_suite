The computation cost of our method can be divided into offline preprocessing cost and online inference cost. The offline cost primarily arises from the Parametric Docu- ment Encoding (Â§3.2.2). Let |ğ‘‘ | be the average number of tokens in a document ğ‘‘, and â„ be the hidden dimension size of the LLM. The computational complexity of a typical decoder-only LLM is O (|ğ‘‘ |2â„ + |ğ‘‘ |â„2), where the attention layers complexity is O (|ğ‘‘ |2â„) plus the FFN layers O (|ğ‘‘ |â„2). Theoretically, our method only in- troduces a constant coefficient change on the number of tokens processed, thus the overall time complexity remainsO (|ğ‘‘ |2â„+|ğ‘‘ |â„2). Based on our implementation settings detailed in Â§ 4.3, the Data Augmentation process takes the original document ğ‘‘ as input and subsequently generates approximately 2|ğ‘‘ | new tokens. This pro- cess requires computational costs equivalent to a forward pass over 3|ğ‘‘ | tokens, including the decoding of 1|ğ‘‘ | tokens and the infer- ence of 2|ğ‘‘ | tokens. Training LoRA parameters on these augmented tokens requires a forward pass over 3|ğ‘‘ | tokens and a backward pass equivalent to processing 6|ğ‘‘ | tokens (typically about twice the forward-pass cost), resulting in an overall computational cost equivalent to processing 9|ğ‘‘ | tokens. Adding the 3|ğ‘‘ | tokens from the Document Augmentation phase, the total computational cost is approximately the cost of decoding 12|ğ‘‘ | tokens in the LLM. The online inference cost mainly depends on the number of in- put and output tokens. For simplicity, we focus on input tokens and ignore the variance in output tokens since they vary signifi- cantly from tasks and LLMs. Let |ğ‘| represent the length of input prompt/question ğ‘, and ğ‘¡ be the number of retrieved documents. Conference, Under Review, Su, et al. Since the time needed