This approach not only saves online computational costs by elimi- nating the need to inject multiple documents into the LLMs’ input context, but also deepens the integration of external knowledge into the parametric knowledge space of the LLM. Experimental results demonstrate that Parametric RAG substantially enhances both the effectiveness and efficiency of knowledge augmentation in LLMs. Also, it can be combined with in-context RAG methods to achieve even better performance1. Keywords Large Language Model, Retrieval Augmented Generation, Knowl- edge Representation, Parametric Information Representation 1 Introduction Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of information retrieval (IR) and ∗Contributed equally † Corresponding author 1We have open-sourced all the code, data, and models in the following anonymized GitHub link: https://github.com/oneal2000/PRAG natural language processing (NLP) tasks [5, 6, 11, 36, 48, 55]. De- spite these successes, a critical limitation remains: once training is complete, an LLM’s internal knowledge becomes effectively static, making it challenging to incorporate newly emerging information or knowledge not included in its pre-training data. To address this challenge, retrieval-augmented generation (RAG) has emerged as a prominent solution. RAG enables LLMs to dynamically access and utilize information beyond their pre-trained parameters by retriev- ing relevant information from an external corpus, thus improving their adaptability and performance [4, 12, 16, 18, 23, 44–46]. Existing studies have explored various aspects of the RAG pipeline, considering factors such as retrieval timing [2, 18, 44, 45], document selection [21, 58], and external knowledge organization [10, 15, 32]. While these innovations improve different stages of the pipeline, all RAG methods, regardless of their variations, share a common characteristic: they inject external knowledge by directly adding passages or documents into the input context of LLMs, which we refer to as the in-context knowledge injection. Although this in-context knowledge injection approach is straight-