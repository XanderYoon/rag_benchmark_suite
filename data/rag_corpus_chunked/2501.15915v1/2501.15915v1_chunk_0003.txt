is to conduct supervised fine-tuning (SFT) with retrieved documents, thereby incorporating relevant knowledge directly into the LLMâ€™s parameters. However, arXiv:2501.15915v1 [cs.CL] 27 Jan 2025 Conference, Under Review, Su, et al. PromptTemplate MergetheParameters MergedDocumentRepresentation {RetrievedDocuments}-----------------------------------------------------------------------AnswerthefollowingQuestionbasedontheprovidedinformation:Question:{Question} LLMWeight:ğœ½ ğœ½!=ğœ½+âˆ†ğœ½ğœ½âˆ†ğœ½=ğ’‡(ğ’Œ,ğœ½) LLMWeight:ğœ½!LLMWeight:ğœ½ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â· Â·Â·Â·Â·Â·Â·Tokenize TraditionalRAG: InjectRetrievedDocumentstotheInputContext OriginalLLM OriginalLLM Question FillinthePromptTe m p l a te ParametricRAG:InjectRetrievedDocumentstotheLLMâ€™sParameter Response Question UserInput Retrieved Documents Corresponding Parametric Representations of Documents Response Figure 1: An illustration of the comparison of in-context RAG and our proposed Parametric RAG paradigms: In-context RAG combines the tokens of relevant documents and the query in the input, using the original LLM ğœƒ to answer the question without modifying its parameters. Our proposed Parametric RAG updates the LLMâ€™s parameters ğœƒ â€² = ğœƒ + Î”ğœƒ based on the retrieved documents, temporarily integrating relevant knowledge into LLMâ€™s parameters to answer the question. such SFT-based methods are considered suboptimal as they require substantial computational resources and GPU memory, making it impractical to inject relevant documents online for every query. In addition, they can negatively affect the original ability of the LLM to follow instructions [7, 54] and lack the flexibility of in-context methods, which allow external knowledge to be added or removed on the fly. The observations above inspire us to raise the following research question: Is it possible to inject external knowledge into LLM pa- rameters effectively, efficiently, and flexibly for retrieval-augmented generation? To this end, we introduce a new RAG paradigm, Parametric Retrieval Augmented Generation (Parametric RAG), which directly injects the external knowledge into the Feed-Forward Net- works (FFN) of an LLM. Specifically, our approach begins with an offline preprocessing phase that parameterizes each document from the external corpus, transforming them into a small set of parameters (usually a few MB per document) that can be directly integrated into the downstream LLM. We refer to this