the relevant external knowledge. Each retrieved document ğ‘‘ğ‘– has a corresponding parametric representa- tion, i.e., a pair of low-rank matrices ğ´ğ‘–, ğµğ‘– , previously obtained by the procedure described in Â§3.2.2. 3.3.2 Update. After retrieval, we merge the low-rank matrices from the top-ğ‘˜ retrieved documents to form a single plug-in module for the LLM. Following the setting of LoRA [14] convention, we use a scalar scaling factor ğ›¼ to modulate the final update. The merged weight update, Î”ğ‘Šmerge, is computed by summing over all retrieved documents: Î”ğ‘Šmerge = ğ›¼ Â· ğ‘˜âˆ‘ï¸ ğ‘—=1 ğ´ğ‘— ğµâŠ¤ ğ‘— . (6) Intuitively, Î”ğ‘Šmerge combines the knowledge from multiple rele- vant documents into a single low-rank update that can be applied to the LLMâ€™s base parameters. Once we obtainÎ”ğ‘Šmerge, we update the original feed-forward weight ğ‘Š by: ğ‘Š â€² = ğ‘Š + Î”ğ‘Šmerge, thus yielding the final set of parameters for that layer at inference time. Conceptually, ğ‘Š â€² encodes the base modelâ€™s knowledge plus the aggregated knowledge from the top-ğ‘˜ retrieved documents. 3.3.3 Generate. After updating all feed-forward layers in the Trans- former with Î”ğ‘Šmerge, we obtain a temporary model Lâ€² (ğœƒ â€²), where ğœƒ â€² represents the updated model parameters, which are obtained by incorporating the merged low-rank parameters for all retrieved doc- uments. We can then directly use Lâ€² to generate the final response to the query ğ‘ using a standard left-to-right decoding process. 3.4 Discussion on Time/Space Efficiency 3.4.1 Computation Cost. The computation cost of our method can be divided into offline preprocessing cost and online inference cost. The offline cost primarily arises from the Parametric Docu- ment Encoding (Â§3.2.2). Let |ğ‘‘ | be the average number of tokens in a document ğ‘‘, and â„ be the hidden dimension size of the LLM. The computational complexity of a typical decoder-only LLM