robust and in- formative parametric representations for documents. Specifically, for each document, we prompt3 the LLM L to rewrite the content multiple times using different wording, styles, or organizational structures. Formally, each documentğ‘‘ğ‘– is transformed into multiple rewritten documents {ğ‘‘ğ‘– 1, ğ‘‘ğ‘– 2, . . . , ğ‘‘ğ‘–ğ‘› }, which preserve the orig- inal facts but vary in language expression. Once each document has been rewritten into multiple documents, we prompt L again to generate question-answer (QA) pairs based on the original docu- ment ğ‘‘ğ‘–. For each document ğ‘‘ğ‘–, L produces a set of questions and their corresponding answers: {(ğ‘ğ‘– 1, ğ‘ğ‘– 1), (ğ‘ğ‘– 2, ğ‘ğ‘– 2), . . . , (ğ‘ğ‘–ğ‘š, ğ‘ğ‘–ğ‘š)}, where ğ‘š is a tunable hyperparameter representing the number of QA pairs we aim to generate per document. Integrating multiple rewrites with corresponding QA pairs transforms each documentğ‘‘ğ‘– into a more comprehensive resource ğ·ğ‘– that preserves its original factual content while incorporating diverse linguistic variations. Formally: ğ·ğ‘– = {(ğ‘‘ğ‘–ğ‘˜, ğ‘ğ‘– ğ‘— , ğ‘ğ‘– ğ‘— ) | 1 â‰¤ ğ‘˜ â‰¤ ğ‘›, 1 â‰¤ ğ‘— â‰¤ ğ‘š}, (2) where each (ğ‘‘ğ‘–ğ‘˜, ğ‘ğ‘– ğ‘— , ğ‘ğ‘– ğ‘— ) triple corresponds to a rewritten docu- ment ğ‘‘ğ‘–ğ‘˜ from the original document ğ‘‘ğ‘–, coupled with a question ğ‘ğ‘– ğ‘— and its respective answer ğ‘ğ‘– ğ‘— . 2Our experimental results corroborate these conclusions, as shown in Figure 3. 3Due to space constraints, we have not included the specific prompts in the main text. However, all prompts used in this study are available at the following anonymous link: https://github.com/oneal2000/PRAG/blob/main/all_prompt.md 3.2.2 Parametric Document Encoding. In this subsection, we in- troduce the second step of the offline document parameterization pipeline, where we leverage the augmented dataset ğ·ğ‘– (defined in Eq. 2) to train the parametric representation ğ‘ğ‘– for each docu- ment ğ‘‘ğ‘–. Specifically, we initialize