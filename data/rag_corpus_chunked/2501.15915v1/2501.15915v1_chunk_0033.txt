merging and loading the LoRA adapter. 2WQA CWQ Time(s) Speed Up Time(s) Speed Up P-RAG 2.34+0.32 1.29x 2.07+0.32 1.36x Combine Both 3.08+0.32 0.98x 2.84+0.32 0.99x Standard RAG 3.03 1.00x 2.82 1.00x FLARE 10.14 0.25x 11.31 0.25x DRAGIN 14.60 0.21x 16.21 0.17x 6 Conclusion and Future Directions This work introduces Parametric RAG, a novel framework that addresses the limitations of in-context knowledge augmentation by parameterizing external documents. Parametric RAG infuses these parameterized documents directly into the model, reducing contextual overload and online computational costs while maintain- ing robust performance. Our experiments on multiple benchmarks demonstrate that Parametric RAG outperforms traditional retrieval- augmented generation methods across different LLMs. Ultimately, Parametric RAG offers a more efficient and scalable pathway to integrate external knowledge into LLMs, paving the way for further innovation in parametric-based knowledge augmentation. Despite its significant potential, Parametric RAG presents sev- eral challenges that warrant further investigation. First, the current parameterization process is computationally intensive, and the para- metric representations of each document are substantially larger than plain text. Future work could explore more methods to improve computational and storage efficiency, making the parameterization process more scalable. Second, the parameterized documents are currently tied to specific LLMs, restricting their ability to generalize across different models. Developing universal, model-agnostic rep- resentations could significantly enhance flexibility and reuse across diverse systems. Finally, we believe the potential applications of information parameterization can be extended beyond RAG. For instance, LLM-based agents could benefit from parameterizing the agentâ€™s profiles and configuration, which could alleviate context- length constraints and improve online computational efficiency. By addressing these challenges, future research could unlock more potential for the Parametric RAG paradigm. References [1] Zeyuan Allen-Zhu and Yuanzhi Li. [n. d.]. Physics of Language Models: Part 3.1, Knowledge Storage and Extraction. In Forty-first International Conference on Machine Learning. [2] Akari Asai,