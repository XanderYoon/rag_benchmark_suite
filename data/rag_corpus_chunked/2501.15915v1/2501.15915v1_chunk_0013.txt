document. Each document in the corpus is associated with its instance of Î”ğœƒ. For each document ğ‘‘ğ‘–, we train its corresponding parametric representation Î”ğœƒ using its corresponding augmented dataset ğ·ğ‘–. Recall from Eq. 2 that ğ·ğ‘– contains triplets ğ‘‘ğ‘–ğ‘˜, ğ‘ğ‘– ğ‘— , ğ‘ğ‘– ğ‘—. For each triplet, we concatenate ğ‘‘ğ‘–ğ‘˜, ğ‘ğ‘– ğ‘— , and ğ‘ğ‘– ğ‘— into a token sequence: ğ‘¥ = [ ğ‘‘ğ‘–ğ‘˜ âŠ• ğ‘ğ‘– ğ‘— âŠ• ğ‘ğ‘– ğ‘— ], (4) where [ Â· âŠ• Â· ] indicates concatenation. Let ğ‘‡ be the total number of tokens in ğ‘¥. We adopt a standard sequential language modeling objective to ensure that the LLM internalizes knowledge from the entire augmented text (i.e., both the documents and QA pairs). Specifically, we optimize: min Î”ğœƒ âˆ‘ï¸ (ğ‘‘ğ‘– ğ‘˜ ,ğ‘ğ‘– ğ‘— ,ğ‘ğ‘– ğ‘— ) âˆˆğ·ğ‘– ğ‘‡âˆ‘ï¸ ğ‘¡ =1 âˆ’ log ğ‘ƒğœƒ +Î”ğœƒ ğ‘¥ğ‘¡ ğ‘¥<ğ‘¡ , (5) 4Other parameter-efficient methods (e.g., Adapters or Prefix-Tuning) could also be used; exploring them is left for future work. In this work, we chose LoRA because it offers practical advantages over other alternatives. For example, LoRA is easier to merge compared to Adapters, and it requires less computational overhead during inference compared to Prefix-Tuning. Parametric Retrieval Augmented Generation Conference, Under Review, where ğœƒ are the frozen pretrained parameters of the LLM, and Î”ğœƒ = {ğ´, ğµ} are the trainable low-rank matrices introduced in Eq. 3. The innermost summation is taken over all tokens ğ‘¥ğ‘¡ in the con- catenated input sequence (document, question, and answer)5. This design inherently encourages the LLM to internalize the factual details in the documents into its parameters during training. Al- though the generated question-answer pairs do not directly cover all the facts within the document, repeated training on the docu- mentsâ€™ tokens allows the model to reinforce its understanding of the textual content. Consequently, once