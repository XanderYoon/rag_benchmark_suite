preceding context to fetch relevant external knowledge dynamically. To summarize, existing RAG approaches have explored vari- ous aspects of the RAG pipeline, considering factors such as re- trieval timing [3, 19, 25, 44, 45, 57], prompt template for in-context knowledge injection [49, 53], document selection [58], and external knowledge organization[10, 15, 32]. While these innovations im- prove different stages of the pipeline, all RAG methods, regardless of their variations, share a common characteristic at the knowledge injection level: relevant passages or documents are appended di- rectly to the LLMâ€™s input context to inject external knowledge. In contrast, our proposed Parametric RAG paradigm diverges from all the existing RAG frameworks by directly injecting documents into the LLMâ€™s parameters. This shift in knowledge integration addresses the inherent limitations of the in-context knowledge injection methods employed in all existing RAG frameworks. 3 Methodology In this section, we introduce our proposed Parametric RAG frame- work, shown in Figure 1. This section begins by formulating the problem and providing an overview of the Parametric RAG frame- work (Â§3.1). Next, we introduce the Offline Document Parameteri- zation process (Â§3.2), which transforms documents into parametric representations through Document Augmentation and Para- metric Document Encoding . Finally, we introduce the Online Inference procedure (Â§3.3), where the parametric representations are retrieved, merged, and integrated into the LLM to generate responses. 3.1 Problem Formulation and Overview This subsection introduces the problem formulation of the RAG task and provides an overview of our proposed Parametric RAG pipeline. Consider an LLM (denoted as L) with base parameters ğœƒ. Given a user query ğ‘, we aim to generate an accurate response using an external corpus ğ¾. Formally, the corpus ğ¾ is defined as: ğ¾ = {ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘ }, where each ğ‘‘ğ‘– represents a text chunk, such as documents,