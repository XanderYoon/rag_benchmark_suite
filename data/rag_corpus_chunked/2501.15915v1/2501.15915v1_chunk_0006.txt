text, allowing the LLM to leverage knowledge beyond its training data [23]. Parametric Retrieval Augmented Generation Conference, Under Review, Building upon the traditional RAG framework, several exten- sions have been proposed to improve its effectiveness and efficiency. One such extension, Adaptive RAG [17, 52], introduces adaptive retrieval strategies that actively adjust the retrieval pipeline based on the complexity of the query to improve the adaptability of RAG in different tasks. From another angle, to make in-context knowl- edge injection more effective in RAG scenarios, IR-CoT [49] designs prompt templates specifically tailored for RAG that demonstrate how to perform chain-of-thought (CoT) reasoning based on the given passage. Each sentence in the CoT reasoning content is then applied to retrieve more relevant documents. Another research direction, GraphRAG [10, 15, 32], employs pre-constructed knowl- edge graphs to retrieve graph elements that encapsulate relational knowledge relevant to the query. GraphRAG has demonstrated enhanced performance, particularly in tasks that require struc- tured, relational information. In the context of long-form genera- tion, where the LLM’s informational needs may change during the generation process, dynamic RAG techniques have been developed to actively decide when and what to retrieve during the generation process [3, 19, 25, 44, 45, 57]. For example, FLARE [19] triggers the retrieval module when the model’s token prediction probability falls below a predefined threshold. Similarly, DRAGIN [45] models the real-time information needs of the LLM, generating queries based on the model’s internal state and preceding context to fetch relevant external knowledge dynamically. To summarize, existing RAG approaches have explored vari- ous aspects of the RAG pipeline, considering factors such as re- trieval timing [3, 19, 25, 44, 45, 57], prompt template for in-context knowledge injection [49, 53], document selection [58], and external knowledge organization[10, 15, 32]. While these innovations im- prove different stages of