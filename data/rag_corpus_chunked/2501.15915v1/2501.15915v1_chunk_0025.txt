available on our official GitHub repository. Generation Configuration. All experiments are conducted us- ing the publicly released Hugging Face implementations of LLaMA and Qwen. We adopt the default hyperparameters and chat tem- plate provided in the official Huggingface repository, with the only modification being the use of greedy decoding to ensure the repro- ducibility of our reported results. 5 Experiments 5.1 Main Experiment In this section, we present the main experimental result and an in-depth analysis of our proposed Parametric RAG compared with other RAG baselines and a combined setting that leverages both parametric and in-context knowledge injection. The experimen- tal results are presented in Table 1, and we provide the following analysis: (1) Overall Analysis. P-RAG outperforms existing RAG frameworks in most of the benchmarks and LLMs evaluated. This trend is especially obvious for Qwen-1.5B and LLaMA-8B. The improvements suggest that the incorporation of knowledge into model parameters can enhance the overall performance of the RAG pipeline, enabling the model to recall and reason over the injected Conference, Under Review, Su, et al. Table 2: Ablation study on the impact of LoRA weight ini- tialization strategies for P-RAG. All metrics reported are F1 scores. “P-RAG Rand. ” and “P-RAG Warm. ” indicate ran- domly initialized LoRA weights and warm-up LoRA initial- ization, respectively. The best results are in bold. 2WQA HQA PQA CWQ LLaMA-1B P-RAG Rand. 0.2764 0.1999 0.2205 0.3482 P-RAG Warm. 0.3546 0.2456 0.2035 0.4263 Qwen-1.5B P-RAG Rand. 0.3025 0.2165 0.1885 0.3280 P-RAG Warm. 0.3542 0.2718 0.2418 0.5018 LLaMA-8B P-RAG Rand. 0.3932 0.3563 0.2413 0.4541 P-RAG Warm. 0.4201 0.4499 0.2952 0.5591 knowledge more effectively. Furthermore, since these gains are observed in models from different series and parameter sizes, the results underscore the robustness and broad applicability of P-RAG. (2) Comparison with DA-RAG. DA-RAG incorporates all the content generated