RAG), which directly injects the external knowledge into the Feed-Forward Net- works (FFN) of an LLM. Specifically, our approach begins with an offline preprocessing phase that parameterizes each document from the external corpus, transforming them into a small set of parameters (usually a few MB per document) that can be directly integrated into the downstream LLM. We refer to this set of pa- rameters as the parametric representation of the document. In the inference phase, we conduct retrieval augmented generation fol- lowing a Retrieve-Update-Generate (RUG) workflow as shown in Figure 1. The Retrieve step retrieves top-n documents from the external corpus based on the input prompt following the same pro- cedure used by the existing RAG pipeline. Then, the Update step uses the parametric representations of the retrieved documents to update the LLM. Finally, in the Generate step, we use the updated LLMs to conduct inference directly based on the original input prompt. Theoretical and empirical analysis show that our Parametric RAG method has superior inference efficiency and outperforms state-of- the-art in-context methods on several RAG benchmarks that involve tasks with complex reasoning. While the preprocessing phase of Parametric RAG introduces an offline computational overhead, this cost is affordable and even neglectable compared to the online cost of large-scale inference requests, leading to long-term savings in terms of power and carbon footprints. Also, similar to in-context RAG, our method can adapt to various numbers of input documents on the fly. Furthermore, our proposed Parametric RAG pipeline is in parallel with existing in-context methods. As shown in our experiments, combining our methods with in-context RAG could produce even better performance on the benchmark datasets. This indicates that parametric knowledge injection could be a fruitful direction for the future development of the RAG system. In summary, this paper makes the following key