text. However, all prompts used in this study are available at the following anonymous link: https://github.com/oneal2000/PRAG/blob/main/all_prompt.md 3.2.2 Parametric Document Encoding. In this subsection, we in- troduce the second step of the offline document parameterization pipeline, where we leverage the augmented dataset ğ·ğ‘– (defined in Eq. 2) to train the parametric representation ğ‘ğ‘– for each docu- ment ğ‘‘ğ‘–. Specifically, we initialize these parametric representations as low-rank matrices corresponding to the feed-forward network (FFN) parameter matrix ğ‘Š of the LLM L, following the LoRA ap- proach [14]. This design allows each document ğ‘‘ğ‘– to be associ- ated with independently trained low-rank parameters, allowing the model to internalize the knowledge specific to ğ‘‘ğ‘– in a parameter- efficient manner.4 Suppose the Transformer layers in L have a hidden dimension â„, and the feed-forward network (FFN) within each layer has an intermediate dimension ğ‘˜. Consequently, each FFN layer of L con- tains a weight matrixğ‘Š âˆˆ Râ„Ã—ğ‘˜. To incorporate document-specific knowledge, we introduce low-rank matrices ğ´ and ğµ such that ğ‘Š â€² = ğ‘Š + Î”ğ‘Š = ğ‘Š + ğ´ ğµâŠ¤, (3) where ğ´ âˆˆ Râ„Ã—ğ‘Ÿ and ğµ âˆˆ Rğ‘˜ Ã—ğ‘Ÿ , with ğ‘Ÿ â‰ª min(â„, ğ‘˜). The original weight matrix ğ‘Š is kept fixed, while ğ´ and ğµ are the only trainable parameters for that layer. We denote these newly introduced param- eters as Î”ğœƒ = {ğ´, ğµ}. By combining the pre-trained weights ğ‘Š and Î”ğœƒ, the model obtains the knowledge from the selected document. Each document in the corpus is associated with its instance of Î”ğœƒ. For each document ğ‘‘ğ‘–, we train its corresponding parametric representation Î”ğœƒ using its corresponding augmented dataset ğ·ğ‘–. Recall from Eq. 2 that ğ·ğ‘– contains triplets ğ‘‘ğ‘–ğ‘˜, ğ‘ğ‘– ğ‘— , ğ‘ğ‘– ğ‘—. For each triplet, we concatenate ğ‘‘ğ‘–ğ‘˜, ğ‘ğ‘– ğ‘— , and ğ‘ğ‘– ğ‘— into a token sequence: