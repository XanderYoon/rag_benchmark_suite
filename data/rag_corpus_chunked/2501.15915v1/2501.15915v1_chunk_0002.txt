and external knowledge organization [10, 15, 32]. While these innovations improve different stages of the pipeline, all RAG methods, regardless of their variations, share a common characteristic: they inject external knowledge by directly adding passages or documents into the input context of LLMs, which we refer to as the in-context knowledge injection. Although this in-context knowledge injection approach is straight- forward and often effective, recent studies have highlighted several limitations of this paradigm. First, injecting knowledge through input prompts will inevitably increase the context length. Long context not only introduces extra computational overhead and la- tency for LLM inference, but also hurts the performance of LLMs in understanding and utilizing external knowledge, especially in tasks that involve complex reasoning [22, 26]. Second, more importantly, the way LLMs process information in context is fundamentally different from the way they utilize internal knowledge stored in their parameters. Studies have shown that LLMs store most of their knowledge within the parameters of their neural network architecture (e.g., the parameters of their feed-forward network layers) [31, 59]. Adding passages or documents in the input context could only affect the online computation of key-value (KV) pairs in the attention networks of LLMs, but not the models stored param- eters, where its knowledge is encoded [59]. This means that LLMs may never be able to utilize external knowledge as effectively as they use their internal knowledge in in-context RAG methods. A straightforward solution to this problem is to conduct supervised fine-tuning (SFT) with retrieved documents, thereby incorporating relevant knowledge directly into the LLMs parameters. However, arXiv:2501.15915v1 [cs.CL] 27 Jan 2025 Conference, Under Review, Su, et al. PromptTemplate MergetheParameters MergedDocumentRepresentation {RetrievedDocuments}-----------------------------------------------------------------------AnswerthefollowingQuestionbasedontheprovidedinformation:Question:{Question} LLMWeight: !=+金解=(,) LLMWeight:!LLMWeight: 路路路路路路路路路路路路 路路路路路路Tokenize TraditionalRAG: InjectRetrievedDocumentstotheInputContext OriginalLLM OriginalLLM Question FillinthePromptTe m p l a te ParametricRAG:InjectRetrievedDocumentstotheLLMsParameter Response Question UserInput Retrieved Documents Corresponding Parametric Representations of