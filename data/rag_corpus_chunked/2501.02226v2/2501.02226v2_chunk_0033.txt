layer to retrieve nu- anced knowledge of both coarse and fine-grained graph structures from KG, achieving a more com- prehensive and precise retrieval. • In the recommendation domain that pursues in- ference speed, excessive retrieval time can se- riously degrade the user experience resulting in user churn. Although many studies have explored selective retrieval for RAGs (Yan et al., 2024; Jiang et al., 2023), it is still an open question to determine whether an item needs to be retrieved and to reduce the retrieval time in the recommen- dation domain. We propose to use popularity to decide whether an item needs to be retrieved based on power law distribution, which greatly reduces the retrieval time. Table 2 shows that K- RagRec achieves inference times close to direct inference while maintaining high recommenda- tion accuracy. • Typical RAG methods usually incorporate the re- trieved content into the prompt as text (Wu et al., 2023b; Baek et al., 2023). However, vanilla RAG methods rely on documents and paragraphs often introduce unnecessary noise and even harmful disturbance, which can negatively impact the ac- curacy and reliability of recommendations. In addition, the structural relationships between en- tities are overlooked in typical RAG, resulting in the sub-optimal reasoning capability of LLM- based recommender systems. We propose to incorporate the retrieved knowledge subgraphs (Knowledge-GraphRAG) into the query as a graph prompt, which facilitates LLMs to better understand the retrieved knowledge subgraphs and avoids long contexts. A.5 Comparison with 10 Candidate Items In this section, we conduct additional experiments to evaluate the effectiveness of K-RagRec with a Table 5: Performance comparison of different KG RAG- enhanced LLM recommendations with candidate item numbers M = 10 on the MovieLens and Amazon Book dataset and LLama-2-7b across two metrics. The best performances are labeled in bold. ACC and R@3