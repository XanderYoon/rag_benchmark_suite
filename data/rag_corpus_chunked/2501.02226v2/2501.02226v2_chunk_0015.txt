KG are presented in Table 3. 4.1.2 Baselines In the realm of LLM-based recommendation re- search, our work pioneers the investigation of re- trieving knowledge from KGs to enhance the rec- ommendation capabilities of LLMs. Therefore, to evaluate the effectiveness, we compare our pro- posed framework with a series of meticulously crafted KG RAG-enhanced LLM recommendation baselines. We first include two typical inference- only methods Retrieve-Rewrite-Answer (Wu et al., 2023b) (KG-Text) and KAPING (Baek et al., 2023), where the former retrieves sub-graphs and textu- alizes them, and the latter retrieves triples. We exclude some knowledge reasoning path-based ap- proaches (Luo et al., 2023), as it is difficult to re- trieve faithful knowledge reasoning paths solely from the user’s interaction items. Next, we com- pare K-RagRec with various prompt-tuning ap- 1https://grouplens.org/datasets/movielens/1m/ 2https://grouplens.org/datasets/movielens/20m/ 3https://jmcauley.ucsd.edu/data/amazon/ 4https://developers.google.com/freebase proaches augmented by retrieval, including Prompt Tuning with KG-Text (PT w/ KG-Text), GraphTo- ken (Perozzi et al., 2024) with retrieval (GraphTo- ken w/ RAG) as well as G-retriever (He et al., 2024). Additionally, we evaluate our method against Lora Fine-tuning with retrieval (Lora w/ KG-Text) (Hu et al., 2021). 4.1.3 Evaluation Metrics To evaluate the effectiveness of our K-RagRec framework, we employ two widely used evalua- tion metrics: Accuracy (ACC), and Recall@k (He et al., 2020). We present results for k equal to 3, and 5. Inspired by recent studies (Zhang et al., 2024; Hou et al., 2022), we adopt the leave-one- out strategy for evaluation. Specifically, for each user, we select the last item that the user inter- acted with as the target item and the 10 interaction items prior to the target item as the historical in- teractions. Then, we leverage LLM to predict the user’s preferred item from a pool of 20 candidate items (M = 20), which contains one target item with nineteen