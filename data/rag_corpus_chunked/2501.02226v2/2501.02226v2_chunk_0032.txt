for sequential recommendations. To further capture higher-order collaborative knowledge and enhance the model’s ability to generalize users and items, TokenRec (Qu et al., 2024) proposes a masked vector-quantized tokenizer to tokenize users and items in LLM-based recommendations. Despite their effectiveness, these models often face chal- lenges, such as hallucinations and the lack of up- to-date knowledge. While fine-tuning can partially mitigate these issues, it is resource-costly and time- consuming due to the massive parameters of LLMs. To overcome these challenges, we propose a knowl- edge retrieval augmented recommendation frame- work that leverages external KGs to provide reli- able and up-to-date knowledge instead of costly fine-tuning. A.4 Comparison with Existing Methods Existing LLM-based recommender systems usually require frequent fine-tuning on specific datasets to address the lack of knowledge and hallucinations, which is time-consuming and costly. To solve these challenges and avoid costly fine-tuning, our work is the first to augment the recommendation per- formance of LLMs by retrieving structured data (i.e., knowledge graph). Specifically, our approach differs from existing work in the following ways: • K-RagRec introduces an indexing GNN to ef- ficiently retrieve structured data to enhance the recommendation capability of LLMs. Al- though some GraphRAG approaches also intro- duce GNNs to capture higher-order neighbour- hood information, they only apply the last layer of the GNN representation, leading to coarse re- trieval (He et al., 2024; Mavromatis and Karypis, 2024). In contrast, our approach leverages the representation of each GNN layer to retrieve nu- anced knowledge of both coarse and fine-grained graph structures from KG, achieving a more com- prehensive and precise retrieval. • In the recommendation domain that pursues in- ference speed, excessive retrieval time can se- riously degrade the user experience resulting in user churn. Although many studies have explored selective retrieval for RAGs (Yan et al., 2024;