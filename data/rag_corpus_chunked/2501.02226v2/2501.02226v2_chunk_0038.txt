Amazon Book dataset and LLama-2-7b across three metrics. ACC and R@k denote Accuracy and Recall@k, respectively. GNN LayersACC R@3 R@5 3 layers 0.496 0.653 0.736 4 layers 0.506 0.690 0.780 5 layers 0.498 0.656 0.729 work (GCN) (Kipf and Welling, 2016) first intro- duces convolutional operations to graph-structured data. By aggregating features from neighboring nodes, GCN facilitates the learning of rich node rep- resentations. GraphSAGE (Hamilton et al., 2017) learns an aggregation function that samples and combines features from a nodeâ€™s local neighbor- hood in an inductive setting, enabling the effec- tive use of new nodes. Graph Attention Network (GAT) (Velickovic et al., 2017) further incorporates attention mechanisms, allowing the model to dy- namically assign varying attention to neighboring nodes, thereby enhancing the focus on the most relevant information. Inspired by the success of the transformer, the Graph Transformer (Shi et al., 2020) adapts transformer architectures to graph data, enhancing the modeling of graphs, particu- larly textual graphs. We report the experiment results on the MovieLens-1M dataset and the LLama-2-7b back- bone in Table 9. It is noted that the GCN Encoder variant method performs second-best on the Re- call@5 metric, although it is slightly worse than other GNN encoders on the Accuracy metric. Over- all, four GNN encoder variants exhibit close perfor- mance on the MovieLens dataset, highlighting the generality and robustness of our framework across different GNN encoders. A.11 Study of GNN Layer Numbers In this subsection, we evaluate the impact of the number of GNN layers on model performance. We vary the numbers of the GNN layer numbers in the range of {3, 4, 5} and test on the Amazon Book dataset and LLama-2-7b across three metrics. As observed in Table 10, the model performance first improves and then decreases as the number of GNN layers increases,