3.14 G-retriever 0.274 5.86 K-RagRec 0.429 1.06 pared with baselines on the MovieLens-1M dataset and LLama-2-7b. We record the time cost for one inference utilizing two NVIDIA A6000-48G GPUs. The time cost for a single inference is reported in Table 2. By observing the experimental results, we notice that various KG RAG approaches signifi- cantly increase the inference time, which is due to the large scale of the KG. In contrast, K-RagRec achieves the best computational efficiency com- pared to various KG RAG methods and is only about 0.1s slower than direct inference without re- trieval. These findings highlight the efficiency of K-RagRec and validate the effectiveness of our popularity selective retrieval policy. 4.5 Parameter Analysis In this section, we evaluate the impact of three main hyper parameters of K-RagRec, namely popular- ity selective retrieval policy threshold p, retrieved knowledge sub-graph numbers K, and re-ranking knowledge sub-graph numbers N. In addition, we analyze the impact of various GNN encoder vari- ants and different GNN layer numbers for the pro- posed framework in Appendix A.10 and A.11. 1) Impact of popularity selective retrieval pol- icy threshold p: To understand how the popular- ity selective retrieval policy threshold p affects K- RagRec, we conduct experiments on MovieLens- 1M and LLama-2-7b across two metrics. Results are shown in Figure 4. As the threshold p in- creases, the recommendation performance initially improves and then decreases. When the thresh- old p is set to a small value, only a few items are augmented. This leads to insufficient retrieval and poor recommendation accuracy. When p is set to a larger value, more items are retrieved for augmen- tation. However, due to the re-ranking sub-graph numbers N being fixed, some retrieved cold-start item knowledge sub-graphs are discarded or ranked at the back of the list, resulting in