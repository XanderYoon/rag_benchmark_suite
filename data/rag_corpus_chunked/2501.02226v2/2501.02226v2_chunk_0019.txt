experiment results, we find that eliminating any component of the framework leads to a decrease in the overall performance of the recommendations, demonstrating the effectiveness of each module. Secondly, removing the GNN Encoder leads to a 37% decrease and a 45.9% decrease in the ac- curacy of the model on MovieLens and Amazon Book datasets, respectively, highlighting the signif- icance of employing GNN to encode the structure of knowledge sub-graphs. Refer to Appendix A.6 for more details. 4.4 Efficiency Evaluation In this sub-section, we evaluate the inference effi- ciency of our proposed K-RagRec framework com- -Indexing -Popularity-Re-ranking -EncodingK-RagRec 0.20 0.25 0.30 0.35 0.40 0.45 0.50ACCURACY (a) ML1M ACC -Indexing -Popularity-Re-ranking -EncodingK-RagRec 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80RECALL@3 (b) ML1M R@3 -Indexing -Popularity-Re-ranking -EncodingK-RagRec 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90RECALL@5 (c) ML1M R@5 -Indexing -Popularity -Re-ranking -EncodingK-RagRec 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55ACCURACY (d) BOOK ACC -Indexing -Popularity-Re-ranking -EncodingK-RagRec 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75RECALL@3 (e) BOOK R@3 -Indexing -Popularity-Re-ranking -EncodingK-RagRec 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85RECALL@5 (f) BOOK R@5 Figure 3: Comparison among K-RagRec and its four ablated variants on MovieLens-1M and Amazon Book datasets and LLama-2-7b across metrics Accuracy, Re- call@3 and Recall@5. Table 2: Comparison of the inference efficiency on the MovieLens-1M dataset and LLama-2-7b in seconds (s). ACC denote Accuracy. Methods ACC Time (s) w/o RAG - 0.92 KG-Text 0.076 2.19 KAPING 0.079 6.47 GraphToken w/ RAG0.268 3.14 G-retriever 0.274 5.86 K-RagRec 0.429 1.06 pared with baselines on the MovieLens-1M dataset and LLama-2-7b. We record the time cost for one inference utilizing two NVIDIA A6000-48G GPUs. The time cost for a single inference is reported in Table 2. By observing the experimental results, we notice that various KG RAG approaches signifi- cantly increase the inference time, which