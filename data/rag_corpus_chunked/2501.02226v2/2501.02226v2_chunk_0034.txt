Comparison with 10 Candidate Items In this section, we conduct additional experiments to evaluate the effectiveness of K-RagRec with a Table 5: Performance comparison of different KG RAG- enhanced LLM recommendations with candidate item numbers M = 10 on the MovieLens and Amazon Book dataset and LLama-2-7b across two metrics. The best performances are labeled in bold. ACC and R@3 denote Accuracy and Recall@3, respectively. Methods MovieLens-1MAmazon Book ACC R@3 ACC R@3 KG-Text 0.185 - 0.142 - KAPING 0.165 - 0.119 - PT w/ KG-Text0.159 0.493 0.1230.384 GraphToken w/ RAG0.512 0.753 0.4440.682 G-retriever 0.469 0.721 0.3670.610 K-RagRec 0.568 0.779 0.6060.770 candidate item number M = 10. In this setting, we randomly select nine negative samples with the target item. The results are presented in Table 5. We exclude the results of some backbone LLM models (e.g., LLama-3 and QWEN2), as similar observations as Table 1 can be found. Observing the experimental results, we can note that our pro- posed K-RagRec method consistently outperforms all baseline methods on MovieLens and Amazon Book datasets, further highlighting the effective- ness of our framework. A.6 Ablation Study Setting To assess the impact of each module in K- RagRec, we compare the framework with four ab- lated variants: K-RagRec (-Indexing), K-RagRec (-Popularity), K-RagRec ( -Re-ranking), and K- RagRec (-Encoding). (1) K-RagRec ( -Indexing) eliminates the GNNIndexing and stores semantic information of PLM in the knowledge vector database. For the retrieved nodes, we extract their second-order sub-graphs as the retrieved knowl- edge sub-graphs. (2) K-RagRec (-Popularity) does not apply the popularity selective retrieval policy and retrieves all items from the userâ€™s historical interactions. (3) ( -Re-ranking) removes the re- ranking module and inputs the knowledge sub- graphs directly. (4) K-RagRec ( -Encoding) re- moves the GNNEncoding and replaces it with a train- able soft prompt. The