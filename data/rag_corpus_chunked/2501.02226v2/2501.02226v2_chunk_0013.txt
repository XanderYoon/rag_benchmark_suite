..., Iron Man}?" . For consistency, we adopt the same PLM to capture the semantic information of the above prompt as p, and re-rank the knowledge sub-graphs in G to obtain a Top-N knowledge sub-graphs set ˆG: ˆG = argtopNg′∗∈G sim p, zg′∗  . (6) 3.7 Knowledge-augmented Recommendation To facilitate the LLM’s better understanding of the structure of retrieved knowledge sub-graphs and to avoid long contexts, we further integrate another GNN encoder GNNEncoding ϕ2 with parameter ϕ2 to enhance the representation learning of structural information: h ˆg∗ = GNNEncoding ϕ2 ({ ˆg∗ : ˆg∗ ∈ ˆG}). (7) An MLP projector MLPθ with parameter θ is fur- ther introduced to shift to mapping all sub-graphs embedding in ˆG into the LLM embedding space: ˆh ˆG = MLPθ([h ˆg1; ...; h ˆgN ]), (8) where [.; .] represents the concatenation operation. The extracted knowledge sub-graphs embedding ˆh ˆG as the soft prompt is then appended before the input token embedding in LLM. 3.8 Optimization for K-RagRec The training process can be broadly considered as soft prompt tuning, where the retrieved knowledge sub-graphs are a series of soft graph prompts. For- mally, the generation process can be expressed as follows: pδ,θ,ϕ1,ϕ2 (Y | ˆG, xq) = rY k=1 pδ,θ,ϕ1,ϕ2 (yk | y<k, ˆh ˆG, xq). (9) Therefore, instead of fine-tuning the LLM model extensively, we only learn parameters of two GNNs (i.e., GNNIndexing ϕ1 , GNNEncoding ϕ2 ) and projec- tor MLPθ, while parameters δ of LLM backbone are frozen. We update the parameters ϕ1, ϕ2 and θ through the Cross-Entropy Loss L(Y, A), where Y is the ground-truth and A is LLM’s prediction. 4 Experiment In this section, we evaluate the effectiveness of our proposed framework through comprehensive exper- iments. First, we present the experimental settings, including details about the datasets,