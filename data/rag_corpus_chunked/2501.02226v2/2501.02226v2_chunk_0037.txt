performance. ACC and R@k denote Accuracy and Recall@k, respec- tively. GNN Types ACC R@3 R@5 GCN (Kipf and Welling, 2016)0.397 0.704 0.809 GAT (Velickovic et al., 2017)0.420 0.693 0.804 Graph Transformer (Shi et al., 2020)0.429 0.711 0.779 GraphSAGE (Hamilton et al., 2017)0.418 0.699 0.823 80.9%, demonstrating the effectiveness of our ap- proach in addressing hallucinations. A.9 Study of Cold Start Recommendation The cold start problem is an important issue in most recommendation research. To comprehensively evaluate our approach, we particularly design a case study to evaluate the modelâ€™s recommendation performance under the cold-start setting. Specif- ically, we construct a separate cold-start dataset based on the MovieLens dataset that only contains these identified cold-start items as target items. We compare K-RagRec with three KG RAG-enhanced LLM recommendation methods, and the experi- ment results are shown in Table 8. The results demonstrate that our proposed K-RagRec still has satisfactory performance under the cold-start rec- ommendation scenario, highlighting the effective- ness of our framework in all the cases. A.10 Study of Four GNN Encoders To further understand the generality of our pro- posed approach, we conduct a comparative study of four variants applying different GNN encoders. Specifically, we compare GCN (Kipf and Welling, 2016), GAT (Velickovic et al., 2017), Graph Trans- former (Shi et al., 2020), and GraphSAGE (Hamil- ton et al., 2017) as four K-RagRec variants of GNN encoder. Specifically, Graph Convolutional Net- Table 10: Comparison of different GNN layers on the Amazon Book dataset and LLama-2-7b across three metrics. ACC and R@k denote Accuracy and Recall@k, respectively. GNN LayersACC R@3 R@5 3 layers 0.496 0.653 0.736 4 layers 0.506 0.690 0.780 5 layers 0.498 0.656 0.729 work (GCN) (Kipf and Welling, 2016) first intro- duces convolutional operations to graph-structured data. By aggregating features from neighboring nodes, GCN facilitates the learning of