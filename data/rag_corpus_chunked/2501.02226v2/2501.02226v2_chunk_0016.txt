for each user, we select the last item that the user inter- acted with as the target item and the 10 interaction items prior to the target item as the historical in- teractions. Then, we leverage LLM to predict the user’s preferred item from a pool of 20 candidate items (M = 20), which contains one target item with nineteen randomly sampled items. For trained models (including prompt tuning and fine-tuning), we compute Recall@k by extracting the probability assigned to each item and evaluating the model’s ability to rank the target item within the top-k pre- dictions. In addition, we also conduct comparison experiments with 10 candidate items (M = 10) as shown in Appendix A.5. 4.1.4 Parameter Settings We implement the framework on the basis of Py- Torch and conduct the experiments on 2 NVIDIA A6000-48GB GPUs. We adopt the SentenceBert to encode entities, relations, and query attributes. We use the 3 layers Graph Transformer as the GNNIndexing and GNNEncoding for MovieLens-1M and 4 layers for MovieLens-20M and Amazon Book. The layer dimension is set to 1024, and the head number is set to 4. The popularity selec- tive retrieval policy threshold p is set to 50%. For each item that needs to be retrieved, we retrieve the top-3 most similar sub-graphs. The re-ranking knowledge sub-graph number N is set to 5. More experiment details are shown in Appendix A.1. We also present several prompt examples in Ap- pendix A.12. 4.2 Overall Performance Comparison We compare the recommendation performance of K-RagRec with various baselines on three open- Table 1: Performance comparison of different KG RAG-enhanced LLM recommendations. The best performance and the second-best performance are marked in red and blue, respectively. ACC and R@k denote Accuracy and Recall@k, respectively. Models Methods MovieLens-1M MovieLens-20M Amazon Book ACC R@3 R@5 ACC