structure of graphs. For- mally, a typical GNN operation can be formulated as follows: x(l+1) j = x(l) j ⊕ AGG(l+1) n x(l) i | i ∈ N xj o , (10) where x(l+1) j express node j’s feature on the l-th layer, and N (xj) is the set of neighbours of node j. AGG is a aggregation function to aggregates neighbors’ features, and ⊕ combines neighbors’ information with the node itself. A.3 More Related Work The remarkable breakthroughs in LLMs have led to their widespread adoption across various fields, particularly in the recommendations. Given power- ful reasoning and generalization capabilities, many studies have actively attempted to harness the power of the LLM to enhance recommender sys- tems (Geng et al., 2022; Bao et al., 2023; Qu Table 4: Statistics of Hyper Parameters. Item Value batch size 5 epochs 3 grad steps 2 learning rate 1e-5 Indexing layer numbers 4 Indexing hidden dimension1024 Encoding layer numbers 4 Encoding hidden dimension1024 Encoding head numbers 4 lora_r 8 lora_alpha 16 lora_dropout 0.1 int8 True fp16 True candidate item numbers 20 thresholdp 50% top-K 3 top-N 5 et al., 2024; Wei et al., 2024; Zhang et al., 2023). For example, P5 (Geng et al., 2022) proposes an LLM-based recommendation model by unifying pre-training, prompting, and prediction for various recommendation tasks, such as sequential recom- mendation and rating predictions. Furthermore, Tallrec (Bao et al., 2023) fine-tunes LLM (i.e., LLaMA-7B) to align with recommendation data for sequential recommendations. To further capture higher-order collaborative knowledge and enhance the model’s ability to generalize users and items, TokenRec (Qu et al., 2024) proposes a masked vector-quantized tokenizer to tokenize users and items in LLM-based recommendations. Despite their effectiveness, these models often face chal- lenges, such as hallucinations and the lack of up- to-date knowledge. While fine-tuning can partially