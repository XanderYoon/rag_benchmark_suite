sub-graphs as the retrieved knowl- edge sub-graphs. (2) K-RagRec (-Popularity) does not apply the popularity selective retrieval policy and retrieves all items from the user’s historical interactions. (3) ( -Re-ranking) removes the re- ranking module and inputs the knowledge sub- graphs directly. (4) K-RagRec ( -Encoding) re- moves the GNNEncoding and replaces it with a train- able soft prompt. The retrieved knowledge sub- graphs will be added to the prompt as triples (e.g., {Moonraker, film writer film, Christopher Wood (writer)}). A.7 Generalization Study To evaluate the generalization capability of our proposed framework in the zero-shot setting, we trained a version of the model on the MovieLens- Table 6: The generalization results for our K-RagRec model in a zero-shot setting. In this setting, our models are trained on MovieLens-1M dataset and evaluated on MovieLens-20M and Amazon Book datasets. ACC and R@k denote Accuracy and Recall@k, respectively. Models Methods MovieLens-20MAmazon Book ACCR@3R@5ACCR@3R@5 LLama2K-RagRec0.5390.7400.7950.3900.5810.671Lora w/K-RagRec0.5390.7830.8630.4050.5800.796 LLama3K-RagRec0.5970.7970.8390.4280.6280.706Lora w/K-RagRec0.6110.7750.8140.4240.6220.732 QWEN K-RagRec0.5070.7690.8610.4180.6120.687Lora w/K-RagRec0.5450.8140.8970.4410.6230.706 Table 7: Quantitative comparison of hallucination on the MovieLens-1M dataset. ∆ denotes the reduction in hallucinations for K-RagRec compared to Direct Infer- ence. Models Direct InferenceK-RagRec ∆ LLama-2 39.1% 2.7% 93.1% QWEN2 4.7% 0.9% 80.9% 1M dataset and assessed it on the MovieLens-20M and Amazon Book datasets. The experiment re- sults are shown in Table 6. We note that although the K-RagRec performance in the zero-shot set- ting is slightly degraded when compared to the well-trained model in Table 1, it still demonstrates 21.6% improvement over SOTA baselines on the MovieLens-20M dataset. Furthermore, despite the differences between the book recommendation and movie recommendation tasks, the model trained on MovieLens-1M delivers about 8.7% improvement in the zero-shot setting compared to prompt-tuned baselines on the Amazon Book dataset. The experi- mental results demonstrate that K-RagRec exhibits strong generalization capabilities and is adaptable across different domains.