group evaluates the relevance of the retrieved data to the query and is mostly repre- sented with the ranking evaluation metrics: Preci- sion@k, Recall@k, F@1, MRR, MAP (Gao et al., 2024). The second group involves generation eval- uation metrics, such as BLEU, METEOR, ROUGE, PPL (Radeva et al., 2024) and Accuracy, Rejection Rate, Error Detection Rate, Error Correction Rate (Chen et al., 2024). 4.5 Limitation Despite multiple advantages and ubiquitous appli- cation, Zhao et al. (2024) outline limitations and possible directions of RAG. We describe the last two dimensions in more detail, also considering failures from Barnett et al. (2024). D15 RAG Failure Points: RAG limitations can be divided into two groups: internal (related to the system component efficiency) and integration (related to the problems of RAG components inter- action). Here, we discuss each type separately. The most evident and the most frequent failure point for RAG is the retrieval step. Noises in re- trieval results or missing relevant content may dras- tically decrease the final performance, as the in- formation provided to the generator may contain irrelevant objects or misleading information. Bar- nett et al. (2024) also state that the reason for that might be the missing content, e.g., “when asking a question that cannot be answered from the avail- able documents”. The next failure point is called “not in context” Barnett et al. (2024). In this case, the extracted documents were not correctly con- solidated during the post-retrieval process. The last three failure points relate to the generated out- put: the incorrect format of the output, incorrect specificity (“not specific enough or is too specific to address the user’s need”), and incomplete out- put that misses essential information even though being extracted by the retriever. When combining RAG with another system, the most common limitation is extra overhead: