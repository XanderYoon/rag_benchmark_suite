across these different modalities, even though slight modifications to the augmenta- tion methods are sometimes required. D12 Granularity: This dimension is for differ- ent granularities of retrieved data based on the in- formation by Gao et al. (2024). The modality can be natural language (or text), yet still, the retrieved granularity might vary from fine to coarse, e.g., document, chunk, sentence, proposition, etc. (Gao et al., 2024). Similarly, there exist several granu- larities in structured data, e.g., sub-graph, triplet, entity, etc. (Gao et al., 2024). 4.4 Evaluation D13 Dataset: Regarding the datasets used for RAG, most RAG surveys consistently list the datasets used regardless of the application task, the RAG step to be evaluated on as well as the dataset availability. Thus, we focused on the matter of availability and considered two characteristics, i.e., publicly available, and proprietary datasets. Some examples of publicly available datasets are e.g. FEVER (Thorne et al., 2018), SQuAD (Ra- jpurkar et al., 2016) etc., and the dataset, e.g. by Bondarenko et al. (2020), is an example for propri- etary datasets. D14 Evaluation Metrics: When reviewing pa- pers discussing separate models and architectures, we can see that the authors mostly use task-specific metrics (Thakur and Vashisth, 2024) or the genera- tion output quality only (Chen et al., 2024). How- ever, Gao et al. (2024) split evaluation metrics into 6 two groups: retrieval evaluation and generation evaluation metrics, which are the base parts of RAG. The first group evaluates the relevance of the retrieved data to the query and is mostly repre- sented with the ranking evaluation metrics: Preci- sion@k, Recall@k, F@1, MRR, MAP (Gao et al., 2024). The second group involves generation eval- uation metrics, such as BLEU, METEOR, ROUGE, PPL (Radeva et al., 2024) and Accuracy, Rejection Rate, Error Detection Rate, Error Correction Rate (Chen