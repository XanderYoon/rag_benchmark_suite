returned seventy-four papers. Additionally, we searched these two queries in ACL Anthology2, and the first one returned two results, while five results are returned with the second one. 1All searches are done on a date, 11.09.2024 2https://aclanthology.org/ Then, inspired by Rafailov et al. (2023), we cre- ated a prompt to ChatGPT to cluster the extracted papers into domains. We formulated the prompt as follows: You are a scientific assistant writing a survey. Here below is a list of paper names. Your task is to cluster those pa- pers into domains. Name those domains (it might be something like NLP, or medicine). After the prompt, we pasted names from the first query separated with the line separator. Based on the titles provided, the ChatGPT model identified the following 10 classes: (1) Artificial Intelligence and Natural Language Processing (AI & NLP); (2) Cybersecurity; (3) Medicine and Healthcare; (4) Business and Economics; (5) Education and Programming; (6) Social Sciences and Ethics; (7) Disaster and Risk Management; (8) Physics and Engineering; (9) Data Science and Knowledge Dis- covery; (10) Adversarial AI and Machine Learning. When providing ChatGPT papers from the “rag application” query, the output is: (1) Artificial In- telligence and Language Models; (2) Legal and Jus- tice; (3) Medicine and Healthcare; (4) Education and Pedagogy; (5) Engineering and Construction; (6) Data Science and Information Retrieval. It is also important to emphasize that in addi- tion to the class names, the model returned paper examples for each class, therefore, we were able to primarily check the correctness of the identi- fied classes. Furthermore, during iterations, we expected to use a manual paper check to prove the ChatGPT clustering efficiency. 3.4 Iterations We performed four iterations to build our initial RAG application taxonomy. Overall, we analyzed 28 papers, including 4 surveys on RAG, that al-