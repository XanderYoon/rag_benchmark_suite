47.35 31.82 54.11 28.70 49.57 CIReVL(4o) + SoFT 30.62 50.69 28.51 49.63 34.52 57.78 31.22 52.70 ViT-L/14 CIReVL 29.49 47.40 24.79 44.76 31.36 53.65 28.55 48.57 CIReVL + SoFT 32.34 51.62 27.51 47.79 35.19 58.18 31.68 52.53 CIReVL(4o) 29.54 46.81 25.53 45.36 31.26 54.51 28.78 48.89 CIReVL(4o) + SoFT 32.43 50.88 28.06 48.54 35.08 58.18 31.86 52.53 Table S8: Quantitative results on FashionIQ under a unified LLM setting (both CIReVL and SoFT using GPT-4o). A.3 Unified Large Language Model Consistency Check To ensure that the observed improvement does not arise from differences in the underlying language model, we uni- fied both CIReVL and SoFT to use GPT-4o as their tex- tual reasoning component. Table S7 and Table S8 show that SoFT consistently improves retrieval performance across all benchmarks, confirming that the gain stems from its filter- ing mechanism rather than from the capability gap between Large Language Models(LLMs). B Multi-Target Triplet Dataset Pipeline B.1 Large Language Model Prompt Templates Prompt Templates for Query Generation.To compre- hensively capture plausible multi-target candidates during candidate retrieval, we design prompt templates that produce two complementary queries from each input pair(reference image, modification text). Rather than serving as simple reformulations, these templates represent distinct semantic perspectives of user intent, helping to prevent the omission of valid yet diverse candidate targets. •Text-focused query: Captures only the intended modifi- cation described in the text. •Compositional query: Integrates the modification with compatible attributes inferred from the reference image. Distinct templates are used for CIRR and FashionIQ to account for domain-specific characteristics and annotation styles. Prompt Templates for Confidence Scoring of Candidate Groups.For each (reference image, modification text) pair, we form three candidate groups based on distinct re- trieval criteria:Textual to Modification,Compositional, and Visual Similarity. Each group is then evaluated by an LLM using a unified prompt structure that