cues. LDRE (Yang et al. 2024) addresses this gap by proposing an ensemble-based approach. However, it still does not explicitly disentangle and control the prescriptive (must-have) and proscriptive (must-avoid) aspects of user intent, as it operates over varia- tions of fused representations. To address these limitations, we proposeSoft Filtering with Textual constraints (SoFT), filtering mechanism tai- lored for the CIR. Unlike traditional retrieval systems that apply hard filters over structured metadata (Niu, Fan, and Zhang 2019; Yee et al. 2003; Zhao et al. 2017), CIR operates over raw image-text pairs without explicit annotations, ren- dering conventional filtering inapplicable. Inspired by clas- sical IR systems that enforce conditions to reduce irrelevant results, SoFT adapts this concept to the unstructured, multi- modal nature of CIR, as illustrated in Figure 1. Instead of hard exclusion, SoFT re-ranks candidate im- ages by interpreting user intent throughdual-faceted tex- tual constraints. We leverage multimodal Large Language Models (LLMs) to automatically generate two complemen- tary constraints from the reference-modification pair: apre- scriptiveconstraint emphasizing attributes that the target should include, and aproscriptiveconstraint describing attributes that should be avoided. These guide similarity- based reward and penalty scores for each candidate image, enabling precise, constraint-aware re-ranking—without any additional training or annotations. Moreover, current ZS-CIR evaluation benchmarks (Wu et al. 2021; Liu et al. 2021b) overlook the ambiguities in modification texts, assuming a single correct target per query despite the existence of multiple valid answers (Wu et al. 2021; Zhang et al. 2025). To address this, we introduce amulti-target triplet construction pipeline, which first identifies semantically valid targets per query, then rewrite the modification text via LLMs to generate single-target triplets. This process could also be used to enrich existing multi-target datasets with diversified constraints. Our contributions are twofold: • We propose SoFT, a training-free re-ranking module that leverages prescriptive and