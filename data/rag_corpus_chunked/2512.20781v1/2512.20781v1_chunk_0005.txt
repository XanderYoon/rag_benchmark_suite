be broadly categorized into fusion-based and inversion-based approaches. Fusion-based models (Baldrati et al. 2022a; Zhang et al. 2025) encode the reference image and modification text independently before merging their representations. Inversion-based models (Saito et al. 2023; Baldrati et al. 2023a; Gu et al. 2024; Tang et al. 2024) rein- terpret the image as a pseudo-text token and perform joint encoding with the modification text. While both approaches avoid triplet training, they still require training dedicated modules for fusion or inversion. Recently, LLMs have been adopted to enhance compo- sitional reasoning in a fully training-free manner. For in- stance, CIReVL (Karthik et al. 2024) reformulates CIR as a two-step text generation process: first generating a cap- tion for the reference image, then producing a modified query conditioned on the reference caption and the modifi- cation text. This formulation avoids rigid template matching (e.g., “a photo of X that Y”) and improves generalization to novel compositions. OSrCIR (Tang et al. 2025) simplifies the pipeline further by simultaneously providing both the reference image and modification text to the LLM, which directly generates a query tailored to the target image. This direct generation reduces the risk of information loss caused by intermediate representations and allows for more flexi- ble reasoning. LDRE (Yang et al. 2024) leverages LLMs to produce multiple pseudo queries, aggregating their re- trieval scores for more robust performance. However, these methods still fall short in explicitly modeling the dual na- ture of user intent—what must be included and what must be avoided. CIR Benchmark Datasets.CIR has been primarily eval- uated on datasets such as CIRCO (Baldrati et al. 2023b), CIRR (Liu et al. 2021b) and FashionIQ (Wu et al. 2021). CIRCO introduces diverse and inherently ambiguous re- trieval scenarios, CIRR emphasizes natural language com- position in generic scenes, and FashionIQ focuses