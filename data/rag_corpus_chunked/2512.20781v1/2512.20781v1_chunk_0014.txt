the respective evaluation servers. Baselines.We compare our method against two publicly available CIR baselines: SEARLE (Baldrati et al. 2023a) and CIReVL (Karthik et al. 2024). Both provide full imple- mentations and serve as reliable references for evaluating plug-and-play compatibility. Reference results from recent Shirt Dress TopTee Average Recall@k @10 @50 @10 @50 @10 @50 @10 @50 Method ViT-B/32 SEARLE 24.44 41.61 18.54 39.51 25.70 46.46 22.89 42.53 SEARLE∗ 24.29 42.64 19.48 40.46 25.54 47.78 23.10 43.63 CIReVL 28.36 47.84 25.29 46.36 31.21 53.85 28.29 49.35 CIReVL∗ 31.26 50.98 27.52 49.13 34.37 58.44 31.05 52.85 LDRE 27.38 46.27 19.97 41.84 27.07 48.78 24.81 45.63 OSrCIR 31.16 51.13 29.35 50.37 36.51 58.71 32.34 53.40 ViT-L/14 SEARLE 26.89 45.58 20.48 43.13 29.32 49.97 25.56 46.23 SEARLE∗ 26.30 43.67 20.77 42.94 27.94 49.97 25.00 45.53 CIReVL 29.49 47.40 24.79 44.76 31.36 53.65 28.55 48.57 CIReVL∗ 32.34 51.62 27.51 47.79 35.19 58.18 31.68 52.53 LDRE 31.04 51.22 22.93 46.76 31.57 53.64 28.51 50.54 OSrCIR 33.17 52.03 29.70 51.81 36.92 59.27 33.26 54.37 Table 2: Quantitative results on FashionIQ. ∗ denotes meth- ods with our SoFT module. models such as LDRE (Yang et al. 2024) and OSrCIR (Tang et al. 2025) are also included in tables for context. Evaluation Metrics.We report Recall (R@K) for single- target benchmarks (CIRR, FashionIQ), measuring the pres- ence of the ground-truth image within the top-K results. For multi-target datasets (CIRCO and our Multi-Target vari- ants), mean Average Precision (mAP@k) is used, which re- wards ranking lists that place valid targets higher and more consistently. Implementation Details.For all components of our framework, including both the proposed SoFT module and the dataset construction pipeline, we consistently use GPT- 4o (Hurst et al. 2024) via OpenAI’s API with temperature set to 0.0. All similarity computations are based on pretrained CLIP models (Radford et al.