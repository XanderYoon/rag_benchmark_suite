a convex com- bination of the base and filtered scores, remains continuous, interpretable, and tunable. Importantly, SoFT ismodel-agnostic and training-free, operating entirely at inference time via score-level modula- tion. Itsplug-and-playnature allows seamless integration text-focused query composed query scored group1 scored group3 scored group2group2 group1 group3 multimodal LLMs Multi-Target Images Stage 1. Multi-Target Triplet Construction Stage 2. Single-Target Rewriting via Contrastive Prompting Target image : text-focused feature composed feature Textual encoder Target feature Visual encoder Image Database Image features Visual encoder sim (Â·) multimodal LLMs threshold filter Reference image : Modification text : "Different dog and has a toy" multimodal LLMs Modification text : "Different dog and has a toy" Reference image : Multi-Target Images Target image :Reference image : Modification text for single target : "Different dog with a gray coat and a crocheted toy" single target constractive distractors top k top k top k Figure 3: Overview of the Multi-target Triplet Dataset pipeline. Stage 1 selects diverse target images for each reference- modification pair to capture open-ended user intent. Stage 2 rewrites the modification text to focus on a specific target with contrastive distractors, enabling precise single-target evaluation. into any CIR systems, making it especially suitable for zero- shot settings where architectural changes or supervision are impractical. 3.2 Multi-Target Triplet Dataset Pipeline Most CIR benchmarks assume a single ground-truth image per query, which limits evaluation under ambiguous or open- ended user intents. To address this, we propose a two-stage pipeline that expands CIR datasets with multi-target triplets and refines them into discriminative single-target triplets. Stage 1 identifies multiple valid targets using both visual and textual similarity signals. Stage 2 rewrites the modifica- tion text to distinguish one target from semantically similar distractors. This design supports both inclusive and precise evaluation. An overview is illustrated in Figure 3. Stage 1: