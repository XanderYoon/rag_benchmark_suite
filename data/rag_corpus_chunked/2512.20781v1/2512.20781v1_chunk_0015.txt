which re- wards ranking lists that place valid targets higher and more consistently. Implementation Details.For all components of our framework, including both the proposed SoFT module and the dataset construction pipeline, we consistently use GPT- 4o (Hurst et al. 2024) via OpenAI’s API with temperature set to 0.0. All similarity computations are based on pretrained CLIP models (Radford et al. 2021), using the inner product of encoder outputs. In SoFT, the final retrieval score is com- puted as in Eq. (2), withλ= 1.0for CIReVL andλ= 0.2 for SEARLE. Main experiments compare ViT-B/32 and ViT- L/14 (Dosovitskiy et al. 2020); the latter is used by default elsewhere. Computation and Cost Analysis.All experiments were conducted on a single NVIDIA RTX 4090 GPU. CLIP- based feature extraction and similarity computations were performed locally on GPU for efficiency, while all SoFT and dataset pipeline operations involving language models were executed via the GPT-4o API. The LLM cost for SoFT was $2.69 for CIRCO, $15.36 for CIRR, and $17.68 for Fash- ionIQ. For dataset construction, the total cost was $196.40 (Stage 1) and $12.31 (Stage 2) on CIRR, and $116.32 (Stage 1) and $16.38 (Stage 2) on FashionIQ. 4.2 Effectiveness of Soft Filtering Module As shown in Table 1, SoFT consistently improves retrieval performance across all metrics and backbones on CIRCO Shirt Dress TopTee Average mAP@k @5 @25 @5 @25 @5 @25 @5 @25 Method ViT-L/14 CIReVL 24.14 23.36 20.71 19.62 23.85 23.17 22.90 22.05 CIReVL∗ 28.17 26.67 24.39 22.31 28.86 27.05 27.14 27.04 SEARLE 41.64 36.54 36.26 31.46 44.44 38.53 40.78 35.51 SEARLE∗ 42.45 37.21 37.50 32.44 45.37 39.14 41.77 36.26 Table 3: Retrieval performance(mAP@5/mAP@25) on the Multi-Target FashionIQ validation set. ∗ indicates methods with our SoFT module applied. SoFT consistently improves mAP@5 and mAP@25 across all categories and base mod- els. and CIRR.