of user intent—what must be included and what must be avoided. CIR Benchmark Datasets.CIR has been primarily eval- uated on datasets such as CIRCO (Baldrati et al. 2023b), CIRR (Liu et al. 2021b) and FashionIQ (Wu et al. 2021). CIRCO introduces diverse and inherently ambiguous re- trieval scenarios, CIRR emphasizes natural language com- position in generic scenes, and FashionIQ focuses on fashion-related attribute modification. However, a common limitation in CIRR and FashionIQ is the assumption of a sin- gle correct target image, which fails to reflect the inherent ambiguity and multi-target validity that can arise from mod- ification texts. These texts often lack specificity, leading to query ambiguity that impairs evaluation reliability. CoLLM (Zhang et al. 2025) mitigates the ambiguity of modification texts by rewriting them via LLMs into more target-specific and discriminative instructions, reducing the likelihood of multiple plausible targets per query. While CoLLM improves retrieval evaluation by clarifying ambigu- ous modification texts, it still operates under the assumption Reference image : Modification text : "Is an office setting instead of a home" Image Database SoFT Module Retriever Image features Base similarityquery feature Final score Convex Combination top k . . . SoFT score multimodal LLMs reward penalty Textual encoer sim (·) Visual encoder prescriptive feature proscriptive feature "An office setting with typical office furniture and decor." "A home setting with a dining table, kitchen island, and home decor." Figure 2: Overview of SoFT, a plug-and-play soft filtering module for Zero-shot CIR. Given a reference image and a modi- fication text, multimodal LLMs extract prescriptive and proscriptive constraints. These are used to softly reward or penalize candidate images using CLIP similarity. of a single correct target. In contrast, our proposed dataset construction pipeline explicitly acknowledges the existence of multiple semantically valid targets, and leverages LLMs not only to identify