Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints Youjin Jung1, Seongwoo Cho1, Hyun-seok Min2, Sungchul Choi1 1Major in Industrial Data Science & Engineering, Department of Industrial and Data Engineering, Pukyong National University 2Tomocube Inc. wjd1dbwls@pukyong.ac.kr, jsw6872@pukyong.ac.kr, hsmin@tomocube.com, sc82@pknu.ac.kr Abstract Composed Image Retrieval (CIR) aims to find a target im- age that aligns with user intent, expressed through a refer- ence image and a modification text. While Zero-shot CIR (ZS-CIR) methods sidestep the need for labeled training data by leveraging pretrained vision-language models, they of- ten rely on a single fused query that merges all descriptive cues of what the user wants—tending to dilute key infor- mation and failing to account for what they wish to avoid. Moreover, current CIR benchmarks assume a single correct target per query, overlooking the ambiguity in modification texts. To address these challenges, we propose Soft Filter- ing with Textual constraints (SoFT), a training-free, plug- and-play filtering module for ZS-CIR. SoFT leverages mul- timodal large language models (LLMs) to extract two com- plementary constraints from the reference-modification pair: prescriptive (must-have) and proscriptive (must-avoid) con- straints. These serve as semantic filters that reward or penal- ize candidate images to re-rank results, without modifying the base retrieval model or adding supervision. In addition, we construct a two-stage dataset pipeline that refines CIR benchmarks. We first identify multiple plausible targets per query to construct multi-target triplets, capturing the open- ended nature of user intent. Then guide multimodal LLMs to rewrite the modification text to focus on one target, while referencing contrastive distractors to ensure precision. This enables more comprehensive and reliable evaluation under varying ambiguity levels. Applied on top of CIReVL—a ZS- CIR retriever—SoFT raisesR@5to 65.25 on CIRR (+12.94), mAP@50to 27.93 on CIRCO (+6.13), andR@50to 58.44 on FashionIQ (+4.59), demonstrating broad effectiveness. Code— https://github.com/jjungyujin/SoFT Datasets—