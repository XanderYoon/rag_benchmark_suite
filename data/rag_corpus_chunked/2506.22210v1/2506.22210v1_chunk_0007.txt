distributed across 12 GPUs, with pointwise reranking and response generation using 25% of total GPU resources and pairwise reranking the remaining 75%. During the challenge day, we used 8 Tesla V100 GPUs and 4 NVIDIA A100 GPUs. 4 Experiments In our experiments, we investigate our system’s robustness with respect to the quality of the retrieved information. We also evaluate its ability to synthesize content from retrieved passages and reduce redundancy. The main goal of these experiments is to find a balance between efficiency—ensuring that responses can be generated for all test queries within a limited time window on the challenge day—and the quality of the generated responses. 4.1 Datasets We generated a test set of 100 instances using the DataMorgana API, a synthetic benchmark generator platform used in the Liv- eRAG challenge [7]. DataMorgana enables RAG developers to cre- ate synthetic questions and answers from a given corpus based on configurable instructions. Half of the questions in our test set have 10Prompts used for context curation can be found in Appendix B.2. 11Prompts used for response generation can be found in Appendix B.3. answers grounded in a single document, while the other half are based on two documents. We experimented with several question categorizations proposed in the original paper, including factu- ality, premise, phrasing, and linguistic variation (see Table 3 in Appendix A). Additionally, we incorporated the user expertise cate- gorization and introduced two new categories for multi-document questions: comparisons between two entities and questions cov- ering two aspects of the same topic. The documents provided by DataMorgana for each question are treated as ground-truth pas- sages, and the generated answers serve as references to evaluate our system’s responses. We additionally employed the TREC RAG’24 dataset [29], de- rived from the MS MARCO v2.1 collection and containing 301 information-seeking queries