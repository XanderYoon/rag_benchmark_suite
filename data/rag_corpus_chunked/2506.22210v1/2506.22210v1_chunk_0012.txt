rely on the original query to maintain relevance. To meet the strict time window on challenge day, we had to rigorously optimize our system for efficiency. This involved exten- sive use of multiprocessing, batching, and distributing processes across multiple GPUs. The most resource-intensive component was the pairwise reranking stage, and the heavy reliance on the Falcon model across modules strained API rate limits. These constraints forced us to reduce the number of documents processed at each stage, carefully balancing efficiency against the quality of generated responses. Finally, evaluating the responses with AutoNuggetizer surfaced key limitations of the framework. Its effectiveness depends on hav- ing a rich set of ground-truth nuggets derived from a broad set of rel- evant passages. In practice, especially for open-ended queries, this was often not the case, leading to unfairly low scores for responses that were, in fact, well grounded. This experience underlines the need for more robust response evaluation strategies, particularly when testing with limited access to ground-truth sources. 5 Conclusions This paper has presented our participation in the LiveRAG Chal- lenge at SIGIR’25, proposing a modular system for retrieval-augmen- ted, nugget-based response generation. Our approach integrates query rewriting, sparse and dense retrieval, and reranking within the Grounded Information Nugget-Based Generation of Responses (GINGER) framework. Evaluation on the TREC RAG’24 dataset and QA test samples from DataMorgana using the AutoNuggetizer framework demonstrates that our system effectively balances time efficiency and response quality. References [1] Griffin Adams, Alex Fabbri, Faisal Ladhak, Eric Lehman, and Noémie Elhadad. 2023. From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. Proceedings of the 4th New Frontiers in Summarization Workshop (2023), 68–74. [2] Valeriia Bolotova-Baranova, Vladislav Blinov, Sofya Filippova, Falk Scholer, and Mark Sanderson. 2023. WikiHowQA: A Comprehensive Benchmark for Multi- Document Non-Factoid Question Answering. In Proceedings of the