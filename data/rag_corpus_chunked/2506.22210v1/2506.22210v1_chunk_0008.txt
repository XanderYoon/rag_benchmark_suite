between two entities and questions cov- ering two aspects of the same topic. The documents provided by DataMorgana for each question are treated as ground-truth pas- sages, and the generated answers serve as references to evaluate our systemâ€™s responses. We additionally employed the TREC RAGâ€™24 dataset [29], de- rived from the MS MARCO v2.1 collection and containing 301 information-seeking queries with graded relevance judgments. Un- like DataMorgana, which offers at most two judged passages per query, TREC RAG provides relevance labels for many candidate documents, giving a more reliable signal for retrieval evaluation. We used these judgments to benchmark the query rewriting com- ponent. 4.2 Evaluation We evaluate the effectiveness of query rewriting primarily using the TREC RAGâ€™24 dataset. The main metric isRecall@500, computed using the trec_eval tool.12 This cutoff corresponds to the number of top-ranked documents passed onto the pointwise reranker. We use the original query without any rewriting as the baseline. For response generation, we use the AutoNuggetizer framework proposed for RAG evaluation and validated at TREC RAGâ€™24 [29]. AutoNuggetizer comprises two steps: nugget creation and nugget assignment. In nugget creation, nuggets are formulated based on relevant documents and classified as either â€œvitalâ€ or â€œokayâ€ [37]. The second step, nugget assignment, involves assessing whether a system response contains specific nuggets from the answer key. The score ğ‘‰ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¡ for the systemâ€™s response is defined as: ğ‘‰ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¡ = Ã ğ‘– ğ‘ ğ‘  ğ‘£ ğ‘– |ğ‘›ğ‘£ | , where ğ‘›ğ‘£ represents the subset of the vital nuggets, and ğ‘ ğ‘  ğ‘£ ğ‘– is 1 if the response supports the i-th nugget and is 0 otherwise. The score of a system is the mean of the scores across all queries. 4.3 Results Results in Table 1 show that using a single rewrite alone underper- forms even the original query, suggesting that naive rewriting can