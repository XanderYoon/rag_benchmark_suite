Original Query + Multi Rewrite (3) 0.397 Original Query + Multi Rewrite (5) 0.400 Original Query + Multi Rewrite (10) 0.398 Table 2 presents the evaluation of responses generated using different GINGER configurations, assessed with the AutoNuggetizer framework. We varied two key parameters: the number of docu- ments used for pairwise reranking (ğ‘˜) and the number of documents used for response generation (ğ‘š). These parameters directly impact both the quality of the generated responses and the systemâ€™s effi- ciency. The reranking step with DuoT5 scales exponentially with ğ‘˜, while the number of Falcon API callsâ€”dependent on ğ‘šâ€”is the main bottleneck in information nugget detection. Given the two-hour time limit for processing 500 queries during the challenge (with three parallel processes), we aimed for a setup capable of handling at least 100 queries per hour. Although the setup with ğ‘˜ = 50 and ğ‘š = 20 produced the best responses, it exceeded our time constraints. Configurations with ğ‘˜ = 40, ğ‘š = 10 and ğ‘˜ = 20, ğ‘š = 10 yielded similar scores with much more efficient runtimes. Despite ğ‘˜ = 20 scoring slightly higher, we selectedğ‘˜ = 40 for our final submission to increase topic coverage and response diversity. This choice is further supported by the limitations of AutoNugge- tizer, which evaluates responses using nuggets extracted from only two documents. As a result, it may overlook relevant content cap- tured by a broader reranking scope. In our manual analysis, we observed low scores for responses that were clearly grounded in relevant retrieved passages but where the available ground-truth nuggets were sparse. Conversely, high scores occurred mainly when our responses aligned exactly with the nuggets identified by Au- toNuggetizer. This suggests that the frameworkâ€™s effectiveness is constrained by its limited access to reference passages, which in turn restricts the evaluation of