subset of the vital nuggets, and ùë†ùë† ùë£ ùëñ is 1 if the response supports the i-th nugget and is 0 otherwise. The score of a system is the mean of the scores across all queries. 4.3 Results Results in Table 1 show that using a single rewrite alone underper- forms even the original query, suggesting that naive rewriting can hurt retrieval effectiveness. While combining the original query with multiple rewrites improves recall, the gains saturate quickly. Adding more than three rewrites yields only marginal improve- ments, indicating diminishing returns beyond a small number of diverse reformulations. Notably, the recall achieved by using mul- tiple rewrites alone is consistently lower than the recall obtained when those rewrites are concatenated with the original query, un- derscoring the importance of preserving the original formulation.13 12https://github.com/usnistgov/trec_eval 13These experiments use TREC RAG data with a different retrieval collection, so the comparison to our pipeline is not direct. However, since we evaluate only the query rewriting component with retrieval frozen, the findings are expected to generalize to similar retrieval setups. Weronika ≈Åajewska, Ivica Kostric, Gabriel Iturra-Bocaz, Mariam Arustashvili, and Krisztian Balog Table 1: Recall@500 for different query rewriting strategies on the TREC RAG‚Äô24 dataset. The best-performing config- uration is shown in bold. Teal background indicates the configuration used in the final submission. Rewriting Strategy R@500 Original Query 0.320 Single Rewrite 0.217 Multi Rewrite (3) 0.325 Multi Rewrite (10) 0.357 Original Query + Single Rewrite 0.343 Original Query + Multi Rewrite (3) 0.397 Original Query + Multi Rewrite (5) 0.400 Original Query + Multi Rewrite (10) 0.398 Table 2 presents the evaluation of responses generated using different GINGER configurations, assessed with the AutoNuggetizer framework. We varied two key parameters: the number of docu- ments used for pairwise reranking (ùëò) and the number of documents used for