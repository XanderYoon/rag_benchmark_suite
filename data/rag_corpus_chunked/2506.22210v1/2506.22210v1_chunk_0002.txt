summaries into a final, cohesive response. This process ensures comprehensive yet concise answers, maintains strong source attribution, and, as demonstrated in the TREC RAG’24 augmented generation task, significantly out- performs strong baselines. The core strength of GINGER lies in the granular, nugget-based processing of highly relevant information. When developing our pipeline for the LiveRAG Challenge,1, we conducted experiments on the TREC RAG’24 dataset as well as a small test dataset generated with DataMorgana [ 7]. Our results show that naive answer-based or single sub-question query rewrit- ing can harm retrieval effectiveness, while combining the original query with a few diverse rewrites improves recall. Furthermore, op- timizing reranking and generation parameters reveals that response quality improves only up to a point, beyond which sacrificing time efficiency yields limited gains. 2 Related Work Unlike traditional search engines that return a ranked list of docu- ments, RAG systems provide a single, comprehensive response by synthesizing varied perspectives from multiple sources, blending the language fluency and world knowledge of generative mod- els with retrieved evidence [11, 25]. In retrieve-then-generate sys- tems, generative processes are conditioned on retrieved material by 1https://liverag.tii.ae/ Weronika Łajewska, Ivica Kostric, Gabriel Iturra-Bocaz, Mariam Arustashvili, and Krisztian Balog Rankingfacet clusters Sub-query Passage 1 Passage 2 Passage 3 Retrieval Passage 3 Passage 1 Passage 2 Detectinginformationnuggets Summarizing facetclusters Improvingresponsefluency Queryrewriting Reranking Facets threshold Clusteringinformationnuggets System’sresponse Response length limit Smooth, fluentresponse Falcon BERTopic DuoT5 Falcon Falcon n k top m Falcon BM25+Sem.Sim. Mono+DuoT5 Document Retrieval Context Curation Response Generation Sub-query Sub-query Query Answer Figure 1: High-level overview of our retrieval-augmented nugget-based response generation pipeline ( GINGER). adding evidence to the prompt [15, 31, 33] or attending to sources during inference. Systems submitted to the Retrieval-Augmented Generation track at the Text REtrieval Conference (TREC RAG’24) [29] have adopted modular architectures that improve the retrieval