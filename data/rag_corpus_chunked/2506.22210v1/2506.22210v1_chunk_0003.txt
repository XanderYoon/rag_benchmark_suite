Document Retrieval Context Curation Response Generation Sub-query Sub-query Query Answer Figure 1: High-level overview of our retrieval-augmented nugget-based response generation pipeline ( GINGER). adding evidence to the prompt [15, 31, 33] or attending to sources during inference. Systems submitted to the Retrieval-Augmented Generation track at the Text REtrieval Conference (TREC RAG’24) [29] have adopted modular architectures that improve the retrieval component by combining sparse and dense retrieval models, followed by rerank- ing with models such as MonoT5 and DuoT5 [27], RankZephyr [28], or other LLM-based graded relevance scoring. A notable enhance- ment involves query decomposition using an LLM to generate sub-questions, each addressing different facets of the information need. While LLM-based rewriting is well-established [ 5, 39], the generation of multiple diverse reformulations per query is a more recent development that shows strong potential for boosting recall and robustness by expanding the query’s semantic coverage [19, 30]. Retrieved and reranked results from these variants are typically merged using reciprocal rank fusion (RRF) [38]. For the generation stage, the most simplistic approach is to use proprietary models to generate responses in a single step based on the provided documents. However, ad hoc retrieval often returns documents with only partial relevance [26], and placing relevant content in the middle of a long prompt can degrade generation quality [24]. While generative models often produce fluent and seemingly helpful responses, they frequently suffer from hallucina- tions and factual errors [16, 20, 23, 36]. These limitations motivate more advanced context curation strategies, including unimportant token removal [17], content aggregation [43], and training extrac- tors and condensers [40, 41]. Approaches at TREC RAG’24 include extracting, combining, and condensing the relevant information [8], enhanced by verifying key facts across documents, rule-based re- dundancy removal, and enhancing coherence [6]. 3 Retrieval-Augmented Nugget-Based Response Generation Our approach, GINGER (which