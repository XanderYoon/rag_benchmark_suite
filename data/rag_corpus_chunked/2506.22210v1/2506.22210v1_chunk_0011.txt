observed low scores for responses that were clearly grounded in relevant retrieved passages but where the available ground-truth nuggets were sparse. Conversely, high scores occurred mainly when our responses aligned exactly with the nuggets identified by Au- toNuggetizer. This suggests that the frameworkâ€™s effectiveness is constrained by its limited access to reference passages, which in turn restricts the evaluation of information quality. 4.4 Lessons Learned Participating in the LiveRAG challenge underscored the need to balance time efficiency with handling diverse query types. The time limit and the diversity of questions generated with DataMorgana posed unexpected challenges, requiring careful pipeline tuning and manual analysis. Our initial query rewriting strategy, designed to sharpen the focus of the question using potential answer clues, worked well for factoid questions but underperformed for open-ended queries, where broader context is needed. This led us to revise our approach: Table 2: Evaluation with AutoNuggetizer of responses gen- erated with GINGER using different setups. All variants use the top ğ‘› = 500 retrieved documents for pointwise reranking. Teal background indicates the configuration used in the final submission. Pairwise Response V_strict Time reranking generation estimate ğ‘˜ = 50 ğ‘š = 20 0.406 70 min ğ‘˜ = 40 ğ‘š = 10 0.397 41 min ğ‘˜ = 20 ğ‘š = 10 0.404 42 min ğ‘˜ = 20 ğ‘š = 5 0.350 26 min using rewritten queries only for retrieval to ensure a diverse doc- ument pool, while letting reranking and generation rely on the original query to maintain relevance. To meet the strict time window on challenge day, we had to rigorously optimize our system for efficiency. This involved exten- sive use of multiprocessing, batching, and distributing processes across multiple GPUs. The most resource-intensive component was the pairwise reranking stage, and the heavy reliance on the Falcon model across modules strained