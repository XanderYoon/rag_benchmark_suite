we introduce how to model usersâ€™ historical inter- actions to get their representations. Specifically, we adopt SASRec [21] as the model backbone, whose encoding module is based on the Transformer encoder [44]. To leverage the transformerâ€™s strong encoding ability, we first convert items to embeddings. Then, we apply the Transformer encoder to generate the user representation. 2.2.1 Embedding Layer. Formally, an embedding tableV âˆˆ R| V | Ã—ğ‘‘ is created upon the whole item set V to project each item (one- hot representation) into a low-dimensional dense vector for better sequential modeling, where ğ‘‘ is the embedding dimension. Addi- tionally, to perceive the time order of the sequence, a learnable position encoding matrix P âˆˆ Rğ‘‡ Ã—ğ‘‘ is constructed, where ğ‘‡ is the maximum sequence length. Owing to the constraint of the maxi- mum sequence length ğ‘‡ , when inferring user ğ‘¢ representation at time step ğ‘¡ + 1, we truncate the user sequenceğ‘ ğ‘¢,ğ‘¡ to the lastğ‘‡ items RaSeRec: Retrieval-Augmented Sequential Recommendation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Figure 2: The overall system framework of the proposed RaSeRec. The upper layer illustrates the workflow of collaborative-based pre-training while the bottom layer shows the workflow of retrieval-augmented fine-tuning. if ğ‘¡ > ğ‘‡ , otherwise left unchanged: ğ‘ ğ‘¢,ğ‘¡ = [ğ‘£ğ‘¡ âˆ’ğ‘‡ +1, ğ‘£ğ‘¡ âˆ’ğ‘‡ +2, ..., ğ‘£ğ‘¡ ], (2) where we omit the superscript (ğ‘¢) for brevity. Subsequently, the item embedding and position encoding are added together to make up the input vector: h0 ğ‘– = vğ‘– + pğ‘–, (3) where h0 ğ‘– is the input vector at position ğ‘–. All input vectors col- lectively form the hidden representation of user sequence ğ‘ ğ‘¢,ğ‘¡ as H0 = [h0 ğ‘¡ âˆ’ğ‘‡ +1, ..., h0 ğ‘¡ ]. 2.2.2 Transformer Encoder. A Transformer encoder is composed of a multi-head self-attention module and a position-wise