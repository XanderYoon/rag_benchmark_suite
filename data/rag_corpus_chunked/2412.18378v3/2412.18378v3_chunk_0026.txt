original perfor- mance, while “w/o aug” drops 18.4%, in terms of NDCG@5, when both have a 10% noise ratio. This fully implies that with retrieval augmentation, RaSeRec can retrieve useful sequential patterns as references to alleviate the influence of noisy interactions and make a suitable recommendation. In a nutshell, we suggest performing retrieval augmentation to alleviate the negative influence of noisy interactions. 5 Related Works 5.1 Retrieval-Augmented Generation Recently, RAG prevails in LLMs, introducing non-parametric knowl- edge into LLMs to supplement their incomplete, incorrect, or out- dated parametric knowledge [12]. It has been shown highly effective in many research fields, not limited to natural language, including computer vision [36, 37, 40, 52], speech [45, 54], code [28, 47], and graph [8]. Although being well-studied in many fields, very limited works exploit RAG in recommender systems, let alone SeRec. A very recent one is RUEL [ 49]. However, it differs from our work in (1) it lacks retrieval training, and (2) it falls short in decoupling implicit and explicit memories, while our work proposes a model- agnostic collaborative-based pre-training and retrieval-augmented fine-tuning paradigm for SeRec that can be applied to any ID-based SeRec backbones and improve their performance. 5.2 Sequential Recommendation Pioneering attempts on SeRec are based on Markov Chain (MC) to model item-item transition relationships, e.g., FPMC [39]. To capture long-term dependencies, some studies first adopt RNN for SeRec by modeling sequence-level item transitions, e.g., GRU4Rec [15]. Simultaneously, CNN has been explored for SeRec by modeling the union-level sequential patterns [42]. Recently, the success of self- attention models in NLP [44] has spawned a series of Transformer- based sequential recommendation models, e.g., SASRec [21] and BERT4Rec [41]. Considering sparse supervised signals, SSL has been widely adopted in SeRec to alleviate this issue [ 33, 34, 53]. Enlightened by generative models,