the input vector: h0 ğ‘– = vğ‘– + pğ‘–, (3) where h0 ğ‘– is the input vector at position ğ‘–. All input vectors col- lectively form the hidden representation of user sequence ğ‘ ğ‘¢,ğ‘¡ as H0 = [h0 ğ‘¡ âˆ’ğ‘‡ +1, ..., h0 ğ‘¡ ]. 2.2.2 Transformer Encoder. A Transformer encoder is composed of a multi-head self-attention module and a position-wise feed- forward Network. More technical details about these two modules can be found in [21] and [44]. We stack multiple blocks to capture more complex sequential patterns for high-quality user representa- tion. Specifically, an ğ¿-layer Transformer encoder (unidirectional attention) is applied to refine the hidden representation of each item in H0, which can be formulated as follows: Hğ¿ = Trm(H0), (4) where Trm(Â·) denotes the ğ¿-layer Transformer encoder, Hğ¿ = [hğ¿ ğ‘¡ âˆ’ğ‘‡ +1, ..., hğ¿ ğ‘¡ ] is the refined hidden representations of the se- quence. The last refined hidden representation hğ¿ ğ‘¡ is selected as the representative of the user sequence. Here, we omit the superscript ğ¿ and subscript ğ‘¡ for brevity, i.e., h. In addition, we use SeqEnc(Â·) to denote the sequence encoder, that ish = SeqEnc(ğ‘ ğ‘¢,ğ‘¡ ) for simplicity. 3 Methodology We propose the Retrieval-Augmented Sequential Recommenda- tion (RaSeRec) paradigm, which endows SeRec to accommodate preference drift and recall long-tailed patterns, as shown in Fig- ure 2. Specifically, it consists of a two-stage training strategy: (1) collaborative-based pre-training, which learns to recommend and retrieve based on collaborative signals (Â§3.1); (2) retrieval- augmented fine-tuning, which learns to leverage retrieved mem- ories to augment user representation (Â§3.2). Lastly, we introduce the inference process and analyze the complexity of RaSeRec (Â§3.3). The notation table can be found in Appendix A. 3.1 Collaborative-based Pre-training In this section, we pre-train the model backbone SeqEnc(Â·) with two learning objections: (1) recommendation learning,