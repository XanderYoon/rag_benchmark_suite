the above limitations. Though being well- studied in Large Language Models (LLMs) [2, 17, 18, 24, 60], RAG is rarely explored in SeRec. The main idea of RAG is to retrieve relevant documents from an external knowledge base to facilitate LLMsâ€™ generation, especially for knowledge-intensive tasks [24]. For example, RAG [ 24] uses the input sequence as the query to retrieve text documents from Wikipedia and then adopts them as additional context to generate the target sequence; Self-RAG [2] learns to retrieve passages on demand adaptively, and interleaves generation and retrieval with reflection tokens by unifying them as the next token prediction. Compared to SSL (similar to pre-training), arXiv:2412.18378v3 [cs.IR] 13 Feb 2025 Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Zhao et al. Figure 1: Comparison between Vanilla, SSL-Augmented, and Retrieval-Augmented SeRec paradigms, where their system working flows are illustrated from left to right, respectively. ğ‘ ğ‘¢ denotes the user sequence, while ğ‘ â€²ğ‘¢, ğ‘ â€²â€²ğ‘¢ represent two aug- mented views. SeqEnc denotes the sequence encoder, refer- ring to Â§2.2 for more technical details. RAG allows us to accommodate preference drift rapidly and recall long tails via looking up an external memory bank directly. Here, we wish to bring the RAGâ€™s superiority into SeRec to en- hance user representation learning, which differs from NLP tasks since they model language semantics while recommender systems model collaborative semantics [61]. To address the above limitations of SSL-Augmented SeRec models, we design a retrieval-augmented mechanism to improve the quality of user representations. Specifi- cally, it consists of two key components: (1) memory retrieval, which recalls collaborative memories for the input user, and (2) representation augmentation, which leverages retrieved mem- ories to augment user representation. On the one hand, memory retrieval adapts well to dynamic preference drifts with a real-time update memory bank. On the other hand,