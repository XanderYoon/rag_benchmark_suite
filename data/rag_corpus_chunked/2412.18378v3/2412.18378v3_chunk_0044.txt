accessed 1 control coefficient ğ›¼* [0.0, 1.0] control coefficient ğ›½* [0.0, 1.0] number of retrieved memories ğ¾ * [5, 55] learning rate 0.001 optimizer Adam mini-batch size 1024 â€¢ HR@N measures whether the target item of the test user se- quence appears in the first ğ‘ recommended items: HR@ğ‘ = 1 |U | âˆ‘ï¸ ğ‘¢ âˆˆ U ğ›¿ (ğ‘£ğ‘¡ +1 âˆˆ ğ‘…(ğ‘¢)), (13) where ğ›¿ (Â·) is an indicator function; ğ‘£ğ‘¡ +1 denotes the target item; ğ‘…(ğ‘¢) denotes the top-ğ‘ recommended list of user ğ‘¢. â€¢ NDCG@N further measures the ranking quality. It logarithmi- cally discounts the position: NDCG@ğ‘ = 1 |U | âˆ‘ï¸ ğ‘¢ âˆˆ U ğ‘âˆ‘ï¸ ğ‘—=1 ğ›¿ (ğ‘£ğ‘¡ +1 = ğ‘…(ğ‘¢) ğ‘— ) 1 log2 ( ğ‘— + 1) , (14) where ğ‘…(ğ‘¢) ğ‘— denotes the ğ‘—-th recommended item for user ğ‘¢. E Implementation Details We use the Adam optimizer [23] to optimize model parameters with the learning rate of 0.001, the mini-batch size of 1024, ğ›½1 = 0.9, and ğ›½2 = 0.999, where the maximum number of epochs is set to 100 for both pre-training and fine-tuning stages. We train models with the early stopping strategy that interrupts training if the HR@10 result on the validation set continues to drop until 10 epochs. For RAM, we tune ğ›¼, ğ›½ and ğ¾ within the ranges of {0.0, 0.1, 0.2, ..., 1.0}, {0.0, 0.1, 0.2, ..., 1.0} and {5, 10, 15, ..., 55}, respectively, referring to Appendix F for more details. As for Faiss, we set the number of clus- ters ğ‘˜ to 128, set the number of clusters accessed as 1, and adopt the cosine similarity as the measurement to retrieve relevant memories. As for the sequence encoder, we adopt a two-layer Transformer and set the head number as 2 for each self-attention block, where the hidden