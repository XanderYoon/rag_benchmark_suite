â€™XX, June 03â€“05, 2018, Woodstock, NY Zhao et al. Figure 3: The architecture of Retrieval-Augmented Module, where Ëœh is the augmented user representation. As such, for each positive pair, there are2(|B|âˆ’ 1) negatives forming the negative set Sâˆ’. For example, for the positive pair âŸ¨hâ€² 1, hâ€²â€² 1 âŸ©, its corresponding negative set is Sâˆ’ 1 = {hâ€² 2, hâ€²â€² 2 , ..., hâ€² | B |, hâ€²â€² | B |}. Having established the positive and negative pairs, we adopt InfoNCE [13] as the loss function, which can be defined as follows: Lğ‘Ÿğ‘’ğ‘¡ = âˆ’  log ğ‘’s(hâ€² ğ‘–,hâ€²â€² ğ‘– )/ğœ ğ‘’s(hâ€² ğ‘–,hâ€²â€² ğ‘– )/ğœ + Ã ğ‘  âˆ’ âˆˆ Sâˆ’ ğ‘– ğ‘’s(hâ€² ğ‘–,hâˆ’ )/ğœ + log ğ‘’s(hâ€²â€² ğ‘– ,hâ€² ğ‘– )/ğœ ğ‘’s(hâ€²â€² ğ‘– ,hâ€² ğ‘– )/ğœ + Ã ğ‘  âˆ’ âˆˆ Sâˆ’ ğ‘– ğ‘’s(hâ€²â€² ğ‘– ,hâˆ’ )/ğœ  , (8) where we take the ğ‘–-th positive pair as an example; Sâˆ’ ğ‘– denotes the negative set; ğ‘  (Â·) measures the similarity between two vectors, which is implemented as the inner product; ğœ is the temperature, which is used to control the smoothness of softmax. We pre-train the model backbone with the training objectives of Lğ‘Ÿğ‘’ğ‘ as well as Lğ‘Ÿğ‘’ğ‘¡ , simultaneously. 3.2 Retrieval-Augmented Fine-tuning After pre-training, the model backbone has been endowed with the ability to recommend the next item and retrieve relevant memories. However, it still suffers from two limitations: (i) It can hardly ac- commodate dynamic preference drifts as it is trained on past user preferences, and (ii) It may fail to recall long-tailed patterns from implicit memory due to skewed data distribution. Given the above issues, we propose Retrieval-Augmented Fine-Tuning, RAFT, mak- ing the model quicker to adapt to preference drifts and easier to recall long-tailed patterns. It mainly consists of two components:(1) memory retrieval, which