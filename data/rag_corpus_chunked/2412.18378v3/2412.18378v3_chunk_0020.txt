0.0226 0.0352 0.0132 0.0173 DuoRec 0.0505 0.0759 0.0318 0.0400 0.0275 0.0436 0.0170 0.0222 RaSeRec 0.0534 0.0787 0.0344 0.0426 0.0288 0.0451 0.0182 0.0236 maintaining a dynamic memory bank, RaSeRec can quickly adapt to preference drifts. (ii) By retrieving memories, RaSeRec can ex- plicitly recall long-tailed patterns that may be overwhelmed by head ones. These above two innovative designs contribute to con- siderable improvements together. 4.3 Improving Base Backbones (RQ2) RaSeRec is a model-agnostic retrieval-augmented SeRec paradigm that can enhance various base models. To verify this, instead of using the default Self-attentive backbone SASRec [ 21], we eval- uate RaSeRec using other backbones, i.e., RNN-based GRU4Rec [15] and Bert-based BERT4Rec [41]. As the average performance of DuoRec and MCLRec is close, we mainly compare with DuoRec in the following. As shown in Table 3, RaSeRec performs best with different backbones, indicating that RaSeRec has a good general- ization ability. The impressive performance gain demonstrates the effectiveness of our retrieval-augmented mechanism, even though we just add a very small set of parameters (about 5%). In summary, RaSeRec possesses the virtue of the plug-and-play property, which can enhance various given base models with retrieval augmentation while leaving the base model backbone unchanged. 4.4 Long-tailed Recommendation (RQ3) As stated in Section 1, existing SeRec models make it hard to recall long-tailed patterns via implicit memories due to skewed super- vised signal distribution. To verify whether RaSeRec is of promise in solving this issue, we split the test user sequences into 10 groups based on the target itemâ€™s popularity measured by the number (a) Results on the Beauty dataset w.r.t. NDCG@5. (b) Results on the Sports dataset w.r.t. NDCG@5. Figure 4: Performance comparison over different item groups between RaSeRec and DuoRec. The bar represents NDCG@5, while the line represents the performance improvement per- centage of RaSeRec