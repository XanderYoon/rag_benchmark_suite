for more details. As for Faiss, we set the number of clus- ters ğ‘˜ to 128, set the number of clusters accessed as 1, and adopt the cosine similarity as the measurement to retrieve relevant memories. As for the sequence encoder, we adopt a two-layer Transformer and set the head number as 2 for each self-attention block, where the hidden size ğ‘‘ and the maximum sequence length ğ¿ are set to 64 and 50 respectively. Additionally, for Non-SeRec and Vanilla SeRec methods, we use results reported by DuoRec [34] as these methods already have generally acknowledged experimental results on the three benchmark datasets used in this paper. We ran each experi- ment five times and reported the average. Table 9 summarizes the detailed hyper-parameter settings. F Parameter Sensitivity (RQ7) We study the effects of RaSeRecâ€™s key hyper-parameters on its performance, including the control coefficients ğ›¼, ğ›½ (Equ. (11)) and the number of retrieved memories ğ¾ (Â§3.2). For ğ›¼ and ğ›½, we vary them from 0 to 1.0, incrementing by 0.1; for ğ¾, we vary it from 5 to 55, incrementing by 5. We ran each experiment five times and computed the average as well as standard deviation results. The experimental results are shown in Figure 7, where the line denotes the average results and the error bar denotes the scaled-up standard deviation results. From the results, we mainly have the following observations: Firstly, we can observe that setting ğ›¼ to a too-big value (e.g., 1.0) or to a too-small value (e.g., 0.1) will significantly hurt the model performance. When alpha is high, implicit memories dominate the user representation, whereas if alpha is low, explicit memories dominate the user representation. The above observation indicates the user representation should not be dominated by one side, i.e., implicit memories or explicit memories. In