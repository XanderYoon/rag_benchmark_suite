representation via modeling the relationship between user representation and the retrieved user representation (Equ. (9)) can produce more informa- tive and accurate user representation. We think the main reason is the model backbone learns to recommend items based on the user sequence instead of recommending users based on the target item. As such, users with similar behaviors lead to similar target items, whereas users who like the same item do not necessarily have similar behaviors. For the above reasons, augmenting user representation via modeling the relationship between user repre- sentation and the retrieved user representation (Equ. (9)) performs better than that between user representation and the corresponding target item embedding of the retrieved user representation (Equ. (10)). In conclusion, we suggest tuning ğ›½ in the range of 0.7 âˆ¼ 1.0 carefully. Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Zhao et al. (a) Results on the Beauty dataset w.r.t. HR@5 and NDCG@5. (b) Results on the Sports dataset w.r.t. HR@5 and NDCG@5. Figure 8: Performance comparison over different user groups between the model with retrieval augmentation and without retrieval augmentation. The bar represents HR@5 and NDCG@5, while the line denotes the performance improvement percentage of â€œw/ augâ€ compared to â€œw/o augâ€. Thirdly, we observe that the performance of RaSeRec reaches the peak to different ğ¾ across three datasets, e.g., 20 is the best value for the Beauty dataset. This demonstrates the effectiveness of augmenting user representation via retrieved explicit memories and manifests that setting a suitable number of retrieved memories can considerably enhance performance. The results on the three datasets follow similar trends, where the performance gradually improves as ğ¾ increases and then starts to degrade when further increasing ğ¾. This phenomenon is also observed in long-context RAG [19, 56], where the generative modeling performance initially increases and then declines