been explored for SeRec by modeling the union-level sequential patterns [42]. Recently, the success of self- attention models in NLP [44] has spawned a series of Transformer- based sequential recommendation models, e.g., SASRec [21] and BERT4Rec [41]. Considering sparse supervised signals, SSL has been widely adopted in SeRec to alleviate this issue [ 33, 34, 53]. Enlightened by generative models, some studies have attempted to apply LLMs to solve SeRec [25, 26, 61, 62]. Albeit studied for ages, almost all existing methods focus on exploiting implicit memories hidden in the model. This work is in an orthogonal direction. It exploits explicit memories in a retrieval-augmented manner and opens a new research direction for SeRec. RaSeRec: Retrieval-Augmented Sequential Recommendation Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY 6 Conclusion and Future Works In this work, we unveiled the limitations of the Vanilla SeRec and SSL-Augmented SeRec paradigms and explored the potential of RAG to solve them. In particular, we propose a retrieval-augmented SeRec paradigm, which learns to refine user representation with explicit retrieved memories and adapt to preference drifts by main- taining a dynamic memory bank. Extensive experiments on three datasets manifest the advantage of RaSeRec in terms of recommen- dation performance, long-tailed recommendation, and alleviating preference drift. Despite our innovations and improvements, we recognize some issues that warrant further study. In this work, we fixed the number of retrieved memories ùêæ during training and inference. In Appendix F, we have studied the impact of different ùêæ values on the model performance. We find both too large and small ùêæ values hurt the model performance. Furthermore, in Sec- tion 4.4 and Appendix G, we have studied the impact of retrieval augmentation on different item groups and user groups. The ex- perimental results show that retrieval augmentation may hurt the performance of