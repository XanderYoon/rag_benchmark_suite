enhances long- tailed recommendation, and alleviates preference drift. 2 Preliminaries 2.1 Problem Formulation In this section, we first introduce the symbols and then formalize the problem of SeRec. Let U and V denote the set of users and items, respectively, where |U | and |V | denote the number of users and items. The historically interacted items of a user are sorted in chronological order: ğ‘ ğ‘¢ = [ğ‘£ (ğ‘¢ ) 1 , ğ‘£ (ğ‘¢ ) 2 , ..., ğ‘£ (ğ‘¢ ) |ğ‘ ğ‘¢ | ], where ğ‘£ (ğ‘¢ ) ğ‘¡ âˆˆ V, 1 â‰¤ ğ‘¡ â‰¤ | ğ‘ ğ‘¢ | denotes the item interacted by user ğ‘¢ âˆˆ U at the time step ğ‘¡ and |ğ‘ ğ‘¢ | denotes the length of interaction sequence of user ğ‘¢. Besides, ğ‘ ğ‘¢,ğ‘¡ = [ğ‘£ (ğ‘¢ ) 1 , ğ‘£ (ğ‘¢ ) 2 , ..., ğ‘£ (ğ‘¢ ) ğ‘¡ ] is a subsequence, where items are interacted by userğ‘¢ before the time step ğ‘¡ + 1. The goal of sequential recommendation is to predict the next itemğ‘£ (ğ‘¢ ) |ğ‘ ğ‘¢ |+1 given the interaction sequence ğ‘ ğ‘¢, which can be formulated as follows: ğ‘£âˆ— = arg max ğ‘£ğ‘– âˆˆ V ğ‘ƒ  ğ‘£ (ğ‘¢ ) |ğ‘ ğ‘¢ |+1 = ğ‘£ğ‘– |ğ‘ ğ‘¢  , (1) where ğ‘£âˆ— denotes the predicted item with the largest matching prob- ability. In what follows, we employ bold lowercase and uppercase symbols to represent vectors and matrices, respectively. 2.2 Transformer for SeRec In this section, we introduce how to model usersâ€™ historical inter- actions to get their representations. Specifically, we adopt SASRec [21] as the model backbone, whose encoding module is based on the Transformer encoder [44]. To leverage the transformerâ€™s strong encoding ability, we first convert items to embeddings. Then, we apply the Transformer encoder to generate the user representation. 2.2.1 Embedding Layer. Formally,