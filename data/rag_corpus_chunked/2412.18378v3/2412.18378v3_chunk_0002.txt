of the total items [ 16], making it difficult to learn high- quality user representations. Recently, there has been a surge of research interest in leveraging self-supervised learning (SSL) within the realm of SeRec to alleviate the data sparsity issue caused by sparse supervised signals [7, 14, 29, 34, 53]. Generally, it involves two procedures: (1) data augmentation, which generates multiple views for each user sequence, and (2) self-supervised learning, which maximizes the agreement between different views of the same (or similar) user sequence. In this way, SSL-Augmented models endow their model backbones with additional parametric knowl- edge to generate more high-quality user representations than those without SSL [48]. Despite their effectiveness, we argue that current SSL-Augmented ones still suffer from some severe limitations: • Preference Drift. In real-world recommender systems, user preference frequently drifts as time goes on [3, 30] due to many reasons, e.g., life changes [1]. As such, models trained on past user preferences can hardly handle preference drifts and may rec- ommend undesirable items to users, which leads to sub-optimal performance. • Implicit Memory. Existing methods encode user sequential patterns into implicit memory (model parameters), where the distribution of observed patterns usually obeys a power law [5, 32]. As such, long-tailed patterns that lack supervised signals may be overwhelmed by head ones during training, making it hard to recall long-tailed ones during inference. In this work, we explore Retrieval-Augmented Generation (RAG) [12] in SeRec, to address the above limitations. Though being well- studied in Large Language Models (LLMs) [2, 17, 18, 24, 60], RAG is rarely explored in SeRec. The main idea of RAG is to retrieve relevant documents from an external knowledge base to facilitate LLMs’ generation, especially for knowledge-intensive tasks [24]. For example, RAG [ 24] uses the input sequence as the query to