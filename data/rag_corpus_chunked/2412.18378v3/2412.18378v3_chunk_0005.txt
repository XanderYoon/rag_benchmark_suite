training, which learns to retrieve memories with the same (or similar) preference as the input user sequence. During the retrieval-augmented fine-tuning stage, we treat <user sequence, target item> pairs as the reference set ex- cept the current input user sequence. We then use the pre-trained model to encode the reference set into a memory bank. Given a user sequence, our model first retrieves similar collaborative mem- ories from the memory bank. Then, a Retrieval-Augmented Module (RAM) is built upon the pre-trained encoder, which learns to aug- ment the representation of the input user with retrieved memories. An intuitive explanation of our idea is that it can be seen as an open-book exam. Specifically, with retrieval augmentation, RaSeRec does not have to memorize all sequential patterns whereas it learns to use retrieved ones (cheat sheets) from the memory bank. Note that RaSeRec is model-agnostic and can be applied to any ID-based SeRec backbones. Here, we adopt SASRec [ 21] as the backbone since it is simple yet effective. We also implement it on different backbones (see §4.3). In summary, our contributions are: • We unveil the major issues existing models suffer from and shed light on the potential of RAG in SeRec. • We propose a new SeRec paradigm, RaSeRec, which explicitly retrieves collaborative memories and then learns to use them for better user representation modeling. • Extensive experiments on three benchmark datasets fully demon- strate that RaSeRec improves overall performance, enhances long- tailed recommendation, and alleviates preference drift. 2 Preliminaries 2.1 Problem Formulation In this section, we first introduce the symbols and then formalize the problem of SeRec. Let U and V denote the set of users and items, respectively, where |U | and |V | denote the number of users and items. The historically interacted items of a user