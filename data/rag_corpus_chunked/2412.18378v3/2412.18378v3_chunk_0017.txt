models? ‚Ä¢ RQ2: How much gain can RaSeRec bring to the existing base backbones? ‚Ä¢ RQ3: Can RaSeRec improve long-tailed recommendation? ‚Ä¢ RQ4: Can RaSeRec alleviate preference drift? ‚Ä¢ RQ5: How do different partitions of the memory bank contribute to RaSeRec‚Äôs performance? ‚Ä¢ RQ6: Can RaSeRec perform robustly against the data noise issue? ‚Ä¢ RQ7: How does the performance of RaSeRec vary with different hyper-parameter values? (Appendix F) ‚Ä¢ RQ8: Whether RaSeRec benefits both high-frequency and low- frequency users? (Appendix G) 4.1 Experimental Settings 4.1.1 Datasets. We conduct extensive experiments on three bench- mark datasets collected from Amazon [ 31]. We adopt three sub- categories, i.e., Beauty, Sports, and Clothing1, with different sparsity degrees. Following [21, 41], we discard users and items with fewer than 5 interactions. The statistics of datasets after preprocessing are provided in Appendix B. 4.1.2 Baselines. To verify the effectiveness of RaSeRec, we com- pare it with the following three groups of competitive models: (1) Non-sequential recommendation models (Non-SeRec) in- clude PopRec and BPR-MF [38]; (2) Vanilla sequential recom- mendation models (Vanilla SeRec) include GRU4Rec [15], Caser [42], SASRec [ 21], BERT4Rec [ 41], and S3RecMIP [63]; and (3) SSL-Augmented sequential recommendation models (SSL- Augmented SeRec) include CL4SRec [53], CoSeRec [29], ICLRec [4], DuoRec [34], and MCLRec [33]. Detailed descriptions of each baseline model can be seen in Appendix C. 4.1.3 Implementation and Evaluation. We use Recbole [57] to im- plement baselines. For models with learnable item embedding, we set the hidden size ùëë = 64. For each baseline, we follow the suggested settings reported in their original paper to set hyper- parameters. More implementation details can be seen in Appendix E. Following the leave-one-out strategy, we hold out the last inter- acted item of each user sequence for testing, the second last item for validation,