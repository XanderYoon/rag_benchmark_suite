Late top-Qwen - 0.383 0.102 0.179 0.351 0.122 0.203 Jina-V2 Early Fix-size 512 0.261 0.064 0.124 0.237 0.075 0.137 Late Fix-size 512 0.280 0.069 0.125 0.255 0.081 0.146 Early Jina-Sem - 0.294 0.079 0.144 0.269 0.092 0.158 Late sim-Qwen - 0.278 0.071 0.130 0.253 0.083 0.146 Late top-Qwen - 0.279 0.070 0.135 0.254 0.081 0.147 BGE-M3 Early Fix-size 512 0.246 0.059 0.120 0.225 0.069 0.130 Late Fix-size 512 0.070 0.010 0.029 0.067 0.013 0.038 Early Jina-Sem - 0.260 0.066 0.122 0.240 0.079 0.144 Late sim-Qwen - 0.091 0.015 0.038 0.081 0.018 0.045 Late top-Qwen - 0.110 0.019 0.044 0.097 0.022 0.048 T able 3. EarlyVsLate Retriever comparison on NFCorpus. Bold values indicate the best performance for each metric Model ChunkSegm LengthNDCG@5MAP@5F1@5NDCG@10MAP@10F1@10 Stella-V5 Early Fix-size 512 0.630 0.501 0.019 0.632 0.502 0.011 Late Fix-size 512 0.503 0.340 0.018 0.505 0.341 0.010 T able 4. EarlyVsLate Retriever comparison MsMarco. Bold values indicate the best performance for each metric. Method NDCG@5 MAP@5 F1@5 NDCG@10 MAP@10 F1@10 Late 0.309 0.143 0.202 0.294 0.160 0.192 Contextual 0.317 0.146 0.206 0.308 0.166 0.202 T able 5.Latechunking (Late) comparison versus ContextualRankFusion (Contextual) best performances, on sameNFCorpus dataset subset (20% of the whole). Embedding Model: Jina-V3. Chunking Method: Fixed-Window Chunking. ios, both cannot be considered definitive solutions to tackle the problem. Late chunking offers a more computationally efficient solution by leveraging the nat- ural capabilities of embedding models. In contrast, contextual retrieval, with its reliance on LLMs for context augmentation and re-ranking, incurs higher com- putational expenses. It also notable that the type of document and itâ€™s length can affect the performances, together with the LLM chosen for the task, smaller and more efficient models performing worse. This distinction is crucial for applications where computational resources are a significant consideration like in real-world scenarios. Preprint Acknowledgment This preprint