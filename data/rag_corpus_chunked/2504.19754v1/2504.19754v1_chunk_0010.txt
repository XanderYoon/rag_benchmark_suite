input text, potentially impacting the quality and depth of the generated content. In RQ#1, the evaluation was conducted on the first 1,000 queries and approx- imately 5,000 documents/passages. For RQ#2, due to the significant computa- tional requirements and hardware limitations, the experiments were restricted to 50 queries and around 300 documents. 6 https://huggingface.co/microsoft/Phi-3.5-mini-instruct 8 J. Singh and C. Merola Model MTEB RankModel Size(M)Memory(GB)Embedding DimMax Token Stella-V5 5 1,543 5.75 1,024 131,072 Jina-V3[20] 53 572 2.13 1,024 8,194 Jina-V2 [10] 123 137 0.51 1,024 8,194 BGE-M3 [4] 211 567 2.11 1,024 8,192 T able 1.Embedding models. 4.1 Embedding generation Common to both RQs.To generate embeddings for our experiments, we utilized different embedding models, as detailed previously (see section 4). Each seg- mentation model approach outlined was paired with an appropriate embedding model to evaluate its influence on downstream tasks. For fixed-size segmentation, we divided the text into equal-sized chunks with a predefined length of 512 characters. This approach ensures uniform chunk sizes, simplifying processing and offering a baseline for comparison with more adaptive methods. For semantic segmentation, we used theJina-Segmenter API ,7 which dy- namically adjusts chunk boundaries based on the semantic structure of the text. This ensures that the segments capture meaningful content, improving the qual- ity of embeddings generated. All the generated embeddings were normalized to unit vectors, facilitating co- sine similarity computations during the retrieval phase and ensuring uniformity across experiments. RQ#1: In addition to the mentioned segmentation approaches, for this work- flow, dynamic segmentation was tested, testing two models to assess their perfor- mance.Thefirstmodel, simple-qwen-0.5,isastraightforwardsolutiondesigned to identify boundaries based on the structural elements of the document. Its sim- plicity makes it efficient for basic segmentation needs, offering a computationally lightweight approach. The second model, topic-qwen-0.5, inspired by Chain-of-Thought reason- ing, enhances segmentation by identifying topics within the