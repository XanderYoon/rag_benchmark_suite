rela- tionships between the query and the document before being given to the LLM. Scorings. For both approaches in RQ#1 and RQ#2, when querying the embed- ding database (generated in 4.1), the output will be a ranked list of chunks, ordered from the most similar to the query to the least similar. We employ a straightforward aggregation strategy to transition from chunk-level rankings to document-level rankings. Specifically, for each document, we consider the score of its most significant chunk as the representative value for the entire document. This approach ensures that a document’s relevance is determined by its most relevant chunk. Once the document scores are determined, we generate a ranked list of docu- ments based on these scores. From this ranking, we extract the top-k documents, focusing on the Top 5 or Top 10 documents, depending on the specific evaluation scenario. This final document ranking is then used to assess the effectiveness of the retrieval process. This methodology highlights the importance of individual chunks in influ- encing the overall document ranking and ensures that highly relevant chunks directly impact the document’s position in the final ranking. Metrics. To evaluate the performance of our model, we utilize three key met- rics: NDCG , MAP, and F1-score. Each metric serves a specific purpose in as- sessing different aspects of the results.Normalized Discounted Cumulative Gain (NDCG): It measures the usefulness of an item based on its position in the rank- ing, assigning higher weights to items appearing at the top of the list. By using NDCG, we aim to assess the relevance of predictions in a way that prioritizes higher-ranked items.Mean Average Precision (MAP): It calculates the mean of the Average Precision (AP) scores for all queries, where AP considers the pre- cision at each relevant item in the ranked list. With