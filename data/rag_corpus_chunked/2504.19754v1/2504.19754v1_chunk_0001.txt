relevant external knowledge, sig- nificantly improving their ability to generate accurate, contextually grounded, and informative responses. Unlike static LLMs that rely solely on pre-trained data, RAG-enabled models can access up-to-date and domain-specific informa- tion. This dynamic integration ensures that the generated content remains both relevant and accurate, even in rapidly evolving or specialized fields. ⋆ Equal contribution. arXiv:2504.19754v1 [cs.IR] 28 Apr 2025 2 J. Singh and C. Merola RAG models combine two key components: a retrieval mechanism and a generative model. The retrieval mechanism fetches relevant documents or data from a large corpus, while the generative model synthesizes this information into coherent, contextually enriched answers. This synergy enhances performance in knowledge-intensive natural language processing (NLP) tasks, enabling models to produce well-informed responses grounded in the retrieved data. The Context Dilemma in Classic RAG: Managing extensive external documentsposessignificantissuesinRAGsystems.Despiteadvancements,many LLMs are limited to processing a few thousand tokens. Although some models have achieved context windows up to millions of tokens [5], these are exceptions rather than the norm. Moreover, research indicates that LLMs may exhibit posi- tional bias, performing better with information at the beginning of a document and struggling with content located in the middle or toward the end [11,16]. This issue is exacerbated when retrieval fails to prioritize relevant information prop- erly. Thus, documents are often divided into smaller segments or "chunks" before embedding and retrieval. However, this chunking process can disrupt semantic coherence, leading to: –Loss of Context:dividing documents without considering semantic bound- aries can result in chunks that lack sufficient context, impairing the model’s ability to generate accurate and coherent responses. –Incomplete Information Retrieval:important information split across chunks may not be effectively retrieved or integrated. To address these issues, we analyse and compare two recent techniques— contextual retrieval1 and late chunking [9]—within a unified setup, evaluating their strengths