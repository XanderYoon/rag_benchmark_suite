range of long-context embedding models and can be implemented without additional training. The two approaches are tested with different embedding models. 3.2 RQ#2: Early or Contextual Chunking? In this workflow, traditional retrieval is compared to Contextual Retrieval with Rank Fusion technique. This has been introduced by Anthropic in September 2024.4 Three steps are added to the Traditional RAG process: Contextualization, Rank Fusion, Reraking. Contextualization. After document segmentation, each chunk is enriched with additional context from the entire document, ensuring that even when seg- mented, each piece retains a broader understanding of the content (Fig. 3.2). In fact, when documents are split into smaller chunks, it might arise the prob- lem where individual chunks lack sufficient context. For example, a chunk might containthetext:"Thecompanyâ€™srevenuegrewby3%overthepreviousquarter." However, this chunk on its own does not specify which company it is referring to or the relevant time period, making it difficult to retrieve the right information or use the information effectively. Contextualization improves the relevance and accuracy of retrieved information by maintaining contextual integrity. 4 https://www.anthropic.com/news/contextual-retrieval Reconstructing Context 5 Chunk Embedding model Chunk Embedding Pooling ..... Long document x N Embedding model Embedding Pooling ..... Long document PoolingPooling Boundary cues .... Embedding Token emb Token emb Token emb Token emb Token emb Token emb Token emb Fig. 1. Comparison of early chunking (left) and late chunking (right) approaches for processing long documents. In early chunking, the document is divided into chunks before embedding, with each chunk processed independently by the embedding model and then pooled. In contrast, late chunking processes the entire document to generate token embeddings first, using boundary cues to create chunk embeddings, which are subsequently pooled. Rank Fusion. In our methodology, we employ a rank fusion strategy that in- tegrates dense embeddings with sparse embeddings of BM25 [19] to improve retrieval performance. Although