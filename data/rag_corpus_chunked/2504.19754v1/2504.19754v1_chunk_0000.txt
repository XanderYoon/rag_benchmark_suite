Reconstructing Context Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation Carlo Merola[0009−0000−1088−1495] and Jaspinder Singh[0009−0000−5147−1249] ⋆ Department of Computer Science and Engineering, University of Bologna carlo.merola@studio.unibo.it, jaspinder.singh@studio.unibo.it Abstract. Retrieval-augmented generation (RAG) has become a trans- formative approach for enhancing large language models (LLMs) by grounding their outputs in external knowledge sources. Yet, a critical question persists: how can vast volumes of external knowledge be man- aged effectively within the input constraints of LLMs? Traditional meth- ods address this by chunking external documents into smaller, fixed- size segments. While this approach alleviates input limitations, it often fragments context, resulting in incomplete retrieval and diminished co- herence in generation. To overcome these shortcomings, two advanced techniques—late chunking and contextual retrieval—have been intro- duced, both aiming to preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coher- ence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice rel- evance and completeness. Keywords: Contextual Retrieval· Late Chunking· Dynamic Chunking · Rank Fusion. 1 Introduction Retrieval Augmented Generation (RAG) is a transformative approach that en- hances the capabilities of large language models (LLMs) by integrating external information retrieval directly into the text generation process. This method al- lows LLMs to dynamically access and utilize relevant external knowledge, sig- nificantly improving their ability to generate accurate, contextually grounded, and informative responses. Unlike static LLMs that rely solely on pre-trained data, RAG-enabled models can access up-to-date and domain-specific informa- tion. This dynamic integration ensures that the generated content remains both relevant and accurate, even in rapidly evolving or specialized fields. ⋆ Equal contribution. arXiv:2504.19754v1 [cs.IR] 28