processed independently by the embedding model and then pooled. In contrast, late chunking processes the entire document to generate token embeddings first, using boundary cues to create chunk embeddings, which are subsequently pooled. Rank Fusion. In our methodology, we employ a rank fusion strategy that in- tegrates dense embeddings with sparse embeddings of BM25 [19] to improve retrieval performance. Although embedding models adeptly capture semantic relationships, they may overlook exact matches, which is particularly useful for unique identifiers or technical terms. BM25 uses a ranking function that builds upon Term Frequency-Inverse Document Frequency (TF-IDF), addressing this limitation by emphasizing precise lexical matches. To combine the strengths of both approaches, we conduct searches across both dense embedding vectors and BM25 sparse embedding vectors generated from both the chunk and its gener- ated context. Initially, the assigned relative importance in the search for the two vector fields has been set to be of equal intensity, resulting in lowering the scoring results in the retrieval evaluation. For this reason we use a weighting strategy assigning higher weights to dense vector fields, emphasizing them more in the final ranking. While different weight parameters have been tested, the final deci- sion has been to define a ratio of importance 4:1 assigning weight 1 for the dense 6 J. Singh and C. Merola Corpus .....Document Embedding model Chunk + Context Prompt LLM for every chunkto generate context from document to be prepended. Chunk Embedding Fig. 2. Contextualization of each chunk is performed prior to embedding. The doc- ument is divided into chunks, and a prompt is used to query an LLM to generate contextual information from the document for each chunk. The context is prepended to the chunk, which is then processed by the embedding model to produce the final chunk embedding. embedding vectors and 0.25