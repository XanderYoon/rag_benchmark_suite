dataset, while the subsequent Generation performance in question answering has been conducted overMSMarco[2]. Another important note for Contextual Retrieval ( RQ#2). The NFCorpus dataset is characterised by a long average document length. Here appear the first limitations of the Contextual Retrieval approach to RAG. For the intrinsic nature of this approach, the segmented chunks are enhanced with a generated context taken from the document, prompting an LLM for the task, leveraging the new advents of Instruction Learning. Chunks and documents are passed together in a formatted prompt to the model. When a document reaches long lengths, the VRAM of the GPU gets filled up quickly. For chunk contextualization, around 20GB of VRAM use can be reached, limiting batch dimensions for generation and slowing down the times needed for effective chunk contextualization. In our experimental setup, we utilized an Nvidia RTX 4090 with 24GB of VRAM. Due to GPU memory constraints, we employed a subset of the dataset, corresponding to 20% of the entireNFCorpus for RQ#2, while the full dataset was used for RQ#1 workflow. For datasets such asMsMarco, which include only passage texts rather than full documents, the system operates within a more constrained context for gen- erating responses. This limitation arises because passages are typically shorter segments of text, providing less information for contextual understanding. As a result in RQ#2, the systemâ€™s ability to generate contextually relevant and com- prehensive responses can be affected by the brevity of the input text, potentially impacting the quality and depth of the generated content. In RQ#1, the evaluation was conducted on the first 1,000 queries and approx- imately 5,000 documents/passages. For RQ#2, due to the significant computa- tional requirements and hardware limitations, the experiments were restricted to 50 queries and around 300 documents. 6 https://huggingface.co/microsoft/Phi-3.5-mini-instruct 8 J. Singh and C. Merola Model