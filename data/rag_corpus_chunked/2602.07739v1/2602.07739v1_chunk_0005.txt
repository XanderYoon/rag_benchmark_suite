els initialize from LLMs to exploit their pretrained linguis- tic knowledge (Muennighoff et al., 2024; Lee et al., 2024; Zhang et al., 2025). However, most retrievers remain reliant on inner products or distances in Euclidean geometry-an in- ductive bias often misaligned with the hierarchical structure of language and document collections. We address this gap by introducing hyperbolic geometry for text embeddings to better capture such a hierarchy. Retrieval Augmented Generation.RAG grounds LLMs in retrieved evidence to improve factuality and access external knowledge (Oche et al., 2025). It typically retrieves top-k contexts (often via dense retrieval) and conditions genera- tion on them (Lewis et al., 2020). Since the context window is limited, retrieval quality is a key bottleneck for relevance and faithfulness (Friel et al., 2024a). Several methods im- prove reliabilityafterretrieval: Self-RAG (Asai et al., 2024) and CRAG (Yan et al., 2024) use learned critics to filter or re-rank evidence, while GraphRAG (Han et al., 2024) lever- ages knowledge graphs for structured subgraph retrieval. These approaches operate downstream of the embedding space and are complementary to ours geometric approach. Our goal is to improve RAG upstream by enhancing the 2 HypRAG retriever representations so that the initial top-k evidence is more reliable under realistic efficiency constraints. Hyperbolic Representation Learning.Hyperbolic ge- ometry is primarily known for its ability to better cap- ture hierarchical, tree-like structures (Yang et al., 2023; Peng et al., 2021), which enhances performance in vari- ous tasks, including molecular generation (Liu et al., 2019), recommendation (Yang et al., 2021; Li et al., 2021), im- age retrieval (Khrulkov et al., 2020; Wei et al., 2024; Bui et al., 2025), and knowledge graph embedding (Ganea et al., 2018a; Dhingra et al., 2018). More recently, hyperbolic ge- ometry has also shown promise for multi-modal embedding models (Desai et al., 2023; Ibrahimi et