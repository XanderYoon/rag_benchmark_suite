are evaluated on MTEB and RAGBench. For HyTE-Hbert, which is fine-tuned with modernbert-base (Warner et al., 2024) as base model, we compare against state-of-the-art em- bedding models smaller than 500M parameters, includ- ing gte-multilingual-base (Zhang et al., 2024), KaLM- embedding-multilingual-mini-v1 (Hu et al., 2025), and embeddinggemma-300m (Vera et al., 2025). Metrics.For MTEB, we report mean scores across tasks and task types. For RAG evaluation, we measure three key metrics using RAGAS (Es et al., 2024): (1)Faithfulness, 6 HypRAG Table 2.RAG benchmark results comparing our model variants. Average CovidQA Cuad Emanual DelucionQA ExpertQA Model F CR AR F CR AR F CR AR F CR AR F CR AR F CR AR EucBERT0.596 0.798 0.647 0.685 0.863 0.582 0.654 0.644 0.641 0.642 0.646 0.674 0.5250.9680.679 0.475 0.872 0.662 HyTE-HEuc 0.706 0.814 0.739 0.708 0.868 0.668 0.7870.652 0.710 0.679 0.835 0.814 0.737 0.857 0.773 0.623 0.859 0.728 HyTE-FH0.732 0.848 0.765 0.764 0.916 0.694 0.747 0.674 0.752 0.6600.807 0.704 0.7890.906 0.861 0.702 0.936 0.814 F = Faithfulness, CR = Context Relevance, AR = Answer Relevance. Best results in bold. Table 3.RAG benchmark results comparing our hybrid model with state-of-the-art embedding models. HyTE-H demonstrates competitive performance particularly in context relevance and answer relevance. Average CovidQA Cuad Emanual DelucionQA ExpertQA Model F CR AR F CR AR F CR AR F CR AR F CR AR F CR AR ModernBert*0.617 0.748 0.6320.656 0.895 0.53780.632 0.709 0.7460.567 0.715 0.6390.655 0.6657 0.51830.575 0.758 0.718 GTE 0.659 0.701 0.6500.695 0.840 0.538 0.733 0.599 0.7790.546 0.608 0.6860.648 0.725 0.549 0.672 0.731 0.698 Gemma 0.603 0.735 0.6840.685 0.760 0.497 0.724 0.600 0.7780.555 0.884 0.6870.612 0.643 0.705 0.442 0.791 0.755 KaLM-mini-v10.624 0.719 0.5910.656 0.787 0.528 0.742 0.789 0.7160.565 0.776 0.6160.553 0.581 0.573 0.607 0.666 0.522 HyTE-Hbert 0.763 0.904 0.8320.797 0.974 0.755 0.760 0.683 0.8040.688 0.943 0.8990.829 0.965 0.871 0.739 0.958 0.834