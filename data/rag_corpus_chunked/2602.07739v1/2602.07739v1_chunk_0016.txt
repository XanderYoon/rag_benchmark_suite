using publicly available corpora following the data curation and filtering protocols introduced in nomic-embed (Nussbaum et al., 2025). For masked language modeling (MLM), we use the high-quality 2023 Wikipedia dump, which provides broad topical cover- age and long-form text suitable for learning general-purpose semantic representations. For contrastive pre-training, we leverage approximately 235 million text pairs curated and filtered as described in (Nussbaum et al., 2025), designed to encourage semantic alignment across paraphrases and re- lated content at scale. Finally, for task-specific fine-tuning, we use the training splits of the BEIR benchmark (Thakur et al., 2021), which comprises a diverse collection of re- trieval tasks spanning multiple domains and query styles. Evaluation Benchmarks.We evaluate our approach on two complementary benchmarks: (1) the Massive Text Em- bedding Benchmark (MTEB) (Muennighoff et al., 2023) to assess embedding quality across diverse tasks, and (2) RAGBench (Friel et al., 2024b) for end-to-end RAG sys- tem evaluation. In MTEB, we particularly use the English part of the benchmark. RAGBench evaluates RAG systems on domain-specific question-answering datasets including CovidQA, Cuad, Emanual, DelucionQA, and ExpertQA. Baselines.We adopt different baseline strategies for our two models based on their training paradigms. For HyTE- FH, which is pre-trained from scratch, we train a fully Euclidean equivalent called EucBERT using the same ar- chitecture and training setup. This controlled compari- son isolates the contribution of hyperbolic geometry. We also evaluate HyTE-HEuc, a hybrid hyperbolic model ini- tialized with EucBERT. The three models are evaluated on MTEB and RAGBench. For HyTE-Hbert, which is fine-tuned with modernbert-base (Warner et al., 2024) as base model, we compare against state-of-the-art em- bedding models smaller than 500M parameters, includ- ing gte-multilingual-base (Zhang et al., 2024), KaLM- embedding-multilingual-mini-v1 (Hu et al., 2025), and embeddinggemma-300m (Vera et al., 2025). Metrics.For MTEB, we report mean scores across tasks and task