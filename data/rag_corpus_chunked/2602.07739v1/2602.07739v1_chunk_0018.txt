0.6500.695 0.840 0.538 0.733 0.599 0.7790.546 0.608 0.6860.648 0.725 0.549 0.672 0.731 0.698 Gemma 0.603 0.735 0.6840.685 0.760 0.497 0.724 0.600 0.7780.555 0.884 0.6870.612 0.643 0.705 0.442 0.791 0.755 KaLM-mini-v10.624 0.719 0.5910.656 0.787 0.528 0.742 0.789 0.7160.565 0.776 0.6160.553 0.581 0.573 0.607 0.666 0.522 HyTE-Hbert 0.763 0.904 0.8320.797 0.974 0.755 0.760 0.683 0.8040.688 0.943 0.8990.829 0.965 0.871 0.739 0.958 0.834 F = Faithfulness, CR = Context Relevance, AR = Answer Relevance. Best results in bold. which assesses whether generated answers are grounded in the retrieved context; (2)Context Relevance, which mea- sures how relevant the retrieved documents are to the query; and (3)Answer Relevance, which evaluates how well the generated answer addresses the userâ€™s question. Implementation.We implement all hyperbolic models using HyperCore (He et al., 2025e) and train on NVIDIA H100 GPUs. All three models, HyTE-FH, HyTE-H, and Eu- cBERT, share the same architecture, each containing 149M parameters with 12 transformer layers and 768-dimensional embeddings. For generation and judging, we use Llama- 3.1-8B-Instruct (Weerawardhena et al., 2025). For RAG benchmarks, we fix the retrieval context window size to 5 for all models to ensure a controlled comparison; we additionally report ablations with larger context sizes in Appendix Table A3. 5.2. Results MTEB Benchmark.Table 1 reports performance on the MTEB benchmark. HyTE-FH achieves the highest mean score across tasks (56.41), outperforming both EucBERT (54.11) and HyTE-HEuc (54.57). On the task-type mean, HyTE-FH and HyTE-HEuc perform comparably (53.75 and 53.71, respectively), with both surpassing EucBERT (51.31). These results demonstrate that hyperbolic representations not only improve RAG retrieval but also remain competitive on general-purpose embedding benchmarks. We present task-wise results in Table A1. RAG Benchmark Results.Table 2 presents RAG bench- mark results across five datasets. HyTE-FH achieves the best average performance across all three metrics: faithful- ness (0.732), context relevance (0.848), and answer rele-