surpassing EucBERT (51.31). These results demonstrate that hyperbolic representations not only improve RAG retrieval but also remain competitive on general-purpose embedding benchmarks. We present task-wise results in Table A1. RAG Benchmark Results.Table 2 presents RAG bench- mark results across five datasets. HyTE-FH achieves the best average performance across all three metrics: faithful- ness (0.732), context relevance (0.848), and answer rele- vance (0.765). HyTE-HEuc ranks second overall, with both hyperbolic variants substantially outperforming EucBERT. On individual datasets, HyTE-FH leads on CovidQA, Cuad, DelucionQA, and ExpertQA, while HyTE-HEuc achieves the best context and answer relevance on Emanual. These results demonstrate that hyperbolic geometry consistently improves retrieval quality for RAG across diverse domains. Table 3 reports RAG performance across five datasets. HyTE-Hbert consistently outperforms strong Euclidean em- bedding baselines across all metrics, with particularly large gains in context relevance and answer relevance. These improvements indicate that hyperbolic representations are more effective at retrieving structurally relevant evidence, which is critical for downstream generation quality in RAG pipelines. In qualitative case studies shows in Appendix E.1, we observe that Euclidean models frequently fail to retrieve key supporting passages altogether, whereas hyperbolic model recover relevant evidence more reliably, leading to more faithful and contextually grounded answers. Concept-Level Hierarchy Analysis.A central motivation for hyperbolic embeddings is their capacity to preserve hi- erarchical relationships (Section 4.2). To understand how models capture document hierarchy, we analyze learned radii (distances from the origin in the Poincar√© ball) across five hierarchical levels: from Level 1 (most general, e.g., document-level topics) to Level 5 (most specific, e.g., fine- grained entities). Figure 4 presents these results. The fully hyperbolic model demonstrates clear hierarchical organi- zation with radii increasing monotonically from Level 1 (2.902) to Level 5 (3.488, +20.2%). This shows the model naturally places general concepts near the origin and spe- cific details