−d K(q, d).The contrastive loss over in-batch negatives is Lctr =− 1 N NX i=1 log exp(s(qi,d i)/τ), whereτ >0is a temperature parameter. Stage 3: Supervised Contrastive Learning Fine-tuning. In the final stage of training, we further fine-tune the encoder using supervised contrastive learning on labeled query– document data. Given a query qi, a set of relevant doc- uments D+ i , and a set of non-relevant documents D− i , the supervised contrastive objective encourages the query rep- resentation to be closer to all relevant documents than to non-relevant ones Lsup =− 1 N NX i=1 log P d+∈D+ i exp(s(qi,d +)/τ) P d∈D+ i ∪D− i exp(s(qi,d)/τ) , where τ >0 is a temperature parameter. This stage explic- itly aligns hyperbolic distances with supervised relevance signals, refining retrieval behavior beyond unsupervised co-occurrence structure. Retrieval-Augmented Generation.At inference time, the trained hyperbolic encoder is used to retrieve the top-k doc- uments C for a given queryt. These retrieved documents are then provided as context to a downstream generative lan- guage model. Prompt formatting and generation follow stan- dard practice and are provided in Appendix B. We present runtime and computational complexity in Appendix D. Table 1.Performance on MTEB benchmark. We report mean scores across tasks and task types. HyTE-FH performs best among the three models. Model Mean (Task) Mean (TaskType) EucBERT 54.11 51.31 HyTE-HEuc 54.57 53.71 HyTE-FH 56.41 53.75 5. Experiments and Results 5.1. Experimental Setup Datasets.We pre-train our models using publicly available corpora following the data curation and filtering protocols introduced in nomic-embed (Nussbaum et al., 2025). For masked language modeling (MLM), we use the high-quality 2023 Wikipedia dump, which provides broad topical cover- age and long-form text suitable for learning general-purpose semantic representations. For contrastive pre-training, we leverage approximately 235 million text pairs curated and filtered as described