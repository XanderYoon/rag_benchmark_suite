language models that operate entirely in hyper- bolic space, but these models target text generation rather than retrieval. In the RAG setting, HyperbolicRAG (Cao et al., 2025) projects embeddings into the Poincaré ball to encode hierarchical depth within a static, pre-built knowl- edge graph, using dual-space retrieval that fuses Euclidean and hyperbolic rankings. However, HyperbolicRAG relies on Euclidean encoders to produce the initial embeddings, leaving the fundamental geometric mismatch. Moreover, ag- gregating token embeddings into document representations poses a challenge that existing work in hyperbolic learning does not address (Yang et al., 2024). As we show in Proposi- tion 4.3, naively averaging tokens in Euclidean space before projecting to hyperbolic space causes representations to col- lapse toward the origin, destroying the hierarchical structure that is meant be to preserved. To this end, we introduce hyperbolic dense retrieval for RAG, framing embedding geometry as a design choice for improving evidence selection and grounding at the repre- sentation level. We study this through two complementary instantiations. First, HyTE-FH (Hyerbolic Text Encoder, Fully Hyperbolic) operates entirely in the Lorentz model of hyperbolic space, enabling end-to-end representation learn- ing. Second, HyTE-H (Hybrid) maps embeddings from off-the-shelf Euclidean encoders into hyperbolic space, al- lowing us to build on existing pre-trained Euclidean models. The Lorentz model’s intrinsic geometry enables parameter- efficient scaling: HyTE-H outperforms Euclidean baselines several times (2-3x) its size, reducing memory footprint in resource-constrained settings. To address the aggregation challenge in both instantiations, we introduce the Outward Einstein Midpoint, a geometry-aware pooling operator that amplifies tokens farther from the origin, provably preserving hierarchical structure during pooling. Through extensive evaluation on RAGBench, we demon- strate that both hyperbolic variants consistently outper- form Euclidean baselines in answer relevancy across multi- ple datasets, while achieving competitive performance on MTEB. Our experiments validate three key findings: (1) hy-