(n+1)×m and b∈R m with z= |W⊤x+b| , the Lorentzian linear transformation (Yang et al., 2024) is the map HLT:L K1,n →L K2,m given by, HLT(x;W,b) = r K2 K1 · " p ∥z∥2 −1/K 2,z # Hyperbolic Layer Normalization.Given token embed- dings X={x i}n i=1 ⊂H d K, hyperbolic layer normalization is defined as HypLayerNorm(X) = r K1 K2 ∥z∥2 2 − 1 K2 , r K1 K2 z ! where z=f LN xi,[1:d]  , fLN(·) denotes standard Eu- clidean LayerNorm applied to the spatial components of the embedding, and K1, K2 >0 are input and output curva- ture respectively. Lorentz Residual Connection.Let x, f(x)∈L K,n where x is an input vector and f(x) is the output of a neural network f. Then, the Lorentzian residual connection (He et al., 2025d) is given byx⊕ L f(x) =α 1x+α 2y, where αi =w i/ √ −K∥w 1x+w 2f(x)∥ L  ,fori∈ {0,1}, where α1, α2 are weights parametrized by constants (w1, w2)∈R 2 \ {(0,0)}. Hyperbolic Self-Attention.In hyperbolic attention, simi- larity is governed by hyperbolic geodesic distance (Ganea et al., 2018b). Given token embeddings X={x i}n i=1 ⊂ Hd K, queries, keys, and values are computed via Lorentz- linear transformations as Q= HLT(X;W Q,b Q), K= HLT(X;W K,b K), and V= HLT(X;W V ,b V ), where HLT(·) denotes a linear map in Lorentz space. Attention 3 HypRAG Figure 2.HyTE Architecture. A) HyTE-FH Encoder Block, B) HyTE-FH architecture, C) HyTE-H Architecture. weights are computed using squared hyperbolic geodesic distances (He et al., 2025c; Chen et al., 2022) as νi,j = exp −d2 K(qi,k j)/√m  Pn l=1 exp(−d2 K(qi,k l)/√m) , with head dimension m. This prioritizes geodesic proximity rather than angular similarity. The attended representation is obtained via a Lorentzian weighted midpoint AttL(x)i = Pn j=1 νi,jλjvj √