retrieval-augmented generation (RAG) systems (Lewis et al., 2020; Fan 1Yale University, USA2Hong Kong University of Science and Technology (Guangzhou), China 3NetApp, USA. Correspondence to: Rex Ying <rex.ying@yale.edu>. Preprint. February 10, 2026. 1The code is available at: https://anonymous.4open. science/r/HypRAG-30C6 Figure 1.Hierarchies in Text.(A) Documents naturally organize into branching hierarchies where general topics spawn increasingly specific subtopics. Euclidean spaces distort such hierarchies due to crowding effects, while hyperbolic geometry preserves hierar- chical relationships through exponential volume growth. (B) Ricci curvature analysis of document embeddings from strong baselines reveals predominantly negative curvature, indicating tree-like se- mantic structure. et al., 2024), where embedding quality directly determines whether generated responses are grounded in evidence or hallucinated. By retrieving relevant documents and condi- tioning generation on this context, RAG systems produce responses that are more attributable and aligned with ver- ifiable sources (Ni et al., 2025). Yet, despite advances in retrieval architectures, current systems continue to rely on Euclidean embeddings, a choice inherited from standard neural networks rather than from language structure itself. Natural language inherently exhibits strong hierarchical or- ganization (He et al., 2025b; Robinson et al., 2024), with semantic structure giving rise to locally tree-like neigh- borhoods. Euclidean spaces struggle to represent such branching hierarchies due to polynomial volume growth (He et al., 2025b), introducing shortcuts between hierarchically distinct regions that distort semantic relationships. In re- trieval settings, these distortions can cause semantically dis- tant documents to appear spuriously similar (Radovanovic et al., 2010; Bogolin et al., 2022), degrading retrieval preci- sion (Reimers & Gurevych, 2021): a query about a specific subtopic may retrieve documents from sibling or parent cat- egories that share similarity but lack the required specificity. To further see why geometry matters for retrieval, consider a query about transformer attention mechanisms (Figure 1A). Relevant documents form a natural hierarchyâ€”from general concepts