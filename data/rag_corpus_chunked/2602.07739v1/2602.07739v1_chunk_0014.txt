Outward Einstein Midpoint. Then, for allp≥1: r(mOEM K,p )≥r(m Ein K ). The OEM weights ˜wi ∝w ixp+1 i,0 concentrate more mass on high-radius points than the Einstein weights wixi,0, in- creasing the pre-projection time component while reducing pairwise dispersion. Full proof in Appendix A.5. Together, these results establish that the Outward Einstein Midpoint provably preserves hierarchical structure during aggregation, in contrast to both Euclidean averaging and the standard Einstein midpoint. We validate this empirically through 5 HypRAG concept-level hierarchy analysis (Section 5.2), showing that models using OEM pooling maintain monotonically increas- ing radii across semantic specificity levels—a property ab- sent in Euclidean baselines. 4.4. Training Methodology We train the hyperbolic encoder in three stages, with all objectives operating directly on the Lorentz manifold using geodesic-based similarity. Stage 1: Hyperbolic Masked Language Modeling.We initialize via masked language modeling (MLM), following the standard BERT objective in hyperbolic space. Contex- tualization is performed through hyperbolic self-attention, with all intermediate representations on the hyperboloid. Predictions are produced using a Lorentzian multinomial logistic regression (LorentzMLR) (Bdeir et al., 2024) head, which defines class logits via Lorentzian inner products. Only HyTE-FH is trained on MLM, while for HyTE-H we choose a pre-trained Euclidean model as the MLM base to leverage a sronger initialization in low-resource settings. Stage 2: Unsupervised Contrastive Pre-Training.We fine-tune the resulting MLM model on query–document pairs by minimizing unsupervised contrastive loss. Sim- ilarity is defined as negative geodesic distance s(q, d) = −d K(q, d).The contrastive loss over in-batch negatives is Lctr =− 1 N NX i=1 log exp(s(qi,d i)/τ), whereτ >0is a temperature parameter. Stage 3: Supervised Contrastive Learning Fine-tuning. In the final stage of training, we further fine-tune the encoder using supervised contrastive learning on labeled query– document data. Given a query qi, a set of relevant doc- uments D+