Lorentzian counterparts. Crucially, these operations preserve the same asymptotic complexity as their Euclidean analogues. Each Lorentz linear transformation (HLT) consists of a matrix multiplication followed by a constant number of scalar operations and a reprojection. This incurs O(nd2) time per layer, identical to a Euclidean linear layer up to constant factors. A5 HypRAG Hyperbolic self-attention computes pairwise geodesic distances between queries and keys. In the Lorentz model, each geodesic distance dK(qi,k j) is computed using a Lorentzian inner product, which costs O(d). Thus, attention score computation scales asO(n 2d)per layer, matching standard dot-product attention. The Lorentzian weighted midpoint used for value aggregation requires a weighted sum and normalization in Rd+1, contributingO(nd)time per token, and is dominated by the attention score computation. Overall, the time complexity of a single HyTE-FH layer is O(n2d+nd 2), and the total encoder complexity is O(L(n2d+ nd2)), which matches the asymptotic complexity of a standard Euclidean transformer. HyTE-H introduces an additional projection from Euclidean to hyperbolic space at the input, costing O(nd), which is negligible compared to the encoder cost. Pooling via Outward Einstein Midpoint.Given a sequence of n token embeddings, the Outward Einstein Midpoint computes radius-dependent weights, a weighted sum in Rd+1, and a single reprojection. This requires O(nd) time and O(d) memory, identical in order to standard mean pooling or the Einstein midpoint. Training Objectives.All training objectives operate on fixed-dimensional query and document embeddings. Geodesic similarity computation costsO(d)per queryâ€“document pair. Unsupervised contrastive pre-training with in-batch negatives of sizeN requires O(N 2d) per batch, matching the complexity of standard contrastive learning. Supervised contrastive fine-tuning has the same asymptotic cost. Masked language modeling introduces no additional asymptotic overhead beyond the encoder forward pass. Retrieval and RAG Inference.At inference time, dense retrieval over a corpus of size |D| requires computing hyperbolic distances between a query embedding and