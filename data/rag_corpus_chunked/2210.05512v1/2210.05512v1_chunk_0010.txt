uses the concatenation of query and the document as the input to a BERT encoder. The BERT encoder is then followed by a projection layer ğ‘Šğ‘ğ‘Ÿğ‘œ ğ‘— on top of its [ğ¶ğ¿ğ‘† ] token to compute the relevance score: ğ‘  (ğ‘, ğ‘‘) = ğµğ¸ğ‘…ğ‘‡ ( [ğ¶ğ¿ğ‘† ] ğ‘ [ğ‘†ğ¸ğ‘ƒ ] ğ‘‘ [ğ‘†ğ¸ğ‘ƒ ]) [ğ¶ğ¿ğ‘† ] âˆ— ğ‘Šğ‘ğ‘Ÿğ‘œ ğ‘— (4) In this equation, ğ‘ and ğ‘‘ represent the query and the document respectively and [ğ¶ğ¿ğ‘† ] as well as [ğ‘†ğ¸ğ‘ƒ ] are special BERT tokens [8]. 4https://github.com/ielab/TILDE/tree/main/TILDEv2 ICTIR â€™22, July 11â€“12, 2022, Madrid, Spain. Amin Abolghasemi, Arian Askari, and Suzan Verberne 3 METHODS AND EXPERIMENTAL SETTINGS In this section, we provide details and preliminaries about our methods and experimental settings. 3.1 Evaluation Benchmark We run our experiments on the SciDocs benchmark [5]. This dataset was originally introduced as a benchmark for representation learn- ing tasks. Later, several works including [1, 19] used the tasks of {co-view, co-read, citation, co-citation}-prediction from this bench- mark as a query-by-example retrieval setting. As Figure 2 depicts, in this setting, given a query document, the goal is to retrieve and rank the most relevant documents out of a collection. The evalu- ation dataset for each of these four tasks includes approximately 30K total papers from a held-out pool of papers, consisting of 1K query papers and a candidate set of up to 5 positive papers and 25 negative papers [5]. To make our results comparable, we follow the prior work on SciDocs to prepare the same training data [ 1]. To this aim, we take the validation set of each of tasks and use 85% of them as training and 15% of them as the validation. Thus, each query in the train set has 5 relevant documents and 25 non-relevant documents. While TILDE is trained over