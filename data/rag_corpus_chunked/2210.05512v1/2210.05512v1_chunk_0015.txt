implicit feedback, or one-class collaborative filtering (OC-CF), problems is SLIM, which makes recommendations based on a learned item-item similarity matrix. While SLIM has been shown to perform well on implicit feedback tasks, we argue that it is hindered by two limitations [...] [...] based on the observed user purchase or recommendation activities. Recently, it has been noticed that side information that describes the items can be produced from auxiliary sources and help to improve the performance of top-N recommendation systems [...] Figure 2: In the Query-by-Example retrieval setting, given a document (in its meaning as a unit of retrieval [17]) as the query ğ‘, the goal is to retrieve and rank the top-k relevant documents {ğ‘‘1, ğ‘‘ 2, ... ğ‘‘ ğ‘˜ } out of a collection of documents. We use the four QBE tasks from SciDocs [5] benchmark including {ğ‘ğ‘–ğ‘¡ğ‘’, ğ‘ğ‘œğ‘ğ‘–ğ‘¡ğ‘’, ğ‘ğ‘œğ‘Ÿğ‘’ğ‘ğ‘‘, ğ‘ğ‘œğ‘£ğ‘–ğ‘’ğ‘¤ }, each of which has its own relevance criterion [5]. three settings: a) using BERTbase as encoder, b) zero-shot utilization of TILDE and TILDEv2 models which are already fine-tuned on MSMARCO, and c) using a domain-specific pre-trained BERT as their encoder. Specifically, we use SciBERT [3] since our evaluation benchmark is from the scientific domain. 3.6 Implementation Details We run our experiments on NVIDIA RTX 3090 GPU machines with 24GB GPU memory. For BERT base, and SciBERT we use the pre- trained models available on Huggingface. All BERT-based models are trained for 5 epochs. We use the Adam optimizer [15] with a learning rate of 2 Ã— 10âˆ’5 for TILDE, and the AdamW optimizer with a learning rate of 5 Ã— 10âˆ’6 for TILDEv2. In addition, we relax the maximum document length to the maximum input length of BERT during indexing. 4 RESULTS RQ1. How effective are TILDE and TILDEv2 in query-by-example retrieval? and RQ4 To what