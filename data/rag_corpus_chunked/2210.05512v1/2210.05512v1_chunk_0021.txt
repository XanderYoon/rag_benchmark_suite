task domain is a typical approach which could result in improvement in their ranking quality; however, we leave this item as a direction to be explored in future work. RQ2. What is the effectiveness of traditional lexical matching models with varying tokenization strategies in comparison to TILDE and TILDEv2 Table 2 shows that leveraging BERT and SciBERT tokenizers results in competitive ranking quality in both probabilistic lan- guage model based retrieval and BM25 in comparison to the three traditional pre-processing setups introduced in section 3.2. Moreover, as the results of Table 1 shows, the ranking quality of BM25ğ‘†ğ‘‡ ğ‘€2 not only outperforms LM and BM25 with different traditional and BERT-based pre-processing approaches, but also it could even outperform TILDEğµğ¸ğ‘…ğ‘‡ , and TILDEv2ğµğ¸ğ‘…ğ‘‡ in most of the tasks. In fact, we do not see a large gap between BM25 compared to TILDEv2 as was shown for retrieval based on short queries in the experiments on MSMARCO and TREC DL Track benchmarks [33]. This finding is important as (1) it sheds light on the challenges of retrieval settings different from the common evaluation benchmarks including MSMARCO and the TREC DL Track; (2) raises the question how effective other contextualized term-based ranking models would be in those settings. RQ3. To what extent do TILDE and TILDEv2 encode a different relevance signal from BM25? The blue lines in Figure 3 show the ranking quality for TILDESciBERT and TILDEv2SciBERT when their scores are interpolated with the BM25 score over varying values of interpolation parameter ğ›¼ with the step of 0.1. Besides, Table 3 shows the ranking quality for the in- terpolations with the ğ›¼ that is tuned over the validation set. We can see that an optimal interpolation between the scores from BM25 and the contextualized term-based ranking models TILDE and TILDEv2 could provide significant