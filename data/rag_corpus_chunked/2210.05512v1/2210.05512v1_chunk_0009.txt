ğ‘ and ğ‘‘ are the query and the document respectively;ğ‘‘ ğ‘— is the ğ‘—th token of the document;ğ‘£ğ‘‘ ğ‘— is the term importance weight for the 3https://github.com/ielab/TILDE ğ‘—th token ofğ‘‘ , and ğ‘ (ğ‘ğ‘– ) is the count of theğ‘–-th unique token which is achieved by using the BERT tokenizer as the query encoder. In this equation, ğ‘£ğ‘‘ ğ‘— is computed using the same method as in Lin and Ma [16] in which a ğ‘…ğ¸ğ¿ğ‘ˆ function is used on the projection layer to force the model to map the token representations into a positive scalar weight: ğ‘£ğ‘‘ ğ‘— = ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘Š 1Ã—ğ‘› ğ‘ğ‘Ÿğ‘œ ğ‘— ğµğ¸ğ‘…ğ‘‡ (ğ‘‘ ğ‘— ) + ğ‘) (3) in whichğ‘‘ ğ‘— is the ğ‘—th token in documentğ‘‘ and ğ‘ is the learnable bias parameter of the projection layerğ‘Šğ‘ğ‘Ÿğ‘œ ğ‘— . Lin and Ma [16] show that using a scalar weight as term importance (uniCOIL [16]) instead of a vector representation (COIL [12]) results in a decrease in the effectiveness; however, by using query expansion, uniCOIL can achieve higher effectiveness. Following the method proposed by Zhuang and Zuccon [33] for query expansion with TILDE, we will show how TILDEv2 will act when we expand documents with TILDE. For TILDEv2, we use the implementation from the authorsâ€™ code repository.4 2.4 Cross-encoder BERT Ranker The state-of-the-art results on SciDocs is reported by Abolghasemi et al. [1] where they use a multi-task optimized cross-encoder BERT ranker [22]. The cross-encoder BERT ranker uses the concatenation of query and the document as the input to a BERT encoder. The BERT encoder is then followed by a projection layer ğ‘Šğ‘ğ‘Ÿğ‘œ ğ‘— on top of its [ğ¶ğ¿ğ‘† ] token to compute the relevance score: ğ‘  (ğ‘, ğ‘‘) = ğµğ¸ğ‘…ğ‘‡ ( [ğ¶ğ¿ğ‘† ] ğ‘ [ğ‘†ğ¸ğ‘ƒ ] ğ‘‘ [ğ‘†ğ¸ğ‘ƒ ]) [ğ¶ğ¿ğ‘† ] âˆ— ğ‘Šğ‘ğ‘Ÿğ‘œ ğ‘— (4) In