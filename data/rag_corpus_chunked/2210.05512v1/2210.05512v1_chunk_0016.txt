Adam optimizer [15] with a learning rate of 2 × 10−5 for TILDE, and the AdamW optimizer with a learning rate of 5 × 10−6 for TILDEv2. In addition, we relax the maximum document length to the maximum input length of BERT during indexing. 4 RESULTS RQ1. How effective are TILDE and TILDEv2 in query-by-example retrieval? and RQ4 To what extent does a highly tailored domain- specific pre-trained BERT model affect the effectiveness of TILDE and TILDEv2 in comparison to when we use a BERT base model? As Table 1 shows, TILDE and TILDEv2 are less effective than a cross-encoder BERT ranker in QBE retrieval despite having longer queries. This could be due to the fact that the cross-encoder BERT ranker applies all-to-all attention across tokens in both the query and the document [17] and thus, query terms and document terms are highly contextualized for the estimation of the relevance score. In addition, we see that TILDEv2BERT outperforms TILDEBERT de- spite TILDEv2 being highly prune to the vocabulary mismatch prob- lem. One hypothesis for this observation could be that in a domain- specific retrieval setup like ours, TILDEv2 with the BERTbase en- coder predicts more effective document term weights than the term weights predicted for all tokens in the BERT vocabulary by TILDE with the BERTbase encoder. In addition, using SciBERT as our domain-specific pre-trained BERT model unsurprisingly improves the ranking quality of both TILDE and TILDEv2; however, this improvement is higher be- tween TILDEBERT and TILDESciBERT than between TILDEv2BERT and TILDEv2SciBERT to an extent where TILDESciBERT even outper- forms both TILDEv2BERT and TILDEv2SciBERT. This observation could be due to the fact that the vocabulary mismatch problem caused by exact matching limits the TILDEv2 ranking quality, even if we use a highly tailored domain-specific BERT as its encoder. In this