fact, not only is it competitive, but also in some cases it could outperform TILDE and TILDEv2. This finding is important as (1) it sheds light on the challenges of retrieval settings different from the common evaluation bench- marks including MSMARCO and the TREC DL Track; (2) raises the question how effective other contextualized term-based ranking models would be in those settings. Our results indicate that QBE retrieval is structurally different from other IR settings and requires special attention for methods development. Furthermore, we investigated the impact of the interpolation between BM25 and TILDE as well as TILDEv2. By doing so, we find that a linear interpolation between the score of TILDE (TILDEv2) with that of BM25 leads to an improvement in the ranking effec- tiveness. This shows that the relevance signals from contextualized ranking models TILDE and TILDEv2 are complementary to the relevance signals from BM25. Additionally, through an analysis on the oracle interpolation between BM25 and TILDE (TILDEv2), we ICTIR ’22, July 11–12, 2022, Madrid, Spain. Amin Abolghasemi, Arian Askari, and Suzan Verberne show that more stratified approaches could benefit more from the interpolation between the scores from these models. 7 ACKNOWLEDGMENTS This work is funded by the DoSSIER project under European Union’s Horizon 2020 research and innovation program, Marie Skłodowska- Curie grant agreement No. 860721. REFERENCES [1] Amin Abolghasemi, Suzan Verberne, and Leif Azzopardi. 2022. Improving BERT- based Query-by-Document Retrieval with Multi-Task Optimization. In Advances in Information Retrieval, 44th European Conference on IR Research, ECIR 2022 . https://arxiv.org/abs/2202.00373 [2] A Askari and S Verberne. 2021. Combining lexical and neural retrieval with longformer-based summarization for effective case law retrieva. InProceedings of the second international conference on design of experimental search & information REtrieval systems. CEUR, 162–170. [3] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained