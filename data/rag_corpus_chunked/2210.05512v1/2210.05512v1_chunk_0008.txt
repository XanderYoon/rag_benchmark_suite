corresponding log probability from the likelihood distribution predicted in the output of the language modeling head. It is worth mentioning that the document likelihood can be computed in a similar way by swapping the query and document; however, we only use the query likelihood (Equation 1) in our experiments. For TILDE, we use the implementation from the authorsâ€™ code repository.3 We report results for the TILDE model with differ- ent initial checkpoints as the BERT encoder for our fine-tuning procedure. TILDEBERT uses bert-base-uncased, TILDESciBERT uses SciBERT, and TILDEMSMARCO uses a TILDE which is already fine- tuned on MSMARCO; we use TILDEMSMARCO in a zero-shot setting on our data. 2.3 Lexical Exact Matching: TILDEv2 TILDE has a drawback in which it expands each document to the size of BERT tokenizer vocabulary. To tackle this problem, the authors proposed TILDEv2. TILDEv2, which builds upon uniCOIL [16] and TILDE, follows a recent paradigm in contextualized lexical exact matching in which BERT is used to output a scalar importance weight for document tokens [ 16, 33]. As it is shown in Figure 1, in TILDEv2, the token representation is downsized into a scalar weight and the relevance score between a query and a document pair is computed by a sum over the contextualized term weights for all terms appearing in both query and document: ğ‘  (ğ‘, ğ‘‘) = âˆ‘ï¸ ğ‘ğ‘– âˆˆğ‘ ğ‘ğ‘– =ğ‘‘ ğ‘— ğ‘šğ‘ğ‘¥ (ğ‘ (ğ‘ğ‘– ) Ã— ğ‘£ğ‘‘ ğ‘— ) (2) Here, ğ‘ and ğ‘‘ are the query and the document respectively;ğ‘‘ ğ‘— is the ğ‘—th token of the document;ğ‘£ğ‘‘ ğ‘— is the term importance weight for the 3https://github.com/ielab/TILDE ğ‘—th token ofğ‘‘ , and ğ‘ (ğ‘ğ‘– ) is the count of theğ‘–-th unique token which is achieved by using the BERT tokenizer as the query encoder. In this equation, ğ‘£ğ‘‘ ğ‘— is