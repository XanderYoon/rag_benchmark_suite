follow the prior work on SciDocs to prepare the same training data [ 1]. To this aim, we take the validation set of each of tasks and use 85% of them as training and 15% of them as the validation. Thus, each query in the train set has 5 relevant documents and 25 non-relevant documents. While TILDE is trained over relevant query-document pairs [34], TILDEv2 needs triplets in the format of (query, positive document, negative document). To prepare these triplets we pick two non- relevant documents per relevant document. By doing so, we create 10 triplets out of 30 training samples for each query. It should be noted that following Cohan et al . [5] we use a concatenation of abstract and title of the papers as documents. 3.2 BERT-based Tokenization in Traditional Models In order to address RQ2, we will examine the effects of transformer- based tokenizers as text pre-processor for traditional retrieval mod- els. Doing so aligns the index vocabulary of traditional models with that of TILDE and TILDEv2, which in turn makes our compari- son more fair. Transformers use different tokenization mechanisms e.g. WordPiece [31], which result in different query and document representations compared to common word-based tokenization ap- proaches that are sometimes combined with normalization steps such as stemming and lemmatizing. Kamps et al . [14] show that using the BERT tokenizer as a pre-processor for BM25 results in a higher efficiency at the cost of a small decrease in effectiveness on the TREC 2020 Deep Learning Track [6]. QBE retrieval, however, has the challenge of long queries. In this work, investigate whether the same effect applies to a QBE retrieval setting. To this aim, we use the BERTbase tokenizer as a pre-processor for LM and BM25. In addition, we use the SciBERT tokenizer, which is