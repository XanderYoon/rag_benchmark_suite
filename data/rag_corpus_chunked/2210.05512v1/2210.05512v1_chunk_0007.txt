[32]. 2.2 Term Independent Likelihood Model: TILDE TILDE is a tokenizer-based term-based retrieval model which fol- lows a term independence assumption and formulates the likelihood of a query as follows: TILDE-QL(ğ‘|ğ‘‘) = |ğ‘ |âˆ‘ï¸ ğ‘– ğ‘™ğ‘œğ‘” (ğ‘ƒğœƒ (ğ‘ğ‘– |ğ‘‘)) (1) in which ğ‘ is the query, and ğ‘‘ is the document. As Figure 1 shows, to compute the relevance score, the text of a document ğ‘‘ is fed as the input for BERT and the log probability for each token is estimated by using a language modeling head on top of the BERT [CLS] token output. In other words, we are pre-computing the 2https://github.com/elastic/elasticsearch On the Interpolation of Contextualized Term-based Ranking with BM25 for Query-by-Example Retrieval ICTIR â€™22, July 11â€“12, 2022, Madrid, Spain. TILDE 0.1 0.20.1 0.10.0 0.30.2... ...0.20 0 10 01 01... ... BERT Tokenizer BERT CLS CLS CLS .... .... .... TILDEv2 0 10 01 01... ... BERT Tokenizer BERT CLS .... ....CLS .... CLS 0.1 ... 1.20 Figure 1: Model architectures. Left: TILDE [34]. Right: TILDEv2 [33]. Both TILDE and TILDEv2 leverage the BERT tokenizer as their query encoder. ğ‘¡ğ‘– stands for the ğ‘–th token of the document. The ğ‘‘ğ‘’ğ‘›ğ‘ ğ‘’ ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ and ğ‘ ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ have the same length as the BERT vocabulary size. term weights over the complete BERT vocabulary. During both training and inference time, the query text is tokenized by using a BERT tokenizer and the resulting token IDs are used to look up the corresponding log probability from the likelihood distribution predicted in the output of the language modeling head. It is worth mentioning that the document likelihood can be computed in a similar way by swapping the query and document; however, we only use the query likelihood (Equation 1) in our experiments. For TILDE, we use the implementation from the authorsâ€™ code repository.3