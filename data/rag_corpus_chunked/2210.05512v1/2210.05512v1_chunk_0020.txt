that using ğ‘š = 100 results in roughly 2.6 new tokens on average. These numbers beside the statistics of the tokens in SciDocs benchmark, provided in Section 3.4, indicate that ğ‘š should be tuned in order to take advantage from the document expansion with TILDE in QBE retrieval setting. Finally, we see that the zero-shot utilization of TILDEMSMARCO and TILDEv2MSMARCO does not show superior performance over the fine-tuned TILDE and TILDEv2 with both BERT and SciBERT encoders. It should be noted that taking models which are already fine-tuned on general domain (like TILDEMSMARCO and TILDEv2MSMARCO) and further On the Interpolation of Contextualized Term-based Ranking with BM25 for Query-by-Example Retrieval ICTIR â€™22, July 11â€“12, 2022, Madrid, Spain. Table 3: Results for non-oracle interpolation (the interpolation parameter ğ›¼ is optimized on the validation set) between BM25ğ‘†ğ‘‡ ğ‘€2, TILDE ğ‘†ğ‘ğ‘–ğµğ¸ğ‘…ğ‘‡ , and TILDEv2 ğ‘†ğ‘ğ‘–ğµğ¸ğ‘…ğ‘‡ . Statistical significance improvements are according to paired t-test (p<0.05) with Bonferroni correction for multiple testing. Rows ğ‘, ğ‘, and ğ‘ are included from Table 1 for ease of comparison. Model Co-view Co-read Co-cite Cite MAP nDCG MAP nDCG MAP nDCG MAP nDCG a) BM25STM2 80.8%ğ‘ 0.9032ğ‘ 81.31% 0.9112 81.53% 0.9171 79.74% 0.9085 b) TILDESciBERT 82.6%ğ‘ğ‘ 0.9115ğ‘ 85.03%ğ‘ğ‘ğ‘’ 0.9256ğ‘ğ‘ 86.38%ğ‘ğ‘ğ‘’ 0.9375ğ‘ğ‘ğ‘’ 87.74%ğ‘ğ‘ğ‘’ 0.9431ğ‘ğ‘ğ‘’ c) TILDEv2SciBERT 79.59% 0.8961 80.74% 0.9080 80.94% 0.9123 84.18%ğ‘ 0.9314ğ‘ d) BM25STM2+ TILDESciBERT 85.29%ğ‘ğ‘ğ‘ğ‘’ 0.9214ğ‘ğ‘ğ‘ğ‘’ 86.52%ğ‘ğ‘ğ‘ğ‘’ 0.9395ğ‘ğ‘ğ‘ğ‘’ 88.32%ğ‘ğ‘ğ‘ğ‘’ 0.9494ğ‘ğ‘ğ‘ğ‘’ 88.46%ğ‘ğ‘ğ‘ğ‘’ 0.9496ğ‘ğ‘ğ‘ğ‘’ e) BM25STM2+ TILDEv2SciBERT 81.56%ğ‘ğ‘ 0.9032ğ‘ 82.63%ğ‘ğ‘ 0.9183ğ‘ğ‘ 83.06%ğ‘ğ‘ 0.9242ğ‘ğ‘ 84.18%ğ‘ 0.9318ğ‘ fine-tuning them on the task domain is a typical approach which could result in improvement in their ranking quality; however, we leave this item as a direction to be explored in future work. RQ2. What is the effectiveness of traditional lexical matching models with varying tokenization strategies in comparison to TILDE and TILDEv2 Table 2 shows that leveraging BERT and SciBERT tokenizers results in