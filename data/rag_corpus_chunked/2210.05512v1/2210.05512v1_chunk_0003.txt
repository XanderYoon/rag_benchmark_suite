time [33]. While the effectiveness of these models is evaluated on tasks and benchmarks where we have short queries, e.g., MSMARCO Passage Ranking [21] and the TREC DL Track [7], in this paper, we evaluate them in the aforementioned QBE retrieval setting where queries are much longer than common keyword queries. In this regard, we address the following research questions: RQ1 How effective are TILDE and TILDEv2 in query-by-example retrieval? 1Throughout this paper, we use the term “document” to refer to a unit of retrieval [17] arXiv:2210.05512v1 [cs.IR] 11 Oct 2022 ICTIR ’22, July 11–12, 2022, Madrid, Spain. Amin Abolghasemi, Arian Askari, and Suzan Verberne A specific direction in answering RQ1 is to investigate the rank- ing quality of TILDE and TILDEv2 in comparison with the effective cross-encoder BERT ranker [1, 22], which is described in section 2.4. We are interested in this direction for two reasons. First, the cross-encoder BERT ranker exhibits quadratic complexity in both space and time with respect to the input length [17] and this is ag- gravated in QBE where we have long queries. TILDE and TILDEv2, however, do not need any BERT inference at query time. Second, due to the maximum input length of BERT, cross-encoder BERT ranker, which uses the concatenation of the query and the docu- ment, might not cover the whole query and document tokens in a QBE setting, whereas in TILDE and TILDEv2, the query can be of any length and documents are covered up to the maximum length of BERT. Additionally, since TILDEv2 pre-computes the term weights only for those tokens existing in the documents, one risk is that it might aggravate the vocabulary mismatch problem. A typical approach to address this issue is to use document expansion methods. Zhuang and Zuccon [33] use TILDE as their document expansion