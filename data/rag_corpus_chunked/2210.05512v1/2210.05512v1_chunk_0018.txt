improvements are according to paired t- test (p<0.05) with Bonferroni correction for multiple testing. Rows ð‘Ž and ð‘ are included from Table 2 for ease of comparison. Model Co-view Co-read Co-cite Cite MAP nDCG MAP nDCG MAP nDCG MAP nDCG a) BM25STM2 80.8%ð‘ð‘ð‘‘ ð‘“âˆ’ð‘— 0.9032ð‘ð‘‘ ð‘“ ð‘” ð‘— 81.31%ð‘‘ ð‘“ ð‘” 0.9112ð‘ð‘‘ð‘” 81.53%ð‘ð‘ð‘‘ ð‘“ ð‘” 0.9171ð‘ð‘‘ ð‘“ ð‘” 79.74%ð‘ð‘‘ð‘” 0.9085ð‘‘ð‘” b) BM25SciBERT-Token 80.08%ð‘ð‘‘ ð‘“ ð‘” 0.8992ð‘ð‘‘ð‘” 80.97%ð‘‘ ð‘“ ð‘” 0.9105ð‘‘ð‘” 80.83%ð‘ð‘‘ ð‘“ ð‘” 0.9141ð‘ð‘‘ð‘” 79.03%ð‘‘ð‘” 0.9051ð‘‘ð‘” c) TILDEBERT 76.74%ð‘‘ 0.8761ð‘‘ 80.57%ð‘‘ð‘” 0.8983ð‘‘ 79.7%ð‘‘ð‘” 0.8999ð‘‘ 82.15%ð‘Žð‘ð‘‘ð‘” 0.914ð‘‘ð‘” d) TILDEMSMARCO 68.22% 0.8261 66.75% 0.8206 65.21% 0.8145 65.29% 0.8186 e) TILDESciBERT 82.6%ð‘Žâˆ’ð‘‘ ð‘“âˆ’ð‘— 0.9115ð‘ð‘ð‘‘ ð‘“âˆ’ð‘— 85.03%ð‘Žâˆ’ð‘‘ ð‘“âˆ’ð‘— 0.9256ð‘Žâˆ’ð‘‘ ð‘“âˆ’ð‘— 86.38%ð‘Žâˆ’ð‘‘ ð‘“âˆ’ð‘— 0.9375ð‘Žâˆ’ð‘‘ ð‘“âˆ’ð‘— 87.74%ð‘Žâˆ’ð‘‘ ð‘“âˆ’ð‘— 0.9431ð‘Žâˆ’ð‘‘ ð‘“ ð‘”â„Ž ð‘— f) TILDEv2BERT 79.17%ð‘ð‘‘ð‘” 0.8948ð‘ð‘‘ 80.16%ð‘‘ð‘” 0.9051ð‘‘ð‘” 80.22%ð‘‘ð‘” 0.9103ð‘‘ð‘” 82.54%ð‘Žð‘ð‘‘ð‘” 0.9230ð‘Žð‘ð‘‘ð‘” g) TILDEv2MSMARCO 77.84%ð‘ð‘‘ 0.8876ð‘‘ 78.53%ð‘‘ 0.8959ð‘‘ 78.17%ð‘‘ 0.9006ð‘‘ 75.62%ð‘‘ 0.8866ð‘‘ h) TILDEv2SciBERT 79.59%ð‘ð‘‘ð‘” 0.8961ð‘ð‘‘ð‘” 80.74%ð‘‘ð‘” 0.9080ð‘‘ð‘” 80.94%ð‘ð‘‘ ð‘“ ð‘” 0.9123ð‘‘ð‘” 84.18%ð‘Žâˆ’ð‘‘ ð‘“ ð‘” 0.9314ð‘Žâˆ’ð‘‘ ð‘“ ð‘” TILDEv2SciBERT i) expansion w/ m=200 80.06%ð‘ð‘‘ ð‘“ ð‘” ð‘— 0.8985ð‘ð‘‘ð‘” 81.29%ð‘‘ ð‘“ ð‘”â„Ž 0.9096ð‘‘ð‘” 81.62%ð‘ð‘‘ ð‘“ ð‘” 0.9153ð‘ð‘‘ð‘” 86.42%ð‘Žâˆ’ð‘‘ ð‘“ ð‘”â„Ž ð‘— 0.9412ð‘Žâˆ’ð‘‘ ð‘“ ð‘”â„Ž ð‘— j) expansion w/ m=300 79.38%ð‘ð‘‘ð‘” 0.8942ð‘ð‘‘ 81.17%ð‘‘ ð‘“ ð‘” 0.9099ð‘‘ð‘” 81.93%ð‘ð‘ð‘‘ ð‘“ ð‘”â„Ž 0.9165ð‘ð‘‘ð‘” 84.4%ð‘Žâˆ’ð‘‘ ð‘“ ð‘” 0.9319ð‘Žâˆ’ð‘‘ ð‘“ ð‘” k) Cross-EncoderSciBERT 85.2%ð‘Žâˆ’ð‘— 0.925ð‘Žâˆ’ð‘— 87.5%ð‘Žâˆ’ð‘— 0.940ð‘Žâˆ’ð‘— 89.7%ð‘Žâˆ’ð‘— 0.955ð‘Žâˆ’ð‘— 94.0%ð‘Žâˆ’ð‘— 0.975ð‘Žâˆ’ð‘— l) Cross-EncoderMTFT-SciBERT 86.2%ð‘Žâˆ’ð‘— 0.930ð‘Žâˆ’ð‘— 87.7%ð‘Žâˆ’ð‘— 0.940ð‘Žâˆ’ð‘— 91.0%ð‘Žâˆ’ð‘— 0.961ð‘Žâˆ’ð‘— 94.2%ð‘Žâˆ’ð‘— 0.976ð‘Žâˆ’ð‘— Table 2: Ranking quality of traditional retrieval models on the four SciDocs benchmark tasks with different tokenization approaches. SA, STM1, STM2, BERT-Token, and SciBERT-Token refer to the pre-processing setting as described in section 3.2. Statistical significance improvements are according to paired t-test (p<0.05) with Bonferroni correction for multiple testing. Model Co-view Co-read Co-cite Cite MAP nDCG MAP nDCG MAP nDCG MAP nDCG a) LMSA 74.78% 0.8724 74.32%ð‘ 0.8750 74.64% 0.8812 71.30% 0.8653 b) LMSTM1 74.82% 0.8737 73.51% 0.8694 74.60% 0.8810 70.98% 0.8636