a small decrease in effectiveness on the TREC 2020 Deep Learning Track [6]. QBE retrieval, however, has the challenge of long queries. In this work, investigate whether the same effect applies to a QBE retrieval setting. To this aim, we use the BERTbase tokenizer as a pre-processor for LM and BM25. In addition, we use the SciBERT tokenizer, which is a domain- specific BERT tokenizer, to find out if a domain-specific tokenizer would have a different effect in comparison to the BERTbase tok- enizer. We use three different pre-processing setups in Elasticsearch to compare with our two transformer-based tokenizers: â€¢ Elasticsearch Standard Analyzer (SA) â€¢ Lowercase token filter, Porter Stemmer, Whitespace tok- enizer (STM1) â€¢ Lowercase token filter, Porter Stemmer, Standard tokenizer (STM2) In Table 2, models corresponding to these setups respectively have SA, STM1, and STM2 as their subscript. BERT-Token and SciBERT-Token as subscripts stand for using BERT and SciBERT tokenizers as the text pre-processors. 3.3 Interpolation between BM25 and TILDE (TILDEv2) scores To answer RQ3 about the difference between BM25 and TILDE (as well as TILDEv2) in terms of their relevance signals, following Wang et al. [30], we evaluate the effect of the interpolation be- tween the relevance scores from BM25 and from the contextualized term-based ranking models TILDE and TILDEv2. To this aim the interpolated score is computed as following: ğ‘  (ğ‘, ğ‘‘) = ğ›¼ âˆ— ğ‘ ğµğ‘€ 25 (ğ‘, ğ‘‘) + ( 1 âˆ’ ğ›¼) âˆ— ğ‘ ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘ğ‘™ğ‘–ğ‘§ğ‘’ğ‘‘ (ğ‘, ğ‘‘) (5) Here, ğ‘ ğµğ‘€ 25 stands for the BM25 score for query ğ‘, and document ğ‘‘, and ğ‘ ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘¢ğ‘ğ‘™ğ‘–ğ‘§ğ‘’ğ‘‘ refers to the relevance score from TILDE or TILDEv2. Also, ğ›¼ is the hyperparameter that controls the impact of the scores from BM25 and TILDE (or TILDEv2). Prior to the interpo- lation both of the relevance scores are normalized using ğ‘§-scaling