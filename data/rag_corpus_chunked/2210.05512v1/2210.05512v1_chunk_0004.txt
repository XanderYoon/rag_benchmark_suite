and documents are covered up to the maximum length of BERT. Additionally, since TILDEv2 pre-computes the term weights only for those tokens existing in the documents, one risk is that it might aggravate the vocabulary mismatch problem. A typical approach to address this issue is to use document expansion methods. Zhuang and Zuccon [33] use TILDE as their document expansion model for TILDEv2. We adopt that approach for our task and further investigate the impact of token-based document expansion with TILDE on the ranking quality of TILDEv2 in a QBE retrieval setting. Apart from comparing TILDE and TILDEv2 to the cross-encoder BERT ranker, we also make a comparison to traditional lexical matching models (BM25 and Probabilistic Language models), which have been shown as strong baselines on QBE tasks in prior work [2, 28]: RQ2 What is the effectiveness of traditional lexical matching mod- els with varying tokenization strategies in comparison to TILDE and TILDEv2? To answer RQ2 we will investigate the effect of using the BERT tokenizer [8] as pre-processing for traditional term-based retrieval models. By doing so, we are aligning the index vocabulary of tradi- tional models with that of TILDE and TILDEv2, which could make our comparison more fair. We will see in the Section 4 that BM25 shows a competitive ranking quality in comparison to TILDE and TILDEv2 in our QBE benchmark. Because of the similar quality on average, we are in- terested to see if the relevance signals of TILDE and TILDEv2 are different from that of BM25, to find out if the methods are comple- mentary to each other. To this aim, we will investigate the following research question: RQ3 To what extent do TILDE and TILDEv2 encode a different relevance signal from BM25? To address the question above, as it is described in details