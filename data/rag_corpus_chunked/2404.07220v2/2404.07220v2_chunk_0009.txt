more details) B. Summary of Retriever Evaluation We evaluated the retrieval accuracy using our approach, quantified by Top-k metrics where k ∈ {5, 10, 20}, across NQ, TREC-COVID, SQUAD, and CoQA datasets. This synopsis demonstrates the capability of our Blended Retrieval method- ology within diverse informational contexts. Key observations are • Enhanced retrieval accuracy is exhibited in all datasets except for CoQA [11]. This enhancement is attributable to the capability of our hybrid queries to effectively utilize available metadata to source the most pertinent results. • Implementing dense vector-based (KNN) semantic search results in a marked improvement over keyword-based search approaches. • Employing semantic search-based hybrid queries realizes better retrieval precision compared to all conventional keyword-based or vector-based searches. • Furthermore, it is discernible that the Sparse Encoder- based semantic search, when amalgamated with the ’Best Fields’ hybrid query, often provides superior results than any other method. V. RAG E XPERIMENTATION From the retriever evaluation experiments, we know the best retriever, i.e., the best combination of indices + query. In this section, we extend this knowledge to evaluate the RAG pipeline. To avoid the effect of LLM size or type, we perform all experiments using FLAN-T5-XXL. Fig. 8: Top-5 Retrieval Accuracy across Datasets A. RAG Evaluation on the SqUAD Dataset SqUAD is a commonly bench-marked dataset for RAG sys- tems or Generative Q&A using LLMs. Our study juxtaposes three variations of the RAG pipeline from prior work using the evaluation metrics of Exact Match (EM) and F1 scores to gauge the accuracy of answer generation, as well as Top-5 and Top-10 for retrieval accuracy. • RAG-original [12]: This variant, a model fine-tuned on the Natural Questions dataset, has been appraised without domain-specific adaptation. • RAG-end2end [12]: As an extension of RAG-original, this model undergoes additional fine-tuning, tailored for domain adaptation