metrics of Exact Match (EM) and F1 scores to gauge the accuracy of answer generation, as well as Top-5 and Top-10 for retrieval accuracy. • RAG-original [12]: This variant, a model fine-tuned on the Natural Questions dataset, has been appraised without domain-specific adaptation. • RAG-end2end [12]: As an extension of RAG-original, this model undergoes additional fine-tuning, tailored for domain adaptation to the SQuAD. • Blended RAG: Distinctively, our Blended RAG variant has not undergone training on the SQuAD dataset or any related corpora. It harnesses an optimized amalgamation of field selections and hybrid query formulations with semantic indices to feed LLMs to render the most precise responses possible. Consequently, as shown in Table IV , our Blended RAG showcases enhanced performance for Generative Q&A with F1 scores higher by 50%, even without dataset-specific fine- tuning. This characteristic is particularly advantageous for large enterprise datasets, where fine-tuning may be impractical or unfeasible, underscoring this research’s principal applica- tion. B. RAG Evaluation on the NQ Dataset Natual Questions (NQ) is another commonly studied dataset for RAG. The Blended RAG pipeline, utilizing zero-shot learn- ing, was evaluated to ascertain its efficacy against other non- fine-tuned models. The assessment focused on the following metrics: Exact Match (EM), F1 Score, and retrieval accuracy (Top-5 and Top-20) in Table V . Blended RAG (Zero-shot): Demonstrated superior perfor- mance with an EM of 42.63, improving the prior benchmark by 35%. TABLE III: Blended Retriever Performance SqUAD Dataset SqUAD BM25+MQ BM25+BF KNN+MQ KNN+BF SPARSE ENCODER+MQ SPARSE ENCODER+BF Top-5 91.5 91.52 94.86 94.89 90.7 90.7 Top-10 94.43 94.49 97.43 97.43 94.13 94.16 Top-20 96.3 96.36 98.57 98.58 96.49 96.52 TABLE IV: Evaluation of the RAG Pipeline on the SquAD Dataset Model/Pipeline EM F1 Top-5 Top-20 RAG-original 28.12 39.42 59.64 72.38 RAG-end2end 40.02 52.63 75.79 85.57 Blended RAG 57.63 68.4