SqUAD BM25+MQ BM25+BF KNN+MQ KNN+BF SPARSE ENCODER+MQ SPARSE ENCODER+BF Top-5 91.5 91.52 94.86 94.89 90.7 90.7 Top-10 94.43 94.49 97.43 97.43 94.13 94.16 Top-20 96.3 96.36 98.57 98.58 96.49 96.52 TABLE IV: Evaluation of the RAG Pipeline on the SquAD Dataset Model/Pipeline EM F1 Top-5 Top-20 RAG-original 28.12 39.42 59.64 72.38 RAG-end2end 40.02 52.63 75.79 85.57 Blended RAG 57.63 68.4 94.89 98.58 TABLE V: Evaluation of the RAG pipeline on the NQ dataset Model/Pipeline EM F1 Top-5 Top-20 GLaM (Oneshot) [13] 26.3 GLaM (Zeroshot) [13] 24.7 PaLM540B (Oneshot) [14] 29.3 Blended RAG (Zero- shot) 42.63 53.96 88.22 88.88 VI. D ISCUSSION While RAG is a commonly used approach in the industry, we realized during the course of this study that various challenges still exist, like there are no standard datasets on which both R (Retriever) and RAG benchmarks are available. Retriever is often studied as a separate problem in the IR domain, while RAG is studied in the LLM domain. We thus attempted to bring synergy between the two domains with this work. In this section, we share some learning on limitations and appropriate use of this method. A. Trade-off between Sparse and Dense V ector Indices The HotPotQA corpus presents substantial computational challenges with 5M documents, generating a dense vector index to an approximate size of 50GB, a factor that signif- icantly hampers processing efficiency. Dense vector indexing, characterized by its rapid indexing capability, is offset by a relatively sluggish querying performance. Conversely, sparse vector indexing, despite its slower indexing process, offers expeditious querying advantages. Furthermore, a stark contrast in storage requirements is observed; for instance, the sparse vector index of the HotPotQA corpus occupied a mere 10.5GB as opposed to the 50GB required for the dense vector equiv- alent. In such cases, we recommend sparse encoder indexes. Furthermore,