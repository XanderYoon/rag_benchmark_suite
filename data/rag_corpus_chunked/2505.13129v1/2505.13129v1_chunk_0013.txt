cosine similarity and Euclidean distance Table 2 BERT-based retriever Metric k = 10 k = 20 k = 30 k = 40 k = 50 k = 0 (Baseline) Mean CS 0.9265 0.9263 0.9254 0.9259 0.9334 0.9338 Variance CS 0.0045 0.0036 0.0064 0.0050 0.0017 0.0022 Trimmed Mean CS 0.9435 0.9415 0.9437 0.9438 0.9425 0.946 Mean ED 5.2026 5.2436 5.163 5.2477 5.0418 5.0682 Variance ED 4.8764 4.3763 6.2543 5.1349 3.2301 3.2026 Trimmed Mean ED 4.6916 4.7734 4.6247 4.7097 4.7065 4.6396 Table 3 SPLADE-based retriever Metric k = 10 k = 20 k = 30 k = 40 k = 50 k = 0 (Baseline) Mean CS 0.9360 0.9189 0.9292 0.9211 0.9116 0.9338 Variance CS 0.0016 0.0049 0.0042 0.0068 0.0100 0.0022 Trimmed Mean CS 0.9453 0.9370 0.9461 0.9423 0.9408 0.946 Mean ED 4.9842 5.4851 5.1187 5.3317 5.5794 5.0682 Variance ED 2.7181 5.3273 4.6761 6.3177 7.9934 3.2026 Trimmed Mean ED 4.6473 4.9433 4.5949 4.7300 4.8053 4.6396 for BERT-based retrieval, as illustrated in Fig. 6 and Fig. 7, included in the appendix, suggests a more consistent performance across different samples. When looking at the performance with 10% of the worst samples removed, we observe that the model with ğ‘˜ = 50 is no longer the best-performing one. This indicates that when disregarding consistency as defined by the ability to limit outliers, a slightly lower value for ğ‘˜ might provide better performance. The SPLADE-based retriever produced mixed results, as shown in Table 3, showing relatively high cosine similarity at lower values of ğ‘˜ = 10 but experiencing a strong decline in performance as ğ‘˜ increased. Notably, SPLADE at ğ‘˜ = 10 outperformed all other retrieval approaches, including the baseline and the best-performing BERT-based retriever (Fig. 3), suggesting that sparse-vector retrieval models may be particularly beneficial when selecting a limited number of relevant chunks. We hypothesize