high cosine similarity at lower values of ğ‘˜ = 10 but experiencing a strong decline in performance as ğ‘˜ increased. Notably, SPLADE at ğ‘˜ = 10 outperformed all other retrieval approaches, including the baseline and the best-performing BERT-based retriever (Fig. 3), suggesting that sparse-vector retrieval models may be particularly beneficial when selecting a limited number of relevant chunks. We hypothesize that SPLADE can leverage exact matching and synonyms to find all relevant chunks quickly. The performance then degrades when we increase k as little to no additional relevant chunks are included. Similar to BERT-based retrieval, our SPLADE-based has a stronger performance for ğ‘˜ = 30 when disregarding outliers. Once again this observation is in part due to the advantage of higher consistency across outputs for ğ‘˜ = 10, but suggests that a different value for ğ‘˜ might be more beneficial when removing outliers. While the absolute improvement in mean cosine similarity appears numerically small, the difference can be semantically meaningful for domain-specific tasks like this one. 3.2. Impact of k We also analyzed how varying the number of retrieved chunks as determined by the parameterğ‘˜ affects model performance. Interestingly, increasing ğ‘˜ does not always lead to improved results. For the BM25 and SPLADE retrievers, performance fluctuated as ğ‘˜ increased, suggesting that excessive retrieval may introduce irrelevant or redundant information, as shown in Table 1 and Table 3. We speculate that excessive retrieval (higher ğ‘˜) can introduce noise, leading to lower similarity scores. Conversely, BERT-based retrieval demonstrated relatively stable performance across different k values, with its best performance occurring at ğ‘˜ = 50, as seen in Table 2. Our results suggest that optimizing the retrieval step by carefully selecting an appropriate ğ‘˜ is crucial to maximizing the benefits of retrieval-augmented generation in the OCL domain, and blindly increasing ğ‘˜ can degrade