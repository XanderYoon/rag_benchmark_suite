relying solely on a model’s pre-trained knowledge, RAG retrieves relevant information from an external knowledge base and incorporates it into the model’s input before generat- ing the final output. This method has shown promise in improving accuracy, reducing hallucinations [15], and ensuring that generated content aligns with the domain [ 13]. Since OCL rules are tightly coupled with the underlying meta-model structure, a standard LLM may not have sufficient context to generate the correct rule. RAG allows us to retrieve relevant meta-model elements (e.g., classes, associations, enumerations) from a retrievable knowledge base and include them in the input prompt. The desired result is that RAG helps the LLM generate rules that adhere to proper OCL syntax and semantics. However, optimizing retrieval strategies for OCL generation has not been extensively studied, particularly in the context of balancing retrieval efficiency and generation accuracy. While fine-tuning can adapt LLMs to specific tasks, such as OCL rule generation, it is resource- intensive and may not generalize well across new or unseen meta-models. Recent research has increas- ingly highlighted the advantages of Retrieval-Augmented Generation over fine-tuning for knowledge injection in large language models. RAG consistently outperforms fine-tuning across multiple datasets, even when dealing with previously known and entirely new knowledge. Fine-tuning struggles with learning new factual information and requires extensive training data [16]. Similarly, another study [17] found that RAG is more effective in handling less popular or low-frequency knowledge, including domain-specific knowledge. The study emphasizes that while smaller language models may still benefit from fine-tuning, larger models gain little additional advantage. Additionally, fine-tuning remains resource-intensive. Combining RAG with fine-tuning can lead to further improvements to performance in specialized domains as shown in [18]. Current research in OCL rule generation has focused on other approaches, such as fine-tuning [9]. This study uses RAG but does