Similarity Metric Jaccard k=1 Jaccard k=3 Jaccard k=5 Mean CS 0.9251 0.9066 0.8986 Mean ED 5.3356 5.8805 6.2185 Table 6 PathOCL performance with Cosine Similarity Metric Cosine k=1 Cosine k=3 Cosine k=5 Mean CS 0.9204 0.8957 0.9030 Mean ED 5.4709 6.1700 6.0020 extremely large meta-models. Given the length of some of the meta-models, having upwards of 100 classes and 300 associations makes it almost impossible to perform the method as outlined, as it uses a brute-force approach to compute all simple paths in the graph, which has a runtime complexity of at least ğ‘‚(ğ‘›!). We thus decided to do the opposite as mentioned in the methodology section and filter our dataset by only considering samples where the number of chunks in the meta-model is less than 100 and randomly sampled 72 instances. We evaluated PathOCL under both Jaccard and cosine similarity configurations with different values of ğ‘˜ = (1 , 3, 5). As shown in Table 4, our SPLADE-based method at ğ‘˜ = 10 outperformed PathOCL across both cosine similarity and Euclidean distance. Additional comparative PathOCL results for varying ğ‘˜ values and similarity measures are presented in Tables 5 and 6. Our results indicate that sparse-vector retrieval provides a significant increase in performance against the no retrieval baseline. Counterintuitively the PathOCL approach is outperformed by the baseline as seen in Table 4. These results suggest that semantic retrieval strategies scale better on complex datasets. 3.4. Limitations and Future Work While our study demonstrates the possible benefits of retrieval-based approaches, it is not without limitations. First, our evaluation was conducted on a relatively small filtered subset of the dataset, which may not generalize to all OCL rule generation scenarios. While these metrics provide an initial assessment of textual similarity and closeness, they do not capture functional correctness. To complement our automated