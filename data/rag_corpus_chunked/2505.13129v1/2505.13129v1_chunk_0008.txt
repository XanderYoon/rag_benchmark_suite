by its name, we have a collection of chunks representing the whole meta-model. To evaluate retrieval impact, we filtered the dataset such that only hard samples were considered. Hard samples were defined as instances where the number of chunks for the meta-model exceeded 50, making retrieval non-trivial and requiring a retriever to select a subset of chunks to use in the context of the prompt. From that filtered dataset we then randomly sampled 72 instances for our evaluation, which corresponds to one-fifth of the size of the original dataset. 1This data set can be found at https://huggingface.co/datasets/fpan/text-to-ocl-from-ecore. Listing 1: Prompt used in the pipeline You are given a meta-model with information about classes, associations and their attributes. You are also given a natural language specification. Your task is to generate an OCL (Object Constraint Language) constraint for this specification and based on the meta-model. Do not provide any explanations or additional text. The meta-model information is: {retrieved chunks} The natural language specification is: {specification} 2.4. Retrieval To enhance the accuracy and relevance of generated responses, a RAG pipeline was integrated. We built the external knowledge base from the textual PlantUML provided in the data set as described before. The RAG pipeline retrieves relevant chunks from the knowledge base before passing them as context to the language model based on the natural language specification. We applied two filtering conditions for relevance: (a) chunks must belong to the same meta-model as the input sample, and (b) chunks must score higher than others based on similarity. Both conditions must be satisfied to be selected. Different retrieval approaches were evaluated, including lexical-based approaches in BM25-based retrieval, and transformer-based retrieval models based on dense and sparse vectors such as BERT and SPLADE. For all retrieval models, we evaluated them using top-k retrieval, where the top-k