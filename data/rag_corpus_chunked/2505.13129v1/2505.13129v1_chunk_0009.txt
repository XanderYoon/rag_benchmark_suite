input sample, and (b) chunks must score higher than others based on similarity. Both conditions must be satisfied to be selected. Different retrieval approaches were evaluated, including lexical-based approaches in BM25-based retrieval, and transformer-based retrieval models based on dense and sparse vectors such as BERT and SPLADE. For all retrieval models, we evaluated them using top-k retrieval, where the top-k chunks regarding the retrieval score with the natural language specification were given as context to the LLM. For each retrieval model, we evaluated them with ùëò set to 10, 20, 30, 40, and 50. We also evaluated the intrinsic performance of the LLM regarding OCL rule generation using no retrieval. For our BM25-based retriever, we tokenized the natural language specification and used the result as our query for the BM25 algorithm. For the transformer-based, approaches we compared the embeddings between the natural language specification and the chunks of the meta-model and selected the top-k most similar chunks using cosine similarity. We used the cosine similarity implementation from the scikit-learn library. 2.5. Generation To generate the output we used a prompt that was slightly adapted from [ 9]. We added "Do not provide any explanations or additional text. " to discourage the LLM from outputting lengthy responses that negatively impact its performance regarding our automated metrics. The final prompt template is shown in Listing 1 2.6. Evaluation Various configurations of the RAG pipeline were tested to assess their impact on response quality. This included differing the parameters such as the number of retrieved chunks, and the embedding model selection. The evaluation was conducted using automated quantitative metrics comparing the generated model output to the actual OCL rule as given in the data set. We use cosine similarity and Euclidean distance as implemented by the scikit-learn library and based on BERT embeddings