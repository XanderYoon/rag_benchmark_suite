respectively, the y-axis is inverted when displaying Euclidean distances. We consider variances close to 0 as more desirable, as they represent output and performance consistency. We also examined the performance when disregarding the worst 10% of generated samples to determine the generation output quality without extreme outliers such as hallucinations or outputs disregarding the instruction not to explain the answer. Trimmed mean refers to the mean calculated after removing the worst 10% of samples based on similarity score. The results are rounded to the 4th decimal place to improve readability. We abbreviate Cosine Similarity with CS and Euclidean Distance with ED. Our results highlight the impact of different retrieval approaches on the performance of the RAG pipeline for OCL rule generation. The baseline results as seen in Table 1, where no retrieval was applied (k=0), indicate that the language model alone achieves a relatively high cosine similarity (0.9338) but still leaves room for improvement through the integration of retrieval strategies. This result indicates that the intrinsic knowledge in the domain of OCL rules or at least the ability to generate similar rules gives us a strong baseline in regards to our semantic similarity metrics. 3.1. Effectiveness of Different Retrieval Approaches We evaluated BM25 as a lexical retriever using cosine similarity and Euclidean distance to measure performance. The results are shown in Table 1 and compared with the no-retrieval baseline. Table 2 presents the results for BERT-based retrieval, which uses dense semantic embeddings for chunk selection. SPLADE-based retrieval results are shown in Table 3. SPLADE uses sparse semantic representations, optimized for balancing lexical precision with semantic flexibility. Comparing the retrieval methods, the BM25-based retriever (Table 1) exhibited a decline in perfor- mance compared to the baseline, particularly at higher values of ùëò, suggesting that lexical retrieval alone is insufficient for effective