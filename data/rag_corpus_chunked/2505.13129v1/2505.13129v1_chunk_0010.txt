quality. This included differing the parameters such as the number of retrieved chunks, and the embedding model selection. The evaluation was conducted using automated quantitative metrics comparing the generated model output to the actual OCL rule as given in the data set. We use cosine similarity and Euclidean distance as implemented by the scikit-learn library and based on BERT embeddings as our evaluation metrics. The choice of the BERT embedding model is due to its open-source nature. The proposed evaluation methodology provides an efficient and scalable way to measure the output quality, allowing us to compare a large number of retriever configurations against each other. Table 1 BM25-based retriever Metric k = 10 k = 20 k = 30 k = 40 k = 50 k = 0 (Baseline) Mean CS 0.9231 0.9208 0.9292 0.9212 0.9133 0.9338 Variance CS 0.0028 0.0023 0.0021 0.0042 0.0064 0.0022 Trimmed Mean CS 0.9366 0.9321 0.9403 0.9364 0.9348 0.946 Mean ED 5.434 5.5972 5.2179 5.4504 5.6474 5.0682 Variance ED 3.5722 3.2460 3.2735 4.6489 6.4171 3.2026 Trimmed Mean ED 5.0244 5.2199 4.8489 5.0026 5.0397 4.6396 3. Results We will first present the results for each retrieval approach and then compare the results across different retrieval approaches. The results are based on the random subset of the filtered data set by Pan et al. [9]. Given that the best possible cosine similarity and the best possible Euclidean distance to the original OCL rule are 1 and 0 respectively, the y-axis is inverted when displaying Euclidean distances. We consider variances close to 0 as more desirable, as they represent output and performance consistency. We also examined the performance when disregarding the worst 10% of generated samples to determine the generation output quality without extreme outliers such as hallucinations or outputs disregarding the instruction not to explain the answer.