/u1D453Hybrid ( /u1D443, { /u1D445/u1D456} /u1D458 /u1D456=1, {( /u1D443 /u1D456, /u1D444 /u1D456)} /u1D45A /u1D456=1) Here, /u1D443 is the input passage, { /u1D445/u1D456} /u1D458 /u1D456=1 are the retrieved docu- ments, and {( /u1D443 /u1D456, /u1D444 /u1D456)} /u1D45A /u1D456=1 are the few-shot examples used to guide the question generation process. 6 EXPERIMENTAL SETUP Our experiments evaluate the eﬀectiveness of the In-Context Learn- ing (ICL), Retrieval-Augmented Generation (RAG), and Hybrid Model for Automatic Question Generation (AQG). We aim to assess an d compare these models’ performance using a variety of automa ted and human evaluation metrics. 6.1 Baseline Models Following [10, 27], we ﬁne-tune the best-performing models(based on automated evaluation), such as the T5-large and BART-lar ge architectures, on the EduProbe training dataset [27]. The T 5-large model, implemented from the Hugging Face Transformers library3, uses a sequence-to-sequence framework with attention mechanisms to align passages with questions, providing a robust baseli ne for comparison. Similarly, the BART-large model, also from Hug ging Face, employs a transformer encoder-decoder structure to g ener- ate questions from passages, serving as another strong base line for our evaluation. 6.2 In-Context Learning (ICL) We implemented ICL using GPT-44, providing it with diﬀerent few- shot settings (/u1D458 = 3, 5, 7). In this setup, the model uses a set of exam- ple passage-question pairs to generate questions for new pa ssages. We tested various numbers of examples to determine the optim al few-shot settings for generating relevant and coherent que stions. 6.3 Retrieval-Augmented Generation (RAG) For RAG, we utilized BART-large 5 as the backbone model and in- tegrated a retrieval module with FAISS for eﬃcient document re- trieval. For each input passage, the retrieval module ident iﬁes and fetches the top /u1D458 = 5 relevant documents from a large corpus of educational material. These documents are concatenated wi th