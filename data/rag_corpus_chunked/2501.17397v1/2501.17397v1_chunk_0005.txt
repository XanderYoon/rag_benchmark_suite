peda- gogical value of each approach. The rest of the paper is structured as follows: Section 2 revi ews related work in AQG, ICL, and RAG. Section 3 outlines the task deﬁnition and mathematical formulation of the problem. Sec tion 4 discusses the dataset used in our study. In Section 5, we desc ribe the methodology, detailing the implementation of ICL, RAG, and our proposed hybrid model. The experimental setup is presen ted in Section 6. Section 7 covers the automated and human evalua - tion metrics used in this study. Results and analysis are pre sented in Section 8, where we compare the models using both automati c and human evaluations and provide a detailed analysis of the re- sults. Finally, Section 9 concludes the paper and discussespotential future work. 2 RELATED WORK 2.1 Automatic Question Generation (AQG) The ﬁeld of Automatic Question Generation (AQG) has evolved from early rule-based systems to more sophisticated neuralnetwork- based approaches. Initial research in AQG primarily focuse d on template-based methods [8, 33, 43], which generate questio ns by applying syntactic and semantic rules to predeﬁned templat es. For example, [13] used syntactic transformations to generate f actual questions from declarative sentences. However, template-based meth- ods such as [43] lack ﬂexibility and are highly dependent on p re- deﬁned rules, which limit their applicability to diﬀerent d omains and complex question types. With the rise of neural networks, sequence-to-sequence (Seq2Seq) models such as T5 [38] and BART [22] have been successfully ap - plied to AQG [10, 18, 27]. These models are trained end-to-en d to map a given passage to a corresponding question, generati ng more diverse and grammatically correct questions. However , neu- ral models often struggle with domain-speciﬁc knowledge, e spe- cially when training data is limited. Moreover, they tend