( /u1D458 = 5, /u1D45A = 5). They rated each question on a scale from 1 (worst) to 5 (best) based on ﬁve crit e- ria: Grammaticality [27, 29, 46] (correctness of grammar indepen- dent of context), Appropriateness [27] (semantic correctness irre- spective of context), Relevance [27] (alignment with the given con- text), Complexity [27] (level of reasoning required to answer), and Answerability [29, 46] (whether the question can be answered from the provided context). To evaluate the level of agreement among the ﬁve raters for each generated question, we use Fleiss’s kappa as the measure of i nter- rater agreement. The resulting kappa scores are 0.51, 0.48, 0.45, 0.45, and 0.49 for grammaticality, appropriateness, relevance, com- plexity, and answerability, respectively. These kappa values sug- gest a moderate level of agreement across all human evaluati on metrics [20]. 8 RESULTS AND ANALYSIS In the automated evaluation (see Table 1), In-Context Learn ing (ICL) with /u1D458 = 7 demonstrates the best overall performance, sur- passing other conﬁgurations such as RAG ( /u1D458 = 5) and the Hybrid Model ( /u1D458 = 5, /u1D45A = 5). ICL with /u1D458 = 7 excels in ROUGE-L, ME- TEOR, CHrF, and BERTScore, while ICL with /u1D458 = 5 achieves the highest BLEU-4 score. This advantage can be attributed to IC L’s ability to eﬀectively leverage multiple examples to genera te more contextually relevant questions. In contrast, RAG and the H ybrid Model depend on external document retrieval, which sometim es retrieves content misaligned with the input passage, reducing their eﬀectiveness in generating closely related questions. Nev ertheless, both RAG and the Hybrid Model consistently outperform the ﬁn e- tuned baseline models (i.e., T5-large and BART-large) acro ss all automated evaluation metrics, demonstrating their overal l supe- riority in question generation tasks. The ﬁne-tuned models ,