to signiﬁcantly diﬀerent outputs [25]. Fu rther- more, ICL’s reliance on in-context examples means it may fai l to capture deeper contextual information that is not present i n the input passage but is essential for generating complex educa tional questions. 2.3 Retrieval-Augmented Generation (RAG) Retrieval-Augmented Generation (RAG), introduced by [23] , ad- dresses the limitations of purely generative models by inco rporat- ing an external retrieval mechanism. In a typical RAG framew ork, the model ﬁrst retrieves relevant documents from an externa l cor- pus based on the input text [16]. The retrieved documents pro - vide additional context, which is used to generate more accu rate and contextually enriched outputs [41]. This approach is pa rticu- larly beneﬁcial for knowledge-intensive tasks where the input text alone does not provide suﬃcient information. Leveraging ICL and RAG for Automatic /Q_uestion Generation in Ed ucational Domains Conference’17, July 2017, Washington, DC , USA RAG has been applied successfully to various tasks, such as knowledge- based question answering [34, 42, 47] and code summarization [26]. Nevertheless, RAG’s performance in the context of AQG has no t been widely explored. In the context of AQG, RAG can retrievesup- plementary information from external sources, such as text books or research papers, to generate questions that are not only c ontex- tually aligned with the input passage but also pedagogicall y rele- vant. However, the retrieval process may introduce noise if irrel- evant or redundant documents are retrieved [9], which can ne ga- tively impact the quality of the generated questions. 3 TASK DEFINITION 3.1 Problem Statement The task of Automatic Question Generation (AQG) can be formu- lated as follows: given an input passage /u1D443 , the objective is to gen- erate a question /u1D444 that is relevant to the content of /u1D443 , contextually