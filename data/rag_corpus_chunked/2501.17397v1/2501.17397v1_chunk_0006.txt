T5 [38] and BART [22] have been successfully ap - plied to AQG [10, 18, 27]. These models are trained end-to-en d to map a given passage to a corresponding question, generati ng more diverse and grammatically correct questions. However , neu- ral models often struggle with domain-speciﬁc knowledge, e spe- cially when training data is limited. Moreover, they tend to gen- erate questions that are either too generic or not pedagogic ally aligned with the content’s educational goals. 2.2 In-Context Learning (ICL) In-Context Learning (ICL) was introduced by [6] as part of th e capabilities of the GPT-3 language model. Unlike ﬁne-tunin g ap- proaches, where a model is trained on a speciﬁc task using a la rge dataset, ICL allows large language models to perform a task b ased on a few examples provided as input [25]. The model processes these examples and generalizes them to generate appropriat e out- puts for unseen instances. ICL has demonstrated success in various tasks, including text classiﬁcation [31], translation [2] , and sum- marization [15, 45], making it particularly valuable in sit uations where data is scarce. However, in the context of AQG, ICL’s performance has not been extensively studied. The challenge lies in crafting eﬀ ective few-shot prompts that guide the model toward generating ped a- gogically meaningful questions. Existing work has shown that ICL is sensitive to prompt design, and small changes in the examp les provided can lead to signiﬁcantly diﬀerent outputs [25]. Fu rther- more, ICL’s reliance on in-context examples means it may fai l to capture deeper contextual information that is not present i n the input passage but is essential for generating complex educa tional questions. 2.3 Retrieval-Augmented Generation (RAG) Retrieval-Augmented Generation (RAG), introduced by [23] , ad- dresses the limitations of purely generative models