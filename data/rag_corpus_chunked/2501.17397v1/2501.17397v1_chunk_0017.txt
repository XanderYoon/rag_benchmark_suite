the ICL with /u1D458 = 7, which also performs strongly across all metrics, particularly in grammaticality, appropriateness, and rele- vance, but slightly lags behind the Hybrid Model in complexity. However, ICL with /u1D458 = 7 outperforms other models in generat- ing more answerable questions, as it leverages multiple examples to better align the questions with the context, making them m ore straightforward to answer. RAG ( /u1D458 = 5), however, shows weaker performance, particularly in answerability, where it scores lower compared to both the ICL and Hybrid models. This could be at- tributed to its reliance on external document retrieval, wh ich may introduce content that is less directly tied to the passage, thus af- fecting the relevance and answerability of the generated questions. The ﬁne-tuned baseline models, T5-large and BART-large, perform adequately but fall behind the more advanced models. T5-lar ge exhibits a stronger performance compared to BART-large acr oss most metrics, particularly in grammaticality and appropriateness, but neither model matches the performance of the ICL or Hybri d approaches. These ﬁndings underscore the beneﬁts of incorp orat- ing retrieval and few-shot examples for generating more con textu- ally relevant and complex educational questions. Tables 3 and 4 show two data samples (context and gold stan- dard question) from the EduProbe dataset [27], covering His tory and Economics. They also display the corresponding questions gen- erated by T5-large (baseline), BART-large (baseline), ICL(/u1D458 = 3, 5, 7), RAG (/u1D458 = 5), and the Hybrid model ( /u1D458 = 5, /u1D45A = 5). It can be ob- served that while baseline models sometimes produce questi ons that are irrelevant or unanswerable from the context, the proposed techniques, particularly ICL, generate questions that aremore closely aligned with the provided context. 9 CONCLUSIONS AND FUTURE WORK In this study, we explored advanced methods