entry in the dataset includes a context (or passage), a long prompt, a short prompt, and a question. For our experiments, we extracted only the context (or passage) and question, focusing on evaluating how well the models generate questions based on the provided context s (or passages). We used the same training and test datasets as in [ 27]. 5 METHODOLOGY In this section, we describe the methodologies employed for ques- tion generation using In-Context Learning (ICL), Retrieval-Augmented Generation (RAG), and a Hybrid Model that combines both ap- proaches. 5.1 In-Context Learning (ICL) Approach For the ICL approach, we use the GPT-4 [1] model to generate ques- tions based on a few-shot prompt. Each prompt consists of /u1D458 ex- ample input-output pairs {( /u1D443 1, /u1D444 1) , . . . , ( /u1D443 /u1D458, /u1D444 /u1D458)} , where each pair consists of a passage and a corresponding question. Given a n ew passage /u1D443 new, the model generates a question /u1D444 new using the few- shot examples as context. The general structure of the ICL pr ompt is as follows: ⟨/u1D443 1, /u1D444 1⟩, ⟨/u1D443 2, /u1D444 2⟩, . . . , /u1D443 new → /u1D444 new We experimented with diﬀerent numbers of examples /u1D458 and op- timized the structure of the prompt for maximum performance . 5.2 Retrieval-Augmented Generation (RAG) Approach The RAG model employs the BART architecture as its generative backbone, further reﬁned by a retrieval module. This retrie val sys- tem, implemented using FAISS [17], searches through an extensive external corpus of educational materials 2 speciﬁcally tailored for school-level subjects such as History, Geography, Economi cs, Sci- ence, and Environmental Studies. For a given passage/u1D443 , the system identiﬁes the most relevant documents from this curated cor pus. These retrieved documents denoted as { /u1D445/u1D456} /u1D458