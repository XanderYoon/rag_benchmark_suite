documents are retrieved [9], which can ne ga- tively impact the quality of the generated questions. 3 TASK DEFINITION 3.1 Problem Statement The task of Automatic Question Generation (AQG) can be formu- lated as follows: given an input passage /u1D443 , the objective is to gen- erate a question /u1D444 that is relevant to the content of /u1D443 , contextually accurate, and aligned with educational goals. 3.2 Input and Output Representation The input to the model is a passage /u1D443 , which can be a sentence, a paragraph, or a longer text excerpt from educational materi al. The output is a question /u1D444 that is relevant to /u1D443 and suitable for use in an educational setting. Formally, we deﬁne the task as learn ing a function: /u1D453( /u1D443 ) = /u1D444 where /u1D443 is the input passage, and /u1D444 is the generated question. 3.3 In-Context Learning (ICL) In the In-Context Learning (ICL) paradigm, given an input passage /u1D443 new and a set of/u1D458 few-shot examples {( /u1D443 1, /u1D444 1) , ( /u1D443 2, /u1D444 2) , . . . , ( /u1D443 /u1D458, /u1D444 /u1D458)} , the model generates a new question /u1D444 new corresponding to /u1D443 new: /u1D444 new = /u1D453ICL ( /u1D443 new, {( /u1D443 /u1D456, /u1D444 /u1D456)} /u1D458 /u1D456=1) Here, the few-shot examples serve as prompts to guide the ques- tion generation process for the new passage. 3.4 Retrieval-Augmented Generation (RAG) For Retrieval-Augmented Generation (RAG), the task is extended by incorporating an external retrieval mechanism. Given a pas sage /u1D443 , the model retrieves a set of relevant documents { /u1D4451, /u1D445 2, . . . , /u1D445 /u1D458} from an external corpus. These documents provide additiona l con- text, and the ﬁnal question /u1D444 is generated as: /u1D444 = /u1D453RAG( /u1D443, { /u1D445/u1D456} /u1D458 /u1D456=1)