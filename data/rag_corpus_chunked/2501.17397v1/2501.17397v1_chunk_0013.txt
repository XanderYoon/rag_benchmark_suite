que stions. 6.3 Retrieval-Augmented Generation (RAG) For RAG, we utilized BART-large 5 as the backbone model and in- tegrated a retrieval module with FAISS for eﬃcient document re- trieval. For each input passage, the retrieval module ident iﬁes and fetches the top /u1D458 = 5 relevant documents from a large corpus of educational material. These documents are concatenated wi th the input passage, and BART-large generates questions based on the combined context. 3https://huggingface.co/models 4https://platform.openai.com/docs/models/gpt-4-turbo -and-gpt-4 5https://huggingface.co/facebook/bart-large 6.4 Hybrid Model The Hybrid Model combines retrieval and in-context learning tech- niques. We ﬁrst retrieve /u1D458 = 5 relevant documents for each pas- sage, then use a few-shot prompt with /u1D45A = 5 examples to generate questions. This model leverages both the additional contex t from retrieved documents and the few-shot learning capabilitie s to pro- duce high-quality questions. 7 EV ALUATION METRICS The performance of the models was evaluated using several au to- mated evaluation metrics: BLEU-4 [35], ROUGE-L [24], METEO R [21], ChRF [36], and BERTScore [48]. We use the implementati ons of these metrics provided by the SummEval package 6. Acknowledging the limitations of automated metrics in textgen- eration research [4, 5, 32, 40], we conducted a human evaluat ion involving three high school teachers and two high school students. Each evaluator reviewed a total of 1,400 questions across seven set- tings: T5-large (baseline), BART-large (baseline), ICL ( /u1D458 = 3, 5, 7), RAG (/u1D458 = 5), and Hybrid model ( /u1D458 = 5, /u1D45A = 5). They rated each question on a scale from 1 (worst) to 5 (best) based on ﬁve crit e- ria: Grammaticality [27, 29, 46] (correctness of grammar indepen- dent of context), Appropriateness [27] (semantic correctness irre- spective of context), Relevance [27] (alignment with the given con- text), Complexity [27] (level of reasoning required to