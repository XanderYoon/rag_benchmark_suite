70.20 Hybrid Model (/u1D458 = 5, /u1D45A = 5) 21.45 53.79 33.69 57.78 71.45 Table 2: Human evaluation results for generated questions a cross diﬀerent models on grammaticality (Gramm), appropriate- ness (Appr), relevance (Rel), complexity (Comp), and answerability (Answ). The highest value for each metric, achieved by any model, is highlighted in blue, and values marked with * are statistically signiﬁcant base d on student’s /u1D461 -test at the 95% conﬁ- dence interval compared to the lowest corresponding baseli ne value. Model Gramm Appr Rel Comp Answ T5-large (Baseline) [27] 4.65 4.45 3.92 3.57 3.21 BART-large (Baseline) [27] 3.81 3.98 3.60 3.60 3.15 ICL (/u1D458 = 3) 4.67* 4.50* 3.97* 3.65 3.20 ICL (/u1D458 = 5) 4.72* 4.56* 4.03* 3.78 3.24 ICL (/u1D458 = 7) 4.76* 4.62* 4.08* 3.84 3.31 RAG (/u1D458 = 5) 3.90 4.10 3.70 3.74 2.90 Hybrid Model (/u1D458 = 5, /u1D45A = 5) 4.84* 4.74* 4.25* 4.02* 3.20 models across key metrics such as grammaticality, appropriateness, relevance, and complexity. The Hybrid Model achieves the highest scores in most human evaluation metrics, suggesting its abi lity to generate more linguistically correct, contextually appropriate, and pedagogically complex questions. In terms of relevance and com- plexity, the Hybrid Model demonstrates a clear advantage, likely due to its integration of both retrieval and few-shot learni ng tech- niques, which allows it to create questions that are well-al igned with the passage and involve deeper reasoning. This is follo wed by the ICL with /u1D458 = 7, which also performs strongly across all metrics, particularly in grammaticality, appropriateness, and rele- vance, but slightly lags behind the Hybrid Model in complexity. However, ICL with /u1D458 = 7 outperforms other models in generat- ing more answerable questions, as it leverages multiple examples to better align the questions with the context, making them m