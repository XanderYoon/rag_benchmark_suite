document retrieval, which sometim es retrieves content misaligned with the input passage, reducing their eﬀectiveness in generating closely related questions. Nev ertheless, both RAG and the Hybrid Model consistently outperform the ﬁn e- tuned baseline models (i.e., T5-large and BART-large) acro ss all automated evaluation metrics, demonstrating their overal l supe- riority in question generation tasks. The ﬁne-tuned models , while reliable, lack the additional context provided by retrieva l or few- shot examples, making them less eﬀective than the more advanced techniques (i.e., ICL, RAG or Hybrid Models). As shown in Table 2, the human evaluation results indicate that the Hybrid Model ( /u1D458 = 5, /u1D45A = 5) consistently outperforms other 6https://github.com/Yale-LILY/SummEval Leveraging ICL and RAG for Automatic /Q_uestion Generation in Ed ucational Domains Conference’17, July 2017, Washington, DC , USA Table 1: Automatic evaluation results for diﬀerent models. The highest value for each metric, achieved by any model, is h igh- lighted in blue, and values marked with * are statistically signiﬁcant base d on student’s /u1D461 -test at the 95% conﬁdence interval compared to the lowest corresponding baseline value. Model BLEU-4 ROUGE-L METEOR ChRF BERTScore T5-large (Baseline) [27] 21.59 53.90 32.20 57.03 71.80 BART-large (Baseline) [27] 20.05 51.60 31.90 54.96 74.20 ICL (/u1D458 = 3) 22.65 54.24 32.98 58.47 74.93 ICL (/u1D458 = 5) 22.87 54.84 33.58 59.42* 75.60* ICL (/u1D458 = 7) 22.69 55.95* 34.62 60.48* 75.92* RAG (/u1D458 = 5) 20.76 52.60 32.07 56.93 70.20 Hybrid Model (/u1D458 = 5, /u1D45A = 5) 21.45 53.79 33.69 57.78 71.45 Table 2: Human evaluation results for generated questions a cross diﬀerent models on grammaticality (Gramm), appropriate- ness (Appr), relevance (Rel), complexity (Comp), and answerability (Answ). The highest value for each metric, achieved by any model, is highlighted in blue, and values marked with * are statistically