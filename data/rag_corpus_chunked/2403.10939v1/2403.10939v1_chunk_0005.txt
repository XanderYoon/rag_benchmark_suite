{p− i }N i=1, the learning task trains the query and passage encoders via minimizing the softm ax cross- entropy: Lp CE = LCE (q, p+, P − ). Positive query-passage pairs are encouraged to have higher similarity scores and negative pairs to have lower scores . 2.1 Dense Retriever with Self-supervised Contrastive Lear ning DR+CL alternates DR with an additional contrastive loss that maximizes the agreement between diﬀerently augmented views of the same query [11]. This loss enforces that a query q and its typoed variation q′, sampled from a set of available typoed variations Q′ = {q′ i}K i=1, are close together in the latent space and distant from other distinct queries Q− = {q− i }M i=1: Lt CE = LCE (q, q′, Q− ). The ﬁnal loss is computed as a weighted summation, L = w1Lp CE + w2Lt CE . 4 Georgios Sidiropoulos and Evangelos Kanoulas DR+CLM is our multi-positive variant of DR+CL. Given a query q, instead of sampling a diﬀerent typoed variant q′ from a set Q′ at each update, we propose simultaneously employing all typoed variants. To do so, we replace Lt CE with the following multi-positive contrastive loss that accounts for multip le positives: Lt M CE = LM CE(q, Q′, Q− ). The ﬁnal loss is: L = w1Lp CE + w2Lt M CE. 2.2 Dense Retriever with Dual Learning DR+DL trains a robust, dense retriever via a contrastive dual learning me cha- nism [7]. In contrast to classic DR, which is optimized for passage retr ieval only (Lp CE ), DR+DL is optimized for the prime task of passage retrieval (i.e., lea rns to retrieve relevant passages for queries) and the dual task of q uery retrieval (i.e., learns to retrieve relevant queries for passages). Therefore, g iven a