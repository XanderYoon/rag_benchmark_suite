in many cases, they do not suﬃciently utilize the multiple p ositives that are available. Speciﬁcally, when tackling the robustifying subta sks, they ignore that multiple positives are available per anchor and consider co ntrastive learning with a single positive. In contrast, we suggest leveraging all the available positives and adopting a multi-positive contrastive learning approac h. We aim to answer the following research questions: RQ1 Can our multi-positive contrastive learning approach increase the r obust- ness of dense retrievers that use contrastive learning with a single positive? RQ2 Does our multi-positive contrastive learning variant outperform its single- positive counterpart regardless of the number of positives? Our experimental results on two datasets show that our propose d approach of employing multi-positive contrastive learning yields improvements in ro bustness compared to contrastive learning with a single positive. 1 1 https://github.com/GSidiropoulos/typo-robust-multi-positive-DR Typo-Robust Dense Retrieval via Multi-positive Contrasti ve Learning 3 2 Methodology Contrastive learning is a vital component for training an eﬀective de nse retriever. Current typo-robust dense retrievers use contrastive learning with a single posi- tive sample and multiple negative ones for both the main task of passa ge retrieval and the robustifying subtasks. In detail, given an anchor x, a positive sample x+, and a set of negative samples X − , the contrastive prediction task aims to bring the positive sample closer to the anchor than any other negat ive sample: LCE (x, x+, X− ) = − log ef (x,x+) ef (x,x+) + ∑ x−∈ X − ef (x,x−) , (1) where f is a similarity function (e.g., dot product). However, in many cases, multiple positive samples are available per anc hor and can be used simultaneously to increase the discriminative perfor mance of the model. As opposed to the aforementioned contrastive loss that su pports a single