robustifying subtasks with contrastive learning, ass uming a single positive sample (query) and a set of negative ones per anchor (dep ending on the approach, the anchor can be either a query or a passage). Howev er, alongside the original query, its multiple typoed variants are available. Hence, there is more than one positive sample per anchor. As a result, we can levera ge all the available positives simultaneously and apply multi-positive contrastive learning instead (i.e., contrastive learning that supports multiple positives). For instance, Tasawong et al. [13] computes the contrastive loss for the query retrieval subta sk using only the original, typo-free query as relevant for a given pass age. Given a passage, we argue that both the original query and its typoed var iations can be considered as relevant and adopt a multi-positive contrastive loss in stead. Literature on contrastive learning has shown that including multiple p ositives can enhance the ability of the model to discriminate between signal a nd noise (negatives)[6,8]. Intuitively, multiple negatives focus on what makes the anchor and the negatives dissimilar, while multiple positives focus on what make s the anchor and the positives similar. To this end, contrasting among mult iple pos- itives and negatives can bring an anchor and all its positives closer to gether in the latent space while keeping them far from the negatives. In this work, we revisit recent methods in typo-robust dense retr ieval and unveil that, in many cases, they do not suﬃciently utilize the multiple p ositives that are available. Speciﬁcally, when tackling the robustifying subta sks, they ignore that multiple positives are available per anchor and consider co ntrastive learning with a single positive. In contrast, we suggest leveraging all the available positives and adopting a multi-positive contrastive learning approac h. We aim to