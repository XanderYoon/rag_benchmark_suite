the Tevatron toolkit [3] to train the models and the Ranx library [1] to evaluate the r etrieval performance. Finally, we use the typo generators from the TextA ttack toolkit [9] for all the methods we experiment with to augment the training qu eries. 2 The original methods and our proposed counterparts employ t he same number of original, typo-free query-passage pairs per batch. Howeve r, our method leverages multiple typoed variants for each query; therefore, the bat ch we need to ﬁt in the GPU memory is larger. 6 Georgios Sidiropoulos and Evangelos Kanoulas 4 Results To answer RQ1, we compare the retrieval performance of our multi-positive con- trastive learning approaches against the original models. From Tab le 1, we see that employing our multi-positive contrastive learning approach yield s improve- ments in robustness against typos upon the original methods that use contrastive learning with a single positive. As expected, the more dramatic improvement comes when applying m ulti- positive contrastive learning on DR+DL since the original work only co nsiders the typo-free query as positive when computing the contrastive lo ss for query re- trieval (see Section 2.2). In contrast, in our DR+DL M , we consider the typo-free query and all its available typoed variants as positives and use a multi- positive contrastive loss for query retrieval. We also see improvements whe n compar- ing DR+CL vs. our DR+CL M . In detail, employing all available positives (ty- poed queries) at once and using multi-positive contrastive loss outp erforms sam- pling a diﬀerent positive at each update and using a single positive cont rastive loss (see Section 2.1). The improvements are held even when compar ing our DR+DL+STM against DR+DL+ST, a model that already uses multiple posi- tives. As seen in Section 2.3, DR+DL+ST uses a contrastive