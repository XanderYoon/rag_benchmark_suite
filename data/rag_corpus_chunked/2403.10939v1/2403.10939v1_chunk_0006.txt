learning me cha- nism [7]. In contrast to classic DR, which is optimized for passage retr ieval only (Lp CE ), DR+DL is optimized for the prime task of passage retrieval (i.e., lea rns to retrieve relevant passages for queries) and the dual task of q uery retrieval (i.e., learns to retrieve relevant queries for passages). Therefore, g iven a passage p, a positive query q+, and a set of negative queries Q− = {q− i }M i=1, it further mini- mizes the loss for the dual task: Lq CE = LCE (p, q+, Q− ). The dual training loss is added to the prime training loss to conduct contrastive dual learn ing and train the dense retriever. Speciﬁcally, the ﬁnal loss is computed as L = Lp CE + wLq CE , where w is used to weight the dual task loss. DR+DLM is our multi-positive variant of DR+DL. Contrary to DR+DL, we propose that for the query retrieval task, given a passage p, we can have a set of relevant queries consisting of the typo-free query and its typo ed variants, Q = {q+, q′ 1, q′ 2, . . . , q′ K}. Thus, we replace the contrastive loss of Lq CE with a multi-positive contrastive loss, which can account for multiple releva nt queries at the same time. We deﬁne the multi-positive contrastive loss for th e dual task as: Lq M CE = LM CE(p, Q, Q− ). The ﬁnal loss is computed as L = Lp CE +wLq M CE. 2.3 Dense Retriever with Dual Learning and Self-T eaching DR+ST+DL trains a dense retriever with dual learning and self-teaching [13]. Similar to DR+DL, it minimizes the Lp CE and Lq CE for the main task of pas- sage retrieval and the subtask of query