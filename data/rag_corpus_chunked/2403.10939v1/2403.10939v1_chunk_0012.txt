.543 .315 DR+ST+DLM ✓ .335 .955 .261 .902 † .687 .870 .579 .426 † .583 .342 † At this point, we want to explore how the diﬀerent numbers of positiv es aﬀect our multi-positive approach ( RQ2). To do so, we compare our DR+DL+ST M against DR+DL+ST. In its training, the latter already employs multiple posi- tives simultaneously to compute the KL-divergence losses. Howeve r, our multi- positive approach fully beneﬁts from the multiple available positives by incor- porating them when computing the contrastive loss for query retr ieval (Lq CE → Lq M CE). Table 2 unveils that our multi-positive variant consistently outper forms the original model for the diﬀerent numbers of typoed variants pe r query. Typo-Robust Dense Retrieval via Multi-positive Contrasti ve Learning 7 Table 2. Retrieval results for diﬀerent query augmentation sizes ( K). We report the results in the format “ R@1000 ( M RR@10)” on MS MARCO with typos. Multi-positive contrastive loss K 1 10 20 30 40 DR+ST+DL ✗ .884 (.251) .892 (.258) .894 (.258) .893 (.259) .893 (.259) DR+ST+DLM ✓ .884 (.251) .898 (.260) .900 (.260) .902 (.261) .902 (.261) 5 Conclusions In this work, we revisit recent studies in typo-robust dense retrie val and showcase that they do not always make suﬃcient use of multiple positive samples . In de- tail, they assume a single positive sample and multiple negatives per anc hor and use contrastive learning for the robustifying subtasks. Opposed to this, we pro- pose to leverage all the available positives and employ multi-positive co ntrastive learning. Experimentation on two datasets shows that following a mu lti-positive contrastive learning approach yields improvements in the robustne ss of the un- derlying dense retriever upon contrastive learning with a single posit ive. Acknowledgements This research was supported by the NWO Innovational