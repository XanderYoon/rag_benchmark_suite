ef (x,x+) ef (x,x+) + ∑ x−∈ X − ef (x,x−) , (1) where f is a similarity function (e.g., dot product). However, in many cases, multiple positive samples are available per anc hor and can be used simultaneously to increase the discriminative perfor mance of the model. As opposed to the aforementioned contrastive loss that su pports a single positive, we propose employing a multi-positive contrastive loss to be neﬁt from all the available positives. Given an anchor x, multiple positive samples X +, and multiple negatives X − , a multi-positive contrastive loss [6] is computed as: LM CE(x, X+, X− ) = − 1 |X +| ∑ x+∈ X + log ef (x,x+) ef (x,x+) + ∑ x−∈ X − ef (x,x−) . (2) This work aims to identify cases in typo-robust dense retrieval met hods where the robustifying subtasks consider only a single positive sample, eve n though multiple ones are available, and optimize a contrastive loss. Next, we r eplace the contrastive loss with its multi-positive alternative to beneﬁt from all the avail- able positives. Below we present the typo-robust dense retrieval methods we build upon followed by our multi-positive variants. We focus on dense retr ievers that follow the dual-encoder architecture [5]. A traditional dense retrie ver, DR, is op- timized only with the passage retrieval task. Given a query q, a positive/relevant passage p+, and a set of negative/irrelevant passages P − = {p− i }N i=1, the learning task trains the query and passage encoders via minimizing the softm ax cross- entropy: Lp CE = LCE (q, p+, P − ). Positive query-passage pairs are encouraged to have higher similarity scores and negative pairs to have lower scores . 2.1 Dense Retriever with Self-supervised Contrastive Lear ning DR+CL alternates DR with an