relevant information, according to gold nuggets. Relevant Sentences= sentence with nuggets all sentences measures how many sentences men- tion relevant nuggets. Citation Support= supported citations citations , measures how many citations actually support their summary sentence. 5 Results Table 1 reports results averaged across NeuCLIR topics.Crucibleconsistently outperformsGingerandGinger-LLaMA on all nugget-oriented metrics. Rela- tive gains are especially pronounced fornugget recall(+42 to +65%) andnugget density(+21 to +25%). 6 Dietz et al. T able 1.Comparison of RAG systems (each with PlaidX retriever) on TREC NeuCLIR 2024. Best results in bold, paired t-test ▲/▽ with referenceBase. System Nugget Recall Nugget Density Sentence Novelty Relevant Sent. Citation Support Crucible-Base0.429 0.448 0.255 0.703 0.902 Crucible-Verified0.4380.457 0.267 ▲0.733 ▲0.961 GptResearcher ▽0.177 ▽0.131 ▽0.083 ▽0.265 ▽0.571 Ginger ▽0.244 ▽0.264 ▽0.162 ▽0.285 ▽0.436 Ginger-LLaMA ▽0.241 ▽0.134 ▽0.097 ▽0.136 ▽0.476 BulletPoints0.508 ▽0.340 0.243 ▽0.468 0.835 T able 2.Downstream effects of the document retrieval method and optional verifica- tion; using the same nugget bank across variations; referenceCrucible. Retrieval Nugget Recall Nugget Density Sentence Novelty Relevant Sent. Citation Support Crucible-BaseFrom nuggets 0.429 0.255 0.448 0.7030.902 Milco0.467 ▲0.300 ▲0.501 0.708 ▽0.839 Qwen3 0.446 ▲0.292 ▲0.491 ▲0.7580.892 PlaidX 0.424 ▲0.3240.468 0.728 ▽0.807 Crucible-VerifiedFrom nuggets 0.438 0.267 0.457 0.7330.961 Milco0.464 ▲0.337 ▲0.5110.706 ▽0.931 Qwen3 0.434 ▲0.308 0.4810.7510.959 PlaidX 0.412 ▲0.305 0.463 0.695 ▽0.921 Overall performance.We attributeCrucible’s gains to its nugget-first de- sign: explicit ideation yields systematic coverage; per-nugget extraction enforces grounding; and fingerprint duplicate checks preserve density. In contrast,Gin- ger’s cluster-based summarization risks citation traceability, while not providing the required information. We remark that GINGER was designed for the TREC RAG 24 [25] task and the Autonuggetizer [21] for evaluation. Ranking model.Crucibleobtains its great performance under all explored retrieval models. Retrievers that offer broad coverage and emphasize recall, such as Milco and Qwen3 consistently yield improvements on nugget metrics, but may hurt citation support. (These