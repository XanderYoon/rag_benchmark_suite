grounding difficult and the pipeline opaque to developers. A rec- ommended remedy is to first hallucinates an answer and then retrieve supporting evidence, as inHyDE[11], which raises lingering concerns about trustworthi- ness. Clustering-based RAG.Ginger[14] is a nugget-based RAG system that defines nuggets as verbatim text spans copied directly from retrieved passages. These spans are first clustered into topical facets using BERTopic, after which the clusters are reranked to identify the most relevant facets. Summaries of the top clusters form the systemâ€™s output, which is then rewritten for fluency. Although the pipeline has access to evidence for generation, the clustering summarization step abstracts away from the original document extractions, im- pacting the faithful citation grounding. Agentic RAG.Agentic frameworks decompose the pipeline into subtasks and let a planner invoke them as needed [1, 27].WebGPT[19] embeds search instruc- tions in the prompt, letting the LLM choose passages to cite. A prominent agentic system isGptResearcher[7, 8], which orchestrates query generation, retrieval, extraction, and report writing, with optional verification and trace-back. 3 Nugget-first RAG Approach:Crucible Figure 1 illustrates the workflow ofCrucible, which builds its pipeline around structured Q&A nuggets that guide retrieval, extraction, and assembly. This contrasts withGinger, which operates on verbatim spans clustered into facets. Incorporating Q&A Nuggets into Retrieval-Augmented Generation 3 Fig. 1.For each generated nugget,Crucibleextracts candidate sentences and adds the bestksentences to the final response. No content clustering is needed. Nugget ideation.We begin by retrieving an initial pool of documents; in our study we use the PLAID-X dense retriever [28]. For each document we gener- ate a short query-focused summary with an LLM, then prompt the same LLM (usingLLaMA-3.3-70B-Instruct) to produce Q&A nuggets conditioned on the summary and the user request. To reduce redundancy among nuggets, we first detect paraphrases with an LLM prompt, then successively merge most confident paraphrases until the