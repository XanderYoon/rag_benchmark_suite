study we use the PLAID-X dense retriever [28]. For each document we gener- ate a short query-focused summary with an LLM, then prompt the same LLM (usingLLaMA-3.3-70B-Instruct) to produce Q&A nuggets conditioned on the summary and the user request. To reduce redundancy among nuggets, we first detect paraphrases with an LLM prompt, then successively merge most confident paraphrases until the de- sired number of canonical nuggets is obtained. Nugget ranking.The resulting nugget bank is reranked with a Support-Vector Classifier (SVC) trained on 19 quality features, which utilize LLM judge prompts to measure how well each nugget addresses the information need (task state- ment, background, role, communication style, and scope), degree of vitality [22], and several features from researchy questions [23]. This is combined with basic readability metrics to quantify reading level [2] and sentence complexity [17]. Using the number of paraphrases as an indicator for nugget quality, we fuse this feature-based ranking with a popularity-based ranking. Obtaining the top 20 nuggets for each request will form the nugget bank that drives the remainder of the response generation. Retrieval.Our base system uses documents of which nuggets were extracted. Additionally, the pipeline is evaluated with the top 100 of several retrieval models with their default configurations: PLAID-X [28] a dense multilingual retriever, using top 25 per language, dense retrieval with Qwen3 [30], and Milco [20]. Scanning and generation.Using the nugget bank, we scan retrieved documents 7 for passages that directly answer each nugget. Using the prompt in Fig. 2 8 we (1) locate a supporting passage, (2) extract a concise self-contained sentence, and (3) record the LLMâ€™s token-likelihood as the extraction confidence. 7 Segmented into 1000 character chunks, split at sentence boundary. 8 The system message was omitted due to a bug. Updated results in online appendix. 4 Dietz et al. Given