Incorporating Q&A Nuggets into Retrieval-Augmented Generation Laura Dietz1 , Bryan Li2 , Gabrielle Liu 3 , Jia-Huei Ju 4 , Eugene Yang5 , Dawn Lawrie5 , William Walden5 , and James Mayfield 5 1 University of New Hampshire, Durham, New Hampshire, USAdietz@cs.unh.edu 2 University of Pennsylvania, Philadelphia, Pennsylvania, USA bryanli@seas.upenn.edu 3 Yale University, New Haven, Connecticut, USAkaili.liu@yale.edu 4 University of Amsterdam, Amsterdam, Netherlandsj.ju@uva.nl 5 Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore, Maryland, USA {eugene.yang,lawrie,wwalden1,mayfield}@jhu.edu Abstract.RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we presentCrucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics—instead of opaque cluster abstractions—while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC Neu- CLIR 2024 collection, ourCruciblesystem substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding. 6 Keywords:RAG·LLM judge·nugget-based evaluation. 1 Introduction Retrieval-Augmented Generation (RAG) has become the dominant framework for grounding LLM outputs in evidence [13, 16]. At the same time, nugget-based evaluation methods have emerged as the standard for measuring relevance of long-form responses [10, 18]. Anugget, i.e., short Q&A pair, fact, or claim, is a fine-grained reusable unit for assessing whether key information is covered. By encoding the information that must appear in a system answer, nuggets enable evaluation metrics such as “nugget recall” that directly quantify the amount of useful information given by the information system. We argue that nuggets are valuable not only for evaluation but also for guiding retrieval and generation, especially given that LLMs can produce high-quality nuggets automatically [4, 9]. 6 Appendix athttps://github.com/hltcoe/ecir26-crucible-system-appendix/ arXiv:2601.13222v1 [cs.IR] 19