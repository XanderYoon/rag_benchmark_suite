to use the RLHF loss function for training, These two datasets (GPT derived and the datasets in [12]) are then combined such that each sample in the final database contains a triplet: an image, a positive statement, and a negative statement. An example of the dataset is given in the following. Fig. 6. A sample entry of dataset used for training/evaluation. To evaluate the entire RAG system and measure its align- ment with human evaluators, we created two datasets (de- scribed above) by collecting human preferences regarding both RAG selection and generation processes. B. Model and Hyperparameters For the structure of the RS model, we use “clip-vit-large- patch14-336” [18] as a vision encoder, and a projector to map the vision encoder output to proper embedding. We use llama-1.5v [15] as a language decoder model and replace the language model final layer head with our custom head which maps from dimension 4096 to 1. To benefit from the pre- training, we use the trained weight of the LLaV A model for the vision encoder projector and language model. We train our RS model on GPU A100. For the CS model, the vision encoder is Siglip [19] and the language model is llama-3. Similarly, the last layer of the llama-3 model is replaced with our custom head which maps a layer of size 4096 to 1 and the weights are taken from the VILA model. The learning rate for both training CS and RS models are α = 10 −4. C. Relevance Score (RS) Performance To evaluate our RS score, we use test data to determine how accurately the RS model can detect samples with relevant text to a given image. Fig. 7 shows the score histogram of 2,000 test samples for either of positive and negative statements. The RS score is