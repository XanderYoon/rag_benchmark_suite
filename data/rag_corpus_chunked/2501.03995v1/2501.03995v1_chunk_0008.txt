encoder (CLIP-VE), and each piece of the text is embedded using the CLIP text encoder (CLIP-TE). The same goes for the query. Then, the cosine similarity is found between the embedding of the query and the embeddings in the vector database, irrespective of its original data type. Based on such preprocessing, a vector database is pro- duced that is composed of a pair of embeddings and the reference to each piece of information (original data) in the enterprise database. The RAG system is also composed of several blocks which are used during the operation. A selection block retrieves the relevant pieces of the enterprise data by performing a cosine similarity search using the embeddings for each piece of the data. The final stage of the selection process is determining the raw context which is composed of the corresponding original pieces of the data for the top- k entries in terms of the cosine similarity. The last block of the RAG system is composed of a generation block which takes the retrieved context and generates a RAG response based on the query. In multi-modal RAG, this block may be a MLLM (such as GPT4o), capable of directly handling multi- modal data. Alternatively, this block can consist of engines that generate a text-based context for each piece of retrieved data, which is then collectively used as input to an LLM (such as LLAMA or GPT3.5) to generate the response. For instance, if the original data is in text form, it is used as is, whereas a VLM is employed to convert any images into text descriptions. Both “selection block” and “generation block” are illustrated in Fig. 2. III. RAG-C HECK Our main objective in RAG-check is to evaluate both the selection process (i.e., how relevant the “retrieved documents” are to the query)