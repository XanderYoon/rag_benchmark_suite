aligns with this difference. Specifically, for two nonzero ratings r1 and r2, where r1 < r 2, given two images for a single query, if the score corresponding to the second image is higher, we consider this a match and assign a reward r2−r1. Otherwise, it is considered unmatched, and the reward is zero. We then calculate the average reward across all pairs of images for each question and across all questions. Table II shows the average reward for different scores including RS, clip-vit-large-patch14, clip- vit-base-patch16, clip-vit-large-patch32, and BLIP. The result indicates that RS has more than 20% improvement in average reward in comparison to the other scores used for relevancy and has a good match with human evaluator’s opinion. We use similar methodology to evaluate CS. We use the same test set of 1,000 questions. We fix the selection scheme by using the CLIP-ViT-large-patch14-336 as the vision en- coder and cosine similarity to select the top-5 relevant images. Next, we employ LLaV A as the VLM and llama-1.5v as the LLM. The VLM extracts text descriptions from the selected images, while the LLM generates the final response. To evaluate the responses, we partition the generated response into atomic statements using our partitioning algorithm. A human evaluator then assesses each atomic statement for each question, choosing one of three options: correct, incorrect, or subjective. To measure the alignment between our proposed CS and human judgment, we calculate the overlap between our CS and human evaluation. The results show a 91% match between the CS and the human evaluator’s responses. TABLE II ALIGNMENT OF RS SCORES WITH HUMAN EVALUATORS . Relevance Scoring Method Value Proposed Relevance Score (RS) 0.879 CLIP-vit-large-patch14-336 0.689 CLIP-vit-base-patch32 0.620 CLIP-vit-base-patch16 0.611 BLIP-large 0.491 F . Evaluation of Different RAG Schemes In this section, we use our proposed