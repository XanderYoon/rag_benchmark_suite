producing the response which is referred to as response- generation-hallucination. Prior works [8]–[10] provide means to evaluate LLMs and measure the correctness of text-based responses by LLM given a text query. FactScore [8] demonstrates that breaking long statements into fine-grained atomic statements can improve the identification of hallucinations. This method allows for more precise verification of each individual statement against the source material. Lookback Lens [9] detects context hallucina- tion by analyzing the attention scores of the corresponding LLM. It calculates a ratio attention scores between query tokens and response tokens to identify parts of the response that may not be adequately supported by the context. MARS [10] builds on the idea that some parts of a statement are more crucial than others in determining hallucination. The work highlights such important part by assigning more weight to these significant parts, thereby improving the detection of inaccuracies by focusing on the most relevant segments. Despite these advancements, a few papers have specifically addressed hallucination detection in text-based RAG. Among these, RAGAS [11] and LlamaIndex evaluation stand out. RAGAS focuses on evaluating the accuracy and relevance of responses generated by RAG systems. It assesses how well the retrieved documents support the generated response and whether the information is correctly incorporated. RAGAS typically involves human evaluators who rate the coherence and factual accuracy of the responses. LlamaIndex relies on GPT4 [1] to evaluate the faithfulness of the generated response and the relevancy of retrieved documents. Note that both RAGAS and LlamaIndex are designed for text-based queries and datasets. On the other hand, some works have focused on detecting hallucination in VLMs [12]–[14]. The work in [12] presents a model inspired by InstructBLIP to detect hallucinations occur- ring in single-image scenarios with user queries. This approach leverages the capabilities of InstructBLIP to scrutinize the