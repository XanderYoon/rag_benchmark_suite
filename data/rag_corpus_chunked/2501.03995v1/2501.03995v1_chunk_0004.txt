retrieved documents. Note that both RAGAS and LlamaIndex are designed for text-based queries and datasets. On the other hand, some works have focused on detecting hallucination in VLMs [12]–[14]. The work in [12] presents a model inspired by InstructBLIP to detect hallucinations occur- ring in single-image scenarios with user queries. This approach leverages the capabilities of InstructBLIP to scrutinize the generated responses for inaccuracies related to the provided image and query. FaithScore [13] is another tool trained to identify the correctness of VLM-generated statements with an image input. FaithScore first extracts a comprehensive list of atomic facts from the generated response, then verifies the accuracy of these fine-grained atomic facts against the input image. However, to the best of our knowledge, there are no existing works that provide hallucination scores specifically for multi-modal RAG systems, where the contexts include multiple pieces of multi-modal data (say multiple images) which is retrieved from a database and then the inference is performed on such retrieved multi-modal context. Multi- modal RAG systems present unique challenges, as they require integrating information from various visual sources along with the text to generate coherent and accurate responses. Our goal is to develop RAG-check, a method to evaluate the performance of multi-modal RAG schemes. RAG-check com- prises three main components: (i) First, we design and train a neural network structure that takes the selected pieces of data by the RAG scheme by tapping into the internal components of a RAG system and producing a relevancy score (RS) between each selected piece and the query. For example, when an image (or text) is selected, RS evaluates how relevant the retrieved image (or text) is to the user query. By assigning a relevancy score to each image, we can determine how well the visual data aligns with the user’s intent