, 2024. [11] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert. Ragas: Au- tomated evaluation of retrieval augmented generation. arXiv preprint arXiv:2309.15217, 2023. [12] A. Gunjal, J. Yin, and E. Bas. Detecting and preventing hallucinations in large vision language models. In AAAI, 2024. [13] L. Jing, R. Li, Y . Chen, M. Jia, and X. Du. Faithscore: Evaluat- ing hallucinations in large vision-language models. arXiv preprint arXiv:2311.01477, 2023. [14] C. Jiang, W. Ye, M. Dong, H. Jia, H. Xu, M. Yan, J. Zhang, and S. Zhang. Hal-eval: A universal and fine-grained hallucination eval- uation framework for large vision language models. arXiv preprint arXiv:2402.15721, 2024. [15] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. A. Lachaux, T. Lacroix, B. Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [16] N. Ziegler, D. M .and Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019. [17] T. Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. DollÂ´ar, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [19] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In IEEE/CVF ICCV, 2023. [20] M. Ren, R. Kiros, and R. Zemel. Exploring models and data for image question answering. Advances in NIPS , 2015. [21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In