RS-labeler is higher. TABLE I PERFORMANCE EVALUATION OF SCORING MODELS . Model Accuracy true0 true1 LLaV A 0.724 0.695 0.746 RS model 0.865 0.909 0.831 VILA 0.732 0.710 0.734 CS model 0.875 0.940 0.806 Fig. 8 illustrates the effect of changing the threshold on true0, true1, and accuracy. As shown, there is a trade-off: for example, if the threshold is decreased to near 0, almost all samples are detected as relevant, increasing the probability of correctly identifying relevant samples at the expense of incor- rectly labeling irrelevant samples. We optimize the threshold η where the probabilities of detecting each type of sample are equal to balance the detection of both relevant and irrelevant samples. This threshold may be fine-tuned after training the RS model. Fig. 8. RS model trade-off performance in changing threshold. D. Correctness Score (CS) Performance Using similar approach as before, we define CS-labeler using CS score with threshold η = 0 .7 and VILA-labeler to evaluate the performance of CS score based on the evaluation dataset. Table I shows the evaluation result of CS-labeler and VILA-labeler in detecting the positive and negative statements as correct (denoted as true0) and incorrect (denoted as true1) depending to the given image, respectively. Similar conclu- sions hold for CS score as Table I indicates that CS model has greater confidence in detecting the correctness of the statement with respect to the given image in comparison to the original VILA as the probability of true detection for both correct (true0) and incorrect (true1) samples and hence the overall accuracy for CS-labeler is higher. E. RAG Scores vs Human Evaluation In this section, we aim to show the alignment between RS and CS with human evaluators. For RS, we randomly select 1,000 questions from the test set of the COCO-QA [20] dataset. We