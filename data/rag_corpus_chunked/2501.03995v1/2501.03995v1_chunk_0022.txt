probability of true detection for both correct (true0) and incorrect (true1) samples and hence the overall accuracy for CS-labeler is higher. E. RAG Scores vs Human Evaluation In this section, we aim to show the alignment between RS and CS with human evaluators. For RS, we randomly select 1,000 questions from the test set of the COCO-QA [20] dataset. We also gather 1,281 random images from the COCO validation set and use the CLIP-ViT-large-patch14-334p model to encode both the images and the selected questions. We obtain the top-5 images for each question that had the highest cosine similarity with the encoded query data. We collect human evaluators’ opinions on the relevance of each retrieved image for each question. Evaluators could choose from five options: unsure, no relevance, mild relevance, high relevance, and complete relevance corresponding to a rating 0,1, . . . , 4, respectively. For each question and retrieved image pair, we calculate the relevancy score with different relevancy scoring algorithms including RS, clip–vit-large-patch14-336, clip-vit- base-patch16, clip-vit-base-patch32, blip [21]. The following algorithm was used to assess how well the human evalua- tors’ judgments match with relevance scoring measures: We consider all possible combinations among the five retrieved images for each question. For each combination, if any image is marked as ‘unsure’ by the human evaluator, we disregard the sample. In cases where there is a difference in human evaluator’s opinion between two images, we evaluate how closely the score aligns with this difference. Specifically, for two nonzero ratings r1 and r2, where r1 < r 2, given two images for a single query, if the score corresponding to the second image is higher, we consider this a match and assign a reward r2−r1. Otherwise, it is considered unmatched, and the reward is zero. We then calculate the average reward