to quantify relevancy of image to query. the query and the image. At this point, a neural network head is trained to extract this information out of the produced embedding by the cross-attention module in the form of a single real number between zero (representing no relevance) and one (representing complete relevance). Fig. 5. RS model structure. Hence, to train for RS, we propose the model as shown in Fig. 5. This model is composed of 5 blocks discussed in the following in more details. (i) Vision encoder, which encodes patches of images separately. We use CLIP large as a vision encoder which has a transformer architecture to encode image patches. (ii) Projector: The embeddings of patches obtained from the vision encoder need to be translated to a language that is known by the transformer block, and the projector performs this conversion between the output embeddings created by the vision encoder and the input embeddings to the transformer. (iii) Tokenization and embedding: Each word in the text query has a corresponding token that will be mapped to an embed- ding. Some tokens require special treatment. For example, if a user query contains a special image token ( <Image>), the token will be replaced with the embeddings of the patches. If there is no reference to an image, the system will add image embeddings to the beginning of user query embeddings. As a result, we have N + P embeddings in total where N is the number of user query tokens (with the exception of special tokens) and P is the total number of patches. The embedding space is d dimensional. (iv) Transformer block: The entire N + P embeddings are processed by the transformer, which contains L transformer blocks each containing multi-head attention (MHA) unit with H attention heads and