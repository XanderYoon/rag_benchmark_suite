the retrieval process including the selection of the information pieces and the context genera- tion. During the retrieval process, depending on the selection algorithm, RAG selects a few, say k, entries from the database that have the highest similarity to the user query. A common approach is to select the k entries whose embeddings have the highest cosine similarity to the query embedding, referred to as the top- k entries. However, this method of selection may not always retrieve the most relevant data for the user query. The wrong selection can result in an incorrect response since the LLM would never see the correct information. We refer to this as selection-hallucination. Another source of hallucination is during context generation in the RAG scheme. In particular, multi-modal RAG systems process each selected piece of information to text and generate a text-based context by concatenating the text-based context for all the selected pieces. The text-based context is then provided to the LLM in the final stage of the RAG system along with the query to produce the response. For example, if the selected piece is an image, VLM may be used, say with a prompt “Describe the image” (DTI) to narrate the images into the text, and it is well-known that VLMs may hallucinate and produce wrong context in this case. We refer to this as context-generation- hallucination. In the final stage, LLM may also hallucinate arXiv:2501.03995v1 [cs.LG] 7 Jan 2025 when producing the response which is referred to as response- generation-hallucination. Prior works [8]–[10] provide means to evaluate LLMs and measure the correctness of text-based responses by LLM given a text query. FactScore [8] demonstrates that breaking long statements into fine-grained atomic statements can improve the identification of hallucinations. This method allows for more precise verification of each individual statement against