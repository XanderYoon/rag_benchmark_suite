I NTRODUCTION Generative models have seen significant improvements with recent advancements in large language models (LLMs) [1]. Although the generated responses often reach human-like quality, hallucinations—generating incorrect or irrelevant re- sponses—remain an issue [2]. This problem is particularly concerning for applications where accuracy is critical, such as in medical evaluations, processing insurance claims, and autonomous decision-making. The hallucination issue also per- sists in vision-language models (VLMs) [3]–[6], which process both images and user queries to generate text responses. Sev- eral robust VLMs, such as LLaV A [3], InstructBLIP [4], and VILA [5], exist. However, these models sometimes produce incorrect responses based on the provided images and user queries. On the other hand, retrieval-augmented generation (RAG) [7] offers a promising solution to improve the relevance of responses generated by LLMs. RAG systems enhance LLMs by incorporating external knowledge, on which the user seeks to base their responses. In a RAG system, external knowledge such as enterprise data is stored in a database. When a user submits a query, the RAG system retrieves a few relevant and similar pieces of data from the database. The LLM then generates a coherent response based on this Fig. 1. RAG-check example. curated information. This approach reduces hallucinations by constraining the LLM to generate responses grounded in the provided external knowledge, thereby increasing accuracy and relevance. However, in addition to the possible hallucinations in the response, the RAG scheme introduces some new sources of hallucinations during the retrieval process including the selection of the information pieces and the context genera- tion. During the retrieval process, depending on the selection algorithm, RAG selects a few, say k, entries from the database that have the highest similarity to the user query. A common approach is to select the k entries whose embeddings have the highest cosine similarity to