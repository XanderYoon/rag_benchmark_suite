to using CLIP alone, demonstrating how the RS model can enhance the RAG system beyond simply evaluating its performance. 2) RAG Context Generation Performance:: We use our pro- posed CS to compare various RAG schemes that incorporate different VLMs and LLMs, or in general different MLLMs. For the VLM component, one can select LLaV A or GPT- 4, and for the LLM, options include LLAMA or GPT-3.5. Alternatively, one can combine both VLM and LLM into a single MLLM, such as GPT-4o, to directly generate responses from retrieved images. Fig. 10 illustrates the comparison between the five RAG configurations in terms of context and generation error. The results show that GPT-4o outperforms Fig. 9. Comparison of different RAG selection mechanisms with RS model. the other schemes by approximately 20%, while the remaining RAG schemes exhibit performance within the 60-68% range. Fig. 10. Comparison of different RAG context and generation mechanism with CS model. REFERENCES [1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. [2] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . J. Bang, A. Madotto, and P. Fung. Survey of hallucination in natural language generation. ACM Computing Surveys , 55(12):1â€“38, 2023. [3] H. Liu, C. Li, Q. Wu, and Y . J. Lee. Visual instruction tuning. Advances in NIPS, 2024. [4] W. Dai, J. Li, D. LI, A. Tiong, J. Zhao, W. Wang, B. Li, P. N Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. In Advances in NIPS , 2023. [5] J. Lin, H. Yin, W. Ping, P. Molchanov, M. Shoeybi, and S. Han. Vila: On pre-training for visual language models. In