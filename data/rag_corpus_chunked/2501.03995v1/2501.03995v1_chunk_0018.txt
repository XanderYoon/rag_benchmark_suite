case of the RS and CS score in this paper is to evaluate any particular instances of response invoked by a query when using a given RAG. In this process, RS assesses the performance of the selection scheme of the RAG and CS assesses the performance of the generation scheme (response and possibly combined with context generation) of the RAG. In the following, we first discuss the training database, the specifics of the model and hyper-parameters, and then provide the evaluation results for the RS and CS models. We also evaluate the matching and alignment of our proposed scores with the data from human evaluation. We finally use our score to compare the average performance of different RAG schemes. A. Dataset Specification In this subsection, we describe the dataset used for train- ing, validation, and evaluation of the RS and CS models. We create and utilize a dataset comprising 121,000 samples partitioned randomly for training (101K samples), validation (10K samples) and evaluation (10k samples). This dataset is a mixture of two sources: a ChatGPT-derived database and a database containing human evaluator samples [12]. The human evaluator database consists of image-statement pairs from the COCO image dataset [17], where the statements were marked as either positive or negative by human evaluators. Additionally, we task GPT-4o with generating positive and negative statements for an additional 2,000 COCO images. The statements are then checked by human for accuracy of generation. Since we aim to use the RLHF loss function for training, These two datasets (GPT derived and the datasets in [12]) are then combined such that each sample in the final database contains a triplet: an image, a positive statement, and a negative statement. An example of the dataset is given in the following. Fig. 6. A sample entry of dataset used for