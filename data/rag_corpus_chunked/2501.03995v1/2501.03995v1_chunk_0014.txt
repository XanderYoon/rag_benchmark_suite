total where N is the number of user query tokens (with the exception of special tokens) and P is the total number of patches. The embedding space is d dimensional. (iv) Transformer block: The entire N + P embeddings are processed by the transformer, which contains L transformer blocks each containing multi-head attention (MHA) unit with H attention heads and a fully- connected layers. The attention mechanism in the transformer is used to find the relation between different patches of image and user query. The output of the transformer unit is a vector of N + P embeddings each with dimension d. (v) LM head: The last generated token by the transformer, i.e., yN +P in Fig. 5, is an embedding of size d which is given to the LM head as input. The LM head is a fully connected layer that maps dimension d to 1 which is trained to represent RS. Training the entire model from scratch to learn both lan- guage and the relationship between language and images requires vast amounts of data and computational power. There- fore, for the backbone of our system, we leverage the weights from the current state-of-the-art model. Specifically, we use LLaV A [3] weights, which include the clip-vit-large-patch14- 336p as the vision encoder and LLAMA [15] as the LLM decoder, to combine the image patches and query tokens. We modify the final head of LLaV A, which originally converts embeddings into vocabulary, and replace it with a dedicated head that maps the LLM embedding dimension d to a 1 (single output). To train the RS model (specifically our head), we use a training dataset consisting of triplets (I, sp, sn), where I is the image, sp is a positive statement about the image I, and sn is a negative statement about