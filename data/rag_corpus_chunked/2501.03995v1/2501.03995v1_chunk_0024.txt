our CS and human evaluation. The results show a 91% match between the CS and the human evaluator’s responses. TABLE II ALIGNMENT OF RS SCORES WITH HUMAN EVALUATORS . Relevance Scoring Method Value Proposed Relevance Score (RS) 0.879 CLIP-vit-large-patch14-336 0.689 CLIP-vit-base-patch32 0.620 CLIP-vit-base-patch16 0.611 BLIP-large 0.491 F . Evaluation of Different RAG Schemes In this section, we use our proposed RS and CS scores to evaluate the performance of different RAG schemes in terms of the relevancy of retrieved entries and the correctness of the generated responses. 1) RAG Selection Performance:: We compare the perfor- mance of different selection mechanisms in multi-modal RAG systems. Fig. 9 illustrates the average relevancy score for each of the top-5 retrieved images across 1,000 test questions. As shown, when a RAG system uses CLIP models (clip-vit-large- patch14-336, clip-vit-base-patch32, clip-vit-base-patch16) as the vision encoder to process the images and queries, and cosine similarity to select the top-5 images, the average relevancy of the retrieved images ranges from 41% to 30%. In contrast, when using the RS model to calculate the RS score for all possible query-image pairs before selecting the top- 5 images, the average relevancy scores significantly improve, ranging from 89.5% to 71%. However, using the RS model to score all possible pairs results in a 35× increase in compu- tation compared to the CLIP dot product, even when using a GPU (A100). The results show a significant improvement in RAG selection performance compared to using CLIP alone, demonstrating how the RS model can enhance the RAG system beyond simply evaluating its performance. 2) RAG Context Generation Performance:: We use our pro- posed CS to compare various RAG schemes that incorporate different VLMs and LLMs, or in general different MLLMs. For the VLM component, one can select LLaV A or GPT- 4, and for