affects the performance of the CS score which is the fine-tuned model with a dedicated LM head. In contrast, VILA has a similar structure to the LLaV A model but is trained specifically for multi-image inference. The training process for the dedicated LM head in our CS model is similar to that of the RS model. The template that we add to the beginning of each statement si is as follows: “I am giving you k images. Evaluate this statement with these images and answer by either ‘correct’ or ‘incorrect’: {si}”. When there is no reference to a particular piece of context, CS is found between the statement and the entire pieces in the raw context. However, in the calculation of CS for a statement that has particular references to pieces of the context, CS is found between the statement and only the referred pieces of the context in the statement. For example, if an evaluation of the statement: sk = “A boy with a cowboy hat is riding a white horse in <image1>”, the CS is only computed by using the template: “I am giving you a statement. Evaluate this statement and answer by either ‘correct’ or ‘incorrect’: sk”, where the embeddings of image1 are inserted in the position of the token <image1>. IV. N UMERICAL RESULT In this section, we present numerical results on the training and evaluation of our proposed RS and CS models. The intended use case of the RS and CS score in this paper is to evaluate any particular instances of response invoked by a query when using a given RAG. In this process, RS assesses the performance of the selection scheme of the RAG and CS assesses the performance of the generation scheme (response and possibly combined with context generation) of the RAG.