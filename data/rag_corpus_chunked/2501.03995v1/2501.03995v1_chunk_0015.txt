vocabulary, and replace it with a dedicated head that maps the LLM embedding dimension d to a 1 (single output). To train the RS model (specifically our head), we use a training dataset consisting of triplets (I, sp, sn), where I is the image, sp is a positive statement about the image I, and sn is a negative statement about the image I. We define RS model as M. The output of our model with the given statement s is a vector y = M (I, s) of dimension N + P . For the sake of short notation, we use y−1(I, s) to represent the last embedding output of the LLM decoder (in Fig. 5) given an image and a statement. We use the following template in processing the user query s by the RS model: “Evaluate the relevancy of the given statement with the image <image>. Evaluate by either ‘relevant’ or ‘irrelevant’. The statement is: {s}.” We use a modified version of the reinforcement learning with human feedback (RLHF) loss function [16] to train our RS model. In the original RLHF model, even though we have data indicating both highly preferable and less preferable instances, the loss function only ensures that the highly prefer- able instance receives a higher score than the less preferable one and there is no lower or upper bound of the loss function. However, since we want to assign a score that falls within the range of [0, 1] for any given statement and image, we modify the RLHF loss function as follows: L = − log (σ (y−1(I, sp)) − σ (y−1(I, sn))) , (1) where σ is our softmax operator. During the inference, given a pair of (I, q), we can obtain the RS as: RS = σ (y−1(I, q)) . (2)