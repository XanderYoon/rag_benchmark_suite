the reconstructed textual features of nodes after text generation, we then train graph machine learning models and evaluate their performance. We con- duct the evaluation using GCN [34], SAGE [19], and MLP to exclude any model-induced bias during evaluation. 6.1.4 Evaluation Metrics. Following conventional works [38, 55], we use the BLEU-4/ROUGE-L/Bert-F1 score as the evaluation met- rics to conduct a comprehensive analysis of the generated texts. In addition, we take the initiative to use the task-oriented evaluation, which quantifies the quality of generated texts based on whether they can fulfill purposes of downstream tasks, e.g., node classifi- cation and link prediction. For node classification, we report the average accuracy of testing nodes (in our setting, the testing nodes are assumed to be the ones with features to be reconstructed). For link prediction, we report the average Hits@100 following [25] of randomly selected edges. 6.1.5 Parameter Settings. For text generation, the number of re- trieved texts and partially observed starting words are both set as 3. Moreover, we set the number of generated words to be 150, 250, 200, 150, 300, 500, 250, 200, 300 according to the average length of texts in Cora, Pubmed, Arxiv, Product, Book, Epinion, Music, Pantry, Eron-Email. For all datasets except Eron-Email, each node is only associated with one text sequence, hence we could directly cal- culate textual/topological similarity metric and retrieve the Top-3 accordingly. For Eron-Email where each node/employee possesses many texts/emails, we first query the sender and receiver of the target email to be generated and then collect emails from the two employees with the highest topological similarity to that sender and receiver. Furthermore, we select the Top-3 emails from those collected emails based on their textual similarity to the partially observed target text. For most of the hyperparameters used for eval- uation with