We pre- process the original emails and extract the sender/receiver/text information of each email following script here3. 6.1.2 Baselines. For baselines, as no previous works have consid- ered the proximity/role-based relations in RAG, we design baselines by equipping LLMs, GPT3 and GPT3.5 for text generation, with the following RAG strategies: ‚Ä¢ None: we do not retrieve any additional texts but completely rely on the partially observed starting words. ‚Ä¢ Random (RD): we randomly retrieve ùêæ texts from the graph- structured knowledge base. 3https://github.com/mihir-m-gandhi/Enron-Email-Analysis/tree/main ‚Ä¢ Text: we calculate the semantic similarity between the partially observed texts in the target sequence and all other texts. Then we select the Top-ùêæ ones according to their semantic similarity. ‚Ä¢ Topo: we calculate the topological similarity according to embed- dings from Eq. (3)/(6) between the node of the target sequence and all other nodes. Then we rank them and select the top- ùêæ ones. This one is essentially our Topo-RAG framework. 6.1.3 Evaluation Tasks. To evaluate the quality of our generated text, we compare it with ground-truth text following established methodologies [38, 55]. We only focus on comparing the generated content, without considering any initially observed words. In addi- tion, we introduce a task-oriented evaluation to assess the quality of the generated texts. Specifically, we adopt two graph-based tasks, node classification and link prediction. In these two tasks, textual features of certain nodes are assumed to be reconstructed using various baselines including our Topo-RAG. With the reconstructed textual features of nodes after text generation, we then train graph machine learning models and evaluate their performance. We con- duct the evaluation using GCN [34], SAGE [19], and MLP to exclude any model-induced bias during evaluation. 6.1.4 Evaluation Metrics. Following conventional works [38, 55], we use the BLEU-4/ROUGE-L/Bert-F1 score as the evaluation met- rics to conduct a