often simply referred to as "text genera- tion", focuses on creating human-readable texts based on the input texts along with task-specific instructions to serve various applica- tions [13, 16, 27, 62], including dialogue systems, document summa- rization, and academic writing [18, 41, 47, 49]. Despite the unprece- dented success achieved by large language models (LLMs) in text generation [3, 60], their performance can still be hampered by two key issues: the limited knowledge available in input texts [68, 74, 80] and the tendency of LLMs to produce non-factual responses [76, 77]. On one hand, relying solely on input text provides limited infor- mation misaligning with the abundant knowledge necessary for the desired output. On the other hand, while pre-training over vast corpora has equipped LLMs with world knowledge, this knowl- edge is encoded in their black-box-like parameters and there is no clear pathway to map these intriguing parameters to interpretable knowledge that can be faithfully used in text generation, which renders the hallucination in the generated responses. arXiv:2405.17602v1 [cs.IR] 27 May 2024 Conference’17, July 2017, Washington, DC, USA To address the challenge of limited knowledge in input texts and avoid hallucination of LLMs, Retrieval-augmented Generation (RAG) can be naturally used to create a well-informed context by retrieving necessary knowledge from external databases [ 12, 29, 37, 44]. One notable direction is to improve RAG by integrating additional knowledge into the retrieval process, such as enabling LLMs to utilize automated tools [22, 57, 82] and access well-curated databases [66]. For example, [46, 59, 66] incorporate the symbolic knowledge in the form of structured triples from the knowledge graph to 1) enhance the faithfulness of LLMs’ answers, 2) enable the knowledge evolution via dynamic editing, and 3) provide in- terpretability for LLMs’ responses. Unfortunately, most of these KG-enhanced RAGs have been exclusively