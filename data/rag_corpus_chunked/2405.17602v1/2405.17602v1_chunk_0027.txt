pattern can be attributed to the inherent charac- teristics of these evaluation metrics. The BLEU score focuses on the overlap of exact words. As we provide more starting words, the length of the remaining part of the target sentence decreases. Figure 6: As the number of beginning words increases, the BLEU score first increases and then decreases on Cora while the BertScore-F1 continually increases. TopoRAG Consis- tently achieves the highest text generation performance than the other two baselines. Consequently, in the latter stages, the likelihood that the gener- ated words precisely match the few remaining words diminishes, leading to a drop in BLEU scores. On the other hand, BertScore-F1 evaluates semantic embedding matching, which does not rely on exact word overlap. Therefore, as the number of provided starting words increases, LLMs gain a better understanding of the general context of the target text. This enhanced contextual understanding facilitates the generation of text that is semantically more aligned, explaining the consistent improvement in BertScore-F1. The reason why we do not see this first-increasing and then-decreasing trend with BLEU score on Book is due to the generally longer lengths of their texts compared with Cora, as verified in Figure 7(b). 6.4 Feature Imputation with TopoRAG Many machine learning models assume a fully observed feature matrix. However, in practice, each feature is only observed for a subset of nodes due to constraints like privacy concerns or limited resources for data annotation [72]. In all these scenarios, the missing feature issues could catastrophically compromise the capability of machine learning models [ 52], which motivates many previous works developing solutions to handling missing feature issue [73]. Since our proposed TopoRAG can naturally generate node fea- tures in graph-based datasets, in this section, we evaluate its effec- tiveness in handling missing features by comparing its performance