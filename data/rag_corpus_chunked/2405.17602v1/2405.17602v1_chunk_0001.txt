integrates these texts into prompts to stimulate LLMs for text generation. We have curated established text-attributed networks and conducted comprehensive experiments to validate the effectiveness of this framework, demonstrating its potential to enhance RAG with topo- logical awareness. KEYWORDS Large Language Model, Retrieval-Augmented Generation, Topology Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference’17, July 2017, Washington, DC, USA © 2024 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn Figure 1: Topology-aware Retrieval for Text Generation.(a): People write paper abstracts by referring to other papers cited in the related work.; (b): Employees owning the same local subgraph structures possess the same titles/responsibilities. ACM Reference Format: Yu Wang, Nedim Lipka, Ruiyi Zhang, Alexa Siu, Yuying Zhao, Bo Ni, Xin Wang, Ryan Rossi, and Tyler Derr. 2024. Augmenting Textual Generation via Topology Aware Retrieval. In Proceedings of ACM Conference (Con- ference’17).ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn 1 INTRODUCTION Text-to-text generation, often simply referred to as "text genera- tion", focuses on creating human-readable texts based on the input texts along with task-specific instructions to serve various applica- tions [13, 16, 27, 62], including dialogue systems, document summa- rization, and academic writing [18, 41, 47, 49]. Despite the unprece- dented success achieved by large language models (LLMs) in text generation [3, 60],