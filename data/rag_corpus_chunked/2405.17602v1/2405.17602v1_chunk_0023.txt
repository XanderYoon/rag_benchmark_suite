1.62% 0.39% 28.80% 11.60% 0.98% 181.5% 6.20% 0.61% 61.78% 7.57% 0.99% GPT 3 None 0.34 10.68 78.42 0.72 13.57 80.03 1.65 7.62 78.42 1.19 13.70 79.43 1.09 13.16 79.80 RD 0.48 10.86 80.16 1.04 14.98 82.49 3.30 10.55 79.49 1.17 14.25 81.53 1.69 14.33 81.53 Text 0.38 10.85 79.54 1.64 15.28 82.72 3.65 11.14 79.79 4.03 15.54 81.81 2.28 14.63 81.51 TopoRAG 1.14 11.68 80.79 2.02 15.51 82.86 3.97 10.69 80.38 5.13 16.10 82.18 3.45 15.54 82.49 Boost 200.0% 7.65% 1.57% 23.17% 1.51% 0.17% 8.77% -4.04% 0.74% 27.3% 3.6% 0.45% 51.32% 6.22% 1.2% 6.2 Performance Comparison 6.2.1 Traditional Evaluation. Here we compare the text genera- tion capabilities of GPT-3.5 and GPT-3 enhanced with our pro- posed Topo-RAG and other baseline methods. Due to resource constraints, we only randomly select 500 nodes along with their partially observed textual sequences to complete and report the average performance in Table 2. Overall, the proposed TopoRAG framework achieves the highest text generation performance with a significantly large margin, as shown by "Average", underscoring the benefits of integrating topological knowledge for retrieving additional contextual information in text generation. The second to best baseline is "Text" because it utilizes textual similarity to di- rectly query relevant context, which augments the text generation to some extent. Interestingly, we also find that including random texts in the generation process, i.e., "RD", significantly improves performance compared to "None" which includes no additional texts at all. This improvement likely arises because each text within the same text-attributed network is generally related to the same domain, e.g., all texts in Cora are papers and all texts in Epinion are reviews. Hence, incorporating additional texts can provide in- sights into potential writing styles and usage of domain-specific terminology, which benefits the text generation of the target node. Comparing performance boosts across different