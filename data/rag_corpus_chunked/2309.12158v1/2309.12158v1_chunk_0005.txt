re-scaled to a 1181 × 835 resolu- tion. This implies that the amount of actual musical content within the fragments can vary significantly due to the dura- tion of the notes and the tempo in which the piece is being performed. For instance, a sheet music snippet with longer notes played slowly would cover a substantially larger dura- tion in the audio than another one with shorter notes that has been played faster. As a consequence, generalisation issues can occur due to differences between what the network sees during training and the data it will see at application time: the musical content fed to the CNN may exhibit consider- ably less information than fragments it has been trained on. To address this problem, we proposed in [2] to let the net- work learn to adjust the temporal content of a given audio excerpt by using a separate soft-attention mechanism. First, the audio excerpt size is considerably expanded, up to four times the original duration. We then append to the audio network an attention pathway which, taking as input the au- dio magnitude spectrogram query, generates a 1-D probabil- ity density function that has the same number of frames as the input spectrogram and acts as an attention mask. Then, before the spectrogram excerpt is fed into the original audio embedding network, each frame thereof is multiplied by its attention mask, in this way cancelling out irrelevant parts of the query excerpt and focusing on the important information that should be embedded. In [2] we conducted a series of quantitative and qualita- tive experiments on synthesised piano music data, with the results indicating that the attention mechanism is capable of increasing the robustness of the audio–sheet music retrieval system. Table 1 summarises the main experimental results for a snippet-wise retrieval scenario: