more robust representations; and by al- lowing longer excerpts, we could relax the required anno- tations from strong to weak labels, meaning that now only the corresponding passage boundaries are needed. We per- formed quantitative and qualitative experiments in diverse retrieval scenarios with artificial and real data, with the re- sults indicating a superior performance of the recurrent ar- chitectures over the purely convolutional baseline. 2.3 Generalisation to real-world (noisy) data As already hinted at above, obtaining training data in the form of audio–sheet music datasets with appropriate fine- grained alignments is tedious and time-consuming, and also requires specialised annotators with proper musical train- ing. As a consequence, the embedding learning approaches rely on synthetic music data generated from the Multi- Modal Sheet Music Dataset (MSMD) [8]. This is a collec- tion of classical piano pieces with rich and varied data, in- cluding score sheets (PDF) engraved via Lilypond 1 and re- spective audio recordings synthetised from MIDI with sev- eral types of piano soundfonts. With over 400 pieces from several renowned composers, including Mozart, Beethoven and Schubert, and covering more than 15 hours of audio, the MSMD has detailed audio–sheet music alignments al- lowing us to obtain perfectly matching audio–sheet snippet pairs. On the downside, the generated scores and audios completely lack real-world artefacts such as scan inaccu- racies or room acoustics, and the audios exhibit perfectly steady tempo and dynamics, which is far from how real- world performances would sound. Using the synthetic MSMD severely affects the capacity of the model from Figure 1 to generalise to realistic retrieval scenarios when real music data is presented. In [4] we proposed to alleviate this problem via self-supervised con- trastive learning. Inspired by the SimCLR framework [7], we pre-trained each independent convolutional pathway (see Figure 1b) by contrasting differently augmented