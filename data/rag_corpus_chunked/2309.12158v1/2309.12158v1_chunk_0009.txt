would sound. Using the synthetic MSMD severely affects the capacity of the model from Figure 1 to generalise to realistic retrieval scenarios when real music data is presented. In [4] we proposed to alleviate this problem via self-supervised con- trastive learning. Inspired by the SimCLR framework [7], we pre-trained each independent convolutional pathway (see Figure 1b) by contrasting differently augmented ver- sions of short snippets of audio or sheet music images. As a key advantage of this approach, the data required for the pre-training step needs no annotations, which means we can use real music data scraped from the Web. We applied self-supervised contrastive pre-training for both modalities, taking the following steps: 1. Given a sample x from the training mini-batch of a given modality, two stochastic sets of data augmenta- tion transforms are applied to x, generating the posi- tive pair ˜xi and ˜xj. 2. Then a network composed of a CNN encoder and a multi-layer perceptron head computes a latent repre- sentation zi = e(˜xi) for each augmented sample. 3. Then the normalized-temperature cross-entropy ( NT- Xent) loss function is applied and summed over all pos- itive augmented pairs (˜xi, ˜xj) within the mini-batch: L = X (i,j) log exp(sim(zi, zj)/τ )P2N v=1 1[v̸=i]exp(sim(zi, zv)/τ ) , (1) where sim(·) is the cosine similarity between zi and zj and the temperature parameter τ ∈ R+ is adjusted to prioritise poorly embedded snippet pairs. As for the augmentations used for pre-training, we ap- plied to the snippets: horizontal and vertical shifts, resizing and rotation, additive Gaussian and Perlin noises, and small and large elastic deformations. Figure 2 shows examples of two pairs of augmented sheet music snippets when ap- plying all transforms randomly. The augmentations used on the audio excerpts are: time shift, polarity inversion, addi- tive Gaussian noise