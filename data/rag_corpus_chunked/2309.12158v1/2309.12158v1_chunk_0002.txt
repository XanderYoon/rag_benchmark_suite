major note- heads of a sheet music image. However, in order to gen- erate such representations a number of error-prone pre- processing steps are still needed, i.e., automatic music tran- scription [13] for the audio part, and optical music recogni- tion [3] on the sheet music side. A solution avoiding such problematic pre-processing components was proposed in [8], by designing a deep con- volutional network (CNN) that can learn an embedding space that is shared between the two underlying modali- ties. As sketched in Figure 1b, this architecture has two in- dependent convolutional pathways, each being responsible for encoding short fragments of its respective music modal- ity into a 32-dimensional embedding vector. This network is fed with pairs of short snippets of sheet music images and magnitude spectrograms, and the embedding space is obtained by minimising the cosine distance between pairs of matching audio–sheet music snippets, while maximis- ing the distance between non-matching pairs. Training is done by optimising a pairwise ranking loss function, and the final canonically correlated layer (CCA) [9] forces the embeddings computed from matching pairs to be correlated to each other in the shared latent space. Then, when the training is finished, snippet-wise retrieval reduces to nearest neighbour search in the joint space (see Figure 1c), which arXiv:2309.12158v1 [cs.SD] 21 Sep 2023 is a simple and fast procedure. This general retrieval frame- work based on short segments (snippets) extracted from the larger original documents (audio recordings, complete scores) supports a variety of possible applications, from piece identification to version detection and music recom- mendation. The deep learning approach is still in its early stages, and a number of obstacles and open problems prevent robust and large-scale deployment under real-world conditions, some of which we have already begun to solve: • Variable tempo and context discrepancies. Global