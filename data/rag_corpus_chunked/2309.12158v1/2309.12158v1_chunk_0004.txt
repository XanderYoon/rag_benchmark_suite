algorithms. • Efficient structures for fast retrieval. Quick cross- modal retrieval algorithms are essential when one is browsing through large-scale and heterogeneous music collections. This aspect can be often overlooked when the main focus is on retrieval quality metrics such as precision and recall. • Instrumentation and genre. Current methods have been developed specifically for classical and, even more specifically, piano music data. Other types of scores (e.g., orchestral), instruments, and genres will present new complications. In this article, we examine these challenges one by one. We first summarise our efforts to address some of the points above, as well as the improvements we obtained over the first and original system architecture. We then turn to the still open problems and propose concrete ideas to address these remaining challenges, aiming to establish a unified and robust methodology for cross-modal music retrieval in the context of truly large collections of musical materials. 2. Some First Solutions 2.1 Variable tempo and context discrepancies A key limitation of the baseline deep learning solution relates to the temporal context (or field of view) that is in- put to the network: both audio and sheet music snippets are fixed in size (see the inputs of the main model in Figure 1b) for a visual example). For the audio part, the fragments span roughly 2.1 seconds, which corresponds to 42 spectrogram frames. For the scores, snippets span 160 × 180 pixels, after sheet music pages being re-scaled to a 1181 × 835 resolu- tion. This implies that the amount of actual musical content within the fragments can vary significantly due to the dura- tion of the notes and the tempo in which the piece is being performed. For instance, a sheet music snippet with longer notes played slowly would cover a substantially larger dura- tion in