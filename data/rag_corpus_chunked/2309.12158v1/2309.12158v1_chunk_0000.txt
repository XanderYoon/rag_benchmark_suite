Towards Robust and Truly Large-Scale Audio–Sheet Music Retrieval Lu´ıs Carvalho1, Gerhard Widmer1,2 1Institute of Computational Perception & 2LIT Artificial Intelligence Lab Johannes Kepler University Linz, Austria {luis.carvalho, gerhard.widmer}@jku.at Abstract A range of applications of multi-modal music informa- tion retrieval is centred around the problem of connecting large collections of sheet music (images) to correspond- ing audio recordings, that is, identifying pairs of audio and score excerpts that refer to the same musical content. One of the typical and most recent approaches to this task em- ploys cross-modal deep learning architectures to learn joint embedding spaces that link the two distinct modalities – au- dio and sheet music images. While there has been steady improvement on this front over the past years, a number of open problems still prevent large-scale employment of this methodology. In this article we attempt to provide an in- sightful examination of the current developments on audio– sheet music retrieval via deep learning methods. We first identify a set of main challenges on the road towards robust and large-scale cross-modal music retrieval in real scenar- ios. We then highlight the steps we have taken so far to ad- dress some of these challenges, documenting step-by-step improvement along several dimensions. We conclude by analysing the remaining challenges and present ideas for solving these, in order to pave the way to a unified and ro- bust methodology for cross-modal music retrieval. 1 Task, Basic Approach, and Challenges A fundamental paradigm in the field of Music Informa- tion Retrieval (MIR) is consists in searching and retrieving items of different modalities, for example video clips, live and studio recordings, scanned sheet music, and album cov- ers. Moreover, the large amounts of music-related contents that are currently available in the digital domain demand for the development offast and robust retrieval methods that