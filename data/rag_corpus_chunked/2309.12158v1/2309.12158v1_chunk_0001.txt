paradigm in the field of Music Informa- tion Retrieval (MIR) is consists in searching and retrieving items of different modalities, for example video clips, live and studio recordings, scanned sheet music, and album cov- ers. Moreover, the large amounts of music-related contents that are currently available in the digital domain demand for the development offast and robust retrieval methods that al- low such extensive and rich collections to be searched and explored in a content-based way. A central and challenging problem in many cross-modal retrieval scenarios is known as audio–sheet music retrieval. The goal here is to, given a query fragment in one of the two modalities (a short audio excerpt, for example), retrieve the relevant music documents in the counterpart modality (sheet music scans). In addition, it is typically the case that no metadata or machine-readable information (i.e. MIDI or MusicXML formats) is available: one has to work di- rectly with raw music material, i.e., scanned music sheet images and digitised audio recordings. Figure 1a illustrates the retrieval task when searching an audio recording within a sheet music collection. A key step towards audio–sheet music retrieval is to de- fine a convenient joint representation in which both modal- ities can be readily compared. The common approaches for defining such shared space rely on handcrafted mid-level representations [12], such as chroma-based features [10], symbolic fingerprints [1], or the bootleg score [14], the latter one being a coarse codification of the major note- heads of a sheet music image. However, in order to gen- erate such representations a number of error-prone pre- processing steps are still needed, i.e., automatic music tran- scription [13] for the audio part, and optical music recogni- tion [3] on the sheet music side. A solution avoiding such problematic pre-processing components was proposed in [8], by designing