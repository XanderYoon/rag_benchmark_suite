audio-to-sheet direction. The base- line model (BL) is compared to its fine-tuned version when both audio and sheet music CNN pathways were pre-trained using self-supervised contrastive learning (BL+A+S). The pre-trained models outperform the baseline in all scenar- ios for all evaluation metrics. In [4] the same trend is re- ported for the sheet-to-audio direction, indicating that self- supervised pre-training is beneficial in our retrieval task. We note however that there is still a substantial degrada- tion when going from synthetic to partly and, in particular, fully real data. The MRR and MR values in the fully real scenario are definitely still unacceptable for real-world use. In [4], we also evaluated the models on the task of cross- modal piece identification, by aggregating snippet embed- dings, and also observed better identification results (e.g. over 100% improvement of the MRR on the task of iden- tifying a piece from an arbitrary recording) when using the pre-trained models. 2https://imslp.org R@1 R@25 MRR MR (I) MSMD (Fully synthetic data) BL 0.54 0.91 0.653 1 BL+A+S 0.57 0.93 0.687 1 (II) Partially real data BL 0.28 0.67 0.375 7 BL+A+S 0.37 0.79 0.481 3 (III) Fully real data BL 0.10 0.36 0.156 76 BL+A+S 0.15 0.48 0.226 29 Table 2. Audio-to-sheet snippet retrieval re- sults on three types of datasets: (I) fully syn- thetic, (II) partially real and (III) entirely real. 2.4 Temporal dependencies between subsequent snippets As briefly mentioned at the end of Section 2.3 and im- plied in the previous paragraph, a popular task scenario in the audioâ€“sheet music retrieval realm is cross-modal piece identification: given an unknown music document in one modality (i.e., a full audio recording), we wish to identify which piece is it based on a collection of documents in an- other modality (i.e., a database of scanned sheet images).