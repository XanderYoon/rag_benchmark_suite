2.3 and im- plied in the previous paragraph, a popular task scenario in the audio–sheet music retrieval realm is cross-modal piece identification: given an unknown music document in one modality (i.e., a full audio recording), we wish to identify which piece is it based on a collection of documents in an- other modality (i.e., a database of scanned sheet images). For deep learning-based embedding methods like in [8], choosing how to aggregate snippet embeddings extracted from full documents is essential in order to achieve robust piece identification. The basic identification method proposed in [8] is as fol- lows. Taking the audio-to-sheet search direction without loss of generality, let D be a collection of L sheet music documents, and Q an unknown audio query. Each doc- ument Di ∈ D is segmented into a set of image snip- pets, which are embedded using the sheet music pathway of Figure 1b, generating a set of sheet music embeddings {yi 1, yi 2, ..., yi Mi } for each piece. Analogously, the full audio query is segmented into short spectrogram excerpts, from which a set of query audio embeddings {x1, x2, ..., xN } is computed. Then for each audio snippet query xj, its nearest neighbour among all embedded image snippets is selected via cosine distance. Each retrieved sheet snippet then votes for the piece it originated from, resulting in a ranked list of piece candidates. A limitation of this vote-based identification procedure is that it completely ignores the temporal relationships be- tween subsequent snippet queries, which are inherent in, and constitutive of, music. In [5], a matching strategy is presented that aligns the sequences of embeddings obtained from the query document and search database items. The sequence of embedded snippets {yi 1, yi 2, ..., yi Mi } of each piece Di