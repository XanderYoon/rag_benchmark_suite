Model Variants PrecisionRecall F1 RAGate-Prompt: LLMs – Zero Shot Llama-2-7B 0.1323 0.0278 0.0460 Llama-2-13B 0.1422 0.1083 0.1230 RAGate-Prompt: LLMs – In-Context Learning Llama-2-7B 0.1417 0.0294 0.0487 Llama-2-13B 0.0989 0.0851 0.0915 RAGate-PEFT:Parameter Efficient Fine-tuned LLMs (Llama2-7B) [contx⊕resp] 0.4926 0.3095 0.3802 contx-only 0.5203 0.3359 0.4082 contx-(syn-resp)-ner 0.6818 0.2321 0.3464 contx-(syn-resp)-ner-know 0.4698 0.0603 0.1069 contx-(syn-resp)-ner-source 0.4000 0.0185 0.0355 RAGate-MHA:Context with / without Knowledge Input MHA(contx)-h(4)-l(5)-emb(64) 0.3210 0.5541 0.4065 MHA([contx⊕know])-h(4)-l(2)-emb(64) 0.2795 0.5201 0.3636 MHA(contx×know)-h(4)-l(2)-emb(64) 0.22720.58350.3271 RAGate-MHA:Context-Response Input MHA([contx⊕resp])-h(4)-l(4)-emb(64) 0.3500 0.5510 0.4281 Table 2: Classification accuracy on adaptive augmenta- tion for system response. "contx", "resp", and "know" refer to the use of context, initial system response, and retrieved knowledge snippets as input. "syn-resp" and "ner" are the additional synthetic response and name en- tity recognition steps in the model fine-tuning prompts. h, l and emb refer to the best-performed configuration on the number of heads, layers and embedding size. RAGate performance with LLM prompting ver- sus fine-tuning. By comparing the corresponding performance reported in Table 2, we observe that, on average, fine-tuning a Llama-2-7B with QLoRA (i.e., RAGate-PEFT) can significantly outperform RAGate-Prompt. For example, by looking at the RAG-PEFT with context-only input, without using extra input features and instruction updates, it can outperform all RAG-Prompt approaches by a big margin (e.g., 0.4082 versus the highest 0.1230 F1 scores). This reflects the difficulty of this adap- tive knowledge augmentation task, which can not be properly addressed by prompting a general pre- trained language model. In particular, the use of larger language models and the in-context learn- ing setup, which often result in improved perfor- mance (Arora et al., 2022), can not guarantee the enhancement of models’ classification accuracy re- garding this classification task. Regarding the performance of RAGate-PEFT approaches, by first examining the effect of us- ing synthetic response and recognised name enti- ties, we observe