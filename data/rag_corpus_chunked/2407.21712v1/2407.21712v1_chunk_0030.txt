the quality of the retrieved snippets to address such insufficiency and, based on that, decide on the re- trieval. Although we do not explicitly provide re- trieved snippets to our model, retrieval comes with a corpus that includes potentially relevant knowl- edge snippets. Consequently, given a query and a retrieval collection, it can be estimated whether useful information for the query exists in the corpus to address the insufficient context. To investigate by following this direction, we randomly selected 50 samples from instances where our proposed ap- proach (RAGate-MHA, the best-performing gate model) predicted using retrieval augmentation. We asked domain experts (co-authors) to score whether they thought the retrieved snippets in those scenar- ios could be useful to response generation. Users rated the snippets on a scale of 0 − 4, with scores of 3 or 4 indicating ‘useful’ or ‘highly useful’. We found that in 54% of cases where the prediction was for augmentation, users also found the snippets useful. This indicates that our proposed approach can implicitly capture the potential for obtaining high-quality retrieval snippets. C Additional experimental results about RAGate for Response Generation In Table 4, we include the complete experimental results of applying RAGate for adaptive retrieval- augmented system response generation. Specif- ically, explore the use of retrieved knowledge snippets to different extents of relevance. We in- clude top-3 knowledge snippets retrieved by BERT- ranker and TF-IDF. In addition, we also explore the use of knowledge snippets in different ranking positions (rank 1 and 5) according to the BERT- ranker retriever. The experimental result shows that precisely using a suitable amount of relevant knowledge can generate a response with higher confidence (i.e., less is more). In addition, this observation also indicates the potential use of con- fidence levels to evaluate the quality of the aug- mented