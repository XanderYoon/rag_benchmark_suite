and its three variants. We describe each of these three variants to clarify the use of RAGate: RAGate-Prompt: As denoted by Arora et al. (2022), a language model can effectively adapt to new tasks by using a natural language prompt that explains the process to address the tasks with- out extra training. Hence, we can formulate a gate function f(·) as f(y|ct) = f(y|Θ, ct, p), where Θ denotes the used language model with its pre-trained weights and p is the devised nat- ural language prompt. Alternatively, if the re- trieved knowledge is also involved in prediction, we have f(y|ct) = f(y|Θ, ct, et,k, p). Specifically, we explore two types of prompts: zero-shot and in-context learning. Zero-shot prompts describe the task that uses the conversational context and, optionally, the retrieved knowledge to generate a response with binary feedback. As for the in- context learning prompts, we augment the zero- shot prompts with illustrative examples. We show the set of prompts in Appendix A. RAGate-PEFT: Despite the high adaptabil- ity of the language model with devised prompts, we further explored the use of instruction tun- ing on language models with a parameter-efficient fine-tuning method (i.e., QLoRA (Dettmers et al., 2024)) to meet the goal of an effective gate func- tion. QLoRA is built upon the known Low-rank Adapter (LoRA) (Hu et al., 2021), which keeps the pre-trained weight matrix W0 frozen and addresses the gradient updates of the weight matrix ∆W through low-rank approximation (i.e., ∆W = BA, where B and A are the result of lower-rank de- composition on ∆W ). Hence, the forward pass during the model training can be updated from h = W0x + ∆W x to h = W0x + BAx. QLoRA (Dettmers et al., 2024), which is used in this study, further quantises the language