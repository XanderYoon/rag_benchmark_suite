through low-rank approximation (i.e., ∆W = BA, where B and A are the result of lower-rank de- composition on ∆W ). Hence, the forward pass during the model training can be updated from h = W0x + ∆W x to h = W0x + BAx. QLoRA (Dettmers et al., 2024), which is used in this study, further quantises the language model into a 4-bit NormalFloat data type and leverages the page-to- page transfer between the CPU and GPU to fur- ther avoid memory spikes. To implement RAGate- PEFT, we format the train data with devised in- structions, joined with paired inputs and outputs for developing parameter-efficient fine-tuned large language models. In particular, we provide a set of instruction-input-output triples for model train- ing. The input can vary with the provision of a set of available features. Apart from the use of the conversational context (contx), we also include the system response (resp), synthetic responses gener- ated by the language model (syn-resp) due to the missing responses as input in the practical scenario, the name entities within the incoming responses (ner), retrieved knowledge (know) and the descrip- tion of the knowledge source, e.g., the WikiHow website (source). By using various combinations of inputs and customising the corresponding instruc- tions, we explore the effectiveness of the result- ing learned language models that implement the RAGate-PEFT. RAGate-MHA: Apart from the use of pre- trained language models and further fine-tuned lan- guage models, we also explore the introduction of a multi-head attention neural encoder to model the context as input and estimate the augmenta- tion necessity (i.e., RAGate-MHA). Here, we de- scribe the model structure of RAGate-MHA. At first, as denoted by (Vaswani et al., 2017), the at- tention mechanism is formulated as the interaction between three objects, queries Q, keys K, and val-