are followed by residual connection (He et al., 2016) Retrieval ModelsRecall@1 Recall@3 TF-IDF 0.0227 0.0871 BERT-Ranker 0.2475 0.4714 Table 1: Retrieval Performance Evaluation when using context as the query. and layer normalisation (Ba et al., 2016). Unlike the introduction of another decoder module that addresses the sequence-to-sequence generation in (Vaswani et al., 2017), we followed the encoder out- put with a linear projection module and a softmax function for our binary classification task. 4 Model Training and Evaluation Setups We evaluate the performance of introducing RA- Gate according to its binary classification perfor- mance and the effectiveness of the resulting re- sponse generation. Specifically, we use the KE- TOD dataset (Chen et al., 2022), which has fully annotated 5,324 dialogues and 52,063 turns of con- versations. In particular, it is associated with 33,761 knowledge snippets to be retrieved and augmented. In addition, KETOD was developed with human labels on turns of conversations (around 12.1% of turns) about the need for augmenting with retrieved knowledge snippets for a natural and informative system response. Hence, we use these human labels as natural ground truths when evaluating RAGate. It is worth indicating that many current knowledge- augmented conversational datasets often ground their conversations on the knowledge snippet, such as Wizard of Wikipedia (Dinan et al., 2018) and CMU_DoG (Zhou et al., 2018), which makes them not a natural fit to be investigated in this study. Due to the limited computational resource avail- ability, we explore the use of Llama-v2-7B and Llama-v2-13B to implement RAGate-prompt and fine-tune Llama-v2-7B for RAGate-PEFT. We im- plement QLoRA using the PEFT library (Man- grulkar et al., 2022) and set the lower rank to 16. As discussed in Section 3, we have various input fea- tures to be combined for performance optimisation. We begin with the use of context