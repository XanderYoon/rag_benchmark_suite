u0, s0, .., ut. With this context information ct, the conversation system can augment it with a list of retrieved exter- nal knowledge, et,k, where k represents the rank- ing cutoff for the retrieved knowledge. Hence, the binary gate mechanism proposed in this study, de- ciding the knowledge augmentation, can be formu- lated as f(ct) = {0, 1} or f(ct, et,k) = {0, 1} if the external knowledge et,k is considered. Then, the follow-up response generation functiong(·) can be formulated as follows: g(·) = ( g(ct, et,k) if f(ct) or f(ct, et,k) g(ct) otherwise. (1) Hence, by evaluating and estimating the necessity of augmenting with external knowledge, we dy- namically update the conversational response gen- eration accordingly. 3.2 RAGate Gate Mechanism To effectively estimate the need to use external knowledge and implement adaptive retrieval aug- mented generation for a conversation system, we introduce our proposed gate mechanism, RAGate, that uses the conversational context and, option- ally, the retrieved external knowledge to predict the binary choice of using external knowledge. In par- ticular, we explore three RAGate variants that are implemented by the use of Large Language Mod- els (LLMs) with devised prompts, with parameter efficient fine-tuning (e.g., QLoRA (Dettmers et al., 2024)) and the construction of an end-to-end multi- head attention encoder. This exploration is moti- vated by the recent advancement of transformer- structured neural models in natural language pro- cessing. In Figure 2, we illustrate the application of RAGate and its three variants. We describe each of these three variants to clarify the use of RAGate: RAGate-Prompt: As denoted by Arora et al. (2022), a language model can effectively adapt to new tasks by using a natural language prompt that explains the process to address the tasks with- out extra training. Hence, we can formulate a gate function f(·)