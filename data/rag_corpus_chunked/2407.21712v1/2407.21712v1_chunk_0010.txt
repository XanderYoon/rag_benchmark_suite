also explore the introduction of a multi-head attention neural encoder to model the context as input and estimate the augmenta- tion necessity (i.e., RAGate-MHA). Here, we de- scribe the model structure of RAGate-MHA. At first, as denoted by (Vaswani et al., 2017), the at- tention mechanism is formulated as the interaction between three objects, queries Q, keys K, and val- ues V : Attention(Q, K, V ) = softmax( QKT √dk )V . To estimate the necessity of augmentation, we fit the context and the retrieved knowledge into the roles of these three objects. Specifically, we in- clude the setups of (1) using context only (contx) or (2) using the concatenated context and retrieved knowledge (contx ⊕ know) as queries, keys, and values, and (3) using the context as queries and interact with the retrieved knowledge as keys and values (contx × know). Next, following (Vaswani et al., 2017) in the encoder construction of a trans- former model, we encode the inputs via an input embedding layer into latent vectors and a position encoding layer to encode the order of tokens in the sequence. After that, we leverage the multi-head attention to learn attention weights on the inputs and then followed by a feed-forward network: F F N(x) = max(0, xW1 + b1)W2 + b2 (2) where W1 and W2 are two learned parameter matrics with two bias terms (b1 and b2). Both multi- head attention and feed-forward neural modules are followed by residual connection (He et al., 2016) Retrieval ModelsRecall@1 Recall@3 TF-IDF 0.0227 0.0871 BERT-Ranker 0.2475 0.4714 Table 1: Retrieval Performance Evaluation when using context as the query. and layer normalisation (Ba et al., 2016). Unlike the introduction of another decoder module that addresses the sequence-to-sequence generation in (Vaswani et al., 2017), we followed the encoder out- put with