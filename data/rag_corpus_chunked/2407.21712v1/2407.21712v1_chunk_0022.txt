effectively identify augmen- tation needs. In addition, RAGate can capture hu- man preference by augmenting the beginning turns of conversations, and RAGate can further identify knowledge augmentation for assisting suggestion- making and enriching description. When applying RAGate to conversational systems, we observe that it can ensure comparable quality of generated re- sponses and enable the system to increase genera- tion confidence for faithful outputs, especially with the appropriate use of relevant knowledge snippets. Limitations There are three limitations of this study. At first, due to the main focus of examining the adaptive retrieval-augmented generation for a conversation system. We only consider a few examples of retrieval techniques (TF-IDF and BERT-ranker), which can be further extended to recent retrieval techniques, such as dense passage retrieval for ad- ditional insights. The second limitation is the miss- ing use of larger language models, such as GPT-4, due to the shortage of computational resources. In- cluding larger language models for conversational systems could introduce additional experimental in- sights. The third limitation is the shortage of appro- priate conversational data for extensive evaluations. This is mainly caused by the recent development of the retrieval augmented generation technique and its application to conversational systems. Future research is encouraged to address this limitation. Ethics Statement All experiments in this study were conducted us- ing publicly available datasets and open-released language models, which do not contain any private information that could raise ethical concerns. References Simran Arora, Avanika Narayan, Mayee F Chen, Lau- rel Orr, Neel Guha, Kush Bhatia, Ines Chami, and Christopher Re. 2022. Ask me anything: A sim- ple strategy for prompting language models. In The Eleventh International Conference on Learning Rep- resentations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Zhiyu Chen, Bing Liu, Seungwhan Moon,