to update the KETOD dialogue system (Chen et al., 2022), which uses GPT-2 (Rad- ford et al., 2019) as the backbone model. To high- light the effect of various augmentation setups, we use the context with the gold action without extra prediction as input to KETOD. Then, we compare the resulting performance to the KETOD model without knowledge augmentation and aug- menting every system response as baselines. To report the response generation effectiveness, we report how close the response is to the ground truth via BLEU, ROUGE-1/2/L and BERTScores and the confidence score calculated by the minimum probabilities of individual tokens that compose the response. As argued by Varshney et al. (2023), this calculated confidence score can highly correlate with a language model’s likelihood of generating hallucinated responses. We trained our models and conducted the evaluations on one machine with one NVIDIA 4090 GPU. 5 Results and Analysis 5.1 Augmentation Need Classification First, we evaluate the classification accuracy of our developed RAGate gate methods for addressing the adaptive RAG to system responses. Table 2 presents the classification performance of RAGate baselines while evaluated on the test collection of the KETOD dataset, which includes rich human labels on the use of RAG for response generation. As discussed in Section 3, we explore the devel- opment of RAGate with three variants: the use of LLM prompting (RAGate-Prompt), parameter- efficient fine-tuned LLMs (RAGate-PEFT), and a neural classifier with Multi-Head Attention struc- ture (RAGate-MHA). Model Variants PrecisionRecall F1 RAGate-Prompt: LLMs – Zero Shot Llama-2-7B 0.1323 0.0278 0.0460 Llama-2-13B 0.1422 0.1083 0.1230 RAGate-Prompt: LLMs – In-Context Learning Llama-2-7B 0.1417 0.0294 0.0487 Llama-2-13B 0.0989 0.0851 0.0915 RAGate-PEFT:Parameter Efficient Fine-tuned LLMs (Llama2-7B) [contx⊕resp] 0.4926 0.3095 0.3802 contx-only 0.5203 0.3359 0.4082 contx-(syn-resp)-ner 0.6818 0.2321 0.3464 contx-(syn-resp)-ner-know 0.4698 0.0603 0.1069 contx-(syn-resp)-ner-source 0.4000 0.0185 0.0355 RAGate-MHA:Context with / without Knowledge