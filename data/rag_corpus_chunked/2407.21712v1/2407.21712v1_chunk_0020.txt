a lower con- fidence level can correlate with a higher chance of generating hallucinated responses, which could be caused by the unnecessary use of external knowl- edge. Hence, to investigate the effectiveness of adaptive knowledge augmentation, we examine the impact of using RAGate. According to the reported experimental results in Table 3, the adaptive aug- mented response generation with fewer knowledge snippets can indeed result in a higher confidence level than Aug-All. Moreover, comparing the performance between RAGate and random selections shows that, consid- ering equal numbers (230 or 787 according to the classification with RAGate) of system responses for augmentation, RAGate can further result in a higher quality of generated response. RAGate-MHA even enables results that are comparable to Aug-Allâ€™s response quality, with only 787 turn augmentations instead of all 4964 turns. Specifically, the use of RAGate-PEFT, which identifies 230 turns of sys- tem responses for knowledge augmentation, can even outperform the random baseline that augments 787 system response turns with improved response quality. Apart from the improved response quality, RAGate also enables the conversational model to maintain a high confidence level and ensure faith- ful responses. Indeed, using RAGate-MHA, which augments 787 system responses, only lowers the average confidence score by 0.36%, instead of the 1.65% when randomly selecting an equal number of turns to augment. In addition, considering the use of different qual- ity and amount of knowledge snippets for augmen- tation, we also include the use of the most rele- vant knowledge snippet according to BERT-ranker in Table 3. We observe that the use of different amounts of knowledge snippets in different rele- vance levels has a marginal effect on this learned dialogue system. However, we observe a signif- icant difference in the confidence level. We ob- serve that using only the most relevant knowledge snippet