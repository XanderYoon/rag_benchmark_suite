in the same context and generates meaningful conclusions for universal AR visual search tasks. In future work, we will make our app more adjustable for human eyes and thereby user-friendly by shortening users’ adaptation time. We will also improve our system by adding adaptive support which can react to users’ gaze by giving additional help to users focusing too long on the wrong parts of the environment. Next, we intend to involve more types of visual/audio hints, as well as more modalities of feedback. Meanwhile, more demographic information will be collected and more effort will be made to involve LGBTQ+ participants. We foresee carrying out a more inclusive user study with more metrics for evaluation. We hope that our research will have an impact on industry-based AR setups and configurations empowering people’s capabilities and levels of efficiency in general AR search tasks. ACKNOWLEDGMENTS This work was partly supported by the Norges Forskningsrad (309339, 314578), MediaFutures user partners and Universitetet i Bergen. REFERENCES [1] A. Ajanki, M. Billinghurst, H. Gamper, T. Järvenpää, M. Kandemir, S. Kaski, M. Koskela, M. Kurimo, J. Laaksonen, K. Puolamäki, et al. An augmented reality interface to contextual information. Virtual reality, 15(2-3):161–173, 2011. 3 [2] F. Anderson, T. Grossman, J. Matejka, and G. Fitzmaurice. Youmove: enhancing movement training with an augmented reality mirror. In Proceedings of the 26th annual ACM symposium on User interface software and technology, pp. 311–320, 2013. 2 [3] S. Arevalo Arboleda, F. Rücker, T. Dierks, and J. Gerken. Assisting manipulation and grasping in robot teleoperation with augmented real- ity visual cues. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1–14, 2021. 2, 3 [4] R. Behringer, J. Park, and V . Sundareswaran. Model-based visual tracking for outdoor augmented reality applications. In Proceedings. international symposium on mixed and