same context. • Exploring the effect of combining hints and instant post-task feedback in a same AR context. This paper is organized as follows. Section 2 describes related work of AR with gaze assistance, visual/audio hints, and task feed- back. In Section 3, we introduce the details of our proposed AR approach. We also incorporate the particular use case of the visual book-searching task. The entire user study is presented in Section 4, with two sub-studies conducted. Section 5 contains the results from the study and quantitative data analysis along with the qualitative summary. Discussion with limitations follows in Section 6, and conclusions and future work are elaborated in Section 7. 2 R ELATED WORK 2.1 Visual Search Tasks in AR Visual search tasks are becoming more common in AR because of the capability of projecting additional virtual information on the physical environment. Contreras et al. [10] presented a mobile application with AR encapsulated to admit users to search for desired places, people or events on a university campus. The superiority of AR lay in the fact that it offered certain visual elements that helped users to better locate the required result. Rafiq and colleagues [37] proposed a dynamic AR framework to support an online book- searching task by using mobile augmented data. This framework also introduced a security layer which ensured the protection of sensitive cloud data. Gebhardt et al. [12] utilized gaze movement data to observe the MR object’s label in a visual searching process through a reinforcement learning method. Trepkowski et al. [49] presented a series of simulating experiments to investigate how visual search performance is affected by the field of view and information density in AR, indicating that a significant effect was caused by these two factors. Van Dam et al. [50] studied the cues in