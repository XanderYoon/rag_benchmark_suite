arXiv:2302.01690v2 [cs.HC] 14 Nov 2023 However, even though there is a substantial body of research into how visual and audio hints work in AR tasks, little has focused on search tasks other than exploring the combination of visual and audio hints in one AR framework. In our study, we observe both the separate effect of these two types of hints and exclusively their combined effects in AR searching. The context of a visual book- searching task is engaged in an AR approach with supportive visual or/and audio hints. Gaze assistance, in particular, has a promising role in AR applica- tions owing to its easy, natural, and fast way of involving people in virtual environments [30]. Gaze-assisted techniques and implementa- tions have been demonstrated to efficiently support AR applications by providing better user experiences (UX) [16] and more accurate target selection techniques [15]. Human eyes are easy to track, and gaze implicitly conveys what people are interested in. Thus, eye-tracking approaches are becoming more pervasive in interactive AR devices, which are mainly mobile head-mounted or hand-held displays (HMDs and HHDs) [59], such as Microsoft HoloLens or Magic Leap [35]. Human gaze is utilized for adjusting and adapting virtual information that is being projected onto the real world. The usage of gaze has been extensively demonstrated to be efficient in visual search tasks. Noteworthily, eye gaze recordings have been har- nessed in many AR-led or AR-guided contexts [28,44,56]. However, very little research has investigated the combination of visual/audio hints and human gaze within visual search tasks within the same context. Another imperative aspect of AR is task feedback. At present, there is almost no in-depth research into involving gaze assistance for task feedback. It is crucial for domain users to obtain better task performance in many situations related to AR [9, 46].