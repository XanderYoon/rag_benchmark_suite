objectâ€™s label in a visual searching process through a reinforcement learning method. Trepkowski et al. [49] presented a series of simulating experiments to investigate how visual search performance is affected by the field of view and information density in AR, indicating that a significant effect was caused by these two factors. Van Dam et al. [50] studied the cues in a drone-based AR signal detection task but found no significant differences across AR cue types. Nevertheless, the effects of hints and task feedback continues to be more valued in AR visual searching. 2.2 Hints in AR Numerous researchers have endeavored to bring different modal- ities of hints into AR/XR systems as auxiliary tools for reaching desired results. Of these, visual hints [4, 26, 54, 60, 67, 68] and au- dio hints [18, 22, 47, 68] are the two most common representations used. Arboleda et al. [3] presented augmented visual hints in a robot-involved AR system which aimed to enhance the visual space of the robot operator about the position of the robot gripper in the workspace, where the visual hints were used to improve distance perception and then the manipulation and grasping task performance. For tangible AR, White et al. [54] examined visual hints to en- able discovery, learning, and completion of gestural interaction in a tangible environment. Seven visual hint types were generated: text, diagram, ghost, animation, ghost+animation, ghost+text, and ghost+text+animation. Two decades ago, Sawhney et al. [42] har- nessed audio information to keep users of wearable devices updated with incoming messages and events. Lyons et al. [22] developed an AR game system named "Guided by V oices", which equipped the user with a narrative sound clip that indicated the scenarios encoun- tered and the correct steps to be taken for proceeding. Interestingly and more recently, Mulloni et al.