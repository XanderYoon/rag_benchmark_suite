stuck on the ground). Then they were instructed to put on and calibrate the HMD. Next, participants opened the designated AR app, and underwent the pre-training of a complete book-searching procedure without receiving any hints/feedback. They then used the floating virtual buttons for choosing their specified groups with a voice prompt: "xx groups is selected". • In the second phase, the participants started the first book-searching task by saying "Start one". As the title of a randomly selected book appeared (Figure 1.a) and a built-in timer began (on the top of Figure 1.c), they started searching based on the given title. In this phase, all participants from studies I and II except those from the control groups received either visual/audio hint(s) or the combined set, which pointed out a specified bookcase shelf. The visual hint remained in the real world space regardless of the participants’ position. The audio hint was, however, not repeated. Upon successfully finding the book, the participants completed the first search task by saying "Stop task" to stop the timer while the TCT of this task was measured and noted. Next, only the participants from Study II began the review of their gaze playback by articulating "Playback". On finishing, they filled out a NASA Task Load Index (TLX) questionnaire [13], a widely-used assessment tool for perceived workload. The participants were then directed to stand one metre in front of the second test bookcase with saying "Start two" and "Stop task" to start and complete the second task using the same process. The sign of the book found in each task is when the user successfully visually locate the book by seeing its title, and then stop the timer orally. All of the voice commands given by participants to the device were recognized at first try during the