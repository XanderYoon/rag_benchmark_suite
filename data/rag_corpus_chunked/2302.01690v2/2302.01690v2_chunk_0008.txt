nessed audio information to keep users of wearable devices updated with incoming messages and events. Lyons et al. [22] developed an AR game system named "Guided by V oices", which equipped the user with a narrative sound clip that indicated the scenarios encoun- tered and the correct steps to be taken for proceeding. Interestingly and more recently, Mulloni et al. [24] found that AR can be inte- grated with a mobile navigation system, while audio hints can be advisable for eye-free usage. Cidota et al. [8] compared the effects of automatic visual and audio notifications regarding workspace awareness in AR remote collaborating. However, the integration of visual and audio hints in one AR context is still insufficient. Figure 2: Block diagram: The proposed AR approach with the two core inter-correlated modules: hints (top) and instant post-task feedback (bottom). The AR app builds on the intersection). 2.3 AR with Gaze Assistance Some research has already pioneered gaze-assisted UI to access more contextual information [1, 21, 31, 41]. It has been shown that the optical see-through (OST) HMD which harnesses human gaze with eye-tracking as the interaction metaphor can contribute to efficient results [20]. Interestingly, there are some studies exploring eye- tracking to present menus to aircraft pilots, and adjust their contents based on what the pilot is looking at [33]. In 2005, Curatu et al. [11] proposed a novel conceptualized system adding eye-tracking capabilities to a Head-Mounted Projection Display (HMPD), which was satisfyingly performed from a low-level optical configuration. Three years later, Park et al. [30] pioneered a system which includes a Wearable Augmented Reality System (WARS) to examine their proposition on an experiment in which they selected desirable items in an AR gallery with content mobility. Rivu et al. [40] successfully demonstrated the superiority of eye-tracking, showing that users are