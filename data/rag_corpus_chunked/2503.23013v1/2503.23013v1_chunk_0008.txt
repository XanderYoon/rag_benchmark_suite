when both retrieval methods fail to return relevant content. • Exclusive preference (1.0 or 0.0) when one method yields a perfect result and the other does not. • Proportional weighting when both methods return partially relevant results. For stability and implementation consistency, the final α(q) value is rounded to one decimal place before being applied in the hybrid scoring function. 4.3 Final Score Fusion With the dynamically determined α(q), we compute the final hybrid ranking score by applying the weighted combination to the normalized scores from both retrieval methods: R(q, d) = α(q) · ˜Sdense(q, d) + (1 − α(q)) · ˜SBM25(q, d) (7) Documents are then ranked based on R(q, d), and the top-K results form the final retrieval output Dfinal(q) = {d1, d2, ...,dK} that is passed to the generation component of the RAG system. Through this dynamic approach, DAT effectively overcomes the limitations of fixed- weight hybrid retrieval methods by intelligently adapting to each query’s characteristics. This query-specific adaptation leads to more relevant and accurate retrieval results across diverse query types, enhancing the overall performance of RAG systems. 5 Experiments 5.1 Experimental Setup Datasets and Preprocessing To evaluate the effectiveness and generalizability of our pro- posed method, we conducted experiments on two benchmark datasets: SQuAD (Rajpurkar 5 Preprint. Under review. et al., 2016), a widely used dataset for evaluating retrieval-based question answering in English, and DRCD (Shao et al., 2019), a large-scale traditional Chinese machine reading comprehension dataset. While SQuAD provides a standard benchmark, we include DRCD to examine whether the proposed method can also perform well in a different language setting. As Chinese is one of the most widely spoken languages, DRCD serves as a valuable testbed for assessing the broader applicability of our approach. For each dataset, we constructed an evaluation corpus by randomly sampling