dataset. While SQuAD provides a standard benchmark, we include DRCD to examine whether the proposed method can also perform well in a different language setting. As Chinese is one of the most widely spoken languages, DRCD serves as a valuable testbed for assessing the broader applicability of our approach. For each dataset, we constructed an evaluation corpus by randomly sampling articles from the original document collection. For each selected article, we included all its paragraphs P = {p1, p2, . . .} and the corresponding questions Q = {q1, q2, . . .} such that each question qi ∈ Q is answerable by a span in paragraph pi ∈ P . The sampling process continued until the number of questions approached 3000, stopping before the next sampled article would exceed this threshold. This yields a paragraph corpus Peval ⊂ P and a query set Qeval ⊂ Q, with aligned pairs (qi, pi) forming the ground truth for retrieval. To better focus our evaluation, we identified a subset of queries Qhybrid ⊂ Qeval where hybrid retrieval strategies can actually make a difference. Our analysis revealed that for many queries in Qeval \ Qhybrid, retrieval performance remained identical regardless of the α value used—suggesting these queries were too simple to benefit from hybrid approaches and could be optimally retrieved using either BM25 or dense retrieval alone. In contrast, the Qhybrid subset specifically contains queries where BM25 and dense retrieval produce different rankings, and where the choice of α directly impacts whether the correct document appears at the top position. This hybrid-sensitive subset serves as a focused testbed for evaluating the effectiveness of dynamic weighting strategies in scenarios where hybrid retrieval is truly beneficial. Detailed dataset statistics are summarized in Table 1. Dataset Articles Paragraphs Questions Hybrid-Sensitive SQuAD 13 585 2976 1111 DRCD