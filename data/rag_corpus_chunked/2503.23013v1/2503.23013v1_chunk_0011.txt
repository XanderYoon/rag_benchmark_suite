BM25 scores. For English (SQuAD) datasets, we use standard word tokenization, while for Chinese (DRCD) datasets, we adopt the tokenizer from ckiplab/albert-base-chinese1. The second baseline is Dense Only (α = 1), which ranks paragraphs based on cosine similar- ity between query and paragraph embeddings obtained from the text-embedding-3-large model (OpenAI, 2024c). The third baseline is Fixed Hybrid (α = α∗), a hybrid method that linearly combines BM25 and dense scores with a fixed weighting parameter α∗. For both datasets, we conducted exhaustive grid search over α values from 0 to 1 with a step size of 1https://huggingface.co/ckiplab/albert-base-chinese 6 Preprint. Under review. 0.1, and found that α∗ = 0.6 maximized retrieval accuracy on both validation sets, making it the optimal fixed weighting value for our experiments. Model Implementation For our proposed DAT method, we experiment with three dif- ferent base models to demonstrate the robustness of our approach across various model sizes and architectures: GPT-4o, OpenAI’s model (OpenAI, 2024a) estimated to have ap- proximately 200B parameters (Abacha et al., 2025); GPT-4o-mini, OpenAI’s model (OpenAI, 2024b) estimated to have approximately 8B parameters (Abacha et al., 2025); and DeepSeek- R1-Distill-Qwen-14B, DeepSeek’s 14B parameter open source model (DeepSeek-AI, 2025). 5.2 Results 5.2.1 Complete Dataset Evaluation We first evaluate all methods on the complete datasets Qeval for each dataset. Table 2 shows the accuracy of α selection for both SQuAD and DRCD, measuring how often each method selects the optimal weighting value for a given query. We define the optimal weighting value as the α that produces the highest retrieval ranking for the ground truth paragraph pi given query qi. The results in Table 2 show that DAT variants achieve higher alpha selection accuracy than fixed weighting approaches on both datasets, with GPT-4o reaching 0.9234 accuracy on SQuAD and GPT-4o-mini achieving 0.9013 on DRCD compared