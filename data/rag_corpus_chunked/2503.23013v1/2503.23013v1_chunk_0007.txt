define a scoring function S(q, d) = fLLM(q, d), which returns an effec- tiveness score in the discrete range {0, 1, 2, 3, 4, 5}—with higher values indicating greater effectiveness. The scoring rubric is carefully designed to reflect retrieval effectiveness: • 5 points: Direct hit—the retrieved document directly answers the question. • 3–4 points: Good wrong result—the document is conceptually close to the correct answer, indicating high likelihood that correct answers are nearby. • 1–2 points: Bad wrong result—the document is loosely related but misleading, with low likelihood that correct answers are nearby. • 0 points: Completely off-track—the result is totally unrelated to the query. Our prompting strategy (see Appendix A) guides the LLM to prioritize factual alignment and informational completeness over superficial similarity or stylistic matching. The LLM independently evaluates each of the top-1 documents and assigns scores: Sv(q) = S(q, dv,1) for dense retrieval and Sb(q) = S(q, db,1) for BM25. This decoupled assessment ensures that the relative retrieval effectiveness is directly captured and can inform downstream weighting. 4.2 Dynamic Alpha Calculation Using the LLM-assigned scores, we compute the dynamic weighting coefficientα(q) through a case-aware formulation that ensures robust behavior across various retrieval outcomes: α(q) =    0.5, if Sv(q) = 0 and Sb(q) = 0, 1.0, if Sv(q) = 5 and Sb(q) ̸= 5, 0.0, if Sb(q) = 5 and Sv(q) ̸= 5, Sv(q) Sv(q)+Sb(q) otherwise. (6) This rule-based approach ensures: • Equal weighting (0.5) when both retrieval methods fail to return relevant content. • Exclusive preference (1.0 or 0.0) when one method yields a perfect result and the other does not. • Proportional weighting when both methods return partially relevant results. For stability and implementation consistency, the final α(q) value is rounded to one decimal place before being applied in the hybrid scoring function.