and where the choice of α directly impacts whether the correct document appears at the top position. This hybrid-sensitive subset serves as a focused testbed for evaluating the effectiveness of dynamic weighting strategies in scenarios where hybrid retrieval is truly beneficial. Detailed dataset statistics are summarized in Table 1. Dataset Articles Paragraphs Questions Hybrid-Sensitive SQuAD 13 585 2976 1111 DRCD 318 908 3000 1523 Table 1: Dataset statistics for SQuAD and DRCD evaluation corpora. The retrieval task involves identifying the most relevant paragraph ˆpi ∈ P eval for each query qi ∈ Q eval. A retrieval is considered successful if ˆpi matches the ground truth paragraph pi that contains the answer. Metrics and Evaluation Protocol To evaluate retrieval performance, we use Precision@1 and Mean Reciprocal Rank at 20 (MRR@20). Precision@1 measures the fraction of queries where the correct answer appears as the top-ranked retrieved document. MRR@20 assesses ranking quality by computing the reciprocal rank of the first correct document (up to position 20) and averaging across queries. These metrics effectively quantify retrieval accuracy and ranking effectiveness. We conduct our evaluation in two phases: a Complete Dataset Evaluation on the entire query set Qeval and a more focused Hybrid-Sensitive Analysis on the subset Qhybrid where hybrid retrieval methods can make a meaningful difference. Baseline Methods We compare our proposed DAT framework against several baseline retrieval methods. The first baseline is BM25 Only (α = 0), a sparse retrieval approach using only BM25 scores. For English (SQuAD) datasets, we use standard word tokenization, while for Chinese (DRCD) datasets, we adopt the tokenizer from ckiplab/albert-base-chinese1. The second baseline is Dense Only (α = 1), which ranks paragraphs based on cosine similar- ity between query and paragraph embeddings obtained from the text-embedding-3-large model (OpenAI, 2024c). The third baseline is Fixed Hybrid (α = α∗),