generated using aligned visual action of top retrieved document Figure 3: Example application architecture for integrating aligned video captions to enabled video enriched RAG (2) The selected query engine tool vectorizes the query and searches the vector database to retrieve (chunked) aligned video caption text blobs. (3) The query engine tool interprets the results and summarizes into a specific pydantic format customized for that answer type; for example a "how to" response should respond with a bulleted list of steps like in Figure 1, whereas a "place" response would describe a location and why it is notable. Timestamps in retrieved docs help give the application point- ers to specific parts of video to enhance user interaction 5 CLOSING REMARKS In this study we show that aligned visual captions provide a com- pelling and adaptable representation of video information that can Video Enriched Retrieval Augmented Generation Using Aligned Video Captions MRR 2024, July 18, 2024, Washington, DC easily plug into basic LLM application architectures. We curate a large scale dataset, demonstrate how to leverage the data represen- tation to generate questions, and offer an automated procedure for measuring video RAG based question answering results. This work gives us a glimpse into the potential of using aligned video captions representation, and is ripe for future exploration. For example, a key practical consideration in deploying this solution in the real world is the availability of video captioning models suitable for the intended use case. Another thing to consider is how one identifies meaningful video clip segments to be summarized by the captioning models in the first place. In future works it would be interesting to study how generic video captioning and clip seg- mentation methods fare on different video domains (e.g. general knowledge vs. surveillance, etc.) and contrast those with strategies that adapt