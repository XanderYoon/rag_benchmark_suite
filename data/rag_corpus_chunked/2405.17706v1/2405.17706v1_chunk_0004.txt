a sanity check we checked how seman- tically similar video summarizations generated by various LLMs were to those generated by GPT-4 Turbo using the aligned video caption transcript. We compared these generated summaries using BERTScore [12], which is an automatic summarization measure has been shown to correlate with human judgment on sentence-level and system-level evaluation. A total of 1.5K videos were summa- rized and evaluated, sampled uniformly from the original dataset. In Table 2 we can see that the various configurations correlate significantly with the GPT-4 based ground truth. In particular we see that sending raw video frames and the automatic speech recognition (ASR) transcript to the GPT-4 scores a high BERTScore; so does the text only based settings using ASR, suggesting much of the information that the LLM is able to tap into resides in speech. Additionally we see that summarizations using Gemini 1.5 Pro with video based input and GPT 4 using video frames (i.e. first frame per scene) as input have similar scores as well, showing that these captions can produce similar quality output with having to send the entire set of frames to the LLM, greatly saving on context window and processing bandwidth at query time. For example, if the entire aligned video caption dataset was sampled at 1 frame per second (as is the case for popular LLMs like Gemini 1.5 pro) and assuming an image is resized to roughly fit the cost of 256 tokens for the LLM, youâ€™re looking at around 4.8 billion tokens including subtitles (roughly 69x bigger compared to using aligned visual captions). Note that Gemini 1.5 pro did not process audio signals in videos at the time this study was conducted. 3 VIDEO RETRIEV AL AUGMENTED GENERATION In this section we determine if text embeddings over video derived data is