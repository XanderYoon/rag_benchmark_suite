tokens for the LLM, you’re looking at around 4.8 billion tokens including subtitles (roughly 69x bigger compared to using aligned visual captions). Note that Gemini 1.5 pro did not process audio signals in videos at the time this study was conducted. 3 VIDEO RETRIEV AL AUGMENTED GENERATION In this section we determine if text embeddings over video derived data is feasible input for retrieval augmented generation, over the task of answering a provided general knowledge question using answers found in videos as support. In this experiment we use 1000 general knowledge questions generated via GPT 4 V as input to an embedding extractor. We also compare retrieval results using two multimodal embeddings, namely BLIP-2’s [3] image feature extractor and CLIP [8] embeddings (ViT-L/14@336px). Then we retrieve the top K results as determined by a simple cosine similarity Video Enriched Retrieval Augmented Generation Using Aligned Video Captions MRR 2024, July 18, 2024, Washington, DC Figure 2: Sample aligned video caption transcript with corresponding example video frames from source scenes DATASET DIMENSION TOTAL MEDIAN Video Count 29,259 - Scene Count 1,476,462 31.00 Video Duration (seconds) 18,584,396 478.00 Text Character Length Title 1,548,810 51.00 Description 30,565,705 780.00 Title + Description 32,114,515 833.00 Visual Video Captions 96,888,187 2,016.00 Subtitles / ASR 141,926,062 3,472.00 Aligned Captions 276,019,918 6,461.00 Table 1: Statistics for Aligned Video Caption Dataset LLM PROMPT CONTEXT BERT Multimodal LLMs GPT 4 V Video Frames + ASR Transcript 0.889 Gemini 1.5 Pro Original Video 0.862 GPT4 V Video Frames 0.860 GPT 4 Turbo Varying Text Input GPT 4 ASR Transcript 0.893 GPT 4 Visual Captions 0.869 GPT 4 Title + Description 0.858 Table 2: Generated video summary comparison against GPT 4 aligned visual captions based generation against the entire 29K video dataset. Using the top K results we use GPT-4 as an automatic