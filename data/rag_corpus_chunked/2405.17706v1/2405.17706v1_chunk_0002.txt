[9] where the authors introduce a retrieval augmented multimodal captioning model that retrieves relevant exocentric videos as ref- erences to generate the captions for egocentric videos, also in [7] they use retrieved text to generate answers for questions about an input video. Additionally in [11] the authors improve multimodal query (image + text) to image retrieval using a large scale (query image, instruction, target image) triplet dataset. Those respective applications are great, but in this work we focus on bringing the context of a large video corpus itself into responses of a retrieval augmented generation chat bot setting. One potential reason for relatively few works in this space is that accessing videos in a large scale can be a daunting engineering en- deavor, given video information’s relative large size and multimodal nature. Another potential reason for a lack of related works can be attributed to the relative difficulty of collecting video data, and further compounded by the time consuming effort of evaluating the retrieval and generation stages’ output manually. In this work, we propose the use of "aligned visual caption" transcripts (see example in Figure 2) in the context of a chat assis- tant. In Section 2 we detail the process of preparing aligned video captions, then describe a video data set we curated for this work, and provide commentary on how these compare to using different signals from videos in conjunction with popular LLMs under the task of video summarization as a proxy for the model’s capabili- ties in general video understanding. Then in Section 3 we describe an experiment that aims to automatically measure feasibility of using these transcripts in a retrieval augmented generation con- text. Then in Section 4 we describe a sample AI chat application architecture that leverages the aligned video caption representation of videos to