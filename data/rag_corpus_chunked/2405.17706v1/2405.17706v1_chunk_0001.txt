foundational model / captioner for particular visual details or fine tuning. In hopes of helping advancing progress in this area, we curate a dataset and describe automatic evaluation procedures on common RAG tasks. CCS CONCEPTS •Information systems → Information retrieval; •Comput- ing methodologies → Visual content-based indexing and re- trieval. arXiv:2405.17706v1 [cs.AI] 27 May 2024 MRR 2024, July 18, 2024, Washington, DC Kevin Dela Rosa KEYWORDS Retrieval Augmented Generation, Cross-modal Retrieval, Multi- modal Retrieval, Large Language Model Applications, Chatbots ACM Reference Format: Kevin Dela Rosa. 2024. Video Enriched Retrieval Augmented Generation Using Aligned Video Captions. In Proceedings of the 2024 SIGIR Workshop on Multimodal Representation and Retrieval (MRR 2024). ACM, New York, NY, USA, 5 pages. 1 INTRODUCTION Video content in the forms of YouTube shorts, TikToks, Instagram Reels or the like are quickly becoming many people’s main form of content ingestion online. At the same time, following the initial deluge of work on large language models there has been a recent surge in work to understand videos, with big corporate systems like OpenAI GPT-4 Vision and Google Gemini incorporating basic image and video chatting capabilities into their chat AI applications, as well as academic systems like [6] [5], [10], [4]. Surprisingly while there are many works that touch on video un- derstanding at various levels, there have been relatively few works that have used videos in a retrieval augmented generation (RAG) [2] context; some notable related works include EgoInstructor [9] where the authors introduce a retrieval augmented multimodal captioning model that retrieves relevant exocentric videos as ref- erences to generate the captions for egocentric videos, also in [7] they use retrieved text to generate answers for questions about an input video. Additionally in [11] the authors improve multimodal query (image + text) to image retrieval using a large scale