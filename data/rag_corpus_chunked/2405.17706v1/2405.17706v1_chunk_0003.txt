summarization as a proxy for the model’s capabili- ties in general video understanding. Then in Section 3 we describe an experiment that aims to automatically measure feasibility of using these transcripts in a retrieval augmented generation con- text. Then in Section 4 we describe a sample AI chat application architecture that leverages the aligned video caption representation of videos to illustrate the ease of integration. For sample demo application, LLM prompts, evaluation scripts and dataset pointers, see: https://github.com/kdr/videoRAG-mrr2024 2 ALIGNED VIDEO CAPTIONS "Aligned Video Caption Transcripts" are temporally synced scene descriptions of a video in the form of machine generated visual captions and the associated subtitles or automatic speech recogni- tion transcripts. In this study we curated a dataset based on public youtube videos sampled from Panda-70M [1], which contains in- dividual clip segments and a general visual scene caption learned from a set of open source video captioning models. Specifically we sampled roughly 2,000 videos from each YouTube category present in Panda-70M [ 1], resulting in a dataset of 29,259 videos (1.5M video clips and corresponding visual captions) or roughly 215 days of footage. We then augment that dataset with subtitles gathered directly from YouTube’s APIs and created the aligned transcripts as seen in Figure 2). General statistics shown in Table 1. In order to verify that the information an LLM can generate from an aligned video caption transcript is roughy comparable to that of a multimodal LLM, as a sanity check we checked how seman- tically similar video summarizations generated by various LLMs were to those generated by GPT-4 Turbo using the aligned video caption transcript. We compared these generated summaries using BERTScore [12], which is an automatic summarization measure has been shown to correlate with human judgment on sentence-level and system-level evaluation. A total of 1.5K videos