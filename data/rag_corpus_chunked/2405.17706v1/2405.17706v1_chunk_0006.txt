Video 0.862 GPT4 V Video Frames 0.860 GPT 4 Turbo Varying Text Input GPT 4 ASR Transcript 0.893 GPT 4 Visual Captions 0.869 GPT 4 Title + Description 0.858 Table 2: Generated video summary comparison against GPT 4 aligned visual captions based generation against the entire 29K video dataset. Using the top K results we use GPT-4 as an automatic judge using the following metrics: • HIT@K: in the top K retrieved results, does any retrieved document contain the information required to answer the posed question. We use this in lieu of recall given the diffi- culty of manually collecting ground truth over every video (also answers are free form sentences and can’t simply be checked for existence via basic string comparisons) • QUALITY@1: answer correctness / quality rating between 1-10, measuring quality of answers generated by GPT-3.5 turbo. In order to control for compounding factors due to the provided context in the LLM prompt, all answers were generated using the aligned video caption transcript of the retrieved result regardless of retrieval method To generate the questions we first sampled 500 videos from the dataset, then provided the aligned video captions as context to GPT 4 and asked the LLM to generate general knowledge questions that the video could help answer but are not specifically tied to the source video, and from the resulting question set we uniformly sampled 1000 questions. In Table 3 we can see that the text embeddings are able to find hits at a relatively low K using the aligned transcript and ASR. We also see that the relevance of results at very low K suffers for the cross- modal embedding configuration, but can ultimately catch up if you have a tolerance for higher K, i.e. LLM can handle processing more retrieved documents in its context