(Palangi et al., 2016) is also composed of two LSTM, where M is the cosine similarity function. In order to capture richer con- text, bidirectional LSTM (bi-LSTM) (Schuster & Paliwal, 1997) utilizes both previous and future contexts by processing the sequence data from two directions using two LSTM. Bi- LSTM is used in MV-LSTM (Wan et al., 2016a) to capture the semantic matching in each (3)F(q,d)=( NNQ(q),NND(d)) M(q,d)= cosine(NNQ(q),NND(d)); or M(q,d)= MLP([NNQ(q);NND(d)]) Information Retrieval Journal 1 3 position of the document and query by generating positional sentence representations. The next step in MV-LSTM is to model the interactions between the generated features using the tensor layer (Socher et al., 2013b, c). The matching between query and document is usually captured by extracting the strongest signals. Therefore, k-max pooling (Kalchbren- ner et al., 2014) is used to extract the top k strongest interactions in the tensor layer. Then, a MLP is used to calculate the relevance score. 6.1.2 Interactionâ€‘focused models Models in the representation-focused group defer the interaction between two inputs until extracting individual features, so that there is a risk of missing important matching signals in the document retrieval task. The interaction-based models start by building local interac- tions for a query-document pair using simple representations, then train a deep model to extract the important interaction patterns between the query and document. The general framework for interaction-focused models is shown in Fig. 3. The interaction-based models capture matching signals between query and document in an early stage. In interaction-focused models, F captures the interactions between query and document. For example, Guo et al. (2016) introduced a Deep Relevance Matching Model (DRMM) to perform term matching using histogram-based features. The interaction matrix between query and document is computed using pairwise cosine similarities between the embed- dings of query tokens and document