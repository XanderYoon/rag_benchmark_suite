probability of a given label l: The parameters of BERT, denoted by /u1D703 , and the softmax layer parameters W are fine-tuned by maximizing the log-probability of the true label. 2.8 Knowledge graphs Large scale general domain knowledge bases (KB), like Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al., 2015), contain rich semantics that can be used to improve the results of multiple tasks in natural language processing and information retrieval. Knowledge bases contain human knowledge about entities, classes, relations, and descrip- tions. Knowledge can be represented as graphs and this leads to the concept of knowledge graphs (KG). Various methods have been proposed for representation learning of knowl- edge graphs, which aims to project entities and relations into a continuous space. TransE (Bordes et al., 2013), inspired by Word2Vec (Mikolov et al., 2013b), is the most repre- sentative translation-based model, which considers the translation operation between head and tail entities for relations. The variants of TransE, such as TransH (Wang et al., 2014) and TransR (Lin et al., 2015), follow a similar principle but use different scoring func- tions to learn the embeddings. Socher et al. (2013a) apply neural tensor networks to learn knowledge graph embeddings. Dettmers et al. (2018) propose a convolutional neural net- work approach to learn knowledge graph embeddings and use them to perform link predic- tion. RDF2Vec (Ristoski & Paulheim, 2016) adapts the Word2Vec (Mikolov et al., 2013b) approach to RDF graphs in order to learn embeddings for entities in RDF graphs. Recently, researchers have explored a new direction called graph neural networks (Cai et al., 2017) to solve multiple tasks (Zhou et al., 2018). Graph neural networks capture rich relational structures between nodes in a graph using message passing and encode the global structure of a graph in low dimensional feature vectors