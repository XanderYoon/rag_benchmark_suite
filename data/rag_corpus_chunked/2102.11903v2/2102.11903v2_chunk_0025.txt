feed-forward networks in the Siamese architecture. The feature extractor F is composed of a CNN that is applied to a letter-trigram input representation, then a max-pooling layer is used to form a global feature vector, while M is the cosine similarity function. CNNs have also been used in ARC-I (Hu et al., 2014) to extract feature representations of the query and document. Each layer of ARC-I contains convolutional filters and max-pooling. The input to ARC-I is any pre-trained word embedding. In order to decrease the dimension of representation, and to filter low signals, a max-pooling of size two is applied for each feature map. After applying several layers of CNN filters and max-pooling, ARC-I forms a final feature vector NNQ(q) and NND(d) for query and document, respectively ( NNQ and NND are identical because ARC-I follows the Siamese architecture). NNQ(q) and NND(d) are concatenated and fed to a MLP to predict the relevance score. CNN is also the main component in the deep neural networks introduced in Convolutional Neural Tensor Net- work (Qiu & Huang, 2015) and Convolutional Latent Semantic Model (Shen et al., 2014a). Recurrent neural networks (RNN), especially the Long Short-Term Memory (LSTM) model (Hochreiter & Schmidhuber, 1997), have been successful in learning to represent each sentence as a fixed-length feature vector. Mueller and Thyagarajan (2016) proposed Manhattan LSTM (MaLSTM) which is composed of two LSTM models as feature extrac- tors. M is a simple similarity measure. LSTM-RNN (Palangi et al., 2016) is also composed of two LSTM, where M is the cosine similarity function. In order to capture richer con- text, bidirectional LSTM (bi-LSTM) (Schuster & Paliwal, 1997) utilizes both previous and future contexts by processing the sequence data from two directions using two LSTM. Bi- LSTM is used in MV-LSTM (Wan et al., 2016a) to capture