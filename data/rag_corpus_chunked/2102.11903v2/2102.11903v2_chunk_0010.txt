Attention mechanism The attention mechanism was first proposed by Bahdanau et al., (2015) for neural machine translation. The original Seq2Seq model (Sutskever et al., 2014) used an LSTM to encode a sentence from its source language and another LSTM to decode the sentence into a tar - get language. However, this approach was unable to capture long-term dependencies. In order to solve this problem, Bahdanau et al., (2015) proposed to simultaneously learn to align and translate the text. They learn attention weights which can produce context vec- tors that focus on a set of positions in a source sentence when predicting a target word. The attention vector is computed using a weighted sum of all the hidden states of an input sequence, where a given attention weight indicates the importance of a token from the source sequence in the attention vector of a token from the output sequence. Although introduced for machine translation, the attention mechanism has been a useful tool in many tasks such as document retrieval (McDonald et al., 2018), document classification (Yang Information Retrieval Journal 1 3 et al., 2016b), sentiment classification (Wang et al., 2017a), recommender systems (Ying et al., 2018), speech recognition (Chan et al., 2016), and visual question answering (Lu et al., 2016). 2.6 Word embedding Words are embedded into low dimensional real-valued vectors based on the distributional hypothesis (Harris, 1954). In many proposed models, the context is defined as the words that precede and follow a given target word in a fixed window (Bengio et al., 2003; Mnih & Hinton, 2007; Mikolov et al., 2013a; Pennington et al., 2014b). Mikolov et al., (2013b) proposed the Skip-gram model which scales to a corpora with billions of words. These pre-trained word embeddings are a key component in multiple models in neural language understanding and information