an exact match in a given document, so it ignores query tokens that have similar meaning to a word in a document. Fig. 5 Query-centric assumption. The sentences that are used in this example are extracted from Wikipe- dia. Each query token is shown with a different color that corresponds to the query-centric context in the document. A binary judgement is shown to indicate the relevance between the query and the query-centric context, where red cross denotes not relevant assessment, and the green check mark denotes relevant assess- ment. The final relevance score is the aggregation of the query-centric relevance scores Information Retrieval Journal 1 3 Co-PACRR (Hui et al., 2018) is an alternative to DeepRank that computes the con- text of each word in a document by averaging the word embeddings of a window around the word, and the meaning of a query by averaging all word embeddings of the query. So, in order to decrease the number of false signals in Co-PACRR due to ambiguity, the extracted matching signals at a given position in a document are adjusted using the simi- larity between query meaning and context vector of a token in the document. Co-PACRR uses a cascade k-max pooling instead of k-max pooling, which consists of applying k-max pooling at multiple positions of a document in order to capture the information about the locations of matches for documents in the scope hypothesis category. Although the loca- tion of matching signals in a document is important, the sequential order of query tokens is ignored in Co-PACRR. It shuffles the feature map with respect to query tokens before feeding the flattened result to dense layers. Shuffling enables the model to avoid learning a signal related to the query term position after extracting n-gram matching patterns. For example, we mentioned