in ad-hoc retrieval because understanding the semantic similarity between sentences in text matching can enhance retrieval results mainly for sentence or passage level document retrieval scenarios. So, in addition to the neural ranking models that are introduced spe- cifically for retrieval tasks, we will review multiple text matching-based neural models that can be applied to document retrieval. Existing surveys on neural ranking models focus on the embedding layer that maps tokens to embedding vectors known as word embeddings. Onal et al. (2017) classified existing pub- lications based on the IR tasks. For each task, the authors discussed how to integrate word embeddings in neural ranking models. In particular, the authors proposed two categories based on how the word embedding is used. For the first category, the neural ranking models use a pre-trained word embedding to aggregate embeddings with average or sum of word embed- dings, or to compute cosine similarities between word embeddings. The second category includes end-to-end neural ranking models where the word embedding is learned or updated while training the neural ranking model. Mitra and Craswell (2018) presented a tutorial for document retrieval with a focus on traditional word embedding techniques such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990), word2vec (Mikolov et al., 2013a), Glove (Pennington et al., 2014a), and paragraph2vec (Le & Mikolov, 2014). The authors reviewed multiple neural toolkits as part of the tutorial and a few deep neural models for IR. Guo et al. Information Retrieval Journal 1 3 (2019) reviewed the learning strategies and the major architectures of neural ranking mod- els. Lin et al. (2020) focused mainly on pretrained Transformers (Vaswani et al., 2017) for text ranking, where they showed that a BERT-based multi-stage ranking model is a potential choice for a tradeoff between effectiveness and efficiency of a neural ranking