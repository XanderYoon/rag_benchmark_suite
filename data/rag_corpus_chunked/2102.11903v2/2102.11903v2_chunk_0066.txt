retrieval tasks. This is empirically justified by achieving significant improvements in retrieval results when using neural ranking models that guarantee both matching signals. For example, the joint model, proposed by MacAvaney et al. (2019), combines the repre- sentation of [CLS] from BERT and existing relevance-based neural ranking models. This model has a semantic matching signal from [CLS] because BERT is pretrained on the next sentence prediction, and a relevance matching signal from existing neural ranking models. Both the semantic and relevance components are interaction-based neural architectures that generally lead to better results than the representation-based architectures in information retrieval tasks (Nie et al., 2018). The disadvantage of using the BERT model as a semantic matching component is the length limit of BERT which causes difficulties in both training and inference. In general, the length of a document exceeds the maximum length limit of BERT, so that the document is divided into sentences or passages. Splitting the document and then aggregating the relevance scores increases the training and inference time. 9.2 Choice of embedding: how to compute query and document representations? In addition to semantic and relevance matching signals, the context-sensitive embedding was shown to have better retrieval results than traditional pretrained embeddings like Information Retrieval Journal 1 3 Glove. A part of using context-sensitive embedding is to incorporate the query context into the ranking model to improve the precision of ad-hoc retrieval. Recent neural rank - ing models use deep contextualized pre-trained language models to compute a contextual representation for each token. There are mainly two advantages from using these models; first, they are bidirectional language representations, in contrast to only left-to-right or right-to-left language models so that every token can attend to previous and next tokens to incorporate the context from both directions. Second, they contain the attention mechanism