of matches between query and document tokens. The first type is the context-free match where the similarity between a given document token and query token is computed regardless of the context of the tokens. Examples of the context- free match are the cosine similarity between a query token embedding and a document token embedding, and exact matching. The second type is the context-sensitive match where the context of a given document token is matched against the context of a query token. In order to capture contextual information of a given token in document and query when calculating the matching similarity, a context-sensitive word embedding can be used to encode tokens. When a query has many context-sensitive matches with a given docu- ment, it is likely that the document is relevant to the query. The idea of context-sensitive matching is incorporated into the neural ranking model which is proposed by McDonald et al. (2018) to extend the DRMM model by using bi-LSTM to obtain context-sensitive embedding, and to capture signals of high density context-sensitive matches. Matching the contexts can be in two directions: matching the overall context of a query against each token of a document, and matching the overall context of a document against each token of a query. To match the contextual representation of the query and document in the two directions, a neural ranking model defines a matching function to compute the similarity between contexts. Wang et al. (2017b) proposed a Bilateral Multi-Perspective Matching (BiMPM) model to match contexts of two sentences. After encoding each token in a given sentence using the bi-LSTM network, four matching strategies are used to com- pare the contextual information. The proposed matching strategies differ on how to aggre- gate the contextual information of the first input, which is matched against each time