important word at each step, taking into consideration the learned information. The selection of important words in the DRr unit is also based on attention weights. Using multiple attention functions in matching with a word-by-word attention (Rock - täschel et al., 2016) can better capture the interactions between the two inputs. Multiple attention functions are proposed in the literature: e.g., bilinear attention function (Chen et al., 2016) and concatenated attention function (Rocktäschel et al., 2016). The bilinear attention is based on computing the attention score between the representations of two tokens using the bilinear function. The concatenated attention starts by summing the two words’ representations, and then uses vector multiplication to compute attention weight. Tan et al. (2018) propose a Multiway Attention Network (MwAN) using multiple atten- tion functions for semantic matching. In addition to the bilinear and concatenated attention functions, MwAN uses two other attention functions which are the element-wise dot prod- uct and difference of two vectors. The matching signals from multiple attention functions are aggregated using a bi-directional GRU network and a second concatenated attention Information Retrieval Journal 1 3 mechanism to combine the four representations. The prediction layer is composed of two attention layers to output the final feature vector that is fed to a MLP in order to obtain the final relevance score. The use of multiple functions in attention is not limited to the calculation of attention weights. Multiple functions can be used to compare the embedding of a given token with its context generated using the attention mechanism. Wang and Jiang (2017) use several comparison methods in order to match the embedding of a token and its context. These comparison methods include neural network layer, neural tensor network (Socher et al., 2013d), Euclidean distance, cosine similarity, and element-wise operations for vectors. In addition