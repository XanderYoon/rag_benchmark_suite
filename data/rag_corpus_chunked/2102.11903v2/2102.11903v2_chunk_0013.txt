Information Retrieval Journal 1 3 token in the input. BERT is trained on large collections of unlabeled data over two pre- training tasks which are next sentence prediction and the masked language model. After the pre-training phase, BERT can be used for downstream tasks on single text or text pairs using special tokens ([SEP] and [CLS]) that are added into the input. For single text clas- sification, [CLS] and [SEP] are added to the beginning and the end of the sequence, respec- tively. For text pairs-based applications, BERT encodes the text pairs using bidirectional cross attention between the two sentences. In this case, the text pair is concatenated using [SEP], and then BERT treats the concatenated text as a single text. The sentence pair clas- sification setting is used to solve multiple tasks in information retrieval including document retrieval (Dai & Callan, 2019; Nogueira et al., 2019; Yang et al., 2019a), passage re-rank- ing (Nogueira & Cho, 2019), frequently asked question retrieval (Sakata et al., 2019), table retrieval (Chen et al., 2020b), and semantic labeling (Trabelsi et al., 2020a). The single sentence setting is used for text classification (Sun et al., 2019; Yu et al., 2019). BERT takes the final hidden state /u1D421/u1D703 of the first token [CLS] as the representation of the whole input sequence, where /u1D703 denotes the parameters of BERT. Then, a simple softmax layer, with parameters W, is added on top of BERT to predict the probability of a given label l: The parameters of BERT, denoted by /u1D703 , and the softmax layer parameters W are fine-tuned by maximizing the log-probability of the true label. 2.8 Knowledge graphs Large scale general domain knowledge bases (KB), like Freebase (Bollacker et al., 2008) and DBpedia (Lehmann et al., 2015), contain rich semantics that can be used to