1 shows the main IR features of each neural ranking model from all proposed catego- ries. The neural ranking models are sorted in chronological order based on the publication year. Unsurprisingly, in recent years, there have been more proposed neural ranking models that are based on BERT because deep contextualized language models achieve state-of-the- art results in multiple tasks for NLP and IR. Later in the discussion part, we will discuss more research directions to reduce the time and memory complexity of BERT-based rank - ing models. Except for DUET (Mitra et al., 2017), all the neural ranking models have either the interaction or representation feature. Recent proposed methods are interaction-based ranking models that prefer building the interactions between query and document in an early stage to capture matching signals. As mentioned in Sect. 6, DUET combines a representation-based model without weights shar- ing, with an interaction-based model to predict the final query-document relevance score. Given the recent advances in the embedding layer for deep learning models, early injec- tion of contextual information is a common design choice for multiple neural ranking mod- els. The contextual information is incorporated from the first stage which consists of an embedding layer either by using traditional neural recurrent components such as LSTM or more advanced deep contextualized representations, such as the Transformer. In general, the models that are proposed primarily for text matching tasks are symmetric because the inputs are homogeneous. On the other hand, many models that are proposed primarily for document retrieval are not symmetric because there are special computations that are applied only to the query or the document. For example, kernel pooling is used in Conv-KNRM (Dai et al., 2018) to summarize the similarities between a given query token and all document tokens. So, this can be seen as