(2016) use bi-LSTMs for context modelling of text inputs. So, each input is encoded to hidden states using weight-shared bi-LSTMs. In the ranking model proposed by Jaech et al., (2017), two independent bi-LSTMs without weight sharing are used to map query and document to hidden states. The query and document have different sequential struc- tures and vocabulary which motivates encoding each sequence with independent LSTM. Fan et al., (2018) proposed a variant of the HiNT model that accumulates the signals from each passage in the document sequentially. In order to achieve that, the feature vectors of all passages are fed to an LSTM model to generate a hidden state for each passage. Then, a dimension-wise k-max pooling layer is applied to select top-k signals. GRU-based ranking models: A 2-D GRU is an extension of GRU for two-dimensional data such as an interaction matrix. It scans the input data from top-left to bottom-right (and bottom-right to top-left in case of bidirectional 2D-GRU) recursively. A 2-D GRU is used in Match-SRNN (Wan et al., 2016b) to accumulate the matching signals. In the neural ranking model that is proposed by Fan et al. (2018), given query-document interaction tensors representing semantic and exact matching signals, a spatial GRU is used to extract the relevance matching evidence. GRU is applied to multiple passages in a docu- ment in order to form the matching signal feature vectors. Then, k-max pooling extracts the strongest signals from all passages of a document. As in Match-SRNN (Wan et al., 2016b) and MatchPyramid (Pang et al., 2016a, b), DeepRank (Pang et al., 2017) has an input interaction tensor between query and document. The input tensor is fed to the GRU network to compute a query-centric feature vector. CNN-based ranking models: A CNN is used in multiple interaction focused models including