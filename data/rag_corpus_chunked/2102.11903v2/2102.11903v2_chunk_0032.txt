Match-Tensor that explores multiple channel representation for the interaction tensor to capture rich signals when com- puting query-document relevance scores. The similarity between query and document is computed for each channel. Two bi-LSTMs are used to encode the word embedding-based representation of the query and document into LSTM states. The encoded sequences cap- ture the sequential structure of query and document. A 3-D tensor (query, document, and channel dimensions) is then calculated by point-wise product for each query term repre- sentation and each document term representation to produce multiple match channels. A set of convolutional filters is then applied to the 3-D tensor in order to predict the query- document relevance score. The idea of n-gram soft matching is further investigated by Dai et al. (2018) in the Conv-KNRM model. CNN filters are used to compose n-grams from query and docu- ment. This leads to an embedding for each n-gram in the query and document. The n-gram embeddings are then fed to a cross-match layer in order to calculate the cosine similarities between query and document n-grams. Similar to Xiong et al. (2017b), kernel pooling is used to build soft matching feature vectors from the cosine similarity matrices. MLP-based ranking models: The objective of ranking models is to predict a real-valued relevance score for a given query-document pair. So, most of the proposed neural archi- tectures contain a MLP layer that is used to map the final feature vector into a real-valued score. In general, the MLP used in neural ranking architectures is a nonlinear function. 6.1.3 Representation+interaction models Retrieval models can benefit from both representation and interaction deep architectures in a combined model. In DUET (Mitra et al., 2017), an interaction-based network, called the local model, and a representation-based network, called the distributed model, are com- bined in a