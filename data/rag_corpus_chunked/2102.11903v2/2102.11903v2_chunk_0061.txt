image to retrieve images that are rel- evant to a keyword-based query. The objective of the annotations is to describe the content of the image so that a large collection of images can be organized and indexed for retrieval. Text-based image retrieval is treated as a text-based information retrieval. With the large increase of image datasets and repositories, describing each image content with textual fea- tures becomes more difficult, which has led to low precision for text-based image retrieval. In general, it is hard to accurately describe an image using only a few keywords, and it is common to have inconsistencies between image annotations and a user’s query. In order to overcome the limitations of text-based methods, content-based image retrieval (CBIR) (Dharani & Aroquiaraj, 2013; Wan et al., 2015; Zhou et al., 2017) meth- ods retrieve relevant images based on the actual content of the image. In other words, CBIR consists of retrieving similar images to the user’s query image. This is known as the query-by-example setting. An example of a search engine for CBIR is the reverse image search introduced by Google. As in representation-focused models for document retrieval, many CBIR neural ranking models (Wiggers et al., 2019; Chung & Weng, 2017) use a deep neural network as a feature extractor to map both the query image and a given candi- date from the image collection into fixed-length representations. So, these neural ranking models have the Siamese feature. Then, a simple ranking function, such as cosine similar - ity, is used to predict the relevance score of a query-image pair. The Siamese architecture is used in multiple CBIR domains such as retrieving aerial images from satellites (Khokhlova et al., 2020) and content-based medical image retrieval (CBMIR) (Chung & Weng, 2017). CBMIR helps clinicians in the diagnosis by exploring similar