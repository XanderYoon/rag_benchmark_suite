document. 4 Task formulation For ranking tasks, the objective is to output a ranked list of documents given a query repre- senting an information need. Neural ranking models are trained using the LTR framework. Thus, here we present the LTR formulation for retrieval tasks. The LTR framework starts with a phase to train a model to predict the relevance score from a given query-document pair. During the training phase, a set of queries Q ={ q1, q2, … , q/uni007C.varQ/uni007C.var} and a large collection of documents D are provided. Without loss of generality, we suppose that the number of tokens in a given query is n, and the number of tokens in a given document is m. The groundtruth relevance scores for query-document pairs are needed to train the neural ranking model. In the general setting, for a given query, the groundtruth relevance scores are only known for a subset of documents from the large collection of documents D . So, we formally define that each query qi is associated with a subset of documents di =( di 1, di 2, … , di li ) from D , where di j denotes the jth document for the ith query, and li is the size of di . Each list of documents di is associated with a list of rel- evance scores yi =( yi 1, yi 2, … , yi li ) where yi j denotes the relevance score of document di j with respect to query qi . The objective is to train a function fw , with parameters w, that is used to predict the relevance score zi j = fw(qi, di j ) of a given query-document pair (qi, di j ). Information Retrieval Journal 1 3 The function fw is trained by minimizing a loss