can also be considered as a structured document. Zhang & Balog, (2018) propose a semantic matching method for table retrieval where various embedding features are used. Chen et al. (2020a) first learn the embedding rep- resentations of table headers and generate new headers with embedding features and curated features (Chen et al., 2018) for data tables. They show that the generated head- ers can be combined with the original fields of the table in order to accurately predict the relevance score of a query-table pair, and improve ranking performance. Trabelsi et al. (2019) proposed a new word embedding of the tokens of table attributes, called MCON, using the contextual information of every table. Different formulations for contexts are proposed to create the embeddings of attribute tokens. The authors argued that the different types of contexts should not all be treated uniformly and showed that data values are useful in creating a meaningful semantic representation of the attribute. In addition to computing word embeddings, the model can predict additional contexts of each table and use the predicted contexts in a mixed ranking model to compute the query-table relevance score. Using multiple and differentiated contexts leads to more useful attribute embeddings for the table retrieval task. Shraga et al. (2020) use different neural networks to learn different unimodal rep- resentations of a table which are combined into a multimodal representation. The final table-query relevance is estimated based on the query representation and multi- modal representation. Chen et al. (2020b) first select the most salient items of a table to construct the BERT representation for the table search, where different types of table items and salient signals are tested. The proposed content selection technique improves the performance of ad-hoc table retrieval. On the other hand, using the que- ries to select the table