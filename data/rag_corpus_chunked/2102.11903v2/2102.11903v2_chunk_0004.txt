al. Information Retrieval Journal 1 3 (2019) reviewed the learning strategies and the major architectures of neural ranking mod- els. Lin et al. (2020) focused mainly on pretrained Transformers (Vaswani et al., 2017) for text ranking, where they showed that a BERT-based multi-stage ranking model is a potential choice for a tradeoff between effectiveness and efficiency of a neural ranking model. Xu et al. (2020b) reviewed deep learning models for matching in document retrieval and recommenda- tion systems. The authors grouped the neural ranking models into two categories which are representation learning and matching function learning. Compared to the survey of Xu et al. (2020b), in addition to grouping neural ranking models into five categories based on the neu- ral components and design (Sect. 6), we summarize multiple models based on nine features that are frequently presented in the neural ranking models (Sect. 7). We also discuss the gen- eralization of neural ranking models to other retrieval tasks with different objects to query and rank. In particular, we describe four retrieval tasks which are: structured document retrieval, Question-Answering, image retrieval, and Ad-hoc video search (Sect. 8). In conclusion, the objective of our survey is to summarize the current progress, and com- pare multiple neural architectures using different dimensions. Our comparison is more fine- grained than existing surveys in terms of grouping and decomposing neural ranking models into important neural components and architecture designs. In addition, our survey includes an overview of models in the literature based on several features that are frequently presented in neural ranking models. The detailed comparison of multiple neural ranking models can help researchers to identify the common neural components that are used in the document retrieval task, understand the main benefits from using a given neural component, and investigate the promising neural components in future