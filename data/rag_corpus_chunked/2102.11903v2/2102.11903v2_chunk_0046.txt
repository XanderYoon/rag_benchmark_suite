token is used for the final retrieval score prediction. While BERT has been successfully applied to Question-answering (QA), applying BERT to ad-hoc retrieval of documents comes with the challenge of having significantly Fig. 6 Overview of BERT model for document retrieval. The input sequence to BERT is composed of the query q = q1q2 … qn (shown with orange color in the first layer) and selected tokens sd = d1d2 … dm (shown with dark blue color in the first layer) from the document d. The BERT-based model is formed of multiple layers of Transformer blocks where each token attends to all tokens in the input sequence for all layers. From the second to the last layer, each cell represents the hidden state of the corresponding token which is obtained from the Transformer. The query and document tokens are concatenated using the [SEP] token, and [CLS] is added to the beginning of the concatenated sequence. The hidden state of [CLS] token is used as input to MLP in order to predict the relevance score (Color figure online) Information Retrieval Journal 1 3 longer documents than BERT allows (BERT cannot take input sequences longer than 512 tokens). Yang et al. (2019a) proposed to address the length limit challenge by dividing documents into sentences and applying BERT to each of these sentences. The sentence- level representation of a document is motivated by recent work (Zhang et al., 2018) which shows that a single excerpt of a document is better than a full document for high recall in retrieval. In addition, using sentence-level representation is related to research in pas- sage-level document ranking (Liu & Croft, 2002). For each document, its relevance to the query can be predicted using the maximum relevance of its component sentences which is denoted as the best sentence.