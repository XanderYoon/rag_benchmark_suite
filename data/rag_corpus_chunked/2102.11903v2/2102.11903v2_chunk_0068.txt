score is predicted based on the interactions between the high-level representations of a given query-document pair. External knowledge bases and graphs are incorporated into neural ranking models to provide additional embeddings for the query and document. Knowledge graphs contain human knowledge and can be used in neural ranking models to better understand queries and documents. In general, the entity-based representation, that is computed from knowl- edge bases, is combined with the word-based representation. In addition, the knowledge graph semantics, such as the description and type of entity, provide additional signals that can be incorporated into the neural ranking model to improve retrieval results and general- ize to multiple scenarios. The existing methods, that incorporate knowledge graphs into the ranking models, lev - erage mainly entities and knowledge graph semantics. However, knowledge graphs often provide rich axiomatic knowledge that can be explored in future research to improve retrieval results. 9.3 How to reduce the complexity of the neural ranking architectures that use deep contextualized language models? After summarizing multiple existing neural ranking models, we can conclude that a potential future research direction is the design of more efficient neural ranking models that are able to incorporate semantic and relevance matching signals. Using BERT as a semantic matching component comes with the disadvantage of BERT length limit where the number of allowed tokens in BERT is significantly smaller than the typical length of a document. So, a possible future research direction is the study of selection tech- niques for sentences or passages with a trade-off between high retrieval results and low computation time. Ranking documents of length m using Transformers, which are the main components of BERT, requires O(m2) memory and time complexity (Kitaev et al., Information Retrieval Journal 1 3 2020). In particular, for very long documents, applying self-attention of Transformers