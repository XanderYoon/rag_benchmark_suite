other hand, many models that are proposed primarily for document retrieval are not symmetric because there are special computations that are applied only to the query or the document. For example, kernel pooling is used in Conv-KNRM (Dai et al., 2018) to summarize the similarities between a given query token and all document tokens. So, this can be seen as a query-level operation that breaks the symmetric property. An example of a document level operation that leads to an asym- metric architecture is included in ABEL-DRMM (McDonald et al., 2018). The attention weights are only computed for each document token against a given query token to pro- duce the attention-based representation of the document. So, the attention mechanism is only applied at the document level, and therefore the neural ranking model is asymmet- ric. The asymmetric property of some BERT-based ranking models comes from multiple facts. First, BERT is applied to the sentence or passage level of a long document (Yang et al., 2019a; Nogueira & Cho, 2019; Dai & Callan, 2019; Zhan et al., 2020a), so that there are some preprocessing steps that are applied only to the document. Second, some BERT- based models, such as MacAvaney et al. (2019), are combined with existing relevance- based ranking models that are asymmetric, and others, such as Nogueira et al. (2019), include a component for pairwise comparison of documents, so that the joint model is asymmetric in both cases. Third, Boualili et al. (2020) include the exact matching of query tokens into the ranking model which leads to an overall asymmetric architecture. In the case of short documents where BERT can accept the full document and only the BERT- based model is used for ranking, the ranking model is symmetric. 8 Beyond document retrieval The idea of using neural ranking models