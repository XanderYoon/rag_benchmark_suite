contextualized pre-trained language models to compute a contextual representation for each token. There are mainly two advantages from using these models; first, they are bidirectional language representations, in contrast to only left-to-right or right-to-left language models so that every token can attend to previous and next tokens to incorporate the context from both directions. Second, they contain the attention mechanism which becomes an important component of sequence representations in multiple tasks, especially the Transformerâ€™s self-attention which captures long-range dependencies better than the recurrent architectures (Vaswani et al., 2017). So, this contextual representation covers both the context-aware representation and the attention-based representation. The expensive computation is still a limitation when incorporating the pre-trained language models. For example, the large BERT model has 340M parameters consisting of 24 lay - ers of Transformer blocks, 16 self-attention heads per layer and a hidden size of 1024. Zhan et al. (2020a) analyzed the performance of BERT in document retrieval using the model proposed by Nogueira and Cho (2019). The analysis showed that the [CLS], [SEP], and periods are distributed with a large proportion of attention because they appear in all inputs. BERT has multiple layers, and the authors showed that there is different behavior in different layers. The first layers are representation-focused, and extract representations for a query and a document. The intermediate layers learn contextual representations using the interaction signals between the query and document. Finally, for the last layers, the rel- evance score is predicted based on the interactions between the high-level representations of a given query-document pair. External knowledge bases and graphs are incorporated into neural ranking models to provide additional embeddings for the query and document. Knowledge graphs contain human knowledge and can be used in neural ranking models to better understand queries and documents. In general, the entity-based representation,