output of the ranking model is a set of relevant documents to the user’s query which are returned to the user in a particular order. The inputs to neural ranking models consist of queries and documents with variable lengths in which the ranking model usually faces a short query with keywords, and long documents from different authors with a large vocabulary. Although exact matching is an important signal in retrieval tasks, ranking models also need to semantically match queries and documents in order to accommodate vocabulary mismatch. In ad-hoc retrieval, features can be extracted from documents, queries, and document-query interactions. Some docu- ment features go beyond text content and can include number of incoming links, page rank, metadata, etc. A challenging scenario for a ranking model is to predict the relevance score by only using the document’s textual content, because there is no guarantee to have addi- tional features when ranking documents. Neural ranking models have been used to extract feature representations for query and document using text data. For example, a deep neural network model can be used to map the query and documents to feature vectors indepen- dently, and then a relevance score is calculated using the extracted features. For query- document interaction, classic information retrieval models like BM25 can be considered as a query-document feature. For neural ranking models with a textual input for query and document, features are extracted from the local interactions between query and document. 4 Task formulation For ranking tasks, the objective is to output a ranked list of documents given a query repre- senting an information need. Neural ranking models are trained using the LTR framework. Thus, here we present the LTR formulation for retrieval tasks. The LTR framework starts with a phase to train a model to predict the relevance score