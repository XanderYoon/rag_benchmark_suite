is the number of Transformer layers in BERT. Pairwise cosine similarities are computed per layer to obtain a 3D interaction tensor Information Retrieval Journal 1 3 2017), and K-NRM (Xiong et al., 2017b)). The overview of a simple example of a joint model is shown in Fig. 8. The representation of the [CLS] token provides a strong seman- tic matching signal given that BERT is pretrained on the next-sentence prediction. As we explained previously, some of the neural ranking models, like DRMM, capture relevance matching for each query term based on the similarities with the document tokens. For rank- ing models, MacAvaney et al. (2019) use pretrained contextual language representations as input, instead of the conventional pretrained word vectors to produce a context-aware representation for each token from the query and document. Dai and Callan (2019) augment the BERT-based ranking model with the search knowl- edge obtained from search logs. The authors show that BERT benefits from tuning on the rich search knowledge, in addition to the language understanding knowledge which is obtained from training BERT on query-document pairs. Nogueira et al. (2019) propose a multi-stage ranking architecture. The first stage consists of extracting the candidate documents using BM25. In this stage, recall is more important than precision to cover all possible relevant documents. The irrelevant documents can be discarded in the next stages. The second stage, called monoBERT, uses a pointwise rank - ing strategy to filter the candidate documents from the first stage. The classification set- ting of BERT with sentence pairs is used to compute the relevance scores. The third stage, called duoBERT, is a pairwise learning strategy that computes the probability of a given document being more relevant than another candidate document. Documents from the sec- ond stage are ranked using duoBERT relevance scores in order