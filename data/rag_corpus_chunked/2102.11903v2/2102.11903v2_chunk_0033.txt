a real-valued score. In general, the MLP used in neural ranking architectures is a nonlinear function. 6.1.3 Representation+interaction models Retrieval models can benefit from both representation and interaction deep architectures in a combined model. In DUET (Mitra et al., 2017), an interaction-based network, called the local model, and a representation-based network, called the distributed model, are com- bined in a single deep learning architecture. Figure 4 shows the overview of the combined representation and interaction models for DUET. The local model takes the interaction matrix of query and document, based on patterns of exact matches of query terms in the document, as input. Then, the interaction matrix is passed through a CNN. The output of Information Retrieval Journal 1 3 the convolutional layer is passed through two fully connected layers, a dropout layer, and a final fully connected layer that produces a final relevance score. The distributed model learns a lower-dimensional feature vector for the query and document. A word embedding- based representation is used to encode each query term and document term. A series of nonlinear transformations is then applied to the embedded input. The matching between query and document representations is calculated using element-wise product. The final score under the DUET architecture is the sum of scores from the local and the distributed networks. In recently proposed models, there are more proposed architectures in the interaction- focused category than the representation-focused category. Nie et al. (2018) show in their empirical study that the interaction-based neural architectures generally lead to bet- ter results than the representation-focused architectures in information retrieval tasks. Although the representation-focused models offer the advantage of efficient computation by having the same feature vector for a document in all tasks, a static feature representation is unable to capture the matching signals in different tasks and datasets.