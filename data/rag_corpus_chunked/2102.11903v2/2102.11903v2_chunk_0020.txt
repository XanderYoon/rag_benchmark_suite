of document di j with respect to query qi . The objective is to train a function fw , with parameters w, that is used to predict the relevance score zi j = fw(qi, di j ) of a given query-document pair (qi, di j ). Information Retrieval Journal 1 3 The function fw is trained by minimizing a loss function L(w). In LTR, the learning categories are grouped into three groups based on their training objectives: the pointwise, pairwise, and listwise approaches. In the next section, we will briefly describe these three learning categories. In general, fw is considered as the composition of two functions M and F, with F is a feature extractor function, and M is a ranking model. So for a given query-document pair (qi, di j ) , zi j is given by: In traditional ranking models, the function F represents a set of hand-crafted features. The set of hand-crafted features include query, document, and query-document features. A ranking model M is trained to map the feature vector F(qi, di j ) into a real-valued relevance score such that the most relevant documents to a given query are scored higher to maxi- mize a rank-based metric. In recently proposed ranking models, deep learning architectures are leveraged to learn both feature vectors and models simultaneously. The features are extracted from query, document, and query-document interactions. The neural ranking models are trained using ground truth relevance of query-document pairs. The main objective of this article is to discuss the deep neural architectures that are proposed for the document retrieval task. To describe the overall steps of training neural ranking models, in the next section, we give a brief overview about the different learning strategies before presenting the existing neural ranking models. 5 Categories of strategies for learning