First, the document embeddings need to be loaded into the system or GPU memory (Johnson et al., 2021) with a limited size to compute the relevance scores of query-document pairs. Second, the dimension of the embedding is very large compared to the bag-of-word index (Zhan et al., 2021a; Xiong et al., 2021). Therefore, vector compression methods (Jégou et al., 2011; Ge et al., 2014) have been integrated into neu- ral ranking models to compress the embedding index and save computational resources with compressed embeddings of documents. The compression methods include Prod- uct Quantization (PQ) (Jégou et al., 2011, 2014) and Locality Sensitive Hashing (LSH) (Indyk & Motwani, 1998). Improving the memory efficiency using index compression can lead to a drop in the performance of the neural ranking model. Zhan et al., (2021b) proposed a joint optimization of query encoding and PQ in order to maintain effective- ness of neural ranking models while compressing index sizes. The authors showed that an end-to-end training strategy of the encoding and compression steps overcomes the training based on the reconstruction error for many compression methods (Jégou et al., 2011; Ge et al., 2014; Guo et al., 2020). In the same direction of reducing the ranking model complexity, Hofstätter et al. (2020) reduced the time and memory complexity of Transformers by considering the local self-attention where a given token can only attend to tokens in the same sliding window. In the particular case of non-overlapping sliding windows of size w << m , the time and memory complexity is reduced from O(m2) to O(m × w) . Recently, Kitaev et al. (2020) improved the efficiency of Transformers and proposed the Reformer which is efficient in terms of memory and runs faster for long sequences by reducing the com- plexity from O(m2) to O(m × log(m))