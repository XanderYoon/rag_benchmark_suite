existing relevance-based neural ranking model extracts the relevance feature from a query-document pair Information Retrieval Journal 1 3 1. Symmetric: We describe a neural ranking architecture as symmetric if the relevance score does not change if we change the order of inputs (query-document or document-query pairs). In other words, for a given query and document, there are no special computations that are applied only to the query or document. 2. Attention: This dimension characterizes neural ranking models that have any type of attention mechanisms. 3. Ordered tokens: The sequential order of tokens is preserved for both query and docu- ment when computing the interaction tensors between query and document, and the final feature vector representing the query-document pair. 4. Representation: This feature characterizes neural ranking models that extract features from the query and document separately, and defer the interactions between the features to the ranking function. – Without weights sharing (W): Two independent deep neural networks without weights sharing are used to extract features from queries and documents. – With weights sharing or Siamese (S): The neural ranking model has Siamese archi- tecture as described by Bromley et al., (1993), where the same deep neural network is used to extract features from both query and document. 5. Interaction: This feature characterizes neural ranking models that build local interactions between query and document in an early stage, so it can be considered as the mutually exclusive category of representation models. 6. Injection of contextual information: Depending on when to inject contextual information, we can distinguish two cases. – Early injection of contextual information (E): Some neural ranking models incorpo- rate contextual information in the embedding phase by considering context-sensitive embeddings (for example the embedding that is computed using LSTM). – Late injection of contextual information (L): Some neural ranking models defer