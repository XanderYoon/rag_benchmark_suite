of matching signals in a document is important, the sequential order of query tokens is ignored in Co-PACRR. It shuffles the feature map with respect to query tokens before feeding the flattened result to dense layers. Shuffling enables the model to avoid learning a signal related to the query term position after extracting n-gram matching patterns. For example, we mentioned in Sect. 4 that the number of tokens in a given query is n which indicates that for models that only support fixed size inputs, short queries are padded to n tokens. Therefore, without shuffling, a model can learn that query tokens at the tail are not important because of padding of short queries, and this leads to ignoring some relevant tokens when calculating relevance score for longer queries. More coarse-grained context than the sliding window strategy can be used to capture local relevance matching signals in the scope hypothesis category, where a system can divide a long document into passages and collect signals from the passages in order to make a final relevance assessment on the whole document. Callan (1994) discussed how passages should be defined, and how they are incorporated in the ranking process of the entire document. The HiNT (Fan et al., 2018) model is based on matching query and pas- sages of a given document, and then it aggregates the passage-level matching signals using either k-max pooling or bi-LSTM or both. We can distinguish two types of matches between query and document tokens. The first type is the context-free match where the similarity between a given document token and query token is computed regardless of the context of the tokens. Examples of the context- free match are the cosine similarity between a query token embedding and a document token embedding, and exact matching. The second type