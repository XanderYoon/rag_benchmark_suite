embedding of document tokens. The document-aware query token encoding is then computed using ele- ment-wise multiplication between the query token embedding and the attention-based rep- resentation of a document. Finally, in order to compute the relevance score, the document- aware query token encodings of all query tokens are fed to the DRMM model. A symmetric attention mechanism or co-attention can focus on the set of important positions in both textual inputs. Kim et al. (2019) incorporate the attention mechanism in Densely-connected Recurrent and Co-attentive neural Network (DRCN). DRCN uses residual connections (He et al., 2016), like in Densenet (Huang et al., 2017), in order to build higher level feature vectors without exploding or vanishing gradient problems. When building feature vectors, the concatenation operator is used to preserve features from previ- ous layers for final prediction. The co-attentive network uses the attention mechanism to focus on the relevant tokens of each text input in each RNN layer. Then, the co-attentive feature vectors are concatenated with the RNN feature vectors of every token in order to form the DRCN representation. The idea of concatenating feature vectors from previous layers before calculating atten- tion weights is also explored by Zhang et al. (2019) in their proposed model DRr-Net. DRr- Net includes an attention stack-GRU unit to compute an attention-based representation for both inputs that capture the most relevant parts. It also has a Dynamic Re-read (DRr) unit that can focus on the most important word at each step, taking into consideration the learned information. The selection of important words in the DRr unit is also based on attention weights. Using multiple attention functions in matching with a word-by-word attention (Rock - t√§schel et al., 2016) can better capture the interactions between the two inputs. Multiple attention functions are proposed in the literature: e.g.,