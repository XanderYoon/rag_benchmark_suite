(2017b) proposed a Bilateral Multi-Perspective Matching (BiMPM) model to match contexts of two sentences. After encoding each token in a given sentence using the bi-LSTM network, four matching strategies are used to com- pare the contextual information. The proposed matching strategies differ on how to aggre- gate the contextual information of the first input, which is matched against each time step of the second input (and vice versa). The four matching strategies generate eight vectors in Information Retrieval Journal 1 3 each time step (there is forward and backward contextual information) which are concat- enated and fed to a second bi-LSTM model to extract two feature vectors from the last time step of forward and backward LSTM. The whole process is repeated to match contexts in the inverse direction and extract two additional feature vectors. The four final feature vec- tors are concatenated and fed to a fully connected network to predict a real-valued score. 6.3 Attentionâ€‘based representation McDonald et al. (2018) proposed a model, called Attention-Based ELement-wise DRMM (ABEL-DRMM), that takes advantage of the context-sensitive embedding and attention weights. Any similarity measure between the term encodings of the query and document tokens already captures contextual information due to the context-sensitive embedding. In ABEL-DRMM, the first step is to calculate attention weights for each query token against document tokens using softmax of cosine similarities. Then, the attention-based repre- sentation of a document is calculated using the attention weights and the embedding of document tokens. The document-aware query token encoding is then computed using ele- ment-wise multiplication between the query token embedding and the attention-based rep- resentation of a document. Finally, in order to compute the relevance score, the document- aware query token encodings of all query tokens are fed to the DRMM model. A symmetric attention mechanism or co-attention can