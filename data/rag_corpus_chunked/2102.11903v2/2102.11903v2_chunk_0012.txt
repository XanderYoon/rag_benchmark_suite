from both the forward and backward RNN improves the performance of the sequence tagging task. However, the embeddings are not deep, in the sense that they are not a function of all of the internal layers of both the forward and background language models. Recently, researchers have focused on creating deep context-sensitive embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) which are trained on large amounts of unlabeled data and achieve high perfor- mance in multiple NLP tasks. 2.7 Deep contextualized language models Peters et al. (2018) proposed ELMo which is a deep contextualized language model composed of forward and backward LSTMs. ELMo produces deep embeddings, where the representations from multiple layers of both the forward and backward LSTM are linearly combined using task-specific learnable weights to compute the embedding of a given token. Combining the internal states leads to richer embeddings. Although ELMo improves the results of multiple NLP tasks, ELMo decouples the left-to-right and right-to- left contexts by using a shallow concatenation of internal states from independently trained forward and backward LSTMs. Devlin et al. (2019) proposed a language model, called Bidirectional Encoder Representations from Transformers (BERT), that fuses both left and right contexts. BERT is a deep contextualized language model that contains multiple layers of Trans- former (Vaswani et al., 2017) blocks. Each block has a multi-head self-attention structure followed by a feed-forward network, and it outputs contextualized embeddings for each Information Retrieval Journal 1 3 token in the input. BERT is trained on large collections of unlabeled data over two pre- training tasks which are next sentence prediction and the masked language model. After the pre-training phase, BERT can be used for downstream tasks on single text or text pairs using special tokens ([SEP] and [CLS]) that are added into