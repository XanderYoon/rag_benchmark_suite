the weights in CNN filters are shared among mul- tiple local regions for the input, and this further reduces the number of parameters. The outputs of the CNN are called feature maps. Pooling operations, such as average and max pooling, are usually applied to the feature map to keep only the significant signals, and to further reduce the dimensionality. A CNN kernel has a predefined size so that in order to handle the information in the border of the input, padding is introduced to enlarge the input. CNNs were first introduced to solve image-related tasks such as image classification (Krizhevsky et al., 2017; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Ioffe & Szegedy, 2015; Szegedy et al., 2016), and were later adapted to solve text- related tasks such as NLP and information retrieval (Collobert et al., 2011; Dai et al., 2018; Hui et al., 2017; Jaech et al., 2017; Lan & Xu, 2018; McDonald et al., 2018; Pang et al., 2016b; Tang & Yang, 2019; Kalchbrenner et al., 2014). 2.2 Recurrent neural network (RNN) An RNN (Elman, 1990) learns features and long-term dependencies from sequential and time-series data. RNN reads the input sequence sequentially to produce a hidden state in each timestamp. These hidden states can be seen as memory-cells that store the sequence information. A current hidden state is a function of the previous hidden state and the current input. Therefore, a hidden state is computed for each timestamp, and the hidden state that corresponds to the last timestamp in the sequence captures the context-aware representation of the entire sequence. Two major problems of the vanilla RNN are the vanishing and exploding gradients (Pascanu et al., 2013; Sutskever et al., 2011) that can occur after back-propagation through time during the training phase. For example, for long sequences, when