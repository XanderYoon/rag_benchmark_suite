to fine-tune BERT on two retrieval tasks: TREC Web Track ad-hoc document ranking and MS MARCO (Nguyen et al., 2016) pas- sage re-ranking. Four BERT-based ranking models are proposed which are related to both representation and interaction based models using the [CLS] embedding, and also the embeddings of each token in the query and document. The authors show that BERT works better with pairs of texts that are semantically close. However, as mentioned before, queries and documents can be very different, especially in their lengths and can benefit from rel- evance matching techniques. BERT contains multiple Transformer layers, where the deeper the layer, the more con- textual information is captured. BERT can be used in the embedding layer to extract tensor contextualized embeddings for both query and document. Then, the interactions between query and document is captured by computing an interaction tensor from the embedding tensors of both the query and document. Finally, a ranking function maps the interaction tensor to a real-valued relevance score. The overview of the BERT model that is used as an embedding layer is shown in Fig. 7. In order to capture both relevance and semantic matching, MacAvaney et al. (2019) pro- pose a joint model that incorporates the representation of [CLS] from the query-document pair into existing neural ranking models (DRMM (Guo et al., 2016), PACCR (Hui et al., Fig. 7 Overview of BERT model used as an embedding layer for document retrieval. L is the number of Transformer layers in BERT. Pairwise cosine similarities are computed per layer to obtain a 3D interaction tensor Information Retrieval Journal 1 3 2017), and K-NRM (Xiong et al., 2017b)). The overview of a simple example of a joint model is shown in Fig. 8. The representation of the [CLS] token provides a strong seman- tic matching