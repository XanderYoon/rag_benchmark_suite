Injection of contextual information: Depending on when to inject contextual information, we can distinguish two cases. – Early injection of contextual information (E): Some neural ranking models incorpo- rate contextual information in the embedding phase by considering context-sensitive embeddings (for example the embedding that is computed using LSTM). – Late injection of contextual information (L): Some neural ranking models defer injecting the contextual information until computing the interaction tensors (for example, applying n-gram convolutions on the interaction tensor to incorporate con- textual information). 7. Exact matching: This feature means that the neural ranking model includes the exact matching signal when calculating relevance score. 8. Incorporate external knowledge bases (KB): This feature characterizes neural ranking models that incorporate external knowledge bases to predict query-document relevance score. 9. Deep language models (LM): This feature refers to the use of deep contextualized lan- guage models to compute query-document relevance scores. We can distinguish two cases for deep LM. – Deep LM in embedding layer (Em): Deep contextualized language models are used as a context-sensitive embedding to compute a word embedding tensor (because Deep LM have multiple layers) for both query and document. Such models neces- sarily have the early injection of contextual information property. – Deep LM as a semantic matching component (Se): Deep contextualized language models are used as a semantic matching component in a neural ranking model. For example, BERT is pretrained on the next sentence prediction so that it captures semantic matching signals. The same for ELMo which is pretrained on the next token prediction. Information Retrieval Journal 1 3Table 1 Overview of Neural Ranking Models Method symmetric attention ordered tokens representation interaction injection of CI exact matching KB Deep LM DSSM (Huang et al., 2013) ✓ S C-DSSM (Shen et al., 2014b) ✓ ✓ S E ARC-I (Hu et