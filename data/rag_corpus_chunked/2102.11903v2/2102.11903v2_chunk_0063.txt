and embedding-based (Li et al., 2019; Cao et al., 2019; Miech et al., 2018, 2019) catego- ries. In concept-based methods, visual concepts are used to describe the content of a video. Then, the userâ€™s query is mapped to related visual concepts which are used to retrieve a set of videos by aggregating matching signals from the visual concepts. This approach works well for queries where related visual concepts are accurately extracted. However, capturing semantic similarity between videos and long queries by aggregating visual concepts is not accurate because these queries contain complex semantic meaning. In addition, extracting visual concepts for a video and query is done independently. Embedding-based methods propose to map queries and videos into a common space, where the similarity between the embeddings is computed using distance functions, such as cosine similarity. As in document retrieval, many video retrieval models are representation-focused mod- els where the only difference is the cross-modal characteristic of the neural ranking model: one deep neural network for video embedding and another deep neural network for text embedding. For example, Yang et al. (2020) propose a text-video joint embedding learn- ing for complex-query video retrieval. The text-based network which is used to embed the query has a context-sensitive embedding with LSTM for an early injection of contextual information as in the document retrieval. Consecutive frames in a video have the temporal dependence feature. So, as in textual input, LSTM can be used to capture the contextual information of frames after extracting frame-based features using pretrained CNN. As in BERT where the self-attention is used to capture token interactions, Yang et al. (2020) introduce a multi-head self-attention for video frames. So, this neural ranking model for video retrieval covers multiple proposed categories for the document retrieval which are: representation-focused models, context-aware based representation, and attention-based