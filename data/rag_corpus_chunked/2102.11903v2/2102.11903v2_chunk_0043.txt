compare the embedding of a given token with its context generated using the attention mechanism. Wang and Jiang (2017) use several comparison methods in order to match the embedding of a token and its context. These comparison methods include neural network layer, neural tensor network (Socher et al., 2013d), Euclidean distance, cosine similarity, and element-wise operations for vectors. In addition to being used in LSTM models, the attention mechanism has been beneficial to CNN models. Yin et al. (2015) proposed an Attention Based Convolutional Neural Net- work (ABCNN) that incorporates the attention mechanism on both the input layer and the feature maps obtained from CNN filters. ABCNN computes attention weights on the input embedding in order to improve the feature map computed by CNN filters. Then, ABCNN computes attention weights on the output of CNN filters in order to reweight feature maps for the attention-based average pooling. 6.4 External knowledge and feedback Many methods have been developed to incorporate knowledge bases into retrieval com- ponents. For example, the description of entities can be used to have better term expan- sion (Xu et al., 2009), or to expand queries to have better ranking features (Dalton et al., 2014). Queries and documents are connected through entities in the knowledge base to build a probabilistic model for document ranking based on the similarity to entity descrip- tions (Liu & Fang, 2015). Other researchers have extended bag-of-word language models to include entities (Raviv et al., 2016; Xiong et al., 2016). Knowledge bases can be incorporated in neural ranking models in multiple ways. The AttR-Duet system (Xiong et al., 2017a) uses the knowledge base to compute an additional entity representation for document and query tokens. There are four possible interactions between word-based feature vectors and entity-based feature vectors based on the inter- and intra-space matching