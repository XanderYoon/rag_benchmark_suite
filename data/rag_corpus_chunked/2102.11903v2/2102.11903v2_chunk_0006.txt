Sect. 8, we show that neural ranking models can generalize beyond document retrieval. In particular, we describe four related retrieval tasks: structured document retrieval, ques- tion-answering, image retrieval, and ad-hoc video search. â€“ In Sect. 9, we summarize the current best practices for the design of neural ranking mod- els, and we discuss multiple research directions in document retrieval. Information Retrieval Journal 1 3 2 Background In this section, we briefly introduce the deep learning terminology and techniques most commonly used in ad-hoc retrieval. This includes Convolutional Neural Networks (CNN) (LeCun & Bengio, 1998), Recurrent Neural Networks (RNN) (Elman, 1990), Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), Gated Recur - rent Units (GRU) (Cho et al., 2014), word embedding techniques (Wang et al., 2020), attention mechanism (Bahdanau et al., 2015), deep contextualized language models (Peters et al., 2018; Devlin et al., 2019), and knowledge graphs (Wang et al., 2017). We will be referring to these neural components and techniques when discussing the differ - ent neural ranking models in the literature. 2.1 Convolutional neural network (CNN) A CNN (LeCun & Bengio, 1998) extracts features from data by defining a set of filters or kernels that spatially connect local regions. Compared to the dense networks, each neuron is connected to only a small number of neurons instead of being connected to all neurons from the previous layer. This design significantly reduces the number of parameters in the model. In addition, the weights in CNN filters are shared among mul- tiple local regions for the input, and this further reduces the number of parameters. The outputs of the CNN are called feature maps. Pooling operations, such as average and max pooling, are usually applied to the feature map to keep only the significant signals, and to further reduce the dimensionality. A