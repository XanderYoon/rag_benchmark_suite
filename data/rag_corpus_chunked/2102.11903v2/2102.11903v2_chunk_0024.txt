1 3 6.1.1 Representationâ€‘focused models The general framework of representation-focused models is shown in Fig. 2. In representa- tion-focused models, two independent neural network models NNQ and NND map the query q and the document d, respectively, into feature vectors NNQ(q) and NND(d) . Thus the fea- ture extractor F for a query-document pair is given by: In the particular case where NNQ and NND are identical, the neural architecture is consid- ered to be Siamese (Bromley et al., 1993). The relevance score of the query-document pair is calculated using a simple M function like cosine similarity, or a Multi-Layer Perceptron (MLP) between the representations of query and document: The representation-focused model extracts a good feature representation for an input sequence of tokens using deep neural networks. Huang et al. (2013) proposed the first deep neural ranking model for web search using query-title pairs. The proposed model, called Deep Structured Semantic Model (DSSM), is based on the Siamese architecture (Brom- ley et al., 1993), which is composed of a deep neural network model that extracts features from query and document independently. The deep model is composed of multiple fully connected layers that are used to map high-dimensional textual sparse features into low- dimensional dense features in a semantic space. In order to capture local context in a given window, Shen et al. (2014b) proposed a Con- volutional Deep Structured Semantic Model (C-DSSM) in which a CNN is used instead of feed-forward networks in the Siamese architecture. The feature extractor F is composed of a CNN that is applied to a letter-trigram input representation, then a max-pooling layer is used to form a global feature vector, while M is the cosine similarity function. CNNs have also been used in ARC-I (Hu et al., 2014) to extract feature representations of the query