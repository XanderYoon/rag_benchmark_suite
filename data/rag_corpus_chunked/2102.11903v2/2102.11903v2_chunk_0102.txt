Conference on Information and Knowledge Management–CIKM ’16. Yang, W., Zhang, H., & Lin. J. (2019). Simple applications of bert for ad hoc document retrieval. arXiv: 1903. 10972. Yang, X., Dong, J., Cao, Y., Wang, X., Wang, M., & Chua T. (2020). Tree-augmented cross-modal encoding for complex-query video retrieval. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. Yang, Z., Yang, D., Dyer, C., He, X., Smola, A. J., & Hovy, E. H. (2016b). Hierarchical attention networks for document classification. In NAACL HLT 2016, The 2016 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016 (pp. 1480–1489). The Association for Computational Linguistics. Yang, Z., Salakhutdinov, R., & Cohen, W. W. (2017). Transfer learning for sequence tagging with hierarchi- cal recurrent networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. https:// OpenR eview. net/. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V. (2019b). Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems (pp. 5753–5763). Yin, W., Schütze, H., Xiang, B., & Zhou, B. (2016). Abcnn: Attention-based convolutional neural net- work for modeling sentence pairs. Transactions of the Association for Computational Linguistics, 4, 259–272. Ying, H., Zhuang, F., Zhang, F., Liu, Y., Xu, G., Xie, X., Xiong, H., & Wu, J. (2018). Sequential rec- ommender system based on hierarchical attention networks. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden (pp. 3926–3932). https:// www. ijcai. org/. Yu, S., Su, J., & Luo, D. (2019). Improving bert-based text classification with auxiliary sentence and domain knowledge. IEEE Access, 7, 176600–176612. Yuan, J., Z.-J. Zha, Y.-T.