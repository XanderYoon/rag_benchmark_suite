) Document ( tokens) Query ( tokens) Fig. 3 Overview of the general architecture of interaction-focused models. An interaction function is used to map the query and document to an interaction output. A ranking function M is used to map the interac- tion output to a real-valued relevance score Information Retrieval Journal 1 3 applied to summarize the cosine similarities into a soft-matching feature vector of dimen- sion K; intuitively, this vector represents the probabilities that the similarities came from the distribution specified by each kernel. The final feature vector is computed by summing the soft-matching feature vectors of query tokens. Cosine similarity interaction matrix is also used in Hierarchical Neural maTching model (HiNT) (Fan et al., 2018), aNMM (Yang et al., 2016a), MatchPyramid (Pang et al., 2016a, b), and DeepRank (Pang et al., 2017). In addition to cosine similarity, other forms of simi- larities include dot product and indicator function which are used in HiNT and MatchP - yramid, and Gaussian Kernel that is introduced in the study of MatchPyramid (Pang et al., 2016a) using multiple interaction matrices. Different architectures are used for feature extractor F to build the query-document interactions, and for the ranking model M to extract matching signals from interactions of query and document tokens. LSTM-based ranking models: As in representation-based models, LSTM is used in mul- tiple neural ranking models (Fan et al., 2018; He & Lin, 2016; Jaech et al., 2017). He and Lin (2016) use bi-LSTMs for context modelling of text inputs. So, each input is encoded to hidden states using weight-shared bi-LSTMs. In the ranking model proposed by Jaech et al., (2017), two independent bi-LSTMs without weight sharing are used to map query and document to hidden states. The query and document have different sequential struc- tures and vocabulary which motivates encoding