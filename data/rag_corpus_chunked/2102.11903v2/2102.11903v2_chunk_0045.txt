mention step for tokens in the inputs. A bi-LSTM computes a context-aware embedding matrix, which is then used to compute context-guided knowledge vectors. These knowledge vectors are Information Retrieval Journal 1 3 multiplied by the attention weights to compute the final context-guided embedding for each token. Inspired by traditional ranking techniques that use Pseudo Relevance Feedback (PRF) (Hedin et al., 2009) to improve retrieval results, PRF can be integrated in neural rank - ing models as in the model proposed by Li et al. (2018). The authors introduce a Neural framework for Pseudo Relevance Feedback (NPRF) that uses two ranking models. The first model computes a query-based relevance score between the query and document corpus and extracts the top candidate documents for a given query. Then, the second model com- putes a document-based relevance score between the target document and the candidate documents. Finally, the query-based and document-based relevance scores are combined to compute the final relevance score for a query-document pair. 6.5 Deep contextualized language model‑based representations The sentence pair classification setting is used to solve the document retrieval task. The overview of BERT for the document retrieval is shown in Fig. 6. In general, the input sequence to BERT is composed of the query q and selected tokens sd from the document d: [[CLS], q, [SEP], sd , [SEP]]. The selected tokens can be the whole document, sentences, passages, or individual tokens. The hidden state of the [CLS] token is used for the final retrieval score prediction. While BERT has been successfully applied to Question-answering (QA), applying BERT to ad-hoc retrieval of documents comes with the challenge of having significantly Fig. 6 Overview of BERT model for document retrieval. The input sequence to BERT is composed of the query q = q1q2 … qn (shown with orange color