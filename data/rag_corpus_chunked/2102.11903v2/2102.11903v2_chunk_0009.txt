influence of the memory cell on the hidden state of the current timestamp. Compared to RNN, LSTM has led to significant improvements in multiple fields with sequential data such as text, video, and speech. LSTM has been applied to solve multiple tasks including language modeling (Kim et al., 2016), text classification (Dai & Le, 2015), machine translation (Sutskever et al., 2014), video analysis (Singh et al., 2017), image cap- tioning (Karpathy & Fei-Fei, 2017), and speech recognition (Graves et al., 2013). 2.4 Gated recurrent units (GRU) Similar to LSTM, GRU (Cho et al., 2014) is used for sequence-based tasks to capture long- term dependencies. However, unlike LSTM, GRU does not have separate memory cells. In LSTM, the output gate is used to control the memory content that is used by other units in the network. On the other hand, the GRU model does not contain an output gate, and therefore uses its content without any gating control. In addition, while LSTM computes the value of the new added memory independently of the forget gate, GRU does not inde- pendently control the new added activation but uses the reset gate to control the previous hidden state. More differences and similarities between LSTM and GRU are summarized by Chung et al., (2014). This model has been shown to achieve good performance in multi- ple tasks such as machine translation (Cho et al., 2014) and sentiment classification (Tang et al., 2015) . 2.5 Attention mechanism The attention mechanism was first proposed by Bahdanau et al., (2015) for neural machine translation. The original Seq2Seq model (Sutskever et al., 2014) used an LSTM to encode a sentence from its source language and another LSTM to decode the sentence into a tar - get language. However, this approach was unable to capture long-term dependencies. In order