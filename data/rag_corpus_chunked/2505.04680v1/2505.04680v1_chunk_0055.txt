of the mean 33 6 Conclusions and recommendations Building a fully factual and trustworthy pipeline to analyse documents with LLMs is still complex, even if many tools and techniques are currently available. In this report, we contributed: (1) RAGEv - a reference pipeline which gets state-of-the-art results on a public benchmark of scientific papers extracted from the PubMedQA datasets, (2) RAGEv -Bench, a collection of manually curated documents, code and metrics to evaluate those pipelines to calculate accuracy, precision and recall in an automated way , (3) An usability test to check how ready and which are the main barriers and obstacles that impede the full uptake of these technologies in the day-to-day work of colleagues handling policy files. The implementati on of the RAG methodology has shown to have a large potential in overcoming some of the main shortcomings of LLMs in synthesizing health scientific knowledge, notably for providing quick factual overviews or summaries of pre -defined document collections. The actual implementation and fine-tuning/setting of the method leads to different strengths and weaknesses, which should be carefully evaluated, lest give the user a false sense of security when the answers might be in fact, incomplete, or incorrect. There is a learning curve for users to interact with the tool and the way they interact with it (type of questions asked, exact wording of the question) have an influence of the outcomes. This demonstrates the need to both clearly define the scope of the appl ication and provide hints on issues where usersâ€™ checks are most necessary. The best pipeline for us was the SHy one, which obtained an average precision of 0.85 on the binary yes/no questions and average BERTScore F1, precision and recalls of respectively 0.83, 0.79 and 0. 88. Even this pipeline shows limitations notably in extracting