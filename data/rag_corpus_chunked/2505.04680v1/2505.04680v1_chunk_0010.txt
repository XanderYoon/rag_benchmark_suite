this needs to be taken into account when using AI technologies to process health documents that are not in public domain (like open access scientific papers) or that contain sensitive personal information. Furthermore, LLMs can perpetuate or amplify existing biases in medical data and practices reported in scientific studies, potentially leading to less accurate or biased results for underrepresented groups [11]. This raises concerns about equitable healthcare delivery and the potential for AI to exacerbate existing disparities in medical outcomes. The lack of explainability and interpretability in LLMs [12] is particularly problematic for the use of retrieving information from health -related publications. On the one hand the "black box" nature of these models makes it difficult to understand how they arrive at conclusions, which can hinder their ad option by healthcare professionals who need to justify their decisions. This lack of transparency also complicates efforts to ensure that LLM outputs align with current medical best practices and guidelines. On the other hand, and despite their limitations, LLMs could redefine interpretability itself by being used to audit their own responses [13]. Factuality and trustworthiness are critical issues when applying LLMs to health documents and scientific papers. These models can generate plausible -sounding but incorrect information, often referred to as “hallucinations” [14]. But , in our case, even smal l inaccuracies can have serious consequences, and verifying the accuracy of LLM outputs in complex medical contexts is both challenging and resource intensive. Toxicity and the generation of inappropriate content are also concerns that must be addressed [15], especially given the sensitive nature of medical information retrieval. Regulatory compliance adds another layer of complexity, since the development and use of AI in healthcare is subject to strict and evolving regulations such as the General Data Protection Regulation (GDPR)8, the AI Act 9 and