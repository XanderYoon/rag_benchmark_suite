BDAP servers. Most of the experiments use the LLMs served by the GPT@JRC infrastructure. 4.4 Datasets for benchmarking and usability checks (RAGEv - Bench) To systematically evaluate our sys tem and its configurations, we used a two -fold approach. First, we ran an automatic evaluation based on public datasets and on classical automatic metrics. This part of the evaluation, called in subsequent sections the “Automatic performance evaluation” (A PE), sets the baseline and allows comparison with existing methods and implementation on public and well -known dataset s. This step has also been 16 Next.js by Vercel - The React Framework (nextjs.org) 17 https://webgate.ec.europa.eu/cas/login 18 https://azure.microsoft.com/en-us/products/kubernetes-service 19 https://milvus.io/ 20 https://jeodpp.jrc.ec.europa.eu/bdap/ 16 used to define many of the hyperparameters of the RAG pipelines that are then used in the second evaluation. As a second evaluation, we are interested in the usability of these pipelines in the day-to-day work of colleagues which are not experts in AI and work on policy support files. To this end, we built three hand -crafted datasets - referred in the subsequent sections as “Horizon Research” (HR), “Virtual Human Twin” (VHT) and “Bacteriophages” (AMR) - that cover three topics of the current work of the Unit JRC.F7. The goal was to assess the system on documents and questions that users across the EC could present to the system and learn from the feedback of colleagues with deep domain knowledge on how to facilitate the integration of LLMs with their workflow. This assessment allowed further improvement of RAGEv. All together, these four datasets are released under the name of RAGEv-Bench for interested readers to replicate our results and advance research. The datasets will be available in the JRC Data catalogue in the weeks after the publication of this report. 4.4.1 Automatic performance evaluation on PubMedQA (APE) The APE dataset