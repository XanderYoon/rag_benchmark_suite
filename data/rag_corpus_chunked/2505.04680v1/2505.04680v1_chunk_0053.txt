analysis We show in Figure 8 the average scores (± standard error) assigned by human annotators to the system’s reply. As shown, the pipeline obtained its best results (judged on a scale [0,5]) on the Horizon Research collection (HR - 4.4), where it showed the lowest variability (± 0.24). The Bacteriophages (AMR - 3.4) and Virtual Human Twin (VHT - 2.8) collections follow in such an order. The difference in results under each combination of collection reinforces the hypothesis of an effect of the collection type, and suggests the absen ce of an interaction effect between collections. Moreover, in each collection, concept -description and summary questions tends to produce rather stable and high performances. 31 In the Figure 8 we can appreciate how BERTScore, a more semantic based metrics, seems to overall better capture the average scores of the human annotations, while Rouge Scores, a metric more focused on single word content captures more reliably the variance expres sed by the annotators. A noticeable exception is the situation under the Horizon collection, where results for BERT and Rouge Scores appear notably more aligned to each other, compared to the other two collections , especially under the SHy pipeline . This might be explained by the fact that such pipeline was developed under the interaction between and the author who developed the pipeline. Figure 9 - Visual correlation analysis between BERTScores F1 and Human. Scores are collapsed by points in each human score value. Error bars repost standard error of the mean Figure 8 - Comparison between the human evaluation and the BERT Score metric. Error bars represent standard error of the mean 32 To better grasp the interaction between the human and machine generation scores, we performed a mixed effect linear regression analysis, comparing a full model (i.e., Human Score