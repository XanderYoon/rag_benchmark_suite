Accessed: Jul. 08, 2024. [Online]. Available: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transf ormers/recursive_text_splitter/ 38 [36] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr, “Knowledge Graph Prompting for Multi -Document Question Answering,” Dec. 25, 2023, arXiv: arXiv:2308.11730. doi: 10.48550/arXiv.2308.11730. [37] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed Representations of Words and Phrases and their Compositionality,” in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2013. Accessed: Jul. 07, 2024. [Onl ine]. Available: https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce9 01b-Abstract.html [38] J. Devlin, M. -W. Chang, K. Lee, and K. Toutanova, “BERT: Pre -training of Deep Bidirectional Transformers for Language Understanding,” May 2 4, 2019, arXiv: arXiv:1810.04805. doi: 10.48550/arXiv.1810.04805. [39] Z. Liu et al., “ChatQA: Surpassing GPT-4 on Conversational QA and RAG,” May 22, 2024, arXiv: arXiv:2401.10225. doi: 10.48550/arXiv.2401.10225. [40] K. Guu, K. Lee, Z. Tung, P. Pasupat , and M. -W. Chang, “REALM: Retrieval - Augmented Language Model Pre -Training,” Feb. 10, 2020, arXiv: arXiv:2002.08909. doi: 10.48550/arXiv.2002.08909. [41] W. Shi et al., “REPLUG: Retrieval -Augmented Black-Box Language Models,” May 24, 2023, arXiv: arXiv:2301.12652. doi: 10.48550/arXiv.2301.12652. [42] B. Wang et al. , “InstructRetro: Instruction Tuning post Retrieval -Augmented Pretraining,” May 29, 2024, arXiv: arXiv:2310.07713. doi: 10.48550/arXiv.2310.07713. [43] T. Mikolov, K. Chen, G. Corrado, and J. D ean, “Efficient Estimation of Word Representations in Vector Space,” Sep. 06, 2013, arXiv: arXiv:1301.3781. doi: 10.48550/arXiv.1301.3781. [44] J. Pennington, R. Socher, and C. Manning, “GloVe: Global Vectors for Word Representation,” in Proceedings of th e 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), A. Moschitti, B. Pang, and W. Daelemans, Eds., Doha, Qatar: Association for Computational Linguistics, Oct. 2014, pp. 1532 –1543. doi: 10.3115/v1/D14-1162. [45] N. Reimers and I. Gurevych, “Sentence -BERT: Sentence Embeddings using Siamese BERT -Networks,” Aug. 2019, [Online]. Available: http://arxiv.org/abs/1908.10084 [46] “[2401.00368] Improving Text Embeddings with Large Language