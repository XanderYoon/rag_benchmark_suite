the scope of the appl ication and provide hints on issues where usersâ€™ checks are most necessary. The best pipeline for us was the SHy one, which obtained an average precision of 0.85 on the binary yes/no questions and average BERTScore F1, precision and recalls of respectively 0.83, 0.79 and 0. 88. Even this pipeline shows limitations notably in extracting numerical answers and answering from figure and tables, which require addressing in further versions. The aim of this work was to gen erate a prototype tool to assess the possibility, added value and limitations of RAG in synthesizing documents in the health domain. In the next sections, we will comment on the lessons learnt and future works. The reference implementation will be made available to the colleagues in the EC and selected features will feed into more operational services within GPT@JRC. 6.1 Lesson learnt and recommendations 6.1.1 On the implementation of a reference pipeline Classical embedding techniques like BERT do not perform as well as large decoder -only embedding models, with two important distinctions: first, embedding techniques like SciBERT or BioBERT, which are specifically trained on scientific and biomedical text, can surpass bigger embedding systems if the text to retrieve is extremely specialized and not already seen by a large model. Secondly, not all embeddings layers from all large decoder-only models are good 34 for embedding text. In one of the earlier tests, we used the ones from the last layers of llama3, which gave very poor results. Different retrieval systems might be needed, depending on the type of the questions. We noticed very early that a combination of full -text and vector search was the best for general questions. However, more specific use cases, such as when the user wants to do a literature review or need to extract