language as a probability distribution. For example, unigram LM modelled language as the probability of encountering a single word, like "dog", given a large set of training data, such as Wikipedia. Current state -of-the-art LMs are based on the Transformer neural architecture [5], the main component of which is the self-attention mechanism [6]. In simple terms, the attention mechanism allows these models to focus on different parts of a sentence or document, much like how humans pay attention to different words when understanding language. For instance, in the sentence "The cat sat on the mat," the attention mechanism might assign higher weights to "cat" and "mat" when determining where the cat is located. This mechanism enables LMs to process and understand context more effectively by giving more importance to relevant words while processing text. Researchers have found that scaling up these models can lead to performance improvements. When the parameter scale exceeds a certain level, these language models show some special emergent abilities that are not present in small -scale language models [7]. Those emerging capabilities allow an LLM to solve even tasks they were not specifically trained for, given that the user writes a prompt with a description of the novel tasks and provide s some examples. These approach is called In Context Learning (ICL). To distinguish the difference in parameter scale, the research community has coined the term "large language models" (LLMs) for the LMs of significant size. The research on LLMs has been largely advanced by industry in the last few years, due to the need of huge, concentrated investment in hardware and talent required . A remarkable milestone was the launch of ChatGPT by OpenAI, which has attracted widespread attention from society. Among the many tools that currently use LLMs to process documents, ScopusAI 3 is