- Scores Score 0 Score 1 Score 2 Score 3 Score 4 Score 5 No answer or incorrect answer The general topic is understood, but not the scope of the question. The information is misinterpreted. The answer is correct but is missing important information. The answer is correct but missing minor information. The answer is correct and complete. As previously stated, the full set of questions for each collection is detailed online as explained in Section 2.3. Given the small sample number of the manual usability checks, we use descriptive statistic and plots for the analysis of the results. 22 5 Results and discussion 5.1 Result on the automatic performance evaluation (APE) These are the results and analysis from the experiment to measure the performance of the different RAGEv pipelines on a single benchmark, namely PubMedQA, as described in Section 4.4.1 and 4.5.2. Figure 5 summarises at very high level the results of the 720 experiments, comparing the different RAGEv pipelines (i.e., Vector, Full -text, Hybrid with Rerank, SHy and ColBERTv2), and grouping by all the other factors and levels. Each pipeline (y -axis) was analysed with respect to its performance on short answers (Yes/No) using the accuracy, and with respect to its performance on the long open answer with Rouge and BERT Score metrics. The Figure above strongly suggests that there is a marked improvement in the performance of the LLMs when adding a RAG component (baseline vs all) and that the variations within the different pipelines are not extremely marked with the exception of the SHY pipeline, which consistently score the highest results with an average precision of 0.85 on the binary yes/no questions and average BERTScore F1, precision and recalls of respectively 0.83, 0.79 and 0.88. In the confusion matrix below ( Figure 6), we see the