and that the variations within the different pipelines are not extremely marked with the exception of the SHY pipeline, which consistently score the highest results with an average precision of 0.85 on the binary yes/no questions and average BERTScore F1, precision and recalls of respectively 0.83, 0.79 and 0.88. In the confusion matrix below ( Figure 6), we see the error ratio of the different pipelines on the short answer test. The best one is the Full -text one, very close to the Hybrid and SHy, closely followed by the Vector one. Figure 5 â€“ Pipeline comparison across several metrics. 23 Figure 6 - PubMedQA error analysis, confusion matrix 5.2 Usability checks on the selected collections This section highlights the results produced by the SHy pipeline on the usability checks as well as the domain expert evaluation performed based on the selected list of specific questions for each of the collections. It also shows a comparison of the of the SHy pipeline and those of the domain experts. 5.2.1 Qualitative Evaluation As described in the methodology section, different types of questions were addressed, covering very general questions, to mo re specific ones, but also questions having numeric answers or the answer in different parts of the document such as body of the text, tables, figures, subheading. Based on the characteristics of RAGEv good performance was expected for all text -based questions, while lower performance was expected from questions having answers based on figures, graphs or tables. In fact, the results obtained during the different stages of the development on each of the three collections, demonstrated some issues when extracting information from tables, figures and/or subheadings. Analysis of the answers showed that the splitting of the documents in chunks did not respect the structure of the tables, more in the case