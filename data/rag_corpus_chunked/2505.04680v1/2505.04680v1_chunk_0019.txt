task-specific instructions on Mistral 7B [47]. It outperforms the state-of-the-art bidirectional models on comprehensive embedding benchmarks [48] by utilizing a massive amount of synt hetic data from the proprietary GPT -4 model. LLM2Vec [49] (METB score: 65.01) tries to build the 13 https://openai.com/index/new-embedding-models-and-api-updates/ 12 embedding model from LLMs while only using public available data, but it is still worse in performance than E5-Mistral. Given the notable success of E5 -Mistral, SFR-Embedding-Mistral [50] (METB: 67.56) further fine-tunes it on the blend of non -retrieval and retrieval datasets for improved accuracy on both tasks. The latest embedding model is NV -Embed from NVIDIA, which is very closely related to the SFR approach but uses only public available data and is not dependent on other embeddings models [51]. At the time of the writing of this report, this was the latest ranking of embedding models on the MTEB benchmark [48]: Figure 2 First ten embedding models from MTEB leaderboard (retrieved from HuggingFace, 07/07/24) 4.2.3 Retrieval Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt. The retrieval phase often struggles with precision and recall, leading to the selection of misaligned or irrelevant chunks, and the missing of crucial information. 4.2.4 Generation The posed query and selected documents are synthesized into a coherent prompt to which a large language model is tasked with formulating a response. The modelâ€™s approach to answering may vary depending on task -specific criteria, allowing it