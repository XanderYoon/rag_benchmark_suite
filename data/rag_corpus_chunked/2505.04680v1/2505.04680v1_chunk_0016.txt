the 10 least flexible but potentially most powerful for specifi c tasks. It might also be the only possibility for tasks that involve a new vocabulary , not present in the original pretraining of the LLM. Regarding resource requirements, finetuning typically demands the most computational power, followed by RAG, with pr ompt engineering being the least resource - intensive. Each of these methods comes with its strengths and limitations, and is suited to different scenarios, depending on the specific requirements of the task at hand, available resources, and the desired balance between performance and flexibility. For additional details on whether it is better to use a generalist LLM and prompt it with examples of the desired task (thus using ICL) or to finetune a smaller and specialized model, we refer the interested reader to [28], [29], [30], [31] . For additional details on how to implement RAG, excellent reviews are [32], [33]. 4.2 Taxonomy of a classical RAG system Figure 1 - Main elements in a RAG pipeline and most common problems As shown in Figure 1, t raditional RAG includes three main steps: indexing, retrieval, and generation. Indexing starts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in a vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase. 4.2.1 Indexing The most common chunking strategy is to split the document into chunks on a fixed number of tokens (e.g., 100, 256, 512) [34]. Larger chunks can capture more context, but they also generate more noise,