testing RAGEv. For this selected set, 8 question s, addressing different types of information, such as numerical values, open questions, more specific questions, contained in a table, in a figure, or in section titles, together with the ground truth answer, were prepared. 4.5 Experiments To ensure the efficacy and reliability of RAGEv, we run first a set of experiments on APE and then the manual usability tests on HR, VHT and AMR datasets. 24 https://digital-strategy.ec.europa.eu/en/policies/virtual-human-twins 25 â€œ((â€œdigital twin*") OR (â€œprocess twin*") OR (â€œdata twin*")) AND ((â€œhealthcareâ€) OR (â€œhealth care"))â€ 26 â€˜resistanceâ€™ AND â€˜phageâ€™ AND â€˜therapyâ€™ AND â€˜environmentâ€™ AND â€˜one healthâ€™ 18 4.5.1 Evaluation metrics In all the cases where the answers of the system could be interpreted as a classification task (e.g., a binary yes/no response or an output that could be divided in classes) we used accuracy, precision, recall and F1 Score to present the results of the analysis. For the experiments that assessed the generative abilities of the system with respect to a reference answer provided by a human evaluator, we make use the Rouge and BERT Scores. While the former are more lexicon oriented the latter is more focused on the semantic. In other words, while Rouge Scores compare the answer of a system to the desired one based on the content of their actual words, BERT Scores are more interested on how well two strings of text align on their overall semantic meaning. Originally introduced to evaluated NLP models on summarisation, Rouge Scores are recall - based evaluation systems that search for the overlap between modelâ€™s prediction and desired output at the n-gram level. For example, with n = 1, Rouge 1 will measure the overlap of single words between what the model has predicted, and the gold standard, following the equation: ğ‘…ğ‘œğ‘¢ğ‘”ğ‘’âˆ’ ğ‘ = âˆ‘