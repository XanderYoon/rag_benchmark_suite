their workflow. This assessment allowed further improvement of RAGEv. All together, these four datasets are released under the name of RAGEv-Bench for interested readers to replicate our results and advance research. The datasets will be available in the JRC Data catalogue in the weeks after the publication of this report. 4.4.1 Automatic performance evaluation on PubMedQA (APE) The APE dataset is a subset of PubMedQA21, containing instances of data used for biomedical research Question Answering tasks and built upon a large collection of PubMed papers. It contains questions that are built, either automatically or manually, from the title of each work. Doing so, the dataset allows for each query to be answered in a yes/no/maybe fashion. To help the model answer the question, the dataset contains a set of 3-4 short textual chunks for each query, extracted by the extended abstract of each paper. Altogether, the PubMedQA dataset contains up to two 200 k instances, each containing a PubMed ID, a question, a context (i.e., the short textual snippets), a short answer (yes/no/maybe), and a longer answer. Since the scope of our assessment study is to evaluate RAGEv on full documents, we focused on a subset of 100 documents, extracted using the PubMed ID. This number was in the end reduced to 88, as we considered only the ones in public acces s to ensure reproducibility of this study. 4.4.2 Horizon research (HR) This collection focuses on the analysis of the scientific output of research project s. The EU is successfully funding research and innovation projects via its dedicated Framework programme, which generate an impressive amount of knowledge and data, accessible via CORDIS22. SOPLAS23, a project on Macro and Microplastic in Agricultural Soil Systems, was chosen to test the performance of the RAGEv pipelines with unrelated documentation. SOPLAS reports 3