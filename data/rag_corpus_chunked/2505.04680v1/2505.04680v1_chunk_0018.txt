tasks. Additionally, the embedding-based retriever plays a crucial role in retrieval -augmented generation [24], which allows LLMs to access the most up-to-date external or proprietary knowledge without modifying the model parameters [39], [40], [41], [42]. Word2Vec [43] was the first neural network -based approach to predict surrounding words for a target word in each context. It learns to encode word semantics into vectors. GloVe [44] (Global Vectors for Word Representation) leverages global statistics to create embeddings. It captures co-occurrence patterns between words in large text corpora. BERT [38] (Bidirectional Encoder Representations from Transformers), a transformer -based model, generates contextualized word embeddings by considering both left and right context. This is one of the most widely used for various NLP tasks. Furthermore, SBERT [45] (Sentence BERT) is specifically designed for sentence similarity tasks. While BERT embeddings can be used for sentences, SBERT embeddings are more effective for capturing sentence semantic similarity. Decoder-only LLMs [7] were believed to underperform bidirectional models on general - purpose embedding tasks due to u nidirectional attention that limits the representation learning capability, along with the scaling of LLMs which leads to very high -dimension embeddings, that may in turn suffer from the curse of dimensionality. Nevertheless, the latest text -embedding-3-large from OpenAI obtain ed a MTEB score of 64.5913 in 2024, opening the way to practical use of LLMs as embedders. Furthermore, E5 - Mistral [46] obtained a METB score of 66.63, applying contrastive learning with task-specific instructions on Mistral 7B [47]. It outperforms the state-of-the-art bidirectional models on comprehensive embedding benchmarks [48] by utilizing a massive amount of synt hetic data from the proprietary GPT -4 model. LLM2Vec [49] (METB score: 65.01) tries to build the 13 https://openai.com/index/new-embedding-models-and-api-updates/ 12 embedding model from LLMs while only using public available data, but it is still worse in