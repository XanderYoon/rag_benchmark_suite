of all three subset of BERT Scores to evaluate generative answer (i.e., non-class-based) in each experiment. 4.5.2 Automatic performance evaluation (APE) The reference dataset for the APE is a subset of PubMedQA, as described in Section 4.3.2. All system configurations in Figure 3 were evaluated using a Vanilla model and RAG pipeline, to answer medical questions. From the questions, we extracted both a yes-no short answer and a long, more detailed explanation. To measure the performance of the system, both standard metrics, such as accuracy and F1 scores, as well as generation -assessing metrics, namely Rouge Scores were adopted. More details can be found in previous Section 4.5.1 19 Figure 4 - Design of experiments: we test both the Retrieval and Generation part of the system. The experiments are designed as multifactor with a structure of 2x2x5x2x3x2x3 as depicted in Figure 4. This amounts to a total of 720 experiments, for which we use the mnemonic codes CKw-EMB-PIP-#c-RER-RTH-MOD such as, for instance: 10-ADA-TEX-GPT, 50-SFR-SHY- NOU etcâ€¦ We additionally add a set of 3 experiments without the use of RAG, that is, interrogating directly the LLM without providing any context, to establish the baseline. We will identify those experiments as NORAG-{GPT, LLA, NOU}. To analyse the results of our factorial experiment, we have used two approaches. First, a four- way Ana lysis of Variance (ANOVA) to examine the main effects of each factor and their interactions. In this case, we first verify that our data meets the ANOVA assumptions: normality of residuals, homogeneity of variances, and independence of observations. Then we use pandas and penguin statistical packages in python to calculate the main effects for each factor, two-way interactions, three-way interactions, and the four-way interaction. For the interpretation of the results, we focus on the F -values and p -values for