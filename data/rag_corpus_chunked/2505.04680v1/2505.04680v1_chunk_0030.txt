evaluated NLP models on summarisation, Rouge Scores are recall - based evaluation systems that search for the overlap between modelâ€™s prediction and desired output at the n-gram level. For example, with n = 1, Rouge 1 will measure the overlap of single words between what the model has predicted, and the gold standard, following the equation: ğ‘…ğ‘œğ‘¢ğ‘”ğ‘’âˆ’ ğ‘ = âˆ‘ ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘šğ‘ğ‘¡ğ‘â„(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›)ğ‘”ğ‘Ÿğ‘ğ‘šğ‘› ğœ– ğ‘† âˆ‘ ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ (ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›)ğ‘”ğ‘Ÿğ‘ğ‘šğ‘› ğœ– ğ‘† where n stands for the length of the n -gram ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›, ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘šğ‘ğ‘¡ğ‘â„(ğ‘”ğ‘Ÿğ‘ğ‘šğ‘›) is the maximum number of n -grams (a single word, in the case n = 1, or uni -gram) co-occurring in the gold standard answer and the answer produced by the system. While Rouge 1 and 2 respectively set n =1 and n = 2, Rouge L refers to the longest common subse quence shared between automatic and gold standard answers. The difference between Rouge L and LSum lays in the fact that Rouge LSum considers each sentence in a given answer independently, whereas Rouge L does not. Given two textual strings A and B, BERT Scores [54] use a multi-step procedure to find a word- level alignment between A and B, and later measure a score for the pair, by weighting the semantic similarity between aligned words, using cosine similarity. To obtain such a score, authors have adapted precision, recall and F1 scores (i.e., BERT Score Precision, BERT Score Recall, BERT Score F1). In this work, we make use of all three subset of BERT Scores to evaluate generative answer (i.e., non-class-based) in each experiment. 4.5.2 Automatic performance evaluation (APE) The reference dataset for the APE is a subset of PubMedQA, as described in Section 4.3.2. All system configurations in Figure 3 were evaluated using a Vanilla model and RAG pipeline, to answer medical questions. From the questions, we