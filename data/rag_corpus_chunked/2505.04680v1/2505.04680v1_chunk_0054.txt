human score value. Error bars repost standard error of the mean Figure 8 - Comparison between the human evaluation and the BERT Score metric. Error bars represent standard error of the mean 32 To better grasp the interaction between the human and machine generation scores, we performed a mixed effect linear regression analysis, comparing a full model (i.e., Human Score ~ Pipeline * Collection * Type of Question + BERTScore F1 + (Question id | 1)) with a base one (i.e., Human Score ~ BERTScore F1 + (question | 1)). A GLRT analysis confirmed that more complex model was significantly better predictor of the data ( p < 0.001). While most of the fixed effects (as well as their interactions) where not significantly impactful, BERTScore F1 factor was found to have a positive and significant effect ( p = 0.001). This overall positive relation between human annotations and BERTScores (F1) can be better appreciate in Figure 9, that summarises how scores vary across the spectrum (error bars indicate standard error of the mean). On the other hand, From Figure 10, where results from Figure 9 are further spitted by collection, one can better appreciated why other factors were not found to be significant in our statistical analysis. Figure 10 - Visual correlation analysis between BERTScores F1 and Human with respect to pipeline version and Collection. Scores are collapsed by points in each human score value. Error bars repost standard error of the mean 33 6 Conclusions and recommendations Building a fully factual and trustworthy pipeline to analyse documents with LLMs is still complex, even if many tools and techniques are currently available. In this report, we contributed: (1) RAGEv - a reference pipeline which gets state-of-the-art results on a public benchmark of scientific papers extracted from the PubMedQA datasets, (2)