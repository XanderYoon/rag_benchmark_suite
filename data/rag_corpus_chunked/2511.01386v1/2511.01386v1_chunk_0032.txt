to expose similar behavior as “agentic RAG”: an agent (LLM) has retrieval as a callable tool and can iteratively reformulate queries, gather more evidence, and only then answer [64, 77]. This is structurally close to Self-RAG, but usually implemented without specialized fine-tuning: the agent is instructed (via prompting) to reason step-by-step, retrieve when uncertain, and verify answers against retrieved context [77]. The key point is the feedback loop: retrieval quality is not treated as fixed, but is adaptively critiqued and expanded by the model itself [75, 77]. Summary.Agentic/self-reflective approaches automateinference-time control: they let the model itself decide how aggressively to retrieve, how to refine queries, and whether its own claims are grounded [64, 75, 77]. They donot, however, automatically choose theunderlyingretriever family, chunking policy, or reranker for each domain; those architectural decisions are still assumed to be fixed and correct. 2.7 Positioning Our Framework Across these strands, we observed the following fundamental limitation. Automated/agentic work addresses only pieces of the search problem. AutoRAG-style methods explore module choices or let the model iteratively steer retrieval [63, 64]. DSPy-style optimizers tune prompts and module wiring to maximize a metric [66, 68]. Self-RAG and agentic RAG teach models to critique retrieval at inference time and request better evidence [75, 77]. RAGAS and RAGXplain move toward automated evaluation and actionable debugging of RAG systems [70, 73]. What isstillmissing is a unified, dataset-drivendiscoveryframework that: (i) systematically enumerates and instantiatesentireRAG pipelines—including Pre-Embedding, Query Expansion, Retrieval, Reranking, Passage Filtering, Passage Augmentation, Passage Compression, Prompt Making and Post-Generation; (ii) runs these candidate pipelines on a target corpusin that corpus’s domain and language; (iii) scores them with objective metrics (retrieval metrics, LLM-Judge, semantic score); and (iv) produces an explicit ranking of which pipeline works best for that corpus. Our framework is built exactly for that role. Unlike generic orchestration