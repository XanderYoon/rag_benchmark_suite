answer should be auditable, sentence by sentence, against known sources. This is especially important for high-stakes domains (biomedicine, law, finance), where unverifiable claims are unacceptable. 2.4.3 Long-Context LLMs vs. Retrieval-Augmented Generation An increasingly popular alternative to retrieval is to skip retrieval entirely and simply feed the model very large chunks of source text. Commercial frontier models have rapidly expanded context windows (e.g., hundreds of thousands of tokens up to the order of a million tokens), making it technically feasible to “paste the corpus into the prompt” and ask the model to answer directly [38, 39]. This 10 long-context (LC) paradigm challenges the premise that wemustretrieve, rerank, and truncate before generation. However, two practical issues keep RAG competitive. First, cost and efficiency: attention over hundreds of thousands of tokens is computationally expensive, and most API pricing scales (roughly) with tokens. RAG can aggressively narrow the context to only the most relevant passages, often yielding similar answers with far fewer tokens processed. Second, precision and controllability: retrieval acts as a targeted filter, regularizing the model to condition on high-salience evidence instead of absorbing an entire noisy corpus. Empirical head-to- head studies comparing LC models to RAG-style systems find that LC can outperform RAG on some long-context benchmarksifthe model is allowed to read the full source, but RAG remains substantially more cost-efficient and can approach LC quality when paired with dynamic routing strategies that decide, per query, whether retrieval alone is enough or whether to escalate to a full long-context read [40]. This hybrid view—“RAG first, escalate to LC only when needed”—suggests that post-retrieval generation is not a solved one-size-fits-all problem. Rather, it is a decision policy conditioned on domain, latency budget, and acceptable risk of hallucination. The generator side of RAG is highly configurable. Systems differ inhowthey admit evidence (FiD vs. concatenation