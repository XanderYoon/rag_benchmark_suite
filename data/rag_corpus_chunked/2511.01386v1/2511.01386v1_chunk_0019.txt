produce answersandquote supporting spans, and even to abstain when evidence is insufficient [32]. Subsequent attribution frameworks argue that model outputs should beAttributable to Identified Sources(AIS): every factual statement in the answer should be traceable to some cited snippet, and that snippet should actually support the claim [33]. More recent “locally attributable” generation goes further by restructuring decoding into stages: first select fine-grained supporting spans from the retrieved corpus, then plan and realize each output sentence so thatevery sentenceis paired with concise, sentence-level evidence pointers [34]. This moves beyond coarse “URL-style” citations toward token-level grounding. Constrained or guided decoding.Another approach is to algorithmically constrain decoding to stay anchored to retrieved evidence. Reranking and constrained decoding have been explored in multi-step RAG agents that repeatedly retrieve, verify, and revise candidate answers, editing or rejecting unsupported claims instead of emitting them verbatim [35, 36]. These systems increasingly frame answer generation as an iterative “research-then-edit” loop: draft an answer, verify each claim against retrieved sources (possibly with explicit follow-up retrieval), and patch or delete hallucinated spans before returning the final answer. Self-RAG operationalizes a related idea end-to-end: a single model learns when to retrieve, how to incorporate that evidence, and how to critique (and, if needed, correct) its own draft using special reflection tokens, yielding measurable gains in factuality and citation quality in open-domain QA and long-form generation [37]. Taken together, these methods treat grounding not as an afterthought but as a contract: the answer should be auditable, sentence by sentence, against known sources. This is especially important for high-stakes domains (biomedicine, law, finance), where unverifiable claims are unacceptable. 2.4.3 Long-Context LLMs vs. Retrieval-Augmented Generation An increasingly popular alternative to retrieval is to skip retrieval entirely and simply feed the model very large chunks of source text. Commercial frontier models have rapidly expanded context