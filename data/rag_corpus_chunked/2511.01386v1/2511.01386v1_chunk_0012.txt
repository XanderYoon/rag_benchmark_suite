generate plausible queries for each document, effectively augmenting the document index with “imagined” questions and improving matching for sparse and dense retrievers alike [20]. More recent approaches invert that direction: instead of expanding documents, they expand thequery. Hypothetical Document 6 Embeddings (HyDE) prompt a large language model to generate a short “pseudo answer passage” for the user question; that passage is then embedded and used as the retrieval query in a dense vector index [21]. HyDE improves dense retrieval in cases where the original user query is short, underspecified, or informal, because the generated hypothetical passage provides richer semantic context [21]. In conversational and multi-turn question answering, query rewriting aims to recover an explicit, context-independent query from anaphoric or underspecified user turns (e.g., “What abouthis patents?” or “And where didthathappen?”). Systems generate a fully specified standalone query that encodes the conversation history, then submit that rewritten query to retrieval [22, 23]. This is critical for conversational search and dialogue-style assistants, where raw user turns are often elliptical and cannot be directly matched in the index. Large language models (LLMs) now make it practical to do on-the-fly query rewriting, self- expansion, or self-clarification at inference time. For example, black-box retrieval-augmented generation systems such as RePlug treat the retriever as a modular component and use iterative query reformulation to surface better supporting evidence before final answer generation [24]. This LLM-driven reformulation can inject synonyms, paraphrases, or domain terms that the original user did not explicitly mention, improving recall@k in downstream RAG. Overall, query rewriting / expansion increases the chance that at least one relevant chunk appears in the retrieved set, which directly benefits the generator in RAG by exposing it to more grounded evidence. 2.3.2 Instruction-to-Query Mapping and Search Intent Normalization In many practical deployments, users do not issue “search queries”; they