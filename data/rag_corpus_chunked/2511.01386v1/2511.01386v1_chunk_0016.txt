is retrieved, and retrieval is heavily influenced by how the query is phrased, these conditioning steps are not optional engineering details but central design decisions. Our broader framework treats these query-conditioning choices as part 8 of the configurable retrieval stack: rather than assuming a single “raw user question→ retriever” pathway, we explicitly acknowledge that different domains may require different pre-retrieval strategies to maximize recall and downstream factual grounding. 2.4 Post-Retrieval Aggregation and Answer Generation Retrieval-augmented generation (RAG) is often described as a two-stage pipeline: retrieve relevant evidence, then generate an answer conditioned on that evidence [7, 9]. In practice, however, the “generation” half of the pipeline is itself a rich design space. How retrieved evidence is filtered, organized, presented to the model, and enforced at decoding time strongly affects factuality, style, hallucination rate, and cost. This section surveys the main classes of post-retrieval control: (i) context integration strategies, (ii) hallucination mitigation and grounding, and (iii) the growing alternative of using long-context LLMs instead of retrieval, and how that trend interacts with RAG. 2.4.1 Context Integration Strategies Naive concatenation / prompt stuffing.The most direct RAG formulation (sometimes called “vanilla RAG”) simply concatenates the top-k retrieved passages with the user query and feeds that augmented prompt to a generator such as a seq2seq model or an instruction-tuned LLM [7]. This strategy treats retrieval as a context expander: the generator’s decoder attends jointly over all retrieved text plus the question. While simple, this approach couples answer quality to the model’s context budget. Ask (and thus total tokens) grows, the model may face attention dilution: relevant evidence is present somewhere in the prompt, but it competes with distractors. This motivates more deliberate curation of which passages actually reach the generator. Rerank-then-truncate (evidence curation).A widely used refinement is toover-retrieve(e.g., dozens of passages), then apply a