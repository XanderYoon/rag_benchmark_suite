loop to reduce hallucination. [36] S. Dhuliawala and M. et al., “Chain-of-verification reduces hallucination in large language models,” inFindings of ACL, 2024. Plan verification questions, fact-check draft answers, then revise. [37] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning to retrieve, generate, and critique through self-reflection,” inarXiv:2310.11511, 2023. Single model that adaptively retrieves, cites, and self-critiques. [38] Anthropic, “Claude 3.5 sonnet technical report / model card,” 2024. Frontier LLM advertised with∼200K-token context window. [39] Anthropic, “Claude sonnet 4 / long-context release notes,” 2025. Reported∼1M-token context handling. [40] Z. Li, C. Li, M. Zhang, Q. Mei, and M. Bendersky, “Retrieval-augmented generation or long- context LLMs? a comprehensive study and hybrid approach,” inarXiv:2407.16833, 2024. Benchmarks RAG vs. long-context and proposes Self-Route routing. [41] G. Izacard, E. Grave, M. Dehghani, O. Vinyals, L. Hosseini, I. Beltagy, N. Cancedda, S. Lam- prier, and G. Obozinski, “Contriever: Unsupervised dense passage retrieval,” inInternational Conference on Learning Representations (ICLR), 2022. 40 [42] X. Zhang, X. Ma, Xueguang Wang, J. Callan,et al., “Mr.tydi: A multi-lingual benchmark for dense retrieval,”arXiv preprint arXiv:2108.08787, 2021. [43] N. Thakur, N. Reimers, J. Daxenberger, A. Kamath, and I. Gurevych, “Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models,” inAdvances in Neural Information Processing Systems (NeurIPS), 2021. [44] J. Lin, X. Ma, S.-C. Lin, R. Pradeep, and R. Nogueira, “Pyserini: An easy-to-use python toolkit to support replicable ir research with sparse and dense representations,” inProceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2021. [45] J. Maillard, L. Wu, F. Petroni, A. Piktus, P. Lewis, É. G. Érman, and S. Riedel, “Multi-task retrieval for knowledge-intensive tasks,” inProceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. [46] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer,