retrieve k∈ [20, 100]candidates using BM25 or DPR, then apply a cross-encoder BERT ranker to produce the final ordering [12]. For RAG, this increases the chance that truly relevant passages 4 appear in the top few contexts fed into the generator, which in turn improves grounding and factual accuracy. Hybrid sparse + dense retrieval.Hybrid retrieval fuses lexical and dense signals, for example by linearly combining BM25 scores with dense similarity scores, or by interleaving candidate sets from both retrievers before reranking [8, 13]. The intuition is that sparse retrieval excels at entity-level lookup and robustness to domain-specific terminology, while dense retrieval excels at semantic generalization and paraphrase matching. Empirical work has repeatedly shown that hybrids often outperform either method alone, especially in heterogeneous corpora where some answers are phrased almost exactly like the query and others are paraphrased [13]. From a RAG perspective, this matters because hybrid retrieval improvesrecall@k—the probability that at least one truly answer-bearing passage is included in the retrieved set. Higher recall@k directly increases the headroom for the downstream generator. 2.2.1 Index Structure, Chunk Granularity, and Memory Access Choosinghowto store and retrieve passages at scale is as important as choosingwhatscoring function to use. Approximate nearest neighbor (ANN) indexes.Dense retrieval relies on fast nearest-neighbor search in high-dimensional vector spaces. Exact search over millions of vectors can be prohibitively slow, so most systems use approximate nearest neighbor (ANN) data structures. FAISS popularized GPU-accelerated and CPU-efficient ANN search via product quantization, inverted file (IVF) lists, and hierarchical clustering [14]. Other widely used ANN structures include graph-based methods such as Hierarchical Navigable Small World (HNSW) graphs, which enable logarithmic-time approximate search with strong recall in practice [15]. Indexchoice directly affectslatency, memoryfootprint, andrecall. Forexample, highlycompressed product-quantized indexes are memory-efficient but may degrade fine-grained similarity; HNSW-style graphs provide excellent recall but can consume