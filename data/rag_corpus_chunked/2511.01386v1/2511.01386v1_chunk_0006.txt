this setup, generation is explicitly grounded in retrieved knowledge, which helps prevent hallucinated content and encourages factual attribution to specific passages. This retrieve-then-generate design also supports domain adaptation without fully fine-tuning large models. A pretrained generator can be reused across domains, while retrieval is redirected to a new in-domain corpus (e.g., internal manuals, scientific articles, legal databases) without retraining the full language model [7, 11]. Dense retrievers such as DPR can also be adapted or further trained on task-specific relevance signals [8], and FiD-style generators can scale to larger evidence sets from that new corpus [9]. This offers a practical alternative to full supervised fine-tuning of a large model for every new knowledge domain. One limitation of early RAG-style models is that they often concatenate only a small number of passages into the generator’s input, creating a bottleneck when many relevant contexts exist. Fusion-in-Decoder (FiD) addressed this by encoding each retrieved passage independently and letting the decoder attend across all encoded representations, rather than concatenating them into a single long input [9]. This architecture allows conditioning on dozens or even hundreds of retrieved passages without collapsing them into a single flat context window, and it has become a strong baseline for retrieval-augmented question answering and knowledge-intensive generation [9]. The pipeline above is the de facto baseline that most modern retrieval-augmented systems inherit [7–9]. Our framework assumes this baseline and asks a higher-level question: given a particular dataset or deployment domain,which concrete retrieval and generation design choices (indexing strategy, retriever type, fusion strategy, number of passages, etc.) actually work best in practice? The work in this study builds on the RAG foundations summarized here and treats them as the starting point for systematic comparison. 2.2 Retrieval Methods and Indexing Strategies Before generation, there is already a large design space in how