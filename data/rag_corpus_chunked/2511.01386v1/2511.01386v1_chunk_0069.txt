contributes: (1) a holistic pipeline optimization approach that accounts for inter-component interactions rather than greedy per-module selection; (2) a question type sensitivity framework linking question distributions to optimization potential of RAG pipelines; (3) empirically- grounded design principles mapping dataset characteristics to effective module combinations; (4) identification of robust RAG components alongside domain-adaptive extensions. These contributions collectively advance our understanding of RAG system design and provide practitioners with both methodology and guidance for optimizing RAG pipelines across diverse domains and question types. 7 Acknowledgement We acknowledge the open-source RAG community whose tools and code informed this work, and the Wikipedia volunteer editors and the Wikimedia Foundation for making key data openly available. We used AI-assisted tools for copy-editing under the authors’ supervision; the authors remain responsible for all content and any errors. 37 References [1] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” inAdvances in Neural Information Processing Systems, 2020. [2] L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei, “Query2doc: Query expansion with large language models,”arXiv preprint arXiv:2303.07678, 2023. [3] W. Lin, Z. Qian, G. Jiang, J. Wang, Y. Wu, X. Wang, and M. Zhang, “Rankgpt: Is chatgpt good at search and reranking?,” inProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 9821–9836, 2024. [4] O. Li, M. Alikhani, and S. Gehrmann, “Contextual position encoding: Learning to count what’s important,”arXiv preprint arXiv:2405.18719, 2024. [5] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion: Language agents with verbal reinforcement learning,”Advances in Neural Information Processing Systems, vol. 36, 2024. [6] L. Gao, Z. Dai, and J. Callan, “Retrieval-augmented generation for large language models: A survey,”arXiv preprint arXiv:2312.10997, 2023.