/ Internal Corpora Enterprise search, legal QA, biomedical and clinical QA, and internal engineering wikis all share two structural properties: (i) their corpora are private and often volatile, (ii) the required answers are high-stakes (compliance, patient safety, incident response). This combination makes generic public RAG recipes brittle. Enterprise & internal documentation.Corporate knowledge bases contain heterogeneous artifacts: Markdown specs, Jira tickets, long PDF reports, architecture diagrams, postmortems, meeting transcripts. These artifacts evolve rapidly and are sometimes mutually contradictory. Thus, retrieval cannot rely purely on static offline indexing; instead, pipelines often integrate recency-aware filters (most recent incident runbook first), access control filters (only retrieve docs the current user can see), and role-specific answer style (e.g., a summary for executives vs. a step-by-step fix for on-call engineers). Hybrid sparse+dense retrieval plus heavy reranking is common here because internal acronyms and code names are poorly covered in general-domain embeddings [43–45]. Biomedical / scientific QA.Biomedical RAG faces extremely long, jargon-dense PDFs (papers, trial reports) and a need for verbatim grounding: models are often required to cite PubMed IDs and cannot “hallucinate” mechanisms of action [55–57]. Domain-specialized encoders such as BioBERT, PubMedBERT, SciNCL, or BioLinkBERT are explicitly trained on biomedical abstracts and clinical notes to improve retrieval recall over PubMed-scale literature [56, 58–60]. Downstream generation is further constrained by factuality filters and safety rails (e.g., “do not provide treatment advice”), which are typically wired in at decoding time rather than left to general-purpose instruction tuning [55, 60]. Legal / regulatory QA.In legal RAG, “hallucinating” a precedent or misquoting a clause can be sanctionable. Accordingly, systems often force extractive answers: the model must produce a span copied from retrieved statutes or case law, optionally followed by a short natural-language 13 paraphrase. Retrievers for this domain are commonly fine-tuned on statute-to-clause relevance and trained to respect jurisdictional boundaries