are motivated by three goals: grounding generated text in explicit evidence, improving factuality, and enabling domain adaptation without full model fine-tuning. Retriever.The retriever maps the input query to a small set of relevant text chunks drawn from a large corpus. Early open-domain QA systems often relied on sparse lexical retrievers such as TF–IDF or BM25, sometimes followed by a neural reader [10]. More recent pipelines use dense retrieval, where both queries and passages are embedded into a shared vector space using a dual-encoder model trained to bring matching question–passage pairs closer together [8]. Dense Passage Retrieval (DPR) showed that dense bi-encoder retrievers, trained with in-batch negatives and supervised question–evidence pairs, can outperform sparse retrieval on open-domain QA benchmarks while remaining efficient at scale [8]. Conditioning generation on retrieved text directly supports factual grounding. Because the retriever surfaces concrete evidence at inference time, the downstream generator can attribute claims to specific passages rather than relying purely on internal memorization [7]. This external grounding is important for transparency and verifiability in knowledge-intensive tasks such as open-domain QA and fact attribution [7, 9]. 3 Generator.Given the retrieved passages, a generator (usually a sequence-to-sequence transformer) produces an answer conditioned on both the query and the retrieved evidence [7]. Lewis et al. introduced the term “Retrieval-Augmented Generation” for a model that jointly learns the retriever and the generator: the retriever proposes candidate passages, and the generator marginalizes over them during training and inference [7]. In this setup, generation is explicitly grounded in retrieved knowledge, which helps prevent hallucinated content and encourages factual attribution to specific passages. This retrieve-then-generate design also supports domain adaptation without fully fine-tuning large models. A pretrained generator can be reused across domains, while retrieval is redirected to a new in-domain corpus (e.g., internal manuals, scientific articles, legal databases) without retraining the