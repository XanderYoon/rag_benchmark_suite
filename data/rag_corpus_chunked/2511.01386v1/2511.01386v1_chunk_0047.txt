reflecting the different nature of Wikipedia articles across domains. Medicine demonstrates the highest content volume (106,552 tokens) and chunk density (50.9 chunks/article), characteristic of comprehensive medical encyclopedia entries covering symptoms, treatments, mechanisms, and related conditions. Computer Science has the most compact representation (34.4 chunks/article, 65,157 tokens), reflecting the more focused, definition-oriented nature of technical computing articles. This variation in dataset characteristics proved instrumental in understanding the efficacy of different RAG technique combinations. Analysis of question categories across datasets reveals important patterns that correlate with RAG performance improvements. Table 2 presents the distribution of question types. Mathematics exhibits the highest proportion oflong-answerquestions (41%), followed by Com- puter Science (33%). Conversely, Medicine (53%), Finance (51%), and Law (49%) demonstrate a predominance ofinterpretationquestions. As we demonstrate in subsequent sections, this distribution has significant implications for the effectiveness of different RAG strategies. 4.3 Baselines The baseline condition for the experiment is defined as a naive RAG (i.e., vanilla RAG) configuration instantiated within RAGSmith. In this setting, none of the advanced RAG techniques are enabled; the system relies solely on vector-based retrieval and straightforward answer generation. The 24 performance obtained under this configuration for each of the six datasets constitutes the baseline against which all other configurations are compared. 4.4 Metrics and Evaluation For retrieval evaluation, we computed an equally weighted aggregate of mean Average Precision (mAP), normalized Discounted Cumulative Gain at rank (k) (nDCG@k), Recall@k, and Mean Reciprocal Rank (MRR). For generation evaluation, we used an equally weighted combination of two metrics: (i) a semantic similarity score derived from an embedding model, and (ii) an LLM-based judgment produced by a sufficiently large language model. These two metrics were likewise assigned equal weights. Beyond separately evaluating retrieval and generation performance, we introduced a unified scalar metric to improve interpretability and to act as the fitness