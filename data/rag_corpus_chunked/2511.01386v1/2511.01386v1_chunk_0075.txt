44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2021. [45] J. Maillard, L. Wu, F. Petroni, A. Piktus, P. Lewis, É. G. Érman, and S. Riedel, “Multi-task retrieval for knowledge-intensive tasks,” inProceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. [46] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du,et al., “Lamda: Language models for dialog applications,”arXiv preprint arXiv:2201.08239, 2022. [47] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,et al., “Training language models to follow instructions with human feedback,” inAdvances in Neural Information Processing Systems (NeurIPS), 2022. [48] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-Brown, and Y. Shoham, “In-context retrieval-augmented language models,” 2023. [49] E. Choiet al., “Towards responsible and ethical qa systems,” inProceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. [50] S. Narayan, O. Shapira, A. Caciularu, D. Lahav, T. Deleu, A. F. T. Martins, R. Schwartz, and P. West, “Conditional generation with grounded factuality constraints,” inProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022. [51] X. Zhang, N. Thakur, S. MacAvaney, A. Vtyurina, M. Abualsaud, J. Lin,et al., “Miracl: Multilingual information retrieval across a continuum of languages,” inProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022. [52] N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings using siamese bert-networks,” inProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP) and the 9th International Joint Conference on Natural Language Processing, 2019. [53] B. Oguz, W.-t. Yih, X. Chen, E. Choi, P. Lewis, S. Min, L. Zettlemoyer, and D. Chen, “Domain-matched pre-training tasks