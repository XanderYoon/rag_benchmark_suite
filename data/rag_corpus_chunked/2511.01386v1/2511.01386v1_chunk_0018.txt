FiD-like decoders are frequently used as strong RAG baselines because they relax the tension between recall (retrieve many passages) and tractable decoding. Reasoning-aware integration (retrieval↔ chain-of-thought).Classical RAG assumes a single retrieval step before answer generation. However, complex or multi-hop questions often require iterativeevidence gathering. Recent work proposes interleaving retrieval with step-by-step reasoning, 9 so that intermediate hypotheses guide what to fetch next, and the newly retrieved evidence, in turn, updates the reasoning chain. For example, ReAct prompts an LLM to alternate between reasoning steps and tool calls (such as web or corpus search), explicitly writing out what it knows, what it needs next, and how it will use the retrieved snippet in the final answer [28]. IRCoT (Interleaving Retrieval with Chain-of-Thought) follows a similar philosophy for multi-hop QA: the model emits partial chain-of-thought (CoT) rationales, retrieves again conditioned on those partial rationales, and then continues reasoning with the new evidence [31]. This style of retrieval-aware reasoning reduces the classic failure mode where a one-shot retriever misses a crucial intermediate fact and the generator hallucinates a bridge. 2.4.2 Hallucination Mitigation and Grounding Even with high-quality retrieval, large language models may still generate assertions that are unsupported (or even contradicted) by the retrieved evidence. A major thread in post-retrieval aggregation research is therefore:make the model prove its claims. Citation-enforcing generation and attribution.One line of work directly couples generation with explicit citations. Early large “open-book” QA systems such as GopherCite trained models to produce answersandquote supporting spans, and even to abstain when evidence is insufficient [32]. Subsequent attribution frameworks argue that model outputs should beAttributable to Identified Sources(AIS): every factual statement in the answer should be traceable to some cited snippet, and that snippet should actually support the claim [33]. More recent “locally attributable” generation goes further by restructuring decoding into stages: first