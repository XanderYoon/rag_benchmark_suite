among the explored RAG techniques. This suggests that these two components constitute a robust, subject-agnostic backbone. In contrast, other modules—such as query expansion, reranking, and prompt maker—exhibit more subject-dependent behavior and operate as adaptive extensions that can be tuned to the specifics of a given task or dataset. The remainder of this paper is organized as follows: Section 2 reviews related work in advanced RAG methods and modular pipeline design. Section 3 describes our modular RAG framework and evolutionary search methodology. Section 4 details experimental setup, datasets, and evaluation metrics. Section 5 presents results and discussion of our experiment. Section 6 concludes with key findings and implications for RAG system design. Our codebase and datasets used in this paper can be found athttps://github.com/yAquila/RAGSmith. 2 Related Work 2.1 Retrieval-Augmented Generation (RAG) Foundations Retrieval-Augmented Generation (RAG) refers to a general inference-time paradigm in which a language model is provided with externally retrieved evidence before (or while) generating an answer [7]. The core idea is simple:retrieve first, then generate. Instead of relying solely on parametric knowledge stored in model weights, a RAG system queries a corpus, selects relevant passages, and conditions the generator on those passages to produce the final output. This retrieve-then-generate loop has become a standard approach for open-domain question answering, knowledge-intensive reasoning, and adaptation to specialized domains [7–9]. A “vanilla RAG” pipeline typically has two main components: (i) a retriever, and (ii) a generator. Beyond this high-level structure, such systems are motivated by three goals: grounding generated text in explicit evidence, improving factuality, and enabling domain adaptation without full model fine-tuning. Retriever.The retriever maps the input query to a small set of relevant text chunks drawn from a large corpus. Early open-domain QA systems often relied on sparse lexical retrievers such as TF–IDF or BM25, sometimes followed by a neural