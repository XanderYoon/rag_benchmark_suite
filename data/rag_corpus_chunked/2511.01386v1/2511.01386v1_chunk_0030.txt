instead of ad hoc prompts [66, 67]. DSPy provides “optimizers” that automatically adjust prompt templates and, in some cases, lightweight model parameters to maximize a user-defined metric such as answer accuracy or factuality [68, 69]. In other words, you specifywhatyou want (a QA module grounded in evidence, a retriever call followed by a summarizer, etc.) plus a metric, and DSPy searches prompt variants and module wiring to improve that metric, rather than relying on manual prompt tinkering [66, 68]. This line of work automates prompt engineering and sometimes retriever–generator pairing for a task, but typically assumes a fixed overall pipeline structure (e.g., “retrieve then answer”) and a single task/domain at a time [66, 69]. Automated evaluators and diagnostic feedback.A related ecosystem aims toevaluateand debugRAG pipelines with minimal human labeling. RAGAS (Retrieval-Augmented Generation Assessment) proposes largely reference-free metrics—faithfulness, context relevance, answer relevance, etc.—computed by LLMs themselves to estimate whether answers are grounded in retrieved evidence [70, 71]. Subsequent work has adapted and stress-tested these metrics in applied domains such as telecom QA, highlighting both their promise and their brittleness under domain shift [72]. More recently, RAGXplain argues that evaluation should not stop at scores: it converts evaluation signals intoactionable guidance, surfacing where the pipeline is failing (retrieval, ranking, synthesis, etc.) and recommending concrete fixes [73, 74]. These systems move toward “closed-loop” optimization: measure, diagnose, suggest improvements. Summary.Automatic RAG tooling already acknowledges that (i) different corpora demand different retrieval/generation behavior, and (ii) LLM-based judges can guide tuning [63, 70, 73]. But existing systems usually optimize one slice at a time: iterative retrieval control [64], prompt templates [66], or evaluator-driven suggestions [73]. They rarely provide asystematic, corpus-driven search across the full pipeline design space(chunking granularity, hybrid vs. dense retrievers, rerankers, generator style, etc.) and thenrankthose pipelines with objective task metrics. 15 2.6.2 Agentic