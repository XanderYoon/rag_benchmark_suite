approach couples answer quality to the model’s context budget. Ask (and thus total tokens) grows, the model may face attention dilution: relevant evidence is present somewhere in the prompt, but it competes with distractors. This motivates more deliberate curation of which passages actually reach the generator. Rerank-then-truncate (evidence curation).A widely used refinement is toover-retrieve(e.g., dozens of passages), then apply a stronger reranker—often a cross-encoder that jointly encodes (query, passage) with a large Transformer such as BERT [29]—to reorder candidates by estimated task-specific relevance. The generator then only sees the topL passages under this reranker, where L is chosen to fit the model’s context window. Many systems additionally apply redundancy-aware selection such as Maximal Marginal Relevance (MMR), which trades off relevance and novelty, so that the final context covers diverse supporting evidence instead of repeating the same snippet phrased slightly differently [30]. Operationally, this “rerank-then-truncate” stage is already a nontrivial hyperparameter surface: Which reranker? How many passages survive? Do we optimize for precision (few, high-quality passages) or for recall (broad coverage for safety-critical QA)? Fusion-in-Decoder (FiD) and multi-passage conditioning.Instead of concatenating pas- sages into a single long encoder input, Fusion-in-Decoder (FiD) encodes each retrieved passage independently, then fuses all encoded evidence in the decoder cross-attention [9]. This architecture lets the generator attend to many pieces of evidence without forcing them to collide in a single flat token stream, which empirically boosts open-domain QA performance and robustness to noisy retrieval. In practice, FiD-like decoders are frequently used as strong RAG baselines because they relax the tension between recall (retrieve many passages) and tractable decoding. Reasoning-aware integration (retrieval↔ chain-of-thought).Classical RAG assumes a single retrieval step before answer generation. However, complex or multi-hop questions often require iterativeevidence gathering. Recent work proposes interleaving retrieval with step-by-step reasoning, 9 so that intermediate hypotheses guide what to