fine-tuning of the selected retrieval model further enhanced performance, as shown in Table 3. As a result, the fine-tuned version of “stella en 1.5B v5 (NovaSearch)”—referred to as FT stella 1.5B achieved an NDCG@10 score of 0.50864. This is a significant improvement over the baseline performance of the “stella en 400M v5 (NovaSearch)” model, which achieved an NDCG@10 score of 0.40186. These results confirm that the fine-tuning process, effectively im- proves retrieval precision and overall performance in the finance domain within the RAG pipeline. 5.3 D ATASET PREPROCESSING To further improve retrieval performance, we implemented tailored preprocessing strategies for both queries and corpus documents. For query preprocessing, we tested three approaches (Table 4). Among these methods, Query Expansion with LLM reached the highest NDCG@10 score. Table 4: NDCG@10 results of query preprocessing methods METHODS NDCG@10 Default (FT stella 400M) 0.40186 Keyword Extraction + LLMLingua 0.43613 Query Expansion with LLM 0.48601 For corpus preprocessing, we compared four methods and Corpus Markdown Restructuring demonstrated the best NDCG@10 score (Table 5). Table 5: NDCG@10 results of corpus preprocessing methods METHODS NDCG@10 Default (Query Expansion) 0.48601 Corpus Markdown Restructuring 0.48645 Corpus Table Augmentation 0.45411 Corpus Table Extraction 0.43604 The notable point is that only Corpus Markdown Restructuring showed the highest perfor- mance. This is attributable to the better readability of Markdown, which led to further contextual comprehension. On the other hand, focusing on tabular data—emphasizing it by extraction or text-aided augmentation—resulted in worse performance than before. Despite enriching contextual information in cells, Table Augmentation led to excessive noise within the corpus. Similarly, Table Extraction assumed significance only in tabular data, discarding all other text. This suggests that fine-tuned retrievers can sufficiently understand knowledge in tabular data without the need for modification. 7 Accepted at the ICLR 2025: Advances in Financial AI Workshop These results