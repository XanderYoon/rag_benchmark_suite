NDCG, a score between 0 and 1, where a higher value indicates a ranking that more closely approximates the ideal ordering. NDCG = DCG IDCG = 1 m mX u=1 X j∈Iu,vj ≤L guj log2 (vj + 1) (2) This metric is particularly useful because it simultaneously accounts for the relevance of each doc- ument and its position in the ranking. Improved NDCG@10 scores in our experiments indicate that our system retrieves a more complete and relevant set of documents for each query. 4.3 E XPERIMENTAL PROCEDURES Retrieval Model Selection Selecting an optimal retrieval model is crucial for maximizing perfor- mance in RAG. We evaluated six candidate models based on the Information Retrieval performance on the MTEB Leaderboard (Muennighoff et al., 2023): e5-large-v2 (intfloat), GritLM-7B, Fin- BERT (ProsusAI), TAPAS (Google), stella en 400M v5 (NovaSearch), and stella en 1.5B v5 (NovaSearch). Embedder Fine-tuning We fine-tuned our selected retrieval models to better align with fi- nancial texts and improve overall performance in our RAG pipeline. We focused on two models—“stella en 1.5B v5 (NovaSearch)” and “stella en 400M v5 (NovaSearch)” For fine-tuning, we prepared relevant query-document pairs, splitting the dataset with a ratio of 8 (train) : 2 (eval). Positive pairs were given a similarity score of 1.0, while negative pairs were scored 0.0, with negatives sampled randomly to ensure diversity. We utilized contrastive learning for fine-tuning, leveraging Multiple Negatives Ranking Loss (MNRLoss)(Henderson et al., 2017). More detailed information regarding the fine-tuned model and hyperparameter settings can be found in the Appendix 10, 11. Query data preprocessing Query data tends to be brief and lack sufficient contextual cues, which can hinder the retrieval model’s ability to fully interpret user intent. To address this, we experimented with three distinct preprocessing methods. First, Default (FT stella 400M) used raw queries without any modifications.