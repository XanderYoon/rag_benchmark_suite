(IR), where the semantic similarity between a query and a corpus is assessed and ranked. A prevalent strategy for enhancing this process is contrastive learning or contrastive fine-tuning—which relies on constructing triplets (query, relevant corpus, irrelevant cor- pus) to form positive and negative training pairs Karpukhin et al. (2020). Despite the effectiveness of contrastive learning in embedders Lu et al. (2024), there remains a notable gap in the literature regarding the impact of embedder fine-tuning on RAG systems, particularly within the finance do- main Setty et al. (2024). By addressing this gap, our work aims to explore and quantify the benefits of embedder fine-tuning in RAG applications, thereby contributing to the broader understanding of domain-adaptive IR. 2.2 P RE-RETRIEVAL Effective dataset preprocessing is essential for enhancing the performance of Retrieval-Augmented Generation (RAG) systems, because it directly impacts the clarity and semantic alignment of both queries and documents (Gao et al., 2024). Previous work has shown that short, context-poor queries can lead to significant ambiguity, which in turn hinders retrieval accuracy (Koo et al., 2024). To mitigate this issue, researchers have explored various query enhancement techniques—such as query expansion and rephrasing—to enrich the original input and better capture user intent (Patel, 2024). In line with these findings, we evaluate query preprocessing methods —from raw queries and keyword extraction with linguistic simplification to LLM-based query expansion—and show that adding contextual information significantly improves retrieval performance. In addition to query enhancement, the heterogeneity of corpus data presents unique challenges that necessitate tailored preprocessing strategies. Unlike approaches that apply a uniform treatment to all documents, recent studies have emphasized the importance of adapting preprocessing to the structural characteristics of the corpus—especially when dealing with diverse formats such as plain text and tabular data. Building on these insights, we explore various corpus preprocessing methods —preserving