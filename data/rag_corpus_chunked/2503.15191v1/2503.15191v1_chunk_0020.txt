Kurzweil. Efficient natural language response suggestion for smart reply, 2017. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. Llama guard: Llm- based input-output safeguard for human-ai conversations, 2023. Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vid- gen. Financebench: A new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: Compressing prompts for accelerated inference of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Lan- guage Processing, pp. 13358–13376, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.825. 11 Accepted at the ICLR 2025: Advances in Financial AI Workshop Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.),Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing (EMNLP) , pp. 6769–6781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. Tobias Kerner. Domain-specific pretraining of language models: A comparative study in the medical field. arXiv preprint arXiv:2407.14076, 2024. Seunghee Kim, Changhyeon Kim, and Taeuk Kim. Fcmr: Robust evaluation of financial cross- modal multi-hop reasoning. arXiv preprint arXiv:2412.12567, 2024. Hamin Koo, Minseon Kim, and Sung Ju Hwang. Optimizing query generation for enhanced docu- ment retrieval in rag. arXiv preprint arXiv:2407.12325, 2024. Joohyun Lee and Minji Roh. Multi-reranker: Maximizing performance of retrieval-augmented gen- eration in the financerag challenge, 2024. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the