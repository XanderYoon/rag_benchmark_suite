from complex and diverse data sources. Next, we utilized task-specific retrieval methods that leverages SOTA models to the unique language and structural features of financial information, thus improving retrieval perfor- mance. Finally, we implemented a reranking method, coupled with a novel method ( document selection), to guarantee that the final generated responses are grounded in the most accurate and relevant data. 2 R ELATED WORK 2.1 E MBEDDER FINE -TUNING Fine-tuning, the process of adapting a pre-trained model to domain-specific tasks using typically smaller datasets, has been widely explored across various applications. While embedding models exhibit strong zero-shot performance on general benchmarks such as MTEB (Muennighoff et al., 2023; Zhang et al., 2017), recent studies have demonstrated that even modestly sized models can benefit substantially from fine-tuning when applied to domain-specific tasks. For instance, fine- tuning embedders on specialized datasets has led to notable improvements in areas such as medical question answering Sengupta et al. (2024) and financial question answering Anderson et al. (2024). In the finance domain, prior research on embedders has underscored several inherent challenges: domain-specific vocabulary and semantic patterns, the complexity of multi-hop queries, and mul- timodal data (e.g. text, tables, and time-series) Tang & Yang (2024); Kim et al. (2024); Xie et al. (2024). These challenges necessitate tailored fine-tuning strategies that can effectively capture the nuanced information contained in financial documents. Within the framework of Retrieval-Augmented Generation (RAG), embedding models are primarily tasked with Information Retrieval (IR), where the semantic similarity between a query and a corpus is assessed and ranked. A prevalent strategy for enhancing this process is contrastive learning or contrastive fine-tuningâ€”which relies on constructing triplets (query, relevant corpus, irrelevant cor- pus) to form positive and negative training pairs Karpukhin et al. (2020). Despite the effectiveness of contrastive learning in embedders Lu et al.