by LLM FinanceBench(Islam et al., 2023) Real-world questions written by non-experts TATQA(Zhu et al., 2021) Basic arithmetic questions FinQA(Chen et al., 2021) Complex arithmetic questions ConvFinQA(Chen et al., 2022) Questions asking for specific value from table values MultiHiertt(Zhao et al., 2022) Questions requiring multi-hop reasoning. *To be announced comprehensive and real-world financial question answering scenarios. Also, each text is chunked at 4 Accepted at the ICLR 2025: Advances in Financial AI Workshop 512 tokens, following the results of Yepes et al. (2024). Experiment Settings Our experiments were conducted in a notebook-based development environment using Google Colab, with access to 40GB NVIDIA A100 GPUs. 4.2 E VALUATION METRIC : NDCG@10 To assess the ranking quality of the retrieved documents, we employed the Normalized Discounted Cumulative Gain(Wang et al., 2013) at 10 (NDCG@10) metric. NDCG is a widely used measure in information retrieval that evaluates how well the predicted ranking of documents aligns with an ideal ranking based on ground-truth relevance. In our experiments, the provided labels of all 7 benchmarks were used to measure the total weighted NDCG@10 score. In this metric, the DCG (Discounted Cumulative Gain) is computed by summing the relevance scores of the retrieved documents, with each score discounted by the logarithm of its rank position. The IDCG (Ideal Discounted Cumulative Gain) represents the maximum possible DCG achiev- able when the documents are ideally ranked in descending order of relevance. Normalizing the DCG by the IDCG yields the NDCG, a score between 0 and 1, where a higher value indicates a ranking that more closely approximates the ideal ordering. NDCG = DCG IDCG = 1 m mX u=1 X j∈Iu,vj ≤L guj log2 (vj + 1) (2) This metric is particularly useful because it simultaneously accounts for the relevance of each doc- ument and its position in the