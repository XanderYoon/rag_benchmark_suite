of model outputs. However, its ef- fectiveness depends heavily on the quality of the retrieval stage. A central factor in retrieval performance is chunking [2], which determines how documents are segmented for indexing. Effective chunking preserves seman- tic coherence, enabling the retrieval of contextually complete and relevant seg- ments and improving downstream tasks like question answering. Yet mainstream arXiv:2512.00367v1 [cs.IR] 29 Nov 2025 2 Authors Suppressed Due to Excessive Length RAG frameworks largely rely on fixed-length, sentence-based, or paragraph- based methods, which frequently yield incoherent or overly broad chunks that weaken retrieval precision. We propose two semantically aware, domain-trained chunking methods, Pro- jected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), which usedistinctboundary-detectionmechanismsandaretrainedonaugmentedbiomed- ical corpora. Our results show substantial improvements in both retrieval and generationcomparedtofixed-length,sentence-level,semantic,andrecursivebase- lines.Furthermore,thechunkersgeneralizewelltoout-of-domaindatasets,demon- strating robustness beyond the biomedical domain. All code, trained models, and augmentation scripts will be released upon acceptance. 2 Background Chunking is a critical preprocessing step in Retrieval-Augmented Generation (RAG),playingafundamentalroleinimprovingboththeefficiencyandrelevance of the retrieved context for downstream tasks. Existing approaches typically basedonfixed-sizesegmentsorrigidpredefinedrules,oftenfailtocaptureseman- tic boundaries effectively. While benchmarks exist for evaluating retrieval [15] and generation [5,14,17,18], along with established metrics for both tasks [1], most studies evaluate the retriever and generator modules independently. The influence of chunking on retrieval and generation performance, however, remains comparatively underexplored. Semantic chunking addresses boundary segmentation by producing coher- ent, contextually aligned text segments instead of arbitrary fixed-length chunks. Some existing library implementations [7] attempt to solve the limitations of uniform chunking by employing embedding-based strategies. In such methods, candidate chunks are identified by measuring the cosine similarity between adja- cent sentences (or merged segments) and splitting when the similarity falls below a threshold, indicating a topic shift. However, cosine similarity has known short- comings for downstream tasks such as information retrieval [23] and often lacks robust, domain-specific performance, as our