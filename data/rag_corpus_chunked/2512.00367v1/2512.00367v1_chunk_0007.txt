[2,4,8]. We adopt a 1000-token chunk size with 200-token overlap, chosen because two con- secutive chunks typically capture a complete semantic unit while avoiding unnec- essary redundancy.Sentence Chunking(Sent) groups a fixed number of full sentences, offering better linguistic consistency than character-level splitting. We use three sentences with a one-sentence overlap, roughly matching the 1000 and 200-token configuration, though this produces variable chunk lengths that com- plicate uniform processing.Recursive Chunking(Rec) segments documents hierarchically (e.g., paragraphs to sentences). While more structured, it requires additional computation to identify segmentation levels. We use the same 1000- token and 200-token setups for comparability.Semantic Chunking(Sem) uses sentence embeddings to detect semantic shifts and form meaningfully aligned chunks. Although conceptually strong, LangChain’s default SemanticChunker under-performs in our evaluations relative to the proposed methods. 2 https://www.langchain.com 6 Authors Suppressed Due to Excessive Length 4.4 Experimental setup We trained our two chunker architectures, PSC and MFC, on three selected sen- tence embedding models, MiniLM, mpnet, and E5, using the augmented PQA-A dataset. The augmented PQA-A dataset contains approximately 93M training samples. All model training was performed on a single NVIDIA RTX 6000 GPU with 48 GB of VRAM. Each embedding–chunker combination was trained for 5 epochs, and empirical evaluation indicated that models trained up to epoch 4 achieved the best overall performance. We used the BCEWithLogitsLoss ob- jective function, which combines a sigmoid activation with binary cross-entropy loss. Training time for each model combination was under 12 hours, resulting in a total cumulative GPU usage of roughly 72 hours across all experiments. For generation and inference, we used a pair of NVIDIA RTX 4090 GPUs (24 GB VRAM each). The PQA-L dataset, comprising 115 documents, was used for in-domain evaluation, requiring about 1 hour per model. Out-of-domain evalua- tion on RagBench averaged 14 hours per model. This setup enabled