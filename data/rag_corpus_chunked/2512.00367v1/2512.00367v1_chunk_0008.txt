in a total cumulative GPU usage of roughly 72 hours across all experiments. For generation and inference, we used a pair of NVIDIA RTX 4090 GPUs (24 GB VRAM each). The PQA-L dataset, comprising 115 documents, was used for in-domain evaluation, requiring about 1 hour per model. Out-of-domain evalua- tion on RagBench averaged 14 hours per model. This setup enabled consistent, large-scale experimentation while maintaining practical computational require- ments. 4.5 Evaluation Setup We evaluate the impact of chunking strategies on both retrieval quality and answer generation. Using the augmented PQA-L dataset, gold-standard chunks and answers enable direct comparison against retrieved contexts and generated outputs. Alongside our PSC and MFC chunkers, we benchmark four baselines: Character, Sentence, Semantic, and Recursive Chunkers. For out-of-domain testing, we use 12 RAGBench [3] datasets to measure generation quality across chunkers. Since RAGBench lacks gold context labels, we evaluate only generated answers, which though LLM-produced, are reliable for benchmarking. This setup allows us to test how well domain-trained chunkers generalize to varied document types. Retriever evaluation is conducted solely on the PubMedQA extension using MRR and Hits@k. PSC and MFC produce naturally variable chunk lengths (averaging 471 tokens), which we find improves retrieval and downstream answer generation compared to fixed-size segmentation. For generation, we retrieve the top-5 chunks and pass them, along with the query, to a Llama-3.1-8B [16] instruction-tuned model with fixed decoding pa- rameters across all datasets. Outputs are evaluated against gold answers using ROUGE [9], BLEU [10], and BERTScore [21]. Keeping the retriever and gener- ator constant ensures all performance differences arise solely from the chunking strategy. Each dataset, chunker, and encoder configuration follows a unified workflow: thedocumentissegmented,itsembeddingsareindexed,themostrelevantchunks are retrieved for a given query, and a generator model produces the final answer. Breaking It Down 7 Model Chunker Hits@3 Hits@5 MRR Query Time