ROUGE [9], BLEU [10], and BERTScore [21]. Keeping the retriever and gener- ator constant ensures all performance differences arise solely from the chunking strategy. Each dataset, chunker, and encoder configuration follows a unified workflow: thedocumentissegmented,itsembeddingsareindexed,themostrelevantchunks are retrieved for a given query, and a generator model produces the final answer. Breaking It Down 7 Model Chunker Hits@3 Hits@5 MRR Query Time TTFT MiniLM Char0.0 0.0 0.0 0.01 5.91 Sent0.0 0.0 0.0 0.01 5.91 Sem0.00 * 0.00* 0.01 0.01 0.56 Rec0.0 0.0 0.0 0.01 0.19 PSC0.10 0.13 0.30 0.00 0.12 MFC0.09 0.12 0.26 0.01 0.13 mpnet Char0.0 0.0 0.0 0.02 5.86 Sent0.0 0.0 0.0 0.01 0.34 Sem0.00 * 0.00* 0.01 0.01 0.62 Rec0.0 0.0 0.0 0.01 0.19 PSC0.11 0.15 0.31 0.010.13 MFC0.10 0.14 0.29 0.01 0.15 e5 Char0.0 0.0 0.0 0.02 5.58 Sent0.0 0.0 0.0 0.01 0.34 Sem0.00 * 0.00* 0.01 0.01 0.63 Rec0.0 0.0 0.0 0.01 0.19 PSC0.13 0.16 0.36 0.010.14 MFC0.12 0.15 0.34 0.01 0.20 Table 1: Retrieval performance measured with Hits@3, Hits@5 and MRR across the three embedding models, show significant improvement, on using our chunkers.* indi- cates infinitesimal non-zero values. The p-test values between PSC and Sem across the three embeddings and metrics were in the order ofeâˆ’11 and t-test values greater than 7. Query Time and Time to First Token (TTFT) indicate that PSC is the fastest. This integrated design removes redundant pipelines, enforces consistent evalua- tion across setups, and offers a scalable framework for systematically comparing RAG components. 5 Analysis Our trained chunkers significantly outperform commonly used methods, MRR improves from 0.0086 (recursive) to 0.2914 (PSC in e5) and Hits@5 from 0.0028 to 0.1234 (PSC in e5), as shown in Table 1. The library implementation of semantic chunker under performs in retrieval (MRR: 0.0130 vs. 0.2914 for PSC) due to its reliance on cosine similarity for