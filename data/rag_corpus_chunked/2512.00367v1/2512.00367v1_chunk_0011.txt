chunking with miniLM, MFC with mpnet and E5 perform the best. The p- test values between PSC and Char across the three embeddings and five metrics were less than0.3and t-test values around than1. Evaluation reveals a mismatch between conventional metrics and actual gen- erationquality.StandardmetricslikeBLEU,ROUGE,andBERTScoreprioritize lexical overlap over semantic nuance and factual accuracy. Consequently, they fail to capture the superiority of PSC and MFC chunkers, which produce con- cise, context-focused responses free from the verbosity and drift found in other methods. A two-tailed independent samples t-test was conducted to evaluate the sta- tistical significance of the performance metrics between all the chunkers. The difference between the reported means of MRR, Hits@3, Hits@5, BLEU and ROUGE was found to be statistically significant, as indicated by a p-value and t-value in the captions of Tables 1 and 2. This suggests that the improvement of the PSC Chunker over the next best chunker is not due to random chance with a confidence of 99.9% for retrieval, and 90% for generation. For out-of-domain tests, the generation results with BLEU and ROUGE- L scores on every dataset from RAGBench are summarized in Tables 3.1, 3.2 and 3.3. Despite our chunkers being trained only on PubMed dataset, we can see Breaking It Down 9 a clear improvement in the performance across all datasets with our chunkers, especially PSC. Embed Chunker CovidQA CUAD DelucionQA EManual ExpertQA FinQA HAGRID HotpotQA MS Marco PubMedQA T A T-QA T echQA BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L MiniLM Char 1.95 3.16 9.86 9.82 2.91 4.9 3.98 6.22 16.34 12.49 2.13 3.44 2.63 4.17 1.03 1.94 1.63 2.53 8.49 10.21 3.07 5.01 13.48 12.05Sent 2.51 4.22 5.83 6.61 5.73 9.28 5.52 8.65 9.29 8.73 4.11