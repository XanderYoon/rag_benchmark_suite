employ an equally weighted combination of Dot Product Similarity, Euclidean Distance, and Manhattan Dis- tance which is passed through a single neural layer to obtain the final similarity score. The scalar similarity scoreSi,j is normalized into a probability range(0,1) using sigmoid. This value represents the modelâ€™s final prediction. If the output probability is greater than or equal to 0.5 (determined emperically), the individ- ual model predicts that the two sentences occur in the same section. Otherwise, if the probability is below 0.5, it predicts that the sentences should not occur one consecutively and we use that to draw our segment boundary or create the chunks. 4 Authors Suppressed Due to Excessive Length 4 Experiments This section presents experiments conducted using the proposed model on our augmented PubMedQA dataset and the RAGBench [3] datasets. It is organized into five subsections: dataset, evaluation metrics, baselines, experimental setup, and evaluation setup. 4.1 Dataset Currently, no publicly available datasets exist for jointly evaluating retrieval and generation performance. To address this gap, we repurposed the PubMedQA (PQA) dataset [6] by augmenting it with corresponding full-text articles from PubMed Central (PMC). The full texts were downloaded in XML format fol- lowing the official instructions1 . PubMedQA is a carefully curated biomedical question-answering dataset de- rived from PubMed articles whose titles are phrased as questions. The dataset is divided into labeled (PQA-L), unlabeled (PQA-U), and artificial (PQA-A) subsets. PQA-L and PQA-U are human-annotated, whereas PQA-A is auto- matically labeled. In the original dataset, the title serves as the question, the context consists of selected sentences from the abstract that support the conclu- sion, and the conclusion itself serves as the long-form answer. Upon downloading the corresponding full-text XML files, we observed that not all PQA entries have available articles on PMC. Specifically, from the original 1k samples