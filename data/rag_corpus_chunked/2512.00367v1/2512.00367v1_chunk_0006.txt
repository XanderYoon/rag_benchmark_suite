differ- ent sections within the same document that never co-occur in any section. This design ensures that the model learns to identify contextually related sentences based on human-defined structure. The final dataset contained 93M sentence pairs for training our models. 1 https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/ Breaking It Down 5 Theoriginaldatasetcolumnsincluded:pubid,question,context,long_answer, andfinal_decision. In our augmented version, the schema is updated to include full_text. We do not usefinal_decision, focusing exclusively on long-form an- swering. By introducing full-text articles alongside the original question answer pairs, our augmented PQA dataset enables a unified framework for evaluating both retrieval quality and answer generation in a controlled setting. The curated con- texts allow us to measure retriever precision, while the long-form conclusions serve as a robust reference for generation metrics, making it uniquely suited for end-to-end RAG evaluation. 4.2 Evaluation metrics Weevaluatethesystemattwolevels:retrievalqualityandanswergeneration.For the retriever, we employ Hits@k and Mean Reciprocal Rate (MRR) to measure theeffectivenessandrankingefficiencyoftheretrievedchunks.Forthegenerator, we evaluate the outputs using standard generation metrics such as BLEU [10], ROUGE [9], and BERTScore [21]. Additionally, we measure query response time and time-to-first-token to assess the responsiveness of our chunker models. 4.3 Baselines In our experiments, we compare several popular chunking strategies against our proposedmethodstoestablishabaselineforperformance.Allchunkingstrategies areimplementedusingLangChain 2 library.Thesentencesandchunksderivedare then encoded using one of three sentence transformer [12] models: all-MiniLM- L6-v2 [20], all-mpnet-base-v2 [13], and e5-large-v2 [19]. The performance of each chunker is evaluated based on its impact on retrieval and generation tasks. Character Chunking(Fixed-Length)(Char)splitstextintofixed-sizeblocks, often ignoring natural linguistic boundaries and reducing coherence [2,4,8]. We adopt a 1000-token chunk size with 200-token overlap, chosen because two con- secutive chunks typically capture a complete semantic unit while avoiding unnec- essary redundancy.Sentence Chunking(Sent) groups a fixed number of full sentences, offering better linguistic consistency than character-level splitting. We use three sentences with a one-sentence overlap, roughly matching the 1000 and 200-token configuration, though this produces