Com- putational Linguistics: ACL-IJCNLP 2021. pp. 2594–2603. Association for Com- putational Linguistics, Online (Aug 2021).https://doi.org/10.18653/v1/2021. findings-acl.229,https://aclanthology.org/2021.findings-acl.229/ 15. Thakur, N., Reimers, N., Rücklé, A., Srivastava, A., Gurevych, I.: BEIR: A het- erogeneous benchmark for zero-shot evaluation of information retrieval models. In: Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021),https://openreview.net/forum?id= wCu6T5xFjeJ 16. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023),https:// doi.org/10.48550/arXiv.2302.13971 17. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.: Superglue: A stickier benchmark for general-purpose language under- standing systems. Advances in neural information processing systems32(2019) 18. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: GLUE: A multi-task benchmark and analysis platform for natural language understand- ing. In: International Conference on Learning Representations (2019),https: //openreview.net/forum?id=rJ4km2R5t7 19. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., Wei, F.: Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022) 20. Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., Zhou, M.: Minilm: Deep self- attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems33, 5776–5788 (2020) 21. Zhang*, T., Kishore*, V., Wu*, F., Weinberger, K.Q., Artzi, Y.: Bertscore: Eval- uating text generation with bert. In: International Conference on Learning Repre- sentations (2020),https://openreview.net/forum?id=SkeHuCVFDr 22. Zhong, Z., Liu, H., Cui, X., Zhang, X., Qin, Z.: Mix-of-granularity: Optimize the chunking granularity for retrieval-augmented generation. In: Rambow, O., Wanner, L.,Apidianaki,M.,Al-Khalifa,H.,Eugenio,B.D.,Schockaert,S.(eds.)Proceedings of the 31st International Conference on Computational Linguistics. pp. 5756–5774. Association for Computational Linguistics, Abu Dhabi, UAE (Jan 2025),https: //aclanthology.org/2025.coling-main.384/ 23. Zhou, K., Ethayarajh, K., Card, D., Jurafsky, D.: Problems with cosine as a measure