(24.68 to 26.12 for BLEU-1 and 8.07 to 8.23 for GLEU), and a comparable leap over another SOTA improvement (27.93 to 29.07 for ROUGE-1 and 0.93 to 0.96 for Distinct-2). Compared to ChatGLM variants, our model shows a significant advantage; however, DoctorGLM performs poorly on BLEU and GLEU metrics. This could be due to the modelâ€™s potential difficulty in handling the subtleties of context within medical texts. Medical language is typically laden with jargon and can be quite nuanced. If DoctorGLM is not fine-tuned on medical-specific data, it may fail to capture the complexity required for higher BLEU and GLEU scores. Nonetheless, it performs well on the Distinct metrics. This is because Doctor- GLM might employ a generation strategy that favors diversity over precision. Such an approach might manifest in a model that is less likely to repeat the same phrases and more inclined to explore the breadth of its training data when generating responses. In summary, our model outperforms nearly all evaluated metrics when compared to others. (a) GPT-4 Evaluation (b) Doctor Evaluation Fig. 4. 1v1 battle between different models and ours In order to make a more direct comparison of different models, we used GPT-4 and a professional doctor to evaluate our model and other models one- on-one. It can be seen that our model has a significant advantage over the others; doctorGLM does not perform well on some of the quantitative metrics, but when viewed by a professional doctor, it has a very good performance, probably because the content it generates is more in line with the human mind. Overall, our model won almost all 1v1 battles. 12 J. Tang et al. T able 3. Test Result on Chinese medical QA dataset Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 GLEU T5 (fine-tuned) 21.27 13.65 9.89 7.21 9.07 DoctorGLM 9.89