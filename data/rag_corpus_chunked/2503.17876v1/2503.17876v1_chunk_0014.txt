sentence-level structure similarity between the generated text and reference texts. In employing these advanced metrics, we aim to thoroughly assess the model’s ability to generate coherent, contextually relevant, and linguistically accurate content. By focusing on both the micro-structure of language (as captured by BLEU-5) and the effective conveyance of meaning and structure (as captured by ROUGE-2/L), we provide a holistic evaluation of the model’s performance in critical tasks of language processing and generation. Baselines. For comprehensive evaluation and fair comparison, we introduce five baseline models that are prominent in the field of language generation and attribute-based modeling. DoctorGLM [27], a specialized model for medical inquiries; ChatGPT-3.5 [28], an iteration of the GPT series with 3.5 billion parameters; ChatGLM-6B [29], a 6 billion parameter model designed for con- versational tasks; Ziya-LLaMA-13B [30], optimized for linguistic tasks with 13 billion parameters; and HuatuoGPT [31], our proprietary model before fine- tuning. Additionally, we showcase our model T5, which is HuatuoGPT after extensive fine-tuning to enhance its performance on a wide array of tasks. Experimental settings. Our experimental setup is built on a robust hardware environment to facilitate efficient and reliable model training and evaluation. Specifically, we utilize NVIDIA-V100-32GB GPUs, known for their powerful computation capabilities, particularly in handling large-scale machine learning tasks. We initialize our models with the pre-trained T5 model, available in the HuggingFace Transformers library. We consider two model sizes, base and large, containing respectively 220M and 770M parameters. Due to GPU memory lim- itations, we use different batch sizes for different models: 8 for T5-large and 16 for T5-base. 10 J. Tang et al. T able 2. AMT study results for medical consultant Model Patient satisfaction DoctorGLM 5.67% (34/600) ChatGPT-3.5 7.17% (43/600) ChatGLM-6B 2.83% (17/600) Ziya-LLaMA-13B 2.17% (13/600) HuatuoGPT 12.50% (75/600) Ours(w/o TEIR) 17.50% (105/600) Ours(w/o EICL) 15.33% (92/600) Ours