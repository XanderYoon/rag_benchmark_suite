into discrete data usable for training while keeping the LM (Language Model) parameters unchanged. Mathematically speaking, this is the search for the opti- mal parameters ϕ that maximize the log-likelihood function given a training set D and fixed model parameters θ. max ϕ P (y|x; θ; ϕ) = max ϕ X yi P (yi|h>i; θ; ϕ) (2) In equation 2, h>i represents the set of all hidden states after thei-th position. If simplified, it can be considered as a state of the LM. h>i is obtained through the model Mϕ, which represents the state of the LM given the model parameters ϕ. It can also be understood by the following example. Train: Patient: I have a fever today; Doctor: Drink more hot water. We predict patient feedback: Negative. Test: I have a fever today; This dialogue serves as a training instance for the model to understand and predict emotional responses based on conversational context. Here, the simplic- ity of the doctor’s advice in response to a possibly serious symptom might not address the patient’s concerns adequately, leading to negative feedback. Emo- tional In-Context Learning aims to use such instances to teach the model the 8 J. Tang et al. T able 1. Case study of regenerated method Input: I have a fever today; Answer: Drink more hot water. Sentiment: Negative. Input: I have a fever today. Please do not say, ”Drink more hot water.”; Answer: Drink adequate amounts of fluids. Water, juices, clear soups, or hot lemonade are all good choices. Avoid caffeine and alcohol; these ingredients increase fluid loss. Sentiment: Positive. subtleties of emotional undercurrents in conversations, helping it predict and, in real-world applications, provide responses that are not only factually correct but also emotionally attuned to the patient’s needs. More specifically, here we construct a regenerated method using