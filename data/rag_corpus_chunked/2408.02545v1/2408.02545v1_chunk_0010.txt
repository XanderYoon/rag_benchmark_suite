a list of metrics to run. 4 Experiments: RAG Tuning To illustrate the usage and usefulness of the RAG F OUNDRY library, we experiment with sev- eral possible RAG improvements to LLMs, and evaluate the results on three knowledge-intensive tasks. 4.1 RAG Augmentation Techniques We explore several techniques for RAG augmenta- tion, and use RAG F OUNDRY to easily implement and evaluate their benefit. As an initial step, we evaluate unmodified models; we set Baseline as a configuration that is defined by running unmodified models and without any external knowledge. We define a RAG setting that introduces top-relevant documents in a consistent prompt template format with a system instruction, and aCoT scheme which guides the model to use the retrieved context, ex- plain the steps, quote relevant parts and produce a final answer. Complementing that, we explore fine-tuning recipes. We fine-tune the model in the RAG setup and denote is as RAG-sft. To comple- ment CoT, we implemented a fine-tuning recipe, denoted as CoT-sft, introduced in (Zhang et al., 2024), where gold documents and purely distractor documents are used in the prompt, determined by probability, in conjunction with a CoT prompt. All prompt templates are included in appendix A.1. 4.2 Datasets We evaluate our models on TriviaQA (Joshi et al., 2017), PubmedQA (Jin et al., 2019), and ASQA (Stelmakh et al., 2022) which are knowledge in- tensive question-answering datasets which ben- efit from external sources. The TriviaQA and PubmedQA datasets contain relevant context; for ASQA, retrieval was done over a Wikipedia corpus using a dense retriever4. Dataset sources and sizes are included in appendix A.2. 4.3 Models We experiment with two representative models: Llama-35 (Touvron et al., 2023; AI@Meta, 2024) and Phi-36 (Abdin et al., 2024) as they represent robust capabilities and are ideal candidate models for RAG use case