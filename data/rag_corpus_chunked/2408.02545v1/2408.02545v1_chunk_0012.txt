0.574 0.439 0.477 0.705 CoT-sft 0.795 0.793 0.749 0.386 0.749 0.839 0.620 0.458 0.631 0.853 Llama-3 8B Baseline 0.722 - - 0.200 - - 0.560 0.366 - - RAG 0.828 0.783 0.746 0.285 0.610 0.861 0.556 0.398 - - RAG-sft 0.916 0.704 0.714 0.291 0.653 0.854 0.770 0.537 - - CoT 0.896 0.518 0.764 0.395 0.536 0.730 0.684 0.480 0.378 0.732 CoT-sft 0.851 0.808 0.697 0.422 0.768 0.790 0.694 0.485 0.777 0.883 Table 1: Evaluation results of baseline and different RAG settings, for the three datasets and two models tested. In addition to the main metrics for each dataset, faithfulness and relevancy are reported for the relevant configurations. In bold are the best configurations per dataset, based on the main metrics. main metrics for each dataset are displayed, as well as faithfulness and relevancy scores, as defined in (Es et al., 2024). For TriviaQA we observe the following: retrieved context improves the results, fine-tuning the RAG setting improves the results, fine-tuning on CoT reasoning (which includes train- ing on a combination of gold passages and distrac- tor passages) decreases performance. Best method is model dependent for this dataset. For ASQA, we similarly observe every method improves upon the baseline, CoT reasoning produces consistent improvement in both models, as well as fine-tuning of the CoT configuration, which shows to perform best. Finally, for PubmedQA, we observe that al- most all methods improve upon the baseline (with one exception); CoT reasoning improves upon the untrained RAG setting, but upon fine-tuning, the RAG method appears to perform best in both mod- els. Inspecting the faithfulness and relevancy scores, notice that not all configurations are valid to be measured: these metrics require context, so are irrelevant for the baseline method. Additionally, in the PubmedQA dataset, the answers are binary Yes/No; only in the CoT