contain relevant context; for ASQA, retrieval was done over a Wikipedia corpus using a dense retriever4. Dataset sources and sizes are included in appendix A.2. 4.3 Models We experiment with two representative models: Llama-35 (Touvron et al., 2023; AI@Meta, 2024) and Phi-36 (Abdin et al., 2024) as they represent robust capabilities and are ideal candidate models for RAG use case deployments. 4.4 Evaluation We measure and report Exact Match (EM) for TriviaQA, STR-EM for ASQA, accuracy and F1 for PubmedQA. Additionally, we evaluate two RAGAS metrics (Es et al., 2024): Faithfulness and Relevancy. Faithfulness measures the relation be- tween the generated text and the context. Relevancy measures the relation between the generated text and the query. These two metrics use the context as input for the LLM critic, so are only relevant in the RAG settings. The critic LLM used is GPT4-32k, version 0613. An embedder 7 is required for the relevancy evaluation. 4.5 Results We present a comparative study of RAG augmenta- tion techniques, on the TriviaQA, ASQA and Pub- medQA datasets. Results are presented in table 1: 4BAAI/llm-embedder 5meta-llama/Meta-Llama-3-8B-Instruct. 6microsoft/Phi-3-mini-128k-instruct. 7BAAI/bge-small-en-v1.5. Model Method TriviaQA ASQA PubmedQA EM Faith. Rel. STR-EM Faith. Rel. Acc F1 Faith. Rel. Phi-3 3.8B Baseline 0.630 - - 0.109 - - 0.476 0.290 - - RAG 0.876 0.821 0.836 0.294 0.685 0.895 0.530 0.281 - - RAG-sft 0.878 0.777 0.750 0.252 0.717 0.833 0.720 0.491 - - CoT 0.923 0.555 0.741 0.367 0.263 0.826 0.574 0.439 0.477 0.705 CoT-sft 0.795 0.793 0.749 0.386 0.749 0.839 0.620 0.458 0.631 0.853 Llama-3 8B Baseline 0.722 - - 0.200 - - 0.560 0.366 - - RAG 0.828 0.783 0.746 0.285 0.610 0.861 0.556 0.398 - - RAG-sft 0.916 0.704 0.714 0.291 0.653 0.854 0.770 0.537 - - CoT 0.896 0.518 0.764 0.395 0.536 0.730 0.684 0.480 0.378