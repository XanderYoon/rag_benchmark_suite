we designed the framework to func- tion as an end-to-end experimentation environment. The backbone of the library consists of four dis- tinct modules: data creation, training, inference, and evaluation. Each module is encapsulated and controlled by a configuration file, ensuring compat- ibility between the output of one module and the input of the next. This modular approach allows each step to be isolated and independently experi- mented with, enabling the production of multiple outputs and the concurrent execution of numerous experiments. Evaluation can be conducted on the generated outputs as well as on any feature within the data, including retrieval, ranking, and reason- ing. To illustrate the utility of the framework, we conducted experiments involving retrieval, fine- tuning, chain-of-thought (CoT) reasoning (Wu et al., 2023) and a negative distractor-documents technique (Zhang et al., 2024). We compared two widely accepted baseline models using vari- ous enhancement methods across three knowledge- intensive question-answering tasks, demonstrating the effectiveness of RAG F OUNDRY . 2 Related Work There are numerous open-source tools related to the different aspects of RAG, namely inference, training and evaluation. LlamaIndex (Liu, 2022), LangChain (Chase, 2022) and Haystack (Pietsch et al., 2019) are well known libraries for composing RAG pipelines; however they are not focused on evaluation and their training capability is under- developed. Hoshi et al. (2023) proposes a framework for developing RAG-based LLMs; while our process- ing may be similar in the sense of being comprised of custom individual steps, they do not introduce any form of training. Khattab et al. (2023, 2022) presents a different approach, where LLM prompt- ing is represented as a programming language, to be optimized and compiled; a rather unique and general approach that could benefit RAG but has a high level of complexity due to the abstractions introduced. Saad-Falcon et al. (2024)