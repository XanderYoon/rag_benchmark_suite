RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation Daniel Fleischer Moshe Berchansky Moshe Wasserblat Peter Izsak Intel Labs {daniel.fleischer, moshe.berchansky, moshe.wasserblat, peter.izsak}@intel.com Abstract Implementing Retrieval-Augmented Genera- tion (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Addi- tionally, evaluating these systems presents sig- nificant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We intro- duce RAG F OUNDRY , an open-source frame- work for augmenting large language models for RAG use cases. RAG F OUNDRY inte- grates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language mod- els in RAG settings. This integration en- ables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowl- edge sources. We demonstrate the frame- work effectiveness by augmenting and fine- tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent im- provements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry. 1 Introduction Large Language Models (LLMs) have emerged as a transformative force in the field of AI, demon- strating an impressive ability to perform a wide range of tasks that traditionally required human in- telligence (Brown et al., 2020; Kojima et al., 2022). Despite their impressive capabilities, LLMs have inherent limitations. These models can produce plausible-sounding but incorrect or nonsensical an- swers, struggle with factual accuracy, lack access to up-to-date information after their training cutoff and struggle in attending to relevant information in large contexts (Huang et al., 2023; Liu et al., 2023). Data Training LoRA  Inference  Loaders Augmentation Selectors Retrievers Samplers Prompters Caching API Evaluation EM  F1 Faithfulness Relevancy Answer Processor ROUGE Figure