upon the untrained RAG setting, but upon fine-tuning, the RAG method appears to perform best in both mod- els. Inspecting the faithfulness and relevancy scores, notice that not all configurations are valid to be measured: these metrics require context, so are irrelevant for the baseline method. Additionally, in the PubmedQA dataset, the answers are binary Yes/No; only in the CoT configurations the LLMs produce a reasoning, which can be evaluated. Fi- nally, the faithfulness and relevancy scores often do not correlate with the main metrics, neither with each other, possibly indicating they capture differ- ent aspects of the retrieval and generated results, and represent a trade-off in performance. The results demonstrate the usefulness of RAG techniques for improving performance, as well as the need to carefully evaluate different aspects of a RAG system, on a diverse set of datasets, as effort on developing generalized techniques is ongoing. 5 Conclusion We introduced RAG F OUNDRY , an open-source library dedicated to the task of RAG-augmentation of LLMs, namely fine-tuning LLMs to become bet- ter at RAG settings. The library is designed to serve as an end-to-end experimentation environment, en- abling users to quickly prototype and experiment with different RAG techniques. We demonstrated the usefulness of the library by augmenting two models with RAG configurations, evaluating on three Q&A datasets and showing the benefit of RAG techniques, as well as of using multi-aspect metrics relevant for RAG systems evaluation. Limitations and Future Plans Our hope is that the library will be useful to as many people and use-cases as possible. However, due to time and resource constraint, we were able to demonstrate its usefulness on a subset of tasks and datasets. Future work can expand the evaluation to other tasks, as well as implementing other RAG techniques and evaluations. Although we designed