Preprint, arXiv:2302.13971. Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, and Xuanjing Huang. 2024. Searching for Best Practices in Retrieval-Augmented Generation. arXiv preprint. Dingjun Wu, Jing Zhang, and Xinmei Huang. 2023. Chain of thought prompting elicits knowledge aug- mentation. In Findings of the Association for Com- putational Linguistics: ACL 2023 , pages 6519â€“6534, Toronto, Canada. Association for Computational Lin- guistics. Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024a. Evaluation of Retrieval- Augmented Generation: A Survey. arXiv preprint. ArXiv:2405.07437 [cs]. Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catan- zaro. 2024b. RankRAG: Unifying Context Rank- ing with Retrieval-Augmented Generation in LLMs. arXiv preprint. ArXiv:2407.02485 [cs]. Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gon- zalez. 2024. Raft: Adapting language model to do- main specific rag. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019. BERTScore: Evaluating Text Generation with BERT. ArXiv. A Implementation Details A.1 Prompts You are a helpful question answerer who can provide an answer given a question and relevant context. Listing 5: System instruction used in the experiments. Question: {query} Context: {docs} Listing 6: Template for inserting relevant documents as context. Question: {query} Context: {docs} Answer this question using the information given in the context above. Here is things to pay attention to: - First provide step-by-step reasoning on how to answer the question. - In the reasoning, if you need to copy paste some sentences from the context, include them in ##begin_quote## and ##end_quote##. This would mean that things outside of ##begin_quote## and ##end_quote## are not directly copy paste from the context. -