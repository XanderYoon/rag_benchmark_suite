given the datasets created by the previous process- ing module. The training module relies on the well established training framework TRL2 and sup- 2https://github.com/huggingface/trl model: _target_: ragfoundry.models.hf.HFInference model_name_or_path: "microsoft/Phi-3-mini-128k-instruct",â†’ load_in_8bit: true instruction: prompts/prompt_instructions/qa.txt lora_path: /path/to/adapter generation: do_sample: false max_new_tokens: 50 return_full_text: false data_file: my-processed-data.jsnol generated_file: model-predictions.jsonl Listing 3: Example of an inference configuration. In ad- dition to model and generation options, a system prompt can be defined. ports advanced and efficient training techniques, e.g. LoRA (Hu et al., 2021). An example of a training configuration is presented in listing 2. 3.3 Inference The inference module generates predictions given the processed datasets created by the processing module. Inference is conceptually separated from the evaluation step, since it is more computation- ally demanding than evaluation. Additionally, one can run multiple evaluations on a single, prepared inference results file. An example configuration for generating predictions given a dataset is presented in listing 3. 3.4 Evaluation The goal of the framework is augmenting LLMs for RAG. The evaluation module allows users to run collections of metrics to evaluate RAG tech- niques and tuning processes. The evaluation mod- ule loads the output of the inference module and runs a configurable list of metrics. Metrics are classes implemented in the library. These classes can be as simple as wrappers around other evalua- tion libraries, or can be implemented by the user. Local metrics can be run on individual examples, like Exact Match (EM), while Global metrics run on the entire dataset as a whole, e.g. Recall (for classification-based metrics). Metrics can use any field and metadata in the dataset, not just the input- output pairs. Some of the metrics implemented in the library include: a wrapper for the Hugging Face evaluate library, EM, F1, classification met- rics, BERTScore (Zhang et al., 2019), Semantic Similarity and a