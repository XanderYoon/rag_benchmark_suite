incorrect or nonsensical an- swers, struggle with factual accuracy, lack access to up-to-date information after their training cutoff and struggle in attending to relevant information in large contexts (Huang et al., 2023; Liu et al., 2023). Data Training LoRA  Inference  Loaders Augmentation Selectors Retrievers Samplers Prompters Caching API Evaluation EM  F1 Faithfulness Relevancy Answer Processor ROUGE Figure 1: An overview of the RAG F OUNDRY frame- work: the Data Augmentation module persists RAG interactions into a dedicated dataset, which is then used for training, inference and evaluation. Retrieval-Augmented Generation (RAG) enhances LLMs performance by integrating external infor- mation using retrieval mechanisms. Combining re- trieval that leverages vast knowledge-bases outside the knowledge of the model, effectively addresses knowledge limitations, can reduce hallucinations, improve the relevance of generated content, pro- vide interpretability and could be vastly more cost- efficient (Lewis et al., 2021; Mallen et al., 2022; Gao et al., 2023; Asai et al., 2023; Borgeaud et al., 2021; Peng et al., 2023; de Jong et al., 2023). Fur- thermore, recent research indicates that fine-tuning LLMs for RAG can achieve state-of-the-art perfor- mance, surpassing that of larger, proprietary mod- els (Yu et al., 2024b; Liu et al., 2024). However, the implementation of RAG systems is inherently complex and requires a series of intricate decisions that can significantly impact the performance of the system. This process de- arXiv:2408.02545v1 [cs.CL] 5 Aug 2024 mands a thorough understanding of the data and use case, and often, solutions do not generalize well to other domains (Barnett et al., 2024; Bala- guer et al., 2024). Some key RAG design decisions include text embedding, indexing parameters, re- trieval algorithms, query building, and prompt de- sign, among other considerations beyond the LLM configuration (Wang et al., 2024). Another issue is reproducibility: achieving consistent and compara- ble