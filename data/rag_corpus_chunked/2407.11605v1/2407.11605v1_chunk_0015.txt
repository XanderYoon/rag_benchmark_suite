simulation can be beneficial to genera- tive IR models in two ways: (i) they provide a means for evaluating generated content, and (ii) they can be used for training. Zhang and Balog [90] propose a user simulator for conversational recommendation to evaluate the system per- formance. This is followed by the work done by Sekulic et al. [59] and Owoicho et al. [48] in using GPT-based models to simulate users in a mixed-initiative information-seeking conversational system where the main goal of the simulator is to provide an answer to a generated clarifying question. They show that such simulators can lead to reliable evaluation of conversational systems. There are various considerations to take into account in simulating and eval- uating interactive generative systems: – User effort: In interacting with the system, users bear different levels of cognitive load, which can lead to user fatigue as the number of interactions increases. – User information gain: To model the true value of a clarifying question in a conversation, we need to model both the gain and effort a clarifying question brings to the conversation [2, 10]. – Information nuggets: Information gain can be modeled by breaking the user’s information need into information nuggets and measuring how much asking a certain clarifying question would help us provide further information nuggets to the user. – User model: As proposed by Balog [11], an effective user simulator should have various components, including a user mental model. Realistically, a single user simulator does not cover the needs and behavior of the wide range of users interacting with the system. 6 Proactive Interactions Typically, users initiate the interaction with a generative retrieval system, for example by submitting a chit-chat utterance, asking a question, or submitting an action request. In mixed-initiative conversational systems, the agent is also able to