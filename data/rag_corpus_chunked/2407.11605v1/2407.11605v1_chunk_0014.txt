Planning. While the early works in this area focused mainly on ranking clarify- ing questions from a pre-collected question bank [3, 4], more recent studies aim towards leveraging the generation power of LLMs to generate clarifying ques- tions [88]. However, generative systems based entirely on LLMs are not effective in proactive interactions, especially in generating clarifying questions when nec- essary [23, 63]. Initial experiments reveal the power of LLMs in understanding the context of a query or a search session [1] and generate potential questions based on the context when prompted [21]; however, they fail at planning when to ask and which question to ask [21, 63]. Shaikh et al. [63] conduct a study where they compare human–human conversations with system–human conversations and find that LLMs fail at effectively planning when to ask clarifying questions in a conversation, even though they can generate high-quality questions if they are explicitly prompted to do so. Deng et al. [22] propose a proactive chain-of- thought approach to enhance the planning capability of LLMs such as ChatGPT and show that it has a considerable effect on their interaction capabilities. Evaluation. Evaluating generative systems comes with various challenges. On top of that, evaluating interactive generative systems involves even more chal- lenges as the user response to a system output is required. A line of research looks at simulating and modeling the user–system interactions in a mixed-initiative setting [90, 11, 55, 2, 10, 48, 59]. User simulation can be beneficial to genera- tive IR models in two ways: (i) they provide a means for evaluating generated content, and (ii) they can be used for training. Zhang and Balog [90] propose a user simulator for conversational recommendation to evaluate the system per- formance. This is followed by the work done by Sekulic et al. [59] and Owoicho