data object, neural models designed for unstructured text generation from structured data, such as tables, could be potentially useful. 6.4 Evaluation of Proactive Systems Assessing proactive generative IR systems poses significant challenges. While IR research has traditionally focused on creating collections for specific information- seeking tasks, these collections are typically based on predefined needs (e.g., TREC5 tracks) or observations (e.g., clickthrough data). However, these evalua- tion methods do not readily apply to scenarios involving proactive interactions. Although evaluating proactive generative IR systems remains largely unexplored in the literature, we can envision two classes of evaluation methodologies: (1) modular evaluation, and (2) end-to-end evaluation. In modular evaluation, the quality of each component in Figure 2 is evaluated in isolation? For example, how accurate is the initiator component in identify- ing opportune moments for proactive interactions? This methodology simplifies evaluation in proactive systems, but does not provide a complete picture of the overall performance of the system from the user’s perspective, and does not reflect real-world complexities. In end-to-end evaluation, one can explore both offline and online evaluation strategies. For offline evaluation, each instance would encompass all necessary information for the system at a given timestamp, including past user-system interactions, user profiles, situational contexts, and streams of new information. The model’s performance would then be assessed based on the generated proac- tive interactions, if applicable. Crafting a single evaluation metric capable of capturing all facets of conversation initiation evaluation presents a challenge, necessitating further investigation. Recently, Samarinas and Zamani [56] intro- duced a large-scale benchmark for proactive interactions to ongoing multi-party human conversations and proposed normalized proactive discounted cumulative gain (npDCG) for end-to-end evaluation of such systems. In a separate inves- tigation, Sen et al. [62] suggested evaluating proactive recommendation within search sessions by aggregating a correlation measure over the session. This