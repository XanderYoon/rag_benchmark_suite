not only are explanations useful in user–system interactions, but they also improve the per- formance of LLMs. They study automatic rationale generation in a chain of thought (CoT) manner. Deng et al. [24] show that rephrasing the user input leads to a better understanding of the user request which in turn results in bet- ter performance of the LLM, which is complementary to CoT reasoning. In their tutorial, Anand et al. [6, 7] review Transformer-based explanation generation. Zhang et al. [89] addresses search explainability via the lens of query understand- ing, where the system’s task is to predict the user intent considering their query as input. LiEGe [79] explains all the documents in the ranking jointly using a listwise explanation generator. Evaluating explanations is challenging. For free-text generations, human eval- uation is employed. In other cases, because of a lack of explanation, proxy ex- planations such as clicks, query descriptions, query aspect annotation, and topic annotation can be used. For feature-based models, explanations are evaluated based on the effectiveness of predicted features. As for counterfactual explana- tions, model-based evaluation is employed. 7.2 Modes of Explanation in Generative IR The main mode of explanation used in generative models is free-form text, where the model would further elaborate why the provided answer is relevant to the user’s input. The explanation often consists of two major parts: (i) a further description of user information need, and (ii) an explanation of the reasons why the generated response is relevant to the user’s input. The system has a limited information bandwidth and cannot present the users with multiple intents of their query. Therefore, describing what the system “thinks” the user wants helps the user understand whether the system understands their intent or not [89]. This type of explanation aims to ensure the user that their