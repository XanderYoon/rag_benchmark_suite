that plant and is it edible?” The authors argue that generative IR systems with multi-modal interactions and multi-modal sensors can accomplish the user’s need in this and even more complex scenarios. Dealing with multi-modal interactions is a multidisciplinary topic, spanning across research areas from information retrieval, recommender systems, multi-media, human-computer interactions, computer vision, and even psychological and cognitive sciences. The intersection of the research areas that enable people to search for information through multi-modal conversations has not received the attention it deserves and it might partially be due to the com- plexity of the topic in terms of both modeling and evaluation. Prior work are mostly limited to two modalities (image and text), e.g., [67, 78], and further de- velopment in multi-modal foundation models [42, 26] and multi-modal retrieval- augmented generation models [54], is expected to speed up progress in this area. 9 User Interfaces While in Section 2 we focused on general interaction methods to assist users in expressing their information needs when interacting with generative AI, in this section we review recent work on interaction techniques and user interfaces for information access with LLMs. The design space is huge, and it is still under- researched and poorly understood. For example, out of approximately 750 pre- prints related to LLMs published on arXiv in the field of Information Retrieval between 2020-2024, only 22 mentioned ”user interface” in their abstracts. New human-LLM interaction frameworks are only starting to emerge. For ex- ample, recent work [27] reviewed 73 papers published in HCI conferences since 2021 to investigate the dynamics of human-LLM interaction. Authors identified four key phases in the interaction flow and developed a taxonomy of four pri- mary interaction modes. The four phases are: planning - before an interaction, facilitating - during an interaction, iterating - refining an interaction, and