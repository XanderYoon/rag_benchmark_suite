the generated response is relevant to the user’s input. The system has a limited information bandwidth and cannot present the users with multiple intents of their query. Therefore, describing what the system “thinks” the user wants helps the user understand whether the system understands their intent or not [89]. This type of explanation aims to ensure the user that their information need is properly understood by the system and can lead to increased trust in the system. Also, in case of misunderstanding the user’s information need, it provides the opportunity for the user to realize what is missing in their input. This can be seen as similar to scanning the SERP by the user, through which the user would have an idea if the system understands their information need correctly. Another form of explanation is to provide citations. This has been studied more extensively in the NLP community where the generated text attribution [29]. The URL citations are supposed to provide evidence of the source of infor- mation from the web. However, there are concerns regarding the quality of the citations, as there is no clear way of controlling the LLM to ground its responses on the cited page [91]. Citing source documents, while being useful as a form of explanation, still does not provide a comprehensive idea of the relevance of the source. Comparing it to a typical SERP where the users are exposed to the URLs of the results, users already have a quality perception by scanning through the page title, summary, and URL. Even though the LLM-based search interfaces aim to mimic this experience, it is not yet clear which parts of the generated response are extracted from the cited document. Moreover, it is not clear how much the system depends on its intrinsic knowledge (i.e., model