among the seven templates they use to generate the clarifying questions, they find clear preference towards certain question templates in terms of user engagement. Zou et al. [95] study the effect of the clarification pane in the same setting in a controlled experimental setup where they introduce three quality levels and measure user satisfaction and performance. They find that asking a low-quality question in a search session risks lower user engagement with questions of higher quality in the same session. This finding was confirmed in a follow-up work [98]. User engagement (i.e., click-through rate) can be considered as a user-oriented quality measure of clarifying questions. Sekulic et al. [58, 60] extract various SERP- and document-based features to predict user engagement while interact- ing with clarifying questions in a web-based interface [83]. Rahmani et al. [50] study the effect of various query- and question-based features to predict user satisfaction in the MIMICS dataset [83] where they find, among others, a posi- tive sentiment in the clarifying question leads to higher user satisfaction. Sekulic et al. [61] instead predicts the usefulness of clarifying questions in the retrieval pipeline. Following an early study on the effect of different types of clarifying questions on retrieval performance [38], they train a classifier to predict the usefulness of a clarifying question and its answer in the retrieval pipeline and incorporate it in the retrieval pipeline if only it is predicted to be useful. 5.2 Technical Challenges Planning. While the early works in this area focused mainly on ranking clarify- ing questions from a pre-collected question bank [3, 4], more recent studies aim towards leveraging the generation power of LLMs to generate clarifying ques- tions [88]. However, generative systems based entirely on LLMs are not effective in proactive interactions, especially in generating clarifying questions when nec- essary