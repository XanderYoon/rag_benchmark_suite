computational efficiency. The Vanilla RAG (Retrieval-Augmented Generation) approach implements a more sophisticated information retrieval mechanism that moves beyond simple temporal ordering. This benchmark lever- ages embedding-based retrieval to identify semantically relevant items from the user’s interaction history, selecting items based on their embedding similarity rather than recency. After retrieving these relevant historical items, the model appends them to the LLM prompt to provide context for generating recommendations. For all the experiments, we used gpt-3.5-turbo (v0125), and set the temper- ature argument to 0 to increase repeatability of the experiments. 3.3 Results NDCG@5 and HIT@5 scores for ARAG as well as benchmark mod- els is presented in the first part of Table 1. The results demonstrate that the Agentic RAG approach significantly outperforms both Recency-based Ranking and Vanilla RAG frameworks across all datasets and metrics. Examining the NDCG@5 scores, Agentic RAG achieves 0.439, 0.329, and 0.289 on Amazon Clothing, Electronics, and Home respectively, compared to the next best performing ap- proach which ranges from 0.299 to 0.238. Similarly, Hit@5 metrics show consistent superiority with Agentic achieving 0.535, 0.420, and 0.383 across the three domains. These substantial improve- ments suggest that the agentic approach to retrieval provides a more effective mechanism for identifying and ranking relevant recommendations in conversational systems. ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation SIGIR, July 13-17, 2025, Padova, Italy The improvement percentages quantify the magnitude of Agen- tic RAG’s performance gains, showing the most dramatic enhance- ment in the Clothing domain (42.12% for NDCG@5, 35.54% for Hit@5), followed by Electronics (37.94% and 30.87%) and Home (25.60% and 22.68%). This pattern suggests that the effectiveness of Agentic RAG may vary by domain characteristics, with potentially greater benefits in categories where item attributes and user prefer- ences are more diverse or complex. The consistency of improvement across all datasets