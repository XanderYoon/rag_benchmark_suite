18.0 14.3 FEVER 77.4 75.8 66.6 64.4 Quora 84.7 86.5 86.2 85.1 SCIDOCS 15.0 16.5 13.4 12.2 SciFact 65.3 67.7 63.1 61.7 Average 47.0 ∗ 46.6 43.1 39.2 Table 15: Zero-shot retrieval performance (NDCG@10, %) on 14 BEIR datasets. MS MARCO is reported for reference but excluded from the average. Note that the original RetroMAE reports average performance over 18 datasets, while our reproduction only considers 14 publicly available datasets. version was pre-trained on English Wikipedia and BookCorpus, which offer broader domain diversity and improved transferability. 2.For Contriever: The original paper uses only one hard negative per query and relies mainly on in-batch negatives, a strategy that mitigates overfitting and preserves generalization. In contrast, our setting introduces more difficult negatives, improving MS MARCO performance but leading to a drop on BEIR. More- over, we adopt a unified setup for all models and use [CLS] pooling, whereas the original Contriever uses mean pooling, which may also contribute to the performance difference. E.4 Further Analysis for SumMargLH From Table 16, we can observe the following: 1) When the number of positive instances is small, the advantage of SumMargLH over Rand1LH is limited. However, as the number increases, Sum- MargLH generally yields better performance. 2) When the average number of positives is simi- lar, UtilSel outperforms UtilRank, suggesting that LLM-selected positives may be more effective than those chosen by thresholding. Annotation Threshold Avg Loss Function SumMargLH Rand1LH UtilRank 10% 1.0 35.6 35.7 20% 1.3 35.4 35.6 30% 1.7 35.1 34.9 40% 2.3 34.7 34.6 50% 3.0 34.6 34.4 UtilSel - 2.9 35.3 34.5 Table 16: Retrieval performance (MRR@10) on MS MARCO Dev using different loss functions across var- ious annotation settings under RetroMAE backbone. “Avg” means the average number of positive instances. F Efficiency and Cost According to Gilardi et al. (2023), the