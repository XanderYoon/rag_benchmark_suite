14.3 8.920.017.3 16.4 17.3 16.4 17.5 17.4 17.2 FEVER 64.4 57.8 67.068.261.4 62.4 66.1 67.0 64.6 67.6 Quora 85.1 67.7 84.3 84.6 82.6 85.0 85.0 84.5 85.5 85.5 SCIDOCS 12.2 10.2 13.2 12.2 13.2 13.212.9 12.4 13.1 13.0 SciFact 61.7 54.8 64.8 61.6 62.2 65.5 62.9 63.765.762.7 Average 39.2 31.0 42.3 41.5 38.0 42.242.640.0 41.8 42.1 Table 12: Zero-shot retrieval performance (NDCG@10, %) of different retrievers (Contriever backbone). Top-kAnnotation Recall Generator: LlaMa-3.1-8B Generator: Qwen2.5-32B-Int8 BLUE-3 BLUE-4 ROUGE-L BERT-score BLUE-3 BLUE-4 ROUGE-L BERT-score Top 1 Human 24.7 17.2 14.2 35.7 67.8 15.8 12.6 34.3 67.4 REPLUG 21.7 15.7 12.9 33.8 66.7 14.7 11.6 32.4 66.2 UtilSel 22.3 16.3 13.4 34.7 67.4 14.9 11.7 33.5 67.1 UtilRank 22.6 16.6 13.6 35.1 67.5 15.2 12.0 33.9 67.3 Top 5 Human 55.4 13.4 11.4 33.9 66.0 14.2 11.1 33.4 67.0 REPLUG 48.4 13.8 11.4 32.9 65.8 13.9 10.8 32.8 66.7 UtilSel 51.5 14.3 11.8 33.3 66.1 13.7 10.7 33.0 66.8 UtilRank 51.6 14.4 11.9 33.3 66.1 13.8 10.7 32.9 66.8 Table 13: RAG performance with different top-kon MS MARCO QA dataset (RetroMAE backbone). BERT-Score (Zhang et al., 2019)2. 2. For factoid QA datasets, such as NQ and HotpotQA, we use Exact Match (EM) and F1 score as main metrics. E Supplementary Experimental Results E.1 Zero-shot Retrieval Performance Using Contriever Backbone Table 12 compares the zero-shot retrieval perfor- mance of various retrievers built on the Contriever backbone. All models are trained on MS MARCO using different annotation strategies, including hu- man labels, REPLUG, utility-based annotations (UtilSel and UtilRank), and corresponding curricu- lum learning variants. E.2 Top-kin RAG Our top-k choices in RAG evaluation reflect the characteristics of each dataset: 1. MS MARCO QA focuses primarily on non-factoid questions. As shown in Table 13, including more passages tends to introduce irrelevant or verbose content, which lead