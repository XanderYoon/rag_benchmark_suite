as even a single false positive can sig- nificantly affect the overall loss. As noted in Sec- tion 3.2, LLM annotations include false positives, which can make this loss function suboptimal. SumMargLH. Considering the quality of LLM annotation may be unstable, we propose a novel ob- jective that maximizes the summed marginal likeli- hood of each positive instance inD +, i.e., Ls(q, D+, D−) =−log X d+∈D+ P(d +|q, D+, D−).(4) It optimizes the overall likelihood of instances in D+ to be positive, and does not require the likeli- hood of each positive to be maximized. Thus, it relaxes the optimization towards potentially false positives, and can better leverage LLM annotations (shown in Section 6). Combining Human and LLM Annotations. When budgets allow, human-labeled data can be used alongside LLM annotations rather than rely- ing solely on the latter. Given that human annota- tions typically have higher quality than those from LLMs, simply merging and treating them equally may not be effective. Therefore, we propose us- ingcurriculum learning(Bengio et al., 2009) (CL) to integrate the two types of data, starting with training retrievers on the lower-quality LLM anno- tations and subsequently refining them with higher- quality human annotations. 4 Experimental Setup 4.1 Datasets Retrieval Datasets.As in many existing works (Xiao et al., 2022; Guo et al., 2022), we train all retrievers on the MS MARCO training set, with about 503K queries and 8.8 million passages. Re- trieval evaluation is conducted on the MS MARCO Dev set, TREC DL 19/20 (Craswell et al., 2020, 2021) with more human annotations, and the 14 public retrieval datasets across various domains with diverse downstream tasks in BEIR (Thakur et al., 2021) benchmark, excluding MS MARCO. RAG Datasets.We use the MS MARCO QA, which has the ground-truth answers for the queries in the MS MARCO