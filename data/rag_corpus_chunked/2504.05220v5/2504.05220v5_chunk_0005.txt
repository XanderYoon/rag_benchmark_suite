retrieval models were pre- dominantly classical term-based models, such as BM25 (Robertson et al., 2009), which combines term matching with TF-IDF weighting. To address the semantic mismatch limitations of classical term- based models, neural information retrieval (IR) emerged by leveraging neural networks to learn semantic representations (Huang et al., 2013; Guo et al., 2016). Subsequently, pre-trained language model (PLM)-based retrievers have been exten- sively explored (Xiao et al., 2022; Wang et al., 2023; Izacard et al., 2021a; Ma et al., 2021; Ren et al., 2021). More recently, LLMs have been di- rectly applied as first-stage retrieval models (Ma et al., 2024; Springer et al., 2024; Zhang et al., 2025; Li et al., 2024), demonstrating unprece- dented potential in IR. Relevant/ Irrelevant P(a|q,d) dq a q Candidate pool (c)(a) (b) Ground-truth answer q Document list Relevant docs q Reference answer a' Relevant docs q Reference answer a' Docs selected by utility ... Docs ranked by utility > ...> Select the passages that have utility in generating the reference answer to the following question. Utility-based Selection Step3 Rank the passages based on their utility in generating the reference answer to the following question. Utility-based Ranking Pseudo Answer Generation Step2 (Generate the reference answer to the question.) Relevance-based Selection Step1 (Select passages that are relevant to the question.) Figure 1: Different annotation methodologies: (a) Human annotation, (b) Using downstream task performance as utility score, (c) Our utility-focused annotation pipeline. The prompts are illustrative, see Appendix G for details. 2.2 Utility-Focused RAG There is a gap between the objectives of retrieval and RAG. Retrieval focuses on topical relevance, while RAG requires reference documents to be use- ful for effective generation. To address this issue, current research mainly focuses on two approaches: 1. Verbalized utility judgments, which directly uti- lized LLMs for selecting useful documents