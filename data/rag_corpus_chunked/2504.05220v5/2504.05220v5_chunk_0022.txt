more closely aligned with the distribution encountered during inference. 0% 20% 40% 60% 80% 100% (a) Human Annotation Ratio in CL 35 36 37 38 39MRR@10 UtilSel UtilRank Human 10% 20% 30% 40% 50% (b) Cutoff Threshold for UtilRank 50 60 70 80Recall & Precision Recall Precision MRR@10 34.50 34.75 35.00 35.25 35.50 35.75 MRR@10 Figure 3: (a): Retrieval performance (%) with differ- ent human annotation ratios in curriculum learning; (b): Annotation quality evaluation (%) and retrieval perfor- mance (%) with different thresholds for UtilRank. 7 Conclusion In this work, we explore the use of LLMs to an- notate large-scale retrieval training datasets with a focus on utility to reduce dependence on costly human annotations. Experiments show that retriev- ers trained with utility annotations outperform re- trievers trained with human annotations in out-of- domain settings on both retrieval and RAG tasks. Furthermore, we investigate combining LLM an- notations with human annotations by curriculum learning. Interestingly, with only 20% of human an- notations, the performance of the retriever trained on utility annotations has no significant decline over full human annotations. Moreover, with 100% human annotations yields a significant improve- ment over training solely on human annotations. This highlights the effectiveness of LLM-generated annotations as weak supervision in the early stages of training. Our study offers a comprehensive ap- proach to utilizing LLM annotations for initializing QA systems on new corpora. 8 Limitations There are several limitations that should be ac- knowledged: 1) Our annotation pool is constructed using human-annotated positives and hard nega- tives retrieved by other models. It may not fully reflect real-world annotation scenarios, where can- didates are typically retrieved using unsupervised methods like BM25 or retrievers trained on other data. We analyze the impact of including human- labeled positives in Appendix B.1. 2) Due to time and resource