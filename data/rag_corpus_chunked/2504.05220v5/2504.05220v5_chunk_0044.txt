compared to their original results. We at- tribute this to the following factors: 1.For Retro- MAE:Our reimplementation uses stronger hard negatives during MS MARCO fine-tuning, which improves in-domain performance but may hinder generalization. Additionally, our model version is pre-trained on MS MARCO, whereas the original Method Pre-training Hard Negatives Dev DL19 DL20 M@10 R@1000 N@10 N@10 BM25 (Lin et al., 2021) No - 18.4 85.3 50.6 48.0 DPR (Karpukhin et al., 2020) No Static(BM25) 31.4 95.3 59.0 - Condenser (Gao and Callan, 2021a) Yes Static(BM25) 33.8 96.1 64.8 - RetroMAE (Xiao et al., 2022) Yes Static(BM25) 35.5 97.6 - - ANCE (Xiong et al., 2020) No Dynamic 33.0 95.9 64.8 - ADORE (Zhan et al., 2021) No Dynamic 34.7 - 68.3 - CoCondenser (Gao and Callan, 2021b) Yes Dynamic 38.2 98.4 71.2 68.4 SimLM (Wang et al., 2022) Yes Dynamic 39.1 98.6 69.8 69.2 RetroMAE Yes Static(CoCondenser+BM25) 38.6 98.6 68.2 71.6 Contriever Yes Static(CoCondenser+BM25) 35.6 97.6 68.5 67.9 Table 14: Retrieval performance on MS MARCO (measured by MRR@10, Recall@1000, NDCG@10). Datasets Static(BM25) Dynamic Static(CoCondenser+BM25) RetroMAE (Xiao et al., 2022) Contriever (Izacard et al., 2021b) RetroMAE Contriever MS MARCO - 40.7 45.2 42.1 DBPedia 39.0 41.3 36.0 34.5 FiQA 31.6 32.9 29.7 28.3 NQ 51.8 49.8 49.2 47.2 HotpotQA 63.5 63.8 58.4 55.1 NFCorpus 30.8 32.8 32.8 30.4 T-COVID 77.2 59.6 63.4 49.9 Touche 23.7 23.0 24.2 20.1 CQA 31.7 34.5 32.2 28.6 ArguAna 43.3 44.6 30.5 16.9 C-FEVER 23.2 23.7 18.0 14.3 FEVER 77.4 75.8 66.6 64.4 Quora 84.7 86.5 86.2 85.1 SCIDOCS 15.0 16.5 13.4 12.2 SciFact 65.3 67.7 63.1 61.7 Average 47.0 âˆ— 46.6 43.1 39.2 Table 15: Zero-shot retrieval performance (NDCG@10, %) on 14 BEIR datasets. MS MARCO is reported for reference but excluded from the average. Note that the original RetroMAE reports average performance over 18