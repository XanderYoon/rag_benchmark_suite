LLM Annotation Figure 1(a)&(b) illustrates two primary types of document labels used in retriever training for RAG: human-annotated relevance labels and utility scores derived from downstream tasks. Retrievers trained using human-annotated relevance typically focus on aboutness and topic-relatedness. In contrast, utility scores, which are estimated based on down- stream tasks, such as the probability of LLMs gen- erating the correct answer given a document, are more beneficial for RAG (Shi et al., 2024). Build- ing on the insight that LLMs can effectively assess utility for RAG (Zhang et al., 2024b), we intro- duce a utility-focused LLM annotation pipeline for training retrievers, as depicted in Figure 1(c). This approach is designed for both initial retrieval stages and RAG, aiming to minimize the manual effort required for annotating document relevance and ground-truth answers. 3.1 Annotation Methodology Annotation Pool Construction.Given a query, the majority of documents in a corpus are irrele- vant, making it impractical to annotate the utility of every document with LLMs. A common prac- tice is to compile a candidate pool by aggregating documents retrieved by effective retrievers, such as unsupervised methods like BM25 (Robertson et al., 2009), and retrievers trained on other collections. We adopt a similar approach in our study. Our anno- tation process is based on the widely used retrieval benchmark, the MS MARCO passage set (Nguyen et al., 2016). It is well-known that MS MARCO typically includes only one annotated positive ex- ample per query and many false negatives due to under-annotation (Craswell et al., 2020, 2021). Retrievers trained with MS MARCO typically gather a pool of hard negatives {dâˆ’ i }n i=1, from which a subset of m samples is randomly selected. 0 10 20 30 Number of Positive Instances 0.0 2.5 5.0 7.5 10.0 12.5 15.0Proportion (%) Llama-3.1-8B RelSel UtilSel 0 10 20 30