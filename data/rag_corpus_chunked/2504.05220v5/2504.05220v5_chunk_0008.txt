many false negatives due to under-annotation (Craswell et al., 2020, 2021). Retrievers trained with MS MARCO typically gather a pool of hard negatives {dâˆ’ i }n i=1, from which a subset of m samples is randomly selected. 0 10 20 30 Number of Positive Instances 0.0 2.5 5.0 7.5 10.0 12.5 15.0Proportion (%) Llama-3.1-8B RelSel UtilSel 0 10 20 30 Number of Positive Instances 0 5 10 15 20 25 30 Qwen-2.5-32B-Int8 RelSel UtilSel Figure 2: Positive annotation distribution of different annotators at various stages. These sampled hard negatives, along with the sin- gle positive d+ and in-batch negatives, are then used for contrastive learning. To neutralize the impact of hard negatives when comparing the re- trievers trained with human and LLM annotations, we utilize the same collection of positives and hard negatives as in Ma et al. (2024) (from BM25 and CoCondenser (Gao et al., 2021)) for LLM anno- tation. This ensures that all comparison models have the same set of n+ 1 annotated documents for each query, differing only in their annotations. m+ 1 instances are selected for training in each epoch, including positives and randomly sampled negatives (n= 30, m= 15 in this paper). To study the effect of whether human-annotated positives are included in the annotation pool, we compare the performance of consistently including and ex- cluding human-annotated positives in training. As presented in Appendix B.1, the essential conclu- sions are similar to those we report in Section 5. Annotation Methods.After collecting the candi- date pool, we apply three annotation methods, as illustrated in Figure 1(c): relevance-based selec- tion (RelSel), utility-based selection (UtilSel), and utility-based ranking (UtilRank). InRelSel, we begin with an initial filtering step where an LLM is used to select a subset of documents that are top- ically relevant to the query. Next, we