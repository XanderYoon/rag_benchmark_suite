12.0 33.9−† 67.3−† REPLUG (CL 20%) 23.2 − 16.7 13.7 34.9 − 67.4− 15.2 12.1 33.6 − 67.1− UtilSel (CL 20%) 24.6 † 17.4 14.3 35.4 † 67.7† 15.8 12.6 34.2† 67.4† UtilRank (CL 20%) 24.6 † 17.4 14.4 35.6† 67.8† 15.8 12.6 34.3† 67.5† REPLUG (CL 100%) 25.0 17.2 14.2 35.8 67.8 15.8 12.6 34.4 67.5 UtilSel (CL 100%)25.6 + 17.8 14.8 36.0 68.0+† 16.2 12.9 34.6+† 67.7+† UtilRank (CL 100%) 25.5 + 17.7 14.7 35.968.0 +† 16.2 12.9 34.6+† 67.7+† Table 4: RAG performance (%) of different retrievers (RetroMAE backbone) trained with various MS MARCO annotations on MS MARCO QA dataset. The symbols +, −, and † are defined in Table 2.Boldand underline are also defined in Table 2. The official BLEU evaluation for MS MARCO QA targets the entire queries, not individual queries, thus no significance tests are conducted. pared to retrievers trained solely on MS MARCO human annotations. This indicates that reliance on MS MARCO human labels may lead to model overfitting to the corpus. The fact that UtilSel out- performs UtilRank and it utilizes more LLM anno- tations than UtilRank, as shown in Table 1, further supports this observation. 2) When incorporating 20% or 100% human labels during training, the OOD retrieval performance decreases compared to not using them, reinforcing the first point. These findings suggest a trade-off between in-domain and OOD retrieval performance, which can be adjusted by varying the mix of MS MARCO human labels with LLM annotations. 5.2 RAG Performance In-domain Results.In Table 4, we present the RAG performance on MS MARCO QA using pas- sages from retrievers (based on RetroMAE) com- pared in Section 5.1 for RAG. The findings are consistent with the first three conclusions regard- ing in-domain retrieval discussed in 5.1, which is expected as more accurate retrieval enhances