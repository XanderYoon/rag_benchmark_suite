in Section 5. Annotation Methods.After collecting the candi- date pool, we apply three annotation methods, as illustrated in Figure 1(c): relevance-based selec- tion (RelSel), utility-based selection (UtilSel), and utility-based ranking (UtilRank). InRelSel, we begin with an initial filtering step where an LLM is used to select a subset of documents that are top- ically relevant to the query. Next, we employ the utility judgment method from Zhang et al. (2024b), which involves generating a pseudo-answer based on the output from RelSel and assessing document utility for downstream generation using the pseudo- relevant documents and pseudo-answer. This list- wise comparison enables the LLM to make accurate relative judgments. InUtilSel, the LLM selects the subset of useful documents. In contrast,UtilRank asks the LLM to rank the input documents accord- ing to their utility, then the top k% documents are annotated as positive (k= 10 in our main experi- ments). The float number is rounded down, and if the result is zero, a single document will be marked LLM Precision Recall Avg Number RS US UR RS US UR RS US UR Llama 7.1 11.9 36.5 97.6 91.6 41.0 13.8 7.7 1.2 Qwen 15.1 29.5 71.3 92.8 84.8 72.0 6.2 2.9 1.0 Table 1: Precision and Recall (%) of human positive under different annotations. “RS”, “US”, “UR” mean “RelSel”, “UtilSel”, “UtilRank”, respectively. as positive. UtilSel can flexibly determine the num- ber of useful documents, whereas UtilRank allows for different thresholds to balance the precision and recall of LLM annotations. All the annotation prompts are detailed in Appendix G. 3.2 Statistics of LLM Annotations We employ two well-known open-source LLMs of different sizes for annotation: LlaMa-3.1- 8B-Instruct (Llama-3.1-8B) (Dubey et al., 2024) and Qwen-2.5-32B-Instruct with GPTQ-quantized (Frantar et al., 2022) 8-bit version (Qwen-2.5-32B- Int8) (Yang et al., 2024). Positive Annotation Distribution.Figure 2 shows the