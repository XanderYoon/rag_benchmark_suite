human labels with LLM annotations. 5.2 RAG Performance In-domain Results.In Table 4, we present the RAG performance on MS MARCO QA using pas- sages from retrievers (based on RetroMAE) com- pared in Section 5.1 for RAG. The findings are consistent with the first three conclusions regard- ing in-domain retrieval discussed in 5.1, which is expected as more accurate retrieval enhances gen- eration. This confirms that UtilSel and UtilRank can significantly reduce human annotation efforts while maintaining comparable RAG performance. Notably, REPLUG performs the poorest among the methods, differing from results in Shi et al. (2024). This discrepancy could arise because we used RE- PLUG for static utility annotation, whereas the original paper iteratively updated retrievers based on generation performance for RAG. OOD Results.Similarly, we assess the RAG per- formance based on MS MARCO-trained retrievers on NQ and HotpotQA. Results are shown in Table 5. Key findings include: 1) UtilSel and UtilRank con- sistently yield the best RAG performance across most generators and datasets (particularly on NQ), Annotation NQ HotpotQA Recall Llama Qwen Recall Llama Qwen EM F1 EM F1 EM F1 EM F1 Human 56.7 42.8 56.4 43.6 57.9 54.8 31.5 42.6 38.6 50.7 REPLUG 46.2 − 41.1− 53.7− 41.6− 55.0− 53.3− 30.6− 41.6− 38.0 50.0 − UtilSel 61.1 +† 44.4+† 58.8+† 44.9† 59.8+† 55.8+† 31.9† 43.2† 39.0† 51.1† UtilRank62.0 +† 45.4+† 59.8+† 45.9+† 60.0+† 55.9+† 31.4† 43.0† 38.7 51.0 † REPLUG (CL 20%) 55.0 − 43.3 56.9 44.7 58.4 56.5 + 31.3 42.6 38.6 50.7 UtilSel (CL 20%) 59.8 +† 43.4 58.0 + 44.9+ 59.3+ 56.2+ 31.9 43.0 38.8 51.0 UtilRank (CL 20%) 59.7 +† 44.7+ 58.9+† 45.6+ 59.7+† 56.2+ 31.5 42.939.0 51.3 REPLUG (CL 100%) 58.2 + 43.5 57.2 45.3 + 59.2+ 57.1+ 31.8 43.3+ 38.8 51.1 UtilSel (CL 100%) 59.9 +† 43.7 57.5 45.4 + 59.8+ 56.6+