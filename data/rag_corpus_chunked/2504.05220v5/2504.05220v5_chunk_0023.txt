1) Our annotation pool is constructed using human-annotated positives and hard nega- tives retrieved by other models. It may not fully reflect real-world annotation scenarios, where can- didates are typically retrieved using unsupervised methods like BM25 or retrievers trained on other data. We analyze the impact of including human- labeled positives in Appendix B.1. 2) Due to time and resource constraints, we did not adopt stronger LLMs for annotation, though they may offer fur- ther improvements. Moreover, our annotations are limited to MS MARCO, a standard dataset for re- trieval. Extending this approach to RAG datasets like NQ remains a promising direction, as our anal- ysis suggests that similar trends would likely hold. To further investigate this, we leverage a SOTA open-source LLM, Qwen3-32B (Yang et al., 2025), for annotation on the NQ dataset. The results are shown in Appendix C. The conclusion is that LLM annotations can achieve comparable performance to relevance annotations based on human answers. 9 Ethics Statement Our research does not rely on personally identi- fiable information. All datasets, pre-trained IR models, and LLMs used in this study are publicly available, and we have properly cited all relevant sources. We firmly believe in the principles of open research and the scientific value of reproducibility. To this end, we have made all our code, data, and trained models associated with this paper publicly available on GitHub. Acknowledgements This work was supported by several grants, in- cluding the National Natural Science Foundation of China (Grant Nos. 62441229 and 62302486), the Innovation Funding of ICT CAS (Grant No. E361140 and No.E561010), the CAS Special Re- search Assistant Funding Project, the project under Grant No. JCKY2022130C039, and the Strategic Priority Research Program of the CAS (Grant No. XDB0680102). References Andrea Bacciu, Florin Cuconasu, Federico Sicil- iano, Fabrizio Silvestri, Nicola Tonellotto,