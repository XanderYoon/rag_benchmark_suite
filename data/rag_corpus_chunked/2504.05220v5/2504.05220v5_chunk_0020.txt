56.5 + 31.3 42.6 38.6 50.7 UtilSel (CL 20%) 59.8 +† 43.4 58.0 + 44.9+ 59.3+ 56.2+ 31.9 43.0 38.8 51.0 UtilRank (CL 20%) 59.7 +† 44.7+ 58.9+† 45.6+ 59.7+† 56.2+ 31.5 42.939.0 51.3 REPLUG (CL 100%) 58.2 + 43.5 57.2 45.3 + 59.2+ 57.1+ 31.8 43.3+ 38.8 51.1 UtilSel (CL 100%) 59.9 +† 43.7 57.5 45.4 + 59.8+ 56.6+ 31.7 43.2 38.7 50.8 UtilRank (CL 100%) 59.4 +† 43.8 57.8+ 45.0+ 59.1+ 56.0+ 31.4 42.9 38.4 50.7 Table 5: RAG performance (%) of different retrievers (RetroMAE backbone) trained with various MS MARCO annotations on the NQ and HotpotQA datasets. The symbols +, −, and † are defined in Table 2.Boldand underline are also defined in Table 2. “Llama” and “Qwen” are “Llama-3.1-8B” and “Qwen-2.5-32B-Int8”, respectively. highlighting the potential of utility-focused LLM annotation in initializing QA systems. 2) On NQ, the best RAG performance is observed when no human annotations are used, mirroring the retrieval performance trend across many BEIR datasets (in Table 3). In contrast, on HotpotQA, retrieval per- formance is improved when human labels are used, while RAG is not enhanced. These results suggest that human annotations do not significantly benefit UtilSel and UtilRank for OOD RAG. 6 Further Analysis Comparison of Strategy Variants.Table 6 com- pares the variants of our annotation method and training strategies regarding the retrieval perfor- mance on MS MARCO. The default setting for each component when using LLM annotations for training is Qwen, UtilSel, and SumMargLH. Key findings are: 1) Within the same GPU memory, the quantized version of larger LLMs has better capac- ity than smaller ones (Qwen better than LLama); 2) UtilSel and UtilRank lead to better performance than RelSel, indicating stricter annotation criterion is needed; 3) When multiple positives exist, Sum- MargLH achieves the best performance, indicating its robustness to