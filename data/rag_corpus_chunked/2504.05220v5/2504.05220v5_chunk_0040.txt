the queries with ground truth answers from 3,452 queries on NQ and then collected 2,255 queries for RAG evaluation. Table 11 shows detailed statistics of the in-domain retrieval datasets and all RAG datasets used in our work. D.2 Implementation Details The retriever is trained for 2 epochs using the AdamW optimizer with a batch size of 16 (per device) and a learning rate of 3e-5. Training is con- ducted on a machine with 8Ã— Nvidia A800 (80GB) GPUs. To ensure reproducibility of the single run, the random seed that will be set at the beginning of training using the default value. In the second stage of curriculum learning, the retriever is further trained for 1 epoch with the same hyper-parameters, except that the learning rate is re-initialized to 3e-5. Unless otherwise specified, we use Qwen-2.5- 32B-Int8 as the annotator, adopt the SumMargLH loss with UtilSel annotations, and apply thePos-all strategy for selecting positives. During curriculum learning, the positive sampling strategy is switched toPos-one(see Appendix B.2 for details). Due to the top 10% ranked list of UtilRank containing an average of one positive, and SumMargLH have no advantage in UtilRank, we use Rand1LH loss for training under UtilRank. For RAG evaluation, the retrieved passages are directly fed to LLMs. We use top-1 passage for MS MARCO QA and top-5 passages for NQ and Hot- potQA. The rationale for these choices is discussed in Appendix E.2. The original REPLUG (Shi et al., 2024) uses Contriever (Izacard et al., 2021b) and optimizes the retriever by aligning its relevance scores with LLM- derived utility scores via KL divergence. Our setup follows the overall REPLUG framework but differs in two key aspects: we adopt the same retriever backbone as in other experiments for fair compari- son, and use static negatives during training instead of dynamically