different annotation strategies, including hu- man labels, REPLUG, utility-based annotations (UtilSel and UtilRank), and corresponding curricu- lum learning variants. E.2 Top-kin RAG Our top-k choices in RAG evaluation reflect the characteristics of each dataset: 1. MS MARCO QA focuses primarily on non-factoid questions. As shown in Table 13, including more passages tends to introduce irrelevant or verbose content, which lead to lower RAG performance. Therefore, we use top-1 passage for evaluation. 2. HotpotQA is a multi-hop factoid QA dataset, which naturally benefits from access to multiple supporting pas- Evaluation 2We use the best model for BERT-Score: ( https:// huggingface.co/microsoft/deberta-xlarge-mnli) sages. Hence, we adopt top-5 passages (NQ also uses top-5 passages for consistency). E.3 Comparison with Reported Retrieval Results in Prior Work In this section, we summarize the retrieval perfor- mance of several representative dense retrievers on MS MARCO and BEIR, based on results reported in their original papers. Table 14 shows performance on MS MARCO. Compared to the original results, our reproduction of RetroMAE shows slight differences. This can be attributed to the use of different hard negatives: while the original model used BM25-mined neg- atives, we employ a combination of BM25 and coCondenser negatives, which are more diverse and challenging. This leads to improved perfor- mance on MS MARCO by enhancing the ability to distinguish fine-grained semantic differences. Table 15 reports zero-shot performance on BEIR, measured by NDCG@10 across 14 datasets. Both RetroMAE and Contriever show a performance drop compared to their original results. We at- tribute this to the following factors: 1.For Retro- MAE:Our reimplementation uses stronger hard negatives during MS MARCO fine-tuning, which improves in-domain performance but may hinder generalization. Additionally, our model version is pre-trained on MS MARCO, whereas the original Method Pre-training Hard Negatives Dev DL19 DL20 M@10 R@1000 N@10 N@10 BM25 (Lin et al.,