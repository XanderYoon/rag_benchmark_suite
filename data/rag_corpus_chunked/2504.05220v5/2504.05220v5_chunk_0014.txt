with utility scores and then updates the model with either 20% randomly se- lected or 100% of the human annotations using curriculum learning. Annotation RetroMAE Contriever Human Test Hybrid Test Human Test Hybrid Test Dev DL19 DL20 M@10 N@10 Dev DL19 DL20 M@10 N@10M@10 R@1000 N@10 N@10 M@10 R@1000 N@10 N@10 Human 38.6 98.6 68.271.683.7 63.1 35.6 97.6 68.5 67.9 82.2 62.0 REPLUG 33.8 − 94.7− 65.5 58.7 75.7 − 54.3− 31.4− 93.1− 64.3 59.7 79.4 53.2 − UtilSel 35.3 −† 97.7−† 68.0 71.0 87.5+† 65.8+† 33.3−† 96.8−† 67.8 67.8 85.0 † 63.7† UtilRank 35.7 −† 97.8−† 67.1 71.0 86.1 † 66.1+† 33.6−† 96.8−† 70.8 68.8 84.6† 63.7† REPLUG (CL 20%) 36.6 − 98.3− 69.5 67.8 81.7 60.2 − 33.7− 97.2− 68.4 66.6 82.9 59.4 − UtilSel (CL 20%) 38.2 † 98.5† 69.6 71.4 83.4 65.5 +† 35.3† 97.4 69.3 68.7 85.4 + 63.4† UtilRank (CL 20%) 38.3 † 98.470.5 70.0 84.3 64.6† 35.6† 97.4 70.4 70.1 86.1+ 64.0† REPLUG (CL 100%) 38.7 98.6 69.5 69.7 83.7 63.1 35.5 97.7 68.0 69.1 80.7 59.0 − UtilSel (CL 100%)39.3 +† 98.670.5 70.9 84.7 64.7+† 36.6+† 97.8 69.3 68.4 85.7 +† 63.8+† UtilRank (CL 100%) 39.2 +† 98.7 69.6 69.9 84.2 64.2 36.5 +† 97.8 69.9 69.2 85.2+† 63.9+† Table 2: Retrieval performance (%) of different annotation methods. “M@k”, “R@k”, “N@k” mean “MRR@k”, “Recall@k”, and “NDCG@k” respectively. “+”, “−”, and “†” indicate significant improvements and decrements over Human, and significant improvements over REPLUG within the same group, respectively, using a two-sided paired t-test (p <0.05). underline andBoldindicate the best performance within each group and overall. Similarly, our methods include using LLM anno- tations alone (UtilSel, UtilRank), and combining them with 20%/100% human annotations using cur- riculum learning. Implementation details of each method can be found in Appendix D.2. 4.3 Evaluation Human annotations