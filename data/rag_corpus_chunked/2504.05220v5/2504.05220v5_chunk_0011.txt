stricter to be more accurate. Qwen has fewer false-positive issues, and its UtilRank has the best overall preci- sion and recall trade-off. Since Qwen has better annotation quality, our experiments in Section 5 are all based on its annotations. 3.3 Training with Utility Annotations Loss Function.Dense retrievers are typically trained to maximize the likelihood of a positive sample d+ compared to a negative passage set D−, which usually includes hard negatives and in-batch negatives (Karpukhin et al., 2020). Given a query q, the probability of a document d to be positive in {d+} ∪D − is calculated as: P(d|q, d +, D−) = exp(s(q, d))P d′∈{d+}∪D− exp(s(q, d′)) ,(1) wheres(q, d)is the matching score ofqandd. SingleLH. As many large-scale retrieval datasets, such as MS MARCO, only have one rele- vant instance per query, the loss function is usually maximizing the likelihood of the single positive: Ls(q, d+, D−) =−logP(d +|q, d+, D−).(2) Since LLMs have multiple positive annotations, SingleLH cannot be used directly. Rand1LH. A straightforward approach is to ran- domly sample one positive instance per query in each epoch and use the standard SingleLH for train- ing, which we name as Rand1LH. JointLH. Another common way is to enlarge {d+} to a positive passage set D+(|D+| ≥1) and optimize the joint likelihood of each positive in- stance inD +: Ls(q, D+, D−) =−log Y d+∈D+ P(d +|q, D+, D−).(3) This function may not be robust to low-quality an- notations, as even a single false positive can sig- nificantly affect the overall loss. As noted in Sec- tion 3.2, LLM annotations include false positives, which can make this loss function suboptimal. SumMargLH. Considering the quality of LLM annotation may be unstable, we propose a novel ob- jective that maximizes the summed marginal likeli- hood of each positive instance inD +,