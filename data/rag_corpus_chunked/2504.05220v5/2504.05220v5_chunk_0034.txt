using RetroMAE backbone. “ +” and “−” indicate significant improvements and decrements over Human using a two-sided paired t-test (p <0.05). Dataset Human Random Exclusion Inclusion 0% (CL, 20%) 0% (CL, 20%) (CL, 30%) 0% (CL, 20%) DBPedia 36.0 38.0 37.439.037.3 37.1 38.8 37.0 FiQA 29.7 32.6 32.1 30.132.831.2 32.6 32.3 NQ 49.2 53.5 51.4 52.2 51.0 51.853.751.0 HotpotQA 58.4 59.6 60.0 59.160.560.4 59.9 60.3 NFCorpus 32.8 33.9 34.234.434.3 33.4 34.134.4 T-COVID 63.4 66.1 65.0 60.3 67.4 66.1 65.167.6 Touche 24.228.524.7 25.3 26.5 26.2 25.0 26.2 CQA 32.2 32.3 33.9 32.234.733.4 32.4 33.8 ArguAna 30.5 34.1 36.439.338.5 36.4 37.9 36.8 C-FEVER 18.019.516.5 19.3 17.2 16.7 18.3 17.2 FEVER 66.6 73.8 69.9 69.9 71.4 71.671.0 71.2 Quora 86.2 85.4 86.1 84.9 86.2 86.385.8 86.2 SCIDOCS 13.4 14.3 14.4 14.514.2 14.1 14.3 14.1 SciFact 63.1 62.864.262.9 63.9 64.263.2 63.2 Avg 43.1 45.3 44.7 44.545.444.9 45.2 45.1 Table 8: Zero-shot retrieval performance (NDCG@10, %) with different UtilSel annotation labels on whether human-annotated relevant passage is included or not during training using RetroMAE backbone. where f <·> is usually implemented as a sim- ple metric, e.g., dot product and cosine similarity. Rq(·)andR d(·)usually share the parameters. A.2 Downstream Task Performance as Utility Score Considering the downstream task for the retriever, i.e., RAG, the goals of the retriever and genera- tor in RAG are different and can be mismatched. To alleviate this issue, the utility of retrieval in- formation fu(q, d, a), where a is the ground truth answer, enables the retriever to be more effec- tively alignment with the generator. fu(q, d, a) mainly has two ways: directly model how likely the candidate passages can generate the ground truth answer (Shi et al., 2024), i.e., P(a|q, d) , which computes the likelihood of the ground truth an- swer; and measure the divergence of model