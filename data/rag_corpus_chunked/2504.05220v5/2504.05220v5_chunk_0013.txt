on the MS MARCO Dev set, TREC DL 19/20 (Craswell et al., 2020, 2021) with more human annotations, and the 14 public retrieval datasets across various domains with diverse downstream tasks in BEIR (Thakur et al., 2021) benchmark, excluding MS MARCO. RAG Datasets.We use the MS MARCO QA, which has the ground-truth answers for the queries in the MS MARCO retrieval dataset, to evaluate the RAG performance when using Llama-3.1-8B and Qwen-2.5-32B-Int8 as generators. Similarly, for two subsets of BEIR, i.e., NQ (Kwiatkowski et al., 2019) and HotpotQA (Yang et al., 2018), we use the ground-truth answers of the questions to evaluate the RAG performance with the two generators. Detailed information about the datasets can be found in Appendix D.1. 4.2 Baselines Our comparisons of data annotation methods are based on the pretrained version of two represen- tative retrievers, RetroMAE (Xiao et al., 2022) and Contriever (Izacard et al., 2021a) (before fine- tuning). Our baselines include retrievers trained with human annotations and downstream task per- formance (shown in Figure 1(a)&(b) respectively): • Human: Retrievers trained with original human annotations in MS MARCO using SingleLH. • REPLUG(Shi et al., 2024): The likelihood of the ground-truth answer given each passage is used as its utility label. Retrievers are optimized towards negative KL divergence between the dis- tribution of passage utility labels and their rele- vance scores (see Appendix A.2 for details). • REPLUG (CL 20%/100%): This approach ini- tially trains the model with utility scores and then updates the model with either 20% randomly se- lected or 100% of the human annotations using curriculum learning. Annotation RetroMAE Contriever Human Test Hybrid Test Human Test Hybrid Test Dev DL19 DL20 M@10 N@10 Dev DL19 DL20 M@10 N@10M@10 R@1000 N@10 N@10 M@10 R@1000 N@10 N@10 Human 38.6 98.6 68.271.683.7 63.1 35.6 97.6 68.5 67.9