REPLUG within the same group, respectively, using a two-sided paired t-test (p <0.05). underline andBoldindicate the best performance within each group and overall. Similarly, our methods include using LLM anno- tations alone (UtilSel, UtilRank), and combining them with 20%/100% human annotations using cur- riculum learning. Implementation details of each method can be found in Appendix D.2. 4.3 Evaluation Human annotations often contain many false neg- atives due to under-annotation, and humans may have different preferences from LLMs. Evaluat- ing retrieval performance using human labels as ground truth may be unfair to models trained with LLM annotations. To create a more balanced com- parison set with more relevance labels and fewer false negatives, we randomly sampled 200 queries from the MS MARCO Dev set. For each query, we collected a candidate pool by merging the top 20 retrieved passages from various retrievers (Human, REPLUG, UtilSel, UtilRank) and used GPT-4o- mini (Hurst et al., 2024) to select positive instances from the pool based on theground-truthanswer, using the UtilSel prompt (see Appendix G). Both the original human and GPT-annotated positives are considered new golden labels. We refer to this combined set as theHybrid Testand the set with only human annotations as theHuman Test. We evaluate retrievers trained with MS MARCO annotated data by humans or LLMs under both in-domain settings (MS MARCO Dev, TREC DL 19/20, MS MARCO Hybrid Test) and out-of- domain settings (14 BEIR datasets). The retrieved results are then directly fed to generators to assess downstream QA performance on MS MARCO QA and two BEIR datasets, NQ and HotpotQA. De- tailed evaluation metrics for retrieval and RAG are provided in Appendix D.3. 5 Experimental Results 5.1 Retrieval Performance In-domain Results.Table 2 shows the overall in-domain retrieval performance. Main findings include: 1) On human-labeled test sets, models trained with human relevance