and retrieval-augmented generation (RAG) is increasingly recognized as a key strategy for reducing hallucinations in large language models (LLMs) in the modern landscape of information access (Shuster et al., 2021; Za- mani et al., 2022; Ram et al., 2023). Typically, re- trieval models rely on human annotations of query- *Contributed equally â€ Corresponding authors document relevance for training and evaluation. In RAG, the goal shifts towards optimizing the final question answering (QA) performance using re- sults from effective retrievers, with less emphasis on retrieval performance itself. Given the high cost of human annotation and the promising potential of LLMs for relevance judgments (Rahmani et al., 2024), we aim to explore whether LLM-generated annotations can effectively replace human annota- tions in training models for retrieval and RAG. This is particularly crucial for initializing QA systems based on a reference corpus without annotations. There is a gap between the objectives of retrieval and RAG. Retrieval focuses on topical relevance, while RAG requires reference documents to be use- ful for generation (i.e., utility). In other words, re- sults considered relevant by a retriever may not be useful for an LLM during generation. Aware of this mismatch, researchers have shifted from using rel- evance annotations as document labels to assessing LLM performance on downstream tasks with the document as its label (Shi et al., 2024; Lewis et al., 2020; Izacard et al., 2023; Glass et al., 2022; Za- mani and Bendersky, 2024; Gao et al., 2024). This includes metrics such as the likelihood of generat- ing ground-truth answers (Shi et al., 2024) or exact match scores between generated and ground-truth answers (Zamani and Bendersky, 2024). Another approach involves prompting LLMs to select docu- ments with utility from relevance-oriented retrieval results for use in RAG (Zhang et al., 2024a,b). Stud- ies from both approaches have demonstrated