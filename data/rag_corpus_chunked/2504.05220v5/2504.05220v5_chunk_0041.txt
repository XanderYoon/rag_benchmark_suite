2024) uses Contriever (Izacard et al., 2021b) and optimizes the retriever by aligning its relevance scores with LLM- derived utility scores via KL divergence. Our setup follows the overall REPLUG framework but differs in two key aspects: we adopt the same retriever backbone as in other experiments for fair compari- son, and use static negatives during training instead of dynamically generated ones. D.3 Evaluation Metrics To evaluate retrieval performance, we employ three standard metrics: Mean Reciprocal Rank (MRR) (Craswell, 2009), Recall and Normalized Dis- counted Cumulative Gain (NDCG) (Järvelin and Kekäläinen, 2002). To evaluate RAG performance, we adopt two different approaches based on the nature of the datasets: 1. For datasets that include non-factoid QA, such as MS MARCO, we evalu- ate answer generation performance using ROUGE (Lin, 2004), BLEU (Papineni et al., 2002) 1, and 1https://github.com/microsoft/ MSMARCO-Question-Answering/tree/master/ Datasets Human REPLUG UtilSel UtilRank Curriculum Learning, 20% Curriculum Learning, 100% REPLUG UtilSel UtilRank REPLUG UtilSel UtilRank DBPedia 34.5 26.637.336.9 33.7 36.3 36.8 35.9 36.7 36.8 FiQA 28.3 22.530.129.3 28.3 29.4 29.6 29.2 29.5 29.2 NQ 47.2 37.050.750.7 43.5 48.2 49.2 47.0 48.9 49.9 HotpotQA 55.1 49.9 56.8 55.5 55.9 56.9 56.7 56.9 57.056.9 NFCorpus 30.4 28.0 31.3 31.1 31.6 31.3 30.9 31.531.831.5 T-COVID 49.9 26.9 53.4 55.1 34.8 59.1 62.248.7 56.6 56.7 Touche 20.1 14.7 23.726.614.1 21.0 26.0 17.0 21.4 24.4 CQA 28.6 24.6 28.9 26.5 29.9 30.929.9 28.1 29.5 29.5 ArguAna 16.9 4.6 30.3 25.3 24.534.232.3 20.4 28.3 27.9 C-FEVER 14.3 8.920.017.3 16.4 17.3 16.4 17.5 17.4 17.2 FEVER 64.4 57.8 67.068.261.4 62.4 66.1 67.0 64.6 67.6 Quora 85.1 67.7 84.3 84.6 82.6 85.0 85.0 84.5 85.5 85.5 SCIDOCS 12.2 10.2 13.2 12.2 13.2 13.212.9 12.4 13.1 13.0 SciFact 61.7 54.8 64.8 61.6 62.2 65.5 62.9 63.765.762.7 Average 39.2 31.0 42.3 41.5 38.0 42.242.640.0 41.8 42.1 Table 12: Zero-shot retrieval