QA tasks with retrieval collections also included in BEIR, i.e., NQ (Kwiatkowski et al., 2019) and HotpotQA (Yang et al., 2018). Our find- ings include: 1) LLM annotations alone result in worse in-domain retrieval performance but better out-of-domain performance compared to human annotations; 2) Combining LLM annotations with 20% of human annotations achieves similar perfor- mance to models trained with 100% human labels; 3) Retrievers trained with both LLM and human annotations using curriculum learning significantly outperform those using only human annotations; 4) The findings for RAG performance are consis- tent with the retrieval performance regarding both in-domain and out-of-domain datasets. We summa- rize our contributions as follows: • We introduce a comprehensive solution for data annotation using LLMs for retrieval and RAG, along with corresponding training strategies. • We conduct an extensive study on the use of LLM-annotated utility to train retrievers for both in-domain and out-of-domain retrieval and RAG. • Extensive experiments and analyses demonstrate the advantages of leveraging utility-focused LLM annotations for retrieval and RAG, particularly for out-of-domain data. • We enhance the MS MARCO dataset with LLM annotations, providing passage labels for approx- imately 500K queries, which can facilitate re- search on false negatives, weak supervision, and retrieval evaluation by LLMs. Our work offers a viable and promising solution for initiating QA systems on new corpora, espe- cially when human annotations are unavailable and budgets are limited. 2 Related Work 2.1 First-Stage Retrieval Initially, the first-stage retrieval models were pre- dominantly classical term-based models, such as BM25 (Robertson et al., 2009), which combines term matching with TF-IDF weighting. To address the semantic mismatch limitations of classical term- based models, neural information retrieval (IR) emerged by leveraging neural networks to learn semantic representations (Huang et al., 2013; Guo et al., 2016). Subsequently, pre-trained language model (PLM)-based retrievers