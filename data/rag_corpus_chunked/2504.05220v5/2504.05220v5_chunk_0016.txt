fed to generators to assess downstream QA performance on MS MARCO QA and two BEIR datasets, NQ and HotpotQA. De- tailed evaluation metrics for retrieval and RAG are provided in Appendix D.3. 5 Experimental Results 5.1 Retrieval Performance In-domain Results.Table 2 shows the overall in-domain retrieval performance. Main findings include: 1) On human-labeled test sets, models trained with human relevance annotations perform better than using LLM annotations alone, and they are both better than training with downstream task performance (REPLUG). 2) When combining 20% human labels, the model performance of UtilSel and UtilRank has no significant difference with using all the human annotations. This means that UtilSel and UtilRank can save about 80% human ef- fort on this dataset to achieve similar performance. 3) With 100% human annotations, UtilSel and Util- Rank can achieve significant improvements over using human annotations alone, which confirms the efficacy of our annotation and training strategy as a data augmentation approach. 4) Regarding both human and GPT-4 annotated golden labels, UtilSel and UtilRank significantly outperform mod- els trained with human annotations alone, indicat- ing their potential in a fairer setting. Out-of-domain (OOD) Results.Table 3 and Ta- ble 12 (in Appendix E.1) report the zero-shot re- trieval performance of RetroMAE and Contriever trained with different annotations. We observe the following: 1) Both UtilSel and UtilRank exhibit superior out-of-domain (OOD) performance com- Datasets BM25 Human REPLUG UtilSel UtilRank Curriculum Learning, 20% Curriculum Learning, 100% REPLUG UtilSel UtilRank REPLUG UtilSel UtilRank DBPedia 31.8 36.0 29.138.037.9 35.9 37.4 37.4 36.1 37.1 37.5 FiQA 23.6 29.7 24.932.631.6 30.8 32.1 31.3 31.3 31.6 30.4 NQ 30.6 49.2 41.2 53.5 53.948.0 51.4 51.9 50.1 51.9 51.7 HotpotQA63.358.4 57.4 59.6 59.6 60.2 60.0 59.8 60.5 60.1 59.5 NFCorpus 32.2 32.8 30.3 33.9 34.0 33.934.233.8 33.7 34.0 33.4 T-COVID 59.5 63.4 54.2 66.1 64.5