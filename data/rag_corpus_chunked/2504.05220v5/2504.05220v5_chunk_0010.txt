precision and recall of LLM annotations. All the annotation prompts are detailed in Appendix G. 3.2 Statistics of LLM Annotations We employ two well-known open-source LLMs of different sizes for annotation: LlaMa-3.1- 8B-Instruct (Llama-3.1-8B) (Dubey et al., 2024) and Qwen-2.5-32B-Instruct with GPTQ-quantized (Frantar et al., 2022) 8-bit version (Qwen-2.5-32B- Int8) (Yang et al., 2024). Positive Annotation Distribution.Figure 2 shows the distribution of positive annotations made by RelSel and UtilSel (UtilRank is not shown since its number of positives is determined by the thresh- old k%). The average number section in Table 1 provides the specific average number of positive annotations. We find that the instances considered useful by LLMs are significantly fewer than those they identify as relevant, consistent with the find- ings in Zhang et al. (2024a). Additionally, the stronger model (i.e., Qwen) tends to select fewer useful documents. Annotation Quality Evaluation.We compare the consistency of annotations by LLMs and humans. Considering human labels as the ground-truth, the precision and recall of the LLM-marked positives for each method are shown in Table 1. It reveals that 1) UtilSel has higher precision and lower recall than RelSel, 2) Qwen is more accurate than Llama in selecting the human positive (precision doubled with some real drop). As we know, there are false negatives in the annotation pool. We also manually checked around 200 LLM annotations and found that LLM-annotated positives are more than actual positives. This means that LLM should be stricter to be more accurate. Qwen has fewer false-positive issues, and its UtilRank has the best overall preci- sion and recall trade-off. Since Qwen has better annotation quality, our experiments in Section 5 are all based on its annotations. 3.3 Training with Utility Annotations Loss Function.Dense retrievers are typically trained to maximize the likelihood of a positive sample d+ compared