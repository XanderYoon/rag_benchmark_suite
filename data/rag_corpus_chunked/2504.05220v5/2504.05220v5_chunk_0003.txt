on four research questions (RQs): (RQ1) What is the optimal training strategy when multi- ple annotated positive samples are available for a query, in terms of data ingestion and retriever op- timization? (RQ2) How do retrievers trained with LLM-annotated utility compare to those trained with human-annotated relevance in both in-domain and out-of-domain retrieval? (RQ3) Can LLM- annotated data enhance retrieval performance when human labels are already available? (RQ4) Do re- trievers trained with utility-focused LLM annota- tions result in better RAG performance compared to those trained with downstream task performance metrics and human annotations in both in-domain and out-of-domain collections? To study the research questions, we employ a state-of-the-art open-source LLM, Qwen-2.5-32B- Int8 (Yang et al., 2024), to annotate the utility of hard negatives in the MS MARCO dataset (Nguyen et al., 2016). In contrast to human annotation on MS MARCO, which has one positive sample per query, Qwen annotates an average of 2.9 positive samples per query. Optimizing the standard joint likelihood of the multiple positives results in sig- nificant performance regression. To address the challenges posed by multiple positives, we intro- duce a novel loss function, SumMargLH, which maximizes their summed marginal likelihood and performs significantly better. For retrieval evalua- tion, we compare retrievers trained with LLM and human annotations on the MS MARCO Dev set and BEIR (Thakur et al., 2021). For RAG evalua- tion, we assess the retrievers on the MS MARCO QA task and two QA tasks with retrieval collections also included in BEIR, i.e., NQ (Kwiatkowski et al., 2019) and HotpotQA (Yang et al., 2018). Our find- ings include: 1) LLM annotations alone result in worse in-domain retrieval performance but better out-of-domain performance compared to human annotations; 2) Combining LLM annotations with 20% of human annotations achieves similar perfor- mance to models trained with