Utility-Focused LLM Annotation for Retrieval and Retrieval-Augmented Generation Hengran Zhang 1,2* Minghao Tang 1,2* Keping Bi1,2† Jiafeng Guo1,2† Shihao Liu3 Daiting Shi3 Dawei Yin3 Xueqi Cheng1,2 1State Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3 Baidu Inc. {zhanghengran22z, tangminghao25s, bikeping, guojiafeng, cxq}@ict.ac.cn, {liushihao02, shidaiting01}@baidu.com, yindawei@acm.org Abstract This paper explores the use of large language models (LLMs) for annotating document utility in training retrieval and retrieval-augmented generation (RAG) systems, aiming to reduce dependence on costly human annotations. We address the gap between retrieval relevance and generative utility by employing LLMs to annotate document utility. To effectively utilize multiple positive samples per query, we introduce a novel loss that maximizes their summed marginal likelihood. Using the Qwen-2.5-32B model, we annotate utility on the MS MARCO dataset and conduct retrieval experiments on MS MARCO and BEIR, as well as RAG experiments on MS MARCO QA, NQ, and HotpotQA. Our results show that LLM-generated annotations enhance out-of-domain retrieval performance and improve RAG outcomes compared to models trained solely on human annotations or downstream QA metrics. Furthermore, combining LLM annotations with just 20% of human labels achieves performance comparable to using full human annotations. Our study offers a comprehensive approach to utilizing LLM annotations for initializing QA systems on new corpora. Our code and data are available at https://github. com/Trustworthy-Information-Access/ Utility-Focused-LLM-Annotation. 1 Introduction Information retrieval (IR) has long been essential for information seeking, and retrieval-augmented generation (RAG) is increasingly recognized as a key strategy for reducing hallucinations in large language models (LLMs) in the modern landscape of information access (Shuster et al., 2021; Za- mani et al., 2022; Ram et al., 2023). Typically, re- trieval models rely on human annotations of query- *Contributed equally †Corresponding authors document relevance for training and evaluation. In