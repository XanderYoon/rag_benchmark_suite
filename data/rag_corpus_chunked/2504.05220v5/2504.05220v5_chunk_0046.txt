1.3 35.4 35.6 30% 1.7 35.1 34.9 40% 2.3 34.7 34.6 50% 3.0 34.6 34.4 UtilSel - 2.9 35.3 34.5 Table 16: Retrieval performance (MRR@10) on MS MARCO Dev using different loss functions across var- ious annotation settings under RetroMAE backbone. “Avg” means the average number of positive instances. F Efficiency and Cost According to Gilardi et al. (2023), the cost of hu- man annotation is approximately $0.09 per annota- tion on MTurk, a crowd-sourcing platform. Each query requires annotations for 31 passages, and there are a total of 491,007 queries, leading to a total human annotation cost of $1,369,910. We uti- lize cloud computing resources, where the cost of using an A800 80GB GPU is assumed to be $0.8 per hour3. Our utility-focused annotation process requires a total of 53 hours on an 8 × A800 GPU machine using the Qwen-2.5-32B-Int8, resulting in 3https://vast.ai/pricing/gpu/A800-PCIE a GPU computing cost of $339. For the REPLUG method, the annotation process takes 70 hours, cost- ing $448 in GPU computing. However, REPLUG requires human-annotated answers for each query, bringing the total to $44,639. More details are pro- vided in Table 17. Although human annotation achieves superior performance on the in-domain dataset, the cost of such annotation is substantial. In contrast, the utility-focused annotation offers the lowest annotation cost, with performance second only to that of human annotation. Annotation Cost($) Time(h) MRR@10 R@1000 Human 1,369,910 - 38.6 98.6 REPLUG 44,639 70+ 33.8 94.7 UtilSel 339 53 35.3 97.7 UtilSel (CL 20%) 274,321 - 38.2 98.5 Table 17: Retrieval performance (%) of different anno- tations on MS MARCO Dev and corresponding annota- tion cost. “R@k” means “Recall@k”. G Prompts for Annotation via LLMs Relevance-based selection, pseudo-answer genera- tion, utility-based selection, and utility-based rank- ing prompts are shown in Figure 4, Figure 5, Figure 6, and