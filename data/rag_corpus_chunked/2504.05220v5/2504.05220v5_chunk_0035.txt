the ground truth answer, enables the retriever to be more effec- tively alignment with the generator. fu(q, d, a) mainly has two ways: directly model how likely the candidate passages can generate the ground truth answer (Shi et al., 2024), i.e., P(a|q, d) , which computes the likelihood of the ground truth an- swer; and measure the divergence of model out- putLLM(q, d)and the answerausing evaluation metrics (Zamani and Bendersky, 2024), e.g., EM, i.e., EM(a, LLM(q, d)) . Given the query q and candidate passage list D= [d 1, d2, ..., dn], where n=|D| . The optimization of the retriever is to minimize the KL divergence between the rel- evance distribution R={s ′(q, di)}N i=1, where s′(q, di) is the relevance s(q, di) from retriever after softmax operation, and utility distribution U={f ′ u(q, di, a)}N i=1, where f ′ u(·) is the utility functionf u(·)from generator after softmax: KL(U||R) = NX i=1 U(d i)log( U(d i) R(di) ).(6) B Additional Analyses of Training Strategies B.1 Impact of Human Annotated Positive When generating LLM annotations, the model re- lies on a pool that includes human-annotated posi- tives and retrieved negatives. To examine whether the presence of human-annotated positives in this pool influences retriever training, we compare three strategies: 1.Random: The default strategy in our main experiments. Positives and negatives of each query are randomly sampled from all LLM annota- tioned positive and negative instances, respectively, without distinguishing human-annotated examples during retriever training. 2.Exclusion: Human-an- notated positives are explicitly excluded during re- triever training. Sepcifically, passages for each query during training are randomly selected from Annotation Top20 Top40 Top60 Top80 Top100 Human (First1LH) 81.9 85.0 86.5 87.0 87.8 UtilSel (First1LH) 81.2 84.5 86.4 87.3 88.2 UtilSel (SumMargLH) 81.6 84.8 86.4 87.2 88.0 Table 9: Retrieval performance (%) of different annotation methods