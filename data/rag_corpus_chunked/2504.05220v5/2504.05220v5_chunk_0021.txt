UtilSel, and SumMargLH. Key findings are: 1) Within the same GPU memory, the quantized version of larger LLMs has better capac- ity than smaller ones (Qwen better than LLama); 2) UtilSel and UtilRank lead to better performance than RelSel, indicating stricter annotation criterion is needed; 3) When multiple positives exist, Sum- MargLH achieves the best performance, indicating its robustness to potential noise introduced by LLM annotations. 4) When integrating human annota- tions, training with higher-quality human annota- tions at last outperforms optimizing towards the union of positives from humans and LLMs. Human Annotation Ratio in CL.Figure 3 shows the retrieval performance of using different ratios of human annotations in CL on the MS MARCO Dev set. It indicates that the in-domain retrieval performance increases with more human-labeled Method/Component Variants MRR@10 R@1000 Human - 38.6 98.6 LLM Annotator Llama-8B 33.0 97.4 Qwen-32B-Int835.3 97.7 Annotation Strategy RelSel 33.597.9 UtilSel 35.3 97.7 UtilRank35.797.8 Training Loss Rand1LH 34.597.9 JointLH 34.0 97.5 SumMargLH35.397.7 +20% Human Labels Positive Union 33.2 97.2 CL38.2 98.5 Table 6: Controlled experiments using LLM annotations for training. See Appendix D.2 for detailed settings. data used in CL. Cutoff Threshold for UtilRank.As illustrated in Figure 3, smaller thresholds result in higher preci- sion while lower recall regarding human-labeled ground truth, and better in-domain retrieval perfor- mance. This again confirms that stricter criteria and fewer positives lead to better in-domain retrieval performance. It is not surprising since this results in a positive-to-negative ratio more closely aligned with the distribution encountered during inference. 0% 20% 40% 60% 80% 100% (a) Human Annotation Ratio in CL 35 36 37 38 39MRR@10 UtilSel UtilRank Human 10% 20% 30% 40% 50% (b) Cutoff Threshold for UtilRank 50 60 70 80Recall & Precision Recall Precision MRR@10 34.50 34.75 35.00 35.25 35.50 35.75 MRR@10 Figure 3: (a): Retrieval performance