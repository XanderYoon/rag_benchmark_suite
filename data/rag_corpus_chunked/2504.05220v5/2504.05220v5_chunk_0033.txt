language models good at utility judgments? In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Infor- mation Retrieval, pages 1941–1951. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Eval- uating text generation with bert.arXiv preprint arXiv:1904.09675. Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao Dong, and Jie Tang. 2024. Longrag: A dual-perspective retrieval-augmented generation paradigm for long-context question an- swering. InProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 22600–22632. A Preliminary A.1 Typical Dense Retrieval Models Dense retrieval models primarily employ a two- tower architecture of pre-trained language models, i.e.,Rq(·) and Rd(·), to encode query and passage into fixed-length dense vectors. The relevance be- tween the queryqand passagediss(q, d), i.e., s(q, d) =f <R q(q),R d(d)>,(5) Annotation Human Test Hybrid Test MRR@10 Recall@1000 DL19 (NDCG@10) DL20 (NDCG@10) MRR@10 NDCG@10 Human 38.6 98.6 68.2 71.6 83.7 63.1 Exclusion(0%) 31.2 − 97.1− 64.6 70.2 84.5 63.3 Exclusion(CL 20%) 37.4 − 98.5 70.5 69.4 84.2 63.0 − Exclusion(CL 30%) 38.2 98.5 69.3 70.4 85.0 64.2 + Random(0%) 35.3 − 97.7− 68.0 71.0 87.5 + 65.8+ Random(CL 20%) 38.2 98.5 69.6 71.4 83.4 65.5 + Inclusion(0%) 36.1 − 98.1− 69.0 71.3 87.7 66.7 + Inclusion(CL 20%) 38.2 98.6 70.9 70.7 84.2 64.6 + Table 7: Retrieval performance (%) with different UtilSel annotation labels on whether human-annotated relevant passage is included or not during training (i.e.,Exclusion,Random,Inclusion) using RetroMAE backbone. “ +” and “−” indicate significant improvements and decrements over Human using a two-sided paired t-test (p <0.05). Dataset Human Random Exclusion Inclusion 0% (CL, 20%) 0% (CL, 20%) (CL, 30%) 0% (CL, 20%) DBPedia 36.0 38.0 37.439.037.3 37.1 38.8 37.0 FiQA 29.7 32.6 32.1 30.132.831.2 32.6 32.3 NQ 49.2 53.5 51.4 52.2 51.0 51.853.751.0 HotpotQA 58.4