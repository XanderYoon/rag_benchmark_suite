2024). This includes metrics such as the likelihood of generat- ing ground-truth answers (Shi et al., 2024) or exact match scores between generated and ground-truth answers (Zamani and Bendersky, 2024). Another approach involves prompting LLMs to select docu- ments with utility from relevance-oriented retrieval results for use in RAG (Zhang et al., 2024a,b). Stud- ies from both approaches have demonstrated im- proved RAG performance. Despite their effectiveness, both approaches have limitations. The first approach requires manu- ally labeled ground-truth answers to assess down- stream task performance, which results in substan- tial QA annotation costs. Additionally, retrievers trained on the performance of a specific task may struggle to generalize to other downstream tasks or even different evaluation metrics within the same arXiv:2504.05220v5 [cs.IR] 9 Oct 2025 task. This issue is exacerbated when dealing with non-factoid questions, where accurate evaluation is challenging, making it less feasible to use QA per- formance as training objectives for retrieval. In con- trast, the second approach, which leverages LLMs to select useful documents for generation (Zhang et al., 2024a,b), does not require human annota- tion and is not confined to specific tasks or metrics. However, the selection is from initially retrieved results and cannot scale to the entire corpus during inference due to prohibitive costs. To address these limitations, this paper proposes using LLMs to annotate document utility for re- triever training, aiming to identify useful docu- ments from the entire collection for RAG. We focus on four research questions (RQs): (RQ1) What is the optimal training strategy when multi- ple annotated positive samples are available for a query, in terms of data ingestion and retriever op- timization? (RQ2) How do retrievers trained with LLM-annotated utility compare to those trained with human-annotated relevance in both in-domain and out-of-domain retrieval? (RQ3) Can LLM- annotated data enhance retrieval