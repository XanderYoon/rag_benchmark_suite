efficacy of our utility-focused annotation pipeline: (a) We constructed annotation candidates using unsupervised (BM25) and two out-of-domain retrievers trained on MS MARCO, i.e., our UtilSel trained on MS MARCO (Retro- MAE backbone) and LLM-QL (Zhang et al., 2025). (b) We annotated candidates via Qwen3-32B (Yang et al., 2025) (a state-of-the-art open-source LLM) to build the training set. We trained retrievers using RetroMAE as the backbone with different annota- tions on NQ, including the original relevance an- notations based on human answers, and our LLM annotations, as shown in Table 9. Following the standard practice for NQ (Karpukhin et al., 2020), we used the First1LH setting (maximizing the like- lihood of the first positive) for the original data, where only the first provided positive passage is used. For our LLM-annotated data, we experi- mented with both First1LH and SumMargLH loss. Our results demonstrate that our utility-focused LLM annotation approach can achieve similar per- formance compared to the original relevance anno- tation based on human-annotated answers, saving considerable manual labeling effort. Retrieval RAG Datasets MS MARCO Dev TREC DL-19 TREC DL-20 MS MARCO-QA NQ HotpotQA #Queries 6980 43 54 6980 2255 7405 #Rel.Passage per query 1.1 95.4 66.8 1.1 1.2 2 #Graded.Retrieval labels 2 4 4 2 2 2 Table 11: Statistics of retrieval and RAG datasets. D Detailed Experimental Settings D.1 Retrieval and RAG Datasets Retrieval Datasets.Three human-annotated test collections are used for in-domain retrieval eval- uation: the MS MARCO Dev set (Nguyen et al., 2016), which comprises 6980 queries, and TREC DL19/DL20 (Craswell et al., 2020, 2021), which include 43 and 54 queries from MS MARCO Dev set. DL19 and DL20 have more human- annotated relevant passages, with each query hav- ing an average of around 95 and 67 positives, re- spectively. We further evaluate the zero-shot per- formance of