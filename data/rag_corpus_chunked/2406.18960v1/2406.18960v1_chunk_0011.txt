que ry rewrites. This adds robustness to dense retrieval as the cen ter of mass of multiple query rewrites will likely correspond better to the user’s information need than a single rewrite would. 4 EXPERIMENTAL SETUP We present the datasets we use in our experimental evaluatio n, in- troduce our baselines, and provide implementation details . 4.1 Dataset & Evaluation Metrics Following previous work [23, 30, 32], we use the QReCC [2] dataset, which contains 14k conversations with 80k question-answer pairs, split into training and test sets (63.5k and 16.4k, respecti vely). The dialogues are based on questions from QuAC [5], TREC CAsT 2019 [6], and Google Natural Questions (NQ) [14], with TREC CAsT appearing only in the test set. Following Ye et al. [32], test instances lacking valid gold passage labels are excluded fr om our analysis. Consequently, our dataset comprises 8,209 test i nstances, distributed as 6,396 for QuAC, 1,442 for NQ, and 371 for TREC- CAsT. For a comprehensive evaluation, we present experimen tal results not only on the overall dataset but also on each subse t. We use mean reciprocal rank (MRR), mean average precision (MAP), and Recall@10 (R@10) as our evaluation metrics. 4.2 Baselines We consider the following baselines for comparison: (1) Manual rewrite: Manually rewritten queries provided by the dataset. (2) T5QR/u1D440/u1D44E/u1D45B/u1D462/u1D44E/u1D459[18]: A strong T5-based [26] QR model. (3) Con- QRR [30]: A T5-based model, optimized for retrieval performanc e using reinforcement learning. (4) ConvGQR [23]: An approach employing two T5-based models: one creates a de-contextual ized query rewrite, the other predicts an answer to the query. The outputs are merged into a single query used for retrieval. (5 ) LLM/u1D44E/u1D451ℎ/u1D45C/u1D450[32]: An LLM query rewrite followed by an LLM query editor in an ad-hoc retrieval pipeline. The authors use Chat GPT 3.5 as their