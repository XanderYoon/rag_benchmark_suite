the user’s previous queries as well as the system’s an - swers to those [35]. Commonly, query rewriting (QR) addresses this by employing neural generative models to produce a sing le de-contextualized query at each conversation turn [9, 18, 3 3], and then feed that query to a retrieval pipeline. While this work s well in many cases, the rewritten query may incorrectly capture t he underlying intent, which leads to the retrieval of non-rele vant an- swers. The challenge arises from the discrete generation pr ocess, which does not accurately capture the underlying probabili ties or importance of terms. In this paper, we seek to improve retrieval performance by ge n- erating multiple queries and modeling the importance of ter ms based on their presence across the queries. We leverage the b eam search algorithm, commonly used in neural QR [10, 19]. Inste ad of keeping track of only the highest-likelihood sequence in a g reedy fashion, the algorithm tracks and considers the best /u1D458sequences at each generation step. We utilize the fact that the token probabilities are already computed in order to produce multiple rewrites a t no additional cost. Thus, the only modiﬁcation we need to make to the original beam search algorithm is to return all tracked sequ ences and their associated probabilities, as opposed to the singl e most probable sequence. The elegance of this method lies in its si mplic- ity; it is computationally inexpensive yet remarkably eﬀec tive. The main research question driving our investigation is: How can we eﬀectively and eﬃciently utilize multiple query rewr ites in conversational passage retrieval? To answer this question, we take into consideration that retrieval can be performed using ei ther sparse or dense retrieval methods. Sparse retrieval typica lly em- ploys pseudo-relevance feedback techniques to expand