ConvGQR [23]: An approach employing two T5-based models: one creates a de-contextual ized query rewrite, the other predicts an answer to the query. The outputs are merged into a single query used for retrieval. (5 ) LLM/u1D44E/u1D451ℎ/u1D45C/u1D450[32]: An LLM query rewrite followed by an LLM query editor in an ad-hoc retrieval pipeline. The authors use Chat GPT 3.5 as their LLM. (6) T5QR/u1D43F/u1D43F/u1D440[32]: A sample of 10k datapoint is taken from the training set and run through the same approach as (4). The outputs are used to train a smaller, distilled model . For each variant of the retrieval pipeline, we use the same T5 - based QR approach, with a beam width of /u1D458= 10, but consider only the top rewrite. Wherever possible, i.e., code/model i s made publicly available, we reproduce results on our system usin g V100 GPUs. Otherwise, we report the numbers from the original pap ers (indicated by † ). 4.3 Implementation Details For QR, we ﬁne-tune a T5 model [26] starting from a t5-base1 checkpoint. We set the beam width to /u1D458= 10 for both single-query and multi-query approaches, as this was found to produce hig h- quality rewrites in [19]. For sparse retrieval, following [2], we employ the Pyserini [17] toolkit and use BM25 for retreival with hyparameters /u1D4581 = 0. 82 and /u1D44F= 0. 68. We generate dense embeddings using a GTR [24] 1https://huggingface.co/t5-base SIGIR ’24, July 14–18, 2024, Washington, DC, USA Ivica Kostri c and Krisztian Balog Table 1: Performance of sparse and dense retrieval with QR me thods. Bold and underlined indicate the best and second-bes t results, respectively. ∗ denotes signiﬁcant improvements with a t-test at p < 0.05 of C MQR over its single-query counterpart. Method QReCC QuAC NQ TREC-CAsT MRR MAP R@10 MRR MAP R@10 MRR MAP