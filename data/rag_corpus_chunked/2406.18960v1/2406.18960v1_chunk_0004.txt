The novelty of our approach is twofold: (1) it generates multiple query rewrites at no extra cost compar ed to current neural QR approaches, (2) it eﬀectively utilizes th e gener- ated rewrites in both sparse and dense retrieval. Using the Q ReCC dataset for evaluation, we show that applying our method on t op of any pipeline featuring generative QR improves performance, re- sulting in state-of-the-art results. All resources developed for this paper (source code, query rewrites, and rankings) can be found at https://github.com/iai-group/sigir2024-multi-query- rewriting. 2 RELATED WORK We focus on the task of conversational passage retrieval, where the goal is to retrieve relevant passages to the user query from a large passage collection. Unlike generative approaches, here, h allucina- tions are ensured not to occur as answers can only come from th e collection. While there is some variety between retrieval p ipeline architectures for conversational search, the vast majorit y include QR, followed by a ﬁrst-pass candidate selection stage and th en by one or more re-ranking steps [13, 15, 19, 29, 33]. This setup p ro- vides a good balance between eﬃciency and eﬀectiveness [3]. We demonstrate the beneﬁts of our approach to ﬁrst-pass retrie val, us- ing both spare and dense retrieval methods. Automatic QR has a long tradition in IR, predominantly in query expansion, and has been shown eﬀective in a range of tasks [4]. Conversational query rewriting (CQR) aims to gen er- ate clear, de-contextualized queries from raw inputs by con sider- ing conversation context and addressing coreferences, ell ipsis [6], and topic transitions [29]. Crucially, it helps clarify and reﬁne the user’s needs in a dialogue setting [25]. Recently, neural re writ- ing methods leveraging large pre-trained language models, like GPT-2 [13, 28] and T5 [19], have become prevalent. While neur al