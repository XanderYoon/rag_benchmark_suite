interpolate diﬀerent queries extracted from co nversa- tional context instead of retrieved documents and do not ass ign a pre-determined portion of the total weight mass to the orig inal query terms. Importantly, our method is seen as complementa ry to relevance feedback and can be combined with it. 3.3 Dense Retrieval Dense retrieval diﬀers from sparse retrieval in that it aims to com- pute a relevance score based on the similarity between queries and documents represented in a continuous embedding space inst ead of matching on exact terms. In the simplest form, this score c an be a dot product of the query and the document embedding vec- tors: /u1D460/u1D450/u1D45C/u1D45F/u1D452( /u1D45E, /u1D451) = ℎ/u1D45E · ℎ/u1D451 , where ℎ/u1D45E and ℎ/u1D451 are the learned query and document embedding vectors, respectively. We note that the learned embedding vectors can be pre-computed and stored fo r all documents in the collection, requiring only the computation of the query embedding vector at retrieval time. Given /u1D45Brewrites with associated weights, we ﬁrst generate em- beddings for all rewrites separately and scale them accordi ng to the associated weights. Then, the scaled embeddings are sum med up into a single vector ( ℎ/u1D45E/u1D456) that can be used in a regular dense retrieval system. Formally, the query representation at turn /u1D456is ob- tained by: ℎ/u1D45E/u1D456= /u1D45B/summationdisplay.1 /u1D457=1 /u1D452/u1D45B/u1D450/u1D45C/u1D451/u1D452/u1D45E ( ˆ/u1D45E/u1D457 /u1D456) /u1D445/u1D446( ˆ/u1D45E/u1D457 /u1D456) . Essentially, the ﬁnal query is a weighted centroid of the que ry rewrites. This adds robustness to dense retrieval as the cen ter of mass of multiple query rewrites will likely correspond better to the user’s information need than a single rewrite would. 4 EXPERIMENTAL SETUP We present the datasets we use in our experimental evaluatio n, in- troduce our baselines, and provide implementation details . 4.1 Dataset & Evaluation