clear, de-contextualized queries from raw inputs by con sider- ing conversation context and addressing coreferences, ell ipsis [6], and topic transitions [29]. Crucially, it helps clarify and reﬁne the user’s needs in a dialogue setting [25]. Recently, neural re writ- ing methods leveraging large pre-trained language models, like GPT-2 [13, 28] and T5 [19], have become prevalent. While neur al rewriting methods tend to outperform traditional query expansion techniques [7, 8], the best results are achieved by combinin g the two [13, 19, 28]. Two previous CQR studies are particularly relevant to our work. Lin et al. [19] propose two query reformulation methods: one fo- cused on term importance and another on making human-like queries. They show that fusing ranked lists after separate r etrieval stages for both queries increases recall. However, fusing t he two lists after re-ranking showed no improvement. The main diﬀerence between this work and ours is that we generate multiple natur al language query rewrites. Mo et al. [23] presents two neural m od- els: one trained on rewriting queries and another to producepoten- tial answers to the query, the idea being that pre-trained la nguage models can directly answer questions by leveraging their in ternal knowledge. At inference, these potential answers are used t o ex- pand the query. Our approach diﬀers in that we use a single mod el to estimate term importance and pick expansion terms. In another line of recent research, deep neural networks are used to generate query embeddings directly from context [21 , 22, 34]. These embeddings, used in conjunction with dense re - trieval, can handle intricate conversational contexts mor e eﬀec- tively. While they integrate seamlessly with advanced neural mod- els for IR, they require systems capable of interpreting the m, po- tentially demanding more computational resources