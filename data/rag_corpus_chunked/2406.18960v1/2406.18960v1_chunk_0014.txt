46.80 67.6 3 31.51 30.88 52.96 LLM/u1D44E/u1D451ℎ/u1D45C/u1D450† 44.99 43.19 67.34 45.21 43.48 68.30 47.64 45.20 67.27 30.91 3 0.48 51.03 T5QR/u1D43F/u1D43F/u1D440 42.46 40.67 64.47 42.78 41.06 65.61 44.78 42.29 63.48 28.02 2 7.55 48.65 CMQR(T5QR/u1D440/u1D44E/u1D45B/u1D462/u1D44E/u1D459) 40.53 ∗ 38.73∗ 63.15∗ 38.72∗ 37.02∗ 62.12∗ 48.50∗ 46.01∗ 67.67∗ 40.68∗ 39.91∗ 63.21∗ CMQR(ConvGQR) 45.82 ∗ 43.96∗ 69.75∗ 45.00∗ 43.20∗ 70.00∗ 51.95∗ 49.50∗ 71.17∗ 36.11∗ 35.50∗ 59.97∗ CMQR(T5QR/u1D43F/u1D43F/u1D440) 45.98∗ 44.17∗ 69.31∗ 45.82∗ 44.08∗ 70.02∗ 49.58∗ 47.14∗ 69.18∗ 34.69∗ 34.09∗ 57.64∗ model from a publicly available checkpoint. 2 The implementation is based on Faiss [12]. 5 RESULTS The evaluation results on the QReCC test set along with a brea k- down of speciﬁc subsets are reported in Table 1. The table is s plit into two groups: Sparse (Top) and dense retrieval (Bottom), with the same query rewriting methods in each group. When multi- ple queries are considered for a given method, it is indicate d by CMQR(*); in all our experiments, we consider 10 query rewrit es. Our main ﬁndings are as follows. First, the CMQR method con- sistently outperforms both its sparse and dense retrieval c ounter- parts across all datasets. This trend highlights CMQR’s eﬀe ctive- ness in improving retrieval metrics. We show signiﬁcant imp rove- ments in the range from 1.06 to 6.31 in sparse retrieval and 3. 52 to 4.45 in dense retrieval in terms of MRR (absolute percenta ge points). The biggest improvement is observed when adding CMQR to the weakest model (T5QR /u1D440/u1D44E/u1D45B/u1D462/u1D44E/u1D459), suggesting that the term im- portance and query expansion methods have a major eﬀect. Thi s observation is further supported by the smallest gain obser ved with ConvGQR, which has integrated query expansion. Second , for both the sparse and dense retrieval groups, our CMQR meth od consistently outperforms all other methods on the overall Q