inexpensive yet remarkably eﬀec tive. The main research question driving our investigation is: How can we eﬀectively and eﬃciently utilize multiple query rewr ites in conversational passage retrieval? To answer this question, we take into consideration that retrieval can be performed using ei ther sparse or dense retrieval methods. Sparse retrieval typica lly em- ploys pseudo-relevance feedback techniques to expand the q uery and bridge the vocabulary gap. Our method eﬀectively perfor ms both term-importance estimation and query expansion to rep re- sent the underlying information need better and improves MR R by 1.06–6.31 percentage points compared to using a single qu ery rewrite. Dense retrieval, based on contextual neural language mod- els, works better with natural language queries (in contras t to bag-of-words models of sparse retrieval). However, it is co mputa- tionally expensive and would scale linearly with the number of rewrites, rendering it impractical. Instead, we represent all query rewrites jointly by merging them into a single vector repres enta- tion in the learned embedding space by weighted average pool ing. Our method outperforms a single-query retrieval by 3.52–4. 45 per- centage points in absolut MRR score. In summary, the main contribution of this paper is a conversa - tional multi-query rewriting method, CMQR, that can be utilized in conversational passage retrieval and applied on top of any pipeline SIGIR ’24, July 14–18, 2024, Washington, DC, USA Ivica Kostri c and Krisztian Balog that uses generative QR. The novelty of our approach is twofold: (1) it generates multiple query rewrites at no extra cost compar ed to current neural QR approaches, (2) it eﬀectively utilizes th e gener- ated rewrites in both sparse and dense retrieval. Using the Q ReCC dataset for evaluation, we show that applying our method on t op of any pipeline featuring generative