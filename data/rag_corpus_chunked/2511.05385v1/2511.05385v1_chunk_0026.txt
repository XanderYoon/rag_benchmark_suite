to further improve the model while preventing overthinking. or consider complex rewards in the PPO framework [66], these approaches make it difficult to achieve efficient, low-resource training of agentic RAG. Therefore, to control the number of reasoning steps ùëò in Pùëò and enable efficient training, we propose a two-stage training paradigm that improves the efficiency and conciseness of reasoning by introducing process-level rewards. The overall training framework is shown in Fig. 5. In the first stage, SFT trains the model on basic reasoning patterns. In the second stage, IP-DPO is applied to further enhance the model‚Äôs generalization. 4.3.1 First Stage: SFT .To build reasoning SFT data, we use the Musique dataset [ 58], which provides structured query decomposition and supporting golden evidence paragraphs. Each reasoning step is constructed as follows: (1) Knowledge extraction:We first process the golden evidence paragraphs of the current subquery with Qwen2.5-14B-Instruct to extract a set of knowledge triples, which are aligned with our knowledge graph construction. From this set, Qwen2.5-72B-Instruct then identifies the supporting triplets that directly answer each intermediate subquery. (2)Important entity recognition:We view the entities in supporting triplets as anchor entities. (3) Subquery generation:We employ Qwen2.5-72B-Instruct to transform the structured intermedi- ate questions from Musique into fluent, natural language subqueries. (4) Context simulation:To improve the model‚Äôs ability to locate supporting facts, we synthetically construct a context for training. We create this context by taking the golden evidence paragraphs and randomly inserting the evidence triplets. This process helps the LLM learn to adapt to diverse and imperfect contexts. (5) Summary formulation:We use diverse templates to construct target summaries. Every summary follows a two-part structure. It first presents the key supporting triplets and then concludes with the answer to the subquery. , V ol. 1, No. 1, Article . Publication date: November 2018. TeaRAG : A