steer the final importance distribution. To ensure the relevance of the context to the subquery, the values for the subquery node ğ‘›ğ‘ğ‘– in the personalized vector ğ‘ are set to 1. Meanwhile, the key entities identified by the LLM have their corresponding values in ğ‘ set to 0.5, serving as anchors for the question. Subsequently, we iterate for ğ‘ rounds to obtain the updated importance distributionğœ‹: ğœ‹=ğ›¼ğ‘Š ğœ‹+ (1âˆ’ğ›¼)ğ‘,(3) where ğ›¼ is a hyperparameter that balances the importance derived from the graph structure with the bias from the personalization vector. Finally, based on the ranking of the scores inğœ‹, we select the ğ‘˜ğ‘“ highest-scoring nodes from the chunk and triplet nodes to form the final context Cğ‘ğ‘– . This context is then wrapped in <Reference> and </Reference> tags to signal to the LLM that it is an external resource. We show a KAG example in Fig. 4 (c). Core relevant knowledge can form a dense graph structure connected by co-occurrence edges. This indicates that using co-occurrence is usually a more precise way to filter noise and prevent the modelâ€™s inference from being distracted by similar but irrelevant content. 4.2.6 Summary Generation.The LLM summarizes the collected external context to produce a summary,ğ‘  ğ‘–, which enables subsequent steps to easily identify the key information for this step. 4.2.7 Final Answer Generation.The LLM autonomously determines whether the information collected in the reasoning path is sufficient. If not, it continues to generate the next reasoning step ğ‘†ğ‘–+1 =LLM(ğ¼, ğ‘,P ğ‘– ). Conversely, when the information is sufficient, the LLM generates the final answer ğ‘=LLM(ğ¼, ğ‘,P ğ‘– ). And the final answer ğ‘ starts with the format â€œFinal answer: â€, which facilitates the format check. 4.3 Model Training To enable LLMs to autonomously execute the aforementioned pipeline, current advanced agentic RAG mainly adopts outcome-based RL