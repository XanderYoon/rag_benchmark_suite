round of optimization leverages the outputs from the model of the preceding round. This enables the model to sequentially master and refine its reasoning paradigms. (2) The performance gains from IP-DPO exhibit diminishing returns with additional rounds. This occurs because the model’s reasoning capabilities begin to converge, establishing a stable inference pattern. After many rounds of iteration, the model consistently produces correct responses for many questions, causing the preference data to provide a diminishing learning signal and thus limiting further improvement. (3) IP-DPO demonstrates consistent and stable improvements across models of varying scales and families.This consistent success demonstrates IP-DPO’s broad applicability for enhancing reasoning across diverse LLMs. 5.5 Hyper-Parameter Analysis 1 2 3 4 5 Number of Input Content per Retrieval 46 48 50 52 54 56 58Score (a) Single-hop QA 1 2 3 4 5 Number of Input Content per Retrieval 35 40 45 50Score (b) Multi-hop QA T eaRAG-8B (F1) Search-R1-base-7B+R (F1) T eaRAG-8B (EM) Search-R1-base-7B+R (EM) 1 2 3 4 5 Number of Input Content per Retrieval 40.0 42.5 45.0 47.5 50.0 52.5 55.0Score (c) Overall QA Fig. 10. Performance of TeaRAG-8B and Search-R1-base-7B+R across varying numbers of input content per retrieval. 5.5.1 Number of Input Contents per Retrieval.In this section, we investigate the impact of the number of input contents per retrieval on method performance. We compare the EM and F1 scores of TeaRAG-8B and Search-R1-base-7B+R, as well as TeaRAG-14B and Search-R1-base-14B+R. The experimental results are shown in Fig. 10 and Fig. 11. We have the following observations: , V ol. 1, No. 1, Article . Publication date: November 2018. TeaRAG : A Token-Efficient Agentic Retrieval-Augmented Generation Framework 25 1 2 3 4 5 Number of Input Content per Retrieval 48 50 52 54 56 58Score (a) Single-hop QA 1 2 3 4 5 Number of