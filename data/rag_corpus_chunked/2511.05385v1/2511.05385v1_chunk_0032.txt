may indicate inefficiencies such as overthinking or repetitive searches. To promote conciseness and , V ol. 1, No. 1, Article . Publication date: November 2018. TeaRAG : A Token-Efficient Agentic Retrieval-Augmented Generation Framework 15 penalize inefficiency, we normalize the summed memory scores by the number of steps,ğ‘˜. The final rewards are calculated as follows: ğ‘Ÿğ‘ (Pğ‘˜ )= Ã Mğ‘ ğ‘˜ , ğ‘Ÿ ğ‘ (Pğ‘˜ )= Ã Mğ‘ ğ‘˜ , ğ‘Ÿ ğ‘  (Pğ‘˜ )= Ã Mğ‘  ğ‘˜ ,(12) where ğ‘Ÿğ‘ (Pğ‘˜ ), ğ‘Ÿğ‘ (Pğ‘˜ ), and ğ‘Ÿğ‘  (Pğ‘˜ ) are the process rewards for subquery generation, the retrieved context, and the summary, respectively. Based on these rewards, the overall process reward is as follows: ğ‘Ÿğ‘ğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘  (Pğ‘˜ )=0.1ğ‘Ÿ consistency(Pğ‘˜ ) +0.3ğ‘Ÿ ğ‘ (Pğ‘˜ ) +0.3ğ‘Ÿ ğ‘ (Pğ‘˜ ) +0.3ğ‘Ÿ ğ‘  (Pğ‘˜ ).(13) DPO pair construction.Based on the above reward design, each reasoning path can obtain three rewards: ğ‘Ÿ ğ‘“ ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ , ğ‘Ÿğ‘œğ‘¢ğ‘¡ğ‘ğ‘œğ‘šğ‘’ , and ğ‘Ÿğ‘ğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘  . For the DPO algorithm [ 49], the key is how to construct positive-negative pairs to learn preferences. To this end, we design the following pair construction algorithm. First, we select the chosen response set, which exhibits a more accurate reasoning process. Our approach varies based on the question type. For single-hop questions, which are typically simple and whose answers can be found with a single retrieval, we do not apply a process reward. This is also because our single-hop dataset lacks golden evidence. Instead, we directly scale the outcome reward by dividing it by the length of the reasoning path ğ‘˜, setting ğ‘Ÿğ‘œğ‘¢ğ‘¡ğ‘ğ‘œğ‘šğ‘’ = ğ‘Ÿğ‘œğ‘¢ğ‘¡ğ‘ğ‘œğ‘šğ‘’ ğ‘˜ and ğ‘Ÿğ‘ğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘  =0 . This approach incentivizes the LLM to solve single-hop questions in a single step. For more complex multi-hop questions, we utilize the process reward to differentiate and evaluate the quality of the reasoning steps. When considering format