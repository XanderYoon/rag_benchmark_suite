weighting parameter ùúÇ is tuned within the range [0.25,0.5,1] and is progressively reduced as DPO iterations increase to prevent overfitting. Besides, the learning rate is set to 1√ó10 ‚àí4, the overall batch size is 16, the per-device batch size is 2, and the weight decay is 0.001. In both training stages, we adopt LoRA [19] for parameter-efficient fine-tuning. We set the LoRA rank to 8, ùõº to 32, and dropout to 0.1, and apply it to the q_proj, k_proj, v_proj, and o_projlayers. 5.2 Overall Performance The overall performance of the model is shown in Table 6. We have the following observations: (1) Our TeaRAG outperforms existing agentic RAG methods on most datasets and achieves the best average performance across all datasets.This demonstrates the effectiveness of TeaRAG, which successfully boosts the performance of LLMs on RAG tasks by constructing a KAG-based agentic workflow combined with process supervision, outperforming previous outcome-based agentic RAG approaches. (2) The two-stage training paradigm significantly boosts the performance of TeaRAG.In the first training stage, SFT yields substantial performance improvements for TeaRAG-SFT, surpassing both the prompt-based agentic RAG approach, IRCOT, and the SFT-trained agentic RAG system, SelfRAG. Based on TeaRAG-SFT, the second stage, IP-DPO, delivers a pronounced enhancement. Specifically, IP-DPO achieves a relative gain in average EM score of38.57% on Llama3-8B-Instruct and 23.43% on Qwen2.5-14B-Instruct. These improvements highlight the effectiveness of IP-DPO in strengthening the model‚Äôs generalization capability. (3) TeaRAG demonstrates strong out-of-domain generalizability.Across both single-hop and multi-hop out-of-domain datasets, including PopQA, 2WikiMultiHopQA, and Bamboogle, our method delivers excellent performance. On the 2WikiMultiHopQA dataset, using the Llama3-8B- Instruct base model, TeaRAG-8B not only surpasses Search-R1-base-7B+R in the out-of-domain test by a significant margin but also achieves results on par with those of R1-Searcher-Qwen- 7B+R in the in-domain evaluation. , V ol. 1, No. 1, Article . Publication date: