retrieve relevant and concise knowledge triplets. Afterwards, based on the recalled chunks and triplet information, we construct a Knowledge Association Graph (KAG) using semantic similarity and co-occurrence. Using Personalized PageRank (PPR) on this graph, we filter out redundant and irrelevant chunk information while supplementing it with the higher-density triplet information. Finally, the LLM summarizes the retrieved content. The LLM repeats this process cyclically until the final answer is obtained. To effectively implement this pipeline and improve the conciseness of LLM reasoning, we propose a two-stage training paradigm. In the first stage, we construct supervised fine-tuning (SFT) data to train models to master the reasoning format and thinking process. Specifically, we leverage the query decomposition processes from the MuSiQue [58] dataset and employ Qwen2.5-72B-Instruct [74] to transform structured question-answer pairs into natural language equivalents. We then assemble these into complete reasoning processes following chain combinations. Using this SFT dataset, we perform SFT on the models. In the second stage, we propose Iterative Process-aware Direct Preference Optimization (IP-DPO), which incorporates a novel process reward and iteratively optimizes LLMs. Specifically, we sample multiple reasoning paths for each query, assign rewards to them, and construct preference-pair datasets based on these rewards. For a given sampled reasoning path, the reward system integrates conventional output-based and format-based rewards with the proposed process reward to produce a comprehensive evaluation score. The process reward employs a knowledge matching mechanism to assess evidence acquisition across three dimensions: subquery generation, context retrieval, and summarization. This mechanism measures the alignment of intermediate reasoning outputs in each dimension with ground truth knowledge. Then, the process reward is computed by aggregating evidence acquisition scores from each dimension and normalizing by the total reasoning steps, resulting in the information gain per step. Besides, the process reward also ensures consistency between extracted entities and subqueries