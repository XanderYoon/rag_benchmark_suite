shown in Fig. 10 and Fig. 11. We have the following observations: , V ol. 1, No. 1, Article . Publication date: November 2018. TeaRAG : A Token-Efficient Agentic Retrieval-Augmented Generation Framework 25 1 2 3 4 5 Number of Input Content per Retrieval 48 50 52 54 56 58Score (a) Single-hop QA 1 2 3 4 5 Number of Input Content per Retrieval 40 45 50 55Score (b) Multi-hop QA T eaRAG-14B (F1) Search-R1-base-14B+R (F1) T eaRAG-14B (EM) Search-R1-base-14B+R (EM) 1 2 3 4 5 Number of Input Content per Retrieval 42.5 45.0 47.5 50.0 52.5 55.0 57.5Score (c) Overall QA Fig. 11. Performance of TeaRAG-14B and Search-R1-base-14B+R across varying numbers of input content per retrieval. (1) The more contents are input per retrieval, the better the model performance, and TeaRAG generally outperforms the baseline.This indicates that TeaRAG is robust to variations in the number of input contents per retrieval and can maintain strong performance across different settings. (2) When the number of input contents per retrieval is small, TeaRAG cannot fully leverage the capabilities of LLMs. When the number is larger (≥3 ), it can achieve outstanding results. This is mainly because TeaRAG ’s training leverages PPR based on the co-occurrence mechanism. LLMs tend to focus on co-occurring data features, and when the number of input contents per retrieval is small, co-occurrence is difficult to achieve. This leads to a mismatch between inference and training, resulting in input distribution drift. When the number is sufficiently large, the information filtered by PPR has a higher information density, and at the same time, the LLM can exploit the co-occurrence in the data to achieve better performance. (3) TeaRAG is more scalable, with a more significant performance boost as the number of input contents per retrieval increases.This is due to the