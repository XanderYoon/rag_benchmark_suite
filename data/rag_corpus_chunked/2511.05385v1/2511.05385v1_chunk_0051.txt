54.5 EM F1 Fig. 8. Performance of TeaRAG with Llama3-8B-Instruct as the LLM changes across IP-DPO iterations. The left and middle figures show the EM and F1 scores of TeaRAG on six specific datasets, respectively. The right figure displays the average EM and F1 scores of TeaRAG across the six datasets. NQ PopQAHotpotQA 2Wiki Musique Bamboogle EM Performance 10 30 50 T eaRAG-SFT T eaRAG-1 T eaRAG-2 T eaRAG-3 NQ PopQAHotpotQA 2Wiki Musique Bamboogle F1 Performance 10 30 50 T eaRAG- SFT T eaRAG-1T eaRAG-2T eaRAG-3 Models 40 45 50 55 60Avg. 38.4 47.4 46.5 55.9 47.0 56.3 47.4 57.1 EM F1 Fig. 9. Performance of TeaRAG with Qwen2.5-14B-Instruct as the LLM changes across IP-DPO iterations. The left and middle figures show the EM and F1 scores of TeaRAG on six specific datasets, respectively. The right figure displays the average EM and F1 scores of TeaRAG across the six datasets. approach on Llama3-8B-Instruct, considering three distinct schemes: using onlyOutcomereward, usingOutcome-Formatreward, and usingOutcome-Format-Processreward. We train models with each reward type for multiple rounds. The results are shown in Table 8. We observe the following phenomena: (1) Process rewards effectively enhance model performance.By providing rewards for each intermediate step of the reasoning path, the LLM can learn from high-quality reasoning examples. (2) Process rewards reduce reasoning steps and increase reasoning efficiency.The reward mechanism penalizes unnecessary reasoning steps that are not instrumental to problem-solving. In contrast, models trained without process rewards exhibit a tendency to gradually increase their reasoning step count. This phenomenon is attributed to the lack of intermediate supervision, which can reinforce redundant reasoning paths that ultimately lead to the correct result. (3) Process rewards make training more stable.With only outcome rewards, performance col- lapses by the second training round. The model learns to favor reasoning paths that reach the ,