system from leveraging both retrieval methodsâ€™ strengths [16] and hinders retrieval systems in preserving critical information and removing irrelevant noise. Finally, most agentic RAG systems train LLMs with outcome-based rewards via RL [27, 54]. This reward design is typically sparse and noisy, which can hinder stable and efficient training [66]. Besides, agentic RAG methods [53, 66] are usually trained using RL algorithms such as PPO [51] and GRPO [52]. These approaches require multi-machine coordination and continuous external retrieval access during training [56], which makes the training process prohibitively slow. To address this limitation, recent approaches employ GPT -4o for process -based reward annotation [ 83], and then optimize using DPO [49]. However, the reward annotation process is costly and depends on proprietary models. To gain deeper insights into the token usage of existing agentic RAG systems, we analyze two representative methods: Search-R1 [27] and R1-Searcher [54]. As illustrated in Fig. 1 (a), the output of an agentic RAG system primarily consists of two types of tokens generated through multiple iterations. The first is the LLMâ€™s thinking process, where these tokens are used for planning, problem , V ol. 1, No. 1, Article . Publication date: November 2018. TeaRAG : A Token-Efficient Agentic Retrieval-Augmented Generation Framework 3 Instruction T!R!â€¦T"R"O (a) Search-R1 (b) TeaRAG OutcomeReward Query Semantic RetrievalSearchSearchChunksChunks Token IntensiveLarge ğ’ Instruction T!R!â€¦T#R#O Overall RewardQuery Hybrid RetrievalSearchSearchContentContent Token EfficientSmall ğ’ T!R!â€¦T"R"OKnowledge Association Graph querychunktripletentity High DensityContent Input Output Input Output OutputRewardFormatRewardProcessReward Fig. 1. ğ‘‡ğ‘– denotes the thinking tokens at the i-th step, ğ‘…ğ‘– denotes the retrieved context at the i-th step, and ğ‘‚ represents the final output. (a) illustrates Search-R1, a representative agentic RAG method optimized based on the final outcome. (b) shows our proposed method TeaRAG, which achieves a token-efficient agentic RAG by optimizing the retrieved content length with high-density triplets