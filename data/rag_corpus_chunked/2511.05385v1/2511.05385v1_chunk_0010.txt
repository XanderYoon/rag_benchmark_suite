planning and comprehension capabilities of LLMs, it iteratively breaks down complex problems into executable operations and queries until an answer is found. Initial works use prompt-based approaches for preset process control [36, 59, 81], determining whether RAG should continue querying or provide an answer to the question. However, due to the lack of training specifically for RAG scenarios, these methods rely on LLMs’ instruction- following and reasoning capabilities. To enhance the adaptability of LLMs to RAG tasks, some studies [2, 40] employ SFT to train agentic RAG systems. However, such approaches often fall short in further stimulating the generalization capability of the models. Recently, as RL methods have greatly enhanced the reasoning and generalization capabilities of LLMs [52], numerous studies have employed outcome-based RL to stimulate LLMs’ thinking and tool-use abilities [ 27, 54]. These works typically use string matching methods to simply determine whether the LLM’s output is consistent with the ground truth [25, 27, 54] or whether the LLM’s reasoning process contains the ground truth [53]. Nevertheless, such approaches often suffer from suboptimal reasoning processes due to the lack of supervision over intermediate steps [66]. Some recent work has explored process- reward methods to more precisely optimize agentic RAG systems [66, 83]. These approaches include employing predefined reasoning format rewards to supervise the model in learning correct reasoning formats [26, 55] and designing reward mechanisms based on the number of retrieval calls [55, 77]. Further improvements involve leveraging more comprehensive evidence paragraphs to supervise the reasoning process [66], or utilizing GPT-4o to annotate intermediate reasoning steps [83]. Although current agentic RAG has made significant progress, existing methods focus on training LLMs on how to invoke retrievers for better performance, often neglecting token efficiency. Besides, these approaches either rely on high-resource RL methods such as PPO [51] and GRPO