been proposed. Agentic RAG adopts agentic workflows that allow LLMs to autonomously control the workflow process to solve complex problems [40, 54]. By leveraging the capabilities of LLMs in task decomposition and dynamic planning, agentic RAG can break down problems into a sequence of tractable steps. At each step, the framework proactively invokes retrieval modules to obtain relevant contextual information, thereby establishing an integration between reasoning and retrieval [27, 59]. Recently, the performance of agentic RAG has been significantly enhanced through reinforcement learning (RL) optimization of agentic workflows for LLMs [53, 54, 56]. For example, Search-R1 [27] employs PPO [ 51] and GRPO [ 52] to optimize agentic RAG based on an output-based reward. However, the current paradigm still faces several challenges. First, existing agentic RAG methods are token-inefficient. This is because current systems predominantly prioritize maximizing the accuracy of the final outputs, while overlooking the substantial token overhead during the reasoning and retrieval processes [27, 53]. This optimization bias often leads models to overthink [7, 8] and perform redundant retrieval [66], resulting in wasted computational resources and reduced system efficiency. Second, current agentic RAG systems largely rely on chunk-based semantic retrieval, which yields low information density and tends to introduce irrelevant noise [40, 79]. While recent studies [39, 77] have investigated the use of higher-density knowledge graphs for retrieval, it remains unvalidated on large-scale graphs and fails to leverage co-occurrence relationships among chunks and knowledge triplets. This prevents the system from leveraging both retrieval methodsâ€™ strengths [16] and hinders retrieval systems in preserving critical information and removing irrelevant noise. Finally, most agentic RAG systems train LLMs with outcome-based rewards via RL [27, 54]. This reward design is typically sparse and noisy, which can hinder stable and efficient training [66]. Besides, agentic RAG methods [53, 66] are usually trained using