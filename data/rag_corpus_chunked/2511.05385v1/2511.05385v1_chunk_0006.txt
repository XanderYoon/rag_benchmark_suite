of these contents may consist of irrelevant or redundant background information that does not enhance the quality of the final answer [ 40]. Therefore, increasing the information density of the retrieved content is essential for improving the token efficiency in agentic RAG. Second, agentic RAG methods generally adopt multi-step reasoning, even when addressing single-hop questions. As shown in Fig. 2 (b), single-hop questions account for 44% of the test set, yet in most cases the number of reasoning steps exceeds one. This is due to the lack of process supervision in outcome-based rewards, which leads to overthinking [7] and redundant retrieval [62]. Therefore, reducing unnecessary and repetitive steps in intermediate processes is key to improving the efficiency of token utilization. , V ol. 1, No. 1, Article . Publication date: November 2018. 4 Zhang et al. To address the above challenges, we introduce TeaRAG, a token-efficient agentic RAG framework that enhances token efficiency by simultaneously optimizing the conciseness of reasoning steps and the density of retrieved content, as shown in Fig. 1 (b). Specifically, the overall workflow of this agentic RAG system is autonomously controlled by an LLM. First, the LLM identifies key entities in the question and breaks it down into sub-questions around these key entities. Subsequently, based on the sub-questions, the LLM calls a retriever to perform semantic retrieval at the chunk level. To improve the density of the retrieved content, the LLM also uses graph retrieval to retrieve relevant and concise knowledge triplets. Afterwards, based on the recalled chunks and triplet information, we construct a Knowledge Association Graph (KAG) using semantic similarity and co-occurrence. Using Personalized PageRank (PPR) on this graph, we filter out redundant and irrelevant chunk information while supplementing it with the higher-density triplet information. Finally, the LLM summarizes the retrieved content. The LLM repeats