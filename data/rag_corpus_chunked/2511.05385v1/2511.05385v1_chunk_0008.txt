context retrieval, and summarization. This mechanism measures the alignment of intermediate reasoning outputs in each dimension with ground truth knowledge. Then, the process reward is computed by aggregating evidence acquisition scores from each dimension and normalizing by the total reasoning steps, resulting in the information gain per step. Besides, the process reward also ensures consistency between extracted entities and subqueries to facilitate the PPR filtering. Based on this reward framework, we can create high-quality preference-pair datasets, enabling iterative DPO to improve model generalization and yield more compact reasoning paths. Comprehensive evaluations on six benchmark datasets demonstrate that TeaRAG attains superior accuracy while reducing both reasoning steps and output length as shown in Fig. 2. On Llama3 -8B-Instruct, it improves the average Exact Match (EM) score by 4% alongside a 61% reduction in output tokens, whereas on Qwen2.5-14B-Instruct it yields a2%EM increase with a59%token reduction. Our contributions can be summarized as follows: • We conduct an in-depth analysis of token inefficiencies in agentic RAG and propose a token- efficient pipeline, TeaRAG, that simultaneously improves the information density of retrieved contents and reduces reasoning steps. • We propose a retrieval method that constructs KAGs based on semantic similarity and co- occurrence, combining the strengths of semantic and graph retrieval. By employing PPR filtering to remove irrelevant contents, our approach retrieves more concise contents without sacrificing performance. • We propose a two-stage training paradigm that effectively activates agentic RAG reasoning capabilities while maintaining the conciseness of the reasoning process through process-aware training. , V ol. 1, No. 1, Article . Publication date: November 2018. TeaRAG : A Token-Efficient Agentic Retrieval-Augmented Generation Framework 5 • We conduct comprehensive experiments and detailed analyses across six benchmark datasets, validating the effectiveness and token efficiency of our pipeline and training methods. 2 Related Work 2.1 Retrieval-Augmented Generation