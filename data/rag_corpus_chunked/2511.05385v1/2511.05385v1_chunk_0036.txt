andPopQA[ 44]. For multi-hop QA datasets, we useHotpotQA[ 75], 2WikiMultiHopQA[ 18],Musique[ 58], andBamboogle[ 48]. For datasets without a test set, we use the development set for testing. The statistical details of these datasets are shown in Table 3. Since we use some training data from NQ, HotpotQA, and Musique, the tests on NQ, HotpotQA, and Musique datasets are in-domain tests, and the tests on PopQA, 2WikiMultiHopQA, and Bamboogle are out-of-domain tests. For the retrieval corpus, following [ 28], we adopt the Wikipedia corpus dated December 20, 2018 [30], as the chunk corpus. The statistics of the chunk corpus are shown in Table 4. Based on the knowledge graph construction method described in Section 4.1, we constructed a large-scale knowledge graph, and the statistical information of this knowledge graph is shown in Table 5. For all datasets, we adopt EM and F1 as the evaluation metrics. 5.1.2 Baselines.We compare our TeaRAG with various baselines, which are divided into the following two categories: one-turn generation and iterative RAG. One-turn generation prepares all the required information and utilize the LLM in a single pass to generate the final answer. The methods in this category are as follows: • Zero-shot:This method directly uses the LLM to generate answers to questions without retrieving relevant information. • RAG[ 35]:This is the standard RAG method that uses semantic retrieval to search for relevant text chunks from the corpus, which are then input as evidence to the LLM to directly generate answers. • Rerank[ 79]:This method uses semantic retrieval to search for relevant text chunks from the corpus, then applies reranking to filter noise, before inputting them as evidence to the LLM to directly generate answers. , V ol. 1, No. 1, Article . Publication date: November 2018. TeaRAG : A Token-Efficient Agentic Retrieval-Augmented Generation Framework 17 Table