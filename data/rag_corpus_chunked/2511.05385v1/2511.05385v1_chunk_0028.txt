are not optimized for real-world retrieval environments. Consequently, their reasoning abilities are poorly suited to realistic retrieval scenarios, and they exhibit limited generalization [32]. To address this issue and control the number of reasoning steps ùëò, we propose IP-DPO, which introduces a process-aware reward and uses iterative DPO [49] to optimize the LLMs for better and more concise reasoning paths. First, we collect a sampling dataset containing a diverse set of tasks and use the LLM to generate outputs for this dataset. Next, we score each sampled instance with a process-aware reward and the final outcome. These scored data are then used to construct data pairs, which serve as the training inputs for IP-DPO. After each training cycle, we resample using the updated model and repeat this process iteratively to improve performance further. Dataset collection.To enhance data diversity, we collect a dataset by sampling from the training sets of NQ [ 33], HotpotQA [75], and Musique [ 58]. From the NQ dataset, we randomly sample 4,000 single-hop questions. From the HotpotQA dataset, we sample questions from the hard level that contain as many evidence sentences as possible, drawing 2,500 questions each from the bridge and comparison types. From the Musique dataset, we sample a total of 1,000 questions, distributed evenly across the 2-hop, 3-hop, and 4-hop data. The data statistic is shown in Table 2. The model then performs inference using the overall pipeline on each question in this dataset to repeatedly generate ùëÖ reasoning paths. For each reasoning path, both the reasoning process and the final answer are recorded for subsequent evaluation. Reward design.To better guide the LLM‚Äôs reasoning process and encourage more concise reasoning steps, we design a process-aware reward based on knowledge matching. This enables a more fine-grained evaluation of the quality of different generated outputs. Our