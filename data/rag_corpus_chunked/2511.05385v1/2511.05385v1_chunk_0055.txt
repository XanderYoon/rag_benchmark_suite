distribution drift. When the number is sufficiently large, the information filtered by PPR has a higher information density, and at the same time, the LLM can exploit the co-occurrence in the data to achieve better performance. (3) TeaRAG is more scalable, with a more significant performance boost as the number of input contents per retrieval increases.This is due to the construction of KAG and effective PPR filtering, which introduce triplets to increase information density and accuracy, thus reducing irrelevant content for each input. In contrast, the baseline method typically hits a performance bottleneck once the number of inputs reaches 3, as traditional retrieval-and-rerank approaches still cannot avoid interference from irrelevant information in chunks. 5.5.2 Effects of Hyperparameter ğ›¼ in PPR.In this section, we explore the PPR hyperparameter ğ›¼, which controls the focus of retrieved content. Whenğ›¼ is small, PPR emphasizes the personalization vector, i.e., semantic information relevant to the query. Whenğ›¼ is large, it prioritizes the structured information of co-occurrence links. The experimental results are shown in Fig. 12. We have the following observations: (1) TeaRAG is robust to changes inğ›¼.The performance of TeaRAG varies little for ğ›¼ between 0.1 and 0.7. This robustness stems from KAG, which is constructed from the outputs of two retrieval methods, ensuring that its content is already highly relevant to the query. (2) TeaRAG achieves better performance by balancing query relevance and co-occurrence relations.TeaRAG typically performs best when ğ›¼ is in the range 0.3 to 0.7. This indicates that PPR filtering effectively combines both factors to enhance information density and accuracy. (3) Greater emphasis on co-occurrence structure reduces the average number of content tokens per retrieval.This is because, as co-occurrence structure receives more weight, information , V ol. 1, No. 1, Article . Publication date: November 2018. 26 Zhang et al. from triplet nodes