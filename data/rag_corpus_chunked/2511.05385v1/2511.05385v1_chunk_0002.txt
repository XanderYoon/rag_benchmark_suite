or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym ’XX, Woodstock, NY © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX , V ol. 1, No. 1, Article . Publication date: November 2018. arXiv:2511.05385v1 [cs.IR] 7 Nov 2025 2 Zhang et al. ACM Reference Format: Chao Zhang, Yuhao Wang, Derong Xu, Haoxin Zhang, Yuanjie Lyu, Yuhao Chen, Shuochen Liu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, and Enhong Chen. 2018. TeaRAG: A Token-Efficient Agentic Retrieval- Augmented Generation Framework. InProceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym ’XX).ACM, New York, NY , USA, 32 pages. https: //doi.org/XXXXXXX.XXXXXXX 1 Introduction Large Language Models (LLMs) have made substantial progress through learning from massive pre-training corpora. Nevertheless, statistical inaccuracies in linguistic distribution modeling during pre-training can lead to hallucination generation [29]. Retrieval-Augmented Generation (RAG) is an effective technique that mitigates hallucinations in LLMs by incorporating external retrieval information [2, 12]. To improve the accuracy of RAG, conventional RAG systems typically adopt a predefined multi-step workflow consisting of planning [61], query rewriting [42], retrieval [40], reranking [65], refinement [23], and generation. However, the fixed workflows constrain their capacity to address tasks that demand multi-step reasoning, adaptive decision-making, and dynamic integration of retrieved information [36, 40]. To further enhance the flexibility and adaptiveness of the RAG framework, agentic RAG has been proposed. Agentic RAG adopts agentic workflows that allow LLMs to autonomously control the workflow process to solve complex problems [40, 54]. By leveraging the capabilities of LLMs in task decomposition and dynamic planning, agentic RAG can break down problems into a sequence of tractable steps. At each step, the framework proactively invokes retrieval modules to obtain relevant contextual information,