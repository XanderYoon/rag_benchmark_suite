reasoning step ğ‘†ğ‘–+1 =LLM(ğ¼, ğ‘,P ğ‘– ). Conversely, when the information is sufficient, the LLM generates the final answer ğ‘=LLM(ğ¼, ğ‘,P ğ‘– ). And the final answer ğ‘ starts with the format â€œFinal answer: â€, which facilitates the format check. 4.3 Model Training To enable LLMs to autonomously execute the aforementioned pipeline, current advanced agentic RAG mainly adopts outcome-based RL [27, 54]. However, these methods rely on high-resource RL methods such as PPO [51] and GRPO [52], which require continuous inference and retrieval during training, and these methods only focus on the final results, making it difficult to effectively control the reasoning steps. Although some methods utilize GPT-4o to annotate intermediate reasoning steps [83] , V ol. 1, No. 1, Article . Publication date: November 2018. 12 Zhang et al. Step i:EntitySubqueryContextSummaryStep i:EntitySubqueryContextSummary First Stage: SFT Second Stage: IP-DPO SFT Data Construction Based on MuSiQueKnowledge ExtractionEntity RecognitionForm Natural SubqueryContext SimulationSummary Formulation Master the format and basic reasoning (ğ¼,ğ‘) Î˜!"# Î˜! Next iteration Inference(ğ¼,ğ‘,ğ’«$,ğ‘) Reward Design (ğ¼,ğ‘,O%,O")DPO+SFTReasoning PathsTraining PairsSampling Data Î˜& 1. Output Reward 2. Format Reward 3. Process RewardPair Construction ğ‘=Kreuzlingenğ‘!"=Kreuzlingen Step i:EntitySubqueryContextSummary ConsistencyKnowledgeMatching Learn from SFT Align entities and subqueries Reasoning Path Process MemoryVector Fig. 5. The overall training framework for TeaRAG follows a two-stage paradigm. First, we conduct SFT on the preprocessed MuSiQue dataset to help the LLM learn the required format and develop basic reasoning skills. In the second stage, we apply IP-DPO with a process-aware reward to further improve the model while preventing overthinking. or consider complex rewards in the PPO framework [66], these approaches make it difficult to achieve efficient, low-resource training of agentic RAG. Therefore, to control the number of reasoning steps ğ‘˜ in Pğ‘˜ and enable efficient training, we propose a two-stage training paradigm that improves the efficiency and conciseness of reasoning by