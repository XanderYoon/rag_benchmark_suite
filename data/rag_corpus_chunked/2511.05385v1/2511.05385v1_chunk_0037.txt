to directly generate answers. ‚Ä¢ Rerank[ 79]:This method uses semantic retrieval to search for relevant text chunks from the corpus, then applies reranking to filter noise, before inputting them as evidence to the LLM to directly generate answers. , V ol. 1, No. 1, Article . Publication date: November 2018. TeaRAG : A Token-Efficient Agentic Retrieval-Augmented Generation Framework 17 Table 1. SFT data distribution. Dataset 1-hop 2-hop 3-hop 4-hop Total Train 2,700 1,800 1,800 1,057 7,357 Test 300 200 200 118 818 Table 2. IP-DPO dataset composition. Source Dataset Number NQ 4,000 HotpotQA 5,000 Musique 1,000 Total 10,000 Table 3. QA datasets for testing. Task Dataset # Dev # Test Single-hop QANQ 8,757 3,610 PopQA ‚Äì 14,267 Multi-hop QA HotpotQA 7,405 ‚Äì 2WikiMultiHopQA 12,576 ‚Äì Musique 2,417 ‚Äì Bamboogle ‚Äì 125 Table 4. Chunk corpus statistics. Metric Value Size of document set 3,232,908 Size of chunk setD21,015,324 Average length of chunk 100 Table 5. Knowledge graph statistics. Metric Value Size of entity setV51,063,765 Size of relation setE130,931,564 Average out-degree per head entity 9.24 Average in-degree per tail entity 3.05 Average degree per entity 5.13 ‚Ä¢ RECOMPùëéùëèùë† [73]:This method uses a compressor to extract core information related to the question from retrieved chunks before inputting them to the LLM. ‚Ä¢ MixPR-RAG[ 1]:This method constructs a semantic graph from each sentence in the chunks and uses PPR to filter out important sentences to input to the LLM. Iterative RAG utilizes LLMs to autonomously determine whether retrieval is needed, making multiple calls to retrieval tools and integrating multiple retrieval contexts to derive the final answer. The methods in this category are as follows: ‚Ä¢ IRCoT[ 59]:This method iteratively utilizes Chain-of-Thought (CoT) to guide retrieval and uses retrieval results to enhance CoT to derive the final answer. ‚Ä¢ SelfRAG[ 2]:This method utilizes LLMs to