[15, 16, 39], lacking validation on sufficiently large-scale graph data. Furthermore, graph- enhanced RAG systems typically select a fixed number of top-ranked triplets [ 6, 10, 70] or their associated text chunks [15, 16], or directly concatenate the retrieved triplets and chunks as input [50, 77]. However, these approaches fail to fully exploit the co-occurrence of different representations of the same knowledge element. In our work, we conduct experimental validation by constructing a large-scale knowledge graph based on a common wiki corpus [ 30]. We use PPR on a KAG of retrieved chunks and knowledge triplets to select the most relevant information. This process boosts information density while preserving key context, achieving a token-efficient agentic RAG. 2.2 Reinforcement Learning for LLMs LLMs gain knowledge through pre-training on vast data [ 3, 74] and are then instruction-tuned via SFT to follow instructions [13, 45]. However, this imitation-based learning limits their ability to generalize. To address these limitations, Reinforcement Learning from Human Feedback [ 45] (RLHF) first trains a reward model to capture human preferences over model outputs. This reward model then guides the LLM optimization using PPO [51], enabling the generation of higher-quality, more aligned responses. Beyond alignment with human values, RL techniques have proven effective in enhancing LLM reasoning capabilities [22, 52]. For example, methods like GRPO [52] improve performance on mathematical reasoning tasks by optimizing the modelâ€™s generation process using either outcome-based rewards [52, 78] or fine-grained process reward models that provide step-by- step supervision [82]. However, the above methods are typically online RL approaches, requiring LLMs to perform sampling simultaneously during training [27, 52], which often leads to low training efficiency when LLM inference is lengthy or requires calling complex tools [11, 49]. By separating the sampling and training phases, easier and more efficient RL methods achieve higher