our TeaRAG. 5.1.3 Implementation Details.To ensure robustness, we employ two widely used LLMs for training and inference: Llama3-8B-Instruct [13] and Qwen2.5-14B-Instruct [74]. For the retrieval stage, following [28], we utilize E5-base-V2 [64] as the retriever and BGE-reranker-v2 [5] as the reranker, which together constitute our retrieval system R. For semantic retrieval, we retrieve the top-20 chunks, which are then reranked to a final set ofğ‘˜ğ‘‘ =5 chunks. For graph retrieval, we initially retrieve the top-10 entities and 20 edges. After reranking, these are narrowed down to 5 entities and ğ‘˜ğ‘¡ =10 edges. The final number of items composing the context is set to ğ‘˜ğ‘“ =5 . During the PPR process, we set the hyperparameters ğœ=0.2 and ğ›¼=0.5 . The iteration ğ‘ of PPR is set to 200. We configure the LLM to perform a maximum of5reasoning steps, i.e.,ğ‘˜â‰¤5. For training, our experiments are conducted using 8 NVIDIA A100 (80G) GPUs. In the SFT phase, the learning rate is set to 5Ã—10 âˆ’4, with an overall batch size of 128 and a per-device batch size of 16. We train for 1 epoch with a weight decay of 0.001. In the IP-DPO phase, ğ‘…=8 reasoning paths are repeatedly sampled for each question. And the number of epochs for the first two DPO rounds is set to 2, while the final round is trained for only 1 epoch to prevent overfitting from data repetition. The hyperparameter ğ›½ is set to 0.5. The SFT weighting parameter ğœ‚ is tuned within the range [0.25,0.5,1] and is progressively reduced as DPO iterations increase to prevent overfitting. Besides, the learning rate is set to 1Ã—10 âˆ’4, the overall batch size is 16, the per-device batch size is 2, and the weight decay is 0.001. In both training stages, we adopt LoRA [19] for parameter-efficient fine-tuning. We set