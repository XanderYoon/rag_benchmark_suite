autonomously determine whether retrieval is needed, making multiple calls to retrieval tools and integrating multiple retrieval contexts to derive the final answer. The methods in this category are as follows: • IRCoT[ 59]:This method iteratively utilizes Chain-of-Thought (CoT) to guide retrieval and uses retrieval results to enhance CoT to derive the final answer. • SelfRAG[ 2]:This method utilizes LLMs to generate special tokens to help the model decide when to retrieve and whether to use the retrieved content. • R1-Searcher[ 54]:This method employs two-stage training. The first stage trains the model to learn generating retrieval queries, and the second stage uses outcome-based and format rewards to train LLMs to automatically invoke retrieval during reasoning. Based on Reinforce++ [20], this method is trained using data from HotpotQA and 2WikiMultiHopQA. We use two versions of models trained by this method: R1-Searcher-Llama-8B and R1-Searcher-Qwen-7B. • Search-R1[ 27]:This method utilizes PPO [ 51] with complete NQ and HotpotQA training data to optimize LLMs based solely on outcome rewards. We test four versions of models trained by this method: Search-R1-instruct-7B, Search-R1-base-7B, Search-R1-instruct-14B, Search-R1-base- 14B. , V ol. 1, No. 1, Article . Publication date: November 2018. 18 Zhang et al. For all baselines, the retrieval systems use E5-base-V2 [ 64] as the retriever unless otherwise specified. The “+R” symbol indicates that the retrieval system incorporates both retrieval and reranking stages, with BGE-reranker-v2 [5] serving as the reranker model. This configuration ensures a fair comparison with our TeaRAG. 5.1.3 Implementation Details.To ensure robustness, we employ two widely used LLMs for training and inference: Llama3-8B-Instruct [13] and Qwen2.5-14B-Instruct [74]. For the retrieval stage, following [28], we utilize E5-base-V2 [64] as the retriever and BGE-reranker-v2 [5] as the reranker, which together constitute our retrieval system R. For semantic retrieval, we retrieve the top-20 chunks, which are then reranked