more comprehensive evidence paragraphs to supervise the reasoning process [66], or utilizing GPT-4o to annotate intermediate reasoning steps [83]. Although current agentic RAG has made significant progress, existing methods focus on training LLMs on how to invoke retrievers for better performance, often neglecting token efficiency. Besides, these approaches either rely on high-resource RL methods such as PPO [51] and GRPO [52], which require multi-machine collaboration and continuous retrieval resource requests during training [66], or utilize GPT-4o for process scoring [83]. Both approaches make it difficult to achieve efficient, low- resource implementation of agentic RAG. Therefore, our work aims to improve the token efficiency of agentic RAG by reducing token usage while maintaining performance. Additionally, we propose an iterative process-aware DPO to implement agentic RAG. , V ol. 1, No. 1, Article . Publication date: November 2018. 6 Zhang et al. 2.1.2 Graph-Enhanced RAG.Most traditional RAG relies on chunk-based retrieval approaches [ 9], which split complete document information into multiple chunks and use these chunks as the units for retrieval and input to LLMs. However, since question-related clues are embedded in complex chunk contexts, existing chunk-based approaches struggle to accurately capture subtle clues [14, 40] and multi-hop knowledge associations [ 15, 16]. Previous work organizes corpus knowledge into knowledge graph structures to capture fine-grained clues and knowledge associations more systematically [ 9, 14, 71, 72]. Leveraging the topological structure of knowledge graphs, many works replace chunk-based retrieval methods with graph algorithms to achieve more accurate results [6, 10, 15, 16, 70]. These graph-enhanced RAG approaches can be categorized into three strategies. First, some methods employ classical graph algorithms such as Prize-Collecting Steiner Tree [17] and PPR [ 1, 15, 16] to retrieve all query-related information in one step, then feed the entire retrieved context to the LLM for direct answer generation