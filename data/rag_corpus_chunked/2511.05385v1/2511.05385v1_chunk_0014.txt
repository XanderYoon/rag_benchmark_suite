models that provide step-by- step supervision [82]. However, the above methods are typically online RL approaches, requiring LLMs to perform sampling simultaneously during training [27, 52], which often leads to low training efficiency when LLM inference is lengthy or requires calling complex tools [11, 49]. By separating the sampling and training phases, easier and more efficient RL methods achieve higher training efficiency, , V ol. 1, No. 1, Article . Publication date: November 2018. TeaRAG : A Token-Efficient Agentic Retrieval-Augmented Generation Framework 7 sometimes at the expense of performance [21]. These include rejection sampling [31, 68, 80], which enhance model performance by filtering high-quality samples from generated data for subsequent SFT. DPO [31, 49] circumvents explicit reward model training by treating the LLM itself as an implicit reward model, enabling RL through preference pair construction. This approach has been successfully applied to reasoning tasks through iterative sampling and training cycles [47, 60]. Building on this, our work utilizes RL algorithms to optimize the multi-turn retrieval and reasoning capabilities of agentic RAG systems. We design novel process-based rewards that employ knowledge matching to evaluate key components throughout the LLMâ€™s reasoning paths. These rewards effectively distinguish between positive and negative samples, and we employ multi-round iterative DPO to train LLMs for adaptation to agentic RAG tasks efficiently. 3 Preliminaries Given a task instruction ğ¼, a question ğ‘, a retriever R, a collection of document chunks D= {ğ‘‘1, . . . , ğ‘‘ğ‘› }, and a knowledge graph G=âŸ¨V,EâŸ© where V={ğ‘£ 1, . . . , ğ‘£ğ‘š } is the entity set and E=  ğ‘’ğ‘– |ğ‘’ ğ‘– =(ğ‘£ â„ ğ‘– , ğ‘Ÿğ‘’ğ‘™, ğ‘£ ğ‘¡ ğ‘– ), ğ‘£ â„ ğ‘– , ğ‘£ğ‘¡ ğ‘– âˆˆ V is the triplet set, RAG utilizes the retriever R to retrieve relevant information based on questionğ‘and inputs it