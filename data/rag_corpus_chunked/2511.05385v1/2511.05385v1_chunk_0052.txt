tendency to gradually increase their reasoning step count. This phenomenon is attributed to the lack of intermediate supervision, which can reinforce redundant reasoning paths that ultimately lead to the correct result. (3) Process rewards make training more stable.With only outcome rewards, performance col- lapses by the second training round. The model learns to favor reasoning paths that reach the , V ol. 1, No. 1, Article . Publication date: November 2018. 24 Zhang et al. correct answer despite being in the wrong format, undermining its ability to follow the intended reasoning structure. Similarly, when using both outcome and format rewards, collapse occurs by the third round. The model still rewards correct answers produced through flawed logic. This failure mode is especially common in binary-choice comparison tasks, leading it to internalize misguided preferences for certain reasoning paths. Process rewards prevent this by construct- ing preference pairs under stricter criteria, blocking these spurious preferences and stabilizing training. 5.4.3 Effect of IP-DPO Iterations.We further investigate the impact of the IP-DPO iterations. We evaluate the models from all training stages of IP-DPO. TeaRAG-SFT denotes the model trained only with SFT. TeaRAG-1, TeaRAG-2, and TeaRAG-3 refer to the models after 1, 2, and 3 rounds of DPO training, respectively. The results are presented in Figure 8 and Figure 9. We can draw the following conclusions: (1) The performance of TeaRAG exhibits continuous enhancement as the number of IP-DPO rounds increases.The underlying mechanism is that each round of optimization leverages the outputs from the model of the preceding round. This enables the model to sequentially master and refine its reasoning paradigms. (2) The performance gains from IP-DPO exhibit diminishing returns with additional rounds. This occurs because the modelâ€™s reasoning capabilities begin to converge, establishing a stable inference pattern. After many rounds of iteration, the model consistently