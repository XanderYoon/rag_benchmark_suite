reasoning iterations. To this end, we propose TeaRAG. TeaRAG provides a fully automated agentic pipeline, including important entity recognition, subquery generation, context retrieval, summary generation, and final answer generation. To raise the information density per retrieval step, we incorporate high-information-density knowledge triplets and adopt a hybrid retrieval strategy that combines semantic and graph retrieval. We further construct a KAG and apply co-occurrence-based PPR to preserve rich context and boost information density. To reduce the number of reasoning iterations, we introduce IP-DPO, which uses process-aware rewards to supervise the reasoning path, preventing overthinking and redundant retrieval. Extensive experiments demonstrate the effectiveness of our approach and its superior token efficiency. References [1] Nicholas Alonso and Beren Millidge. 2024. Mixture-of-PageRanks: Replacing Long-Context with Real-Time, Sparse GraphRAG. arXiv preprint arXiv:2412.06078 (2024). [2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In ICLR. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. NeurIPS 33 (2020), 1877â€“1901. [4] Nikhil Chandak, Shashwat Goel, and Ameya Prabhu. 2025. Incorrect Baseline Evaluations Call into Question Recent LLM-RL Claims. https://safe-lip-9a8.notion.site/Incorrect-Baseline-Evaluations-Call-into-Question-Recent-LLM-RL- Claims-2012f1fbf0ee8094ab8ded1953c15a37?pvs=4. Notion Blog. [5] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2023. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv:2309.07597 [cs.CL] [6] Liyi Chen, Panrong Tong, Zhongming Jin, Ying Sun, Jieping Ye, and Hui Xiong. [n. d.]. Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs. In NeurIPS. [7] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024. Do not think that much for 2+ 3=? on the overthinking of o1-like