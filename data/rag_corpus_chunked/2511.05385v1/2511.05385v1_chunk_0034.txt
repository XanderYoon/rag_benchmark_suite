violates the required format. â€¢ Easy rejection: ğ‘Ÿ âˆ’ format =1 and ğ‘Ÿ âˆ’ outcome â‰¤0.3 . The path adheres to the format but fails to reach the correct answer. â€¢ Hard rejection: ğ‘Ÿ âˆ’ format =1 , 0.3<ğ‘Ÿ âˆ’ outcome â‰¤ğ‘Ÿ + outcome âˆ’0.3 , and ğ‘Ÿ âˆ’ process â‰¤ğ‘Ÿ + process. The path largely progresses toward an answer but still contains specific errors. To maintain the data diversity of DPO pairs, once a rejected response is obtained for a chosen sample, the algorithm continues traversing the ranked list from the next position to find the next rejected sample for the next chosen sample of the questionğ‘. This approach prevents the selection of duplicate rejected samples. Additionally, for single-hop questions, due to their simplicity, rejected samples are limited to format and easy rejection. Following this procedure, we construct the DPO pair dataset. We denote the dataset at iterationğ‘byS ğ·ğ‘ƒğ‘‚âˆ’ğ‘ ={(ğ¼, ğ‘, ğ‘‚ +, ğ‘‚âˆ’)}. Iterative DPO training.Given that the quality of samples generated by the preliminary SFT model remains suboptimal, the performance gains from a single round of DPO training are inherently limited. To address this, we adopt an iterative DPO procedure designed to enhance the modelâ€™s comprehension , V ol. 1, No. 1, Article . Publication date: November 2018. 16 Zhang et al. of preference patterns and enable progressively stronger alignment. Following [46, 47], we jointly optimize the DPO objective together with the SFT objective to mitigate distribution shift in the modelâ€™s chosen responses. The training losses are defined as follows: Lğ‘†ğ¹ğ‘‡ =âˆ’E (ğ¼,ğ‘,ğ‘‚ +,ğ‘‚ âˆ’ )âˆ¼S ğ·ğ‘ƒğ‘‚âˆ’ğ‘ |ğ‘‚ + |âˆ‘ï¸ ğ‘¡=1 I[ğ‘‚ + ğ‘¡ not masked]logğ‘ƒ(ğ‘‚ + ğ‘¡ |ğ¼, ğ‘, ğ‘‚ + <ğ‘¡ ;Î˜ ğ‘ ),(15) Lğ·ğ‘ƒğ‘‚ =âˆ’E (ğ¼,ğ‘,ğ‘‚ +,ğ‘‚ âˆ’ )âˆ¼S ğ·ğ‘ƒğ‘‚âˆ’ğ‘ " logğœ ğ›½ |ğ‘‚ + |âˆ‘ï¸ ğ‘¡=1 I[ğ‘‚ + ğ‘¡ not masked]log ğ‘ƒ(ğ‘‚ +