1. ğ‘‡ğ‘– denotes the thinking tokens at the i-th step, ğ‘…ğ‘– denotes the retrieved context at the i-th step, and ğ‘‚ represents the final output. (a) illustrates Search-R1, a representative agentic RAG method optimized based on the final outcome. (b) shows our proposed method TeaRAG, which achieves a token-efficient agentic RAG by optimizing the retrieved content length with high-density triplets and controlling the number of LLM reasoning steps via a process-aware reward. Content per Retrieval Thinking Retrieved Content T otal0 500 1000 1500 2000 2500Average T oken Count (a) T oken Usage 1 2 3 4 5 Reasoning Steps 0 10 20 30 40 50 60 70 80Proportion (%) (b) Reasoning Steps Distribution Single-hop Multi-hop Overall40.0 42.5 45.0 47.5 50.0 52.5 55.0 57.5 60.0F1 Score (c) Performance R1-Searcher-Qwen-7B+R Search-R1-base-7B+R T eaRAG-8B Fig. 2. (a) shows the token usage. (b) shows the distribution of reasoning steps. (c) shows the F1 performance on single-hop, multi-hop, and overall QA benchmarks. decomposition, and reasoning over retrieved content. The second is the retrieved content that the LLM obtains by invoking a retriever to access external sources. Through statistical analysis of different token types and iteration rounds, as observed in Fig. 2 (a) and Fig. 2 (b), we identify two critical inefficiencies that impact token utilization. First, the retrieved content constitutes the majority of the overall output. This is because chunk-based retrieval methods typically return entire document segments as input to LLMs. However, a substantial portion of these contents may consist of irrelevant or redundant background information that does not enhance the quality of the final answer [ 40]. Therefore, increasing the information density of the retrieved content is essential for improving the token efficiency in agentic RAG. Second, agentic RAG methods generally adopt multi-step reasoning, even when addressing single-hop questions. As shown in Fig. 2