This process helps the LLM learn to adapt to diverse and imperfect contexts. (5) Summary formulation:We use diverse templates to construct target summaries. Every summary follows a two-part structure. It first presents the key supporting triplets and then concludes with the answer to the subquery. , V ol. 1, No. 1, Article . Publication date: November 2018. TeaRAG : A Token-Efficient Agentic Retrieval-Augmented Generation Framework 13 After constructing each reasoning step, we concatenate them in the order specified by the Musique dataset to form a complete reasoning path. Alternatively, each individual reasoning step can be treated as a single-hop question. We denote the SFT dataset as Sğ‘†ğ¹ğ‘‡ ={(ğ¼, ğ‘, ğ‘‚)}={(ğ¼, ğ‘,[P ğ‘˜, ğ‘])} , where ğ‘‚ represents either the direct output of the LLM or context autonomously retrieved by the LLM. The SFT dataset statistics are shown in Table 1. To avoid interfering with the LLMâ€™s ability to learn reasoning patterns due to the retrieved context, following [27], we mask the context between the two special tokens <Reference> and </Reference>. We fine-tune the model on the SFT dataset, using the following objective function: Lğ‘†ğ¹ğ‘‡ =âˆ’E (ğ¼,ğ‘,ğ‘‚)âˆ¼S ğ‘†ğ¹ğ‘‡ |ğ‘‚|âˆ‘ï¸ ğ‘¡=1 I[ğ‘‚ ğ‘¡ not masked]logğ‘ƒ(ğ‘‚ ğ‘¡ |ğ¼, ğ‘, ğ‘‚ <ğ‘¡ ;Î˜),(4) where I is the indicator function, and Î˜ denotes the parameters of the LLM. Through SFT training, the model can generate outputs following the constructed reasoning path structure and acquire preliminary reasoning capabilities. 4.3.2 Second Stage: IP-DPO.The models trained with SFT are not optimized for real-world retrieval environments. Consequently, their reasoning abilities are poorly suited to realistic retrieval scenarios, and they exhibit limited generalization [32]. To address this issue and control the number of reasoning steps ğ‘˜, we propose IP-DPO, which introduces a process-aware reward and uses iterative DPO [49] to optimize the LLMs for better and more concise reasoning paths.