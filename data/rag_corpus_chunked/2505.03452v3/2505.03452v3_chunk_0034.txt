0.45 0.50 0.55Lexical-FF ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.475 0.500 0.525 0.550 0.575Lexical-FF MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.45 0.50 0.55 0.60Lexical-FF ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC Figure 7: Per-iteration performance of all HPO algorithms on the test sets of five datasets, optimizing answer faithful- ness. The dashed black lines denote the best achievable per- formance on each test set. See §4 for a discussion of the main results. by ‘[End]’. Similarly, for Llama each retrieved chunk was prefixed with ‘[document]:’. (a) Total number of tokens from chunks sent to the embedding models. (b) Total number of tokens for prompts sent to the generation models. Figure 8: Cost estimation for each algorithm after each iteration. Lexical-FF Dataset Worst Best SE AIArxiv 0.28 0.64 0.03 BioASQ 0.38 0.60 0.01 MiniWiki 0.39 0.56 0.01 ClapNQ 0.28 0.56 0.01 WatsonxQA 0.37 0.65 0.03 Table 5:WorstandBestconfiguration scores per dataset on the development set for the Lexical-FF metric. Also shown is the maximum standard error (SE) observed across all con- figurations. H Use Of AI Assistants AI Assistants were only used in writing for minor edits and rephrases. They were also used to aid in obtaining the correct LateX syntax for the various figures. Table 6: Results of a likelihood ratio test for the grid-search results of the AIArxiv dataset (LLMaaJ-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 401 105.5×10 −80 Embedding model 42 109.1×10 −6 Chunk size 48 81.2×10 −7 Chunk overlap 1.6 3 0.65K 29 65.5×10 −5 Generative model * Embedding model 2.2 4 0.7Embedding model * Chunk size 31 43.2×10 −6 Chunk size * Chunk overlap 0.28 2 0.87Generative model * K 16 43.5×10 −3 Table 7: Results of a likelihood