not annotated in ARAGOG dataset, we added such labels where they could be found, obtaining71answerable QA pairs out of107in the original benchmark. 8https://huggingface.co/datasets/jamescalam/ai-arxiv2 BioASQ(Krithara et al. 2023) A subset of the BioASQ Challenge train set.9 Its corpus contains40200passages ex- tracted from clinical case reports. The corresponding bench- mark of4200QA pairs includes multiple gold documents per question. MiniWikiA benchmark of918QA pairs over Wikipedia derived from Smith, Heilman, and Hwa (2008). 10 The con- tents are mostly factoid questions with short answers. This dataset has no gold document labels. ClapNQ(Rosenthal et al. 2024) A subset of the Natu- ral Questions (NQ) dataset (Kwiatkowski et al. 2019) on Wikipedia pages, of questions that have long answers. The original benchmark contains both answerable and unanswer- able questions. For our analysis we consider only the former. ClapNQ dataset consists of178890passage texts generated from4293pages. These passages constitute the input to the pipeline. WatsonxQA(ProductDocs) A new open-source dataset and benchmark based on enterprise product documentation, consisting of5534passage texts created from1144HTML product documentation pages.11 These passages serve as the RAG pipeline input. The benchmark includes75QA pairs and gold document labels, of which25were generated by two subject matter experts, and the rest were synthetically produced and then manually filtered. All QA pairs were ad- ditionally reviewed by two of the authors, ensuring high data quality. Further details are in Appendix A. Overall, these datasets exhibit variability in many aspects. They represent diverse domains â€“ research papers, biomed- ical documents, wikipedia pages and enterprise data (see question examples in Table 3). They also vary in ques- tion and answer lengths; for example, MiniWiki has rela- tively short answers, while ClapNQ was purposely built with long gold answers. Corpus sizes also vary, representing real- world retrieval scenarios over small or large sets of docu- ments. Every benchmark was split into development