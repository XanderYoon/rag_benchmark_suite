Model, Chunk Size, Chunk Overlap, Top-K.Retrieval- first(Greedy-R) is a prevalent option following the RAG pipeline structure, starting with retrieval optimization (still with model first), then generation: Embedding Model, Chunk Size, Chunk Overlap, Generative Model and Top- K.Retrieval-first with context correctness(Greedy-R-CC) uses the same order, yet optimizes the retrieval-related pa- rameters with a context correctness metric evaluated solely on the retrieval results, saving the costs of LLM inference until all retrieval parameters are chosen. Remaining param- eters are then optimized with answer correctness. Setup Our experimental design reflects a realistic use case: an HPO algorithm is executed over a benchmark (a development set) and the best RAG configuration is chosen for deployment; the deployed configuration is expected to generalize to un- seen questions â€“ simulated here via a test set. Specifically, each HPO algorithm ran on each development set for10it- erations. After every iteration, the best configuration on the development set was evaluated on the test set, enabling a per-iteration generalization analysis. Since all algorithms in- volve a random element, each run was repeated with10dif- ferent random seeds. LLMaaJ-AC Lexical-AC Dataset Worst Best SE Worst Best SE AIArxiv 0.36 0.62 0.03 0.40 0.66 0.04 BioASQ 0.43 0.56 0.01 0.49 0.63 0.01 MiniWiki 0.32 0.51 0.02 0.61 0.85 0.01 ClapNQ 0.46 0.57 0.01 0.34 0.61 0.01 WatsonxQA 0.52 0.76 0.03 0.74 0.87 0.04 Table 4:WorstandBestconfiguration scores per dataset on the development set, reported for both LLMaaJ-AC and Lexical-AC metrics. Also shown is the maximum standard error (SE) observed across all configurations. [0.0 0.1) [0.1 0.2) [0.2 0.3) [0.3 0.4) [0.4 0.5) [0.5 0.6) [0.6 0.7) [0.7 0.8) [0.8 0.9) [0.9 1.0] LLMaaJ-AC 0 5 10 15 20 25% of RAG configurations Dataset ClapNQ BioASQ AIArxiv MiniWiki WatsonxQA Figure 2: The distribution of configurations across bins for the normalized LLMaaJ-AC metric on the