6 7 8 9 10 # Iterations 0.50 0.55 0.60Lexical AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.80 0.85 0.90Lexical AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.750 0.775 0.800 0.825 0.850Lexical AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.550 0.575 0.600 0.625Lexical AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.62 0.64 0.66 0.68Lexical AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.55 0.60Lexical AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.80 0.85 0.90Lexical AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.750 0.775 0.800 0.825 0.850Lexical AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.550 0.575 0.600 0.625Lexical AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.62 0.64 0.66 0.68Lexical AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.55 0.60Lexical AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.80 0.85 0.90Lexical AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.750 0.775 0.800 0.825 0.850Lexical AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC (b) Lexical-AC Figure 3: Per-iteration performance of all HPO algorithms on the test sets of five datasets, optimizing answer correctness computed with an LLMaaJ metric (a) and a lexical metric (b). The dashed black lines show the best achievable performance. The red lines are the performance of the best configuration chosen with development set