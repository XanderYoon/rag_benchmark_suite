good balance between speed and quality (Adlakha et al. 2024). Second, a standard LLM-as-a-Judge answer correctness (LLMaaJ-AC) implementation from the RAGAS library (Es et al. 2023), with GPT4o-mini (OpenAI 2024) as its backbone. This implementation performs3calls to the LLM on every invocation, making it much slower and more expensive than the lexical variant. Given a benchmark, all metrics were computed per- question. Averaging the per-question scores yields the over- all metric score. We note that all benchmarked HPO approaches are metric-agnostic - they are not tailored to any specific metric, nor a specific performance axis (such as answer correctness). Similarly, HPO can be applied to multiple RAG metrics at once, forming a single optimization objective. HPO Algorithms An HPO algorithm takes as input a RAG search space, a dataset (corpus and benchmark), and one evaluation met- ric designated as the optimization objective. Its goal is to find the RAG configuration that achieves the highest perfor- mance on the dataset with respect to the objective. The HPO algorithms in our experiments operate itera- tively. At each iteration, the algorithm reviews the scores 12Note that as labeling is at the document level, any chunk from the gold document is considered as a correct prediction, even if it does not include the answer to the question. of all previously explored configurations and selects an un- explored configuration to evaluate next. To simulate a con- strained exploration budget, the algorithm terminates after a fixed number of iterations and returns the best performing configuration. An efficient HPO algorithm identifies a top- performing configuration with minimal iterations. We examine two categories of HPO algorithms: (i) stan- dard HPO algorithms that are not specifically tailored to RAG; (ii) RAG-aware greedy algorithms that leverage some knowledge of the components within the optimized RAG pipeline. All algorithms optimize answer