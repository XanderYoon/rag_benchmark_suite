ClapNQ Wikipedia178890 1000 150 WatsonxQA Business5534 45 30 Table 2: Properties of the RAG Datasets in our experiments: the number of documents within the corpus (#Doc), and counts of QA pairs in the#Devand#Testsets. (3∗3) different configurations for answering (retrieval and generation). The values chosen for the chunk size, overlap, and top-k reflect common practices (see Appendix B for de- tails). Popular open source models were selected as options for the embedding and generative models. We opted to focus on LLMs that are similar in size, since otherwise an obvious strategy is to simply discard the smaller less capable models from the search. Our experiments involve performing a full grid search – i.e., evaluating all possible configurations – in order to es- tablish upper bound baselines for the optimization strategies. Hence, due to computational constraints we avoid using an even larger search space, and choose a set of moderately sized LLMs as our generators. Still, to the best of our knowl- edge, our search space is the largest considered to date that compares to full grid-search. Datasets Each RAG dataset is comprised of a corpus of documents and a benchmark of QA pairs, with most also annotating the document(s) with the correct answer. Below are the RAG datasets we used: AIArxivThis dataset was derived from the ARAGOG benchmark (Eibich, Nagpal, and Fred-Ojala 2024) of techni- cal QA pairs over a corpus of machine learning papers from ArXiv.8 As gold documents are not annotated in ARAGOG dataset, we added such labels where they could be found, obtaining71answerable QA pairs out of107in the original benchmark. 8https://huggingface.co/datasets/jamescalam/ai-arxiv2 BioASQ(Krithara et al. 2023) A subset of the BioASQ Challenge train set.9 Its corpus contains40200passages ex- tracted from clinical case reports. The corresponding bench- mark of4200QA pairs includes multiple gold documents per question. MiniWikiA benchmark of918QA pairs