RAG configurations assigned to each bin of normalized metric scores (with the Lexical-AC or Lexical-FF metrics), on the dev sets. E Embedding and Generation Costs Figure 8 details accumulated numbers of (a) embedded to- kens and (b) number of tokens used in generation part for HPO algorithms overall tested configurations. See §4 for a discussion of costs. F Hardware and Costs All used embedding and generation models are open source models. An internal in-house infrastructure containing V100 and A100 GPUs was used to run embedding computations and generative inference. Specifically, embeddings were computed using one V100 GPU, and inference was done on one A100 GPU (i.e. no multi-GPU inference was required). The evaluation of the LLMaaJ-AC metric was done with GPT4o-mini (OpenAI 2024) as its backbone LLM. That model was used through Microsoft Azure. The overall cost was∼500$. G Generation Prompt Details The RAG prompts used by each model are shown in Figure 9 for Granite, Figure 10 for Llama and Figure 11 for Mistral. In each prompt the{question}placeholder indicates where the user question was placed, and{retrieved documents} the location of the retrieved chunks. For Granite, each re- trieved chunk was prefixed with ‘[Document]’ and suffixed 1 2 3 4 5 6 7 8 9 10 # Iterations 0.4 0.5 0.6Lexical-FF AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.55 0.60Lexical-FF BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.45 0.50 0.55Lexical-FF ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.475 0.500 0.525 0.550 0.575Lexical-FF MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.45 0.50 0.55 0.60Lexical-FF ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC Figure 7: Per-iteration performance of all HPO algorithms on the test sets of five datasets,