Following sampling, we repeat the experiments using the best HPO methods: Random, TPE, and Greedy-M. Figure 5 compares performance when optimizing for LLMaaJ-AC and Lexical-AC, using the full (solid lines) or sampled (dot- ted) development sets. For BioASQ sampling has a negli- gible impact. For ClapNQ, results differ per algorithm and metric, with a suboptimal configuration identified by TPE and Random. For Greedy-M and the LLMaaJ-AC metric, the performance drop is moderate. With this approach, cost reductions are substantial. In- ference costs for a given configuration are10x cheaper, as 10% of the questions are used. Indexing costs drop by4x for BioASQ and178x for ClapNQ. These savings make sam- pling highly attractive for HPO over large datasets. In summary, development set sampling offers a promis- ing path towards efficient RAG HPO. Combined with the Greedy-M approach, the trade-off between cost and perfor- mance remains favorable, making it a practical choice for real-world applications. 5 Conclusion We presented a comprehensive study of HPO for RAG in a generalization setup that reflects real-world usage. Our eval- uation spans five HPO algorithms, three evaluations metrics, and multiple datasets from diverse domains. One is a newly curated enterprise product documentation dataset, released as part of this work, for use by the community. 16BioASQ has multiple gold documents per question, which yields more sampled documents. Our findings are that HPO systematically boosts RAG performance significantly. Compared to an arbitrarily cho- sen RAG configuration, running10HPO iterations yield gains of up to20% (see Figure 3, comparing the perfor- mance at the first and last iterations). While our experiments focus on core RAG components, the potential impact on more complex systems parametrized by larger search spaces is likely even greater. For example, exploring HPO in the context of multi-modal or agentic RAG pipelines seems a promising direction for future