Per-iteration plots of these counts are in Appendix E. Overall, generation costs were similar across algorithms and datasets. The embedding costs, dependent on the num- ber of different indices created by the algorithm, behaved similarly across datasets with variations between algorithms. (a) LLMaaJ-AC (b) Lexical-AC Figure 4: The effect of chosen optimization metric on the generative model within the best RAG configuration. Shown is the maximal answer correctness score per dataset and model (the highest of the54configurations in which the model appears). 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.52 0.54 0.56 0.58RAGAS AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.52 0.54 0.56 0.58 0.60RAGAS AC ClapNQ Grid/Full Grid/Sample Random/Full Random/Sample TPE/Full TPE/Sample Greedy-M/Full Greedy-M/Sample (a) LLMaaJ-AC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.62 0.64 0.66 0.68Lexical AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.52 0.54 0.56 0.58 0.60 0.62Lexical AC ClapNQ Grid/Full Grid/Sample Random/Full Random/Sample TPE/Full TPE/Sample Greedy-M/Full Greedy-M/Sample (b) Lexical-AC Figure 5: Per-iteration performance on the test sets of the two largest datasets, for HPO algorithms optimized using thefull development data (solid lines) or itssample(dotted). The dashed black lines show the best achievable test performance. The solid (dashed) red lines are the performance of the best configuration chosen by (sampled) development set evaluation. The Greedy-R and Greedy-R-CC algorithms create a new index at each iteration until all retrieval parameters are op- timized, making them initially expensive. Other algorithms like TPE and Random lack a mechanism to favor index reuse. Greedy-M begins by optimizing the generative model using a single index, making it cost-efficient when budget constraints or iteration limits are tight. Efficient HPO The results of Figure 3 were