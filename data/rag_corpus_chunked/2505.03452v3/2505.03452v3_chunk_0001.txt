RAG solutions focus LLMs on grounded data, reducing the likelihood of dependence on irrelevant preexisting knowledge. The popularity of RAG is largely thanks to its modular design, allowing full control over which data sources to pull data from and how to process that data. While advantageous, this modularity also means that practitioners are faced with a wide array of decisions when designing their RAG pipelines. One such choice is which generative LLM to use; other choices pertain to parameters of the retrieval system, such as how many items to retrieve per input question, how to rank them, and so forth. Furthermore, evaluating even a single RAG configuration is costly in terms of time and funds: the embedding step as part of corpus indexing is compute-intensive; generat- ing answers using LLMs is also a demanding task, espe- cially for large benchmarks; and evaluation with LLM-as- Copyright Â© 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: We study hyper parameter optimization over a RAG pipeline with5parameters. The explored search space includes162RAG configurations formed from combina- tions of the depicted hyper parameters. a-Judge (LLMaaJ) adds another costly round of inference. As a result, exhaustively exploring the exponential search space of RAG parameters is prohibitively expensive. At the same time, suboptimal parameter choices may significantly harm result quality. A promising approach to this challenge ishyper- parameter optimization (HPO)for RAG (Fu et al. 2024; Kim et al. 2024), which aims to identify high-performing configurations by systematically evaluating a subset of the search space. Existing methods range from established HPO algorithms to simple random sampling. Importantly, despite growing interest, the effectiveness of HPO in realistic RAG scenarios has not been rigorously tested. Our work addresses this gap through a comprehensive study of HPO for RAG in a setup that mirrors