search space with full grid-search evaluations to-date, showing that RAG HPO can be done efficiently, either greedily or with simple random search, and that it significantly boosts RAG performance for all datasets; (ii) a detailed analysis of the results, explor- ing the connections between the optimized parameters, the dataset and the optimization objective. For greedy HPO ap- proaches, we show that the order in which the parameters are optimized is of great importance; (iii) new open-source re- sources: the full grid search results of our experiments, and an enterprise product documentation RAG dataset. 2 Related Work Within the open-source community, several tools offer out- of-the-box HPO algorithms for RAG. AutoRAG (Kim et al. 2024) adopts a greedy approach, optimizing one RAG pa- rameter at a time following the sequential pipeline order. 3 RAGBuilder employs TPE for HPO. 4 Additionaly, RAG- centric libraries such as LlamaIndex 5 support HPO by inte- grating general-purpose optimization frameworks like Ray- 2The grid results are at https://github.com/IBM/rag-hpo-bench. 3https://github.com/Marker-Inc-Korea/AutoRAG 4https://github.com/KruxAI/ragbuilder 5https://www.llamaindex.ai/ Tune (Liaw et al. 2018), optuna (Akiba et al. 2019) and hy- peropt (Bergstra, Yamins, and Cox 2013). Other works investigate the impact of RAG hyper- parameters without automated optimization. For example, Lyu et al. (2024) and Wang et al. (2024c) focus on man- ual tuning of RAG hyper-parameters, while Zhu et al. (2024) evaluates multiple configurations via grid search. Their stud- ies highlight the critical role of hyper-parameter tuning in RAG and motivate the need for automated RAG HPO. Another interesting line of work (Fu et al. 2024) is moti- vated by a setting of online feedback from users. It describes an online HPO algorithm that iteratively updates the reward for the various RAG parameters based on small batches of queries. In contrast, our work addresses offline HPO, where optimization is performed on full