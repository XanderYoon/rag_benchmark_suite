a new index at each iteration until all retrieval parameters are op- timized, making them initially expensive. Other algorithms like TPE and Random lack a mechanism to favor index reuse. Greedy-M begins by optimizing the generative model using a single index, making it cost-efficient when budget constraints or iteration limits are tight. Efficient HPO The results of Figure 3 were obtained with each RAG con- figuration evaluated on the whole development set. While simple, this option is costly for large datasets. Prior work in other domains suggests that random sampling of evaluation benchmarks can reduce costs without sacrificing evaluation quality (Perlitz et al. 2024; Polo et al. 2024), and we there- fore explore this direction. To our knowledge, this is the first study of that direction in the context of HPO for RAG. To adapt sampling to RAG, we sample both the bench- mark and the underlying corpus. Specifically, focusing on the larger datasets of BioASQ and ClapNQ, 10% of the de- velopment QA pairs were sampled along with their corre- sponding gold documents (those containing the answers to the sampled questions). To preserve realistic retrieval condi- tions we add “noise” – documents not containing an answer to any of the sampled questions – at a ratio of9such doc- uments per gold document. This yields100sampled bench- mark questions per dataset, with the sampled corpora com- prising of1K (i.e.1000) documents for ClapNQ (out of 178K), and10K for BioASQ (out of40K). 16 Following sampling, we repeat the experiments using the best HPO methods: Random, TPE, and Greedy-M. Figure 5 compares performance when optimizing for LLMaaJ-AC and Lexical-AC, using the full (solid lines) or sampled (dot- ted) development sets. For BioASQ sampling has a negli- gible impact. For ClapNQ, results differ per algorithm and metric, with a suboptimal configuration identified by TPE and