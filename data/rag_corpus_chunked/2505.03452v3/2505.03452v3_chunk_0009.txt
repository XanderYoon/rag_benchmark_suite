wikipedia pages and enterprise data (see question examples in Table 3). They also vary in ques- tion and answer lengths; for example, MiniWiki has rela- tively short answers, while ClapNQ was purposely built with long gold answers. Corpus sizes also vary, representing real- world retrieval scenarios over small or large sets of docu- ments. Every benchmark was split into development and test sets. To keep computations tractable, the number of questions in the large benchmarks (BioASQ and ClapNQ) was limited 9https://huggingface.co/datasets/rag-datasets/rag-mini-bioasq 10https://huggingface.co/datasets/rag-datasets/rag-mini- wikipedia 11http://huggingface.co/datasets/ibm-research/watsonxDocsQA Dataset Example Question AIArxiv What significant improvements does BERT bring to the SQuAD v1.1,v2.0 and v13.5 tasks compared to prior models? BioASQ What is the implication of histone lysine methylation in medulloblastoma? MiniWiki Was Abraham Lincoln the sixteenth Pres- ident of the United States? ClapNQ Who is given credit for inventing the print- ing press? WatsonxQA What tuning parameters are available for IBM foundation models? Table 3: One question example from each dataset. to1000for development and150for test. Table 2 lists the corpora benchmark sizes and domains. Metrics The following metrics were used in our experiments:Re- trieval qualitywas measured usingcontext correctness with Mean Reciprocal Rank (V oorhees and Tice 2000). 12 Answer faithfulness(Lexical-FF) measures whether a gen- erated answer remained faithful to the retrieved contexts, with lexical token precision.Answer correctnesscompares generated and gold answers, assessingoverall pipeline quality, and is measured in two ways: First, a fast, lex- ical, implementation based on token recall (Lexical-AC), which provides a good balance between speed and quality (Adlakha et al. 2024). Second, a standard LLM-as-a-Judge answer correctness (LLMaaJ-AC) implementation from the RAGAS library (Es et al. 2023), with GPT4o-mini (OpenAI 2024) as its backbone. This implementation performs3calls to the LLM on every invocation, making it much slower and more expensive than the lexical variant. Given a benchmark, all metrics were computed