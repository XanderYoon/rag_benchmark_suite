Grid Random TPE Greedy-M Greedy-R Greedy-R-CC (b) Lexical-AC Figure 3: Per-iteration performance of all HPO algorithms on the test sets of five datasets, optimizing answer correctness computed with an LLMaaJ metric (a) and a lexical metric (b). The dashed black lines show the best achievable performance. The red lines are the performance of the best configuration chosen with development set evaluation, on the test set. Figure 4 reports the maximal answer correctness score per dataset for each generative model, computed as the highest of the162/3 = 54configurations in which the model ap- pears. When optimizing by LLMaaJ-AC, the best RAG con- figuration consistently involves Llama, whereas for Lexical- AC, Granite or Mistral are better. This difference stems from the nature of the metrics, as Lexical-AC is recall-oriented while LLMaaJ-AC balances precision and recall. These find- ings emphasize the critical role of optimization objective se- lection. Optimal configurations can differ substantially de- pending on the chosen metric, and thus this choice should carefully align with the intended application. Cost Considerations The cost of each HPO algorithm was tracked by counting the number of tokens embedded during indexing, and the num- ber of tokens used in generation (input and output). For each algorithm, we computed its total cost so far at a specific it- eration, by accumulating these token counts over the config- urations evaluated up to that iteration (including). The cost of indices used by multiple configurations was counted just once. Per-iteration plots of these counts are in Appendix E. Overall, generation costs were similar across algorithms and datasets. The embedding costs, dependent on the num- ber of different indices created by the algorithm, behaved similarly across datasets with variations between algorithms. (a) LLMaaJ-AC (b) Lexical-AC Figure 4: The effect of chosen optimization metric on the generative model within the best