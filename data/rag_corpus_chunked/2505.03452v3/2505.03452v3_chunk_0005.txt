need for automated RAG HPO. Another interesting line of work (Fu et al. 2024) is moti- vated by a setting of online feedback from users. It describes an online HPO algorithm that iteratively updates the reward for the various RAG parameters based on small batches of queries. In contrast, our work addresses offline HPO, where optimization is performed on full benchmark datasets prior to best configuration deployment. More recently, Barker et al. (2025) introduced multi- objective HPO for RAG. The studied methods select a set of RAG configuration deemed Pareto-optimal by multiple met- rics. Choosing the best configuration from this set remains an open challenge. Our study instead focuses on single- objective HPO returning a single configuration as output. Despite these efforts, still missing is a systematic evalua- tion of HPO algorithms across diverse datasets under realis- tic conditions where optimized configurations are tested on held-out sets. This gap is the primary focus of our work. Building upon existing approaches, our evaluation priori- tizes HPO algorithms already considered in the context of RAG, over introducing alternatives such as BOHB (Falkner, Klein, and Hutter 2018) or SMAC (Lindauer et al. 2022). Specifically, the greedy algorithms we explore resemble those used by Kim et al. (2024), and the TPE algorithm we assess is the same one used by RAGBuilder. 3 Experimental Setup Search space In our explored RAG pipeline (see Figure 1), process- ing starts withchunkingthe input documents into smaller chunks, based on two parameters: thesizeof each chunk in tokens, and theoverlapbetween consecutive chunks. This is followed by representing each chunk with a dense vec- tor created by anembeddingmodel â€“ our third parame- ter. The vectors are stored in a vector-database, 6 alongside their original text. Upon receiving a query, the topkrelevant chunks are retrieved (retrieval);kbeing our fourth param- eter. Lastly, a