the maximum standard error (SE) observed across all configurations. [0.0 0.1) [0.1 0.2) [0.2 0.3) [0.3 0.4) [0.4 0.5) [0.5 0.6) [0.6 0.7) [0.7 0.8) [0.8 0.9) [0.9 1.0] LLMaaJ-AC 0 5 10 15 20 25% of RAG configurations Dataset ClapNQ BioASQ AIArxiv MiniWiki WatsonxQA Figure 2: The distribution of configurations across bins for the normalized LLMaaJ-AC metric on the development sets. Most datasets have a few top-performing configurations. 4 Results Grid Search We conducted a comprehensive grid search over all162con- figurations (including18different indexes), across the de- velopment and test sets from all datasets. The worst and best performing configuration scores for each dataset, on the de- velopment set, are presented in Table 4.13 There is a substan- tial gap between the two extremes. The exhaustive grid search enables a deeper analysis of the configuration landscape, including the proportion of high and low performing configurations. To quantify this, we computed a min-max normalized score per dataset and met- ric, binned the scores uniformly, and assigned each config- uration to a bin by its normalized metric score. Figure 2 shows the distribution of configurations across bins for the LLMaaJ-AC metric. Notably, for most datasets, there are a few top-performing configurations. One example is the BioASQ dataset with fewer then5%of configurations in the top two bins. In contrast, the MiniWiki dataset exhibits a dense cluster of good configurations, suggesting that a good configuration will be easy to find. These trends exemplify that the difficulty of the HPO setup is dataset and metric de- pendent.14 13For results with Lexical-FF see Appendix D. 14For results with Lexical-AC and Lexical-FF see Appendix D. The grid results serve to establish two important perfor- mance baselines. The first is the performance of the best con- figuration selected directly from test set evaluation (dashed black lines in Figure