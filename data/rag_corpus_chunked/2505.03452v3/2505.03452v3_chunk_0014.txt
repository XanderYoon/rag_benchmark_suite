the difficulty of the HPO setup is dataset and metric de- pendent.14 13For results with Lexical-FF see Appendix D. 14For results with Lexical-AC and Lexical-FF see Appendix D. The grid results serve to establish two important perfor- mance baselines. The first is the performance of the best con- figuration selected directly from test set evaluation (dashed black lines in Figure 3). The second is the best configuration chosen based on development set evaluation, and evaluated on the test set (red lines). The gap between these baselines reflects the inherent challenge of generalization. Since HPO algorithms operate solely on the development set, their real- istic target is the second baseline. The grid search results also reveal the impact of the dif- ferent RAG pipeline parameters on the measured RAG per- formance. Overall, we see that almost all of the parameter choices have some effect on performance, however in our experiments the choice of generative model had the largest impact on the eventual answer correctness. For a detailed statistical analysis of the impact of the different RAG pa- rameters, refer to Appendix C. HPO Results Figure 3 presents the per-iteration performance of the HPO algorithms on the test sets, when optimizing for the lexical and LLMaaJ-based answer correctness metrics.15 Across all datasets and metrics, the results consistently show that ex- ploring around10configurations suffices to match the per- formance of a full grid search over all162configurations. This is a strong result, demonstrating the robustness of HPO over diverse domains and evaluation metrics. Also evident is that the difficulty of the optimization prob- lem varies between datasets. For instance, in ClapNQ con- vergence is rather slow. For MiniWiki, which is rich in good performing configurations (see Figure 2), finding a top con- figuration is easier, with an effective HPO algorithm such as Greedy-M, as