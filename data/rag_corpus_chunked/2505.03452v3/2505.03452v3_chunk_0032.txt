for each dataset and metric we report the marginal means for each chosen pipeline parameter, and their delta from the overall mean metric result. As can be seen in Tables 16-20, the largest differences in the metric scores relate to the choice of generative model. In addition, the relative success of the different generative models de- pends on the chosen metric, as also shown in Figure 4. We conduct all analyses using thestatsmodelspython li- brary (v0.14.6). Parameters and marginal means were es- timated using Restricted Maximum Likelihood (REML). For the likelihood-ratio-testing of effect significance, mod- els were compared using Maximum Likelihood (ML). D Additional results See §4 for a discussion of the main results. • Table 5 shows the worst and best per-dataset Lexical-FF metric scores on the development set. • Figure 6 depicts the percentage of good and bad con- figurations for the five datasets and the Lexical-AC and Lexical-FF metrics. • Figure 7 details HPO results for the Lexical-FF metric. [0.0 0.1) [0.1 0.2) [0.2 0.3) [0.3 0.4) [0.4 0.5) [0.5 0.6) [0.6 0.7) [0.7 0.8) [0.8 0.9) [0.9 1.0] Lexical-AC 0 5 10 15 20 25 30 35% of RAG configurations Dataset ClapNQ BioASQ AIArxiv MiniWiki WatsonxQA [0.0 0.1) [0.1 0.2) [0.2 0.3) [0.3 0.4) [0.4 0.5) [0.5 0.6) [0.6 0.7) [0.7 0.8) [0.8 0.9) [0.9 1.0] Lexical-FF 0 5 10 15 20% of RAG configurations Dataset ClapNQ BioASQ AIArxiv MiniWiki WatsonxQA Figure 6: The percentage of RAG configurations assigned to each bin of normalized metric scores (with the Lexical-AC or Lexical-FF metrics), on the dev sets. E Embedding and Generation Costs Figure 8 details accumulated numbers of (a) embedded to- kens and (b) number of tokens used in generation part for HPO algorithms overall tested configurations. See §4 for a discussion of costs. F Hardware and