boosts accuracy by up to 12.97% and cuts latency by 51% compared to conventional RAG systems, setting a new standard for single-agent RAG performance [15]. Although pre-trained language models are good at storing knowledge, they have trouble updating and precisely accessing it, particularly when performing knowledge-intensive operations. To overcome this, RAG models integrate non-parametric memory, such as a vector index of Wikipedia, that is retrieved by a neural retriever with pre-trained parametric memory. This method enhances the modelâ€™s capacity to produce precise and accurate answers. There are two primary RAG approaches: one modifies the retrieved passages per token, while the other uses the same passages throughout the output. Optimized RAG models have demonstrated improved generation quality and factual correctness on open-domain QA tasks, outperforming task-specific architectures and conventional parametric models [3]. 3.2 Multi-Agent Systems in AI By focusing on relevant feedback and multi-agent communication patterns, this work examines how large language models (LLMs) might improve Retrieval-Augmented Generation (RAG) systems for technical support. Larger models outperform traditional approaches in determining relevance, according to experiments with LLMs such as GPT-4, GPT-3.5-turbo, and Llama3. However, smaller models, such as Llama3, produced comparable findings when given appropriate prompts. Although they required more computing power, patterns like Reflection and Planning improved accuracy for multi-agent communication by 55% compared to simpler techniques. This study demonstrates how LLMs enhance RAG systems while balancing quality and efficiency [11]. The study also examines how to use Large Language Models (LLMs) to enhance 6G communication systems while addressing limitations in logic, data protection, and refinement. Three elements of a proposed multi-agent system improve LLMs: multi-agent evaluation and reflection (MER) to evaluate and improve results; multi-agent data retrieval 6 (MDR) to increase knowledge; and multi-agent collaborative planning (MCP) to provide solutions. This method demonstrates how multi-agent collaborations can enhance LLM performance