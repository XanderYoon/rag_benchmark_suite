applications. arXiv preprint arXiv:2402.07927, version 1, 2024. https://doi.org/10.48550/arXiv.2402.07927. [17] Fouad Trad and Ali Chehab. Prompt engineering or fine-tuning? A case study on phishing detection with large language models. Machine Learning and Knowledge Extraction , 6(1):367–384, 2024. https://doi.org/10. 3390/make6010018. [18] Zijian Yang and Noémi Ligeti-Nagy. Improve performance of fine-tuning language models with prompting. Infocommunications Journal, 15:62–68, 2023. https://doi.org/10.36244/ICJ.2023.5.10. [19] Yuanyuan Liang, Keren Tan, Tingyu Xie, Wenbiao Tao, Siyuan Wang, Yunshi Lan, and Weining Qian. Aligning large language models to a domain-specific graph database. arXiv preprint arXiv:2402.16567, version 1, 2024. https://doi.org/10.48550/arXiv.2402.16567. [20] Daniel Glake, Felix Kiehn, Mareike Schmidt, Fabian Panse, and Norbert Ritter. Towards polyglot data stores – overview and open research questions. arXiv preprint arXiv:2204.05779, version 1, 2022. https://doi.org/ 10.48550/arXiv.2204.05779. 15 [21] Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427 , version 3, 2024. https://doi.org/10.48550/arXiv. 2309.02427. [22] Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent LLM agents. arXiv preprint arXiv:2306.03314, version 1, 2023. https://doi.org/10.48550/arXiv.2306.03314. [23] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V . Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges. arXiv preprint arXiv:2402.01680, version 2, 2024. https://doi.org/10.48550/arXiv.2402.01680. 16