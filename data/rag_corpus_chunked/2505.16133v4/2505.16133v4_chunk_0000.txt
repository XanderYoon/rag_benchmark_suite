arXiv:2505.16133v4 [cs.IR] 3 Jun 2025 HASH-RAG: Bridging Deep Hashing with Retriever for Efficient, Fine Retrieval and Augmented Generation Jinyu Guo1, Xunlei Chen 1,*, Qiyang Xia 1,∗, Zhaokun Wang 1, Jie Ou 1 Libo Qin2, Shunyu Yao3, Wenhong Tian1† 1University of Electronic Science and Technology of China 2 Central South University 3 Big data and artificial intelligent institute, China Telecom Research Institute guojinyu@uestc.edu.cn Abstract Retrieval-Augmented Generation (RAG) en- counters efficiency challenges when scaling to massive knowledge bases while preserving con- textual relevance. We propose Hash-RAG, a framework that integrates deep hashing tech- niques with systematic optimizations to ad- dress these limitations. Our queries directly learn binary hash codes from knowledgebase code, eliminating intermediate feature extrac- tion steps, and significantly reducing storage and computational overhead. Building upon this hash-based efficient retrieval framework, we establish the foundation for fine-grained chunking. Consequently, we design a Prompt- Guided Chunk-to-Context (PGCC) module that leverages retrieved hash-indexed propositions and their original document segments through prompt engineering to enhance the LLM’s con- textual awareness. Experimental evaluations on NQ, TriviaQA, and HotpotQA datasets demon- strate that our approach achieves a 90% reduc- tion in retrieval time compared to conventional methods while maintaining considerate recall performance. Additionally, the proposed sys- tem outperforms retrieval/non-retrieval base- lines by 1.4-4.3% in EM scores. Code avail- able at https://github.com/ratSquealer/ HASH-RAG. 1 Introduction In the era of rapidly expanding data, an increasing number of downstream tasks rely on large language models (LLMs). Within these tasks, Retrieval- Augmented Generation (RAG) is a popular tech- nical framework that incorporates external knowl- edge sources to tackle knowledge-intensive prob- lems (Lewis et al., 2020). By combining a non- parametric retrieval module with the main model, RAG effectively alleviates hallucination issues in large models (Yao et al., 2022; Bang et al., 2023). Moreover, this retrieval-and-generation mechanism ∗These authors contributed equally