(Robertson and Walker, 1997), BM25 (Robert- son et al., 2009)) before transitioning to vector- ized representations (Karpukhin et al., 2020), en- abling end-to-end tunable systems with context- aware retrieval. Recent efforts focus on two phases: pre-retrieval query-data matching to enhance pre- cision (Ma et al., 2023) and post-retrieval content re-ranking or transformation to optimize generator input (Glass et al., 2022). A persistent challenge lies in chunking strategy design, balancing gran- ularity trade-offs: coarse chunks risk redundancy despite contextual richness (Shi et al., 2023), while fine-grained units sacrifice semantic completeness for precision (Raina and Gales, 2024). Scalability- induced efficiency bottlenecks in retrieval optimiza- tion now critically constrain RAG advancement. 2.2 Deep Hashing Methods Approximate Nearest Neighbor (ANN) algorithms address large-scale search inefficiency by trading exact precision for efficiency. Unlike tree-based (Annoy (Bernhardsson, 2015)), quantization-based (PQ (Jegou et al., 2010)), or graph-based (HNSW (Malkov and Yashunin, 2018)) methods, hashing reduces memory requirements and search latency while preserving vector locality, which makes it a mainstream ANN solution. Early techniques like Locality-Sensitive Hashing (LSH) (Charikar, 2002; Indyk and Motwani, 1998) relied on predefined mappings to hash buckets, requiring multiple tables for satisfactory recall. Learning-to-hash methods (e.g., Spectral Hashing (Weiss et al., 2008), Se- mantic Hashing (Salakhutdinov and Hinton, 2009)) later optimized hash functions to improve retrieval efficiency and accuracy. Current research focuses on deep-supervised hashing methods (e.g., Con- volutional neural network hashing (CNNH) (Xia et al., 2014): two-phase of binary codes generation and convolutional neural networks (CNNs) training, Deep supervised hashing (DSH) (Liu et al., 2016): pairwise similarity loss with regularization for end- to-end training, Maximum-margin hamming hash- ing (MMHH) (Kang et al., 2019): discriminative enhancement through t-distribution and semi-batch optimization). These methods demonstrate adapt- ability to large-scale retrieval, establishing techni- cal foundations for optimizing RAGâ€™s chunking strategies and retrieval efficiency. 3 Method In