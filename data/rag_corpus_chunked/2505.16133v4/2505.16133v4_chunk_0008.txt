negative (Si,j = −1) sam- ples. We minimize L2 loss between binary code inner products and supervision information: LHbE = mX i=1 nX j=1 h fhqi T hpj − lSij i2 (3) where hp denotes the proposition hash code. For proposition-only training data ∆p = {qj}n j=1, we construct simulated queries by randomly sampling m propositions from all proposition indices set Γ = {1, 2, . . . , n}, forming index subset Ω = {i1, i2, . . . , im} ⊆ Γ. The extended loss function is as follows: LHbE = X i∈Ω X j∈Γ  tanh(βvpi)T hpj − lSij 2 + γ X i∈Ω  hpj − tanh(βvpi) 2 (4) where γ constrains hpi and fhpi = tanh( βvpi) to be as close as possible, which enables effective optimization of proposition hash codes through iterative parameter updates. We implement an alternating optimization strat- egy, alternately fixing and updating the neural net- work parameters θ and the proposition sentence hash code matrix H. Update θ with H fixed With H fixed, we com- pute the gradient of LHbE w.r.t. vpi: ∂LHbE ∂vpi = ( 2 X j∈Γ h fhpi T hpj − lSij  hpj i + 2γ  fhpi − hpi ) ⊙  1 −fhpi 2 (5) The chain rule propagates this gradient through BERT’s parametersθ, which are then updated via backpropagation. Update H with θ fixed With θ fixed, we rewrite Equation 4 in matrix form: LHbE = ∥eV H T − lS∥2 F + γ∥HΩ − eV ∥2 F = ∥eV H T ∥2 F − 2ltr(H T STeV ) − 2γtr(HΩeV T ) + const (6) where eV = [fvp1,fvp2, . . . ,gvpm]T ∈ [−1, +1]m×l, and HΩ = [hp1, hp2, . . . , hpm]T denotes the sam- pled proposition hash