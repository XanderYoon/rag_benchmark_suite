2024) employs the clusters of Residual Quantiza- tion (RQ) to search for documents semantically; LSH’s (Charikar, 2002) hash-bucket mapping re- duces the scope of nearest neighbor search; DSH (Liu et al., 2016) integrates deep feature extraction with semantic label optimization in hash space. 4.2 Main Result Table 1 illustrates HbR’s recall@k (k ∈ 5, 20, 100) and latency on NQ, TQA, and HotpotQA bench- marks. Our framework reduces query latency to one-tenth of conventional retrievers while achiev- ing 0.2-8.6% higher recall@20/100. Notably, HbR achieves optimal performance at k = 20. While H- RAG underperforms MEVI at k = 5, such small-k Model Top 5 Top 20 Top 100 ALBERT 63.2 78.1 82.4 Bert-base 72.4 80.3 87.5 RoBERTa 72.6 84.7 87.6 Bert-large 73.1 85.8 87.9 Table 3: Top k recall of HbR using different versions of embedding models on NQ Dataset. scenarios are secondary since users typically re- quire ≥20 passages for answer generation. The hashing mechanism maintains index size advan- tages despite data volume increases from chunking. Next, Table 2 compares Hash-RAG with base- line RAG systems using LLAMA2-7B/13B. Our framework outperforms retrieval-optimized meth- ods (FILCO (Wang et al., 2023), RRR (Ma et al., 2023)) and LLAMA-Retrieval hybrids (REPLUG (Shi et al., 2024)), with all retrieval-augmented models surpassing Toolformer’s non-retrieval base- line (Schick et al., 2023). By feeding top-20 results to generators, we optimally balance context volume and generation quality. This design choice lever- ages the complementary strengths of hashing-based retrieval and modern LLMs, demonstrating signifi- cant EM improvements across all benchmarks. 4.3 Ablations Encoder Version We investigated the com- patibility of various encoder versions (ALBERT, Bert-base, Bert-large, RoBERTa) with our model. As shown in Table 3, proposition-level chunk- ing achieves significantly superior retrieval perfor- mance compared to sentence-level and paragraph- level strategies in terms of Recall@20 metrics. Al- though