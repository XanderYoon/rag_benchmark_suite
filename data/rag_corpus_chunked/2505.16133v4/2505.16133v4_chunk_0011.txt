Optimization We employ LLAMA2 as the generator with optimized prompts. The Hash-Retriever identifies top-j propositions Pj = {P1, . . . , Pj} and their corresponding document indices Dock, forming the generator’s context through three key components: (1) Additional Prompt instructing semantic integration of propo- sitions and indexed documents for precise re- sponses, (2) Retrieved Segments containing similarity-ranked propositions Pj with document references Docj, and (3) Indexed Documents Doc1, . . . , Dock providing contextual grounding. The prompt template activates the generator’s capability through chunk-to-context: propositions supply direct evidence while documents offer broader context, enabling accurate intent under- standing with balanced retrieval-context integra- tion. For details, see appendix A. 4 Experiment 4.1 Experimental Settings Datasets and Retrieval Corpus We evaluated our model on three QA benchmarks using development sets. These datasets contain Wikipedia and web- sourced questions, representing diverse knowledge- intensive tasks: NQ (Kwiatkowski et al., 2019) and TRIVIAQA (Joshi et al., 2017) assess direct knowledge recall, while HOTPOTQA (Yang et al., 2018) requires multi-hop reasoning across docu- ments. Different retrieval granularities from sen- tences to full documents refer to Appendix B. Metrics With more retrieval units, we retrieve additional propositions, map them to source doc- uments, deduplicate, and return the top k unique documents. We evaluate using document recall@k and retrieval efficiency (index size/query time). For generation, Exact Match (EM) assesses whether the ground truth appears exactly in the output. Implementation Details In our paper, the en- coders (Embedding) utilize BERT base, large, AL- Model Top 5 Top 20 Top 100 Index Query NQ TQA HQA NQ TQA HQA NQ TQA HQA size time BM25 45.2 55.7 - 59.1 66.9 - 73.7 76.7 - 7.4 913.8 SimCSE 28.8 44.9 26.7 44.3 59.4 44.1 47.0 62.4 46.1 64.6 548.2 Contriever 47.8 59.4 42.5 67.8 74.2 67.4 82.1 83.2 76.9