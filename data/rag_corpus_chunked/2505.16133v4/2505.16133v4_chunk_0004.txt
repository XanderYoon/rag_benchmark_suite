their con- texts through specifically designed prompts to gen- erate, achieving optimal coordination between ac- curacy and efficiency. We conducted experiments on open-domain question-answering datasets, including Natural Questions (NQ) (Kwiatkowski et al., 2019), TRIV- IAQA (Joshi et al., 2017), and the more complex multi-hop HOTPOTQA (Yang et al., 2018). Exper- imental results show that our model significantly reduces retrieval time, which requires only 10% of the time needed for conventional retrieval methods while maintaining advanced recall performance. In combination with the PGCC module, we have achieved a performance increase in the generation task while retaining efficiency. Main contributions of this paper are as follows: 1. We propose HASH-RAG, a framework that systematically integrates deep hashing into RAG with stage-wise optimizations. This approach significantly enhances computa- tional efficiency in large-scale knowledge re- trieval, thereby accelerating end-to-end infer- ence throughout the overall RAG. 2. Building upon our hash-based efficient re- trieval framework, we propose the PGCC module that enables fine-grained retrieval while enhancing contextual information through prompt-based optimization. 3. Experimental results on multiple datasets demonstrate that HASH-RAG significantly improves the efficiency of the retrieval. With PGCC module, our method surpasses RAG baseline models in overall performance, achieving optimal coordination between effi- ciency and performance. 2 Related Work 2.1 Retrieval-Augmented Generation RAG mitigates LLM hallucinations through non- parametric memory integration and compensates for factual deficiencies via external knowledge retrieval (Gao et al., 2023). Early implementa- tions relied on statistical similarity metrics (TF- IDF (Robertson and Walker, 1997), BM25 (Robert- son et al., 2009)) before transitioning to vector- ized representations (Karpukhin et al., 2020), en- abling end-to-end tunable systems with context- aware retrieval. Recent efforts focus on two phases: pre-retrieval query-data matching to enhance pre- cision (Ma et al., 2023) and post-retrieval content re-ranking or transformation to optimize generator input (Glass et al., 2022).