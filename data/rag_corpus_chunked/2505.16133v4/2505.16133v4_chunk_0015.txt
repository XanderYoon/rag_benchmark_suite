retrieval and modern LLMs, demonstrating signifi- cant EM improvements across all benchmarks. 4.3 Ablations Encoder Version We investigated the com- patibility of various encoder versions (ALBERT, Bert-base, Bert-large, RoBERTa) with our model. As shown in Table 3, proposition-level chunk- ing achieves significantly superior retrieval perfor- mance compared to sentence-level and paragraph- level strategies in terms of Recall@20 metrics. Al- though BERT-large achieved the highest Top-k re- call rates, to ensure a fair comparison with existing models that employ BERT-base as their encoder architecture, we adopt the identical configuration in our implementation. Chunk Strategy The performance hierarchy in Table 4 demonstrates proposition-level chunking surpassing sentence- and paragraph-level strate- gies on Recall@20 metrics respectively. Exper- imental analysis reveals sentence-level segmenta- tion fractures predicate-argument coherence, while paragraph-level processing incorporates extrane- ous content. The performance hierarchy reflects proposition-level chunking’s dual advantage: pre- serving self-contained semantic units and system- atically eliminating contextual noise. Chunk Strategy Recall@20 sentence 62.9 paragraph 68.8 prop. 80.2 Prompt Optimization EM HbR w/o prop. 25.3 HbR w/o doc. 24.8 HbR 29.4 HbR w/ prompt (Ours) 31.1 Table 4: Metrics (Recall@20 and EM) of different chunk strategies and prompt optimizations on HotpotQA dataset, with prop. denoting the propositions and doc. representing the original source documents associated with the retrieved propositions. Prompt Optimization Table 4 presents our com- parative analysis of prompt optimization strate- gies, where the PGCC module empirically demon- strates superior EM performance over all base- lines. Notably, the w/o doc. configuration out- performs w/o prop., suggesting that in multi-hop datasets under Recall@20 settings, self-contained propositions still require cross-verification with source documents even when guided by optimized prompts. The non-prompted configuration achieves secondary performance due to sufficient contextual data supporting LLM reasoning, while prompt in- tegration enhances the model’s focus on retrieved results through structured attention guidance. 5 Analysis 5.1 Chunk-Unit