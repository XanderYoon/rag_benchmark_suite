neural networks (CNNs) training, Deep supervised hashing (DSH) (Liu et al., 2016): pairwise similarity loss with regularization for end- to-end training, Maximum-margin hamming hash- ing (MMHH) (Kang et al., 2019): discriminative enhancement through t-distribution and semi-batch optimization). These methods demonstrate adapt- ability to large-scale retrieval, establishing techni- cal foundations for optimizing RAG’s chunking strategies and retrieval efficiency. 3 Method In this study, we propose a Hash Retrieval- Augmented Generation (Hash-RAG) framework, a pipeline that enhances RAG from the perspectives of deep hashing and contextual enhancement. Fig- ure 2 illustrates the comprehensive architecture of Hash-RAG. Sections 3.1 and 3.2 respectively intro- duce the Hash-Based Retriever (HbR) and the mod- ule Prompt-Guided Chunk-to-Context (PGCC). 3.1 Hash-Based Retriever Hash-based Encoder (HbE) HbE module com- prises two components: a query encoder Eq and a proposition encoder Ep, which generate binary hash codes for queries q and propositions p (de- rived from knowledge base document segmenta- tion), respectively. Inspired by ADSH (Jiang and Li, 2018), we design asymmetric encoding strate- gies for queries and propositions. The query en- coder Eq employs BERT-base-uncased (Kenton and Toutanova, 2019) as its embedding model to map queries into a d-dimensional vector space (d = 768). The query embedding vector vq ∈ Rd: vq = BERT (q, θ) (1) where θ denotes BERT’s parameter. Subsequently, the binary hash code of query q is computed through the sign function in the hashing layer: hq = sign(vq). To address the vanishing gradi- ent problem of the sign function, we approximate it using a scaled tanh function (Cao et al., 2017). ehq = tanh(βvq) ∈ {−1, 1}l (2) where l is the fixed hash code length (768 bits) and β is the scaling hyperparameter controlling approx- imation smoothness (β → ∞ converges to the sign function). We set σ = 0.1 and β