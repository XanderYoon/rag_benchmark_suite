form: LHbE = ∥eV H T − lS∥2 F + γ∥HΩ − eV ∥2 F = ∥eV H T ∥2 F − 2ltr(H T STeV ) − 2γtr(HΩeV T ) + const (6) where eV = [fvp1,fvp2, . . . ,gvpm]T ∈ [−1, +1]m×l, and HΩ = [hp1, hp2, . . . , hpm]T denotes the sam- pled proposition hash codes. To update H, we adopt a column-wise updating strategy. For the k-th column H ∗k with residual matricescH k (excluding column k), and the k-th column gV ∗k with residual matrices bV k (excluding column k), the optimisation objective function is: L(H ∗k) =tr  H ∗k h 2gV ∗k TbV kdH k T + QT ∗k i + const (7) where Q = −2lSTeV −2γV , with the k-th column Q∗k. The optimal solution is: H ∗k = −sign  2cH kcV k TgV ∗k + Q∗k  (8) The alternating optimization between θ and H drives gradual convergence through multiple itera- tions, ultimately producing an effective query hash function and robust proposition hash code. 3.2 Prompt-Guided Chunk-to-Context Retrieval Unit Granularity We employ the in- formation bottleneck theory (Tishby et al., 2000) to optimize retrieval unit selection, where proposition- based chunks preserve maximal generator-relevant information while minimizing noise. Given the joint probability distribution p(X, Y ) between doc- ument X and generator output Y , we quantify the information content about Y contained within com- pressed proposition eX through mutual information: I(eX; Y ) = Z eX Z Y p(ex, y) log p(ex, y) p(ex)p(y) dexdy (9) The compression objective minimizes LIB = I(eX; X) − βI(eX; Y ), where the Lagrange multi- plier β balances information retention and compres- sion. Unlike conventional sentence/paragraph units (Karpukhin et al., 2020), we adopt proposition units (Min et al., 2023) that