pervised hashing framework. This approach marks an unprecedented breakthrough in large-scale im- age retrieval and significantly outperforms conven- tional method (Chen et al., 2021), which also pro- vide a novel perspective for optimizing RAG effi- ciency. In the era of RAG, the knowledge bases signifi- cantly surpass traditional image retrieval datasets in scale and growth velocity. In light of this, ANN techniques exemplified by hashing demonstrate sig- nificant potential for RAG applications with their capacity to rapidly target results and reduce compu- tational complexity in large-scale data processing. In this paper, we introduce ANN-based tech- niques into RAG frameworks and propose Hash- RAG through systematic integration of deep hash- ing methods. Specifically, our architecture con- verts query embeddings into binary hash codes via sign function operations. For knowledge bases, we adopt an asymmetric processing strategy to opti- mize training efficiency by directly learning binary hash codes without feature learning. Based on this, we achieve fine-grained retrieval through corpus chunking, which filters redundant content while preserving precision. Nevertheless, we notice that existing chunking approaches result in retrieved segments lacking essential contextual information, which substantially degrades the quality of gener- ated outputs. To address this, we also propose a Prompt-Guided Chunk-to-Context (PGCC) mod- ule, which splits documents into factual fragments (i.e., propositions) as retrieval units. These propo- sitions are structured in a concise, self-contained natural language format and indexed to their origi- nal documents. During generation, LLM processes hash-based retrieved propositions and their con- texts through specifically designed prompts to gen- erate, achieving optimal coordination between ac- curacy and efficiency. We conducted experiments on open-domain question-answering datasets, including Natural Questions (NQ) (Kwiatkowski et al., 2019), TRIV- IAQA (Joshi et al., 2017), and the more complex multi-hop HOTPOTQA (Yang et al., 2018). Exper- imental results show that our model significantly reduces retrieval time,