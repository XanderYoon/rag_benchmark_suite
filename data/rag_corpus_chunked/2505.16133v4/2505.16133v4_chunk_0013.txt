55.0 19.4 27.3 60.4 23.9 REPLUG♦ 27.1 57.1 20.5 29.4 62.7 26.8 Hash-RAG 28.5 ±0.1 57.1±0.1 22.1±0.2 34.9±0.2 64.5±0.1 31.1±0.3 Table 2: EM of open-domain QA. ♢ Generation models in this experiment involve the GPT series, all of which are modified to the LLAMA2 series and w/o train reader in this experiment. ♦ Contriever and a zero-shot setting are selected. BERT, and ALBERT, with each model initialized using the official pre-trained weights. The number of top j propositions is fixed at 100. The generator (generator) LLMs include LLaMA2-7B and 13B (Touvron et al., 2023). For the HbR model used in our primary experiments, the training batch size is set to 128, with one additional BM25 negative pas- sage per question. Each encoder is trained for 40 epochs, employing linear scheduling with warm-up and a dropout rate of 0.1. Baselines We compare HbR with BM25 (Robert- son et al., 2009), DPR (Karpukhin et al., 2020), SimCSE (Gao et al., 2021), Contriever (Izac- ard et al., 2021), Model-enhanced Vector Index (MEVI) (Zhang et al., 2024), LSH (Charikar, 2002), and DSH (Liu et al., 2016). BM25 (Robertson et al., 2009) employs TF- IDF principles for document relevance ranking, while DPR (Karpukhin et al., 2020) utilizes a dual- encoder architecture; SimCSE (Gao et al., 2021), an unsupervised learning architecture, optimizes semantic representations via positive/negative pair discrimination; Contriever (Izacard et al., 2021) employs Transformer-based encoder and optimize a contrastive loss function; MEVI (Zhang et al., 2024) employs the clusters of Residual Quantiza- tion (RQ) to search for documents semantically; LSH’s (Charikar, 2002) hash-bucket mapping re- duces the scope of nearest neighbor search; DSH (Liu et al., 2016) integrates deep feature extraction with semantic label optimization in hash space. 4.2 Main Result Table 1 illustrates HbR’s recall@k (k ∈ 5, 20, 100) and latency on NQ,