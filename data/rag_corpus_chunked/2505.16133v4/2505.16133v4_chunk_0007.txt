ent problem of the sign function, we approximate it using a scaled tanh function (Cao et al., 2017). ehq = tanh(βvq) ∈ {−1, 1}l (2) where l is the fixed hash code length (768 bits) and β is the scaling hyperparameter controlling approx- imation smoothness (β → ∞ converges to the sign function). We set σ = 0.1 and β = √σ · step + 1, where step counts completed training steps. The proposition encoder Ep directly learns binary hash codes without embedding model training through specialized loss functions and an alternating optimization strategy to reduce training overhead. For training data ∆ = {⟨qi, p+ i , {p− i,j}n j=1⟩}m i=1 containing m instances, the supervision matrix S ∈ {−1, 1}m×(n+1) labels Similarity information (B) Inference Asymmetric Pairwise Loss (A) Training Knowledgebase Code HbE Hash-Based Retriever Hash Code Retrieve Embedding Model Hash Layer Hash-Based Encoder Query Query Hash Code Propositionizer Propositions Documents Directly Learn Knowledgebase Code Doc. Corresponding Index Prompt Prop. Answer Generator Prompt-Guided Chunk-to-Context Figure 2: Framework Overview. (a) Training. The hash-based encoder generates compact query hash codes, while the knowledge base creates binarized propositional codes from factually chunked corpora. Both components are jointly optimized through an asymmetric pairwise loss with similarity constraints. (b) Inference. The hash- based retriever efficiently fetches relevant propositions, augmented by indexed document references for contextual grounding. The generator synthesizes evidence from these elements using optimized prompts to produce responses. positive (Si,j = 1) and negative (Si,j = −1) sam- ples. We minimize L2 loss between binary code inner products and supervision information: LHbE = mX i=1 nX j=1 h fhqi T hpj − lSij i2 (3) where hp denotes the proposition hash code. For proposition-only training data ∆p = {qj}n j=1, we construct simulated queries by randomly sampling m propositions from all proposition indices