configuration out- performs w/o prop., suggesting that in multi-hop datasets under Recall@20 settings, self-contained propositions still require cross-verification with source documents even when guided by optimized prompts. The non-prompted configuration achieves secondary performance due to sufficient contextual data supporting LLM reasoning, while prompt in- tegration enhances the model’s focus on retrieved results through structured attention guidance. 5 Analysis 5.1 Chunk-Unit & Prompt-Guided Information Bottleneck of Proposition To demonstrate how HASH-RAG’s chunking strategy leverages the information bottleneck to enhance text generation capabilities, we analyze content preserved through document segmentation. Ex- perimental analysis based on Table 5 reveals a potential correlation between compression rates and the conciseness of conditional mutual informa- tion I( ˜X; X|Y ; Q), comparing exact and greedy search methods across context lengths for gener- ator, mutual information metrics, and EM scores. We propose that applying information bottleneck principles to factual chunking generates concise in- termediate answer representations with supporting evidence, outperforming alternative strategies in multi-hop queries. This indicates replacing propo- sitions with complete documents would increased noise, distracted attention, and reduced storage ef- Dataset Filtering Candidates Words I( ˜X; X|Y ; Q) EM NQ Exact Paragraph-Level 78.1 0.597 21.2 Sentence-Level 28.4 0.561 23.8 Greedy Query & Answer 26.2 0.562 19 Answer 18.2 0.556 24.3 Exact Proposition-Level 33.6 0.594 27.4 HotpotQA Exact Paragraph-Level 120.0 0.679 26.3 Sentence-Level 41.2 0.619 27.8 Greedy Query & Supporting Facts & Answer 32.5 0.614 25.8 Supporting Facts & Answer 14.8 0.604 26.9 Exact Proposition-Level 42.6 0.679 28.7 Table 5: The effectiveness of the information bottleneck theory on the filtering data compression rate and concise mutual information in the test sets of the PGCC module for NQ and HotpotQA. The proposition-level chunking in Hash-RAG, combined with the PGCC module guiding LLMs to focus on the correspondence between propositions and documents, is theoretically optimized and experimentally