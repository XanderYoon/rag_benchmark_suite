on average. This validates that the chain-of-retrieval mechanism 17 Table 9: Extension to the Qwen3 model families. 2WikiQA HotpotQA Bamboogle MuSiQue EM F1 EM F1 EM F1 EM F1 Fine-tuned Qwen3-4B w/ E5large 49.3 55.3 45.0 57.9 32.8 43.1 13.4 23.8 CoRAG-Qwen3-4B (L=6, greedy)69.3 74.1 51.6 64.2 49.6 62.5 24.0 34.5 Fine-tuned Qwen3-8B w/ E5large 52.1 57.9 47.1 60.0 33.6 47.6 15.3 26.3 CoRAG-Qwen3-8B (L=6, greedy)70.0 74.8 52.8 66.0 49.6 63.7 25.2 35.9 is broadly applicable across different model architectures and confirms the generalizability of our approach beyond specific model families. Case AnalysisTable 11 presents several model predictions on the validation set of the HotpotQA dataset. We compare the performance of RAG without chain-of-retrieval against CoRAG. CoRAG effectively decompose the complex multi-hop queries into a sequences of simpler sub-queries and dynamically conducts query reformulation when the retrieved information proves unhelpful. In the fourth example, the model initially hallucinates some incorrect information but subsequently self-corrects by verifying the poetâ€™s name and country of origin through additional retrieval steps. C Statistical Significance We compute the 95% confidence intervals for our main results in Table 1 and 2 using the bootstrap resampling method. On all datasets except the Bamboogle dataset, we observe that the performance of CoRAG-8B is significantly better than the baselines. D Prompts Table 10: Task descriptions for each dataset. Dataset Task Description HotpotQA / 2WikiMulti- hopQA answer multi-hop questions NQ answer natural questions from Google search AidaYago 2 / WnWi / WnCw / Blink link the mention surrounded by [START_ENT] and [END_ENT] to the title of the correct Wikipedia page FEVER verify if the claim is supported or refuted T-REx / Zero-Shot RE given head entity and relation separated by [SEP], find the correct tail entity, return the title of its Wikipedia page Trivia QA answer trivia questions MuSiQue /