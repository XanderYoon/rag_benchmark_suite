generate retrieval chains. “Weak-to-strong Generalization” utilizes weaker LLMs for retrieval chain generation while using stronger LLMs (Llama-3.1-8B-Inst.) for training. “Different Retrievers” replaces the text retriever at test time. 2WikiQA HotpotQA Bamboogle MuSiQue EM F1 EM F1 EM F1 EM F1 CoRAG-8B (L=6, greedy) 70.6 75.5 54.4 67.5 48.0 63.5 27.738.5 ▷iterative training 72.2 76.9 53.4 66.5 45.6 60.9 26.6 37.6 ▷distill from GPT-4o75.1 79.5 56.6 70.2 51.2 67.0 28.2 38.5 Weak-to-strong Generalization w/ Llama-3.2-1B-Inst. 59.3 64.2 50.3 63.6 40.8 51.6 22.3 32.7 w/ Llama-3.2-3B-Inst. 69.9 74.0 53.9 67.3 45.6 59.8 25.2 36.0 Different Retrievers E5-base w/o chain-of-retrieval 53.1 58.9 47.9 61.1 38.4 52.7 15.8 26.4 ▷L=6, best-of-470.8 75.4 53.0 66.2 47.2 59.8 26.3 37.6 BM25 w/o chain-of-retrieval 49.1 55.3 46.9 60.3 36.8 48.6 14.3 24.8 ▷L=6, best-of-462.6 67.7 51.6 64.7 37.6 52.5 23.5 33.0 5.1 Iterative Rejection Sampling Our framework facilitates self-improvement through iterative training, akin to the iterative rejection sampling employed in LLM post-training [ 5]. By utilizing the same prompt templates for both data generation and model training, a trained CoRAG model can generate new sets of retrieval chains. However, the results in Table 3 are mixed, showing performance improvements on the 2WikiMultihopQA dataset but slight declines on other datasets. This indicates that instruction-tuned LLMs already possess a strong ability to generate high-quality retrieval chains. 5.2 Robustness and Generalization Different RetrieversWe further investigate the influence of various text retrievers at test time. Instead of using the E5-large dense retriever, we substitute it with two weaker alternatives in a plug-and-play fashion: E5-base and BM25. Across all datasets, we observe consistent performance gains when investing more test-time compute, although stronger retrievers continue to outperform in terms of absolute performance. Improvements to text retriever quality represent an orthogonal dimension that can further amplify CoRAG’s performance gains. Weak-to-strong GeneralizationDue to the need