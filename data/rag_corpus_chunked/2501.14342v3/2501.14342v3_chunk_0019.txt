a model variant that learns to stop at test time. After each retrieval step, the model is prompted to predict whether the information gathered thus far suffices to answer the query. Note that this prompt itself also incurs token consumption and additional cost. The decoding space is constrained to two tokens:“Yes”and “No”. If the decoded output is “Yes”, no further sub-queries are generated. By adjusting the logit bias of the “Yes” token, we can control the early stopping behavior. During the training phase, an additional loss term is added for the stop prediction task. The target output is “Yes” if the current retrieval chain encompasses the prefix that maximizes the likelihood of the final answer, and “No” oth- erwise. The associated prompt template is in Appendix Section D. In Figure 5, we illustrate how the performance varies along with the token consumption on the MuSiQue dataset. While early stopping can save some amount of token quota, it comes at the cost of performance degradation. The optimal configuration depends on the dataset characteristics and the quality expectations. 9 5.5 Does CoRAG Learn to Retrieve Better? To evaluate whether CoRAG improves retrieval quality beyond just answer accuracy, we measure retrieval recall across multiple datasets. We report Recall@k metrics for k∈ {10,20,100} , compar- ing standard retrieval using E5large against our approach. We follow the evaluation protocol from DPR [18] for calculating recall based on answer matches, as not all datasets provide gold supporting paragraphs. For CoRAG, we utilize reciprocal rank fusion to merge multiple retrieval results from the chain into a single ranked list, from which recall is calculated. Table 4: Retrieval recall comparison between standard retrieval and CoRAG across multi-hop QA datasets. R@10 R@20 R@100 HotpotQA w/ E5large 59.1 65.2 76.8 w/ CoRAG72.1 76.7 84.3 2WikiMultiHopQA w/ E5large 54.9 62.1 74.6