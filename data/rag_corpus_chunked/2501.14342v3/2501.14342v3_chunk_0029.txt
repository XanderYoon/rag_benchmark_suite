In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html. [41] Tian Yu, Shaolei Zhang, and Yang Feng. Auto-rag: Autonomous retrieval-augmented generation for large language models.ArXiv preprint, abs/2411.19443, 2024. URL https://arxiv.org/ abs/2411.19443. [42] Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation.ArXiv preprint, abs/2410.04343, 2024. URL https://arxiv.org/ abs/2410.04343. [43] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors,Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html. A Implementation Details Rejection SamplingFor each training instance, we sample up to 16 retrieval chains, with the maximum length randomly selected from the interval [1,5] . The sampling temperature is set to 0.7 for sub-query generation and 0 for sub-answer generation. Chain generation is terminated if the sub-answer matches the correct answer or if the average conditional log-likelihood of the correct 14 answer exceeds −0.05. For each sub-query, we utilize the E5-large retriever 2 to retrieve the top-5 most relevant documents from the KILT version of the Wikipedia corpus [25]. This corpus comprises 36million passages. Table 5: Hyperparameters for training CoRAG. Multi-hop QA KILT Benchmark InitializationLlama-3.1-8B-Instruct Learning rate5×10 −6 10−5 Batch size256 1024 Epoch1 1 Warmup steps100 100 # Training samples125k660k # Retrieved passages20 20 Max sequence length3072 3072 Table 6: Statistics of the datasets used for multi-hop