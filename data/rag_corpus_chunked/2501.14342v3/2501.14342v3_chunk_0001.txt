Pareto frontier (a) Where did the star of Dark Hazard study? What was the name of the star of Dark Hazard? Edward G. Robinson Where did Edward G. Robinson go to college? No relevant information found. What college did Edward G. Robinson attend? City College of New York. City College of New York decomposition retrieval failure reformulation (b) Figure 1: (a) Test-time scaling behavior of CoRAG. Increased token budget leads to consistent performance improvements. (b) An example of CoRAG on the MuSiQue dataset. It learns to decompose the complex query and conduct query reformulation when encountering a retrieval failure. âˆ—Correspondence to wangliang@microsoft.com 39th Conference on Neural Information Processing Systems (NeurIPS 2025). arXiv:2501.14342v3 [cs.IR] 10 Oct 2025 1 Introduction Retrieval-augmented generation (RAG) [20] is one of the core techniques in enterprise applications, necessitating the integration of large foundation models with proprietary data sources to produce responses that are both grounded and factual. Conventionally, foundation models are trained on large-scale datasets comprising trillions of tokens and remain frozen post-deployment. Nonetheless, these models frequently struggle to memorize long-tail factual knowledge or may hallucinate false claims, resulting in unreliable responses in real-world scenarios. RAG mitigates this challenge by augmenting the generation process with retrieved information, thereby improving the trustworthiness of model-generated content and facilitating the incorporation of up-to-date information. Contemporary RAG systems typically employ a sequential pipeline of retrieval and generation, wherein the retrieved information serves as additional input to the generative model. The effectiveness of RAG systems predominantly relies on the quality of the retrieved information. Retrieval models are engineered for efficiency to ensure scalability to large corpora. For instance, dense retrievers [18, 35] commonly utilize a bi-encoder architecture to compress documents and queries into fixed-size vector representations. This architectural choice permits the use of fast approximate nearest neighbor search algorithms but simultaneously