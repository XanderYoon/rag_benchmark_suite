76.7 88.0 87.2 63.1 60.6 88.393.1 2 4 6 8 10 Chain Length 56 58 60 62 64 66 68 70 72EM 2WikiMultihopQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 50 51 52 53 54 55 56 HotpotQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 40 42 44 46 48 50 52 54 Bamboogle greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 18 20 22 24 26 28 30 MuSiQue greedy best-of-4 best-of-8 4k 8k 16k 32k 64k 128k # Avg. T okens 57.5 60.0 62.5 65.0 67.5 70.0 72.5EM greedy best-of-4 best-of-8 tree search pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 50 51 52 53 54 55 56 greedy best-of-4 best-of-8 tree search pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 40 42 44 46 48 50 52 54 greedy best-of-4 best-of-8 tree search pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 18 20 22 24 26 28 30 greedy best-of-4 best-of-8 tree search pareto frontier Figure 3: Scaling test-time compute on multi-hop QA datasets. The Pareto frontier is in the form of y=a×log(x+b) +c fitted on the Pareto optimal points. A point is consideredPareto optimalif no other point achieves a higher EM score with less token consumption. The metric “# Avg. Tokens” represents the average number of tokens consumed per test instance, summing up both the prompt and generated tokens. We observe that increasing the retrieval chain lengthL results in substantial performance improve- ments when L is small, but the gains diminish as L increases. This observation aligns with the intuition that longer chains can encapsulate more reasoning steps and allows for trial-and-error explo- ration of various query rewriting strategies. Several examples are provided in Appendix Table