intermediate retrieval steps, yet its performance remains significantly below that of state-of-the-art models. AQA [3] learns to reformulate questions using reinforcement learning but only focuses on single-hop QA tasks. In this study, rather than exclusively on few-shot prompting or distillation from proprietary models, we propose a novel approach to explicitly train LLMs to iteratively retrieve and reason over relevant information. Scaling Test-time ComputeInstead of prompting LLMs to directly generate the final answer, Chain- of-Thought (CoT) [ 36] demonstrates that letting the model to think step by step can drastically improve the performance on mathematical reasoning tasks. Tree-of-Thought (ToT) [ 40] extends the idea of CoT by adopting a tree structure, allowing the model to explore the search space more comprehensively. To further enhance the reasoning capabilities of LLMs, STaR [ 43] proposes to leverage bootstrapping techniques to generate intermediate states for training. OpenAI o1 [ 12] conducts large-scale reinforcement learning and exhibits promising test-time scaling behaviors on advanced reasoning datasets, but the technical details are not publicly available. A drawback of these methods is the increased token consumption, which consequently increases the response latency. In the realm of RAG, test-time compute can be increased by retrieving more documents or performing additional retrieval steps. LongRAG [14] posits that RAG performance can be enhanced by integrating long-context LLMs with more retrieved documents. In contrast, IterDRAG [42] empirically examines the test-time scaling law through few-shot prompting and iterative retrieval for up to 5M tokens. Search-o1 [22] combines the open-source QwQ model [37] with active search from Bing, achieving competitive results on knowledge-intensive tasks. Concurrent works such as Search-R1 [15] train LLMs to use retrieval as a tool via reinforcement learning. Our work extends the study of test-time scaling in RAG to a targeted fine-tuning paradigm under diverse decoding strategies. 3 Methodology The CoRAG