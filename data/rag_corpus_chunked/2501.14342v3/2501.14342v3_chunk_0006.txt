tokens. Search-o1 [22] combines the open-source QwQ model [37] with active search from Bing, achieving competitive results on knowledge-intensive tasks. Concurrent works such as Search-R1 [15] train LLMs to use retrieval as a tool via reinforcement learning. Our work extends the study of test-time scaling in RAG to a targeted fine-tuning paradigm under diverse decoding strategies. 3 Methodology The CoRAG framework is illustrated in Figure 2. The “Current State” denotes the input context and instructions provided to the LLM, while the “Next Action” refers to the LLM output responding to the given instruction. In this section, we describe the key components of CoRAG, including retrieval chain generation through rejection sampling, model training with augmented datasets, and strategies for scaling test-time compute. 3.1 Retrieval Chain Generation Most RAG datasets only come with a query Q and the corresponding final answer A, without providing intermediate retrieval steps. We propose an automated method for generating retrieval chains through rejection sampling. Each sampled chain consists of a sequence of sub-queries Q1:L ={Q 1, Q2, . . . , QL} and the corresponding sub-answers A1:L, where L is a predetermined maximum chain length. The sub-query Qi =LLM(Q <i, A<i, Q) is generated by sampling an LLM based on the query Q and the preceding sub-queries and sub-answers. To generate the sub-answer Ai, we first retrieve the top-k most relevant documents D(i) 1:k using a text retriever with Qi as the search 3 output query … … output query … …… … … … output query … … … … … … rolloutsrollouts Greedy Best-of-N Tree Search Inference Training Rejection Sampling LLM Current State Next Action Query sub-query 1 sub-answer 1 sub-query n sub-answer n … … P(Ans | Chain 1) sub-query 1* sub-answer 1* sub-query n* sub-answer n* … … P(Ans | Chain 2) …