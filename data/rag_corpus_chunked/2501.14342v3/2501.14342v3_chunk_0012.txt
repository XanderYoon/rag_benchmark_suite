63.1 28.1 39.7 ▷ L=6, tree search 71.7 76.4 55.8 69.0 48.8 64.4 29.0 40.3 ▷ L=10, best-of-872.5 77.3 56.3 69.854.468.3 30.9 42.4 4.2 Main Results Multi-hop QAIn Table 1, we present a comparative analysis of CoRAG-8B against several models, including few-shot Llama-3.1-8B-Instruct [5], GPT-4o [10], Self-RAG-7B [1], ITER-RETGEN [30], DRAG, IterDRAG [42], and Search-o1-32B [ 22]. For a fair comparison, we also include a fine- tuned Llama-8B baseline utilizing the E5-large retriever, which is fine-tuned on the same datasets as CoRAG-8B but without retrieval chain augmentation. CoRAG-8B substantially surpasses all baselines, with the exception of the Bamboogle dataset, despite being based on a weaker LLM compared to Search-o1-32B and IterDRAG. Conversely, we recognize that fine-tuning on multi-hop QA datasets creates an advantage for CoRAG-8B, compared to the few-shot setting for DRAG and IterDRAG. The Bamboogle dataset comprises only 125 instances, resulting in considerable variance in perfor- mance across different runs. Certain questions within Bamboogle necessitate access to knowledge more recent than the Wikipedia dump used for retrieval. Systems like Search-o1-32B, which rely on commercial search engines, possess an advantage in this regard. KILT BenchmarkWe present several strong systems on the KILT benchmark in Table 2, including KILT-RAG [27], SEAL [2], Atlas-11B [11], RA-DIT 65B [23], and FiD with RS [9]. For submission to the KILT leaderboard, we choose the best decoding configuration for each task based on the public validation set. The results of different decoding strategies are detailed in Appendix Table 7. Our CoRAG-8B model achieves a new state-of-the-art performance across all tasks, with the exception of FEVER, where it marginally trails behind a larger model with 11B parameters. 4.3 Scaling Test-Time Compute In alignment with OpenAI o1 [12], our model allows for scaling test-time compute to potentially achieve better performance without updating model weights. There are multiple