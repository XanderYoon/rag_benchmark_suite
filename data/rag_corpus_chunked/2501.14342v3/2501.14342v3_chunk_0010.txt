evaluate the model’s capacity to perform multi-hop reasoning, whereas the KILT benchmark assesses the framework’s ability to generalize across more diverse tasks. For each training dataset, we prompt the open-sourceLlama-3.1-8B- Instructmodel to perform rejection sampling, unless specified otherwise. We utilize E5-large [ 34] as the text retriever for intermediate retrieval steps. The retrieval corpus is the English Wikipedia provided by KILT, comprising approximately36 million passages [25]. The selected retrieval chains are employed to augment the original QA-only datasets for subsequent model training. Regarding evaluation metrics, we report the exact match (EM) and F1 scores [29] for the multi-hop QA datasets. For the KILT benchmark, we submit the model’s predictions to the official evaluation server and report the downstream metrics on thehidden test set. To adhere to the leaderboard submission policy, we reportpublic validation setresults when conducting ablation studies on the KILT benchmark. Note that while HotpotQA and MuSiQue maintain public leaderboards, these adopt either a simplified reading comprehension setting or an abstract-only retrieval configuration. Conse- quently, the leaderboard results are not directly comparable to our open-domain QA evaluation setting. Model TrainingWe conduct full-parameter fine-tuning on the augmented datasets, initializing from theLlama-3.1-8B-Instructcheckpoint. Two separate models are trained: one for the multi-hop QA datasets and another for the KILT benchmark. The compiled multi-hop QA dataset comprises 125k training instances, whereas the KILT benchmark includes 660k instances after sub-sampling. The model is fine-tuned for 1 epoch with a maximum sequence length of 3k tokens. For the KILT benchmark, we fine-tune an E5-Mistral retriever [ 35] and a RankLLaMA re-ranker [ 24] on the respective training set to boost the ranking quality. Further implementation details are provided in Appendix A. 5 Table 1: Results on multi-hop QA datasets. We report the performance of CoRAG-8B using various decoding strategies and retrieval chain lengths L. The