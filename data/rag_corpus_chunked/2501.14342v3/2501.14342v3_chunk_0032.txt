evaluation scripts provided by the organizers. For Open QA tasks, the main evaluation metric is the EM score, while other task types are evaluated using accuracy scores. The KILT benchmark also offers a variant of the evaluation protocol that requires the model not only to generate the correct answer but also to provide the correct supporting evidence. However, our method spreads the evidence documents across the retrieval chain, rendering it challenging to conform to such an evaluation protocol. B Additional Results Table 7: Downstream results on the publicvalidation setof the KILT benchmark. System Entity Linking Slot Filling Open QA Fact AIDA WnWi WnCw T-REx zsRE NQ HoPo TQA FEVER CoRAG-8B (Ours) ▷ L=1, greedy 90.4 86.076.8 87.082.1 62.5 56.4 88.4 91.4 ▷ L=6, greedy92.7 87.475.8 86.683.8 63.259.1 88.6 93.8 ▷ L=6, best-of-492.587.475.8 86.3 83.5 62.6 59.6 88.793.9 ▷ L=6, tree search 91.8 86.8 75.5 86.4 83.0 62.459.9 88.9 93.9 Table 8: Retrieval results (R-Precision) on the publicvalidation setof the KILT benchmark. For re-ranking, we use the top-100candidates from the fine-tuned retriever as input. System Entity Linking Slot Filling Open QA Fact AIDA WnWi WnCw T-REx zsRE NQ HoPo TQA FEVER Fine-tuned E5mistral 92.9 86.7 76.0 80.5 95.3 77.7 66.7 78.9 90.9 ▷w/ re-ranking 93.3 88.0 77.1 83.2 97.6 78.2 78.2 81.5 92.3 Different Decoding Strategies on the KILT BenchmarkIn Table 7, we present the results of various decoding strategies applied to thevalidation setof the KILT benchmark. Given that most tasks within the KILT benchmark are much easier for strong dense retrievers compared to multi-hop QA, the disparity in performance across different decoding strategies is less pronounced. This observation underscores the necessity of developing a system capable of adaptively selecting the optimal decoding strategy to effectively balance the trade-off between performance and test-time compute. 4 8 16 Rejection Sampling Size 66