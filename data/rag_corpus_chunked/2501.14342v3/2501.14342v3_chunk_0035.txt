scaling upper bound. 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 70.0 70.5 71.0 71.5 72.0 72.5EM 2WikiMultihopQA L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 54.0 54.5 55.0 55.5 56.0 56.5 HotpotQA L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 46 48 50 52 Bamboogle L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 26.5 27.0 27.5 28.0 28.5 29.0 29.5 30.0 MuSiQue L=6, best-of-4 Figure 8: Effects of varying the sampling temperature on multi-hop QA datasets. Effects of Sampling TemperatureIn best-of- N sampling, the sampling temperature controls the diversity and quality trade-off in the generated retrieval chains. A higher temperature results in more diverse chains, albeit with the potential introduction of increased noise. Figure 8 illustrates the lack of a consistent conclusion regarding the impact of sampling temperature on performance. For the MuSiQue and HotpotQA datasets, a lower temperature generally yields superior results, whereas for the 2WikiMultihopQA dataset, a medium temperature leads to the best performance. As a result, we stick to a temperature of0.7for both rejection sampling and test-time decoding for simplicity. Extension to Other Model FamiliesTo demonstrate that our CoRAG framework is agnostic to model families and not limited to Llama-based architectures, we conduct experiments using Qwen3-4B and Qwen3-8B [38] models following the same training procedure. As shown in Table 9, CoRAG consistently outperforms the baseline fine-tuned models across all datasets and model sizes, with improvements of over 10 EM points on average. This validates that the chain-of-retrieval mechanism 17 Table 9: Extension to the Qwen3 model families. 2WikiQA HotpotQA Bamboogle MuSiQue EM F1 EM F1 EM F1 EM F1 Fine-tuned Qwen3-4B w/ E5large 49.3 55.3 45.0 57.9 32.8 43.1 13.4 23.8 CoRAG-Qwen3-4B (L=6, greedy)69.3 74.1 51.6 64.2 49.6 62.5 24.0 34.5 Fine-tuned Qwen3-8B w/ E5large 52.1 57.9 47.1 60.0 33.6