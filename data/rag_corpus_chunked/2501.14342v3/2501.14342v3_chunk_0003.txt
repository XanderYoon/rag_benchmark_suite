augmented datasets using standard next-token prediction objectives. To examine the scaling behavior of our model, we propose various test-time decoding strategies, including greedy decoding, best-of-N sampling, and tree search. Diverse decoding strategies and hyperparameter configurations can be employed to control test-time token consumption and the frequency of retriever calls. Our empirical evaluation demonstrates that CoRAG substantially surpasses strong baselines in QA tasks that require multi-hop reasoning, where retrievers frequently struggle to recall all necessary information in a single retrieval step. Across diverse decoding strategies, the Pareto frontier approxi- mately adheres to a log-linear relationship between total token consumption and model performance, although the coefficients differ across datasets. On the KILT benchmark [27], which encompasses a more diverse array of tasks, new state-of-the-art scores are achieves on thehidden test setfor nearly all tasks. Additionally, we uncover that CoRAG exhibits varied scaling behaviors across different task types. For datasets such as NQ [ 19], where state-of-the-art retrievers already achieve high recall, the benefits of test-time scaling are often marginal. This suggests the potential for dynamically allocating test-time compute based on the complexity of the query and the quality of the retriever. Upon further analysis, we find that CoRAG can effectively decompose complex queries and perform flexible query reformulation to improve the quality of the generated responses. It also shows robustness against retrievers of varying quality. We posit that CoRAG represents a promising avenue for future research in the RAG domain, with the potential to mitigate hallucination in model-generated content. Our code, data and trained models are available athttps://github.com/microsoft/LMOps/tree/main/corag. 2 Related Work Retrieval-Augmented Generation (RAG)integrates information retrieval techniques with generative models to enhance the quality and factual accuracy of generated content [ 20, 21]. By equipping LLMs with the ability to browse the web [ 26], RAG systems can access real-time data, thereby