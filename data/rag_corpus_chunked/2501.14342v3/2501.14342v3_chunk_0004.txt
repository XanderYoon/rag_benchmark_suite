potential to mitigate hallucination in model-generated content. Our code, data and trained models are available athttps://github.com/microsoft/LMOps/tree/main/corag. 2 Related Work Retrieval-Augmented Generation (RAG)integrates information retrieval techniques with generative models to enhance the quality and factual accuracy of generated content [ 20, 21]. By equipping LLMs with the ability to browse the web [ 26], RAG systems can access real-time data, thereby 2 providing responses that are both up-to-date and grounded. The relevance and quality of the retrieved information are pivotal for the efficacy of RAG systems. A substantial body of recent research has concentrated on developing better general-purpose text embeddings [ 18, 35]. Nevertheless, text embeddings frequently face limitations in addressing complex queries due to their reliance on fixed-size vector representations for efficiency purposes. To mitigate this constraint, contemporary research has extended the conventional paradigm of a single retrieval step followed by generation, advancing to multi-step iterative retrieval and generation [6]. FLARE [ 13] prompts an LLM to actively determine when and what to retrieve during the generation process. ITER-RETGEN [ 30] proposes to interleave retrieval-augmented generation with generation-augmented retrieval, demonstrating enhancements in multi-hop QA tasks. Similarly, IRCoT [ 33] employs a chain-of-thought methodology, which recursively refines the reasoning thought for subsequent retrieval steps. Self-RAG [1] empowers LLMs to adaptively retrieve, generate, and critique through self-reflection, thus improving factual accuracy and citation precision in open-domain QA and long-form generation tasks. Auto-RAG [41] utilizes heuristic rules and exact answer matching to construct intermediate retrieval steps, yet its performance remains significantly below that of state-of-the-art models. AQA [3] learns to reformulate questions using reinforcement learning but only focuses on single-hop QA tasks. In this study, rather than exclusively on few-shot prompting or distillation from proprietary models, we propose a novel approach to explicitly train LLMs to iteratively retrieve and reason over relevant information.