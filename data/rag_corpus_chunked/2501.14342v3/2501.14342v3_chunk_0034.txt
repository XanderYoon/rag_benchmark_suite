4 6 8 10 Chain Length 30 32 34 36 38 40 42 44 46EM 2WikiMultihopQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 34 36 38 40 42 HotpotQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 25 30 35 40 45 Bamboogle greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 8 10 12 14 16 18 MuSiQue greedy best-of-4 best-of-8 4k 8k 16k 32k 64k 128k # Avg. T okens 30.0 32.5 35.0 37.5 40.0 42.5 45.0 47.5EM greedy best-of-4 best-of-8 pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 34 36 38 40 42 greedy best-of-4 best-of-8 pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 25 30 35 40 45 50 greedy best-of-4 best-of-8 pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 8 10 12 14 16 18 20 greedy best-of-4 best-of-8 pareto frontier Figure 7: Scaling test-time compute on multi-hop QA datasets withLlama-3.1-8B-Instruct. No fine-tuning is performed on the model weights. chains are already of high quality and that LM fine-tuning exhibits considerable robustness to noisy training data. Scaling Test-Time Compute without Model Fine-TuningIn Figure 7, we present the scaling results on multi-hop QA datasets using theLlama-3.1-8B-Instructmodel directly without any fine- tuning. The scaling curves are similar to those observed in Figure 3, but the absolute performance is significantly lower, indicating that targeted fine-tuning is essential for improving the scaling upper bound. 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 70.0 70.5 71.0 71.5 72.0 72.5EM 2WikiMultihopQA L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 54.0 54.5 55.0 55.5 56.0 56.5 HotpotQA L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 46 48 50 52 Bamboogle L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature