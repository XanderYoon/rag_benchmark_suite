query Q and each sub-query. We fine-tune an LLM on the augmented dataset using the standard next-token prediction objective within a unified multi-task learning framework. The model is simultaneously trained on three tasks: next sub-query prediction, sub-answer prediction, and final answer prediction. We employ the same prompt templates as utilized in the retrieval chain generation process, with the exception that we also incorporate the top retrieved documents D1:k for the original queryQas input for the final answer prediction task. Lsub_query =−logP(Q i|Q, Q<i, A<i), i∈[1, L] Lsub_answer =−logP(A i|Qi, D(i) 1:k), i∈[1, L] Lfinal_answer =−logP(A|Q, Q 1:L, A1:L, D1:k) The cross-entropy loss is computed only for the target output tokens. As we reuse the prompt templates for both data generation and model training, a fine-tuned model can be utilized for the next round of rejection sampling in an iterative manner. 3.3 Test-time Scaling Given a trained CoRAG model, we propose several decoding strategies to control the trade-off between model performance and test-time compute. The test-time compute is measured by the 4 total number of token consumptions, excluding the retrieval costs. Unlike previous approaches that consider only prompt tokens [42] or generated tokens [12], we account for both. To simplify further discussion, the prompt tokens are treated equally as the generated tokens, despite prompt tokens typically being less expensive due to prefix caching and computation parallelism of the prefilling stage. Greedy DecodingThis strategy utilizes greedy decoding to generate L sub-queries and their corre- sponding sub-answers sequentially. The final answer is generated using the same prompt template as employed during the training phase. Best-of-N SamplingThis method involves sampling N retrieval chains with a temperature 0.7, subsequently selecting the best chain to generate the final answer. As the ground truth answer is not available at test time, we instead calculate the conditional log-likelihood