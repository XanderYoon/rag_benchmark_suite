their corre- sponding sub-answers sequentially. The final answer is generated using the same prompt template as employed during the training phase. Best-of-N SamplingThis method involves sampling N retrieval chains with a temperature 0.7, subsequently selecting the best chain to generate the final answer. As the ground truth answer is not available at test time, we instead calculate the conditional log-likelihood of“No relevant information found”as a penalty score for each chain. The retrieval chain with the lowest penalty score is chosen. Tree SearchWe implement a breadth-first search (BFS) variant with retrieval chain rollouts. At each step, the current state is expanded by sampling several sub-queries. For each expanded state, we perform multiple rollouts, and then compute the average penalty score of these rollouts. The state with the lowest average penalty score is retained for further expansion. To control the test-time compute, the maximum length of the retrieval chain L can be adjusted across all decoding strategies. For best-of-N sampling, the number of sampled chainsN offers an alternative option to scale the test-time compute. In tree search, the number of rollouts and expansion size are two additional hyperparameters. 4 Experiments 4.1 Setup Data and EvaluationWe evaluate CoRAG utilizing two sets of benchmarks: (1) a collection of multi-hop QA datasets, including 2WikiMultihopQA [ 8], HotpotQA [ 39], Bamboogle [ 28], and MuSiQue [32]; (2) the KILT benchmark [27], which encompasses a broad spectrum of knowledge- intensive tasks. The multi-hop QA datasets serve to evaluate the model’s capacity to perform multi-hop reasoning, whereas the KILT benchmark assesses the framework’s ability to generalize across more diverse tasks. For each training dataset, we prompt the open-sourceLlama-3.1-8B- Instructmodel to perform rejection sampling, unless specified otherwise. We utilize E5-large [ 34] as the text retriever for intermediate retrieval steps. The retrieval corpus is the English Wikipedia provided by