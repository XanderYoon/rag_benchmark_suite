within the KILT benchmark are much easier for strong dense retrievers compared to multi-hop QA, the disparity in performance across different decoding strategies is less pronounced. This observation underscores the necessity of developing a system capable of adaptively selecting the optimal decoding strategy to effectively balance the trade-off between performance and test-time compute. 4 8 16 Rejection Sampling Size 66 68 70 72 74 76EM 2WikiMultihopQA L=6, greedy 4 8 16 Rejection Sampling Size 50 52 54 56 58 60 HotpotQA L=6, greedy 4 8 16 Rejection Sampling Size 40 42 44 46 48 50 52 Bamboogle L=6, greedy 4 8 16 Rejection Sampling Size 22 24 26 28 30 32 MuSiQue L=6, greedy Figure 6: Scaling rejection sampling compute for training data generation. We vary the number of sampled chains from4to16while maintaining all other hyperparameters fixed. Scaling Compute for Training Data GenerationWithin our proposed framework, rather than investing more compute at test time, we can scale the compute for retrieval chain generation during rejection sampling. By increasing the number of sampled chains, we may identify better chains that contribute to higher-quality training data. However, as illustrated in Figure 6, no definitive trend emerges indicating that increasing the number of sampled chains always leads to better performance. Conversely, the training loss consistently decreases as we scale up rejection sampling, suggesting that the training data becomes less noisy and easier to fit. We hypothesize that the majority of sampled 16 2 4 6 8 10 Chain Length 30 32 34 36 38 40 42 44 46EM 2WikiMultihopQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 34 36 38 40 42 HotpotQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 25 30 35 40 45 Bamboogle greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 8 10