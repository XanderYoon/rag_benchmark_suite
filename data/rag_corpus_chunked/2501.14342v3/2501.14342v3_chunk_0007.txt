… output query … …… … … … output query … … … … … … rolloutsrollouts Greedy Best-of-N Tree Search Inference Training Rejection Sampling LLM Current State Next Action Query sub-query 1 sub-answer 1 sub-query n sub-answer n … … P(Ans | Chain 1) sub-query 1* sub-answer 1* sub-query n* sub-answer n* … … P(Ans | Chain 2) … … … … … … … … … … P(Ans | Chain 3) LLM retriever chosen state discarded state answer Figure 2: Overview of CoRAG. Rejection sampling is utilized to augment QA-only datasets with retrieval chains. Each chain starts with the original query, followed by a sequence of sub-queries and sub-answers. An open-source LLM is then fine-tuned to predict the next action based on the current state. During inference, multiple decoding strategies are available to control the test-time compute. query, and subsequently prompt an LLM to yield the answer Ai =LLM(Q i, D(i) 1:k). This procedure is iterated until the chain reaches the maximum lengthLorA i matches the correct answerA. To assess the quality of a retrieval chain, we calculate the log-likelihood of the correct answer logP(A|Q, Q 1:L, A1:L) conditioned on the chain information. The retrieval chain with the highest log-likelihood score is selected to augment the original QA-only dataset. 3.2 Training Each training instance in the augmented dataset is represented as a tuple (Q, A, Q1:L, A1:L), accom- panied by the corresponding top- k retrieved documents for the query Q and each sub-query. We fine-tune an LLM on the augmented dataset using the standard next-token prediction objective within a unified multi-task learning framework. The model is simultaneously trained on three tasks: next sub-query prediction, sub-answer prediction, and final answer prediction. We employ the same prompt templates as utilized in the retrieval chain generation process, with the exception that