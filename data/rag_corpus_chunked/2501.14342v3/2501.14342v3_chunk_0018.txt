Length 87 88 89 90 91EM TQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 59 60 61 62 63 64 65 66Accuracy NQ greedy best-of-4 best-of-8 Figure 4: Scaling test-time compute across three datasets from the KILT benchmark. We report scores on the public validation set. Multi-hop QA datasets are specifically designed to evaluate complex reasoning capabilities and are expected to benefit from the chain-of-retrieval mechanism. Table 1 presents empirical evidence supporting this assertion. In contrast, for tasks that a single retrieval step is typically sufficient, the advantage tends to be marginal, as demonstrated in Figure 4. Datasets such as NQ [ 19] and TriviaQA [17] are known for their (mostly) single-hop nature. This phenomenon implies that decoding strategies should be adaptive based on the complexity of the query. Additional results on the full KILT benchmark are listed in Appendix Table 7, where similar observations for other task types also hold. 5.4 Learning to Stop at Test Time 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Early Stop Logit Bias 18 20 22 24 26 28EM EM Score L=6 L=0 3.9k 4.9k 5.9k 6.8k 7.8k T oken Consumption # T okens MuSiQue (Greedy Decoding) Figure 5: Learning to stop at test time. Larger logit bias values result in earlier stopping. L= 6 correspond to always performing 6 retrieval steps, while L= 0 indicate no intermediate retrieval steps. Instead of always performing L retrieval steps, we explore a model variant that learns to stop at test time. After each retrieval step, the model is prompted to predict whether the information gathered thus far suffices to answer the query. Note that this prompt itself also incurs token consumption and additional cost. The decoding space is constrained to two tokens:“Yes”and “No”. If the decoded output is “Yes”, no further