adhere to the exact training hyperparameters outlined in the original papers, except that the training data is replaced with the KILT training set. For training the RankLLaMA re-ranker, the backbone is initialized with theLlama-3-8B-Basemodel, as opposed to Llama-2, to enhance performance. Retrieval and re-ranking scores are presented in Table 8. All training jobs are conducted using 8 A100 GPUs. The multi-hop QA task requires less than 6 hours of training, whereas the KILT training takes approximately 30 hours. When submitting to the KILT leaderboard, we select the optimal decoding strategy for each task based on validation set performance. Decoding StrategiesIn the context of best-of- N sampling, the temperature is set to 0.7 for sub-query generation. For sub-answer generation and final answer prediction, the temperature is always set to 0 across all decoding strategies. Regarding tree search, we set the expansion size to4and the number of rollouts to 2. Given that tree search incurs a significantly higher token consumption compared to other decoding strategies, we limit the rollouts to a maximum of 2 steps for each expansion. To avoid the model from generating repetitive sub-queries endlessly, any generated sub-query identical to previous ones is discarded. EvaluationFor multi-hop QA tasks, we evaluate the performance using the exact match (EM) and F1 scores [18]. For Self-RAG-7B, we reproduce the results utilizing the FlashRAG [16] toolkit with the official checkpoint released by the authors. 2https://huggingface.co/intfloat/e5-large-v2 15 For the KILT benchmark, we employ the official evaluation scripts provided by the organizers. For Open QA tasks, the main evaluation metric is the EM score, while other task types are evaluated using accuracy scores. The KILT benchmark also offers a variant of the evaluation protocol that requires the model not only to generate the correct answer but also to provide the correct supporting evidence. However, our method