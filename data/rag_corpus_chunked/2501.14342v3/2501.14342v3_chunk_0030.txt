most relevant documents from the KILT version of the Wikipedia corpus [25]. This corpus comprises 36million passages. Table 5: Hyperparameters for training CoRAG. Multi-hop QA KILT Benchmark InitializationLlama-3.1-8B-Instruct Learning rate5×10 −6 10−5 Batch size256 1024 Epoch1 1 Warmup steps100 100 # Training samples125k660k # Retrieved passages20 20 Max sequence length3072 3072 Table 6: Statistics of the datasets used for multi-hop QA training. 2WikiMultihopQA HotpotQA Bamboogle MuSiQue # Training Samples15,000 90,447-19,938 # Validation Samples12,576 7,405 125 2,417 Multi-Hop QA Training HyperparametersThe training set is the union of the 2WikiMultihopQA, HotpotQA, and MuSiQue datasets, comprising a total of 125k samples as shown in Table 6. The Bamboogle dataset, consisting of only 125 questions, is reserved for evaluation only. Additional hyperparameters are detailed in Table 5. To balance the three loss terms in Section 3.2, we set a sample ratio of 0.2 for both the sub-query and sub-answer generation tasks; this ratio is also applied to the KILT training. KILT Training HyperparametersWe utilize the official training set of the KILT benchmark, omitting the ELI5 and WoW datasets due to the lack of reliable evaluation metrics. To balance the task distribution, we only select 100k samples for large datasets like T-REx and Zero-Shot RE. In accordance with the benchmark’s guidelines, we also add100k samples from the BLINK dataset for entity linking. Rather than using off-the-shelf retrievers, we fine-tune an E5-Mistral retriever following Wang et al., and a RankLLaMA re-ranker following Ma et al.. We adhere to the exact training hyperparameters outlined in the original papers, except that the training data is replaced with the KILT training set. For training the RankLLaMA re-ranker, the backbone is initialized with theLlama-3-8B-Basemodel, as opposed to Llama-2, to enhance performance. Retrieval and re-ranking scores are presented in Table 8. All training jobs are conducted using 8 A100 GPUs. The