(i.e., CHI and Cosine Similarity) generated by our attributed clustering algorithm compared to those produced by the Lei- den algorithm, which is used in GraphRAG for structural clustering. As shown in Table 7, our attribute-based cluster- ing consistently yields higher-quality communities. â€¢ More experiments on the additional dataset. We further conduct experiments on the RAG-QA Arena dataset (Han et al. 2024), a high-quality, multi-domain benchmark featuring human-annotated, coherent long-form answers. To the best of our capability, we use publicly avail- able data from five domains (including lifestyle, recreation, science, technology, and writing), selecting 200 questions per domain. Following prior work, we employ LLMs as evaluators to compare the RAG-generated responses with Table 6: Comparing ArchRAG with other RAG methods on the RAG-QA Area dataset. Each entry denotes the win ratio and win + tie ratio of the corresponding method against the ground-truth annotations, based on LLM evaluation. Method Lifestyle Recreation Science Technology Writing Vanilla RAG 17.5 / 20.5 17.0 / 25.0 32.5 / 37.0 28.5 / 34.0 15.0 / 16.5 HippoRAG 26.5 / 26.5 29.5 / 30.5 49.5 / 49.5 42.0 / 42.5 21.0 / 21.5 RAPTOR 17.0 / 19.5 18.5 / 24.5 39.0 / 45.0 33.0 / 35.5 25.0 / 26.5 ArchRAG 49.5 / 50.0 41.5 / 41.5 56.0 / 56.0 59.0 / 59.5 45.0 / 45.0 Table 7: Comparison of Community Quality between Our Attributed Clustering Method and Leiden Method Multihop-RAG HotpotQA (CHI) (Sim) (CHI) (Sim) Leiden 3.02 0.71 3.42 0.71 Ours 4.68 0.89 4.82 0.88 ground-truth answers in terms of win ratio and win + tie ratio. We evaluate the top-performing methods (including Vanilla RAG, HippoRAG, RAPTOR, and our ArchRAG) and present the results in terms of the win ratio and win + tie ratio against the ground-truth annotations in the table be- low. As shown in