to the query, with any content exceeding the text limit being truncated. This process can be represented as: Ai = LLM(Pf ilter||Ri) (1) Output = LLM(Pmerge||Sort({A0, A1, · · · , An})) (2) where Pf ilter and Pmerge represent the prompts for extract- ing relevant information and summarizing, respectively, Ai, i ∈ 0 · · · n denotes the filtered analysis report. The sort func- tion orders the content based on the relevance scores from the analysis report. Experiments In this section, we conduct a comprehensive evaluation of our ArchRAG, focusing on both efficiency and performance. Setup Table 1: Datasets used in our experiments. Acc, Rec, Blue, Met, and Rou denote Accuracy, Recall, BLEU-1, METEOR, and ROUGE-L F1. Dataset Multihop-RAG HotpotQA NarrativeQA Passages 609 9,221 1,572 Tokens 1,426,396 1,284,956 121,152,448 Nodes 23,353 37,436 650,571 Edges 30,716 30,758 679,426 Questions 2,556 1,000 43,304 Metrics Acc, Rec Acc, Rec Blue, Met, Rou Datasets. We evaluate ArchRAG on both specific and abstract QA tasks. For specific QA, we use Multihop- RAG (2024), HotpotQA (2018), and NarrativeQA (2018), all of which are extensively utilized within the QA and Graph-based RAG research communities (2022; 2024; 2022; 2024; 2024; 2023). For abstract QA, we follow the GraphRAG (2024) method and reuse the Multihop-RAG corpus, prompting LLM to generate questions that convey a high-level understanding of dataset contents. The statistics of these datasets are reported in Table 1. Baselines. Our experiments consider three configurations: • Inference-only: Using an LLM to answer questions with- out retrieval, i.e., Zero-Shot and CoT (2022). • Retrieval-only: Retrieval models extract relevant chunks from all documents and use them as prompts for LLMs. We select strong and widely used retrieval models: BM25 (1994) and Vanilla RAG. • Graph-based RAG: These methods leverage graph data during retrieval. We select RAPTOR (2024), HippoRAG (2024), GraphRAG