that not all communities are suitable for specific QA tasks, as the GGraphRAG performs poorly. Furthermore, GraphRAG does not consider node attributes during clustering, which causes the community’s summary to become dispersed, making it difficult for the LLM to extract relevant informa- tion from a large number of communities. Thus, we gain an interesting insight: LLM may not be a good retriever, but is a good analyzer.We further analyze the reasons behind the un- derperformance of each graph-based RAG method and sup- port our claims with empirical evidence in the appendix. Zero-Shot CoT BM25 Vanilla RAG RAPTOR HippoRAG LLightRAG HLightRAG HyLightRAG LGraphRAG GGraphRAG ArchRAG Multihop-RAG HotpotQA102103104105 N/A time (s) (a) Time cost Multihop-RAG HotpotQA100101102103 N/A token (M) (b) Token cost Figure 4: Comparison of query efficiency. • Efficiency of ArchRAG.We compare the time cost and token usage of ArchRAG with those of other baseline meth- ods. As shown in Figure 4, ArchRAG demonstrates signifi- cant time and cost efficiency for online queries. For example, token usage on the HotpotQA dataset is cut by 250× with ArchRAG compared to GraphRAG-Global, from 1,394M tokens down to 5.1M tokens. To further evaluate ArchRAG, we test the efficiency of hierarchical search, indexing performance, and effective- ness on an additional dataset, RAG-QA Arena (2024). Re- sults show that ArchRAG achieves up to5.4× faster retrieval than basic HNSW, maintains efficient indexing, and achieves state-of-the-art performance on the RAG-QA Arena dataset. Additional details are provided in the appendix. Table 2: Performance comparison of different methods across various datasets for solving specific QA tasks. The best and second-best results are marked in bold and underlined. OOT: Not finished within 3 days. Baseline Type Method Multihop-RAG HotpotQA NarrativeQA (Accuracy) (Recall) (Accuracy) (Recall) (BLEU-1) (METEOR) (ROUGE-L F1) Inference-only Zero-shot 47.7 23.6 28.0 31.8 8.0 7.9 8.6 CoT 54.5 28.7 32.5