concurrent threads. Figure 15 demonstrates the prompt used in Adaptive filtering-based generation. Please refer to our repository (https://anonymous.4open.science/r/H-CAR-AG-1E0B/) to view the detailed prompts. • Details of clustering methods. The graph augmen- tation methods we choose are the KNN algorithm, which computes the similarity between each node, and CODI- CIL (Ruan, Fuhry, and Parthasarathy 2013), which selects and adds the top similar edges to generate better clustering results. In the KNN method, we set the K value as the aver- age degree of nodes in the KG. Additional experiments • Efficiency of hierarchical search (C-HNSW). To evalu- ate the efficiency of C-HNSW, we conduct experiments on a synthetic hierarchical dataset comprising 11 layers. The bot- tom layer (Layer 0) contains 10 million nodes, and the num- ber of nodes decreases progressively across higher layers by randomly dividing each layer’s size by 3 or 4. The top layer (Layer 10), for example, contains only 74 nodes. This hier- archical structure simulates the process of LLM-based hier- archical clustering. Each node is assigned a randomly gener- ated 3072-dimensional vector, simulating high-dimensional embeddings such as those produced by text or image en- coders (e.g., text-embedding-3-large, used in Chat- GPT, can generate 3072-dimensional vectors, and text- embedding-v3 can generate 1024-dimensional vectors). For each layer, we generate 200 random queries and compute the top-5 nearest neighbors for each query. Both C-HNSW and Base-HNSW are configured with identical parameters: M = 32, ef Search = 100, and ef Construction = 100. Importantly, our method maintains comparable retrieval ac- curacy to Base-HNSW, with recall of 0.5537 and 0.6058, respectively. We compare our hierarchical search based on C-HNSW with a baseline approach (Base-HNSW), which indepen- dently builds a vector index for attributed communities at each layer and performs retrieval separately for each. As shown in Figure 5, on the