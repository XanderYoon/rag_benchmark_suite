of the retrieved content, and P is the token of the prompt. Proof. The token consumption for analyzing all retrieved in- formation is O(kL(c + P ))), while the token consumption for generating the final response is of constant order. There- fore, the total token consumption for the online retrieval is O(kL(c + P ))). In practice, multiple retrieved contents are combined, al- lowing the LLM to analyze them together, provided the total token count does not exceed the token size limit. As a result, the time and token usage for online retrieval are lower than those required for analysis. Experimental details Metrics This section provides additional details on the metrics. • Metrics for specific QA tasks. We choose accuracy as the evaluation metric based on whether the gold answers are included in the model’s generations rather than strictly requiring an exact match, following (Schick et al. 2024; Mallen et al. 2022; Asai et al. 2023). This is because LLM outputs are typically uncontrollable, making it difficult for them to match the exact wording of standard answers. Sim- ilarly, we choose recall as the metric instead of precision, as it better reflects the accuracy of the generated responses. Additionally, when calculating recall, we adopt the same ap- proach as previous methods (Guti´errez et al. 2024; Asai et al. 2023): if the golden answer or the generated output contains “yes” or “no”, the recall for that question is set to 0. There- fore, the recall metric is not perfectly correlated with accu- racy. • Metrics for abstract QA tasks. Following exist- ing works, we use an LLM to generate abstract questions, with the prompts shown in Figure 13, defining ground truth for abstract questions, particularly those involving complex high-level semantics, poses significant challenges. We build on existing works (Edge et al.