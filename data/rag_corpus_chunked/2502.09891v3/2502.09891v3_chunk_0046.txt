0.71 3.42 0.71 Ours 4.68 0.89 4.82 0.88 ground-truth answers in terms of win ratio and win + tie ratio. We evaluate the top-performing methods (including Vanilla RAG, HippoRAG, RAPTOR, and our ArchRAG) and present the results in terms of the win ratio and win + tie ratio against the ground-truth annotations in the table be- low. As shown in Table 6, ArchRAG consistently achieves state-of-the-art performance across all evaluated settings. Accuracy Recall 2 3 5 7 920406080Metrics (a) Multihop-RAG 2 3 5 7 920406080Metrics (b) HotpotQA Figure 10: Comparative analysis of the different numbers of retrieval elements in ArchRAG. • Effect of k values. We compare the performance of ArchRAG under different retrieved elements. As shown in Figure 10, the performance of ArchRAG shows little vari- ation when selecting different retrieval elements (i.e., com- munities and entities in each layer). This suggests that the adaptive filtering process can reliably extract the most rele- vant information from the retrieval elements and integrate it to generate the answer. • Case study. We present an additional case study from Multihop-RAG. As shown in Figure 11, only our method generates the correct answer, while others either provide in- correct or irrelevant information. We only show the core output for brevity, with the remaining marked as “¡/¿”. We also show the retrieval and adaptive filtering process of ArchRAG in Figure 12. The results demonstrate that ArchRAG effectively retrieves relevant information and fil- ters out noise, leading to a correct final answer. • Discussion of the performance of other graph-based RAG methods. On the Multihop-RAG dataset, HippoRAG Table 8: Distribution of HippoRAG’s ER Errors Datasets Null Entity Rate Low-Quality Entity Rate Multihop-RAG 1.3% 11.9% HotpotQA 5.0% 15.8% performs worse than retrieval-only methods while outper- forming retrieval-based methods on the HotpotQA dataset. This is mainly because,