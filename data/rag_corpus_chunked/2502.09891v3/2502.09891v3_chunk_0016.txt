an LLM to answer questions with- out retrieval, i.e., Zero-Shot and CoT (2022). • Retrieval-only: Retrieval models extract relevant chunks from all documents and use them as prompts for LLMs. We select strong and widely used retrieval models: BM25 (1994) and Vanilla RAG. • Graph-based RAG: These methods leverage graph data during retrieval. We select RAPTOR (2024), HippoRAG (2024), GraphRAG (2024), and LightRAG (2024). Partic- ularly, GraphRAG has two versions, i.e., GGraphRAG and LGraphRAG, which use global and local search methods, respectively. Similarly, LightRAG integrates local search, global search, and hybrid search, denoted by LLightRAG, HLightRAG, and HyLightRAG, respectively. In GGraphRAG, all communities below the selected level are first retrieved, and then the LLM is used to filter out ir- relevant communities. This process can be viewed as uti- lizing the LLM as a retriever to find relevant communities VR LR C1 C2 AR VR 50 46 18 18 1 LR 54 50 21 29 16 C1 82 79 50 86 18 C2 82 71 14 50 16 AR 99 84 82 84 50 (a) Comprehensiveness VR LR C1 C2 AR VR 50 64 3 12 8 LR 36 50 52 63 33 C1 97 48 50 52 46 C2 88 37 48 50 42 AR 92 67 54 58 50 (b) Diversity VR LR C1 C2 AR VR 50 39 15 21 8 LR 61 50 59 30 35 C1 85 41 50 60 42 C2 79 70 40 50 38 AR 92 65 58 62 50 (c) Empowerment VR LR C1 C2 AR VR 50 46 14 18 4 LR 54 50 48 31 22 C1 86 52 50 70 31 C2 82 69 30 50 30 AR 96 78 69 70 50 (d) Overall Figure 3: Head-to-head win rates for abstract QA, comparing each row method against