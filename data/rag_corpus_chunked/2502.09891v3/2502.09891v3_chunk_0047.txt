noise, leading to a correct final answer. • Discussion of the performance of other graph-based RAG methods. On the Multihop-RAG dataset, HippoRAG Table 8: Distribution of HippoRAG’s ER Errors Datasets Null Entity Rate Low-Quality Entity Rate Multihop-RAG 1.3% 11.9% HotpotQA 5.0% 15.8% performs worse than retrieval-only methods while outper- forming retrieval-based methods on the HotpotQA dataset. This is mainly because, on the HotpotQA dataset, passages are segmented by the expert annotators; that is, passages can provide more concise information, whereas on Multihop- RAG, passages are segmented based on chunk size, which may cause the LLM to lose context and produce incorrect answers. Besides, HippoRAG also suffers from inaccurate entity recognition. In addition, we also provide a brief anal- ysis of why some graph-based RAG methods underperform. Specifically, they often fail to retrieve relevant information during the search process. For example: • GraphRAG: The suboptimal performance of GraphRAG has been widely observed in research. For instance, Zhang et al. (Zhang et al. 2024a) find that GraphRAG tends to respond with ”I don’t know” when retrieving ir- relevant content, indicating that it prefers not to give a concrete answer. PIKE-RAG (Wang et al. 2025) identi- fies that GraphRAG tends to echo the query and include meta-information about answers within its graph struc- ture. Through manual verification, we confirm the pres- ence of similar issues in our experimental results. In par- ticular, GraphRAG-Local Search also frequently returns irrelevant content. • HippoRAG/LightRAG: We attribute part of HippoRAG’s suboptimal performance to entity recognition issues. HippoRAG first extracts relevant entities from questions and then retrieves chunks based on these entities. How- ever, it sometimes fails to identify appropriate entities (Entity Recognition Errors, ER Errors), and we conduct corresponding experiments, as shown in Table 8. For ex- ample, on HotpotQA, HippoRAG fails to detect entities in 5.0%