50 38 AR 92 65 58 62 50 (c) Empowerment VR LR C1 C2 AR VR 50 46 14 18 4 LR 54 50 48 31 22 C1 86 52 50 70 31 C2 82 69 30 50 30 AR 96 78 69 70 50 (d) Overall Figure 3: Head-to-head win rates for abstract QA, comparing each row method against each column (higher is better). VR, LR, and AR denote Vanilla RAG, HyLightRAG, and ArchRAG, respectively. within the corpus. According to the selected level of commu- nities (2024), GGraphRAG can be further categorized into C1 and C2, representing high-level and intermediate-level communities, respectively, with C2 as the default. Metrics & Implementation. For the specific QA tasks, we use Accuracy and Recall to evaluate performance on the first two datasets based on whether gold answers are in- cluded in the generations instead of strictly requiring exact matching, following (2024; 2022; 2023). We also use the official metrics of BLEU, METEOR, and ROUGE-l F1 in the NarrativeQA dataset. For the abstract QA task, we fol- low prior work (2024) and adopt a head-to-head compari- son approach using an LLM evaluator (GPT-4o). Overall, we utilize four evaluation dimensions: Comprehensiveness, Diversity, Empowerment, and Overall. For implementation, we mainly use Llama 3.1-8B (2024) as the default LLM and use nomic-embed-text (2024) as the text embedding model. We use KNN for graph augmentation and the weighted Lei- den algorithm for community detection. For retrieval item k, we search the same number of items at each layer, with k = 5 as the default. All methods are required to complete index construction and query execution within 3 days, re- spectively. Additional details are provided in the appendix, and our codes are provided in the supplementary material. Overall results We compare our method with baseline methods in solving