X.; and Hooi, B. 2024. G-retriever: Retrieval- augmented generation for textual graph understanding and question answering. arXiv preprint arXiv:2402.07630. Hu, Y .; and Lu, Y . 2024. Rag and rau: A survey on retrieval- augmented language model in natural language processing. arXiv preprint arXiv:2404.19543. Hu, Z.; Xu, Y .; Yu, W.; Wang, S.; Yang, Z.; Zhu, C.; Chang, K.-W.; and Sun, Y . 2022. Empowering language models with knowledge graph reasoning for question answering. arXiv preprint arXiv:2211.08380. Huang, Y .; and Huang, J. 2024. A Survey on Retrieval- Augmented Text Generation for Large Language Models. arXiv preprint arXiv:2404.10981. Huang, Y .; Zhang, S.; and Xiao, X. 2025. KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG. arXiv preprint arXiv:2502.09304. Jeong, S.; Baek, J.; Cho, S.; Hwang, S. J.; and Park, J. C. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. arXiv preprint arXiv:2403.14403. Jiang, J.; Zhou, K.; Dong, Z.; Ye, K.; Zhao, W. X.; and Wen, J.-R. 2023. Structgpt: A general framework for large lan- guage model to reason over structured data. arXiv preprint arXiv:2305.09645. Koˇcisk`y, T.; Schwarz, J.; Blunsom, P.; Dyer, C.; Hermann, K. M.; Melis, G.; and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. Transactions of the As- sociation for Computational Linguistics, 6: 317–328. Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa, Y . 2022. Large language models are zero-shot reason- ers. Advances in neural information processing systems, 35: 22199–22213. Li, D.; Yang, S.; Tan, Z.; Baik, J. Y .; Yun, S.; Lee, J.; Chacko, A.; Hou, B.; Duong-Tran, D.; Ding, Y .; et al. 2024. DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer’s Disease Questions with Scientific Literature. arXiv preprint arXiv:2405.04819. Li, Y .; Wang, S.; Ding, H.; and Chen, H. 2023. Large lan- guage models in