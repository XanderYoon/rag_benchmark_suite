is the number of query results. • Stopping Condition: The traversal terminates if a node x expanded from Q satisfies d(n, q) > d(n, f), where f is the furthest node in K from the query point q. After completing the hierarchical search and obtaining the ACs and entities from each layer, we further extract their associated textual information. In particular, at the bottom layer, we also extract the relationships between the retrieved entities, resulting in the textual subgraph representation de- noted as R0. We denote all the retrieved textual information from each layer as Ri, where i ∈ 0, 1, . . . , L, which will be used in the adaptive filtering-based generation process. Adaptive filtering-based generation. While some opti- mized LLMs support longer text inputs, they may still en- counter issues such as the “lost in the middle” dilemma (Liu et al. 2024b). Thus, direct utilization of retrieved informa- tion comprising multiple text segments for LLM-based an- swer generation risks compromising output accuracy. To mitigate this limitation, we propose an adaptive filtering-based method that harnesses the LLM’s inherent reasoning capabilities. We first prompt the LLM to extract and generate an analysis report from the retrieved informa- tion, identifying the parts that are most relevant to answer- ing the query and assigning relevance scores to these reports. Then, all analysis reports are integrated and sorted, ensuring that the most relevant content is used to summarize the final response to the query, with any content exceeding the text limit being truncated. This process can be represented as: Ai = LLM(Pf ilter||Ri) (1) Output = LLM(Pmerge||Sort({A0, A1, · · · , An})) (2) where Pf ilter and Pmerge represent the prompts for extract- ing relevant information and summarizing, respectively, Ai, i ∈ 0 · · · n denotes the filtered