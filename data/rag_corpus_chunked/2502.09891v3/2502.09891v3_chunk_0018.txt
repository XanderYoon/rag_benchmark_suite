we search the same number of items at each layer, with k = 5 as the default. All methods are required to complete index construction and query execution within 3 days, re- spectively. Additional details are provided in the appendix, and our codes are provided in the supplementary material. Overall results We compare our method with baseline methods in solving both abstract and specific QA tasks. • Results of abstract QA tasks. We compare ArchRAG against baselines across four dimensions on the Multihop- RAG dataset. For the LightRAG, we only compare the Hy- LightRAG method, as it represents the best version (2024). As shown in Figure 3, GGraphRAG outperforms other base- line methods, while our method achieves comparable perfor- mance on the diversity and empowerment dimensions and significantly surpasses it on the comprehensive dimension. Overall, by leveraging ACs, ArchRAG demonstrates supe- rior performance in addressing abstract QA tasks. • Results of specific QA tasks. Table 2 reports the performance of each method on three datasets. Note that GGraphRAG fails to complete querying on the NarrativeQA dataset within the 3-day time limit. RAPTOR is unable to build the index on datasets like HotpotQA, which con- tains a large number of text chunks. Its Gaussian Mix- ture Model (GMM) clustering algorithm requires prohibitive computational time and suffers from non-termination issues during clustering. Clearly, ArchRAG demonstrates a sub- stantial performance advantage over other baseline meth- ods on these datasets. The experimental results suggest that not all communities are suitable for specific QA tasks, as the GGraphRAG performs poorly. Furthermore, GraphRAG does not consider node attributes during clustering, which causes the community’s summary to become dispersed, making it difficult for the LLM to extract relevant informa- tion from a large number of communities. Thus, we gain an interesting insight: LLM may not be a