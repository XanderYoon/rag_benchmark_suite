clustering method. • To index ACs, we propose a novel hierarchical index structure called C-HNSW and also develop an efficient online retrieval method. • Extensive experiments show that ArchRAG is both highly effective and efficient, and achieves state-of-the- art performance on both abstract and specific QA tasks. Related Work In this section, we review the related works, includ- ing Retrieval-Augmentation-Generation (RAG) approaches, and LLMs for graph mining and learning. • RAG approaches. RAG has been proven to excel in many tasks, including open-ended question answer- ing (2024; 2023), programming context (2024b; 2023), SQL rewrite (2025; 2024), and data cleaning (2024; 2022; 2024). The naive RAG technique relies on retrieving query- relevant contexts from external knowledge bases to mit- igate the “hallucination” of LLMs. Recently, many RAG approaches (2025; 2024; 2024; 2024; 2024c; 2024; 2024; 2024) have adopted graph structures to organize the infor- mation and relationships within documents, leading to im- proved performance. For more details, please refer to the re- cent survey of graph-based RAG methods (2024). • LLM for graph mining. Recent advances in LLMs have offered opportunities to leverage LLMs in graph min- ing. These include using LLMs for KG construction (2024), addressing complex graph mining tasks (2024a; 2024; 2024; 2024b), and employing KG to enhance the LLM reason- ing (2023; 2024c; 2023; 2023; 2024; 2023; 2025). For in- stance, RoG (2023) proposes a planning-retrieval-reasoning framework that retrieves reasoning paths from KGs to guide LLMs conducting faithful reasoning. StructGPT (2023) and ToG (2023) treat LLMs as agents that interact with KGs to find reasoning paths leading to the correct answers. Our Approach ArchRAG We begin by presenting the overall workflow and design ra- tionale of ArchRAG, followed by detailed descriptions of each component. As illustrated in Figure 2, our proposed ArchRAG consists of two phases. In the