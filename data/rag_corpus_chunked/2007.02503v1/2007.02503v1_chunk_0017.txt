the GRU operation in Eq. (10), the basic idea is to first project the video sequence representation V into multiple embedding spaces and perform scaled dot-product attention between query frame and key frame, followed by a softmax operation to obtain the nor- malized weights on the value frames. We finally concatenate the outputs from multiple attention spaces as the final value: ˆVi = Softmax  1√di  Wi Q V T Wi K V  Wi V V, (11) ˆV = N orm  V + Wp  Concat  ˆV1, ˆV2, · · · , ˆVZ  , (12) where Wi Q ∈ Rdi ×dv , Wi K ∈ Rdi ×dv , and Wi V ∈ Rdi ×dv are three trainable parameters that transform the original input V to the query, key, and value matrices in the i-th attention space with dimension di . ˆVi ∈ Rdi ×M denotes the attended value in the i- th attention space. Concat (·) denotes the concatenation operation. Wp ∈ Rdv ×dv is a trainable parameter that projects the concate- nated features into original space. N orm(·) denotes the LayerNorm operation. ˆV = {ˆ vt ∈ Rdv }M t =1 is the final video sequence repre- sentation. The above multi-head attention mechanism allows the model to jointly attend to information from different representation spaces at different positions, which effectively captures the feature interaction among frames. Temporal-attentive Video Representation . To make informa- tive frames (e.g., foreground frames) contribute more to the final video representation, we design a temporal attention neural net- work with three trainable parameters uva ∈ Rdv a , bva ∈ Rdv a , and Wva ∈ Rdv a ×dv : ηt = Softmax  uT vaσ Wva ˆ vt + bva / p dva  , ¯ v= MÕ t