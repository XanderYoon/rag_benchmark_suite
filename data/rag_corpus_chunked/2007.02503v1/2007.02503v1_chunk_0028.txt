fusion is not our focus in this paper, we leave the fusion experiment on MSR-VTT for future study. 4.2.2 Ablation Studies. To effectively answer the research question R2, we conduct extensive ablation studies on MSR-VTT based on the standard split. Specifically, we mainly organize the ablation studies into two groups: one for query encoder and the other for video encoder. The batch normalization is used to normalize the query/video representation in the following counterparts. On Query Encoder: We use the following baselines and variants to transform the natural language queries to vector representations. • WordEmb+AvgPand WordEmb+MaxP: Add a fully connected (FC) layer after the word embedding layer and aggregate its output with average-pooling (AvgP) operation or max-pooling operation (MaxP). • LSTM and LSTM+AvgP: Instead of composing the latent seman- tic tree, we directly use the last hidden state (LSTM) or apply an average-pooling over the output of LSTM (LSTM+AP). • TCE (w/o-Cxt): Remove the memory-augmented context vector ut in the score module (Eq. (6)) and directly normalize the scaled dot-product between a global query vector and the hidden state of nodes. It is the standard implementation in [4]. • TCE (w/o-LSTM): Remove the leaf node LSTM module. Instead, we use an FC layer to transform the word embedding to the default input of TreeLSTM. • TCE (w/o-TAtt)+AvgP: Remove the text attention module in Eq. (9), instead, we use the average-pooling operation. On Video Encoder: We use the following baselines and variants to transform videos to vector representations. • Frame+AvgP and Frame+MaxP: Use a FC layer to first trans- form the frame features, followed by average-pooling (AvgP) or max-pooling (MaxP). • GRU and GRU+AvgP: Directly use the last hidden state of GRU or apply average-pooling over the output of GRU (GRU+AvgP). • TCE (w/o-Mha): Remove the multihead attention module. • TCE