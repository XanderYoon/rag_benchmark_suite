the query attention network using the same setting with MSR-VTT. Since [27] did not release the frame-level features, we directly use the provided multi-modal video-level features (appearance, motion, audios and face) to evaluate the effectiveness of our complex-query modeling module. Note that we do not use the gated embedding module and weighted-fusion of similarity scores in [27]. We first transform the multiple-modal features to the default embedding spaces in [27] with multiple projection matrices and concatenate the multiple features into a long vector, followed by a feature trans- formation into the 512-D joint embedding space. 4.1.3 Evaluation Metrics. Following the setting of [7, 27, 49], we report the rank-based performance metrics, namely R@K (K = 1, 5, 10) and Median rank (MedR). R@K is the percentage of test queries for which at least one relevant item is found among the top- K retrieved results. MedR is the median rank of the first relevant Table 2: State-of-the-art performance comparison (%) on LSMDC [33]. Our TCE performs the best with a much lower- dimensional embedding (512-D). The Mot. and Aud. refer to the motion feature and audio feature, respectively. Method R@1 R@5 R@10 MedR C+LSTM+SA+FC7 [40] 4.3 12.6 18.9 98 VSE-LSTM [15] 3.1 10.4 16.5 79 SNUVL [50] 3.6 14.7 23.9 50 Kaufman et al. [14] 4.7 15.9 23.4 64 CT-SAN [51] 5.1 16.3 25.2 46 Miech et al. [26] 7.3 19.2 27.1 52 CCA (FV HGLMM) [16] 7.5 21.7 31.0 33 JSFusion [49] 9.1 21.2 34.1 36 Miech et al. . [28] 7.2 18.3 25.0 44 MEE [27] 10.2 25.0 33.1 29 TCE (Visual) 7.9 20.8 27.8 46 TCE (Visual+Mot.) 9.7 23.3 34.8 32 TCE (Visual+Mot.+Aud.) 10.6 25.8 35.1 29 item in the search results. Higher R@K and lower MedR indicate better performance. 4.1.4 Training Details. Our work is implemented using