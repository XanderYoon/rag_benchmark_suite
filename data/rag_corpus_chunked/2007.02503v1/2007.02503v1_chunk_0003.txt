by aggregating the matching results from different concepts [53]. Despite its efficiency, it is usually ineffec- tive for complex long queries, since they carry complex linguistic context and cannot be simply treated as an aggregation of extracted concepts. Besides, it is also quite challenging to effectively train concept classifiers and select the relevant concepts. 2)Embedding- based paradigm [1, 7, 21, 27, 28, 43, 49], as shown in Figure 1 (b). Recent efforts proposed to learn a joint text-video embedding space [7, 27, 28, 30] to support video retrieval by leveraging the strong representation ability of deep neural networks [17, 34]. The natural language queries are usually transformed into dense vector repre- sentations by Recurrent Neural Networks (RNNs) [34] (e.g., Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)) that are powerful for modeling sequence data. The videos are usually modeled as a temporal aggregation of frame/clip-level features, extracted from pre-trained Convolutional Neural Networks (CNNs) [17]. Both the queries and videos are mapped into a shared em- bedding space where semantically-similar videos and text queries are mapped to close points. Although the embedding-based meth- ods have shown much better performance, simply treating queries holistically as one dense vector representations may obfuscate the keywords or phrases that have rich temporal and semantic cues. Some prior works proposed to transform the complex queries into structured forms, e.g., semantic graph [ 22], to describe the spatial or semantic relations between concepts. However, such so- lutions usually require the text query to be well annotated with syntactic labels (e.g., part of speech (POS) tag) and rely on complex predefined rules to construct the structure of text queries, which make it hard to be applied in a new scenario with different lin- guistic expression patterns. Although so much efforts have been devoted to complex-query video retrieval,