frames (e.g., foreground frames) contribute more to the final video representation, we design a temporal attention neural net- work with three trainable parameters uva ∈ Rdv a , bva ∈ Rdv a , and Wva ∈ Rdv a ×dv : ηt = Softmax  uT vaσ Wva ˆ vt + bva / p dva  , ¯ v= MÕ t =1 ηt ˆ vt , (13) whereηt denotes the normalized importance score of thet-th frame, and ¯ v∈ Rdv denotes the final video representation. Eq. (13) has a similar formulation as Eq. (9), both of which are easy to implement and effective to exploit the informative frame/word features for representation. 3.4 Text-Video Joint Embedding Formally, given a natural language query Q = {q1, q2, · · · , qN } and a video sequence V = {v1,v2, · · · ,vM }, we transform Q and V to low-dimensional vector representations ¯ q∈ Rdt and ¯ v∈ Rdv using the query encoder described in Section 3.2 and the video encoder in Section 3.3, respectively. Then we map the text query and video into a joint embedding space by two linear projection matrices: f t : Rdt → Rd ∗ and f v : Rdv → Rd ∗ , where we define the cross modal matching score as the cosine similarity: s (Q, V) = f t (¯ q)T f v (¯ v) ∥ f t (¯ q)∥2 ∥ f v (¯ v)∥2 , (14) where f t (¯ q) and f v (¯ v) are implemented by f t (¯ q) = W∗ t ¯ q+ b∗ t , f v (¯ v) = W∗ v ¯ v+ b∗ v , (15) where W∗ t ∈ Rd ∗×d t , b∗ t ∈ Rd ∗ , W∗v ∈ Rd ∗×d