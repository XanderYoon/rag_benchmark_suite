202 movies. Each video clip has only one caption, either extracted from the movie script or from the transcribed audio description. It is originally used for evaluation in the Large Scale Movie Description Challenge (LSMDC). In this work, we only consider the text-to-video task in LSMDC: given a natural language query, the system retrieves the video of interest from the 1,000 test video set. 4.1.2 Implementation Details. On MSR-VTT, for the word features, we initialize the word embedding matrix using a 500-D word2vec model provided by [6] which optimized word2vec on English tags of 30 million Flickr images. The textual sequence is fed into a uni- directional LSTM with the hidden size of dt =512 for leaf node transformation. The hidden sizes of the TreeLSTM and query at- tention modules are set to dt =512 and dt a=256, respectively. The final query representation has the dimension of dt =512. For the video features, we use the frame-level visual features provided by [7], where the 2048-D features are extracted with ResNet-152 [10] pre-trained on ImageNet. The video frame sequences are fed in a Table 1: State-of-the-art performance comparison (%) on MSR-VTT with different dataset splits. Note that TCE uses bidirectional GRU and LSTM for better performance in this experiment based on 1024-D query and video embeddings. Method R@1 R@5 R@10 MedR Data split from [44] Dong et al. [6] 1.8 7.0 10.9 193 Mithun et al. [30] 5.8 17.6 25.2 61 DualEncoding [7] 7.7 22.0 31.8 32 TCE 7.7 22.5 32.1 30 Data split from [27] Random 0.3 0.7 1.1 502 CCA [43] 7.0 14.4 18.7 100 MEE [27] 12.9 36.4 51.8 10.0 MMEN (Caption) [43] 13.8 36.7 50.7 10.3 JPoSE [43] 14.3 38.1 53.0 9 TCE 17.1 39.9 53.7 9 Data split from [49] Random 0.1 0.5 1.0 500 C+LSTM+SA+FC7