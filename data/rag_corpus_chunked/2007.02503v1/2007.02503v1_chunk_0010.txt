parent nodes (i.e., constituents). A parent node takes in two adjacent child nodes and describes more com- plex semantics than its child nodes. In this section, we first briefly describe how to apply TreeLSTM to compute the parent node repre- sentation from its two child nodes and then describe how to select the parent node at each layer for recursively building the LST. TreeLSTM. Given the representations of two adjacent child nodes (hi , ci ) and (hi+1, ci+1) as inputs, the parent node representation hp , cp  is computed by  i fl fr o g  =  σ σ σ σ tanh   Wp  hi hi+1  + bp  , (1) cp = fl ⊙ ci + fr ⊙ ci+1 + i ⊙ g, (2) hp = o ⊙ tanh(cp ), (3) where Wp ∈ R5dt ×2dt and bp ∈ R5dt are trainable parameters, σ(·) denotes the activation function sigmoid, and ⊙ denotes the element-wise product. Similar to the standard LSTM, each node is represented by a hidden state h ∈ Rdt and a cell state c ∈ Rdt . Layer-wise Node Transformation . At the bottom layer, given a query with N words as inputs, we first represent it as a se- quence of word embeddings and then transform the word embed- dings to the representations of leaf nodes at the corresponding locations. Assume the t-th layer of the LST consists of Nt nodes {rt i = (ht i , ct i )}Nt i=1. If two adjacent nodes rt i and rt i+1 are selected to be merged, then we can utilize the above-mentioned TreeL- STM to compute the representation of the parent node rt +1 i = TreeLSTM(rt i , rt i+1) at the t + 1 layer. The representations of