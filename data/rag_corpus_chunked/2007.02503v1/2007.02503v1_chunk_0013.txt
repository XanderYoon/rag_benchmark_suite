ht +1 i ut +1 i  + bs  / p 2dt  , (6) whereσ(·) is the nonlinear activation functionReLU and ws ∈ R2dt , bs ∈ R2dt , and Ws ∈ R2dt ×2dt are trainable parameters. The main intuition of this node scoring module is to inject the semantic context into each decision for a better parent node selection. In such a recursive process, we select the candidate with the maxi- mum validity score using Eq. (6) based on the Straight-Through (ST) Gumberl-Softmax estimator [8]. In the forward pass, the ST Gumberl-Softmax estimator discretizes the continuous signal, while in the backward pass, the continuous signals are used for stable training. Note that only the representation of the selected node is updated using the outputs of Eq. (1), (2), and (3). The other nodes that are not selected are not updated. The above procedure is recursively repeated until only a single node is left. By this procedure, we can automatically compose a N -layers binary latent semantic tree with semantically-meaningful constituents to better understand the complex query without any syntactic annotations. 3.2 Tree-augmented Query Encoder One-hot Representation. Given a query Q = {q1, q2, · · · , qN }, we first represent it as a sequence of one-hot vectors{q′ 1, q′ 2, · · · , q′ N }, where q′ t indicates the vector of the t-th word. We further convert the word vectors to word dense representations {q1, q2, · · · , qN } based on pretrained word embedding matrix [29]. Leaf Node LSTM. We use RNNs as the basic sequence modeling block. For keeping consistency with the TreeLSTM in our LST module, we use LSTM to transform the word embeddings to the leaf node representations at the bottom layer. More formally, the LSTM