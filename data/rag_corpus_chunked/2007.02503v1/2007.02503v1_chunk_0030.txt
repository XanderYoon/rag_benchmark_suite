Com- pared with the baselines WordEmb+AvgP and Frame+AvgP, the LSTM and GRU help to improve the accuracy by a small margin, due to the modeling of the dependence between words/frames. However, if we remove LSTM or GRU from our query encoder or video encoder, the model exhibits a minor performance de- generates. TCE (w/o-LSTM) and TCE (w/o-GRU) still report high accuracy. This indicates the effectiveness of our latent seman- tic tree in capturing the structure information of the complex queries. It also reveals the necessity of the temporal interaction module that models the frame-wise feature interaction beyond the dependence between consecutive frames. â€¢ We also observe that the widely used average-pooling strategy does not performs well for the complex-query video retrieval task. Our introduced attention mechanisms in Eq. (9) and (13) performs well by attending to the informative constituent nodes and frames. 4.2.3 Analysis on Different Types of Queries.( R3). To investigate how our proposed TCE perform on different groups of complex queries, we group 59,800 test queries of the MSR-VTT dataset (Data split from [44]) according to their query lengths and categories. We compare TCE on different groups with the baseline model DualGRU which utilizes the bidirectional GRU with average pooling for both text encoding and video encoding. The performance comparison Table 3: Ablation studies on the MSR-VTT dataset using the standard dataset split [44] to investigate the effects of the tree-based query encoder and the temporal-attentive video encoder. The proposed method performs the best. Method R@1 R@5 R@10 MedR On Query Encoder WordEmb+AvgP 6.79 20.98 30.68 32 WordEmb+MaxP 5.92 18.90 27.82 40 LSTM 6.91 21.31 31.17 31 LSTM+AvgP 6.95 21.28 30.68 35 TCE (w/o-Cxt) 6.98 21.46 31.49 30 TCE (w/o-LSTM) 7.09 21.86 31.67 31 TCE (w/o-TAtt)+AvgP 6.59 20.57 30.48 34 On Video Encoder Frame+AvgP 6.67 20.41 29.89 36