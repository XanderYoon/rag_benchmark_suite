we design a memory-augmented scoring module fscor e (· ; Θscor e ) to select the parent node: st +1 i = fscor e  rt +1 i , ut +1 i ; Θscor e  , (4) where st +1 i denotes the probability of thei-th parent node candidate being selected and ut +1 i is a node-specific context vector derived from a global memory M which stores the semantic context. The global memory is defined as the set of leaf node hidden state rep- resentations M = [h1 1, h1 2, · · · , h1 N ] ∈ RN ×dt at the bottom layer which preserves the original semantic context in the given sentence. To obtain the context vector ut +1 i , we use the hidden state ht +1 i of each parent node candidate to query the global memory and then attentively aggregate the global memory M: at +1 i j = Softmax  (ht +1 i )Tσ(Wmh1 j + bm )/ p dt  , ut +1 i = (at +1 i )TM, (5) where at +1 i = [at +1 i1 , at +1 i2 , · · · , at +1 i N ] is the normalized attention vector over the memory and Wm ∈ Rdt ×dt and bm ∈ Rdt are trainable parameters. We then implement our scoring module as: st +1 i = Softmax  wT sσ  Ws  ht +1 i ut +1 i  + bs  / p 2dt  , (6) whereσ(·) is the nonlinear activation functionReLU and ws ∈ R2dt , bs ∈ R2dt , and Ws ∈ R2dt ×2dt are trainable parameters. The main intuition of this node scoring module is to inject the semantic context into each decision for a better parent