dense representations {q1, q2, · · · , qN } based on pretrained word embedding matrix [29]. Leaf Node LSTM. We use RNNs as the basic sequence modeling block. For keeping consistency with the TreeLSTM in our LST module, we use LSTM to transform the word embeddings to the leaf node representations at the bottom layer. More formally, the LSTM unit, at the i-th time step, takes the features of the current word qi , previous hidden state h1 i−1, and cell state c1 i−1 as inputs, and yields the current hidden state h1 i and cell state c1 i : h1 i , c1 i  = LSTM qi , h1 i−1, c1 i−1 . (7) Eq. (7) functions as the leaf node transformation module. Tree Construction. The outputs of Eq. (7) are directly fed into the TreeLSTM module for the transformation of parent node candidates, as detailed in Eq. (1), (2), and (3). AfterN steps of the transformation, scoring, and selection, as described in the Section 3.1, we recur- sively compose a N -layers latent semantic tree, consisting of N -1 constituent nodes (i.e., parent nodes), formulated by {e1, e2, · · · , eN −1} = LSTree({q1, q2, · · · , qN }), (8) where LSTree indicates the overall tree construction procedure and ei ∈ Rdt denotes the representation of the i-th constituent node. The tree can clearly describe the syntactic structure of complex queries, which is helpful to better understand the user query. A similar procedure of tree construction can be found in [4]. Structure-aware Query Representation. The next step is to de- rive the query representation based on the recursively extracted constituent nodes in the LST. In previous work [4, 36], only the last constituent node is used for task-specific inference. However, as mentioned