much more diverse, and it is hard to learn the relation between news words and the visual scenes with limited training data. Despite the varied difficulty for each group, our proposed TCE model consistently beats the baseline on all groups. 4.2.4 Qualitative Analysis. Figure 4 shows four cases of qualita- tive results about the composed tree structures and the retrieved videos. For each query, the top three videos retrieved from the MSR- VTT are showed. Although only one correct video is annotated for each query, the retrieved three videos in Figure 4 are typically semantically relevant to the given query to some extent, showing the effectiveness of TCE. We observe that our approach is able to construct syntactically reasonable tree structures (e.g., Query 1 and Query 2) and also identify the informative constituent nodes based on the attention mechanism, thus being helpful to better understand the complex query. For example, in Query 1, “performs and signs autographs” describes the action of the video clip, which is easy to be visually distinguished and usually reflects the main search intention, whose corresponding node in the tree was assigned a relatively large weight of 0.265. In Query 2, “a person playing in- struments” refers to the key search intention, whose corresponding node in the tree was assigned a relatively large weight of 0.236. For Query 3, the composed tree is far from perfect, while the node “at a” contains less semantic information and was reasonably assigned the smallest attention weight of 0.008. For Query 4, the composed tree is syntactically reasonable, but some relative important nodes were assigned with small attention weights. Although some noises have been introduced in the latent semantic tree construction, our model still finds relevant videos for Query 4, which shows the robustness of our model. 5 CONCLUSION In