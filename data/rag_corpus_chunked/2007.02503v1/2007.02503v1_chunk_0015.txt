is helpful to better understand the user query. A similar procedure of tree construction can be found in [4]. Structure-aware Query Representation. The next step is to de- rive the query representation based on the recursively extracted constituent nodes in the LST. In previous work [4, 36], only the last constituent node is used for task-specific inference. However, as mentioned previously, the complex query usually consists of multi- ple visual concepts and their reference descriptions, in which some concepts or reference descriptions may not have clear visual evi- dence or just have very short temporal durations in the videos. The last constituent node may not effectively cover the full linguistic context of the complex queries. In this work, we introduce an at- tention network to explore the importance of each constituent and then derive the structure-aware query representation by attending to the informative constituent nodes: βi = Softmax  uT t aσ Wt aei + bt a / p dt a  , ¯ q= NÕ i=1 βi ei , (9) whereσ(·) is the non-linear activation function ReLU and Wt a ∈ Rdt a×dt , bt a ∈ Rdt a, and ut a ∈ Rdt a are trainable parameters, βi denotes the normalized importance score of the node ei , and ¯ q∈ Rdt denotes the query representation that aggregates the representations of all constituent nodes. 3.3 Temporal-Attentive Video Encoder Given a video clipV, we first sample uniformly a sequence of video frames {v1,v2, · · · ,vM } from V with a pre-specified interval. We extract the frame features using pre-trained CNNs and represent the video clip as V = {vt }M t =1 where vt ∈ Rd ∗ v denotes the frame vector of the t-th frame. In this paper, we deal with two types of video characteristics: