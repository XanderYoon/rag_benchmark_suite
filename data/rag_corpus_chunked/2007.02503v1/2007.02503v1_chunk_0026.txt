evaluate our method on the other two splits from [27] and [49], respectively. Unless otherwise stated, we use unidirectional RNNs (512-D) in our experiments by default. MSR-VTT: Table 1 clearly shows that our proposed TCE outper- forms all other available methods in all three dataset splits. Specifi- cally, on the first split [44], we surpass the results of DualEncoding [7] w.r.t. R@5, R@10, and MedR. DualEncoding is the best reported state-of-the-art method on the first split that fuses multi-levels tex- tual and video features for joint embedding learning with the embed- ding size of 2048. While, our TCE just uses the temporal/sequential features (i.e., the 2nd level features in DualEncoding) with the final embedding size of 1024 for retrieval. Hence, TCE is able to report higher retrieval accuracy while using a much smaller embedding size. TCE also outperforms the multimodal fusion (object, activ- ity, and audio) method in Mithun et al. [30] by a large margin, which indicates the effectiveness of our proposed tree-augmented query modeling and temporal-attentive video sequence modeling methods. Note that our proposed TCE can achieve consistent per- formance improvement if we integrate some other modalities, like motion features or audio features into the video embedding. We evaluate our method in the multi-modality setting on LSMDC (See Table 2). For the second split [27], we observe a large improvement over the state-of-the-art JPoSE which disentangles the text query into multiple semantic spaces (Verb, Noun) for score-level fusion. Compared with JPoSE, TCE directly composes a latent semantic tree to describe the user complex query in an end-to-end manner and also includes an attention mechanism to capture the most in- formative constituent nodes in the tree. A similar improvement can also be observed in the third split [49], which further validates the effectiveness of TCE. LSMDC: Table 2 compares