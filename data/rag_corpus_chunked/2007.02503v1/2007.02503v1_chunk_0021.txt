form compared with state-of-the-art methods? (2)R2: What are the impacts of different components on the overall performance of our approach? (3) R3: How does the proposed method perform on dif- ferent types of complex queries (e.g., different lengths and different categories)? Can the latent semantic tree help to better understand the complex query and drive stronger query representation? 4.1 Experimental Settings 4.1.1 Datasets. We use two public datasets: MSR-VTT video cap- tion dataset [44] and LSMDC movie description dataset [33]. MSR-VTT [44]: It is an increasingly popular dataset for text-to- video retrieval, consisting of 10K YouTube video clips. Each of them is annotated with 20 crowd-sourced English sentences, which re- sults in a total of 200K unique video-caption pairs. We notice that there are three different dataset partitions for this dataset. The first one is the official partition from [44] with 6,513 clips for training, 497 clips for validation, and the remaining 2,990 clips for testing. The second one is from [27] with 6,656 clips for training and 1000 test clips for testing. The last one is from [49], 7,010 and 1K video clips are used for training and testing respectively. Note for the last two data partitions, only one sentence associated with each video clip is used as the testing query. For a comprehensive evaluation, we evaluate our proposed model on all data partitions. LSMDC [33]: It is another popular dataset that contains 118,081 short video clips extracted from 202 movies. Each video clip has only one caption, either extracted from the movie script or from the transcribed audio description. It is originally used for evaluation in the Large Scale Movie Description Challenge (LSMDC). In this work, we only consider the text-to-video task in LSMDC: given a natural language query, the system retrieves the video of interest from the