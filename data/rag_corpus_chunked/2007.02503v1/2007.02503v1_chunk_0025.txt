JSFusion [49] 9.1 21.2 34.1 36 Miech et al. . [28] 7.2 18.3 25.0 44 MEE [27] 10.2 25.0 33.1 29 TCE (Visual) 7.9 20.8 27.8 46 TCE (Visual+Mot.) 9.7 23.3 34.8 32 TCE (Visual+Mot.+Aud.) 10.6 25.8 35.1 29 item in the search results. Higher R@K and lower MedR indicate better performance. 4.1.4 Training Details. Our work is implemented using the Py- Torch framework. We train our model using the ADAM optimizer and use an initialized learning rate of 0.0005 with a batch size of 128. Each epoch training is just performed using a single GPU and takes no more than 10 minutes. 4.2 Experimental Results and Analysis 4.2.1 Comparison with State-of-the-Arts. To answer the research question R1, we compare our proposed Tree-augmented Cross- modal Encoding (TCE) with recently proposed state-of-the-art methods: (1) RNN-based methods: DualEncoding [7], Kaufman et al. [14], CT-SAN [51], SNUVL [ 50], C+LSTM+SA+FC7 [40], and VSE-LSTM [15], (2) Multimodal Fusion methods: Mithun et al. [30] , MEE [27], MMEN [43], and JPoSE [ 43], and (3) other methods: JSFusion [49], CCA (FV HGLMM) [16], and Miech et al. [26]. The experimental results on MSR-VTT and LSMDC are summarized, respectively, in Table 1 and Table 2. Note that there are different dataset splitting strategies of the MSR-VTT dataset. To fairly com- pare with the reported results of state-of-the-art methods, we first report our results based on the standard split from the official pa- per [44] and then evaluate our method on the other two splits from [27] and [49], respectively. Unless otherwise stated, we use unidirectional RNNs (512-D) in our experiments by default. MSR-VTT: Table 1 clearly shows that our proposed TCE outper- forms all other available methods in all three dataset splits. Specifi- cally, on the first split [44], we surpass the results of DualEncoding [7]