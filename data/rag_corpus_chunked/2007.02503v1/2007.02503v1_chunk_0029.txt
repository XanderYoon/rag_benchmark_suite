variants to transform videos to vector representations. • Frame+AvgP and Frame+MaxP: Use a FC layer to first trans- form the frame features, followed by average-pooling (AvgP) or max-pooling (MaxP). • GRU and GRU+AvgP: Directly use the last hidden state of GRU or apply average-pooling over the output of GRU (GRU+AvgP). • TCE (w/o-Mha): Remove the multihead attention module. • TCE (w/o-GRU): Replace the GRU module with a FC layer to transform the frame embedding. • TCE (w/o-V Att)+AvgP: Remove the temporal video attention module in Eq. (13) with an average-pooling operation instead. Note that in the ablation studies, we only change one (e.g., query) part of our proposed TCE to the above baselines or variants, while keeping the rest of TCE (e.g., video) unchanged. Table 3 shows the performance comparison of our proposed full TCE model with different ablations on the MSR-VTT dataset. • Overall, we observe that our full model performs the best except in terms of MedR. Removing each component from TCE, such as Cxt, Mha, LSTM/GRU, and TAtt/V Att, would result in relative performance degeneration, but not dramatically. It not only re- flects the effectiveness of each component of our TCE, but also shows the robustness of our method. Each module can effectively complement each other, but is not very sensitive to each other. • There are also some interesting findings: the RNNs do not play a much more important role than we wish in this task. Com- pared with the baselines WordEmb+AvgP and Frame+AvgP, the LSTM and GRU help to improve the accuracy by a small margin, due to the modeling of the dependence between words/frames. However, if we remove LSTM or GRU from our query encoder or video encoder, the model exhibits a minor performance de- generates. TCE (w/o-LSTM) and TCE (w/o-GRU) still report high