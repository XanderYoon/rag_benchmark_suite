(¯ v)∥2 , (14) where f t (¯ q) and f v (¯ v) are implemented by f t (¯ q) = W∗ t ¯ q+ b∗ t , f v (¯ v) = W∗ v ¯ v+ b∗ v , (15) where W∗ t ∈ Rd ∗×d t , b∗ t ∈ Rd ∗ , W∗v ∈ Rd ∗×d v , and b∗v ∈ Rd ∗ are trainable parameters. We expect Eq. (14) to yield a higher score when the video V is matched with the complex query Q or a lower score if not match. We also apply a batch normalization [13] followed by a non-linear activation Tanh(·) on f t (¯ q) and f v (¯ v), respectively, for stable training. Note that both f t (·) and f v (·) are not indispensable if we enforce the output of the query encoder to have the same dimension as the output of video encoder, i.e., dt = dv . We introduce f t (·) and f v (·) in Eq. (14) just for more formal expression and also make the section 3.4 self-contained. Be- sides, with f t (·) and f v (·) , we can derive much lower dimensional embeddings for fast retrieval without modifying the parameters in the two encoders. Loss Function: To train the model, we use the margin ranking loss to optimize the network with a batch-hard negative sampling strategy. More formally, during training, we sample a batch of query-video pair X = {(Qi , Vi )}B i=1. We wish to enforce that, for any given (Qi , Vi ), the similarity score s (Qi , Vi ) between a query Qi and its ground truth video Vi is larger than the score of any negative pairs s Qi , Vj  by