Compared with JPoSE, TCE directly composes a latent semantic tree to describe the user complex query in an end-to-end manner and also includes an attention mechanism to capture the most in- formative constituent nodes in the tree. A similar improvement can also be observed in the third split [49], which further validates the effectiveness of TCE. LSMDC: Table 2 compares the performance of TCE with nearly all reported results on the LSMDC video clip retrieval task. The results again show that our proposed TCE performs the best on this challenging benchmark dataset. Specifically, we outperform the MEE method by a relative improvement of 6% w.r.t. R@10. We use the same multi-modal video features with MEE, but with a simpler feature fusion strategy, i.e., concatenation. We can observe a more significant improvement over the JSFusion method, which is the winner of the LSMDC 2017 Text-to-Video and Video-to-Text retrieval challenge. Besides, in Table 2, we also investigate the effect of multi-modal fusion in our proposed TCE. Specifically, when we just use the 2048-D globally-pooled appearance features to describe the video, our model still outperforms most of the listed methods in Table 2. By augmenting the video representation with the motion feature, we can obtain a relative improvement of 25% w.r.t. R@10. The audio features can further stably improve the performance. That is to say, TCE has the potential of improving its performance on MSR-VTT by leveraging more informative features. Since the multi-modal fusion is not our focus in this paper, we leave the fusion experiment on MSR-VTT for future study. 4.2.2 Ablation Studies. To effectively answer the research question R2, we conduct extensive ablation studies on MSR-VTT based on the standard split. Specifically, we mainly organize the ablation studies into two groups: one for query encoder and the other for video encoder.