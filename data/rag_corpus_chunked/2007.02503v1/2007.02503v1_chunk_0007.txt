28, 35, 43]. However, they ignored the sequential order in textual queries. To alleviate this, Mithun et al. [30] utilized GRU for modeling the word orders. Further, Dong et al. [21] and Li et al. [21] jointly employed multiple text embedding strategies in- cluding bag-of-words, word2vec, and GRU, to obtain robust query representation. In a follow-up work [ 7], Dong et al. proposed a multi-level text encoding to capture the global, local, and temporal patterns in the textual queries. Despite their effectiveness, these methods simply treating queries holistically as one dense vector representations, which may obfuscate the keywords or phrases that CNN RNN Frame-wise Temporal Interaction Video Embedding Attention RNN Word Embedding A baby plays with a cat Latent Semantic Tree Constituent Node Embedding Query Embedding Attention A boys plays with a cat Bottom-to-Up Complex query : Three opera singers sing with a live orchestra ... 1 2 3 Search Frame Embedding Video Database User (a) Text-Video Joint Embedding Learning (b) Text-to-Video Retrieval Figure 2: An illustration of our tree-augmented cross-modal encoding method for complex-query video retrieval. have rich semantic cues and are less interpretable than the concept- based paradigm. In this work, we explicitly explore the syntactic structure of natural language query, thus will help to better under- stand the search intention. Lin et al. [22] and Xu et al. [45] have made attempts in this direction. Lin et al. first obtained the parse tree of the textual query, and modeled the word dependency based on a series of manually derived rules. In [45], Xu et al. constructed the dependency-tree structure based on subject-verb-object triplets extracted from a sentence and modeled the structure by a recursive neural network. For video embedding, a typical approach is to first extract the frame-level features by pre-trained CNNs and subsequently ag- gregate them