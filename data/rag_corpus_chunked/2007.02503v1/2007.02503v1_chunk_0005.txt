then attentively aggregates the frame embeddings into the temporal-attentive video representation. Finally, both the user queries and videos are mapped to a text-video joint embedding space where semantically-similar videos and text queries are mapped to close points. All the modules are jointly optimized in an end-to-end fashion using only the paired query-to-video supervision. We evaluate the proposed approach on two large-scale text-to-video retrieval datasets, which clearly demonstrates the effectiveness of each component in our approach. The contributions of this paper are roughly summarized as follows: • We develop a novel complex-query video retrieval framework that can automatically compose a flexible tree structure to model the complex query and derive the query and video representa- tions in a joint text-video embedding space. • We design a memory-augmented node scoring and selection method to explore linguistic context for the tree construction. We also introduce the attention mechanism into the encodings of complex queries and videos, which can identify the informative constituent nodes and frames. • We conduct extensive experiments on large-scale datasets to demonstrate that our approach can achieve state-of-the-art re- trieval performance. 2 RELATED WORK In this section, we briefly introduce two representative research directions in text-based video retrieval. One is the concept based methods and the other one is the embedding based methods. Concept based methods [18, 24, 25, 31, 41] mainly rely on estab- lishing cross-modal associations via concepts [12]. Markatopoulou et al. [24, 25] first utilized relatively complex linguistic rules to extract relevant concepts from a given query and used pre-trained CNNs to detect the objects and scenes in video frames. Then the similarity between a given query and a specific video is measured by concept matching. Ueki et al. [41] depended on a much larger con- cept vocabulary. In addition to pre-trained CNNs, they additionally trained SVM-based