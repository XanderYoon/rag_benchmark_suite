linguistic rules to extract relevant concepts from a given query and used pre-trained CNNs to detect the objects and scenes in video frames. Then the similarity between a given query and a specific video is measured by concept matching. Ueki et al. [41] depended on a much larger con- cept vocabulary. In addition to pre-trained CNNs, they additionally trained SVM-based classifiers to automatically annotate the videos. Snoek et al. [38] trained a more elegant model, called VideoStory, from freely available web videos to annotate videos, while they still represented the textual query by selecting concepts based on part-of-speech tagging heuristically. Despite the promising per- formance, the concept based methods still face many challenges, e.g., how to specify a set of concepts and how to extract relevant concepts for both textual queries and videos. Moreover, the extrac- tion of concepts from videos and textual queries are usually treated independently, which makes it suboptimal to explore the relations between two modalities. In contrast, our method is concept free and jointly learns the representation of textual queries and videos. Deep learning technologies have been popularly explored for video retrieval recently [7, 20, 21, 27, 28, 30, 43, 45, 49]. Most works proposed to embed textual queries and videos into a common space, and their similarity is measured in this space by distance metric, e.g., cosine distance. For textual query embedding, the word2vec models pre-trained on large-scale text corpora are increasingly pop- ular [27, 28, 35, 43]. However, they ignored the sequential order in textual queries. To alleviate this, Mithun et al. [30] utilized GRU for modeling the word orders. Further, Dong et al. [21] and Li et al. [21] jointly employed multiple text embedding strategies in- cluding bag-of-words, word2vec, and GRU, to obtain robust query representation. In a follow-up work [ 7], Dong