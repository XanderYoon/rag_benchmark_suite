DualEncoding [7] 7.7 22.0 31.8 32 TCE 7.7 22.5 32.1 30 Data split from [27] Random 0.3 0.7 1.1 502 CCA [43] 7.0 14.4 18.7 100 MEE [27] 12.9 36.4 51.8 10.0 MMEN (Caption) [43] 13.8 36.7 50.7 10.3 JPoSE [43] 14.3 38.1 53.0 9 TCE 17.1 39.9 53.7 9 Data split from [49] Random 0.1 0.5 1.0 500 C+LSTM+SA+FC7 [40] 4.2 12.9 19.9 55 VSE-LSTM [15] 3.8 12.7 17.1 66 SNUVL [50] 3.5 15.9 23.8 44 Kaufman et al. [14] 4.7 16.6 24.1 41 CT-SAN [51] 4.4 16.6 22.3 35 JSFusion [49] 10.2 31.2 43.2 13 Miech et al. [28] 12.1 35.0 48.0 12 TCE 16.1 38.0 51.5 10 unidirectional GRU with the hidden size of dv =512. The output of GRU is further fed into an 8-head attention module. The dimension of each head subspace is 64. The temporal attention module with the hidden size of dva =256 aggregates the outputs of multi-head attention module and produces a video representation with the di- mension of dv =512. As mentioned previously, since video encoder and query encoder have the same dimension, we omit the two pro- jection matrices in Eq. (15) for compressing the size of parameters. The number of hard negative samples used in Eq. (16) is 5. On LSMDC, following [27], we use 300-DGoogleNews pre-trained word2vec word embeddings as the input of a unidirectional LSTM with the hidden size of 512, followed by our tree construction and the query attention network using the same setting with MSR-VTT. Since [27] did not release the frame-level features, we directly use the provided multi-modal video-level features (appearance, motion, audios and face) to evaluate the effectiveness of our complex-query modeling module. Note that we do not use the gated embedding module and weighted-fusion of similarity scores in [27]. We first transform