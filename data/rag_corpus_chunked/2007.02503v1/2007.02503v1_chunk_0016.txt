of video frames {v1,v2, · · · ,vM } from V with a pre-specified interval. We extract the frame features using pre-trained CNNs and represent the video clip as V = {vt }M t =1 where vt ∈ Rd ∗ v denotes the frame vector of the t-th frame. In this paper, we deal with two types of video characteristics: 1) temporal dependence between consecutive frames along the sequence, and 2) frame-wise temporal interaction over the whole video space. Temporal Dependence Modeling. We leverage the GRU to model the temporal dependence between consecutive frames. At each time step, GRU takes the feature vector of the current frame and the hidden state of the previous frame as inputs and yields the hidden state of the current frame: h′ t = GRU vt , h′ t −1  , (10) where h′ t ∈ Rdv denotes the hidden state of the t-th frame. By the operation in Eq. (10), we can effectively capture the dependence between adjacent frames. For representing a video (clip), previous works either leverage the last hidden state or aggregate all the hidden states of frames using average-pooling, forgoing modeling the frame interaction over the whole video space. Frame-wise Temporal Interaction Modeling . To further en- hance of the video sequence representation, we propose to leverage the frame-wise correlation based on the multi-head self-attention mechanism [42]. Given a video sequence V = {h′ t }M t =1 produced by the GRU operation in Eq. (10), the basic idea is to first project the video sequence representation V into multiple embedding spaces and perform scaled dot-product attention between query frame and key frame, followed by a softmax operation to obtain the nor- malized weights on the value frames. We finally concatenate the outputs from multiple attention spaces as the final