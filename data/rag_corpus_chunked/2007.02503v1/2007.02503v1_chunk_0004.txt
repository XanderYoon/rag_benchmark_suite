lutions usually require the text query to be well annotated with syntactic labels (e.g., part of speech (POS) tag) and rely on complex predefined rules to construct the structure of text queries, which make it hard to be applied in a new scenario with different lin- guistic expression patterns. Although so much efforts have been devoted to complex-query video retrieval, it still remains to be a very challenging task. Towards this research goal, this paper aims to model complex queries in a more flexible structure to facilitate the joint learning of the representations of the queries and videos in a unified framework. Specifically, we develop a Tree-augmented Cross-modal Encod- ing (TCE) framework for video retrieval with complex queries. As shown in Figure 2, for the modeling of the complex query, we first recursively compose a Latent Semantic Tree (LST) to describe the query (e.g., A baby plays with a fatty cat ) without any syntactic an- notations, where each node (e.g., a baby plays) denotes a constituent in the complex query. We also propose a memory-augmented node scoring and selection method to inject linguistic context into the construction of LST. We then design a tree-augmented query en- coder that identifies the informative constituent nodes in LST and aggregates the constituent embeddings into the structure-aware query representation. For the modeling of the videos, we introduce a temporal attentive video encoder that first models the temporal dependence and interaction between frames and then attentively aggregates the frame embeddings into the temporal-attentive video representation. Finally, both the user queries and videos are mapped to a text-video joint embedding space where semantically-similar videos and text queries are mapped to close points. All the modules are jointly optimized in an end-to-end fashion using only the paired query-to-video supervision. We evaluate the proposed approach on two