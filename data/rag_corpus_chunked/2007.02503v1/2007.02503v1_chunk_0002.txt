Wang, Meng Wang, and Tat- Seng Chua. 2020. Tree-Augmented Cross-Modal Encoding for Complex- Query Video Retrieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’20), July 25–30, 2020, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.3401151 1 INTRODUCTION With the exponential growth of user-generated videos on the In- ternet, searching the videos of interest has been an indispensable activity in people’s daily lives. Meanwhile, text-based video re- trieval has attracted world-wide research interests and achieved promising progress for retrieval with keyword-based simple queries [37]. However, the expression of text query has been transformed from the keyword-based mechanism to complex queries in recent years. A complex query is usually defined as a natural language query, e.g., “Two girls are laughing together and then another throws her folded laundry around the room ", which carries far more com- plex semantics than short queries. How to correctly understand the complex queries has become one of the key challenges in the multimedia information retrieval community. Existing efforts on video retrieval with complex queries can be roughly categorized into two groups: 1) Concept-based paradigm [18, 24, 25, 31, 41, 52, 53], as shown in Figure 1 (a). It usually uses a arXiv:2007.02503v1 [cs.CV] 6 Jul 2020 large set of visual concepts to describe the video content, then trans- forms the text query into a set of primitive concepts, and finally performs video retrieval by aggregating the matching results from different concepts [53]. Despite its efficiency, it is usually ineffec- tive for complex long queries, since they carry complex linguistic context and cannot be simply treated as an aggregation of extracted concepts. Besides, it is also quite challenging to effectively train concept classifiers and select the relevant concepts. 2)Embedding- based paradigm [1, 7, 21,