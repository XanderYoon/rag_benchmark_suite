to create mutant codes from the solution script. LLM sample killing prompts llama-2-70b to generate several incorrect solutions to the problem. We run all test cases against these different codes to per- form the test-suite reduction. Finally, we generate the answer using the solution scripts. B Prvate Library We notice that Zan et al. (2022) crafted three bench- marks named TorchDataEval, MonkeyEval, and BeatNumEval to evaluate the capability of lan- guage models in code generation with private li- braries. Their benchmarks share some similarities with our two datasets on updated libraries, where we both modified popular Python libraries to ex- plore the setting for LLM generalization. Different from them, our datasets are built with increased complexity, where we not only use the simple syn- onym to update the API names, but additionally combine two APIs and create new class objects. This indicates that our datasets are likely to cover broader scenarios of library updates in real life. Nonetheless, we also benchmark our system on their datasets with varied knowledge source. Table 5 shows that CodeLlama achieves exceptionally high score in all three datasets, with zero-shot ac- curacy 80.2% in Monkey. Since the three datasets were available in Github as early as 2022, which is well ahead of the time CodeLlama was released, we suspect that CodeLlama has been trained on the three datasets. Although our system still looks to be effective in their benchmarks with performance gain by including more knowledge sources, we are concerned that these datasets may not be able to reflect the generalization capabilities of LLM. C LLM-generated program inputs To verify the syntax of the generated program, one effective way is to execute it with test cases. To simulate the scenario where no test case is avail- able, we investigate whether it is possible to gen-