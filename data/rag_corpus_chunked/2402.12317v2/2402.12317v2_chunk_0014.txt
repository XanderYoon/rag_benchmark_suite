16.2% absolute gain with ChatGPT and CodeLlama re- spectively on top of DocPrompting. This can be explained by the fact that DocPrompting only uses the documentation as a single retrieval source, with- out evolution in both queries and knowledge. By combining EVOR with MPSC, ExeDec, or Reflex- evolution Scipy-M Tensor-M Ring Pony Avg Model: ChatGPT No evolution 32.6 40.0 11.7 5.2 22.4 Evolve query 32.9 44.4 27.8 8.5 28.4 Evolve knowledge 33.5 42.2 13.5 6.1 23.8 EVOR (Evolve both) 37.9 53.3 36.6 13.5 35.3 Model: CodeLlama No evolution 23.9 42.2 8.2 7.3 20.4 Evolve query 26.6 44.4 11.7 12.8 23.9 Evolve knowledge 25.8 44.4 12.6 8.3 22.8 EVOR (Evolve both) 31.2 53.3 26.7 17.4 32.2 Table 3: The performance of ChatGPT and CodeLlama when neither queries nor knowledge evolves, only the query evolves, only the knowledge evolves and when both evolve (EVOR). Results show that evolving both is consistently better across all datasets. ion, we observe further performance increase by up to 2.6% on average with ChatGPT. This suggests that EVOR is flexible to be integrated with existing approaches to further push forward the boundary of LLM performance in RACG. 4 Analysis 4.1 Synchronous evolution We investigate how the synchronous evolution of queries and knowledge influences the RACG per- formance of LLMs. We compare EVOR to the setting where we only evolve queries (skip line 13- 20 in Algorithm 1), only evolve knowledge (skip line 7 in Algorithm 1), and evolve neither of them (skip line 7, 13-20 in Algorithm 1, and terminate in a single iteration). We adopt the default setting in ยง3.2 except the specified changed in the algorithm. Table 3 shows evolving either queries or knowledge significantly enhances the results, highlighting that knowledge evolution also contributes to improving RACG in addition to the query rewriting. By ap-