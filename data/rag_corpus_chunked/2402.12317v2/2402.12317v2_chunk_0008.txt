placement, utiliza- tion of special tokens, and other grammar. Before evaluation, we collect a set of code snippets ver- ified to be free of syntax errors (more details in Appendix D). Additionally, we also accumulate code solutions generated by LLMs. 2.2.2 Evolution The evolution of knowledge bases is primarily con- tributed by the execution feedback and code snip- pets. In each iteration, we execute the generated program with sample inputs (Appendix C). If the execution successfully exits, we classify the code snippet as “syntax-correct”, which can serve as a demonstration for other instances to refer to. Otherwise, we add the (code snippet, error messages) pair to the knowledge base. Through- out the process of iterative generation, the knowl- edge base evolves to include increasingly rich in- formation. 2.3 E VOR Pipeline Algorithm 1 demonstrates the EVOR pipeline. In every iteration i, we first formulate the query qi (§2.1), and use it to retrieve relevant information Kr from the knowledge base K. The program pi is then generated conditioned on both n and Kr. We get the execution feedback F i by executing pi on LLM-generated test inputs I using the compiler or interpreter E. The knowledge base K is then evolved to include pi if the execution is successful, or the pair of (pi, F i) otherwise. The pipeline exits either upon reaching the termination condition or the maximum iteration steps. In the experiments, we set the maximum iterations to 30 and the termi- nation condition to be the same execution feedback in consecutive 3 iterations, i.e., the algorithm exits if the program is successfully executed or results in the same error in consecutive 3 iterations. 2.4 Datasets Since LLMs are extensively trained on public data, we curate a new benchmark to evaluate their gen- eralization capability with EVOR. Specifically,