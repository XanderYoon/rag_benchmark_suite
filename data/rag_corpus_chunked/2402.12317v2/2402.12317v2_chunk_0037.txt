In every iteration of active retrieval and LLM generation, we add the examples with correct syntax (judged by executors with sample inputs) to the set of code snippets and only rectify the code with syntax error. For each of the knowledge sources considered in this paper, we adopt the following principles if Model: ChatGPT Model: CodeLlama Method Monkey BeatNum TorchData Avg. Monkey BeatNum TorchData Avg. Vanilla 38.6 27.7 42.0 36.1 80.2 70.3 54.0 68.2 EVOR 67.3 70.3 74.0 70.5 93.1 90.1 92.0 91.7 Table 5: We evaluate the zero-shot ChatGPT and CodeLlama on three private libraries. Although we observe significant improvements of EVOR, the exceptionally high accuracy of CodeLlama Vanilla (zero-shot) performance suggests the risk of data leakage, making it less reliable to assess model generalization capabilities. Model: ChatGPT Model: CodeLlama Scipy-M Tensor-M Ring Pony Scipy-M Tensor-M Ring Pony 89.2 93.3 100.0 100.0 86.8 91.1 95.6 96.8 Table 6: The accuracy of ChatGPT (left) and CodeLlama (right) in generating valid program inputs. Although LLMs cannot guarantee to write accurate test cases, their performance in generating only program inputs is exceptionally high. it is included in the prompt for LLM generation: (1). For web search content, include it until the maximum allowed length, e.g., 4,096, as we do not merge it without other knowledge sources; (2). For execution feedback, include the error message and the line of the code that leads to the error; (3). For code snippets, allocate a maximum length of 300 to them, as they are usually short; (4). For documenta- tion, always include other types of knowledge first, and include documentation to fill in the rest length. For example, if we want to include both documen- tation and code snippets as the knowledge source and the maximum context length is 4,096, we will allocate a maximum length