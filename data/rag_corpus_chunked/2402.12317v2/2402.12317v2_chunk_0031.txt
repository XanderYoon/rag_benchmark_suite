Zhengbao Jiang, and Graham Neubig. 2022. Docprompting: Gener- ating code by retrieving the docs. In The Eleventh International Conference on Learning Representa- tions. Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. 2023. Docprompting: Gener- ating code by retrieving the docs. In The Eleventh International Conference on Learning Representa- tions. A Dataset curation We introduce more details about our dataset cura- tion process for updated library (§A.1) and long- tail programming languages (§A.2). In §A.3, we describe our implementation of the test case con- struction for the dataset Ring and Pony. A.1 Library-oriented data collection Following Zan et al. (2022), we use the syn- onyms of original API names and API argu- ments in the updated library, such as converting stack to pile. Additionally, we combine two similar APIs into one, with newly added argu- ments to distinguish the authentic functionalities, e.g., linear_interpoloate integrates two APIs, griddata and interp1d. Finally, we create new class objects and align methods with the original class. For instance, a new SparseMatrix object is created to include all sparse matrix objects in Scipy. We rewrite the ground truth solution for each example with new APIs. To construct the documentation of the updated libraries, we first collect the original libraries7. We then replace the old documentation files with our modified version. For each question, we annotate the oracle documentation by checking the ground truth answer. We grasp the corresponding docu- mentation pages and concatenate them to serve as the minimum documentation required for answer- ing the problem. We reuse the test cases introduced in DS-1000 to evaluate LLM generalization perfor- mance. A.2 Language-oriented data collection For each programming problem collected from LeetCode, we rewrite the function signatures to adapt them to the target programming lan- guage. We collect the whole documentation for Ring