ecution feedback, and code snippets in the knowl- edge soup K for EVOR, as the content of web search contains large portions of noisy informa- tion (Appendix G) and only marginally improves the results (ยง4.2). We use ChatGPT for both Mq to evolve queries and Mt to generate test in- puts, and vary M between ChatGPT and CodeL- lama to output code answers. We employ the INSTRUCTOR-xl (Su et al., 2023) as the primary retrieval model (Appendix H) and allow a maxi- mum context length of 4,096 for both ChatGPT and CodeLlama, as the further increase incurs a higher cost, but fails to provide additional improve- ments (Appendix I). 3.3 Results Table 2 shows that existing code generation ap- proaches perform poorly on EVOR-BENCH . With CodeLlama, the improvements of MPSC, ExeDec, and Reflexion are smaller than 2% on average, compared to the vanilla generation. In particular, the execution accuracy remains 0 in Ring across three methods. This indicates that, even though existing approaches excel in code generation tasks that do not require external knowledge (e.g., Hu- manEval (Chen et al., 2021)), they cannot be di- rectly applied to the setting of RACG without de- signing extra mechanisms to retrieve and utilize the external information. In contrast, by explicitly using documentation, DocPrompting significantly surpasses MPSC, ExeDec, and Reflexion by a large margin, further confirming that domain knowledge is critical to solving tasks in EVOR-BENCH . Furthermore, EVOR achieves 16.1% and 16.2% absolute gain with ChatGPT and CodeLlama re- spectively on top of DocPrompting. This can be explained by the fact that DocPrompting only uses the documentation as a single retrieval source, with- out evolution in both queries and knowledge. By combining EVOR with MPSC, ExeDec, or Reflex- evolution Scipy-M Tensor-M Ring Pony Avg Model: ChatGPT No evolution 32.6 40.0 11.7