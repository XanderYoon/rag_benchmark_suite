BENCH . EVOR demonstrates significantly superior results, with further improvement when combined with other baseline methods. subgoal. Subprograms are finally combined as the output answer to solve the original coding problem. In experiments, we use ChatGPT as the subgoal model and compare LLMs to synthesize programs following the subgoal predictions. Reflexion: Shinn et al. (2024) uses a framework to reinforce LLMs through linguistic feedback. It employs an iterative optimization process. In each iteration, the actor model produces a trajectory con- ditioned on the instructions and memories. The evaluator model then evaluates the trajectory and calculates a scalar reward. Self-reflection model generates verbal experience feedback on the pairs of trajectories and rewards, which are stored in the memory. Throughout experiments, we use the com- piler or interpreter as the evaluator model, which returns 0 upon execution errors, and 1 otherwise. By default, we use ChatGPT as the self-reflection model and compare the capabilities of LLMs to generate programs as actor models. DocPrompting: Zhou et al. (2022) proposed to explicitly leverage code documentation by first re- trieving the relevant documentation pieces given a natural language (NL) intent, and then generat- ing code based on the NL intent and the retrieved documentation. It can be viewed as a degraded ver- sion of EVOR where neither queries nor knowledge bases evolve and the retrieval pool encompasses the documentation as a single source. 3.2 Default E VOR Configuration By default, we incorporate the documentation, ex- ecution feedback, and code snippets in the knowl- edge soup K for EVOR, as the content of web search contains large portions of noisy informa- tion (Appendix G) and only marginally improves the results (ยง4.2). We use ChatGPT for both Mq to evolve queries and Mt to generate test in- puts, and vary M between ChatGPT and CodeL- lama to