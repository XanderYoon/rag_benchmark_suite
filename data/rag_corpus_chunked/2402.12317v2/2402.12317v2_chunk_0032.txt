concatenate them to serve as the minimum documentation required for answer- ing the problem. We reuse the test cases introduced in DS-1000 to evaluate LLM generalization perfor- mance. A.2 Language-oriented data collection For each programming problem collected from LeetCode, we rewrite the function signatures to adapt them to the target programming lan- guage. We collect the whole documentation for Ring and Pony from their websites: https:// ring-lang.github.io/doc1.19/ and https:// www.ponylang.io/. For each question, we labeled the oracle documentation of the specific grammar used in the ground truth, such as data structures or branching syntax. We concatenate the document for each new syntax used in the ground truth to obtain a minimum document that contains the re- quired syntaxes for answering the question. 7https://docs.scipy.org/doc/, https://www. tensorflow.org/api_docs A.3 Test-case generation for language-oriented data To accurately evaluate the performance of LLM in writing code of long-tail programming languages, we follow (Liu et al., 2023b) to construct a compre- hensive set of test cases for each problem. Specifi- cally, we first prompt ChatGPT to write a validation script and solution script using Python. The valida- tion script will check for the input constraints (e.g. single line of a positive integer, two binary strings, etc.) of the problem. The solution script is sup- posed to generate the correct answer given a valid input. We then manually check both scripts and modify them if necessary for all problems. Next, we prompt ChatGPT to create complex and corner cases until there are 20 test cases for each problem. We then apply mutation-based strategies to extend the number of test cases for each problem to 200. The mutation-based strategy works as follows. We first parse the input into the appropriate format and types (e.g. list of strings, tuple of integers, etc. ) We will then randomly mutate the