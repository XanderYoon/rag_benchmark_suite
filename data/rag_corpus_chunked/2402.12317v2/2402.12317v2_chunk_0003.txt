Specifically, two of these datasets focus on mod- ifications made to widely-used Python libraries, Scipy and Tensorflow. The remaining two datasets simulate the introduction of new grammars, with the help of two less-common programming lan- guages Ring and Pony. To conduct thorough experi- ments, we employ both proprietary models, such as ChatGPT (OpenAI, 2022), and open-source models like CodeLlama (Roziere et al., 2023). Experimen- tal results across these four datasets demonstrate that our method yields a significant improvement in the average performance over existing code gen- eration methods. For example, EVOR outperforms DocPrompting (Zhou et al., 2023) by 18.6% on average using CodeLlama (§3). Further analysis unveils that both synchronous evolution and diverse sources in knowledge bases are critical to the suc- cess of EVOR (§4.1, §4.2). We demonstrate that EVOR is flexible to integrate with many other code generation approaches including the agent-based one, e.g., swe-agent, offering further performance enhancement in both EVOR-BENCH and existing benchmarks (§4.3). Finally, we showcase EVOR is a more effective approach to using tokens, and demonstrates superior results in all levels of token consumption ranging from 4k to 24k (§4.4). In summary, our contributions are: • We propose a novel pipeline, EVOR, high- lighting the complementary strength of syn- chronous evolution of queries and diverse knowledge bases in RACG. • We compile a new benchmark,EVOR-BENCH , on two realistic RACG settings related to fre- quently updated libraries and long-tail pro- gramming languages. • We conduct extensive analyses and find that EVOR can be easily combined with existing code generation approaches including agent- based ones to provide further improvements. 2 Evolving Retrieval Given a question n in natural language, the ob- jective of retrieval-augmented code generation is to first retrieve relevant information K+ from ex- ternal knowledge K and then augment large lan- guage models