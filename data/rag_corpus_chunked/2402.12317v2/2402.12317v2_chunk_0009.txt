30 and the termi- nation condition to be the same execution feedback in consecutive 3 iterations, i.e., the algorithm exits if the program is successfully executed or results in the same error in consecutive 3 iterations. 2.4 Datasets Since LLMs are extensively trained on public data, we curate a new benchmark to evaluate their gen- eralization capability with EVOR. Specifically, we introduce four datasets where two focus on updated libraries and two are about long-tail programming languages. We first modified two popular Python libraries, Scipy and Tensorflow, to simulate the real updates3, and denote them as Scipy-M and Tensorflow-M re- spectively. We then collect problems of the Scipy and Tensorflow split from DS-1000 (Lai et al., 2023) and adapt them to our modified version. For the long-tail programming languages, we select Ring and Pony. They have little public data and are excluded from the StarCoder training set, which in- volves 88 mainstream programming languages (Li et al., 2023b). We make use of the problems in LeetCode 4 for these two datasets. For each prob- lem in modified libraries or long-tail programming languages, we manually write the ground truth so- lution and annotate the oracle documentation based on it. We present the dataset statistics in Table 1. More details about our curation process can be found in Appendix A. 3 Experiment To verify the effectiveness of EVOR, we conduct extensive experiments with both the proprietary 3We do not use a real library update version because it is potentially exposed to LLM training data, which deviates from our purpose to evaluate LLMsâ€™ generalization ability. 4https://leetcode.com/problemset/ Dataset # P # D A.T A.P.L A.S.L A.D.L Scipy-M 142 3920 3.1 322.6 44.1 499.7 Tensor-M 45 5754 4.1 234.5 39.0 517.6 Ring 107 577 18.2 108.3 98.3 334.0 Pony 113 583 18.4 116.9 129.8 3204.0