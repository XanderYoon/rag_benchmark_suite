library update version because it is potentially exposed to LLM training data, which deviates from our purpose to evaluate LLMs’ generalization ability. 4https://leetcode.com/problemset/ Dataset # P # D A.T A.P.L A.S.L A.D.L Scipy-M 142 3920 3.1 322.6 44.1 499.7 Tensor-M 45 5754 4.1 234.5 39.0 517.6 Ring 107 577 18.2 108.3 98.3 334.0 Pony 113 583 18.4 116.9 129.8 3204.0 Table 1: Data statistics of four benchmarks. We re- port the number of problems (# P), the number of official documentation files (# D), the average num- ber of test cases (A.T), the average problem length (A.P.L), the average solution length (A.S.L) and the average gold documentation length (A.D.L). Tensor- M refers to Tensorflow-M . Problem length, solution length and document length are calculated by the tik- token (https://pypi.org/project/tiktoken/) package with model gpt-3.5-turbo-1106. model ChatGPT (gpt-3.5-turbo-1106 5) and the open-source model CodeLlama 6. In §3.1, we de- scribe 5 baseline settings of other code generation approaches and specify the default configuration of EVOR in §3.2. In §3.3, we compare the results of EVOR, existing code generation methods, as well as their combinations. By default, we use the exe- cution accuracy (pass@1) as the metric throughout the paper. 3.1 Baselines We compare EVOR to the vanilla generation and four recent methods that demonstrate signifi- cant performance improvement in code generation tasks: Vanilla: we implement the vanilla generation baseline where we directly get the outputs from LLMs based on the coding question n without aug- menting external knowledge. MPSC: Huang et al. (2023) proposed Multi- Perspective Self-Consistency (MPSC) incorporat- ing both inter- and intra consistency. Following the original implementation, we prompt LLMs to generate diverse outputs from three perspectives: Solution, Specification and Test case, construct the 3-partite graph, and pick the optimal choice of so- lutions based on confidence scores. ExeDec: Shi