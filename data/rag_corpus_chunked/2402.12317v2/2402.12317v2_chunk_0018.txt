an agent to explore directories, use tools, make decisions, and more. Recent efforts have demonstrated the success of such agent-based meth- ods (OpenDevin Team, 2024; Yang et al., 2024). We explore the applicability of EVOR in this challenging setting. Specifically, we employ the popular SWE-bench-Lite (Jimenez et al., 2023) as the testbed, use all the repository content as the documentation, and adopt the configuration in §3.2. Due to the difficulty of the tasks, we experiment with two settings: (1) use GPT-4-1106 for all LLMs in Algorithm 1; (2) use Claude-3-opus. Figure 2 shows that EVOR outperforms the traditional RAG by a large margin, and is comparable with SWE- agent. This highlights the generalizability ofEVOR with successful application in repo-level code gen- eration. Furthermore, we integrate EVOR with SWE- agent where we augment the search space of SWE- agent to include the execution feedback and code snippets without syntax errors, and dynamically update queries and the knowledge base in every iteration of generation. Figure 2 demonstrates ad- ditional performance improvements on top of both EVOR and SWE-agent, further proving EVOR’s flexibility in its integration to agent-based ap- proaches. GPT-4-1106 Claude-3-opus Models 0 2 4 6 8 10 12 14 16 18 20% Resolved RAG EvoR SWE-agent EvoR + SWE-agent Figure 2: Performance of RAG, EVOR, SWE-agent (Agent) and combination of EVOR and SWE-agent. 4.4 Effective Token Usage Olausson et al. (2023b) argues that iterative code generation, e.g., self-repair, may not yield higher pass rates when taking the cost into account. We conduct additional experiments to check the perfor- mance of EVOR at different levels of token budgets. In Algorithm 1, we adopt the termination condition as the limit of maximum token consumption, i.e., the algorithm exits when the tokens used by LLMs throughout iterations exceed a given threshold. Ad- ditionally, we