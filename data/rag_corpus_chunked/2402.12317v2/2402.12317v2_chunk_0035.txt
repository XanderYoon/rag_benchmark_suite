knowledge sources, we are concerned that these datasets may not be able to reflect the generalization capabilities of LLM. C LLM-generated program inputs To verify the syntax of the generated program, one effective way is to execute it with test cases. To simulate the scenario where no test case is avail- able, we investigate whether it is possible to gen- erate program inputs with LLMs. Specifically, we prompt ChatGPT and CodeLlama to generate 5 test cases for each problem, and only save the inputs for evaluating the syntax of other programs. As an ablation study, we execute the gold program of each problem with the generated inputs and count a generated input as valid if no error is reported during execution. We calculate the accuracy as the percentage of examples where all the generated test inputs are valid. Table 6 shows that both ChatGPT and CodeLlama exhibit superior performance in generating test inputs. This indicates that LLM- generated test inputs serve as good resources as syntax verifiers. D Sample code snippets and execution feedback We collect sample code snippets and execution feedback in constructing the knowledge base. Specifically, we prompt LLMs to write short scripts of sample usage of each function in the documen- tation corpus. We then execute those scripts. If the execution of a code snippet reports errors, we include it as a pair of (code, error); otherwise, we regard it as a code snippet that could demonstrate the syntax and function usage. E Cost Analysis Despite significant enhancement of EVOR in the generalization results, the iterative process that in- volves multiple LLM generations incurs large costs. In this section, we discuss the trade-off between the cost and the performance. To measure the cost, we count the total tokens processed by LLM through- out the process in each