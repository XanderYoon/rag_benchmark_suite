average performance of ChatGPT and CodeLlama with different knowledge sources. Web refers to web search, Exec refers to execution feedback, Code refers to code snippets and Doc refers to documen- tation. The results show that diverse types of knowledge enhance RACG performance, where the improvement is larger under the setting with evolution. and the setting where all of them are included. For each constructed knowledge soup, we experiment with evolving neither queries nor knowledge, and evolving both of them. We skip line 14 in Algo- rithm 1 when the code snippets are not included in the knowledge soup and skip line 16 when the execution feedback is not incorporated. Exceptions occur when the knowledge soup consists solely of web search content or documentation, where we only evolve queries. In Table 4, we present the average performance of ChatGPT and CodeLlama using different types of knowledge sources, under two settings where evolution is and is not involved. The results show that, when augmenting with code snippets or syn- tax documentation, the performance of ChatGPT and CodeLlama is significantly higher than those using the web search or execution feedback. In par- ticular, both models achieve less than 1% improve- ment when only using the web search as the knowl- edge source without evolving queries. This indi- cates that the general web search may not provide the most effective information to adapt LLMs in RACG. Compared to single-source retrieval, LLMs consistently achieve better results when more types of knowledge are integrated. For example, with- out queries and knowledge evolution, ChatGPT archives 6.2% higher average performance by us- ing both code snippets and documentation as the knowledge sources, compared to only employing the code snippets. This indicates the advantage of diverse knowledge soup in enhancing the RACG performance of LLMs. On the other hand,