et al., 2023c). Recent works have applied it to the code generation task (Patel et al., 2023; Guo et al., 2023; Parvez et al., 2021a; Wang et al., 2023b). Specifically, Zhou et al. (2022) explored the natural-language- to-code generation approach that explicitly lever- ages code documentation. Zan et al. (2022) in- troduced a framework designed to adapt LLMs to private libraries, which first utilizes an APIRe- triever to find useful APIs and then leverages an APICoder to generate code using these API docs. Zhang et al. (2023a) employed the itera- tive generate-retrieval procedure to do repository- level code completion. There are also recent efforts showing much enhanced performance by simply rewriting the queries (Ma et al., 2023; Anand et al., 2023; Chan et al., 2024) To the best of our knowl- edge, we are the first to adopt the synchronous evolution of queries and diverse knowledge to ex- plore the setting where LLMs need to incorporate external information in code generation. Code Execution Previous works have exten- sively employed executors (Interpreters/Compilers) in code-related tasks (Wang et al., 2022; Liu et al., 2023a; Olausson et al., 2023a; Chen et al., 2023). Shi et al. (2022) introduced execution resultâ€“ based minimum Bayes risk decoding for program selec- tion. Yang et al. (2023) established an interactive coding benchmark by framing the code as actions and execution feedback as observations. Chen et al. (2023) use the execution result as feedback to help LLM refine the code. In this work, we utilize the executor to provide feedback and check code out- puts on syntax errors, which contributes to evolve both queries and knowledge in RACG. 6 Conclusion Much recent work illustrated the ability of LLMs to incorporate external knowledge with retrieval- augmented generation. We propose a novel pipeline, EVOR, which achieves two to four times