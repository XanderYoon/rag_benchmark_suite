higher pass rates when taking the cost into account. We conduct additional experiments to check the perfor- mance of EVOR at different levels of token budgets. In Algorithm 1, we adopt the termination condition as the limit of maximum token consumption, i.e., the algorithm exits when the tokens used by LLMs throughout iterations exceed a given threshold. Ad- ditionally, we compare EVOR to DocPrompting, the best baseline approach in Table 2. Follow- ing Olausson et al. (2023b), we sample LLMs mul- tiple times until the token limit, using the concate- nation of the given question n and the retrieved documentation Kr as the prompt. We calculate pass@t and set the token threshold to 4,000, 8,000, 12,000, 16,000, 20,000 and 24,000. Figure 3 shows that EVOR achieves significantly higher perfor- mance at all token levels for both ChatGPT and CodeLlama. With the increase of consumed to- kens, EVOR demonstrates larger improvements compared to DocPromting, indicating the more ef- fective token usage of EVOR in generalizing LLMs in RACG. 5 Related works Since the focus of our work is to enhance code gen- eration with retrieval, our work is closely related to code generation and retrieval-augmented code generation. Additionally, we are connected to the line of code execution since we also leverage it as an important retrieval source. LLM-based Code Generation LLMs that have been pre-trained on extensive code corpus have ex- hibited impressive abilities in the domain of code 4000 8000 12000 16000 20000 24000 Consumed tokens 15 20 25 30 35Pass@t ChatGPT - EvoR CodeLlama - EvoR ChatGPT - DocPrompting CodeLlama - DocPrompting Figure 3: The pass rate of ChatGPT and CodeLlama at different token consumption levels. The results show that EVOR achieves a more significant increase com- pared to DocPrompting when the consumed tokens in- crease. generation (Li et