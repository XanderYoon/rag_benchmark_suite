G, we include more discussions about the content in the web search. Documentation is commonly accessible upon the release of a new programming language or an updated library version. Official documentation serves to thoroughly elucidate the essential syn- tax and grammar required for coding. Zhou et al. (2022) demonstrated that language models can ef- fectively leverage code documentation after fine- tuning. In this work, we focus on understanding the capability of LLMs in utilizing the documenta- tion of updated libraries or long-tail programming languages in code generation, without making any parameter update. Execution feedback is a specific knowledge type for code generation. It exposes syntax mistakes and locates code errors, which are frequently ref- erenced by human programmers to debug. While multiple types of execution can provide feedback (e.g., execution by LLMs), we focus on the com- piler or interpreter execution in this work. Previous works (Shinn et al., 2024) have demonstrated that LLMs are capable of repairing buggy code using the execution feedback. Instead of only leverag- ing the error messages obtained from executing the generated faulty programs, we further enrich the knowledge base by preparing sample code-error pairs. More details can be found in Appendix D. Code snippets are the short pieces of code that demonstrate sample usage of certain functions or syntax. Different from other types of knowledge that involve natural language, code snippets in pro- 1https://pypi.org/project/google/ 2https://pypi.org/project/html2text/ Algorithm 1 EVOR Pipeline 1: Input: n: the coding problem description; M: the LLM to generate the code answer; Mq: the LLM to evolve queries; Mt: the LLM to generate test inputs; R: the retriever to output a list of relevant passages; K: the knowledge base; m: the maximum number of iterations; E: the compiler or interpreter to execute programs 2: Initialization: I = [], p = null. 3: