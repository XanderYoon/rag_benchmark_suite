as they can utilize more external knowledge to enhance their coding. How- ever, our experiments do not imply the case. We adopt the default setting of EVOR, but only change the maximum context length of LLMs to 2k, 4k, 8k, 12k, and 16k tokens. Figure 6 indi- cates that ChatGPT achieves the best performance when only using external knowledge of 4k tokens. This aligns with the findings in Xu et al. (2023b). With extended context lengths, i.e., more retrieved content is included in the prompt, the performance does not further increase. The potential reasons to explain this situation include: (1). Only a few documents are required to answer a specific question. As shown in Ta- ble 1, the length of gold documentation, i.e., mini- mum required syntax descriptions, never surpasses 4k, which does not even surpass 1k in Scipy-M , Tensorflow-M and Ring. This implies that the retriever has a good chance to include the gold doc- umentation within 4k context length; (2). LLMs have low attention in the middle of long contexts (Liu et al., 2023c). With long contexts, LLMs may fail to identify the relevant content in the middle that can help solve the problem. We leave it to future research to design a more delicate retrieval system that can appropriately reg- ulate the content utilized for LLM generation. J Personally Identifying Infomation We collect data from the domain of code generation. We authors carefully reviewed all the collected data, and confirm that the data that was collected/used does not contain any information that names or uniquely identifies individual people or offensive content. K Intended use EVOR is an advanced pipeline for RACG, and it is expected to be applied in customized code gen- eration. EVOR-BENCH consists of four realistic benchmarks for RACG, and is expected to be