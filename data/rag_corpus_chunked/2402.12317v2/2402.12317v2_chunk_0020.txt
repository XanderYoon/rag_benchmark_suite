12000 16000 20000 24000 Consumed tokens 15 20 25 30 35Pass@t ChatGPT - EvoR CodeLlama - EvoR ChatGPT - DocPrompting CodeLlama - DocPrompting Figure 3: The pass rate of ChatGPT and CodeLlama at different token consumption levels. The results show that EVOR achieves a more significant increase com- pared to DocPrompting when the consumed tokens in- crease. generation (Li et al., 2022; Nijkamp et al., 2022; Li et al., 2023b; Roziere et al., 2023; Wei et al., 2023). Numerous techniques have been suggested to improve the coding capabilities of LLM without the need to adjust its parameters (Chen et al., 2022; Huang et al., 2023; Li et al., 2023a; Zhang et al., 2023c; Chen et al., 2023; Key et al., 2022) However, most of these works set up the evaluation in scenar- ios LLMs are familiar with, e.g., HumanEval (Chen et al., 2021), HumanEvalPack (Muennighoff et al., 2023) and MBPP (Austin et al., 2021), where they are capable of demonstrating superior zero-shot performance by only utilizing internal knowledge. In this work, we focus on evaluating the capabili- ties of LLMs to incorporate external knowledge for the purpose of code generation in updated libraries or less-common programming languages. Our task reflects a more realistic yet challenging scenario for LLMs. Retrieval-Augmented Generation The retrieval- augmented generation (RAG) is an appealing paradigm that allows LLMs to efficiently utilize external knowledge (Shi et al., 2023b; Izacard and Grave, 2020; Xu et al., 2023a; Jiang et al., 2023c). Recent works have applied it to the code generation task (Patel et al., 2023; Guo et al., 2023; Parvez et al., 2021a; Wang et al., 2023b). Specifically, Zhou et al. (2022) explored the natural-language- to-code generation approach that explicitly lever- ages code documentation. Zan et al. (2022) in- troduced a framework designed to adapt LLMs to private