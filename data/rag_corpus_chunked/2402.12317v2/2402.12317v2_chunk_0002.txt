e.g., web con- tent (Parvez et al., 2021a; Wang et al., 2022), code snippets generated by LLM (Zhang et al., 2023b), etc. This information is easily obtained and can en- rich knowledge bases, which are shared among all instances of the same task. Furthermore, the unique characteristic of execution in code generation en- ables more information collected on-the-fly. For instance, if a code snippet generated by LLMs is successfully executed without reporting error mes- sages, it is guaranteed to be syntactically correct and can serve as a concrete example to demonstrate the corresponding grammar or function usage. In this work, we introduce EVOR, a novel pipeline that applies synchronous evolution of both queries and documents in RACG. In the traces of multi-round interactions among retrievers, LLMs and executors, both queries and knowledge bases arXiv:2402.12317v2 [cs.CL] 3 Dec 2024 are updated based on the execution feedback and LLM outputs in every iteration. This strategic re- finement aims to facilitate the extraction of the most pertinent information. Apart from the given library documentation, we construct a diverse knowledge soup to further integrate the web search content, execution feedback, and code snippets generated by LLMs in the inference time. To prevent the issue of data leakage associated with large language models pretrained on massive public datasets, and assess EVOR under a reliable generalization setting, we compile a new bench- mark, EVOR-BENCH , comprising four datasets de- signed to simulate realistic scenarios in RACG. Specifically, two of these datasets focus on mod- ifications made to widely-used Python libraries, Scipy and Tensorflow. The remaining two datasets simulate the introduction of new grammars, with the help of two less-common programming lan- guages Ring and Pony. To conduct thorough experi- ments, we employ both proprietary models, such as ChatGPT (OpenAI, 2022), and open-source models like CodeLlama (Roziere