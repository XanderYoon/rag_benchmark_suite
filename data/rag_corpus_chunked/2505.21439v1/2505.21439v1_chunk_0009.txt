present InF-IR, a large-scale training corpus specifically curated for training a bi-encoder retrieval model capable of effectively following instructions. To ensure generalizability, we utilize MS MARCO (Bajaj et al., 2018) as our seed dataset to construct corresponding<instruction, query, passage> tuples.3 MS MARCO provides a large-scale, general-domain dataset consisting of anonymized real-world queries paired with human-annotated relevant passages. We selected 2For simplicity, D and B also represent the sets of indices corresponding to the sample pairs they contain. 3We use the MS MARCO v2.1 available at https://huggingface.co/datasets/microsoft/ms_marco. 4 InstructionQuery FollowIRPromptrieverInF-IR (Ours) (a) t-SNE Visualization of Semantic Coverage FollowIR Promptriever InF-IR0.70 0.75 0.80 0.85 0.90 0.95APS FollowIR Promptriever InF-IR 106 107 108 109 inter-sample N-gram Frequency Instruction Query Passage (b) Diversity Metrics, APS (↓) and INGF (↑) Figure 3: Visualization and diversity analysis of synthetic training samples from InF-IR. MS MARCO because of its extensive query-passage coverage and high-quality annotations, which provide a solid foundation, allowing us to focus primarily on instruction generation. Overview. Our data curation pipeline proceeds in three stages: (i) We first synthesize explicit instruc- tions aligned to each query-passage pair, creating positive tuples<instruction, query, passage> ; (ii) To enhance discriminative representation learning, we then employgpt-4o-mini (Hurst et al., 2024) to generate challenging negative examples by introducing subtle alterations to instructions and queries; and (iii) We rigorously validate tuple quality using o3-mini as a proxy evaluator, filtering out low-quality tuples where the intended passage relevance is ambiguous or not clearly identifiable. Instruction Generation. We initiate data synthesis by generating a suitable instruction for each query-passage pair in MS MARCO. We prompt gpt-4o-mini to produce instructions that add specificity or stylistic context, thereby explicitly linking queries more precisely to their corresponding ground-truth passages. Leveraging gpt-4o-mini enables scalable instruction generation with a careful balance between effectiveness and computational efficiency. Contrastive Negatives.