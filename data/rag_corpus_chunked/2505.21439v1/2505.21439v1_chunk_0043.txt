(2022)24.6 -0.7 12.8 2.0 17.0 2.8 18.1 1.4 – – – – – –monot5-3B(2020) 27.3 4.0 16.5 1.8 18.2 1.8 20.7 2.5 – – – – – – XL Size and Proprietary LLMs: >5B parameters(for reference) e5-mistral(7B) (2023a) 23.1 -9.6 27.8 -0.9 18.3 0.1 23.1 -3.5 50.3 33.7 35.1 58.3 84.8 52.4InF-Embed(e5-mistral) 25.5 6.2 23.9 1.5 23.0 6.3 24.1 4.7 52.0 37.3 37.4 58.4 89.1 54.8Qwen2.5-7B 10.1 1.0 13.8 3.1 7.3 -0.3 10.4 1.3 2.6 5.5 3.4 1.9 12.9 5.3InF-Embed(Qwen2.5-7B) 26.7 6.4 25.6 1.8 23.4 6.5 25.2 4.9 47.6 32.1 36.8 51.5 86.6 50.9GritLM-7B(2024) 28.6 -1.7 24.4 -1.0 20.8 2.6 24.6 -0.0 52.3 36.0 36.3 58.3 82.7 53.1NV-Embed-v1(7B) (2024) – – – – – – – – 45.0 31.5 30.8 43.0 84.7 47.0repllama-v1-7b(2024) 24.0 -8.9 24.5 -1.8 20.6 1.3 23.0 -3.1 – – – – – –promptriever-llama2-7b(2025) 28.3 11.7 28.5 6.4 21.6 15.4 26.1 11.2 – – – – – –OpenAI-v3-large 27.2 -5.8 27.2 -2.0 21.6 -0.2 25.3 -2.7 – – – – – –cohere-embed-english-v3.022.3 -3.6 28.3 0.2 20.6 2.8 23.7 -0.2 – – – – – –google-gecko(2024) 23.3 -2.4 29.5 3.9 23.2 5.4 25.3 2.3 – – – – – – • Impact of Instruction-Tuning. Instruction-tuned modele.g., FLAN-T5, Instructor, and GritLM) significantly outperform models without explicit instruction-tuning. These improvements underscore the critical role of task-specific instructions in enhancing retrieval capabilities and aligning model outputs more closely with user intentions. G.2 Additional Configuration Benchmarks Table 6 benchmarks various contrastive loss configurations (Section 5.2) with detailed comparisons on the FollowIR dataset. Our key observations are as follows: • Contrastive Loss. Models trained with ℓmulti P,I achieve the highest performance. This result highlights the critical role of simultaneously contrasting instructions and passages: instruction contrasts enable the model to understand their guiding function, while passage contrasts reinforce the alignment between instruction-aware queries and