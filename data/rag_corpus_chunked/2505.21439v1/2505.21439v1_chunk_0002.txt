inherently lack robust representation learning capabilities, inadequately capturing the complex interactions among instructions, queries, and documents (Xiao et al., 2024; Wang et al., 2022a; Izacard et al., 2021). This fundamental dilemma underscores the pressing need for an effective embedding-based instruction-aware retrieval model that simultaneously excels in both efficiently encoding and accurately interpreting complex instruction-query-passage interactions. Addressing this challenge requires high-quality training resources specifically tailored for instruction- aware representation learning; unfortunately, existing instruction-following IR datasets (Petroni et al., 2021; Thakur et al., 2021; Muennighoff et al., 2023; Oh et al., 2024; Zhou et al., 2025; Sun et al., 2024; Su et al., 2024) serve primarily as evaluation benchmarks with insufficient training data. Recent studies (Weller et al., 2024, 2025) employ large language models (LLMs) to synthesize both relevant and irrelevant documents corresponding to specificinstruction-query pairs. Yet, they often rely merely on binary relevance signals or simplified negative examples, failing to capture the intricate relational dynamics inherent in instruction-based retrieval tasks. Moreover, current training paradigms focus heavily on computationally intensive reranking tasks with decoder-only architectures, thereby neglecting the substantial efficiency and scalability advantages of embedding-based retrieval models. To summarize, it is still crucial yet challenging to effectively and efficiently unleash the capability of retrieval models for complex instruction-following IR. In this study, we introduce InF-IR, a large-scale training corpus designed to advance instruction- following capabilities in retrieval models. We extend traditional retriever training samples by trans- forming standard <query,passage> pairs into expressive <instruction,query,passage> triplets, explicitly modeling complex interactions in instruction-following IR. Specifically, we generate diverse instruction-query combinations paired with corresponding retrieved documents as positive samples, while systematically poisoning both instructions and queries separately to create challenging negative samples. To further strengthen representation learning, we employ an advanced reasoning model (o3-mini) to ensure negative sample quality by validating semantic plausibility while maintaining instructional