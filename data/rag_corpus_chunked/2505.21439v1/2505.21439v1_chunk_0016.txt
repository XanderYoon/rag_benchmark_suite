Alternatively, we can ensure instruction- following by keeping instructions as part of the contrasting inputs. This naturally leads to conditional modeling with multivariate inputs such as(P, I), (P, IQ), (I, IQ ), and (P, I, IQ ) conditioned on the remaining variables. Using the marginal sampling strategy, we formulate the multivariate objective for (P, I, IQ ) as: ℓmulti P,I,IQ = − Ei∼B " log exp sim(pi,iqi,i)  P m∼B exp sim(pm,iqi,i)  | {z } Marginal Negatives for Pi + P j∼B exp sim(pi,iqj,i)  | {z } Marginal Negatives for Ii + P k∼B exp sim(pi,iqk,k)  | {z } Marginal Negatives for IQ i # , (7) where other variations of the multivariate objective, such as ℓmulti P,I , ℓmulti P,IQ , and ℓmulti I,IQ , can be readily derived by eliminating the corresponding marginal negatives from the denominator in Eq. (7). Empirically, the univariate contrastive objective in Eq. (6) may experience competition among its individual terms. In contrast, the multivariate objective presented in Eq. (7) formulates a more challenging ranking-based contrastive task by introducing a larger set of hard negatives that the retriever must effectively differentiate. Consequently, this multivariate formulation potentially exhibits greater robustness to competition-related issues, as evidenced in similar same-tower retrieval contexts (Moiseev et al., 2023; Ren et al., 2021). See additional details in appendix C. 7 Table 2: Main experimental results comparing base models and their variants trained withInF-Embed on multiple instruction-following retrieval benchmarks. Datasets (→) Robust04 News21 Core17 FollowIR DD-15 DD-16 DD-17FR-21 FR-22Bright Metrics (→) MAPp-MRRnDCGp-MRRMAPp-MRRscorep-MRRnDCG nDCG nDCGnDCG nDCGnDCG Base Size: < 1B parameters e5-base-v2 13.4 -6.7 20.9 -2.0 14.0 -2.9 16.1 -3.9 40.3 31.5 32.7 29.4 61.5 3.7+InF-Embed 14.0 6.9 23.8 3.2 11.6 5.3 16.5 5.1 47.5 35.5 32.9 49.8 78.9 8.4e5-large-v2 17.4 -4.2 24.3 0.9 17.0 0.1 19.6 -1.1 41.1