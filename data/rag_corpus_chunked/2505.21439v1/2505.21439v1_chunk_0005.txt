(2025) ✓ ✗ ✓ ✓ ✗ ✗ ✗ gpt-4o 2.1K 943 1.4M 99.35 36.52 224.97 Promptriever (2025) ✓ ✓ ✓ ✓ ✗ ✗ ✓ FollowIR-7B 489K 489K 1.6M 103.2 5.95 56.27 InF-IR (Ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓ o3-mini 77.5K 77.5K 116.2K 35.57 8.06 55.2 objectives across multiple embedding models and LMs with varying sizes, thereby supporting rapid future advances in instruction-following retrieval systems. The remainder of this paper is structured as follows: We discuss related works in section 2. We introduce the instruction-following retrieval problem setup in section 3, InF-IR dataset in section 4, and InF-Embed method in section 5, respectively. We present the experimental results in section 6 and conclude the paper in section 7. 2 Related Works In this section, we mainly focus on instruction-following retrieval related datasets and models. Instruction-Following Retrieval Datasets. Integrating explicit instructions into IR models represents a recent research focus that contrasts with traditional dense retrievers emphasizing phrase-level semantic matching (Wang et al., 2022a; Izacard et al., 2021). While several datasets (Petroni et al., 2021; Thakur et al., 2021; Oh et al., 2024; Su et al., 2024; Sun et al., 2024; Zhou et al., 2025; Muennighoff et al., 2023; Weller et al., 2024, 2025; Song et al., 2025) have emerged to comprehensively evaluate the instruction-following capabilities of retrieval models, there remains a notable scarcity of sufficient and high-quality training resources (Table 1). FollowIR (Weller et al., 2024) offers a small set of 104 instructions with simple binary relevance signals. Although Promptriever (Weller et al., 2025) contributes a significantly larger training set, it generates negative examples by only contrasting documents and relies extensively on an under-trained small instruction- tuned LM for quality assurance. Motivated by these limitations, we introduce InF-IR, an instruction- following IR data synthesis pipeline that systematically generates challenging negative