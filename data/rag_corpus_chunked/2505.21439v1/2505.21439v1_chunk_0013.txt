section, we introduce InF-Embed, a training framework aimed at improving instruction-aware IR. Specifically, we propose two distinct interactions between instructions and queries (section 5.1), and then further explore various contrastive learning objectives (section 5.2). 5.1 Instruction-Query Interaction and Representation We adopt a dual-encoder paradigm (Karpukhin et al., 2020), comprising two encoders g (· ; θP ) and g (· ; θI,Q ) to represent corresponding entities within a shared d-dimensional embedding space: pi = g (Pi ; θP ) , ii = g (Ii ; θI,Q ) , qi = g (Qi ; θI,Q ) , (2) where pi, ii, qi ∈ Rd denote the embedding for the passage, instruction, and query, respectively. Instruction-Aware Query Representation.Our primary goal is to improve the instruction-awareness of retrieval models by explicitly incorporating instruction semantics into query representations. To this end, we introduce an instruction-aware queryIQ j,k and its embedding iqj,k designed to integrate instruction-specific context from Ij while interpreting query Qk. We then propose two interaction strategies to compute the combined embedding iq: ⋄ Interaction I (Self-Attention) : For each instruction-query pair (I, Q), we concatenate the in- struction I with the query Q to construct the instruction-aware query using the simple template of "<Instruction> <Query> ". The corresponding embedding iq is then computed as: iqj,k = g (concat (Ij, Qk) ; θI,Q ) . (3) When using a decoder-based retriever where g (· ; θI,Q ) employs causal attention exclusively, this concatenation naturally allows the model to incorporate instructional context when processing the query. Note that this straightforward approach enables instruction-following retrieval without requiring architectural modifications or introducing additional training parameters. ⋄ Interaction II (Cross-Attention): Although effective, concatenation in Eq. (3) can be computa- tionally expensive as it requires a full forward pass for every instruction-query pair. To mitigate this inefficiency, we