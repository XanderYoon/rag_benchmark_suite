setup involving passages (P ), instructions (I), and instruction-aware queries (IQ) results in combinatorial sampling complexity O |B||y| , growing combinatorially for large batch sizes, where |y| denotes the number of input variables. For instance, setting y = (P, I, IQ ) yields a cubic summation in the denominator of Eq.(1), i.e.,P m∼B P j∼B P k∼B exp (sθ (Pm, Ij, IQk)). To enhance computational efficiency, we propose a marginal negative sampling strategy, independently sampling negatives for each variable in y, while fixing others to their positives. This simplifies the denominator in Eq. (1) for a positive example indexed by i as follows:P m∼B exp (sθ (Pm, Ii, IQi)) +P j∼B exp (sθ (Pi, Ij, IQi)) +P k∼B exp (sθ (Pi, Ii, IQk)) , reducing complexity from combinatorial to linear, i.e., O (|B| · |y|). ⋄ Objective I (Univariate Conditional Modeling) : Building upon the conditional probability perspective (section 3), we propose a univariate objective modeling three conditional distributions, P(P |I, Q), P(I|P, Q), and P(IQ|P ), via separate contrastive terms: ℓuni P,I,IQ = − Ei∼B h log exp(sim(pi,iqi,i))P m∼B exp(sim(pm,iqi,i)) | {z } ℓuni P w.r.t. P(P |I,Q) + log exp(sim(pi,iqi,i))P j∼B exp(sim(pi,iqj,i)) | {z } ℓuni I w.r.t. P(I|P,Q) + log exp(sim(pi,iqi,i))P k∼B exp(sim(pi,iqk,k)) | {z } ℓuni IQ w.r.t. P(IQ|P ) i , (6) which flexibly enables any combination of univariate conditional modeling by selectively retaining the desired contrastive terms. ⋄ Objective II (Multivariate Conditional Modeling) : Alternatively, we can ensure instruction- following by keeping instructions as part of the contrasting inputs. This naturally leads to conditional modeling with multivariate inputs such as(P, I), (P, IQ), (I, IQ ), and (P, I, IQ ) conditioned on the remaining variables. Using the marginal sampling strategy, we formulate the multivariate objective for (P, I, IQ ) as: ℓmulti P,I,IQ