<instruction,query,passage> triplets, explicitly modeling complex interactions in instruction-following IR. Specifically, we generate diverse instruction-query combinations paired with corresponding retrieved documents as positive samples, while systematically poisoning both instructions and queries separately to create challenging negative samples. To further strengthen representation learning, we employ an advanced reasoning model (o3-mini) to ensure negative sample quality by validating semantic plausibility while maintaining instructional misalignment. The resulting InF-IR comprises 38,759 positive samples and 77,518 meticulously crafted hard negative samples, effectively guiding retrievers to accurately interpret user intentions while distinguishing between semantically similar but instructionally distinct contexts. Importantly, InF-IR not only supports training large, computationally expensive auto-regressive LMs, but also enables efficient training and scaling of smaller embedding-based models for instruction- aware representation learning. Building upon InF-IR, we propose InF-Embed, an instruction-aware text embedding model trained via contrastive learning and instruction-query attention to optimize embeddings, accurately capturing complex relationships among instructions, queries, and retrieved documents. Our key contributions can be summarized as follows: • (i) Dataset Wise, we introduce InF-IR, a publicly available large-scale, high-quality training corpus specifically designed to enhance retrieval models in instruction-following IR. InF-IR features over 38,000 expressive <instruction,query,passage> triplets with carefully crafted hard negative examples, effectively addressing the critical shortage of high-quality training resources for instruction-aware representation learning; • (ii) Methodology Wise, we propose InF-Embed, an instruction-aware embedding modeloptimized via contrastive learning and instruction-query attention. InF-Embed efficiently encodes and precisely interprets complex user instructions, resolving the efficiency-effectiveness trade-off faced by traditional decoder-only and encoder-only instruction-following retrieval models; and • (iii) Experimental and Benchmark Wise , extensive empirical evaluations demonstrate that InF-Embed consistently improves instruction-following performance for both embedding-based (+9.0 p-MRR) and auto-regressive (+4.2 p-MRR) LMs, facilitated by our diverse training corpus, InF-IR. Moreover, we systematically benchmark a comprehensive suite of contrastive learning 2 Table 1: Summary of existing instruction-following IR datasets.