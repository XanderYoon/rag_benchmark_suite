web pages, programming syntax manuals, and solution expla- nations unified by shared logical, algorithmic, or theoretical foundations. Relevance labels are human-validated, ensuring alignment with intricate reasoning criteria. E Baseline Details Here are the details for each instruction-following retrieval baseline used in our experiments: • Contriever (Izacard et al., 2021) is a bi-encoder dense retriever trained via unsupervised con- trastive learning on large text corpora, providing general-purpose semantic representations for zero-shot retrieval. • FLAN-T5 (Chung et al., 2022) is an instruction-finetuned variant of T5 designed for zero-shot generalization. It employs an encoder-decoder architecture to generate relevance judgments through prompting without retrieval-specific fine-tuning. • E5 (Wang et al., 2022a) models (base and large) are dual-encoder retrieval systems fine-tuned on extensive weakly supervised contrastive pairs. They excel in embedding-based retrieval tasks, explicitly leveraging query and passage instructions for generalized semantic representation. 18 • MonoT5 (Nogueira et al., 2020) is a cross-encoder reranker using the T5 framework to jointly model queries and documents, producing highly accurate relevance scores through generation- based prompting. • Bge (Xiao et al., 2024) employs RoBERTa-based dual encoders fine-tuned with contrastive learning, optimized for stable and accurate dense retrieval without explicit task instructions. • Instructor (Oh et al., 2024) generates task-specific embeddings conditioned on natural language instructions, trained with contrastive learning across diverse NLP tasks, allowing flexible zero- shot application in retrieval and similarity tasks. • Tart-contriever (Asai et al., 2022) extends Contriever with instruction-aware embedding genera- tion via multi-task distillation, enhancing zero-shot retrieval capabilities across varied domains. • GritLM (Muennighoff et al., 2024) integrates generative and embedding-based tasks into a single LLaMA-based instruction-tuned model, achieving state-of-the-art embedding benchmarks while supporting flexible instruction-based retrieval. • Repllama (Ma et al., 2024) fine-tunes the LLaMA-2 model for dense retrieval, leveraging contrastive training on retrieval tasks to encode comprehensive document-level information into embeddings,