19.5 1.1 12.2 3.6 16.4 3.0 49.2 30.1 34.0 53.2 75.3 10.5Qwen2.5-3B-Inst5.0 -1.3 9.7 2.4 6.6 -0.4 7.1 0.2 1.3 3.1 2.2 1.7 8.8 0.3+InF-Embed 19.6 3.3 22.4 1.8 14.6 3.7 18.9 2.9 45.3 29.2 35.0 55.4 72.7 10.6 6 Experiments 6.1 Experiments Setup Evaluation Datasets. We conduct a comprehensive evaluation across the following representative instruction-following retrieval datasets: (1) FollowIR (Weller et al., 2024) including Robust04, News21, and Core17, (2) MAIR (Sun et al., 2024) including Dynamic Domain (DD) and Fair Ranking (FR), and (3) Bright (Su et al., 2024). Detailed descriptions are in appendix D. Evaluation Metrics. Following Weller et al. (2024); Oh et al. (2024), we consider the (1) mean average precision (MAP), (2) pairwise mean reciprocal rank (p-MRR), and (3) normalized discounted cumulative gain (nDCG@5 for FollowIR and nDCG@10 for MAIR) jointly as the metric, while p-MRR is used as the main metric to evaluate the effectiveness of the instruction-following retrieval. Benchmarks and Baselines. We compare the following categories of baselines for a comprehensive benchmark evaluation: (1) non-instruction retrieval models, (2) instruction-following retrieval models, and (3) instruction-tuned LMs. We include additional details of baselines in appendix E. Implementation Details. We consider both embedding models ( e5-base-v2, e5-large-v2, ModernBERT-base) and decoder-only LMs (Llamma-3.2 and Qwen-2.5 variants) as backbone LMs for instruction-aware tuning. Model training and testing are conducted on 8 NVIDIA A100 80G GPUs. We use the AdamW optimizer with an initial learning rate of 5 × 10−5 for both embedding models and LMs. The batch size is set to 4 per device. To prevent test set contamination (Oren et al., 2023) in external evaluations, we have conducted a string-matching analysis, where we do not observe any overlap between the training data in InF-IR and the evaluation datasets utilized in this study. Additional details of the implementation are