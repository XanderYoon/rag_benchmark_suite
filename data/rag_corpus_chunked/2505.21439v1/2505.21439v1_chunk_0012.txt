well as in-batch negatives. Figure 4 reports the average agreement scores (Cohen’s Kappa) between human annotators and various models. The results clearly indicate that o3-mini achieves higher agreement with human judgments compared to other models, including FollowIR-7B, gpt-4o-mini, and gpt-4o, thus confirming the robustness and validity of our filtering strategy. 4.2 Data Visualization and Analysis We conduct a comparative experimental analysis using 10,000 random samples from each of Fol- lowIR (Weller et al., 2024), Promptriever (Weller et al., 2025), and ourInF-IR. Qualitative Analysis of Semantic Coverage. To qualitatively assess topic coverage of our gen- erated training data compared to FollowIR (Weller et al., 2024) and Promptriever (Weller et al., 2025), we first embed instructions, queries, and passages using the off-the-shelf embedding model E5-Mistral(Wang et al., 2023a). As shown in Figure 3(a), samples from our InF-IR cover a sig- nificantly larger semantic space compared to FollowIR and Promptriever. This broader coverage highlights the effectiveness of our synthesized negative instructions and queries in capturing complex semantic variations, crucial for robust contrastive learning. Quantitative Analysis of Diversity. To quantitatively evaluate data diversity, we employ two diversity metrics: average pairwise sample similarity (APS) and inter-sample N-gram frequency (INGF) (Mishra et al., 2020). Results presented in Figure 3(b) clearly indicate that InF-IR achieves superior diversity scores compared to FollowIR and Promptriever, with a lower APS (indicating fewer redundant samples) and a higher INGF (reflecting greater textual diversity). 5 InF-Embed: Instruction-Aware Embedding Training Paradigm In this section, we introduce InF-Embed, a training framework aimed at improving instruction-aware IR. Specifically, we propose two distinct interactions between instructions and queries (section 5.1), and then further explore various contrastive learning objectives (section 5.2). 5.1 Instruction-Query Interaction and Representation We adopt a dual-encoder paradigm (Karpukhin et al., 2020), comprising two encoders g (· ; θP ) and g (· ;