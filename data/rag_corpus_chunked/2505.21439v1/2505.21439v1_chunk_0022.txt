(→) Share Encoder Pooling Epochp-MRR Qwen2.5-1.5B ✓ last 2 2.27 Qwen2.5-1.5B ✓ avg. 2 -0.39 Qwen2.5-1.5B ✗ last 2 0.26 Qwen2.5-1.5B ✓ last 1 -0.06 e5-base qwen2.5-1.5B qwen2.5-1.5B -Instruct Llama-3.2-1B Llama3.2-1B -Instruct 0 1 2 3 4 5Avg. p-MRR InF-IR InF-IR w/o filter Figure 6: Effect of quality filtering. Effect of Negative Pairs Synthesis. We analyze the impact of various training configurations in Table 4, using Qwen2.5-1.5B as the base model. For decoder-only LLMs, our results indicate that using a shared encoder for instruction-aware queries and passages, combined with last-token pooling, consistently yields the best performance and is thus recommended as the default configuration. Effect of Quality Check. We investigate the effectiveness of our data quality-check step by comparing model performance trained on the original unfiltered data versus our quality-filteredInF-IR (Figure 6). Despite the unfiltered dataset being substantially larger, its lower data quality significantly degrades model performance under identical training conditions. This highlights the critical importance of rigorous data validation in our synthesis pipeline. 7 Conclusion In this paper, we introduce InF-IR, a large-scale, high-quality training corpus explicitly de- signed to enhance instruction-following retrieval models. By providing over 30,000 expressive <instruction,query,passage> positive triplets accompanied by more than 60,000 rigorously con- structed hard negative triplets, InF-IR effectively addresses the critical shortage of suitable training data for instruction-aware embedding models. Leveraging InF-IR, we proposed InF-Embed, an instruction-aware embedding-based retrieval model trained through contrastive learning and an instruction-query attention mechanism, effectively resolving the long-standing dilemma between the efficiency of auto-regressive LMs and instruction adherence of embedding models. Extensive experi- ments demonstrated that InF-Embed substantially outperforms state-of-the-art instruction-following retrieval baselines, showcasing robust instruction-following capabilities, superior scalability, de- ployment efficiency, and broad applicability in diverse IR including RAG scenarios. InF-Embed demonstrates robust improvement in instruction-following capabilities across multiple retrieval benchmarks, achieving substantial performance gains for