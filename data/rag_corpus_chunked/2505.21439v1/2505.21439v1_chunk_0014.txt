naturally allows the model to incorporate instructional context when processing the query. Note that this straightforward approach enables instruction-following retrieval without requiring architectural modifications or introducing additional training parameters. ⋄ Interaction II (Cross-Attention): Although effective, concatenation in Eq. (3) can be computa- tionally expensive as it requires a full forward pass for every instruction-query pair. To mitigate this inefficiency, we propose an alternative cross-attention-based mechanism, which explicitly integrates 6 instruction embeddings into the query embeddings via attention: iqj,k = softmax  (ij · Wi) (qk · Wq,1)⊤ / √ d  (qk · Wq,2) , (4) where Wi, Wq,1, Wq,2 ∈ Rd×d are learnable linear transformations. We then define the scoring function for retrieval as: sθ (Pi, Ij, Qk) = sim (pi, iqj,k) , (5) where θ = θP ∪θI+Q denotes parameters from both passage and instruction-query encodersg (· ; θP ) and g (· ; θI,Q ); sim (·, ·) represents the cosine similarity between these embeddings. 5.2 Contrastive Learning Objectives After constructing the positive and negative samples in InF-IR, we flatten them into training tuples (Pi, Ij, Qk), where identical indices (i = j = k) indicate matched positive samples, while differing indices represent unpaired hard negatives. Let the training set be denoted asD = {Pi, Ii, Qi}n i=1. We then introduce an efficient negative sampling strategy along with two contrastive learning objectives. Marginal Sampling Strategy for Negatives. Direct specializing the general NCE objective (Eq.(1)) in a multivariate setup involving passages (P ), instructions (I), and instruction-aware queries (IQ) results in combinatorial sampling complexity O |B||y| , growing combinatorially for large batch sizes, where |y| denotes the number of input variables. For instance, setting y = (P, I, IQ ) yields a cubic summation in the denominator of Eq.(1), i.e.,P m∼B P j∼B P k∼B exp (sθ (Pm,