decoder-only and encoder-only instruction-following retrieval models; and • (iii) Experimental and Benchmark Wise , extensive empirical evaluations demonstrate that InF-Embed consistently improves instruction-following performance for both embedding-based (+9.0 p-MRR) and auto-regressive (+4.2 p-MRR) LMs, facilitated by our diverse training corpus, InF-IR. Moreover, we systematically benchmark a comprehensive suite of contrastive learning 2 Table 1: Summary of existing instruction-following IR datasets. "I", "Q", and "P" denote "instruction", "query", and "passage", respectively. " −" denotes negative samples; for example, "I −" indicates contrasting instruction for negative sample generation. Notations are consistent across tables. Datasets Eval. Train (Q, P)+ I+ I− Q− P− Quality Check #I #Q #P Avg. |I| Avg. |Q| Avg. |P| KILT (2021) ✓ ✗ ✓ ✗ ✗ ✗ ✗ - - 50.7K 5.9M - 160.83 18.23 BEIR (2021) ✓ ✗ ✓ ✗ ✗ ✗ ✗ - - 54.3K 52.8M - 14.78 113.77 MTEB (2023) ✓ ✗ ✓ ✗ ✗ ✗ ✗ - - 1.0M 172M - 25.64 100.14 InstructIR (2024) ✓ ✗ ✓ ✓ ✗ ✗ ✗ gpt-4 9.9K 9.9K 16.1K 49.04 5.57 91.23 FollowIR (2024) ✓ ✗ ✓ ✓ ✗ ✗ ✗ gpt-4 104 104 98.3K 43.51 11.44 122.69 Bright (2024) ✓ ✗ ✓ ✗ ✗ ✗ ✗ - - 1.3K 1.3M - 203.05 343.01 MAIR (2024) ✓ ✗ ✓ ✓ ✗ ✗ ✗ - 805 10.0K 4.3M 33.18 315.16 547.51 InfoSearch (2025) ✓ ✗ ✓ ✓ ✗ ✗ ✗ gpt-4 1.6K 600 6.4K 17.21 8.19 175.98 IFIR (2025) ✓ ✗ ✓ ✓ ✗ ✗ ✗ gpt-4o 2.1K 943 1.4M 99.35 36.52 224.97 Promptriever (2025) ✓ ✓ ✓ ✓ ✗ ✗ ✓ FollowIR-7B 489K 489K 1.6M 103.2 5.95 56.27 InF-IR (Ours) ✓ ✓ ✓ ✓ ✓ ✓ ✓ o3-mini 77.5K 77.5K 116.2K 35.57 8.06 55.2 objectives across multiple embedding models and LMs with varying sizes, thereby supporting rapid