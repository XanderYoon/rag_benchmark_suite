two objects based on a shared quality, as explained in Section 3.1. We pr esented the participant with two differently colored versions of the same virtual object via a handheld mobile AR interface and asked them to select the object that appeared closer to them. The participants were instructed to evaluate all possible comparison pairs from the set of color conditions. Suppose that ğ‘› is the number of colors that we wish to compare against one another. For a given 3D model, each participant is presented with (ğ‘›(ğ‘› âˆ’ 1))/2 pairs. In our experiment, in which we have 12 color conditions, each participant compares 66 pairs of stimuli per model (see Figure 3). A userâ€™s vote is recorded for every selection. After evaluation of all pairs for a model, these votes are combined into a single ğ‘› x ğ‘› preference matrix. An example of one such preference matrix can be seen in Figure 3. If a matrix cell at row ğ‘– and column ğ‘— contains a 1, then this indicates that the participant selected the color at row ğ‘– as closer in depth than the color at row ğ‘—. This also implies that the matrix cell at row ğ‘— and column ğ‘– would be filled with a 0. The preference matrices of all participants are then added together to determine overall scores of each color for an experimental condition . The participant repeats this process for all 4 models, leading to a total of 264 comparisons. 3.4 Research Questions and Hypotheses RQ1: How does the shape of a 3D model interact with color and luminance as depth cues? H1 (Shape): Simple shapes will be more affected by color and luminance as depth cues than complex shapes . Previous work found that a realistic 3D object was less subject to the