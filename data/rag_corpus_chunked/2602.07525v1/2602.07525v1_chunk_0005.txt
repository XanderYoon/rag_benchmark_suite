abstraction to support deep reasoning and significantly improve decision accuracy. Evaluations on six benchmarks show IGMiRAG outper- forms the state-of-the-art baseline by 4.8% EM and 5.0% F1 on average, with token costs adapting to task difficulty. These results validate that by mimicking intuition-guided reasoning, our approach improves efficiency while simulta- neously enhancing memory precision and reasoning depth, offering a viable solution to memory fragmentation and retrieval inefficiency in RAG systems. 2. Related Works Structure Optimization.Text-based RAG methods (Lewis et al., 2020; Gao et al., 2023a) extend LLM capabilities by simply concatenating raw text chunks, but often struggle with semantic sparsity and cross-contextual tasks (Gupta et al., 2024). Recent work has introduced graphs to sys- tematically capture entity-level relationships, enhancing the richness of knowledge connections. However, these meth- ods overlook higher-order multi-entity interactions, leading to information gaps (Srinivasan et al., 2018; Santos et al., 2022; Labatut & Bost, 2019). GraphRAG (Edge et al., 2024) thus supplemented thematic summaries with dense commu- nity reports to enable macroscopic analysis. Hypergraphs extend graphs by enabling a single hyperedge to connect multiple vertices at once (Gao et al., 2022; Feng et al., 2024). 2 IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory leveraging this capability, Hyper-RAG (Feng et al., 2025) further unifies high-order multi-entity relations, thereby re- ducing fragmentation. Nevertheless, both homogeneous graphs and hypergraphs only capture surface-level seman- tic links. While NodeRAG (Xu et al., 2025) introduced node-type heterogeneity to encode cross-granular structural connections, IGMiRAG advances further by constructing a hierarchical heterogeneous hypergraph whose layered hy- peredges explicitly encode deductive pathways, achieving improvements at both structural and semantic levels. Query Optimization.Beyond structural enrichment, a complementary line of work optimizes the query side to improve recall. Query rewriting narrows semantic gaps via context augmentation or rephrasing (Gao et al., 2023b), while query decomposition