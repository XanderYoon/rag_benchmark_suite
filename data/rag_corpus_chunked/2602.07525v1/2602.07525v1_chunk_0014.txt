knowl- 5 IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory edge coverage for complex problems while preserving effi- ciency and cost-effectiveness for simple queries. 4. Experiment 4.1. Experimental Setup Baselines.We compared our approach with the state-of- the-art (SOTA) and widely adopted RAG methods. These include: Naive RAG; graph-enhanced methods ( LightRAG, PathRAG, and NodeRAG) and hypergraph-enhanced meth- ods (Hyper-RAG and Cog-RAG). Detailed descriptions of these baselines are provided in Appendix D.1. Benchmarks.To comprehensively assess the cross-task generalisation of RAG methods, we utilize six public bench- marks spanning three representative tasks: (i)Detail Cap- ture (Simple QA)—PopQA (Mallen et al., 2023); (ii)Multi- Hop Reasoning (Multi-Hop QA)—MuSiQue (Trivedi et al., 2022)and 2WikiMultihop (Ho et al., 2020) (2Wiki), and HotpotQA (Yang et al., 2018); and (iii)Knowledge Explanation (Explanatory QA)—Mix (Qian et al., 2024) and Pathology (Xiong et al., 2024). Simple QA targets single-fact detail questions, and Multi-Hop QA demands cross-context aggregation and logical deduction, emphasiz- ing error correction and chained reasoning following initial retrieval failures. Explanatory QA utilizes long passages with random-hop questions to evaluate comprehensive se- mantic representation and latent association mining. Bench- mark statistics are detailed in Appendix D.2. Metrics.We adopt three metrics across tasks: Exact Match (EM) andF1scores to measure literal overlap and n-gram recall rates between model outputs and reference answers, and average tokens per query (Avg. Tokens) to measure method efficiency. For Explanatory QA, we employ an LLM-based evaluator to assign EM and F1 scores, assessing factual correctness and semantic relevance. Metric details are provided in Appendix D.3. Implementation Details.We employ text-embedding-3- small for text encoding and GPT-4o-mini (Achiam et al., 2023) as the LLM. For IGMiRAG, the slice length is set to 1024 tokens for Explanatory QA and 780 for all others. All baselines are configured using their officially recom- mended indexing and retrieval hyperparameters (detailed