IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory Xingliang Hou 1 Yuyan Liu1 Qi Sun 1 Haoxiu Wang 2 Hao Hu 3 Shaoyi Du 3 Zhiqiang Tian 1 Abstract Retrieval-augmented generation (RAG) equips large language models (LLMs) with reliable knowledge memory. To strengthen cross-text as- sociations, recent research integrates graphs and hypergraphs into RAG to capture pairwise and multi-entity relations as structured links. How- ever, their misaligned memory organization ne- cessitates costly, disjointed retrieval. To address these limitations, we propose IGMiRAG, a frame- work inspired by human intuition-guided reason- ing. It constructs a hierarchical heterogeneous hypergraph to align multi-granular knowledge, incorporating deductive pathways to simulate re- alistic memory structures. During querying, IG- MiRAG distills intuitive strategies via a question parser to control mining depth and memory win- dow, and activates instantaneous memories as an- chors using dual-focus retrieval. Mirroring human intuition, the framework guides retrieval resource allocation dynamically. Furthermore, we design a bidirectional diffusion algorithm that navigates deductive paths to mine in-depth memories, em- ulating human reasoning processes. Extensive evaluations indicate IGMiRAG outperforms the state-of-the-art baseline by 4.8% EM and 5.0% F1 overall, with token costs adapting to task com- plexity (average 6.3 k+, minimum 3.0 k+). This work presents a cost-effective RAG paradigm that improves both efficiency and effectiveness. 1School of Software Engineering, Xi’an Jiaotong University 2School of Engineering, Westlake University 3State Key Labora- tory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applica- tions, and Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University. Correspondence to: Zhiqiang Tian <zhiqiang- tian@xjtu.edu.cn>. Preprint. February 10, 2026. 1. Introduction Humans instantly activate long-term memory and think rapidly to generate goal-directed solutions in complex envi- ronments. Our cognitive maturity is rooted in the continuous consolidation of memory and reasoning. While Large Lan- guage Models (LLMs)