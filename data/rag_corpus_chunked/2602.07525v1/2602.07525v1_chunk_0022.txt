by improving retrieval efficiency and effectiveness to enhance memory precision and reasoning depth in Large Language Models. While our work may have various potential societal impli- cations, we do not foresee specific concerns that warrant emphasis beyond the general risks associated with large language models and information retrieval systems. References Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report.arXiv preprint arXiv:2303.08774, 2023. Asai, A., Wu, Z., Wang, Y ., Sil, A., and Hajishirzi, H. Self- rag: Learning to retrieve, generate, and critique through self-reflection. InThe Twelfth International Conference on Learning Representations, 2023. Chen, B., Guo, Z., Yang, Z., Chen, Y ., Chen, J., Liu, Z., Shi, C., and Yang, C. Pathrag: Pruning graph-based re- trieval augmented generation with relational paths.arXiv preprint arXiv:2502.14902, 2025a. Chen, S., Zhou, C., Yuan, Z., Zhang, Q., Cui, Z., Chen, H., Xiao, Y ., Cao, J., and Huang, X. You don’t need pre-built graphs for rag: Retrieval augmented genera- tion with adaptive reasoning structures.arXiv preprint arXiv:2508.06105, 2025b. Cormack, G. V ., Clarke, C. L., and Buettcher, S. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. InProceedings of the 32nd interna- tional ACM SIGIR conference on Research and develop- ment in information retrieval, pp. 758–759, 2009. Dagdelen, J., Dunn, A., Lee, S., Walker, N., Rosen, A. S., Ceder, G., Persson, K. A., and Jain, A. Structured infor- mation extraction from scientific text with large language models.Nature communications, 15(1):1418, 2024. Edge, D., Trinh, H., Cheng, N., Bradley, J., Chao, A., Mody, A., Truitt, S., Metropolitansky, D., Ness, R. O., and Larson, J. From local to global: A graph rag ap- proach to query-focused summarization.arXiv preprint arXiv:2404.16130, 2024. Feng, Y ., Yang, C., Hou, X., Du, S., Ying, S., Wu,