methods, while subfigure(b)provides a comprehensive comparison of all RAG methods on MuSiQue regardingEM,Avg. Tokens, andAvg. Time. baseline, securing the top spot on Simple QA and second place on Mix. Hypergraph-enhanced Cog-RAG observably exceeds Hyper-RAG on Explanatory QA but underperforms on Simple QA and Multi-Hop QA, with drops up to 19.0% EM and 15.6% F1 on PopQA. Crucially, on the two most challenging benchmarks—MuSiQue and 2WikiMultiHop, IGMiRAG surpasses the second-best method by4.4%/4.7% and 12.2%/13.4% in EM/F1, respectively, confirming its effectiveness in multi-hop reasoning. Besides securing sec- ond place on Simple QA, IGMiRAG achieves the highest performance on all five other benchmarks, demonstrating strong generalization and robustness. QA Efficiency.Query efficiency is critical for the practi- cal viability of RAG systems. As shown in Figure 3 (a), the token consumption varies significantly across structure- enhanced methods. NodeRAG consumes the fewest tokens on average 5.4 k+, while IGMiRAG requires only ≈0.9 k more (6.3 k+). In contrast, all other methods exceed 11 k to- kens (maximum 33.8 k+) per query. A task-level breakdown reveals the following efficiency/performance trade-offs. (i) Simple QA: IGMiRAG ranks second in performance while consuming only 3.0 k+ avg. tokens— 1.7 k+ fewer than NodeRAG (4.7 k+). (ii) Multi-Hop QA: Averaged across three benchmarks, IGMiRAG achieves the best performance with a minimum of5.1 k+ tokens, 10.37% (0.5 k+) less than NodeRAG (5.7 k+) and 60.04% (7.7 k+) less than Hyper- RAG (12.9 k+). (iii) For Explanatory QA, where cross- domain knowledge fusion is required, IGMiRAG increases its token usage to 8.6 k+ on Mix and 11.0 k+ on Pathology. Despite this increase, it achieves the highest performance at the second-lowest token cost. Notably, the token con- sumption of all baselines, including NodeRAG, remains approximately constant across varying task complexities, exhibiting only minor and irregular fluctuations. In contrast, IGMiRAG’s consumption scales proportionally