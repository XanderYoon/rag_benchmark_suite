correctness and semantic relevance. Metric details are provided in Appendix D.3. Implementation Details.We employ text-embedding-3- small for text encoding and GPT-4o-mini (Achiam et al., 2023) as the LLM. For IGMiRAG, the slice length is set to 1024 tokens for Explanatory QA and 780 for all others. All baselines are configured using their officially recom- mended indexing and retrieval hyperparameters (detailed in Appendix D.4) to ensure fair and reproducible comparisons. 4.2. Main Results We report QA performance and average token costs per query across all benchmarks, calculated against the gold- standard question–answer pairs. QA Performance.Table 1 presents the performance of each method across six benchmarks. IGMiRAG achieves the highest average scores (58.3% EM, 65.9% F1), maintaining a consistent lead across different tasks. Naive RAG exhibits monotonic improvement with additional slices, albeit with diminishing marginal returns. While Naive RAG (T op−5) ranks third overall, it performs second-best on MuSiQue and HotpotQA, outperforming several structure-enhanced meth- ods despite its lower retrieval cost. Among graph-enhanced methods, PathRAG ranks last overall ( 42.1% EM, 49.5% F1), with minimal gains (4.5% EM and 7.5% F1) over the LLM baseline. NodeRAG, however, emerges as the SOTA Table 1.QA performanceincluding EM, F1 scores (%) on six RAG benchmarks. This table, along with the following ones, highlight the bestand second-best results. Methods Simple QA Multi-Hop QA Explanatory QA Overall PopQA MuSiQue 2Wiki HotpotQA Mix Pathology Avg. EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 GPT-4o-mini 20.7 24.5 12.4 21.7 32.2 37.4 30.8 41.4 57.5 57.5 72.1 69.4 37.6 42.0 Naive RAG RAG (T op−1) 41.3 51.9 21.4 31.4 28.8 35.1 41.4 55.6 69.6 68.4 76.5 74.8 46.5 52.9 RAG (T op−3) 46.6 58.6 27.4 38.4 37.9 45.1 48.5 62.7 73.9 72.2 76.8 75.5 51.8 58.8 RAG (T op−5) 48.9 60.6 28.6 40.3 41.1