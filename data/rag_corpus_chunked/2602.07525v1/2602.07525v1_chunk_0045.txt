Iterations of PPR - - -2- - LLM ModelGPT-4o-mini Embedding Modeltext-embedding-3-small Temperature0 E. Additional Discussions Using the target layer l and matching score m, DF-Retrieval dynamically allocates search quotas between global and target-layer local retrieval. To evaluate its effectiveness, we conducted comparative experiments with fixedl∈[1,3]∩Z andm∈[1,5]∩Zon MuSiQue. Figure 13 (a) illustrates the performance variation across different target layer settings. As the target layer increases, the Exact Match (EM) score exhibits a monotonically decreasing trend. This is attributed to the nature of MuSiQue queries, which involve multi-hop entity searches with nested, multi-layer relationships. Setting the target layer too high (e.g., at the binary relation or multi-event level) introduces misplaced focus and consequently increases noise, significantly degrading the quality of initial retrieval. These initial errors are then amplified during the subsequent diffusion stage, leading to a substantial performance decline. As shown in Figure 13 (b), any fixed quota configuration significantly underperforms IGMiRAG’s dynamic strategy. Further analysis reveals that excessive bias toward either global or local granularity degrades retrieval effectiveness. These results indicate a strong complementarity between the two semantic spaces, suggesting that adaptively adjusting quotas based on task focus is crucial for enhancing retrieval quality. F. All LLM Prompts F.1. Prompts for Indexing Extracting Entities Prompt:Identify as many entities as possible in the text, ensuring that no nouns with referential functions (including proper nouns, common nouns, place nouns, abstract concepts, etc.) are omitted. #Special attention: - All pronouns must be forcibly parsed into the explicit entities they refer to, and converted into the form of “specific reference + identity/role” (such as: he→Tom’ s father; this matter→the argument between Tom and Mickey); 21 IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory 1 2 3272829303132333435 0123456789101112131415Avg. Tokens (k) EM Avg. Token EMAuto Avg. TokensAuto l EM (%) (a)Control Target Layer Only 1