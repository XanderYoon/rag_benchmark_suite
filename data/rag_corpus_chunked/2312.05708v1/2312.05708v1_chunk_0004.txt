query augmentation while achieving comparable performance. Recent studies have shown LLMs can act as zero-shot rankers through pairwise ranking prompt- ing (Qin et al., 2023). While addition of rank- ing for retrieval component has shown improve- ment in QA tasks, direct use of LLMs for the ranking task, in addition to plan generation, incurs twice the inference cost. We empirically show that our proposed lightweight context tuning method, LambdaMART (Burges, 2010) based RRF (Cor- mack et al., 2009), outperforms both fine-tuning approach and GPT-4 (OpenAI, 2023) based CoT Augmentation. 3 Methodology Our experiments train and evaluate tool retrieval and planning with and without context tuning. Fig- ure 1 illustrates how a context-seeking query uses context retrieval to enhance tool retrieval and plan generation. 3.1 Data Generation Our study employed a data generation methodology using synthetic application data, aimed at simulat- ing real-world scenarios for a digital assistant. The data encompasses 7 commonly used applications: mail, calendar, google, music, reminders, notes, and phone call. We generated this data using GPT- 4, ensuring diversity in the dataset to reflect a wide range of user personalities. The synthetic dataset contained a diverse range of context items spanning various applications. A total of 791 distinct per- sonas were synthesized, yielding 4,338 unique im- plicit queries for training and 936 implicit queries for evaluation. Additionally, we developed a toolbox containing APIs for each of the applications we considered. This toolbox was created using in-context learn- ing with GPT-4 and contained a total of 59 APIs distributed across the applications. To simulate user interaction with a virtual assis- tant, GPT-4 was also utilized to generate realistic queries grounded in the application data. Following this, we employed GPT-4 to retrieve the appropri- ate tool from the generated toolbox in response to these queries. Finally, GPT-4 was used