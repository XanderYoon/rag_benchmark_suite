contextual understand- ing. Our empirical observations reveal that CoT augmentation enhances context retrieval when fine- tuning is not applied, while fine-tuning the retrieval model eliminates the need for CoT augmentation. Furthermore, we observe that context augmenta- tion at the plan generation stage reduces halluci- nations. Finally, we showcase the superiority of our proposed lightweight model using RRF with LambdaMART over the GPT-4-based system. Limitations The current work does not utilize conversation his- tory, which is crucial for handling explicit multi- turn instructions that contain anaphora or ellipsis. This limitation also hinders the model’s ability to effectively process and respond to complex tasks that require multi-hop context retrieval. Addition- ally, the absence of conversation history impedes the model’s ability to adapt to topic shifts that may occur throughout a dialogue. Furthermore, the performance of the planner model is constrained by the length of the context window. While employing LLMs with longer con- text windows can enhance performance, it also in- creases model size and computational complexity. To address this limitation, incorporating context compression techniques could potentially improve end-to-end performance without incurring signifi- cant increases in model size. Due to privacy constraints, we simulated real- world data by generating synthetic user profiles and personas that mirrored real-world use cases for a digital assistant. Ethics Statement To safeguard privacy, this study exclusively utilizes synthetically generated data, eliminating the use of real user information under ethical considerations. Acknowledgements We would like to thank Stephen Pulman, Barry Theobald and Joel Moniz for their valuable feed- back. References Christopher J.C. Burges. 2010. From ranknet to lamb- darank to lambdamart: An overview. Microsoft Re- search Technical Report MSR-TR-2010-82. Gordon V . Cormack, Charles L. A. Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd International ACM