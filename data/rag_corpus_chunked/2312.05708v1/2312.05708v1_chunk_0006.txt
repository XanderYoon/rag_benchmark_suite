fine-tuned variants of this method. CoT Augmentation To enhance the likelihood of semantic alignment with pertinent contextual elements, we augment the under-specified or im- plicit query with GPT-4 (OpenAI, 2023) generated CoT.3 We evaluate both pre-trained and fine-tuned semantic search versions utilizing CoT. LambdaMART with RRF Reciprocal Rank Fu- sion (RRF) (Cormack et al., 2009) is shown to outperform individual rank learning methods. To leverage this advantage, we propose a lightweight 2Refer to Appendix A for more details on data generation. 3Please refer Appendix A.6 for the GPT-4 prompt used and Table 5 for CoT examples. model that uses LambdaMART (Burges, 2010) for initial ranking of data across context stores, fol- lowed by re-ranking using RRF. 3.3 Tool Retrieval While advanced ranking models can enhance the recall of tool retrieval, we employ the pre-trained GTR-T5-XL model for semantic search using co- sine similarity to retrieve the top-K tools. Extend- ing the tool retrieval process to incorporate ranking should be a straightforward endeavor. We evaluate tool retrieval performance with and without context retrieval using Recall@K. 3.4 Planner The planner’s objective is to select the most appro- priate tool from the retrieved tool list and gener- ate a well-formed plan. A plan comprises an API call constructed using the chosen tool and parame- ters extracted from the query and retrieved context. We fine-tune OpenLLaMA-v2-7B (Touvron et al., 2023) for plan generation. To assess the planner’s performance, we employ the Abstract Syntax Tree (AST) matching strategy to compute plan accuracy. A hallucination is defined as a plan generated using an imaginary tool. 4 Results 4.1 Context Retrieval Consistent with expectations, vector-based search surpasses text-based search, as shown in Table 1. Nevertheless, both approaches struggle to retrieve relevant context for under-specified queries. Fine- tuned semantic search and CoT augmentation with pre-trained semantic search both