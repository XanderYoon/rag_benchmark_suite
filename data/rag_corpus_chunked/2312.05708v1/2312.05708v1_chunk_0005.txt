ing with GPT-4 and contained a total of 59 APIs distributed across the applications. To simulate user interaction with a virtual assis- tant, GPT-4 was also utilized to generate realistic queries grounded in the application data. Following this, we employed GPT-4 to retrieve the appropri- ate tool from the generated toolbox in response to these queries. Finally, GPT-4 was used to re- solve the toolâ€™s API with the correct parameters. This methodology provided a comprehensive and realistic dataset, essential for the evaluation of our Figure 1: Context-tuned RAG pipeline illustrating end-to-end processing of a complex request with progressive plan generation. context tuning approach in RAG-based planning systems.2 3.2 Context Tuning To compare various context retrieval methods, we employ both text-based and vector-based retrieval baselines. We simulate different context stores by structuring context data per persona and train mod- els to perform federated search. We use query and persona meta-signals, such as frequency, usage his- tory, and correlation with geo-temporal features, to perform retrieval. We evaluate context retrieval using the Recall@K and Normalized Discounted Cumulative Gain (NDCG@K) metrics. BM25 For text-based search, we use an improved version of BM25, called BM25T (Trotman et al., 2014). Semantic Search For vector-based search, we employ the widely adopted Semantic Search ap- proach. We use GTR-T5-XL (Ni et al., 2021) to generate query and context item embeddings, which are then ranked using cosine similarity to se- lect the top-K results. We evaluate both pre-trained and fine-tuned variants of this method. CoT Augmentation To enhance the likelihood of semantic alignment with pertinent contextual elements, we augment the under-specified or im- plicit query with GPT-4 (OpenAI, 2023) generated CoT.3 We evaluate both pre-trained and fine-tuned semantic search versions utilizing CoT. LambdaMART with RRF Reciprocal Rank Fu- sion (RRF) (Cormack et al., 2009) is shown to outperform individual