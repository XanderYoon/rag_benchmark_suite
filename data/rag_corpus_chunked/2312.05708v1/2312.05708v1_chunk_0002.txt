that traditional RAG is inadequate for implicit/context-seeking queries and present context tuning as a viable solution; 2. We provide a systematic comparison of vari- ous context retrieval methods applied on both lightweight models and LLMs; 3. We share empirically the insight that Chain of Thought (CoT) augmentation improves con- text retrieval when no fine-tuning is applied, whereas fine-tuning the retrieval model re- moves the need for CoT augmentation; 4. We propose a lightweight model using Re- ciprocal Rank Fusion (RRF) (Cormack et al., 1Typically, the query along with retrieved tools undergo dynamic prompt construction before presented to an LLM. This process is called Query Decoration/Transformation. We omit that in this work for the sake of simplicity. arXiv:2312.05708v1 [cs.IR] 9 Dec 2023 2009) with LambdaMART (Burges, 2010), which outperforms GPT-4 (OpenAI, 2023) system, and finally; 5. We show that context augmentation at plan generation reduces hallucinations. 2 Related Work Using retrieval to incorporate tools into plan gen- eration with LLMs has emerged as a burgeoning area of research, with ongoing investigations aimed at enhancing both the retrieval component and the LLMs themselves. Our work falls within the for- mer category, placing a particular emphasis on refining retrieval methodologies to enhance con- textual understanding of implicit and ambiguous queries that demand context-seeking capabilities. The integration of tools into generation has been demonstrated to enhance the capabilities of LLM- based planners in recent studies (Schick et al., 2023; Lu et al., 2023). However, these works primarily fo- cus on well-defined or unambiguous queries, where retrieving supplementary information to augment the query is not strictly required. For question an- swering (QA) tasks, incorporating any off-the-shelf document retriever has been shown to improve LLM generation, with the addition of re-ranking further boosting performance (Ram et al., 2023). While re-ranking is preferred, employing any pre- trained