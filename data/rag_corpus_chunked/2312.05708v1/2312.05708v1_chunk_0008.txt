an in- crease in K and Recall@K. 4.2 Tool Retrieval Figure 2 illustrates the performance of tool retrieval using semantic search. Incorporating relevant con- text into tool retrieval consistently yields substan- tial gains across various K-values. 4.3 Planner To establish the planner’s lower bound, we remove the retrieval step, while the upper bound is set by directly utilizing context and/or tool labels, effec- Table 2: End-to-end planner evaluation both with and without context tuning. “Lower Bound" excludes re- trieval and performs direct plan generation while “Upper Bound" assumes perfect context and tool retrieval. Setting AST-based Plan Acc↑ Exact Match↑ Hallucination↓ Lower Bound 43.77 39.45 2.59 RAG-based Planner 76.39 58.12 1.76 Context-tuned RAG Planner 85.24 67.33 0.93 Upper Bound 91.47 72.65 0.85 Context-tuned Upper Bound 91.62 72.84 0.53 tively employing oracle retrievers. Table 2 encap- sulates the end-to-end evaluation of the fine-tuned planner, demonstrating that the context-tuned plan- ner significantly outperforms the planner based on traditional RAG using semantic search. Notably, even when the correct tool is retrieved, incorpo- rating relevant context in plan generation, as evi- denced by the upper bound, helps in reducing hal- lucination. 5 Conclusion Our work introduces context tuning, a novel compo- nent that enhances RAG-based planning by equip- ping it with essential context-seeking capabilities to address incomplete or under-specified queries. Through a systematic comparison of various re- trieval methods applied to both lightweight models and LLMs, we demonstrate the effectiveness of context tuning in improving contextual understand- ing. Our empirical observations reveal that CoT augmentation enhances context retrieval when fine- tuning is not applied, while fine-tuning the retrieval model eliminates the need for CoT augmentation. Furthermore, we observe that context augmenta- tion at the plan generation stage reduces halluci- nations. Finally, we showcase the superiority of our proposed lightweight model using RRF with LambdaMART over