these works primarily fo- cus on well-defined or unambiguous queries, where retrieving supplementary information to augment the query is not strictly required. For question an- swering (QA) tasks, incorporating any off-the-shelf document retriever has been shown to improve LLM generation, with the addition of re-ranking further boosting performance (Ram et al., 2023). While re-ranking is preferred, employing any pre- trained retriever, particularly a text-based retriever, would be sub-optimal due to the inadequate in- formation expected from ambiguous queries. Our work demonstrates the inadequacy of text-based retrievers for context retrieval and the necessity of more advanced retrieval models. To address the lack of context inherent in under- specified queries, some studies have explored the use of CoT (Wei et al., 2022) mechanisms to gener- ate text that closely approximates the semantic sim- ilarity of relevant context (Ma et al., 2023). While CoT augmentation improves upon baseline meth- ods, such as vanilla semantic search, CoT may potentially increase the input length to the LLM, which has a limited context window size. Addi- tionally, studies have demonstrated that the place- ment of relevant information impacts LLM gen- eration (Liu et al., 2023). Therefore, it is prefer- able to avoid increasing input sequence length if the same or better results can be achieved with- out query augmentation. Distillation-based query augmentation approaches have been proposed to address this problem (Srinivasan et al., 2023). Our work unveils that fine-tuning semantic search ob- viates the necessity for query augmentation while achieving comparable performance. Recent studies have shown LLMs can act as zero-shot rankers through pairwise ranking prompt- ing (Qin et al., 2023). While addition of rank- ing for retrieval component has shown improve- ment in QA tasks, direct use of LLMs for the ranking task, in addition to plan generation, incurs twice the inference cost. We empirically