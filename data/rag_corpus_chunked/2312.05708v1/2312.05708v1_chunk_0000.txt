Context Tuning for Retrieval Augmented Generation Raviteja Anantha, Tharun Bethi, Danil V odianik, Srinivas Chappidi Apple Abstract Large language models (LLMs) have the re- markable ability to solve new tasks with just a few examples, but they need access to the right tools. Retrieval Augmented Generation (RAG) addresses this problem by retrieving a list of relevant tools for a given task. However, RAGâ€™s tool retrieval step requires all the required in- formation to be explicitly present in the query. This is a limitation, as semantic search, the widely adopted tool retrieval method, can fail when the query is incomplete or lacks context. To address this limitation, we propose Context Tuning for RAG, which employs a smart con- text retrieval system to fetch relevant informa- tion that improves both tool retrieval and plan generation. Our lightweight context retrieval model uses numerical, categorical, and habitual usage signals to retrieve and rank context items. Our empirical results demonstrate that context tuning significantly enhances semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for context retrieval and tool re- trieval tasks respectively, and resulting in an 11.6% increase in LLM-based planner accu- racy. Additionally, we show that our proposed lightweight model using Reciprocal Rank Fu- sion (RRF) with LambdaMART outperforms GPT-4 based retrieval. Moreover, we observe context augmentation at plan generation, even after tool retrieval, reduces hallucination. 1 Introduction Large language models (LLMs) excel in a variety of tasks ranging from response generation and log- ical reasoning to program synthesis. One of the important active areas of LLM research is to uti- lize them as planning agents (Huang et al., 2022). Planning is an essential functionality for processing complex natural language instructions. A planner should possess the ability to select the appropriate tools to complete each sub-task. While LLMs ex- hibit exceptional generation