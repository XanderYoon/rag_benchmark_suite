presents the Mean Reciprocal Rank (MRR) results, confirming the superior ranking quality of our approach. Appendix A, C and D have various ablation studies showing how each design choice (e.g., FIM vs. non-FIM formatting, context length, etc.) impacts accuracy in our dataset. Table 1 Top-K Accuracy of Retrieval Methods (Non-FIM) Method @5 (%) @10 (%) @20 (%) @40 (%) BM25 (Baseline) 26.69 34.16 40.28 53.02 Prefix Code Embed 58.21 65.71 75.36 85.36 LLM Description 63.35 68.68 76.87 82.92 Hypothetical Code Gen63.93 71.79 81.43 87.86 Table 2 Mean Reciprocal Rank (MRR)@K of Retrieval Methods (Non-FIM) Method @5 @10 @20 @40 BM25 (Baseline) 0.17 0.18 0.18 0.19 Prefix Code Embed 0.43 0.44 0.45 0.45 LLM Description 0.48 0.49 0.49 0.49 Hypothetical Code Gen0.51 0.52 0.52 0.53 Table 3 Latency Analysis (Non-FIM) Reranker Latency (ms) None ~22 4B @ Dense 40 (HF) ~342 4B @ Dense 40 (vLLM) ~89 8B @ Dense 40 (vLLM) ~121 6.2. Latency Analysis Table 3 presents latency measurements for different pipeline configurations. Our optimized 0.6B reranker matches or exceeds the 8B model while maintaining significantly reduced latency. 7. Post Training and Optimization To bridge the performance gap between the 0.6B and 8B Qwen reranker for our use case while enabling real-time predictions, we developed a comprehensive post-training pipeline that optimizes compact reranker models to achieve performance comparable to much larger models. The goal is to match or surpass an 8B rerankerâ€™s ranking quality at much lower latency and cost. This matters in production: a 0.6B model fits tighter memory budgets, runs with higher concurrency, and reduces tail latency. Our SFT+RL results now exceed the 8B baseline while keeping the 2.5x latency gain, which makes the approach deployment-ready. 7.1. Training Dataset A critical challenge in training our reranker was ensuring no training contamination from our evaluation datasets. To address