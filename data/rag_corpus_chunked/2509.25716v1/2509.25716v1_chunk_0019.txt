Loss Analysis E.1. Supervised Fine-Tuning (SFT) Loss Figure 9 shows the training loss progression during supervised fine-tuning. The model shows stable convergence with the loss decreasing steadily over epochs, indicating effective learning of the ranking task. Figure 9:Training loss progression during supervised fine-tuning (SFT). Figure 10 shows the epoch-wise loss during SFT training, providing a more granular view of the training progression across different epochs. Figure 10:Epoch-wise loss during supervised fine-tuning (SFT). E.2. Reinforcement Learning Reward Progression Figure 11 shows the reward progression during reinforcement learning training. The completion-aware yes/no reward captures how frequently sampled responses align with the supervision label, so rising curves indicate the model is pushing more of its rollouts toward the correct decision token. Figure 11:Reward progression during reinforcement learning training. E.3. Reinforcement Learning Training Loss Figure 12 shows the training loss during reinforcement learning, providing insight into the convergence behavior of the RL optimization process. Figure 12:Training loss during reinforcement learning. F. Out-of-Distribution Generalization To validate that our training pipeline preserves the base modelâ€™s generalization capabilities, we evaluated all trained models on an out-of-distribution dataset that none of the models had seen during training. Table 7 reports the results. Table 7 Out-of-Distribution Performance Validation Model @5 (%) @10 (%) @15 (%) Qwen 0.6B Base 85.50 89.90 91.30 Qwen 0.6B SFT + RL 85.30 88.90 90.70 Qwen 0.6B SFT 85.10 89.40 90.90 Qwen 0.6B Extended SFT86.70 91.40 92.10 These results demonstrate that our trained models maintain strong generalization capabilities without suffering from catastrophic forgetting. The Qwen 0.6B models neither significantly outperform nor degrade compared to the base model performance, indicating successful specialization for our specific use case while preserving general code understanding abilities. G. Reward Function for Reinforcement Learning Our reinforcement learning stage uses completion-aware supervision that inspects the first decision token produced by the reranker.