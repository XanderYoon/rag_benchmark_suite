Shanmugham); chitra.ganapathy@servicenow.com (C. Ganapathy) Â©2025 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). evaluation methodology, Section 6 shows main results and ablation studies, and Section 7 details our post-training pipeline for optimizing compact reranker models. 2. Related Work Our work builds on recent advances in neural code retrieval, retrieval-augmented generation (RAG), structural code analysis, and search refinement techniques, adapting them to a large-scale enterprise environment. 2.1. Neural Code Retrieval Code search has evolved from keyword-based methods like BM25 [2] to dense retrieval models such as CodeBERT [3], which embed queries and code into a shared semantic space. Yet recent evaluations (e.g., CoIR [4]) show that general-purpose dense retrievers degrade in domains different from what they were trained on, mainly because their pretraining rarely covers such niche knowledge. To control for this effect, we adopt BM25 as a strong, domain-agnostic baseline that remains competitive under out-of-domain conditions. Our work then targets the missing piece: a domain-aware retrieval pipeline tailored to ServiceNow Script Includes. 2.2. RAG with Filtering and Query Enhancement Retrieval-Augmented Generation (RAG) [5] improves LLM outputs by dynamically providing relevant context, now common in coding assistants [6]. A key challenge is ensuring retrieved context relevance, which can be addressed by leveraging code structure [7] and query enhancement techniques. Inspired by Hypothetical Document Embeddings [8], LLMs can generate complete hypothetical code snippets from partial code, creating richer queries. While many systems build Code Knowledge Graphs from source code [ 9, 10] to enable filtering, our RAG pipeline constructs a Knowledge Graph from ServiceNow platform metadata for scope-level filtering. This constrains the search space and enables efficient retrieval of relevant Script Includes within our enterprise-specific context. 2.3. Reranking Following retrieval, a cross-encoder or long-context LLM reranker [11] can re-order top candidates,