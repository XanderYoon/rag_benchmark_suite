Rank (MRR):The average of the reciprocal ranks of the correct Script Include across all queries, providing a more nuanced view of ranking quality. 5.2. Baselines and Implementation We compare our proposed pipeline against BM25 [2] as the primary baseline, which achieved 53.02% top-40 accuracy on our dataset. Our implementation uses the following components: • Embedding Model:Linq-AI-Research/Linq-Embed-Mistral (7B parameters, 32K context length) •Reranker Models:Qwen-8B (baseline) and our optimized Qwen-0.6B models •Judge Model:Gemini 2.5 Flash (1M context length) for intent clarity evaluation 5.3. Pretraining Knowledge Check To test whether Script Include knowledge was already present in model pretraining corpora, we prompted LLMs to autocomplete our evaluation samples without any retrieval context (non-FIM, no KG, no index). The model produced the correct Script Include namespace in only 5% of cases, indicating limited memorization/coverage and motivating retrieval for this domain. 6. Experiments and Results 6.1. Main Results We evaluate three primary retrieval methods against our BM25 baseline: (1)Prefix Code Embed (Non-FIM), which uses embeddings of the code preceding the cursor; (2)LLM Description, which generates a natural language description of user intent; and (3)Hypothetical Code Generation, which generates hypothetical code completions for retrieval. For concise method prompts and working examples, see Appendix J and Section I, respectively. Table 1 shows the performance of these methods on our clear-intent evaluation subset (705 samples). The Hypothetical Code Generation method consistently outperforms all other approaches, achieving 87.86% top-40 accuracy, more than doubling the BM25 baseline performance. Table 2 presents the Mean Reciprocal Rank (MRR) results, confirming the superior ranking quality of our approach. Appendix A, C and D have various ablation studies showing how each design choice (e.g., FIM vs. non-FIM formatting, context length, etc.) impacts accuracy in our dataset. Table 1 Top-K Accuracy of Retrieval Methods (Non-FIM) Method @5 (%) @10 (%) @20 (%) @40 (%) BM25