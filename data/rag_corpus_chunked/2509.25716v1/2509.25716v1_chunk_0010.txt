these we kept 204 samples that offered enough hard negatives via the sentence-transformers mine_hard_negativeshelper; the remaining 81 lacked suitable negatives, so we discarded them. 7.2. Training Pipeline Our training approach addressed the challenge of training a small model on limited data without overfitting or catastrophic forgetting. The pipeline consisted of three main stages: 7.2.1. Supervised Fine-Tuning (SFT) We began with supervised fine-tuning using the dataset described in Section 7: • Parameter-Efficient Training:Experiments with full fine-tuning and PEFT + LoRA revealed that LoRA adapters provided the best performance improvements for the 0.6B reranker while maintaining efficiency. • Balanced Training:We ensured balanced training by randomly selecting either positive or negative samples during training, preventing class imbalance bias. Figure 3: Qwen 0.6B: Top-5 accuracy after finetuning, Mean Reciprocal Rank (MRR) and Inference latency • Loss Function:We employed negative log-likelihood loss ( nll_loss) to optimize for the true document label (1 for positive, 0 for negative). Training exclusively on synthetic samples led to rapid overfitting. The CodeR-Pile dataset alone provided better generalization and superior performance compared to mixed training approaches, indicating that combining synthetic and open-source data did not improve results. 7.2.2. SFT + RL To further improve ranking performance, we implemented a GRPO-based reinforcement learning pipeline: • Reward Function:Training uses a completion-aware binary reward that reads the first token produced in each GRPO rollout; matching the supervision label with “yes” yields +1, while an incorrect “no” response receives −1. Sampling eight completions per prompt injects the variance GRPO needs to shift the policy instead of collapsing toward the reference distribution. See Appendix G for the full setup and diagnostics. • Training Strategy:We observed that RL training on the small synthetic dataset alone led to catastrophic forgetting. The optimal approach involved applying RL to a checkpoint from the SFT model trained on the CodeR-Pile