prompt injects the variance GRPO needs to shift the policy instead of collapsing toward the reference distribution. See Appendix G for the full setup and diagnostics. • Training Strategy:We observed that RL training on the small synthetic dataset alone led to catastrophic forgetting. The optimal approach involved applying RL to a checkpoint from the SFT model trained on the CodeR-Pile dataset, then fine-tuning with our synthetic dataset. • Performance Achievement:This two-stage approach enabled the 0.6B model to achieve performance very close to the 8B reranker on our evaluation benchmark dataset. 7.2.3. Extended SFT As an alternative to RL, we experimented with extended supervised fine-tuning where we took the SFT checkpoint trained on the CodeR-Pile dataset and further fine-tuned it separately with our synthetic dataset. While this approach provided decent results, it did not show noticeable improvements over the previous SFT checkpoint. 7.3. Training Results and Validation Our comprehensive evaluation demonstrates the effectiveness of the post-training pipeline. Figure 3 shows the complete post-training results across all stages, while Table 4 presents the detailed perfor- mance comparison: The post-training pipeline successfully bridges the performance gap between the 0.6B and 8B models, with the SFT + RL optimized 0.6B model achieving 68.58% top-5 accuracy compared to 66.10% for the 8B model—outperforming it by 2.48 percentage points. Detailed out-of-distribution evaluation metrics appear in Table 7 in Appendix F. Table 4 Training Results Comparison (FIM – Hypothetical Code Generation, Dense 40) Model @5 (%) @10 (%) @15 (%) @20 (%) 8B Reranker (Baseline) 66.10 74.61 77.87 79.72 Qwen 0.6B Base 61.84 71.35 75.60 77.30 Qwen 0.6B SFT 63.26 70.21 74.04 75.89 Qwen 0.6B Extended SFT 63.69 70.92 74.89 78.01 Qwen 0.6B SFT + RL68.58 76.84 82.59 83.84 8. Conclusion We present DeepCodeSeek, a comprehensive solution for real-time API retrieval in enterprise code completion scenarios. Our