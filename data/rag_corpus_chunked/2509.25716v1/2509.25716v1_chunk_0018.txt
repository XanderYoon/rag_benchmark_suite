models, most notably linq-embed-mistral against Jina, which is a widely used embedding model for code retrieval. The study was performed without any advanced indexing or retrieval techniques to purely assess the baseline performance of the models. The results, shown in Figure 7, demonstrated that linq-embed-mistral performed significantly better than Jina. Based on these preliminary findings, we chose it for all subsequent experiments in our pipeline. Figure 7:Ablation study comparing the retrieval performance of various models. D. Code Summarization Strategy Chunking the raw Script Include (SI) code proved to be ineffective and, in some cases, degraded retrieval performance. Given the availability of large-context models, we explored alternative summarization techniques. This led to the development of automated JSDoc signature generation, which creates JSDoc signatures from SI code. Using these JSDoc signatures as a concise summary of the script’s functionality proved to be a more effective strategy, improving the relevance of our retrieval results. Figure 8:Performance comparison between JSDoc-based indexing and raw code descriptions across different retrieval methods. JSDoc indexing shows consistent improvements in retrieval accuracy. Figure 8 demonstrates the performance improvement achieved by using JSDoc documentation compared to raw code descriptions. The comparison shows that JSDoc-based indexing consistently outperforms raw code indexing across different retrieval methods, with particularly significant im- provements in top-5 and top-10 accuracy metrics. This improvement is attributed to JSDoc’s structured nature, which provides cleaner, more focused representations of API functionality while eliminating noise from implementation details. E. Training Loss Analysis E.1. Supervised Fine-Tuning (SFT) Loss Figure 9 shows the training loss progression during supervised fine-tuning. The model shows stable convergence with the loss decreasing steadily over epochs, indicating effective learning of the ranking task. Figure 9:Training loss progression during supervised fine-tuning (SFT). Figure 10 shows the epoch-wise loss during SFT training, providing a more granular view of the