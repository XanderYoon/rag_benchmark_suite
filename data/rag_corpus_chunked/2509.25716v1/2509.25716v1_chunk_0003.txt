systems build Code Knowledge Graphs from source code [ 9, 10] to enable filtering, our RAG pipeline constructs a Knowledge Graph from ServiceNow platform metadata for scope-level filtering. This constrains the search space and enables efficient retrieval of relevant Script Includes within our enterprise-specific context. 2.3. Reranking Following retrieval, a cross-encoder or long-context LLM reranker [11] can re-order top candidates, ensuring the most relevant results are prioritized for the final generation step. Recent reinforcement- learning approaches explicitly inject reasoning steps to boost reranking quality: REARANK [ 12] introduces a list-wise reasoning reranking agent, while SWE-RL [ 13] shows that RL on large-scale software-evolution data substantially improves LLM reasoning for code-centric tasks. Our reranker adopts a similar RL fine-tuning strategy but is trained on enterprise-specific Script Include pairs, enabling higher precision in our domain. 3. Proposed Method Our approach is a multi-stage retrieval pipeline designed to provide highly relevant Script Includes for code completion. The pipeline begins with setting a baseline and incorporates several techniques to progressively refine the search space and improve accuracy. The overall architecture is depicted in Figure 1. The core components of our method are as follows: • Knowledge Graph for Search Space Reduction:We leverage a Knowledge Graph (KG) constructed from platform metadata to prune the search space. This pre-filtering step significantly narrows the field of potential candidates before the main retrieval stage. Figure 1: Overview of the proposed multi-stage retrieval pipeline. • Enriched Indexing:Rather than indexing the raw code, we created a structured index. All methods belonging to a single Script Include are grouped under their parent namespace. This index is further enriched with SI code metadata and their corresponding structured JSDoc, including API usage example. This organization helps the embedding model better distinguish between different functionalities and reduces ambiguity during retrieval. • LLM-Powered Code Expansion:Developer’s