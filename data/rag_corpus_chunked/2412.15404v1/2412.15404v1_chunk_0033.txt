3 0.0367 0.0065 Yes 3 vs. 4 0.1374 <0.001 Yes 4 vs. 5 0.0315 0.0342 Yes 5 vs. 6 -0.0002 1.0000 No Faithfulness 1 vs. 2 0.0651 <0.001 Yes 2 vs. 3 0.0137 0.834 No 3 vs. 4 -0.0868 <0.001 Yes 4 vs. 5 -0.0224 0.3643 No 5 vs. 6 0.0002 1.0000 No Answer Relevance 1 vs. 2 -0.0103 0.3491 No 2 vs. 3 0.0140 0.0736 No 3 vs. 4 -0.0295 <0.001 Yes 4 vs. 5 -0.0090 0.5050 No 5 vs. 6 0.0270 <0.001 Yes The significant pairwise comparisons in Table 6 indicate consistent improvements in Con- text Relevance across each step, up until the final configuration involving Enhanced Prompt- ing. This aligns with expectations, as Enhanced Prompting primarily influences the response generation phase rather than context retrieval. The Abstract-First approach (configuration 5) produced a significant enhancement in Context Relevance, contrasting with its lack of significant effect in Experiment 3 (Table 4). This suggests that Abstract-First may facilitate more efficient retrieval processes in certain experimental conditions. Faithfulness signifi- cantly increased only with the initial addition of GROBID (configuration 2), while it showed a notable decline with the introduction of Semantic Chunking (configuration 4), possibly due to the increased length of retrieved content impacting alignment with the generated response. For Answer Relevance, the metric significantly decreased with the addition of Se- mantic Chunking but later improved with Enhanced Prompting, reflecting the latterâ€™s role in refining the generated responses to align more closely with the retrieved context. 5. Discussion The experiments conducted in this study aimed to refine the Retrieval-Augmented Gener- ation (RAG) pipeline for academic literature, focusing on enhancing the retrieval of context relevant to data science-related queries. Several configurations were evaluated across key metrics, including Context Relevance (CR), Faithfulness, and Answer Relevance, providing insight into how different components of the