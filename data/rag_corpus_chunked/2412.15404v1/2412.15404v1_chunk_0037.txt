By integrating these methods, RAG systems could better manage complex information retrieval tasks, thereby further enhancing decision-making processes across various domains. 6. Conclusion In conclusion, this study illustrates that the proposed Enhanced RAG Architecture, which integrates fine-tuning on domain-specific datasets, employs GROBID for data structuring, implements Semantic Chunking, and utilizes an Abstract-First strategy, significantly en- hances the capability of the RAG pipeline to retrieve pertinent academic content. The fine-tuning models with larger and more diverse training sets, as demonstrated in Experi- ments 3 and 6, were particularly effective in increasing Context Relevance (CR), which aligns with the studyâ€™s goal of improving retrieval of academic literature for data scientists. While Semantic Chunking and Abstract-First approaches further refined retrieval precision, the observed trade-offs in Faithfulness suggest a need for future work to optimize how retrieved content is utilized in generating responses. The Enhanced Prompting Technique contributed to more contextually aligned answers, highlighting the role of prompt design in enhancing LLM performance within RAG systems. These findings offer useful insights for advancing context retrieval systems in academic research, with further investigations needed to assess the broader applicability of these methods. Despite its contributions, this study has several limitations. One key challenge is the lack of a standardized ground truth for evaluating the quality of the generated answers. To address this, we utilized the RAGAS framework, which relies on LLMs for evaluation. While this method was the most viable given the circumstances, it introduces a degree of subjectivity into the assessment, potentially affecting the interpretation of the results. Additionally, the reliance on a custom test set, developed specifically for this study, poses another limitation. Although the questions were designed to reflect realistic challenges in data science, the lack of a widely accepted benchmark for RAG evaluation in academic literature restricts the generalizability of the