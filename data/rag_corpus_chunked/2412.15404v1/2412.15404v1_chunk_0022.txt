To assess the performance of our enhanced RAG application, we employed the RAGAS (Retrieval-Augmented Generation Assessment System) framework, which evaluates the qual- ity of generated outputs based on three key metrics: Context Relevance, Faithfulness, and Answer Relevance [7]. Each of these metrics plays a critical role in ensuring that the gener- ated responses are accurate, relevant, and grounded in the retrieved context. The RAGAS framework utilizes the OpenAI API to automatically determine metric values, such as identi- fying which sentences in the retrieved context are relevant to answering the given question or which claims in the generated answer can be inferred from the given context, based on pre- defined prompts outlined in the RAGAS paper. Since, unfortunately, there is no benchmark test set or established ground truth available for this kind of study, we had to create a custom test set and use the RAGAS framework, which relies on LLMs in the background as eval- uators. Given these limitations, this approach was the only viable option to systematically assess the performance of our proposed architecture. Context Relevance measures the relevance of the retrieved context, which is essential because the context should contain only the information needed to answer the provided query. The metric ranges from 0 to 1, with higher values indicating better relevance. To compute Context Relevance, we identified the sentences within the retrieved context that are pertinent to the query: Context Relevance = |S| Total number of sentences in retrieved context (1) Where |S| is the number of sentences in the retrieved context that are relevant to an- swering the given question. A high Context Relevance score indicates that the retrieved context is highly focused and contains minimal extraneous information. This metric is particularly important in our application because our enhancements aim to optimize the retrieval and presentation