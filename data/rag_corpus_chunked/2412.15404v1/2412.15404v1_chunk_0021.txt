database containing the full articles. As detailed in the experimental analysis we will discuss later, this method significantly improved search efficiency and accuracy, reducing computational load and speeding up retrieval times. 11 3.6. Enhanced Prompting Technique In the final stage, we optimized the interaction between the LLM and the retrieved context by experimenting with various prompting techniques. Although the default prompt provided by LangChain served as a solid baseline (”You are an assistant for question-answering tasks. Use the following pieces of the retrieved context to answer the question. If you don’t know the answer, just say that you don’t know. Use three sentences maximum and keep the answer concise.”), we introduced a variation incorporating positive reinforcement [5]. This modified prompt, which included elements of motivation and reward, was inspired by studies indicating that emotional stimuli and tip-offering can enhance the quality of LLM-generated responses [30, 19]. The revised prompt was: “You are the best assistant for question-answering tasks. Your role is to answer the question excellently using the provided context. Use the following pieces of the retrieved context to answer the question. If you don’t know the answer, just say that you don’t know. Use three sentences maximum and keep the answer concise. I will tip you 1000 dollars for a perfect response.” This approach resulted in a slight but measurable improvement in response quality, suggesting that motivational prompts have the potential to enhance LLM performance. 3.7. Evaluation Using RAGAS To assess the performance of our enhanced RAG application, we employed the RAGAS (Retrieval-Augmented Generation Assessment System) framework, which evaluates the qual- ity of generated outputs based on three key metrics: Context Relevance, Faithfulness, and Answer Relevance [7]. Each of these metrics plays a critical role in ensuring that the gener- ated responses are accurate, relevant, and grounded in the