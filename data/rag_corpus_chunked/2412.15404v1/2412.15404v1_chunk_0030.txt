when GROBID, fine-tuning, and Semantic Chunking were combined, while Enhanced Prompting improved the quality of responses. 16 Table 4: Significant pairwise comparisons for different metrics using Tukey’s HSD test for Table 3. (CR = Context Relevance, F = Faithfulness, AR = Answer Relevance) Metric Comparison Mean Difference p-value Significant? Context Relevance 1 vs. 2 0.2713 <0.001 Yes 2 vs. 3 0.0704 <0.001 Yes 3 vs. 4 0.0838 <0.001 Yes 4 vs. 5 -0.0089 0.952 No 5 vs. 6 0.0073 0.980 No Faithfulness 1 vs. 2 0.0651 <0.001 Yes 2 vs. 3 0.0691 <0.001 Yes 3 vs. 4 -0.1576 <0.001 Yes 4 vs. 5 0.0068 0.992 No 5 vs. 6 -0.0171 0.673 No Answer Relevance 1 vs. 2 -0.0103 0.454 No 2 vs. 3 0.0141 0.126 No 3 vs. 4 -0.0496 <0.001 Yes 4 vs. 5 -0.0036 0.989 No 5 vs. 6 0.0380 <0.001 Yes The significant pairwise comparisons presented in Table 4 illustrate the impact of each RAG pipeline enhancement on the key performance metrics. Until the fourth configuration, the successive improvements in Context Relevance and Faithfulness were statistically signif- icant, as evidenced by the p-values below 0.001 for each step. This indicates that adding GROBID, fine-tuning, and Semantic Chunking sequentially contributed meaningfully to the improvement in these metrics. However, the Abstract-First strategy and Enhanced Prompt- ing Technique (configurations 5 and 6) did not produce statistically significant changes in Context Relevance or Faithfulness, suggesting that these strategies may not further optimize these aspects. In contrast, Answer Relevance only showed significant differences in configu- rations 4 and 6, corresponding to the application of Semantic Chunking and the Enhanced Prompting Technique. This is consistent with the role of Semantic Chunking in retrieving more relevant context, thereby improving the relevance of the generated responses, and the Enhanced Prompting Technique’s ability to refine the