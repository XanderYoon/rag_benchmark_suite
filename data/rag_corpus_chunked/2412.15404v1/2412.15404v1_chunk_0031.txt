optimize these aspects. In contrast, Answer Relevance only showed significant differences in configu- rations 4 and 6, corresponding to the application of Semantic Chunking and the Enhanced Prompting Technique. This is consistent with the role of Semantic Chunking in retrieving more relevant context, thereby improving the relevance of the generated responses, and the Enhanced Prompting Technique’s ability to refine the model’s output, yielding more precise answers to the posed questions. 17 4.3. Performance metrics for RAG pipeline configurations using the fine-tuning model with Experiment 6 Table 5: Performance metrics for different RAG pipeline configurations using the fine-tuning model with Experiment 6. (B = Baseline, G = GROBID, FT = Fine-tuning, SC = Semantic Chunking, AF = Abstract First, EPT = Enhanced Prompting Technique) Configuration Context Relevancy Faithfulness Answer Relevance Average Word Count 1. B 0.031 0.622 0.898 772.92 2. B + G 0.302 0.687 0.888 853.88 3. B + G + FT 0.339 0.701 0.902 841.72 4. B + G + FT + SC 0.475 0.618 0.872 840.24 5. B + G + FT + SC + AF 0.507 0.592 0.863 753.16 6. B + G + FT + SC + AF + EPT 0.507 0.592 0.890 753.16 The results in Table 5 present the performance of various RAG pipeline configurations using the fine-tuned model from Experiment 6 across key metrics. The baseline (B) config- uration shows low Context Relevance (CR) at 0.031, with higher Faithfulness (0.622) and Answer Relevance (0.898), but the retrieved context remains minimally effective. Adding GROBID (B + G) improves CR to 0.302, with a slight increase in Faithfulness (0.687) and a small decrease in Answer Relevance (0.888). The average word count also rises to 853.88, reflecting broader content retrieval. Introducing fine-tuning alongside GROBID (B + G + FT) further increases CR to 0.339, with Faithfulness