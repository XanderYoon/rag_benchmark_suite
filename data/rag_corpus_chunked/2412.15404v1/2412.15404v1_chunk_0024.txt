directly addresses the user’s query. In our context, Answer Relevance is vital for assessing the overall effectiveness of the RAG application in producing useful and accurate responses. Answer Relevance = 1 N NX i=1 cos(Egi, Eo) (3) Where: • Egi is the embedding of the generated question i. • Eo is the embedding of the original question. • N is the number of generated questions Using these detailed metrics, the RAGAS framework provides a comprehensive evalua- tion of the performance of our enhanced RAG system. Each metric measures performance in different but complementary areas. Figure 3 visually illustrates the aspects that these metrics cover. The metrics not only validate the improvements we have made but also offer insights into areas where further enhancements can be pursued, ensuring that the applica- tion continues to deliver high-quality, reliable, and contextually relevant answers to data scientists. We have also incorporated the average word count into the analysis and displayed it in the results table to consider its impact on these metrics. Specifically, metrics like context relevance are highly dependent on the number of words in the retrieved contexts. When more sentences are included in a context, it becomes more challenging to achieve high scores across these metrics because, for context relevance to be maximized, all sentences should be relevant to the question. With more sentences, the likelihood of including unrelated content increases, potentially lowering the overall score. 13 Figure 3: Three Performance Metrics of RAGAS Framework and Their Corresponding Aspects 4. Results The results of our experiments, summarized in the tables, demonstrate the impact of various enhancements to the RAG pipeline on key metrics: Context Relevance (CR), Faith- fulness, and Answer Relevance. Each metric’s value is averaged over 1,500 replications -30 replications for each one of the 50 questions used in the analysis.