materials. Without fine-tuning, the baseline model incorporated with GROBID exhibited relatively low Context Relevance (CR) at 0.0302, despite relatively high Faithfulness (0.687) and Answer Relevance (0.888). This 14 highlighted the necessity of fine-tuning to improve the model’s ability to retrieve relevant content. Fine-tuning on 5 textbooks (Experiment 2) significantly improved CR to 0.337, with a slight rise in Faithfulness and a small decline in Answer Relevance. When expanded to 17 textbooks (Experiment 3), CR reached 0.372, demonstrating the advantage of a larger training dataset, with improved Faithfulness (0.756) and Answer Relevance (0.902). This model was selected for further testing due to its strong overall performance in CR. In Experiment 4, adding GROBID to fine-tune with 5 textbooks caused a drop in CR to 0.062, likely due to the removal of important content during preprocessing. However, combining GROBID with fine-tuning with 17 textbooks (Experiment 5) restored CR to 0.335, improving both Faithfulness and Answer Relevance. Further improvements were seen in Experiment 6, where the addition of Semantic Node Splitter (SNS) slightly increased CR to 0.339, maintaining competitive Faithfulness and high Answer Relevance. Overall, fine-tuning with larger datasets significantly improved CR, while GROBID and SNS added further refinements. The models from Experiments 3 (17 TB) and 6 (17 TB + G + SNS) were chosen for further evaluation, having demonstrated the best performance in retrieving relevant academic context, which is the main focus of this study. Table 2: Significant Pairwise Comparisons for Context Relevance using Tukey’s HSD Test. Comparison Mean Difference p-value Significant? 1 vs. 2 0.035 <0.001 Yes 2 vs. 3 0.035 <0.001 Yes 3 vs. 4 -0.310 <0.001 Yes 3 vs. 5 -0.037 <0.001 Yes 3 vs. 6 -0.034 <0.001 Yes 6 vs. 2 -0.002 0.999 No 6 vs. 5 -0.003 0.994 No 2 vs. 5 -0.002 0.9996 No Context