of RAGAS Framework and Their Corresponding Aspects 4. Results The results of our experiments, summarized in the tables, demonstrate the impact of various enhancements to the RAG pipeline on key metrics: Context Relevance (CR), Faith- fulness, and Answer Relevance. Each metric’s value is averaged over 1,500 replications -30 replications for each one of the 50 questions used in the analysis. The choice of 30 repli- cations per question is grounded in the Central Limit Theorem (CLT), which asserts that with a sample size of 30, the distribution of the sample means will approximate a normal distribution, ensuring the robustness of our results [23]. 4.1. Performance metrics for different fine-tuning experiments Table 1: Performance metrics for different fine-tuning experiments. (FT = Fine-tuning, TB = Textbooks, G = GROBID, SNS = Semantic Node Splitter) Experiment Context Relevancy Faithfulness Answer Relevance Average Word Count 1. No FT 0.302 0.687 0.888 853.88 2. 5 TB 0.337 0.658 0.871 853.90 3. 17 TB 0.372 0.756 0.902 836.10 4. 5 TB + G 0.062 0.198 0.790 913.00 5. 17 TB + G 0.335 0.740 0.909 839.34 6. 17 TB + G + SNS 0.339 0.701 0.902 841.72 In Table 1, all experiments, including No FT case which corresponds to B + G in Table 3), the models incorporated GROBID for preprocessing and fine-tuning to standardize data structuring. Variations in performance were solely driven by differences in the fine-tuning data, particularly the amount and type of training materials. Without fine-tuning, the baseline model incorporated with GROBID exhibited relatively low Context Relevance (CR) at 0.0302, despite relatively high Faithfulness (0.687) and Answer Relevance (0.888). This 14 highlighted the necessity of fine-tuning to improve the model’s ability to retrieve relevant content. Fine-tuning on 5 textbooks (Experiment 2) significantly improved CR to 0.337, with a slight rise in Faithfulness and