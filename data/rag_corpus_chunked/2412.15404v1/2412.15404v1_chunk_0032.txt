(0.898), but the retrieved context remains minimally effective. Adding GROBID (B + G) improves CR to 0.302, with a slight increase in Faithfulness (0.687) and a small decrease in Answer Relevance (0.888). The average word count also rises to 853.88, reflecting broader content retrieval. Introducing fine-tuning alongside GROBID (B + G + FT) further increases CR to 0.339, with Faithfulness rising to 0.701 and Answer Relevance stabilizing at 0.902. Semantic Chunk- ing (B + G + FT + SC) brings the highest CR yet (0.485), though Faithfulness drops slightly to 0.640, with Answer Relevance decreasing to 0.874. The Abstract-First strategy (B + G + FT + SC + AF) improves CR to 0.507, though both Faithfulness (0.592) and Answer Relevance (0.863) decrease, possibly due to the narrower focus on abstract-level content. Finally, integrating the Enhanced Prompting Technique (B + G + FT + SC + AF + EPT) maintains CR at 0.507 while increasing Answer Relevance to 0.890. This suggests that enhanced prompts help the model generate more coherent responses, even as Faithfulness remains stable at 0.592. Overall, the best performance in CR was achieved with Semantic Chunking and Abstract-First strategies, while enhanced prompting improved answer quality. 18 Table 6: Significant pairwise comparisons for different metrics using Tukeyâ€™s HSD test for Table 5. (CR = Context Relevance, F = Faithfulness, AR = Answer Relevance) Metric Comparison Mean Difference p-value Significant? Context Relevance 1 vs. 2 0.2713 <0.001 Yes 2 vs. 3 0.0367 0.0065 Yes 3 vs. 4 0.1374 <0.001 Yes 4 vs. 5 0.0315 0.0342 Yes 5 vs. 6 -0.0002 1.0000 No Faithfulness 1 vs. 2 0.0651 <0.001 Yes 2 vs. 3 0.0137 0.834 No 3 vs. 4 -0.0868 <0.001 Yes 4 vs. 5 -0.0224 0.3643 No 5 vs. 6 0.0002 1.0000 No Answer Relevance 1 vs. 2 -0.0103 0.3491