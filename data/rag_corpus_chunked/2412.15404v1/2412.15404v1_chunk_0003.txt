application responds to a significant need within the data science community for accessible and relevant academic guidance. Unlike traditional literature review meth- ods, which can be time-consuming and overwhelming due to the vast amount of available information, our application facilitates the process by extracting and embedding only the most pertinent sections of articles into the LLM. This approach accelerates decision-making and ensures that the information provided is both current and directly relevant to the user’s query, thereby addressing the prevalent issue of information overload in data science research. The primary innovation of our RAG application lies in itsfive-stage enhancement process, which substantially improves the traditional RAG pipeline. Initially, we utilize GROBID (GeneRation Of BIbliographic Data) to clean and structure article data before constructing the vector database, ensuring the accuracy and organization of the information [22]. Next, we implement a specialized fine-tuning process using academic textbooks, which enhances the embedding model’s capacity to interpret and process complex queries. Furthermore, our application employs semantic chunking rather than traditional recursive chunking which en- 2 sures that text is divided into meaningful units and results in a more coherent and relevant vector database [14]. We also adopt an abstract-first method, prioritizing searches within a separate vector database of article abstracts, thereby streamlining the retrieval process. Fi- nally, advanced prompting techniques are employed to optimize the LLM’s responses, leading to more accurate and contextually appropriate answers. Llamaindex and LangChain were instrumental in creating fine-tuning data and RAG pipelines, respectively [21, 5]. Addition- ally, OpenAI’s GPT-4o model was employed as the LLM throughout the study, chosen for its cost-effectiveness and efficiency. To validate the effectiveness of our application, we employed the RAGAS (Retrieval Augmented Generation for Academic Search) framework and conducted tests with 50 sample questions covering various domains and problem types [7]. This rigorous testing