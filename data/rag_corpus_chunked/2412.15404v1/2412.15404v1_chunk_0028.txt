<0.001) enhancement in Context Relevance from the baseline Experiment 1 (No 15 FT) to Experiment 3 (17 TB). Evidently, Experiment 3 exhibits a statistically significant superiority over all other experimental conditions concerning Context Relevance. While the results for Experiment 6 (17 TB + G + SNS), Experiment 2 (5 TB), and Experiment 5 (17 TB + G) are closely aligned, exhibiting no statistically significant differences among them as detailed in the last three rows of Table 2, we have chosen to further evaluate Experiment 6 alongside Experiment 3 due to its position as the second highest in overall average Context Relevance. 4.2. Performance metrics for RAG pipeline configurations using the fine-tuning model with Experiment 3 Table 3: Performance metrics for different RAG pipeline configurations using the fine-tuning model with Experiment 3. (B = Baseline, G = GROBID, FT = Fine-tuning, SC = Semantic Chunking, AF = Abstract First, EPT = Enhanced Prompting Technique) Configuration Context Relevancy Faithfulness Answer Relevance Average Word Count 1. B 0.031 0.622 0.898 772.92 2. B + G 0.302 0.687 0.888 853.88 3. B + G + FT 0.372 0.756 0.902 836.10 4. B + G + FT + SC 0.456 0.599 0.852 947.94 5. B + G + FT + SC + AF 0.447 0.606 0.848 915.80 6. B + G + FT + SC + AF + EPT 0.454 0.588 0.887 917.86 The performance metrics for different RAG pipeline configurations using the fine-tuned model from Experiment 3, as shown in Table 3, illustrate how various enhancements affect the retrieval process. Adding GROBID (B + G) significantly improved Context Relevance (CR) from 0.031 to 0.302, demonstrating its effectiveness in structuring the data. Faithful- ness increased to 0.687, while Answer Relevance slightly declined to 0.888, indicating some variability in the use of the retrieved context. The