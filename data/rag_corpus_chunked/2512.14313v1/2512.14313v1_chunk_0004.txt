number of passages to retrieve based on similarity score distribution. Ìn addition to retrieval size, the positional placement of retrieved content has also been shown to influence generation per- formance. Liu et al. [8] demonstrated that large language models often exhibit a position bias, prioritizing information at the beginning and end of the context, and neglecting passages located in the middle. In contrast to prior methods, our approach employs a classifier that directly predicts the precise number of doc- uments required for each query. While methods such as Adaptive-RAG rely on classifiers to select among predefined retrieval strategies, our approach explicitly estimates the exact number of documents required for each query. 3 Preliminary Analysis 3.1 Dataset Overview We conduct our experiments using the MuSiQue-Ans dataset, a benchmark de- signed to evaluate complex multi-hop reasoning in open-domain QA systems. The dataset is comprised of queries created through the composition of two to four single-hop queries, combined into a single coherent multi-hop question that requires multiple reasoning steps. The dataset contains a collection of≈22k queries, (≈20k in the training set, and≈2.5k in the dev set), wherein a query can be categorized as either 2-hop, 3-hop, or 4-hop. These ‘hops’ represent the number of reasoning steps required to answer the query. Each multi-hop ques- tion is paired with a fixed number of ‘gold’ supporting passages, wherin 2-hop questions are provided with two gold documents, 3-hop with three, and 4-hop 4 M. Iratni et al. with four. Table 1 depicts the number of queries belonging to each hop type in both the train and dev set. Table 1: Number of queries by type 2-hop 3-hop 4-hop Train 14376 4387 1175 Dev 1252 760 405 3.2 Exploratory Analysis We conducted a series of experiments with the goal of evaluating the impact of distractors, and context