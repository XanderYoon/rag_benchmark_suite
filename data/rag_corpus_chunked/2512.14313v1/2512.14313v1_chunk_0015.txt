LLM to perform additional reranking. Retrieval is first performed with a fixedk= 5and gen- 12 M. Iratni et al. eration follows as in standard fixed-kpipeline. In the classifier+LLM pipeline, the LLM receives the classifer-k value, the five retrieved contexts, and the query. The LLM is prompted to rank the contexts by relevance, and select the top classifier-kcontexts. The selected contexts, and the query are then provided to the generator. We also evaluated a control pipeline without the classifier, in which the LLM receives the five contexts, and tasked to determine the opti- malkitself, and to select the most relevant documents for generator. Table 3 reports the results of the Control Pipeline and compares them with the Base- line and the Classifier-LLM pipelines accross each retrieval configuration. The results show that the Classifier-LLM configuration outperformed the baseline with every retrieval method. In addition, we applied the concept of context po- sitioning introduced in Section III. Passages judged most relevant by the LLM were placed at the end of the context fed to the generator. As shown in the table in the Classifier-LLM (Structured) part, this modification further improved per- formance. To evaluate the significance of the performance difference between our model and the baseline, we conducted a paired t-test between the Baseline and Classifier-LLM pipeline in the BGE retrieval configuration, which yielded the best results. The test yielded a p-value <0.01 on all three datasets, indicating that the improvement achieved by our model is statistically significant. Comparative AnalysisWe compare the performance of our model against the Adaptive-Rag [5] results obtained in their paper, which addresses a related task using a different retrieval method. Results against our Classifier-LLM pipeline (BGE retriever) were comparable on the MuSiQue dataset (with 23.6 against our 20.2 using EM), while our model outperformed on the 2WikiMultihopQA dataset