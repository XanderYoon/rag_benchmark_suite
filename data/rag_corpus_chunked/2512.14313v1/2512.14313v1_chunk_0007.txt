position biases in LLMs, which tend to prioritize information appearing at the extremities of the input. The end-position advantage suggests models pri- oritize more recent content. These findings highlight the importance of context placement in RAG systems. When input space is limited or distractors exist, placing relevant content near the end improves generation. Table 2: Relevant context position Beginning Middle End Exact Match 0.5447 0.5391 0.5567 F1 score 0.6212 0.6126 0.6361 4 Proposed Approach Based on our preliminary analysis, we propose an enhanced RAG system de- signed to improve generation quality for multi-hop QA by reducing the impact of distractors. This section presents the architecture and components of our sys- tem which extends the standard RAG pipeline with two extensions designed to reduce noise from irrelevant retrieved passages. Figure 2 illustrates the overall system architecture, contrasting the baseline RAG pipeline with our proposed extensions: including the dynamic-k classifier and the LLM-based reranker. 6 M. Iratni et al. Fig.2: Diagram of Baseline Pipeline, Classifier-k Pipeline, and Classifier-k+LLM pipeline 4.1 Baseline Pipeline Our starting point is a standard RAG setup consisting of two main modules: – Retriever:Given an input queryq, the retriever searches through a corpus and returns the top-kpassages most relevant toq. – Generator:The retrieved passages are concatenated withqand fed into a generative model, which produces the final answer. Given a queryq, the retriever fetcheskpassagesP k ={p 1, p2, . . . , pk}, and the generator produces output:y=Generator(q, P k) This architecture provides strong end-to-end performance, but uses a fixedk for all queries, which can lead to over or under retrieval depending on query complexity. 4.2 Classifier-k Pipeline The first major modification to this pipeline is the integration of a query-specific kpredictor. We introduce a classifier to estimate the optimal number of context (k) that should be retrieved for each