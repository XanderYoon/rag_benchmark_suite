our model is statistically significant. Comparative AnalysisWe compare the performance of our model against the Adaptive-Rag [5] results obtained in their paper, which addresses a related task using a different retrieval method. Results against our Classifier-LLM pipeline (BGE retriever) were comparable on the MuSiQue dataset (with 23.6 against our 20.2 using EM), while our model outperformed on the 2WikiMultihopQA dataset (with 40.6 against our 53.1 using EM). We did not compare our model di- rectly against DynamicRAG [10] as their evaluation was conducted on a different set of datasets, and their approach requires computationally intensive training procedures that are not directly comparable to our setting. 7 Conclusion In this work, we examined two key limitations of standard RAG systems: the adverse impact of distractor passages and the positional bias introduced by the “lost in the middle” effect. Using the MuSiQue-Ans dataset, we showed that both distractors and suboptimal passage placement substantially reduce generation quality, particularly for multi-hop queries. To adress these issues, we introduced a dynamic context selection framework that combines a query-specific classifier with an LLM-based reranking to dynamically determine the optimal number of documents to retrieve. Empirical results indicate that while classifier-only retrieval can lower recall, integrating classifier predictions with LLM reranking yields significant improvements in exact match and F1. Moreover, positioning the most relevant passages at the end of the context sequence yields additional gains, reinforcing the importance of input structure in RAG pipelines. These findings highlight the need for more dynamic context selection methods. Disclosure of Interests.The authors have no competing interests to declare that are relevant to the content of this article. 7. CONCLUSION 13 References 1. C. Amiraz, F. Cuconasu, S. Filice, and Z. Karnin. The distracting effect: Under- standing irrelevant passages in rag.arXiv preprint arXiv:2505.06914, 2025. 2. Y. Gao, Y. Xiong, X. Gao,