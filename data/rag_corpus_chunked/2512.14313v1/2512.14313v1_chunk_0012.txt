classifier is ideal. The result is shown in the Ideal Classifier row, using the dense retrieval method, compared the the Baseline. The retrieval results (not listed in the paper) show that while precision scores were 10 M. Iratni et al. Table 3: Evaluation Results Retrieval Pipeline MuSiQue MultihopRAG 2WikiQA EM F1 EM F1 EM F1 BM25 Baseline 0.060 0.105 0.499 0.550 - - Classifier 0.046 0.086 0.501 0.549 - - Classifier+LLM0.063 0.107 0.530 0.585- - Dense Baseline 0.141 0.221 0.567 0.612 - - Classifier 0.124 0.195 0.594 0.642 - - Classifier+LLM0.153 0.232 0.597 0.648- - MonoT5 Baseline 0.171 0.252 0.594 0.626 - - Classifier 0.159 0.239 0.621 0.653 - - Classifier+LLM0.195 0.277 0.625 0.659- - BGE Baseline 0.177 0.260 0.601 0.645 0.488 0.576 Classifier 0.162 0.243 0.619 0.655 0.486 0.572 Classifier+LLM 0.199* 0.283*0.625* 0.666* 0.529* 0.625* Classifier+LLM (Structured)1 0.202*0.291*0.625*0.672*0.531*0.629* Control Pipeline4 0.171 0.212 - - - - Oracle StudyIdeal Classifier2 0.128 0.200 - - - - Ideal Reranker3 0.216 0.303 - - - - 1 The input was reordered so that the most relevant texts are at the end of the input. 2 Comparison of the Baseline pipeline(k=5) and the Classifier pipeline with the op- timal k under dense retrieval. 3 Using the BGE retrieval method. 4 In the control pipeline, the reranker LLM was given the candidate texts and tasked with selecting the optimal number independently of the classifier. âˆ— These measurements were compared against the results obtained using the BGE retriever and Baseline pipeline configuration using a paired t-test. The results show a p-value of <0.01 for all three datasets. increased with the classifier-k retrieval, the recall significantly decreases. This means that while the approach may be reducing the number of distractor docu- ments, it cuts-off relevant contexts, which decreases generation performance. If a relevant context is found