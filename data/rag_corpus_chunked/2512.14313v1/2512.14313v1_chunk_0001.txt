during in- ference. In a standard RAG system, the user’s query is first encoded, and used to retrieve a ranked list of documents from a large collection [2] [7]. The top-K pas- sages,wherekisafixed,predeterminedvalue,arethenprovidedtothegenerative model as additional context, allowing the model to generate a response based on both the query and the retrieved information [7] [3]. This top-K approach has become widely adopted because it is straightforward to implement, and gen- erally provides adequate context for many tasks such as open-domain question arXiv:2512.14313v1 [cs.IR] 16 Dec 2025 2 M. Iratni et al. answering [2]. RAG systems often suffer from two critical challenges: (i) mislead- ing irrelevant retrieved documents (distractors), that can distort the generation results [1], and (ii) the ’lost-in-the-middle’ phenomenon. [8]. A distractor is a retrieved text that appears similar to the query, but is actually semantically irrelevant. Such documents can confuse the generator and reduce output qual- ity by introducing noise into the context, which can mislead the generator and weaken attention over relevant passages [1] [6]. Due to this, using a fixed number of retrieved documents for all queries has significant limitations. A fixed, top-k strategy risks excluding important information whenkis too small or including excessiveirrelevantcontentwhenkistoolarge,makingitsuboptimalforcomplex tasks, such as multi-hop QA [9]. This balancing problem is directly related to the precision–recall trade-off in retrieval. Increasingkgenerally improves recall, so fewer relevant passages are missed. However, it simultaneously deceases the precision by including more irrelevant context, diluting relevant documents and degrading generation quality [6]. An additional challenge in retrieval-augmented generation lies in the positional effect of the retrieved passages within the input sequence. Prior studies have shown that Large Language Models (LLM) tend to prioritize information that appears at the beginning or end of the input se- quence, attributing less attention to information placed in the middle. This leads to