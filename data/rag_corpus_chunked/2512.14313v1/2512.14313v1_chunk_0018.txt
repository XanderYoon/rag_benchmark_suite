How language models use long contexts.arXiv preprint arXiv:2307.03172, 2023. 9. W. Su, Y. Tang, Q. Ai, Z. Wu, and Y. Liu. Dragin: Dynamic retrieval augmented generation based on the information needs of large language models.arXiv preprint arXiv:2403.10081, 2024. 10. J. Sun, X. Zhong, S. Zhou, and J. Han. Dynamicrag: Leveraging outputs of large language model as feedback for dynamic reranking in retrieval-augmented genera- tion.arXiv preprint arXiv:2505.07233, 2025. 11. C. Taguchi, S. Maekawa, and N. Bhutani. Efficient context selection for long-context qa: No tuning, no iteration, just adaptive-k.arXiv preprint arXiv:2506.08479, 2025. 12. Y.TangandY.Yang. Multihop-rag:Benchmarkingretrieval-augmentedgeneration for multi-hop queries.arXiv preprint arXiv:2401.15391, 2024. 13. H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal. Musique: Multihop questions via single-hop question composition.Transactions of the Association for Computational Linguistics, 10:539â€“554, 2022.