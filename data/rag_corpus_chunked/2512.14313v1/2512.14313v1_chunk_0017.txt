the need for more dynamic context selection methods. Disclosure of Interests.The authors have no competing interests to declare that are relevant to the content of this article. 7. CONCLUSION 13 References 1. C. Amiraz, F. Cuconasu, S. Filice, and Z. Karnin. The distracting effect: Under- standing irrelevant passages in rag.arXiv preprint arXiv:2505.06914, 2025. 2. Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, H. Wang, and H. Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2(1), 2023. 3. K.Guu,K.Lee,Z.Tung,P.Pasupat,andM.Chang. Retrievalaugmentedlanguage model pre-training. InInternational conference on machine learning, pages 3929– 3938. PMLR, 2020. 4. X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa. Constructing a multi- hop qa dataset for comprehensive evaluation of reasoning steps.arXiv preprint arXiv:2011.01060, 2020. 5. S. Jeong, J. Baek, S. Cho, S. J. Hwang, and J. C. Park. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. arXiv preprint arXiv:2403.14403, 2024. 6. B. Jin, J. Yoon, J. Han, and S. O. Arik. Long-context llms meet rag: Overcoming challenges for long inputs in rag.arXiv preprint arXiv:2410.05983, 2024. 7. P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances in neural information processing systems, 33:9459–9474, 2020. 8. N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in the middle: How language models use long contexts.arXiv preprint arXiv:2307.03172, 2023. 9. W. Su, Y. Tang, Q. Ai, Z. Wu, and Y. Liu. Dragin: Dynamic retrieval augmented generation based on the information needs of large language models.arXiv preprint arXiv:2403.10081, 2024. 10. J. Sun, X. Zhong, S. Zhou, and J. Han. Dynamicrag: Leveraging outputs of large language model as feedback for dynamic reranking