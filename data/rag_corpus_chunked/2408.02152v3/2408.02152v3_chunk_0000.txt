Generative Retrieval with Few-shot Indexing Arian Askari1†[0000−0003−4712−832X], Chuan Meng2†[0000−0002−1434−7596], Mohammad Aliannejadi3[0000−0002−9447−4172], Zhaochun Ren1[0000−0002−9076−6565], Evangelos Kanoulas3[0000−0002−8312−0694], and Suzan Verberne1[0000−0002−9609−9505] 1 Leiden University, Leiden, The Netherlands {a.askari, z.ren, s.verberne}@liacs.leidenuniv.nl 2 The University of Edinburgh, Edinburgh, United Kingdom chuan.meng@ed.ac.uk 3 University of Amsterdam, Amsterdam, The Netherlands {m.aliannejadi, e.kanoulas}@uva.nl Abstract.Existing generative retrieval (GR) methods rely ontraining- based indexing, which fine-tunes a model to memorise associations be- tween queries and the document identifiers (docids) of relevant docu- ments. Training-based indexing suffers from high training costs, under- utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose afew-shotindexing-basedGRframework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ulti- mately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing withone-to-many mappingto further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training. Keywords:Generative retrieval·Neural ranking·Few-shot learning 1 Introduction Generative retrieval (GR) [5,3,36,35,11,16,15] is a new paradigm in information retrieval (IR). Unlike traditional IR that decouples indexing and retrieval, GR unifies both processes into a single model [31]. Studies in GR typically regard indexing and retrieval as training and inference processes, respectively. The in- dexing (training) process typically trains a seq2seq model [28] to map queries to the docids corresponding to relevant documents, using extensive training data of query–docid pairs [37]. In the retrieval (inference) process, the trained model takes a query text as input and directly generates potentially relevant docids. Limitations. Existing