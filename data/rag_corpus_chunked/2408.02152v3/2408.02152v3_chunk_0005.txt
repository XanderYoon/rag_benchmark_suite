[2] to generate pseudo queries. Methods marked† are our reimplementa- tions; all other results are from the corresponding papers [34,30,29,34]. The best value in each column is marked inbold, and the second best is underlined. Method NQ320K MS300K Recall@1 Recall@10 MRR@100 Recall@1 Recall@10 MRR@10 BM25 29.7 60.3 40.2 39.1 69.1 48.6 DocT5Query 38.0 69.3 48.9 46.7 76.5 56.2 ANCE 50.2 78.5 60.2 45.6 75.7 55.6 SentenceT5 53.6 83.0 64.1 41.8 75.4 52.8 GTR-base 56.0 84.4 66.2 – – – SEAL 59.9 81.2 67.7 25.9 68.6 40.2 DSI 55.2 67.4 59.6 32.4 69.9 44.3 NCI 66.4 85.7 73.6 30.1 64.3 41.7 DSI-QG† 63.1 80.7 69.5 41.0 71.2 50.7 DSI-QG (InPars)† 63.9 82.0 71.4 41.3 71.5 50.0 TOME 66.6 – – – – – GLEN 69.1 86.0 75.4 – – – GenRET 68.1 88.8 75.9 47.9 79.8 58.1 NOVO 69.3 89.776.7 49.1 80.8 59.2 Few-Shot GR70.187.677.4 49.6 81.259.1 3 Experimental setup Datasets. We evaluate on NQ320K [13,30,31] and MS300K [34,18]; both have widely been used for GR evaluation. NQ320K is a version of Natural Questions (NQ) [12]; NQ320K consists of 320k relevant query–document pairs, 100k doc- uments, and 7,830 test queries. MS300K is a version of MS MARCO; MS300K contains 300k query–document pairs, 320k documents, and 5,187 test queries. Baselines. We use non-GR and GR baselines. Following [13], we use the follow- ing non-GR baselines: BM25, DPR [9], SentenceT5 [25], and GTR-base [26]. We use the following GR baselines (training-based indexing): (i) SEAL [1] learns to generate n-grams-based docids and applies FM-index [7]. (ii) DSI [31] learns to generate numeric identifiers. (iii) DSI-QG [37] augments DSI training by using pseudo queries; we replicate DSI-QG using the pseudo query generator provided bytheoriginalpaper.(iv)DSI-QG(InPars)usesthepseudoquerygeneratorfrom InPars [2]. (v) TOME [29] learns to generate document URLs. (vi) GLEN [13] learns dynamic lexical docids. (vii) GenRET [30] learns