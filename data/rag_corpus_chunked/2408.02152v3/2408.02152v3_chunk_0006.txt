SEAL [1] learns to generate n-grams-based docids and applies FM-index [7]. (ii) DSI [31] learns to generate numeric identifiers. (iii) DSI-QG [37] augments DSI training by using pseudo queries; we replicate DSI-QG using the pseudo query generator provided bytheoriginalpaper.(iv)DSI-QG(InPars)usesthepseudoquerygeneratorfrom InPars [2]. (v) TOME [29] learns to generate document URLs. (vi) GLEN [13] learns dynamic lexical docids. (vii) GenRET [30] learns to assign numeric docids based on an auto-encoding scheme. (viii) NOVO [34] learns interpretable docids. Evaluation metrics. In line with recent GR work [34,13,30], we report Re- call@1,10onbothdatasets,plusMRR@100(NQ320K)andMRR@10(MS300K). Implementation details. We equip Few-Shot GR with llama-3-8B-Instruct for indexing and retrieval.We generate 10 docids per document during few-shot indexing. We set the maximum and minimum lengths for docid generation to 15 Generative Retrieval with Few-shot Indexing 5 # generated docids per document Recall@10 55.0 60.0 65.0 70.0 75.0 80.0 85.0 90.0 2 4 6 8 10 12 14 llama-3-8B-Instruct Zephyr-7B-β Fig.2: Few-Shot GR’s retrieval quality w.r.t. # generated docids per document in few-shot indexing on NQ320K. Table 2: Retrieval quality of Few-Shot GR with different LLMs on NQ320K. MethodRecall@1 Recall@10 MRR@100 T5-base 52.4 66.4 55.8 Zephyr-7B-β69.9 87.277.8 llama-3-8B-Instruct70.1 87.677.4 and 3 tokens, respectively. We employ the query generator from InPars [2] for generating pseudo queries in Equation 1. We conduct parameter tuning on the training set of NQ320K or MS300K. 4 Result and analysis Comparison with baselines. Table 1 shows the retrieval quality of Few-Shot GR and all baselines on NQ320K and MS300K. The leading observation is that Few-Shot GR outperforms all baselines across all metrics, except Gen- RET/NOVO on Recall@10 (NQ320K)/MRR@10 (MS300K). This shows that our proposed few-shot indexing is highly effective versus training-based indexing. Notably, while GenRET/NOVO is slightly better on those metrics, it requires large training corpora and heavy model-specific training, which may not be fea- sible in