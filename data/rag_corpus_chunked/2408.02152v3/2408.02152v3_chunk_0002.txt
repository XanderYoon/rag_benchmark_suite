prompts an LLM in a few-shot way to generate a free-text docid for each document in a corpus. This process ultimately produces adocid bankfor all documents in an entire corpus. During the retrieval process (inference), the same LLM used in few-shot indexing takes a query as input and uses constrained beam search [6] to ensure the generated docid matches a valid docid created during few-shot indexing. However, the implementation of Few-Shot GR brings one new challenge: We found that generating only one docid per document during few-shot indexing re- sults in limited retrieval quality. This occurs because a document can be relevant to multiple diverse queries; during retrieval, when the LLM is fed with different queries that share the same relevant document, it is hard for the LLM to always point to one docid. We therefore further improve Few-Shot GR to address the challenge. Unlike most GR studies that generate a single docid per document, we devise few-shot indexing withone-to-many mapping, which enhances few- shot indexing by, for each document, generating multiple docids. This approach allows a relevant document to be mapped back by multiple various docids that are generated in response to different queries during retrieval. Experiments. We equip Few-Shot GR with LLMs for few-shot indexing and retrieval. Experiments on Natural Questions (NQ) [12] and MS MARCO show that Few-Shot GR outperforms or performs comparably to state-of-the-art GR methods [13,30]. Moreover, our analyses reveal that two critical factors con- tribute to the success of Few-Shot GR: conducting one-to-many mapping during few-shot indexing, and selecting an effective LLM. Finally, we demonstrate that few-shot indexing is significantly more efficient than training-based indexing. Our main contributions are as follows: –We propose Few-Shot GR, a novel GR framework, which conducts GR index- ing solely with prompting an LLM without requiring any training. –We