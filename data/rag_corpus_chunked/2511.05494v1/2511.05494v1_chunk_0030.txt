in: Australasian Joint Conference on Artificial Intelligence, Springer, 2023, pp. 403–415. [32] Z. Zhao, W. Fan, J. Li, Y . Liu, X. Mei, Y . Wang, Z. Wen, F. Wang, X. Zhao, J. Tang, et al., Recommender systems in the era of large language models (llms), arXiv preprint arXiv:2307.02046 (2023). [33] J. Lin, X. Dai, Y . Xi, W. Liu, B. Chen, H. Zhang, Y . Liu, C. Wu, X. Li, C. Zhu, et al., How can recommender systems benefit from large language models: A survey, arXiv preprint arXiv:2306.05817 (2023). [34] P. Liu, L. Zhang, J. A. Gulla, Pre-train, prompt, and recommendation: A comprehensive survey of language modeling paradigm adaptations in recommender systems, TACL 11 (2023) 1553–1571. [35] L. Wu, Z. Zheng, Z. Qiu, H. Wang, H. Gu, T. Shen, C. Qin, C. Zhu, H. Zhu, Q. Liu, et al., A survey on large language models for recom- mendation, World Wide Web 27 (5) (2024) 60. [36] D. Shu, T. Chen, M. Jin, C. Zhang, M. Du, Y . Zhang, Knowledge graph large language model (kg-llm) for link prediction, arXiv preprint arXiv:2403.07311 (2024). [37] S. Geng, S. Liu, Z. Fu, Y . Ge, Y . Zhang, Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5), in: ACM RecSys, 2022, pp. 299–315. [38] Z. Cui, J. Ma, C. Zhou, J. Zhou, H. Yang, M6-rec: Generative pretrained language models are open-ended recommender systems, arXiv preprint arXiv:2205.08084 (2022). [39] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, J. Zhang, Chat- rec: Towards interactive and explainable llms-augmented recommender system, arXiv preprint arXiv:2303.14524 (2023). [40] J. Zhang, R. Xie, Y . Hou, X. Zhao, L. Lin, J.-R. Wen, Recommendation as instruction following: A large language model empowered recom- mendation approach, ACM TOIS (2023). [41] K.