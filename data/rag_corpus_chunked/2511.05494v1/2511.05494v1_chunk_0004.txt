candidate relevance. Finally, during generation, the LLM synthesizes personalized recommendations solely from these augmented and meticulously filtered candidates, ensuring that the unlearning process has minimal negative impact on other related users, thereby achieving precise, debiased recom- mendation unlearning. We summarize the main contributions of this paper as follows: • To the best of our knowledge, CRAGRU is the first frame- work to unify retrieval-augmented LLMs with traditional recommenders for unlearning. By treating each user’s recommendations as an atomic unit, it achieves minimal impact on non-target users and efficient unlearning with- out retraining or parameter updates. • We design three novel interaction retrieval mechanisms to balance unlearning efficacy and recommendation quality. Specifically, Preference-aware retrieval (the most rep- resentative interactions); Diversity-constrained retrieval (item coverage); and Attention-guided retrieval (identify the important user interactions ). • Extensive validation across three datasets and two back- bone models (LightGCN, BPR). CRAGRU reduces the average unlearning time by 4.5× versus SOTA baselines, while retaining approximately 90% of the recommenda- tion model’s performance before unlearning. II. RELATED WORKS A. Machine Unlearning Machine unlearning is designed to remove the impact of a specific subset of training data from a trained model [18]. A direct approach is to update the dataset and retrain; however, this will incur a significant computational overhead. Initial research focused on traditional machine learning tasks [19], [20]. For example, efficient data deletion of K-means cluster- ing [21], incremental and decremental learning algorithms for linear support vector machines [22], [23], and fast Bayes data deletion based on statistical query learning [24]. However, due to the limited application scenarios and lack of generalization of these methods, it is difficult to apply them to non-convex models such as deep neural networks with huge parameter spaces. To improve the generalization of unlearning, Bour- toule et al. proposed SISA [25], a