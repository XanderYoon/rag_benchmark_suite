BPR LightGCN BPR LightGCN Retrain 248s 141s 1935s 4209s 527s 1019s SISA 27s 28s 298s 435s 116s 131s GraphEraser 26s 17s 302s 269s 310s 640s RecEraser 29s 35s 386s 275s 404s 880s SCIF 18s 18s 66s 64s 299s 177s IFRU 55s 57s 78s 90s 104s 117s CRAGRU 14s 14s 15s 15s 16s 17s Improve 1.8x 1.2x 4.4x 4.3x 7.3x 6.9x compared to Retrain, the partition-aggregation framework (i.e., SISA, GraphEraser, and RecEraser) significantly improved the unlearning efficiency. Moreover, since SISA uses a very simple partition and aggregation strategy, it is faster than the other two partition aggregation frameworks. However, this simplicity also limits the performance of SISA in terms of model utility (see Table II). Our method demonstrates a significant advan- tage in unlearning time efficiency. Compared to the second- best values of these methods, it achieved average speedups of 1.5x, 4.4x, and 7.1x in the three datasets, respectively. This is because we leverage LLMs to shift the recommendation task from the dataset level to the user level. This atomic user- level recommendation enables more flexible control over each userâ€™s recommendations and unlearning, thereby enhancing unlearning efficiency. As the number of unlearning interactions increases, the time consumption of our method grows linearly, while partition-aggregation methods experience exponential growth. According to Liu et al. [17], when the dataset is split into 10 partitions and 100 interactions are randomly selected for unlearning, the probability that all sub-models require retraining is close to 100%. In real-world scenarios, user unlearning requests typically come gradually, so the user- level unlearning method can quickly fulfill these needs in a timely manner. D. Unlearning Completeness (RQ3) To assess the effectiveness of CRAGRU in erasing user- specific influence, we compare recommendation performance on the forgotten set versus the remaining set, following simu- lated unlearning of 10% user-item interactions on