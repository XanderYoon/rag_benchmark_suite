loss. However, these methods often suffer from unlearning bias due to the entanglement of forgotten users with co-located users in training shards. Approximate methods (SCIF, IFRU) reduce computation by estimating user influence via gradients or sim- ilarity propagation, but can cause latent drift in similar users’ embeddings, leading to degraded recommendation quality. CRAGRU outperforms almost all other unlearning baselines across datasets and backbones while approaching retraining performance, indicating strong bias mitigation. For instance, on the ML-1M dataset with LightGCN, CRAGRU improves HR@10 by 9.64% and NDCG@10 by 12.3% over RecEraser. These gains reflect CRAGRU’s ability to precisely isolate and remove target user influence at the retrieval stage, thus minimizing adverse impacts on behaviorally similar users. Performance improvements are statistically significant ( p < 0.01) across all metrics and datasets. This experiment confirms that CRAGRU achieves near-retraining performance while avoiding the performance degradation and bias propagation common in both exact and approximate unlearning baselines. C. Unlearning Efficiency (RQ2) Our method can achieve unlearning in the LLM inference phase, which is equivalent to the training process of other methods. Therefore, when evaluating the unlearning efficiency TABLE II COMPARISON OF DIFFERENT UNLEARNING METHODS IN TERMS OF MODEL UTILITY , THE BEST RESULTS ARE HIGHLIGHTED IN BOLD . W E COMPUTED THE PAIRED T -TEST P -VALUES FOR CRAGRU AND RECERASER ON THREE DATASETS , AND ALL P -VALUES WERE LESS THAN 0.01. ML-100K BPR LightGCN HR@5 NDCG@5 HR@10 NDCG@10 HR@20 NDCG@20 HR@5 NDCG@5 HR@10 NDCG@10 HR@20 NDCG@20 Retrain 0.6643 0.2914 0.7845 0.2910 0.8728 0.3091 0.6455 0.2796 0.7739 0.2790 0.8634 0.2963 SISA 0.2968 0.0908 0.4240 0.0949 0.5607 0.1085 0.2733 0.0818 0.3899 0.0849 0.5183 0.0925 GraphEraser 0.3239 0.1011 0.4488 0.1093 0.6148 0.1282 0.3805 0.1156 0.5336 0.1257 0.6808 0.1460 RecEraser 0.3204 0.1011 0.4582 0.1100 0.6337 0.1274 0.4806 0.1557 0.6137 0.1668 0.7491 0.1873 SCIF 0.2524 0.0720 0.4093