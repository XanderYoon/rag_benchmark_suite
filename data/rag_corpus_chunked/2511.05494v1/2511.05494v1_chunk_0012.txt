is calculated as: pu,c = |Du,c| |Du| , where | · | denotes the cardinality of a set. Given a target number of interactions to retain K (e.g., K = 100 ), the number of interactions Kc to retain for each category c is: Kc = ⌊pu,c × K⌋, where ⌊·⌋ is the floor function. Finally, we randomly sample Kc interactions from each subset Du,c to form the filtered interaction set Dfiltered u : Dfiltered u = {x | x ∈ Sample(Du,c, Kc), c ∈ C} , (3) where Sample(S, k) is a function that randomly samples k elements from set S. This preference-based filtering strategy effectively retains the most relevant historical interactions, minimizing the impact on recommendation performance dur- ing unlearning. It considers both the overall interaction behav- ior and the preference distribution across different categories. 2) Diversity-aware Filtering: This strategy addresses the issue where simple filtering based on category proportions can result in interaction data Dfiltered u being overly concentrated in a few categories. We frame it as a resource allocation problem—how to distribute a limited number of interaction records across different categories, balancing item diversity while maximizing overall recommendation performance. We pre-compute a performance matrix M, where M [c][p] represents the Hit Rate achieved by retaining p% of interac- tions from category c. Here, p takes values from a predefined set, e.g., {10%, 20%, ...}. We model this as a knapsack problem, aiming to find the optimal allocation of retention ratios across categories, maximizing overall performance under a fixed total retention ratio (e.g., retaining a total of K interactions, corresponding to a total retention ratio K ′). Formally, let x = [x1, x2, ..., xn] be a vector where xc denotes the retention ratio for category c. Our objective is to find the optimal x∗ such that: