Unlearn-ALS [28] introduces a fine-tuning optimization approach tailored for bilinear models. AltEraser [17] breaks down the Unlearning problem into multiple sub- problems to simplify computation. FRU [29] concentrates on unlearning within federated recommendation systems, striving to eliminate the influence of particular users. Additionally, some studies employ influence functions to estimate the impact of data on the model, enabling rapid updates without the need for retraining, as seen in IFRU [11]. Furthermore, research has explored unlearning in various models, including session- based [30] and sequential-based [31] approaches. C. LLMs for Recommendation Large Language Models (LLMs) have demonstrated im- mense application potential in recommendation systems, at- tracting extensive research interest [32]â€“[36]. Some studies ex- plore using LLMs as inference models by designing prompts to guide them in performing recommendation tasks. For example, P5 [37] leverages item indices to convert user interactions into text prompts for model training, while M6-Rec [38] textual- izes user behavior data and transforms recommendation tasks into language tasks. M6-Rec achieves efficient recommenda- tion models under limited hardware resources by employing techniques such as enhanced prompt tuning, post-processing interactions, and early exiting. Chat-REC [39] translates user profiles and interaction information into prompts to con- struct conversational recommendation systems, whereas In- structRec [40] and TALLRec [41] utilize instruction fine- tuning methods to enable LLMs to execute recommendation tasks more effectively. Additionally, some research attempts to model the structured relationships between user behaviors and items using LLMs to improve recommendation performance. For instance, LLMRec [42] strengthens recommendation sys- tems by adopting three simple yet effective LLM-based graph augmentation strategies. However, directly applying LLMs for recommendations often encounters challenges such as high computational costs and slow inference speeds. To address these issues, some studies have adopted alternative approaches, such as combining LLM-based data augmentation methods with classical Collaborative Filtering (CF) [43], aiming