rec unlearning. Index Terms —Machine Unlearning, Recommender Systems, Large Language Model, Prompt Learning I. I NTRODUCTION Recommender systems (RS) rely heavily on user-generated data to deliver personalized experiences [1]–[3], raising con- cerns over privacy and data integrity. Users now demand the “right to be forgotten” under regulations like GDPR [4], while poisoned or outdated data further threaten model quality [5]. These challenges have driven growing interest in recommen- dation unlearning —removing specific user influences from trained models while preserving overall utility and efficiency. The most straightforward unlearning approach, retraining the model from scratch on the remain dataset, guarantees complete removal but incurs prohibitive computational costs, especially for large-scale system [6], [7]. To mitigate this, two primary classes of methods have been developed. Exact unlearning methods, often based on a partition-and-retrain framework like SISA [8], aim to isolate changes by retraining only affected sub-models, with extensions like GraphEraser [9] † Corresponding Author. Original Dataset Original model User A User B User C Harry Potter fans User A I'm not interested in Harry Potter anymore recommender A No longer recommended User B User C recommend recommender B recommender C We are still harry potter fans recommend Training Guide Fig. 1. Traditional methods use a single shared recommendation model for all users, where unlearning one user’s data alters global parameters, potentially degrading recommendations for others. In contrast, our method leverages Retrieval-Augmented Generation (RAG) with LLMs to perform efficient and precise user-level unlearning without affecting unrelated users. and RecEraser [10] for recommendation scenarios. Alterna- tively, approximate unlearning methods aim for efficiency by estimating and reversing the impact of data to be forgotten. Among these, IFRU [11], [12], and SCIF [13] have been proposed to approximate the impact of individual training data points via influence function. However, these methods often struggle with the computational burden