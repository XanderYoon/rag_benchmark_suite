for Netflix and LightGCN. This consistent performance drop in the forgotten set demonstrates that CRAGRU effectively localizes the unlearn- ing effect to the target users without impacting the recom- mendation quality for others. Moreover, as K increases, the performance gap narrows—this is likely due to the expansion of the candidate item pool Icand u provided by the backbone model, introducing items with weaker user relevance and reducing filtering precision. In summary, CRAGRU achieves high unlearning effectiveness while preserving the perfor- mance of non-target users, addressing the central challenge of unlearning bias in recommendation systems. E. Effectiveness of Retrieval Strategies (RQ4) This experiment investigates how CRAGRU’s three retrieval filtering strategies contribute to both recommendation quality and unlearning bias mitigation. We compare them with the original backbone model (without retrieval filtering) and the other unlearning method RecEraser on ML-1M and Netflix datasets, using BPR and LightGCN as backbones. Figure 4 reports NDCG@K for K = 5, 10, 20. CRAGRU consistently outperforms RecEraser across datasets and backbones, indicating superior preservation of recommendation quality while performing user-level unlearn- ing. For example, on ML-1M with BPR, CRAGRU improves NDCG@10 by 10.72% over RecEraser; with LightGCN, the gain is 7.54%. All three retrieval strategies improve over the unfiltered CRAGRU baseline, demonstrating that retrieval-stage filtering is critical to reducing collateral performance loss—a direct manifestation of unlearning bias. 1) User preference-based filtering performs well by preserving long-term semantic con- sistency, e.g., on Netflix (LightGCN), improving NDCG@10 from 0.2400 to 0.2827. 2) Diversity-aware retrieval ensures representation of different item clusters to avoid overfitting to specific behaviors. While slightly less precise than preference- based filtering, it reduces retrieval bias with lower computa- tional cost via global distribution optimization. 3) Attention- aware retrieval—using multi-head attention to score and pri- oritize impactful interactions—achieves the highest gains. On ML-1M, it improves NDCG@10 by