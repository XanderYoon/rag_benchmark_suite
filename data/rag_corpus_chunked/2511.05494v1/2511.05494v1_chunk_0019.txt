evaluate recommendation per- formance after unlearning, we employ two common metrics: Hit Ratio (HR@ K) and Normalized Discounted Cumulative Gain (NDCG@ K). HR@ K measures the precision of the recommendation by calculating the proportion of times that the user’s target item is present in the topK recommendations. NDCG@K is a ranking metric that gives higher weights to top-ranked items, considering their positions in the recom- mendation list. Both metrics are calculated on the remaining interaction data Dr after unlearning. In our experiments, we evaluate the performance at K ∈ { 5, 10, 20}. Higher values indicate better performance. Furthermore, we measure unlearning efficiency by comparing the unlearning time; a shorter unlearning time indicates greater efficiency. B. Model Utility (RQ1) We evaluate the model utility of CRAGRU on three public datasets using HR@K and NDCG@K (K = 5, 10, 20) under two backbone models: BPR and LightGCN. To simulate unlearning requests, 10% of user interactions are randomly selected and removed. This experiment not only measures the absolute performance of each method, but more importantly, reflects the extent to which forgetting targeted users negatively affects the recommendation utility for others—i.e., unlearning bias. We compare CRAGRU against six baselines, including retraining, partition-based methods (SISA, GraphEraser, Re- cEraser), and approximate unlearning methods (SCIF, IFRU). Retraining achieves the highest performance since no actual unlearning is required, but at impractical computational cost. Among partition-based methods, RecEraser performs best, as it aggregates similar users to reduce utility loss. However, these methods often suffer from unlearning bias due to the entanglement of forgotten users with co-located users in training shards. Approximate methods (SCIF, IFRU) reduce computation by estimating user influence via gradients or sim- ilarity propagation, but can cause latent drift in similar users’ embeddings, leading to degraded recommendation quality. CRAGRU outperforms almost all other unlearning baselines across