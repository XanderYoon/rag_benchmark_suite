In real-world scenarios, user unlearning requests typically come gradually, so the user- level unlearning method can quickly fulfill these needs in a timely manner. D. Unlearning Completeness (RQ3) To assess the effectiveness of CRAGRU in erasing user- specific influence, we compare recommendation performance on the forgotten set versus the remaining set, following simu- lated unlearning of 10% user-item interactions on ML-1M and Netflix datasets. BPR and LightGCN are used as backbone Fig. 3. Comparison of the performance between the forget set and the remain set on ML-1M and Netflix datasets. Fig. 4. Comparison of the performance of different retrieval strategies for the CRAGRU model on the ML-1M and Netflix. models. We report HR@K and NDCG@K for K = 1, 3, 5 to measure personalized recommendation quality. Unlike traditional unlearning methods that often leave resid- ual influence of forgotten users on the model, CRAGRU filters these users’ traces at the retrieval stage, ensuring atomic removal. As shown in Figure 3, the recommendation quality for the forgotten set is consistently and significantly lower than that of the remaining set across all settings, indicating effective removal of memorized patterns and minimal cross- user leakage. For instance, on ML-1M with BPR, the HR@1 and NDCG@1 of the forgotten set are only 55.85% of those of the remaining set; As K increases to 3 and 5, the HR and NDCG ratios rise to 67.61% / 56.89% and 72.36% / 59.19%, respectively. Similar trends are observed for Netflix and LightGCN. This consistent performance drop in the forgotten set demonstrates that CRAGRU effectively localizes the unlearn- ing effect to the target users without impacting the recom- mendation quality for others. Moreover, as K increases, the performance gap narrows—this is likely due to the expansion of the candidate item pool Icand u provided by the backbone model, introducing