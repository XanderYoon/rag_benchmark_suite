Italy Table 1: Statistics of the datasets. Dataset Split # Conv. # Turns Collection QReCC Train 10,823 63,501 54MTest 2,775 16,451 TopiOCQA Train 3,509 45,450 25MTest 205 2,514 TREC CAsT 20 Test 25 216 38M TREC CAsT 22 Test 18 205 138M TREC iKAT 23 Test 25 176 116M topic switch in a conversation built around that Wikipedia topic. It features 4K conversations and 48K turns. ‚Ä¢ TREC CAsT 2020, 2022 [9, 49] and TREC iKAT 2023 [3] are smaller conversational datasets, but carefully hand-crafted to include various conversational complexities. They are coupled with high-quality relevance judgements done by NIST assessors. Effectiveness Metrics. We report the results in terms of the main IR metrics [42, 65], as well as official TREC CAsT and iKAT ones [3, 10]. Metrics are Mean Reciprocal Rank (MRR), Normalized Dis- counted Cumulative Gain (nDCG) [22] at 3. As we focus on first- stage retrieval, we also include Recall at different ranks [62]. We determine the statistically significant differences by doing a two- sided paired t-test with Bonferroni correction at 95% confidence (ùëù < 0.05). Efficiency Metrics. In terms of inference efficiency, we report the number of LLM inference calls when used for query rewriting, as it is a strong indicator of inference latency. As an example, it takes an average of 4.4 seconds to generate 64 tokens on Llama 3.1, on A100 GPU, while the typical dual-encoder retrieval latency is in the range of 100 milliseconds on CPU [27, 30, 32]. We also provide Rewrite and Retrieval efficiency, of several mod- els. This is performed per query, averaged over the entire test set. Rewrite is the latency for rewriting, while retrieval is both query encoding and search through the inverted index (in the case of SPLADE models). We used a 4th AMD EPYC CPU and