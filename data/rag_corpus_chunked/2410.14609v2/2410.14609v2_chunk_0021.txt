the relaxed distillation affect the efficiency and sparsity of the student models? 5.1 Performance Comparison In this first subsection, we aim to answerRQ1 and RQ2 by compar- ing the performance of our proposed DiSCo with state-of-the-art query rewriting, and distillation-based methods. We first compare the performance of DiSCo in the in-domain and out-of-domain retrieval setups in Tables 3 and 4, respectively. The tables report the performance of diverse baselines in terms of various metrics, as well as the number of LLM calls every model requires at inference. This is an important factor while comparing the performance of conver- sational retrieval models for various reasons, namely, (i) LLM call leads to considerable inference latency, delaying inferencing from hundreds of milliseconds to seconds; and (ii) LLM-based methods take advantage of the vast parameter size and knowledge learned by the training of the LLMs, making their comparison to other smaller models unfair. In-domain retrieval. Trying to address RQ1, we report in Table 3 in-domain performance on TopiOCQA and QReCC. Looking at the results, we do not observe a big gap between the rewriting and distillation-based methods, even though the rewriting-based methods make use of the LLM knowledge in the rewriting phase. Distillation-based methods (i.e., ConvDR, QRACDR, LeCoRe) even DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search SIGIR ‚Äô25, July 13‚Äì18, 2025, Padua, Italy Table 4: Zero-shot Performance on Out-Of-Domain. DiSCo multi-teach uses both ùëá2 and ùëá3 (Mistral and Human teachers). DiSCo Fusion is the fusion of the SPLADE MistralQR with DiSCo multi-teach. FC refers to the models that do not use any rewriting at inference and just take the Full Context as input. denotes the number of LLM calls used at inference. Method RW CAsT 2020 CAsT 2022 iKAT 2023 R@100 MRR nDCG@3 R@100 MRR nDCG@3 R@100 MRR nDCG@3 Query