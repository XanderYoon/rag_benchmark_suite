latent representations, while LeCoRe uses learned sparse rep- resentations. All of them distill query representations, while DiSCo the similarity scores, as a relaxation of their distillation method. 4.3 Implementations details We use the SPLADE++ [16]4 checkpoints from Huggingface [60] for all our SPLADE models. To further finetune on the CS task, we fine-tune for 5 epochs, with a learning rate of2ğ‘’ âˆ’5, a max sequence length for queries of 64 tokens, and 100 tokens for answers, with 2We used the latest Llama meta-llama/Meta-Llama-3.1-8B-Instruct, Mistral mistralai/Mistral-7B-Instruct-v0.2 and T5 castorini/t5-base-canard 3We did not manage to reproduce the results reported in the original papers, as our reproduced results were much lower. Therefore, we report the numbers in the original papers. Therefore, we are not able to perform any significance tests with these models. 4naver/splade-cocondenser-ensembledistil [16] SIGIR â€™25, July 13â€“18, 2025, Padua, Italy Simon Lupart, Mohammad Aliannejadi, and Evangelos Kanoulas Table 3: In-Domain Performance on TopiOCQA and QReCC. DiSCo multi-teach for TopiOCQA is the combination of ğ‘‡1 and ğ‘‡2, and QReCC uses ğ‘‡2 and ğ‘‡3. Hyperscripts â€  are paired t-test ğ‘ < 0.05 comparing multi-teachers with single-teacher DiSCo. RW denotes the LLM/human rewriting method used as input to the model. FC refers to the models that do not use any rewriting at inference and just take the Full Context as input. denotes the number of LLM calls used at inference. Method RW TopiOCQA QReCC R@100 R@10 MRR nDCG@3 R@100 R@10 MRR nDCG@3 Query Rewriting SPLADE no rewrite FC 0 0.472 0.258 0.155 0.141 0.840 0.673 0.485 0.459 SPLADE HumanQR (ğ‘‡3) Human 0 - - - - 0.912 0.714 0.448 0.433 SPLADE T5QR (ğ‘‡0) T5 1 0.667 0.501 0.321 0.314 0.840 0.617 0.382 0.366 SPLADE LlamaQR (ğ‘‡1) Llama 3 1 0.761 0.572 0.365 0.352 0.826 0.613 0.377 0.360 SPLADE MistralQR (ğ‘‡2) Mistral 2 1 0.759