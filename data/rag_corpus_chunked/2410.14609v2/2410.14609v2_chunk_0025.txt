Although SPLADE MistralQR exhibits high performance (e.g., 0.57 and 0.33 in terms of Recall@100 resp. on CAsT 2020, 2022), fusing it with the distilled DiSCo leads to further significant improvements (4 points increase on both datasets). This indicates that relaxing the distillation pro- cess to the similarity score enables the model to go beyond the knowledge of the teacher in the representation space. This gain is even stronger on the precision of CAsT 2022, where DiSCo-Fusion outperforms SPLADE MistralQR by 8 points in terms of nDCG@3. Comparison with the teacher models. Now looking at the per- formance of the students and teachers models in Table 3, we see SIGIR ‚Äô25, July 13‚Äì18, 2025, Padua, Italy Simon Lupart, Mohammad Aliannejadi, and Evangelos Kanoulas SPLADE QR DiSCo 0.775 0.800 0.825 0.850 0.875 0.900 0.925 0.950Recall@100Llama (T1) Mistral (T2) Human (T3) T1 T1-T2 T1-T2-T3 T2-T3 T eachers Students with Multi-T eachers Figure 4: Teacher Selection on QReCC. (Left) SPLADE Teacher Models with different LLM QR. (Right) DiSCo Students when trained with multi-teachers. Best set in red ( ùëá2 and ùëá3). that the student models outperform the teachers significantly for in- domain retrieval. In particular, DiSCo Human outperforms SPLADE HumanQR by 3.5 MRR points on QReCC; and similarly with DiSCo Mistral. This demonstrates that our distillation objective allows the student model to learn representations that surpass the original teacher‚Äôs representations. This is possible thanks to the relaxation of the distillation, as the student can adapt and learn optimal repre- sentations based on gradient descent optimization. This was not possible with the previous distillation objective on the representa- tion, as student models were trained to copy the representations of the teacher model. Out-of-domain however, we see in Table 4 that the student models have more difficulties outperforming the perfor- mance of the teacher models. This