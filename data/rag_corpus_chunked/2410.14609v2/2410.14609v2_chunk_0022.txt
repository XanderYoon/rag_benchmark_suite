DiSCo Fusion is the fusion of the SPLADE MistralQR with DiSCo multi-teach. FC refers to the models that do not use any rewriting at inference and just take the Full Context as input. denotes the number of LLM calls used at inference. Method RW CAsT 2020 CAsT 2022 iKAT 2023 R@100 MRR nDCG@3 R@100 MRR nDCG@3 R@100 MRR nDCG@3 Query Rewriting SPLADE HumanQR Human 0 0.646 0.636 0.475 0.422 0.590 0.423 0.285 0.359 0.262 SPLADE T5QR T5 1 0.479 0.477 0.332 0.226 0.355 0.218 0.115 0.200 0.132 SPLADE LlamaQR Llama 3 1 0.550 0.515 0.376 0.312 0.453 0.300 0.198 0.281 0.177 SPLADE MistralQR Mistral 2 1 0.572 0.553 0.403 0.337 0.487 0.298 0.178 0.291 0.194 LLM4CS [41] GPT 3.5 5 0.489 0.615 0.455 - - - - - - LLM4CS (ours) GPT 4 5 0.504 0.618 0.444 0.283 0.425 0.272 0.133 0.154 0.099 CHIQ FT [43] T5 1 - 0.463 0.316 - - - - - - CHIQ Fusion [43] Llama 2 6 - 0.540 0.380 - - - - - - DiSCo Fusion Mistral 2 1 0.611 0.566 0.425 0.379 0.578 0.384 0.201 0.297 0.192 SFTconvSPLADE (ours) FC 0 0.446 0.338 0.234 0.274 0.382 0.227 0.101 0.144 0.085 Distillation QRACDR [46] FC 0 0.324 0.442 0.303 - - - - - - LeCoRe [42] FC 0 0.467 - 0.290 - - - - - - DiSCo Mistral FC 0 0.519 0.457 0.341 0.322 0.463 0.287 0.135 0.193 0.126 DiSCo Human FC 0 0.523 0.455 0.339 0.314 0.490 0.308 0.151 0.202 0.131 DiSCo multi-teach FC 0 0.531 0.483 0.353 0.334 0.512 0.322 0.147 0.192 0.125 outperform the rewriting-based methods while being more efficient too, as they have zero LLM calls. Besides, our DiSCo outperforms all rewriting- and distillation- based baselines with a large margin, showing that relaxing the distillation constraint