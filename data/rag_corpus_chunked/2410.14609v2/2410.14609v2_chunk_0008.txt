dot product of the query representation and a doc- ument), rather than the representation itself. This way, not only do we enable converging towards indefinite possible query repre- sentations, but also we take advantage of the contrastive nature of ranking by aligning the rewrite task to retrieval. Furthermore, we are not bound to one query or one teacher. Learned sparse representation. As Learned Sparse Retrieval (LSR) gained popularity in ad-hoc retrieval [28, 29, 31], SPLADE ar- chitectures were proposed for conversational passage retrieval [19, 37, 42], to benefit their interpretability and robustness proper- ties [16, 17, 38]. In coSPLADE [19], the authors use an MSE loss between full conversation and human rewrite representations of the sparse bag-of-words representations (of dimension 30k), and show promising performance. In the meantime, LeCoRe [42] also aims to distill human rewrites on sparse representations, through interme- diate embedding layers and by filtering a maximum of dimensions, to avoid the MSE on large dimensionality vectors. Distilling sparse representations is, however, challenging, as it requires controlling which dimensions should be activated, often restricting the po- tential of the sparse representations. Additionally, using a MSE is problematic since it treats each dimension separately, neglecting the significance of the associated dimension. These issues limit the effectiveness of sparse retrieval in previous work. In our work, by only distilling end scores, we give the model complete freedom to learn which dimensions to activate. We also investigate sparsity further, including methods to control it within CS. 3 Proposed Method In this section, we first recall notations from the CS and LSR fields, before presenting DiSCo and our relaxed distillation objective. 3.1 Preliminaries Notation. We consider a set of conversations between a user and a system, each composed of multiple turns. At turn ùëõ, we have access to previous queries and answers,