DiSCo Human outperforms QRACDR by 3.8 points in terms of nDCG@3, and LeCoRe by 5.5 points in terms of Recall@100, both on CAsT 2020. Note that those two models are two state-of-the- art models in the same zero-shot settings, trained on QReCC and evaluated on CAsT 2020. It is also noteworthy that even though our distillation-based approach is not able to outperform some of the rewriting-based (LLM-based) methods, our DiSCo-Fusion, which is a fusion method based on LLM rewriting and distillation manages to outperform all rewriting-based methods by a large margin in terms of Recall@100, reaching SotA conversational passage retrieval per- formance. When comparing closely with the SotA LLM4CS model, we note that while LLM4CS takes advantage of 5 GPT-4 calls, our DiSCo multi-teach model outperforms it in terms of recall on all out-of-domain datasets. Comparing the precision-oriented metrics, we see that DiSCo multi-teach outperforms LLM4CS on all datasets, except TREC CAsT 2020. This dataset was the second edition of TREC CAsT and is considered to be simpler than both 2022 and 2023 versions, showing the performance of our model on complex information needs. Also, considering that we focus on the retrieval task, we consider recall to be preferable, as reranking can be added post-retrieval on a smaller set of documents [11, 62]. Distillation-rewriting fusion. Furthermore, note in Table 4 that DiSCo-Fusion is the fusion of our DiSCo and SPLADE MistralQR, as the fusion of the student with the teacher. Although SPLADE MistralQR exhibits high performance (e.g., 0.57 and 0.33 in terms of Recall@100 resp. on CAsT 2020, 2022), fusing it with the distilled DiSCo leads to further significant improvements (4 points increase on both datasets). This indicates that relaxing the distillation pro- cess to the similarity score enables the model to go beyond the knowledge of the teacher in