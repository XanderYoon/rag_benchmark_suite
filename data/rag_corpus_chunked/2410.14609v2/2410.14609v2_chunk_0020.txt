also use a 256-token limit for passages. We use a batch size of 10, with 16 negatives per query, and in-batch negatives [35]. Default experiments with SPLADE use FLOPS and L1 regularization, as in the original code, with values ğœ†ğ‘ = 1ğ‘’ âˆ’3, ğœ†ğ‘‘ = 5ğ‘’ âˆ’4. We use mixed precision and fp16 to maximize memory use. For sparse retrieval, we use inverted indexes based on the numba library and pyserini [33] together with Pytorch [51]. The fusion method uses the ranx library [5]. All teachers use In-Context learning to generate the rewrites. We provide the one-shot prompt used for rewriting with the LLM Teacher models in Table 2. We experimented in both zero-shot and one-shot, but decided to use one-shot to improve the quality of the teacher models. Given the high efficiency of our fine-tuning, we were able to run the experiments on a single A100 GPU with 40GB of GPU memory. 5 Results In this section, we present the results of our DiSCo and the proposed distillation for in-domain and out-of-domain retrieval, together with an analysis of the multi-teacher distillation and the efficiency- effectiveness trade-off. Research questions. We aim to answer the following questions: RQ1 Would relaxing the distillation training objective improve conversational sparse retrieval in-domain performances? RQ2 How would the relaxation affect the out-of-domain general- ization capacities of the models? RQ3 How does the distillation of multiple LLM teachers compare to single teacher distillation? RQ4 How does the relaxed distillation affect the efficiency and sparsity of the student models? 5.1 Performance Comparison In this first subsection, we aim to answerRQ1 and RQ2 by compar- ing the performance of our proposed DiSCo with state-of-the-art query rewriting, and distillation-based methods. We first compare the performance of DiSCo in the in-domain and out-of-domain retrieval setups in Tables 3 and 4,