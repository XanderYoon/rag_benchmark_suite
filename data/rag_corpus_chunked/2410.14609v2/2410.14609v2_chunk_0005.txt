• We propose DiSCo, a relaxation of the training objective of CS model distilling similarities of rewritten queries rather than representations. 1 1Models checkpoints and code at https://github.com/SimonLupart/disco-conv-splade • We propose to distill knowledge from multiple teachers, un- locking the potential of mixtures of LLMs in CS. • We evaluate the effectiveness of DiSCo on in-domain (QReCC, TopiOCQA) and out-of-domain (CAsT 2020, 2022, and iKAT 2023) datasets, achieving state-of-the-art performance. • We analyze the sparsity of the learned representations com- pared to those of the original teacher models, and demon- strate the efficiency gains of the approach. Our results demonstrate that our Distillation of Sparse Con- versational retrieval, DiSCo, leads to in-domain performance im- provements, through our relaxation of the distillation loss. We also see improved generalization capacities with 12% gains on recall and 16% on precision compared to previous zero-shot models (LeCoRe and QRACDR) on CAsT 2020. We demonstrate that distilling from multiple LLM teachers for the query rewriting task brings further performance gains, compared to single LLM teacher. Finally, thanks to the increased freedom on the representations, we show that we can better control the model sparsity, even for long contexts, in deeper turns of the conversations. 2 Related Work Conversational Search (CS)differentiates itself from ad-hoc search primarily through the nature of the input. Conversational history can grow very long with turns’ dependencies, whereas search en- gine queries are typically concise, limited to a few words [ 6, 39]. The main challenge in addressingCS is modeling the conversational context, to only represent useful information from the previous turns [61]. Two possibilities exist to learn this noise reduction: first on the token level, with generative models that learn to generate contextualized queries from past conversations [14], or within the representation spaces [65] of retrieval models [44]. Query rewriting. Learning