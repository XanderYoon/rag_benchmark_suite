SPLADE no rewrite FC 0 0.472 0.258 0.155 0.141 0.840 0.673 0.485 0.459 SPLADE HumanQR (ğ‘‡3) Human 0 - - - - 0.912 0.714 0.448 0.433 SPLADE T5QR (ğ‘‡0) T5 1 0.667 0.501 0.321 0.314 0.840 0.617 0.382 0.366 SPLADE LlamaQR (ğ‘‡1) Llama 3 1 0.761 0.572 0.365 0.352 0.826 0.613 0.377 0.360 SPLADE MistralQR (ğ‘‡2) Mistral 2 1 0.759 0.591 0.366 0.356 0.884 0.668 0.424 0.409 IterCQR [21] GPT-3.5 1 0.620 0.426 0.263 0.251 0.841 0.655 0.429 0.402 LLM4CS [41] GPT-3.5 5 - 0.433 0.277 0.267 - 0.664 0.448 0.421 CHIQ FT [43] T5 1 - 0.510 0.300 0.289 - 0.576 0.369 0.340 CHIQ-Fusion [43] Llama 2 6 - 0.616 0.380 0.370 - 0.707 0.472 0.442 SFT convANCE [42] FC 0 0.710 0.430 0.229 0.205 0.872 0.715 0.471 0.456 convSPLADE [42] FC 0 0.720 0.521 0.295 0.307 0.878 0.699 0.500 0.466 Distillation ConvDR [65] FC 0 0.611 0.435 0.272 0.264 0.778 0.582 0.385 0.357 QRACDR [46] FC 0 0.758 0.571 0.377 0.365 0.897 0.748 0.516 0.516 LeCoRe [42] FC 0 0.735 0.543 0.320 0.314 0.897 0.739 0.511 0.485 DiSCo T5 FC 0 0.834 0.617 0.363 0.345 0.917 0.719 0.456 0.442 DiSCo Mistral FC 0 0.842 0.634 0.387 0.370 0.925 0.743 0.489 0.477 DiSCo Human FC 0 - - - - 0.927 0.741 0.483 0.470 DiSCo multi-teach FC 0 0.859â€  0.640 0.390 0.375 0.928 0.754 â€  0.498â€  0.487â€  a total limit of 256 tokens for the full conversational context. We also use a 256-token limit for passages. We use a batch size of 10, with 16 negatives per query, and in-batch negatives [35]. Default experiments with SPLADE use FLOPS and L1 regularization, as in the original code, with values ğœ†ğ‘ = 1ğ‘’ âˆ’3, ğœ†ğ‘‘ = 5ğ‘’ âˆ’4. We use mixed precision and fp16 to maximize memory use. For sparse retrieval,