DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search Simon Lupart University of Amsterdam Amsterdam, Netherlands s.c.lupart@uva.nl Mohammad Aliannejadi University of Amsterdam Amsterdam, Netherlands m.aliannejadi@uva.nl Evangelos Kanoulas University of Amsterdam Amsterdam, Netherlands e.kanoulas@uva.nl Abstract Conversational Search (CS) involves retrieving relevant documents from a corpus while considering the conversational context, inte- grating retrieval with context modeling. Recent advancements in Large Language Models (LLMs) have significantly enhanced CS by enabling query rewriting based on conversational context. How- ever, employing LLMs during inference poses efficiency challenges. Existing solutions mitigate this issue by distilling embeddings de- rived from human-rewritten queries, focusing primarily on learning the context modeling task. These methods, however, often separate the contrastive retrieval task from the distillation process, treating it as an independent loss term. To overcome these limitations, we introduce DiSCo (Distillation of Sparse Conversational retrieval), a novel approach that unifies retrieval and context modeling through a relaxed distillation objective. Instead of relying exclusively on representation learning, our method distills similarity scores be- tween conversations and documents, providing more freedom in the representation space and better leveraging the contrastive na- ture of document relevance. Extensive experiments on Learned Sparse Retrieval (LSR) across five CS datasets demonstrate that DiSCo achieves substantial improvements in both in-domain and out-of-domain retrieval tasks, achieving up to a six-point gain in recall for out-of-domain datasets over state-of-the-art methods. Additionally, DiSCo employs a multi-teacher distillation strategy, using multiple LLMs as teachers, further enhancing performance and surpassing the individual teachers in in-domain settings. Fur- thermore, analysis of model sparsity reveals that DiSCo allows for more effective control over the sparsity of the trained models. CCS Concepts •Information systems → Query representation; Language models; Information retrieval. Keywords conversational search, query understanding, neural sparse retrieval ACM Reference Format: Simon Lupart, Mohammad Aliannejadi, and Evangelos Kanoulas. 2025. DiSCo: LLM