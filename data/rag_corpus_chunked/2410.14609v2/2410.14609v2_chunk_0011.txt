with a direct con- straint on the representation. The goal is to have the representations of the full conversations converging toward the representations of SIGIR â€™25, July 13â€“18, 2025, Padua, Italy Simon Lupart, Mohammad Aliannejadi, and Evangelos Kanoulas StudentTraining score TeacherInference 1 2 !!" E# x !"#!!!"=#!(%"#)#$(') =#(!(%%&'()#$(')!!#$%& )*+(!!#$%&,!!!") Figure 3: Distillation process. The first step stores scores from the rewritten queries with documents from the corpus. Then the student query encoder Ëœğ¸ğ‘ is trained to reproduce the output scores of the teacher. the rewritten queries: Ëœğ¸ğ‘ (ğ‘ğ‘ğ‘œğ‘›ğ‘£ ) â†’ ğ¸ğ‘ (ğ‘ğ‘Ÿ ğ‘¤) . (1) Relaxation of the distillation. In our work, we propose to dis- till the scores rather than the representations. This allows for a relaxation of the training objective. Instead of forcing Ëœğ¸ğ‘ (ğ‘ğ‘ğ‘œğ‘›ğ‘£ ) to converge to ğ¸ğ‘ (ğ‘ğ‘Ÿ ğ‘¤), we allow the entire hyperplane on which the similarity for ğ‘ğ‘ğ‘œğ‘›ğ‘£ and ğ‘ğ‘Ÿ ğ‘¤ with an anchor document ğ‘‘ is equal: Ëœğ¸ğ‘ (ğ‘ğ‘ğ‘œğ‘›ğ‘£ ) â†’ { ğ‘‹ âˆˆ Râ„ | ğ‘‹ âŠ¤ğ¸ğ‘‘ (ğ‘‘) = ğ‘ ğ‘ğ‘Ÿ ğ‘¤ } , (2) with targeted scores ğ‘ ğ‘ğ‘Ÿ ğ‘¤ = ğ¸ğ‘ (ğ‘ğ‘Ÿ ğ‘¤)âŠ¤ğ¸ğ‘‘ (ğ‘‘), and â„ the dimension of the representation space. More intuitively, while the existing dis- tillation loss functions bound the model to converge to one single embedding [65], our loss allows for infinite possible optimum em- beddings, as long as they have the same dot product with the target relevant document embedding (i.e., any point in the hyperplane). Such distillation on scores can be trained with a Kullback Leibler Divergence loss [25] on the distribution of scores: LKLD = ğ·KL (Sğ‘ğ‘Ÿ ğ‘¤ || Sğ‘ğ‘ğ‘œğ‘›ğ‘£ ) , (3) where Sğ‘ğ‘Ÿ ğ‘¤ and Sğ‘ğ‘ğ‘œğ‘›ğ‘£ are the distributions of similarity scores within the batch. It differs from the existing learning objective which is achieved with an MSE loss on each dimension