adapt and learn optimal repre- sentations based on gradient descent optimization. This was not possible with the previous distillation objective on the representa- tion, as student models were trained to copy the representations of the teacher model. Out-of-domain however, we see in Table 4 that the student models have more difficulties outperforming the perfor- mance of the teacher models. This result on out-of-domain retrieval is, however, not fair considering that teacher model generalization relies on billions of parameters, while students on a few hundred. Recall also that DiSCo-Fusion as the fusion of the teacher and the student was further improving performance out-of-domain. Reliance on weak teacher model. Finally, we explore the reliance on the teacher models, by training DiSCo with a weaker teacher model, such as T5. This is interesting in scenarios where we do not have access to strong teachers and also shows the robustness of the method when trained on more noisy labels. From Table 3, we see that DiSCo T5 performs almost on par with DiSCo Mistral, while SPLADE MistralQR outperforms SPLADE T5QR by an important margin, on both TopiOCQA and QReCC. Our method is thus robust even when trained on lower-quality rewrites from the teacher. 5.2 Multiple Teachers Distillation While the previous section focused on the distillation of single teachers, here we answer RQ3 on the use of multiple teachers. Thus, we compare our DiSCo model trained with multiple teachers. In-domain and out-of-domain retrieval. Trying to addressRQ3, both Tables 3 and 4 include our DiSCo multi-teach model trained with multiple teachers. Considering the performances of DiSCo multi-teach for in-domain retrieval, we observe significant gains compared to DiSCo Mistral on QReCC. In particular, we observe a 1-point increase in terms of R@10, MRR, and nDCG@3 on QReCC. This result remains consistent for out-of-domain retrieval, where within Table