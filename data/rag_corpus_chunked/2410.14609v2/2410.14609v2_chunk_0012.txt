Such distillation on scores can be trained with a Kullback Leibler Divergence loss [25] on the distribution of scores: LKLD = π·KL (Sπ‘π‘ π‘¤ || Sπ‘π‘π‘π‘›π‘£ ) , (3) where Sπ‘π‘ π‘¤ and Sπ‘π‘π‘π‘›π‘£ are the distributions of similarity scores within the batch. It differs from the existing learning objective which is achieved with an MSE loss on each dimension of the vector representations independently [19, 65]: LMSE = MSE( ΛπΈπ‘ (π‘π‘π‘π‘›π‘£ ), πΈ π‘ (π‘π‘ π‘¤)) . (4) This new LKLD loss includes the benefit of the contrastive objective, anchoring itself on relevant and hard documents from the corpus. Also note that in the previous optimization, the final loss was the sum of the distillation MSE loss with the contrastive InfoNCE loss, while we only use a single contrastive distillation KLD loss, unifying both objectives into one distillation loss. Figure 3 illustrates the distillation process. First, the teacher retrieves documents using the rewritten queries and stores the similarity scores. Then, the student model encodes the full conversation, learning to have equal similarities to the teacher on these documents, by minimizing the KLD distillation loss. This defines the final learning objective of our DiSCo models, as minimizing the distributions of scores between the student and teacher models. Hard negatives. As our new training objective from Equation 3 is now contrastive, we can benefit from hard negative mining. This was not possible with the previous distillation objective, as it only uses conversation representations, independently from documents. Thus applied to the case of multiple negatives per query, the target space from Equation 2 becomes a subspace of (h-N) dimen- sions. This still keeps a higher degree of freedom, since the number of negatives is much lower than the dimension of the embedding space (β„ >> π‘ ). {π‘‹ β Rβ„ | π‘‹