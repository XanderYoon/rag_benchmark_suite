surpassing the individual teachers in in-domain settings. Fur- thermore, analysis of model sparsity reveals that DiSCo allows for more effective control over the sparsity of the trained models. CCS Concepts ‚Ä¢Information systems ‚Üí Query representation; Language models; Information retrieval. Keywords conversational search, query understanding, neural sparse retrieval ACM Reference Format: Simon Lupart, Mohammad Aliannejadi, and Evangelos Kanoulas. 2025. DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Con- versational Search. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR This work is licensed under a Creative Commons Attribution 4.0 International License. SIGIR ‚Äô25, Padua, Italy ¬© 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1592-1/2025/07 https://doi.org/10.1145/3726302.3729966 ùëú Ed (d) Eq (qrw) ÀúEq (qconv) Y = {X ‚àà R2 | X‚ä§Ed (d) = sqrw } ùë• ùë¶ convDR DiSCo Hyperplane Figure 1: Similarity Score Distillation in R2. Existing loss functions bound representation of the full conversation rep- resentation to converge to a single rewrite representation (convDR, red arrow), while if we consider document d as anchor, an infinite number of representations, other than Eq (qrw), have the same similarity with d (Y, blue hyperplane). DiSCo allows the model to converge to the best representa- tion from the Y hyperplane (green arrows), as a relaxation. ‚Äô25), July 13‚Äì18, 2025, Padua, Italy. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3726302.3729966 1 Introduction Conversational Search (CS) is a well-established task, that has seen major improvements recently, thanks to the development of Large Language Models (LLMs) [3, 41, 43]. The goal of the CS task is to retrieve relevant documents from a corpus within a conversational context, in response to the user‚Äôs latest utterance. While sharing similarities with ad-hoc retrieval, the main challenge ofCS remains to model the conversational context [8, 54, 64]. More specifically, as the