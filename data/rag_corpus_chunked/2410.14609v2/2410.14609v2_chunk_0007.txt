the CQR task, using LLMs in zero- or few-shot fashion. CHIQ [43] proposes to decompose the context modeling task into several simpler sub-tasks for the LLMs – history enhancement, answer generation, question disambiguation – to gain in interpretability and effectiveness, LLM4CS aggregates several rewrites and answers within the representation space of the retrieval models [41], and DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search SIGIR ’25, July 13–18, 2025, Padua, Italy MQ4CS focuses on multi-aspect query rewrites [1]. However, con- text modeling using LLMs is too computationally costly, making it impossible for production. Distillation in CS. Lots of research aims at modeling the conver- sational context via learning to represent the conversation based on gold rewritten queries. Existing methods achieve this goal by taking the representation of the rewritten gold query as a teacher model and learning to distill the representation [ 19, 42, 46, 65]. ConvDR [65] learns the mapping between full conversation and human rewrites representations by minimizing an Mean Square Error (MSE) loss on the CLS tokens of both, performing well for dense bi-encoders. QRACDR [46], also proposes a similar distilla- tion, with several new MSE terms, between documents and queries to improve the representations and better align with the contrastive nature of the task. We differentiate from these works as we inspire from contrastive margin distillation (Margin-MSE [20]) and relax the query rewriting distillation by learning based on the relevance scores (i.e., the dot product of the query representation and a doc- ument), rather than the representation itself. This way, not only do we enable converging towards indefinite possible query repre- sentations, but also we take advantage of the contrastive nature of ranking by aligning the rewrite task to retrieval. Furthermore, we are not bound to one query or one teacher. Learned sparse