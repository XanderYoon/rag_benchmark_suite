conversation representations, independently from documents. Thus applied to the case of multiple negatives per query, the target space from Equation 2 becomes a subspace of (h-N) dimen- sions. This still keeps a higher degree of freedom, since the number of negatives is much lower than the dimension of the embedding space (â„ >> ğ‘ ). {ğ‘‹ âˆˆ Râ„ | ğ‘‹ âŠ¤ğ¸ğ‘‘ (ğ‘‘ğ‘– ) = ğ‘ ğ‘– ğ‘ğ‘Ÿ ğ‘¤ âˆ€ğ‘– âˆˆ [ 1, ğ‘ ]} , (5) with ğ‘ ğ‘–ğ‘ğ‘Ÿ ğ‘¤ = ğ¸ğ‘ (ğ‘ğ‘Ÿ ğ‘¤)âŠ¤ğ¸ğ‘‘ (ğ‘‘ğ‘– ) being the scores from multiple hard negatives. These negatives are mined during the Teacher Inference step from Figure 3, further improving training objective [16, 63]. Teacher Models. An important component of distillation relies on the choice of the teacher. The teacher is first used to rewrite the conversation utterance, into ğ‘ğ‘Ÿ ğ‘¤, which will be encoded ğ¸ğ‘ (ğ‘ğ‘Ÿ ğ‘¤) and distilled through the similarities scores ğ‘ ğ‘ğ‘Ÿ ğ‘¤. We use multiple LLMs, together with human rewrite (when available), as teachers. Having several teachers is motivated by the well-established effec- tiveness of ensembling strategies, where stronger teachers would provide higher-quality training signals, leading to better student model performance. This results in several trained DiSCo students, depending on the teacher used. Below we list all the teachers: â€¢ T0: T5QR â€¢ T1: LlamaQR â€¢ T2: MistralQR â€¢ T3: HumanQR Furthermore, another property of the proposed distillation is that we can distill from multiple teachers, making the distill from several LLMs or humans possible. We do so by averaging the similarity scores of the ğ‘‡ teachers, each having a different rewrite, thus a different similarity with the documents. We experimented with mean, min, and max aggregation methods, observing no significant differences in performance, and thus decided to use the more simple mean aggregation. Overall, the distilled score is: ğ‘ ğ‘ğ‘Ÿ ğ‘¤ğ‘ğ‘™ğ‘™