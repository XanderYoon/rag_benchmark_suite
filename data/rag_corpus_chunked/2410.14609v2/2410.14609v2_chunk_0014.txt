LLMs or humans possible. We do so by averaging the similarity scores of the ğ‘‡ teachers, each having a different rewrite, thus a different similarity with the documents. We experimented with mean, min, and max aggregation methods, observing no significant differences in performance, and thus decided to use the more simple mean aggregation. Overall, the distilled score is: ğ‘ ğ‘ğ‘Ÿ ğ‘¤ğ‘ğ‘™ğ‘™ = ğ‘ ğ‘rw1 + ğ‘ ğ‘rw2 + Â· Â· Â· +ğ‘ ğ‘rwğ‘‡ ğ‘‡ . (6) The final learning objective of our DiSCo multi-teacher is the same as for the regular DiSCo (i.e., KLD from Equation 3), but distilling the average score of the set of teachers, instead of using a single similarity from a unique teacher. Student-Teacher Fusion. Finally, we propose a fusion of the teacher and the student model, combining the ranked lists of both at inference. This is achieved using an average normalized score over the ranked list of both models. 4 Experiments Design 4.1 Datasets and Metrics Datasets. We evaluate our methodâ€™s effectiveness on various con- versational passage retrieval datasets, detailed in Table 1, namely: â€¢ QReCC [4] is built upon ORConvQA [53], NQ [26] and TREC CAsT 2019 [ 10]. It features 13K conversations and 80K turns created by human annotators around existing documents. â€¢ TopiOCQA [2] distinguishes itself from QReCC focusing on topic switches, where every Wikipedia hyperlink is considered a DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search SIGIR â€™25, July 13â€“18, 2025, Padua, Italy Table 1: Statistics of the datasets. Dataset Split # Conv. # Turns Collection QReCC Train 10,823 63,501 54MTest 2,775 16,451 TopiOCQA Train 3,509 45,450 25MTest 205 2,514 TREC CAsT 20 Test 25 216 38M TREC CAsT 22 Test 18 205 138M TREC iKAT 23 Test 25 176 116M topic switch in a conversation built around that Wikipedia topic. It