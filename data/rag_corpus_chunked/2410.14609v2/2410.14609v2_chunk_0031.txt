as the TopiOCQA dataset has topic switches every few turns, making the context modeling task more complex. Examining sparsity, we observe that the representations across all models remain sparse, with at most 60 non-zero dimensions even at deeper conversation levels ( > 10). At these depths, con- versations become significantly longer, reaching up to 350 tokens. This suggests that the models effectively perform noise reduction as part of the context modeling task, activating only a subset of the tokens from the context. Now, comparing the models, as SPLADE MistralQR relies on Mistral rewrite, we see that the representation sparsity is consistent even for long conversations. For DiSCo and convSPLADE, sparsity decreases with the turns of the conversation. 6 Conclusion & Future Work In this work, we propose DiSCo, a novel distillation strategy in CS to distill query rewriting from LLMs. DiSCo trains CS models with a relaxed distillation strategy that unifies context modeling and retrieval tasks. Our experiments on LSR and DiSCo models demon- strate that this training objective achieves important performance gains in both in-domain and out-of-domain retrieval. By distilling LLM rewrites, our method effectively learns from a single or several LLM teachers, outperforming the teachers, and reaching SoTA on several CS datasets. The proposed training strategy of DiSCo also shows robustness to the quality of the teacher model when trained on weaker teacher models. We further examine the inference effi- ciency and sparsity of our approach after distillation. Our findings emphasize the importance of aligning the rewriting and retrieval tasks in CS, with training objectives that unify both tasks. As a future work, we plan to study different teachers, as any model pro- ducing a similarity score could be distilled, e.g., cross-encoder [20], paired with stronger LLMs for rewrite. Acknowledgments This research was partly supported by the Swiss