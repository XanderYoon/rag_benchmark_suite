[36, 57, 58], but we also relax the training objective toward the final goal of retrieval, which is to compute similarities between queries and documents [24, 56, 59]. This relax- ation allows the student to further learn the context modeling task, considering relevant and irrelevant documents from the corpus, rather than mapping all representations to the human representa- tions. This objective also follows the precepts from Hofstätter et al. [20] on distillation for ad-hoc retrieval and can benefit from hard negatives within the distillation loss. Both dense and sparse meth- ods could be subject to this relaxation; however, in this work, we focus on learned sparse architectures [66], such as SPLADE [15, 31], as the relaxation would have more degrees of freedom due to the high dimensionality of the representations for LSR. Besides, the proposed relaxation also reduces the level of con- straints on the teachers, as any method producing similarity scores could be used for the distillation. In the context of CS, this allows the model to learn from multiple LLM rewrites, by fusing simi- larity scores of several teachers into a single score to be distilled. To the best of our knowledge, our work is the first to distill the knowledge of multiple LLM teachers for query rewriting. This also distinguishes our method from the work from Hofstätter et al. [20], as we distill from LLM knowledge through the rewrites. Through our work, we make the following contributions: • We propose DiSCo, a relaxation of the training objective of CS model distilling similarities of rewritten queries rather than representations. 1 1Models checkpoints and code at https://github.com/SimonLupart/disco-conv-splade • We propose to distill knowledge from multiple teachers, un- locking the potential of mixtures of LLMs in CS. • We evaluate the effectiveness of DiSCo on in-domain (QReCC, TopiOCQA) and out-of-domain