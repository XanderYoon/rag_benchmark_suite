of the Figure gives the performances of the teachers on QReCC, the right part shows the gains when incrementally adding teachers to the distillation. We can also notice the important gains from the distillation of LlamaQR. In particular, DiSCo Llama improves by 8 points compared to SPLADE LlamaQR in terms of Recall@100, while in comparison DiSCo Mistral by only 4 points compared to his teacher. This shows that the relaxation is robust and can learn from even lower-quality rewrites. 5.3 Effectiveness-Efficiency Trade-off In this subsection, we aim to answer RQ4 by analyzing the effecti- veness-efficiency trade-off of the proposed DiSCo models. First, we measure the efficiency of our method compared to rewriting-based methods. Then, we show the possibility of controlling the sparsity of the model through the FLOP regularization. Finally, we dive into sparsity at different depths of conversations. Efficiency.As to explicit the inference efficiency gain of our method compared to rewrite-based approaches, we plot in Table 5 the ef- ficiency in milliseconds of the models on TREC CAsT 2020. We observe from the table that while SPLADE MistralQR achieves very high performance here out-of-domain, the efficiency is very low compared to DiSCo Mistral. This is because of the rewriting step, which involves 1 LLM call for MistralQR. Also note that the rewrite needs to be executed on GPU, while most of the retrieval time is a search through the inverted index on CPU. DiSCo Mis- tral and multi-teach thus appear to have a better trade-off when it comes to efficiency. Finally, DiSCo Fusion has the best performance overall, without an important computational overload compared DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search SIGIR ’25, July 13–18, 2025, Padua, Italy avgq_len37.0avgq_len22.7 Figure 5: Effectiveness-efficiency trade-off on TopiOCQA. Sparser representations have a lower latency but also a lower