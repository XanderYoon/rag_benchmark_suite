about the benefit of Yoga? system: Increased flexibility, muscle strength. # Please rewrite the following user question: Does it help in reducing stress? # Re-written query: Does Yoga help in reducing stress? # Example 2: # Context: <ctx> # Please rewrite the following user question: <utterance> # Re-written query: Query rewriting methods. SPLADE-[T5/Llama/Mistral/Human]QR does retrieval using the SPLADE ad-hoc retrieval model trained on MS MARCO [ 47]. As input, we pass the rewritten query us- ing T5 [55], Llama 3.1 [13], Mistral [23], or gold human rewrites2. SPLADE no rewrite is the same ad-hoc retrieval model without rewrite, on the original conversation.IterCQR [21], CHIQ-Fusion [43], and LLM4CS [41] are state-of-the-art query rewriting baselines; however, they require multiple LLM calls at inference, which puts them under a high disadvantage. For LLM4CS, we reproduced their best setting (RAR, Mean aggregation from ùëÅ = 5, CoT, GPT-4). Supervised fine-tuned methods. convSPLADE [42] and convA- NCE [42] are two methods fine-tuned using the InfoNCE loss on the conversational contrastive labels. Their input is the whole con- versational context. We reproduce convSPLADE for out-of-domain retrieval using the same hyperparameters as in the original paper. Distillation-based method. LeCoRe [42], QRACDR [46] and Con- vDR [65]3 all learn to distill the gold human rewrite representations and usually combine the distillation loss with an InfoNCE. Like our DiSCo, these models do not rely on LLM calls either, making them comparable. ConvDR and QRACDR are both dense approaches, using latent representations, while LeCoRe uses learned sparse rep- resentations. All of them distill query representations, while DiSCo the similarity scores, as a relaxation of their distillation method. 4.3 Implementations details We use the SPLADE++ [16]4 checkpoints from Huggingface [60] for all our SPLADE models. To further finetune on the CS task, we fine-tune for 5 epochs, with a learning rate