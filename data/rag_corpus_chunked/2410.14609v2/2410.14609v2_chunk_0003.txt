the rewrite phase that can propagate to the retrieval phase. Hence, the need for unified retrieval models that do both tasks together in the representation space [46, 52]. Approaches such as ConvDR [65], coSPLADE [19] and LeCoRe [42] all learn both conversational context modeling and retrieval tasks within the representation space, either in a dense or sparse embedding space. As illustrated in Figure 1, they use a distillation objective enforcing the conversation representations to converge to the representations of gold human-rewritten queries (red arrow in the figure). This objective is, however, restrictive and assumes gold-rewritten query representations are theonly optimal rewriting and only learning target in the representations space, leaving little freedom for the model to further learn and optimize the represen- tations. DiSCo, differently from ConvDR relaxes the distillation targeting an entire hyperplane (blue hyperplane in the figure) in- stead of a single representation. This is achieved by focusing on query document similarity scores, rather than solely on the rewrite. Besides, previous work [19, 42, 65] distills query rewrite indepen- dently from the ranking objective. We thus propose to fill this gap, with a single distillation loss that unifies both context modeling and ranking tasks, while not limiting the model to learn one single representation. Our method learns to distill the similarity scores between rewrit- ten queries and documents rather than query representations di- rectly. By distilling similarities, we first align with the contrastive nature of the ranking task [36, 57, 58], but we also relax the training objective toward the final goal of retrieval, which is to compute similarities between queries and documents [24, 56, 59]. This relax- ation allows the student to further learn the context modeling task, considering relevant and irrelevant documents from the corpus, rather than mapping all representations to the human representa- tions. This