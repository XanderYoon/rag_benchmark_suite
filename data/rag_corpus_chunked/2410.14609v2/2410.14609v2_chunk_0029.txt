appear to have a better trade-off when it comes to efficiency. Finally, DiSCo Fusion has the best performance overall, without an important computational overload compared DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search SIGIR â€™25, July 13â€“18, 2025, Padua, Italy avgq_len37.0avgq_len22.7 Figure 5: Effectiveness-efficiency trade-off on TopiOCQA. Sparser representations have a lower latency but also a lower MRR. avg q_len is the average number of activated tokens in the conversation representations, as indicator of efficiency. to SPLADE MistralQR 5. Note that retrieval is not optimized, and libraries such as PISA could reduce retrieval of SPLADE models below 100 ms on CPU, as shown in recent papers [27, 30, 32]. This would further increase the efficiency gap between rewrite-based and distillation-based approaches. We also focus here on inference efficiency, as training efficiency does not directly affect the user, and would involve training LLMs. Those results provide a first answer to RQ4 on the efficiency of DiSCo compared to existing methods. Controlling Sparsity. We then focus here on the sparsity of the sparse representations, as a measure of efficiency within the in- verted index. Figure 5 plots the performance and FLOPs of several DiSCo models when trained with different levels of sparsity. This is possible thanks to the regularization loss from the SPLADE archi- tecture, controlled by the hyper-parameters (ğœ†ğ‘, ğœ†ğ‘‘ ) [17]. From the Figure, we see for example that the same DiSCo T5 model can be trained with different degrees of regularization, leading to different FLOPs and MRR. Furthermore, Figure 5 shows the FLOPs of the as- sociated teacher models: SPLADE MistralQR and T5QR. We notice that our student DiSCo models are more sparse compared to their teachers, as having only a constraint on similarities during train- ing allows more freedom on the representations and their sparsity.