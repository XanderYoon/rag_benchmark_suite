onlyğ¸ğ‘ needs further fine-tuning to adapt to long conversational contexts, noted Ëœğ¸ğ‘. This can be done with a contrastive InfoNCE [48] loss on the CS datasets (e.g., convANCE, convSPLADE), or with a distillation loss (e.g., ConvDR and other distillation approaches such as DiSCo). Learned Sparse Retrieval. We rely on the SPLADE architec- ture [15, 17], based on the BERT pretrained transformers [12, 34]. SPLADE makes use of the Masked Language Modeling embedding layer of BERT to create sparse representations, where each dimen- sion corresponds to a token of the vocabulary. ğ¸ğ‘, ğ¸ğ‘‘ and Ëœğ¸ğ‘ are the MLM outputs of the model, of dimension 30k (vocabulary di- mension). To control the sparsity of the model, the authors propose to use a regularization loss [50], The sparsity of the models can be controlled with the two hyperparameters ğœ†ğ‘ and ğœ†ğ‘‘, as weights for the regularization loss. As we rely on the same SPLADE model, we also included this loss. 3.2 DiSCo We describe our distillation process in Figure 2. The conversation is passed through the teacher model, rewriting the last user utterance, and computing similarities with several documents from the corpus. Meanwhile, the student encodes the full conversation and scores it with those documents. The final training objective for the student is to match the teacher similarity scores. This relaxes the previous objective, which distilled representations directly. Representation distillation. Existing approaches [19, 42, 46, 65] distill query rewrites on the representation space, with a direct con- straint on the representation. The goal is to have the representations of the full conversations converging toward the representations of SIGIR â€™25, July 13â€“18, 2025, Padua, Italy Simon Lupart, Mohammad Aliannejadi, and Evangelos Kanoulas StudentTraining score TeacherInference 1 2 !!" E# x !"#!!!"=#!(%"#)#$(') =#(!(%%&'()#$(')!!#$%& )*+(!!#$%&,!!!") Figure 3: Distillation process. The first step stores scores from the