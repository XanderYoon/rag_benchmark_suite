Human FC 0 0.523 0.455 0.339 0.314 0.490 0.308 0.151 0.202 0.131 DiSCo multi-teach FC 0 0.531 0.483 0.353 0.334 0.512 0.322 0.147 0.192 0.125 outperform the rewriting-based methods while being more efficient too, as they have zero LLM calls. Besides, our DiSCo outperforms all rewriting- and distillation- based baselines with a large margin, showing that relaxing the distillation constraint to learn the similarity score, rather than the representations leads to further improvements. In particular, we see that DiSCo Mistral manages to outperform the learned sparse baseline, LeCoRe, by 10 and 2.8 points on TopiOCQA and QReCC resp. in terms of Recall@100. Out-of-domain retrieval. To further study the effectiveness of our proposed distillation approach, and study its generalizability, we report in Table 4 the results in the out-of-domain retrieval setting on TREC CAsT 20,22 and iKAT 23. Addressing RQ2, we see that, unlike the in-domain setting, there is a considerable gap between the rewriting-based and distillation-based methods when it comes to out-of-domain retrieval. On average, rewriting-based methods perform 19% better than distillation-based methods because they mainly rely on the massive parameter size and knowledge of LLMs, leading to high generalizability. However, as mentioned earlier mak- ing LLM calls on inference puts the models at a high disadvantage because of the high latency. Focusing on distillation-based approaches, our DiSCo outper- forms other methods by a large margin, showing the remarkable generalizability of our proposed relaxed distillation. In particular, we see that DiSCo Human outperforms QRACDR by 3.8 points in terms of nDCG@3, and LeCoRe by 5.5 points in terms of Recall@100, both on CAsT 2020. Note that those two models are two state-of-the- art models in the same zero-shot settings, trained on QReCC and evaluated on CAsT 2020. It is also noteworthy that even though our distillation-based approach is not able