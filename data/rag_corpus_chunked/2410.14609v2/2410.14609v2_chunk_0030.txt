with different degrees of regularization, leading to different FLOPs and MRR. Furthermore, Figure 5 shows the FLOPs of the as- sociated teacher models: SPLADE MistralQR and T5QR. We notice that our student DiSCo models are more sparse compared to their teachers, as having only a constraint on similarities during train- ing allows more freedom on the representations and their sparsity. This answers part of RQ4, showing that DiSCo can be trained with different sparsity levels. Sparsity at different Conversation Depths. To further address RQ4, we plot the sparsity and effectiveness of several baselines in Figure 6. On the left part of the graph, we see the performance according to the depth of the conversation – as the number of previous interactions between the user and the system – and on the right the number of activated tokens in the representations. In particular, we see the difficulty for long conversations across models. Comparing our DiSCo with the convSPLADE baseline, we see that the main improvement comes from longer conversations. 5DiSCo Fusion being an ensembling of two approaches, we report the maximum execution time of the two (as they could be executed in parallel). Figure 6: (Left) Performance with respect to depth on Topi- OCQA. (Right) Sparsity of query representations with respect to depth of conversations. We also see that DiSCo outperforms its teacher model, SPLADE MistralQR, on the first few turns of the conversation. This is also a difficult task here as the TopiOCQA dataset has topic switches every few turns, making the context modeling task more complex. Examining sparsity, we observe that the representations across all models remain sparse, with at most 60 non-zero dimensions even at deeper conversation levels ( > 10). At these depths, con- versations become significantly longer, reaching up to 350 tokens. This suggests that the