in an appropriate way. In particular, our assessment of answer relevance does not take into account fac- tuality, but penalises cases where the answer is incomplete or where it contains redundant informa- tion. To estimate answer relevance, for the given answer as(q), we prompt the LLM to generate n potential questions qi based on as(q), as follows: Generate a question for the given answer. answer: [answer] We then obtain embeddings for all questions us- ing the text-embedding-ada-002 model, avail- able from the OpenAI API. For each qi, we cal- culate the similarity sim(q, qi) with the original question q, as the cosine between the correspond- ing embeddings. The answer relevance score, AR, for question q is then computed as: AR = 1 n nX i=1 sim(q, qi) (1) This metric evaluates how closely the generated answer aligns with the initial question or instruc- tion. Context relevance The context c(q) is consid- ered relevant to the extent that it exclusively con- tains information that is needed to answer the ques- tion. In particular, this metric aims to penalise the inclusion of redundant information. To estimate context relevance, given a question q and its con- text c(q), the LLM extracts a subset of sentences, Sext, from c(q) that are crucial to answer q, using the following prompt: Please extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase "Insufficient Information". While extract- ing candidate sentences youâ€™re not al- lowed to make any changes to sentences from given context. The context relevance score is then computed as: CR = number of extracted sentences total number of sentences in c(q) (2) 4 The WikiEval Dataset To evaluate