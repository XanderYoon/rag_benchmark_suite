tuning, as the overall per- formance will be affected by the retrieval model, the considered corpus, the LM, or the prompt for- mulation, among others. Automated evaluation of retrieval-augmented systems is thus paramount. In practice, RAG systems are often evaluated in terms of the language modelling task itself, i.e. by mea- suring perplexity on some reference corpus. How- ever, such evaluations are not always predictive of downstream performance (Wang et al., 2023c). Moreover, this evaluation strategy relies on the LM probabilities, which are not accessible for some closed models (e.g. ChatGPT and GPT-4). Ques- tion answering is another common evaluation task, but usually only datasets with short extractive an- swers are considered, which may not be represen- tative of how the system will be used. To address these issues, in this paper we present Ragas1, a framework for the automated assessment 1Ragas is available at https://github.com/ explodinggradients/ragas. arXiv:2309.15217v2 [cs.CL] 28 Apr 2025 of retrieval augmented generation systems. We fo- cus on settings where reference answers may not be available, and where we want to estimate different proxies for correctness, in addition to the useful- ness of the retrieved passages. The Ragas frame- work provides an integration with both llama-index and Langchain, the most widely used frameworks for building RAG solutions, thus enabling devel- opers to easily integrate Ragas into their standard workflow. 2 Related Work Estimating faithfulness using LLMs The prob- lem of detecting hallucinations in LLM generated responses has been extensively studied (Ji et al., 2023). Several authors have suggested the idea of predicting factuality using a few-shot prompt- ing strategy (Zhang et al., 2023). Recent analy- ses, however, suggest that existing models struggle with detecting hallucination when using standard prompting strategies (Li et al., 2023; Azaria and Mitchell, 2023). Other approaches rely on linking the generated responses to