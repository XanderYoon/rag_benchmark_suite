LLMs through an API. For models that do not provide access to token probabilities, such as ChatGPT and GPT-4, differ- ent methods are needed. SelfCheckGPT (Manakul et al., 2023) addresses this problem by instead sam- pling multiple answers. Their core idea is that factual answers are more stable: when an answer is factual, we can expect that different samples will tend to be semantically similar, whereas this is less likely to be the case for hallucinated answers. Automated evaluation of text generation systems LLMs have also been leveraged to automatically evaluate other aspects of generated text fragments, beyond factuality. For instance, GPTScore (Fu et al., 2023) uses a prompt that specifies the consid- ered aspect (e.g. fluency) and then scores passages based on the average probability of the generated tokens, according to a given autoregressive LM. This idea of using prompts was previously also considered by Yuan et al. (2021), although they used a smaller fine-tuned LM (i.e. BART) and did not observe a clear benefit from using prompts. An- other approach directly asks ChatGPT to evaluate a particular aspect of the given answer by provid- ing a score between 0 and 100, or by providing a rating on a 5-star scale (Wang et al., 2023a). Re- markably, strong results can be obtained in this way, although it comes with the limitation of being sensitive to the design of the prompt. Rather than scoring individual answers, some authors have also focused on using an LLM to select the best answer among a number of candidates (Wang et al., 2023b), typically to compare the performance of different LLMs. However, care is needed with this approach, as the order in which the answers is presented can influence the result (Wang et al., 2023b). In terms of how ground truth answers or, more