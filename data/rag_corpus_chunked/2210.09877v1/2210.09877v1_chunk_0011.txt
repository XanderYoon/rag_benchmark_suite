Wikipedia annotations are sourced using the Wikifier service [25]. 3.4.1. Evaluation To evaluate if the Jaccard similarity coefficients in relation to RQ1 are significantly different, we use the test statistic of the hypothesis test. We present the performance of the ranking models related to validating RQ2 using NDCG, NDCG@30 and Precision@10 as these are the same evaluation metrics used in prior work [9]. NDCG uses graded relevance instead of binary relevance/non-relevance to focus on highly relevant documents, implying that retrieving a highly relevant document is much more important than retrieving a less important document. Whereas precision measures the number of relevant documents retrieved over the total relevant documents. Similar to the prior work, we calculate NDCG for the entire set of relevant segments available and up-till rank 30. Precision is computed by using a cut-off of top-10 ranked segments. 4. Experimental Results The results of the experiments are reported in this section. Figure 1 shows the difference of medians of Jaccard similarity coefficients, while Table 1 summarises the results obtained from the hypothesis test conducted to verify RQ1. Table 2 outlines the ranking performance obtained in the experiments run in order to verify RQ2. 4.1. Query Keyword Disambiguation using the Context We hyptothesise that the context gathered by the user can also be used to disambiguate the meaning of keywords in a search query. To test this, we take 7 queries from the training dataset that belongs to categories topical and known item (”story about riding a bird” is ignored as it is a query that belongs to the refinding category). We enrich these queries using Wikification in two settings, i) query only vs. ii) query + description, to investigate if the identified Wiki concepts are different. Table 3 reports how the salient entities in the query keywords are detected