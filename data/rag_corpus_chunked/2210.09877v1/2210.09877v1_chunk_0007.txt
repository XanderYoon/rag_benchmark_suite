segment ğ‘ . 3.1.1. Baseline Models We use BM25 and DPH models as probabilistic baseline models for this work. These two models use the textual tokens of the query ğ‘ and the segment ğ‘  to compute the relevance score ğ‘Ÿğ‘’ğ‘™ as per equation 1 where txt identifies the text token features.. ğ‘Ÿğ‘’ğ‘™(ğ‘, ğ‘ ) = ğ‘“ (ğ‘txt, ğ‘ txt) (1) As described in section 2, we implement and test the DCU run 2 model [14] (referred to as DCU model hereafter) that expands the query with entities extracted from the description as per equation 2 that expands equation 1. The entities are extracted using Named Entity Recognition (NER). ğ‘Ÿğ‘’ğ‘™(ğ‘, ğ‘‘, ğ‘ ) = ğ‘“ (ğ‘txt+ğ‘‘ent, ğ‘ txt) (2) where ent represents the entities extracted from the query description ğ‘‘. 3.2. Proposed Models The proposed models in this work uses Wikipedia concepts to enrich the representations used in the IR model. We hypothesise that adding Wikipedia concepts can improve precision of the model as exact entities can be matched between the query + context and the segments. We propose two models that use Wikipedia topics both in query and segment expansion. Wiki_rel , This model expands the query and the document by adding the Wikipedia concepts extracted using both the query ğ‘ and the query description ğ‘‘ as per equation 4. This model replaces the entities in the DCU model by Wikipedia concepts. ğ‘Ÿğ‘’ğ‘™(ğ‘, ğ‘‘, ğ‘ ) = ğ‘“ (ğ‘txt+ ğ‘wiki+ ğ‘‘wiki, ğ‘ txt+ ğ‘ wiki) (3) where Â·wiki represents the concepts extracted from the query ğ‘, description ğ‘‘ and segment ğ‘  . Ent_Wiki_rel , This model expands the DCU model by enriching the query, description and the segment with Wikipedia concepts. This formulation is presented in equation 4. ğ‘Ÿğ‘’ğ‘™(ğ‘, ğ‘‘, ğ‘ ) = ğ‘“ (ğ‘txt+ğ‘‘ent+ ğ‘wiki+ ğ‘‘wiki, ğ‘ txt+ ğ‘ wiki) (4) The two proposed models help us validate RQ2 by