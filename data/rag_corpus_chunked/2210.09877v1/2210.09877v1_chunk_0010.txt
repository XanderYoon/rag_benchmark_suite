prompting the user to provide further clarifications. To answer RQ1, we aim to see if there is a significant alignment between the Wikipedia concepts present in the query and a relevant document in contrast to a non-relevant document. To measure this quantitatively, we measure the Jaccard similarity coefficient ùúå between the query and the document using the sets of Wikipedia concepts. Then, for each query, we take the sets of values ùúårel and ùúånon rel and compare the difference of medians using a one-tailed Mann‚ÄìWhitney U test (ùêªùëéùëôùë°. : ùúårel > ùúånon rel). To answer RQ2, we take the annotated Wikipedia concepts of the query and the podcast segments as additional features. Then we develop a new set of information retrieval algorithms outlined in section 3.1 to account for the alignment of Wikipedia concepts present. Then we use the same Podcast Small dataset to evaluate if the ranking of relevant documents is different between the baselines and the proposed models. To validate how Wikifying the context can be used to disambiguate a query, we ran Wikification on each query using i) the query words exclusively and ii) the query words + the description words, to observe if more precise identification of Wikipedia concepts is carried out. In order to run the evaluation experiments needed for RQ2, we utilise Python Terrier library [27]. The text processing and NER enrichment needed for the DCU model is done using the Spacy library. The Wikipedia annotations are sourced using the Wikifier service [25]. 3.4.1. Evaluation To evaluate if the Jaccard similarity coefficients in relation to RQ1 are significantly different, we use the test statistic of the hypothesis test. We present the performance of the ranking models related to validating RQ2 using NDCG, NDCG@30 and Precision@10 as these are the same evaluation metrics used in