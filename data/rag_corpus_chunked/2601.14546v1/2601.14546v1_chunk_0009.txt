the retriever’s perspec- tive, and we therefore describe these asretriever-centric. These predictors rely on signals available directly from the retrieval process, such as retrieval scores, dense embeddings, and document texts. QPP encompasses a diverse set of methods, each targeting a different aspect of retrieval quality. Some estimate the upper bound of list relevance [56], while others examine the distribution of the retrieval scores [11,50]. Still others focus on the coherence of retrieved documents [21], which can also influence RAG performance. However, all QPP methods treat documents as independent units, making it difficult to capture cross-document dependencies and sentence-level interactions that emerge once the documents are concatenated and presented as a single context to an LLM in RAG [1]. Perplexity-based predictorsIn contrast to QPP approaches, the second cat- egory of predictors isreader-centric, analysing the RAG context and answer from the LLM’s perspective. These predictors leverage the token probabilities assigned by the model to estimate its internal certainty: given a query, how well the retrieved context aligns with the model’s expectations, and given both query and context, how confident it is in its generated answer [35]. A widely-used mea- sure of an LLM’s confidence in its output isanswer perplexity(PerpA), defined as the exponential of the mean negative log-probability of generated answer to- kens [49]. Higher perplexity PerpAis empirically associated with increased risk of hallucination [32,59]. We extend this perplexity-based measurement to evaluate the retrieved con- text before generation. Given a query, if a retrieved context causes a lower perplexity as analysed by an LLM, it may indicate greater consistency with the model’s expectations and inherent knowledge. To distinguish it from post- generation PerpA, we refer to this measure ascontext perplexity(PerpC). Predicting Retrieval Utility and Answer Quality in RAG 7 You are an expert at answering questions based on your own knowledge and