predictors, such combinations can potentially lead to better estimates for both RPP and GPP, as also observed in QPP applications [23]. In particular, we construct an ensemble of the predictors by applying linear regression to capture the relationship between their outputs and the target values —context utility for RPP and answer quality for GPP—on training queries. Formally, L(Φ, Y;w) = X q∈Q (y(q)−w·ϕ(q)) 2,(2) wherew∈R n are the learnable weights of the predictor outputϕ(q)∈R n for queryq,Qis a set of training queries with ground-truth labels, andy(q) denotes either the context utility for RPP, or the answer quality for GPP. This loss function measures the squared error between the predictionw·ϕ(q)and the target metric’s ground truthy(q). Minimising this loss corresponds to finding weightsthatproducepredictionsthatareasclosetothegroundtruthaspossible, thereby achieving higher accuracy in RPP and GPP. 4 Experimental Setup Research Questions.To assess our framework’s ability to predict context utility (RPP) and answer quality (GPP), we pose four research questions. RQ-1 8 Tian et al. evaluates existing QPP methods when applied directly to the two prediction tasks. RQs 2-4 progressively enrich the predictor set in the linear regression model described in Section 3.3, trained for both tasks using the loss in Equa- tion (2). Together, these RQs examine the contribution of different predictor categories and their combinations to RPP and GPP accuracy. –RQ-1: How accurate are existing QPP approaches for RPP and GPP? –RQ-2: Does adding reader-centric context perplexity (PerpC) improve pre- diction accuracy over QPP alone? –RQ-3: Do query-agnostic document quality and readability metrics provide additional predictive value for RPP and GPP? –RQ-4: Does integrating pre-generation predictors with post-generation answer perplexity (PerpA) improve prediction accuracy? Datasets.We evaluate the prediction accuracy of our proposed approaches on Natural Question [37] (NQ), a widely-used open-domain QA dataset. Answer quality is evaluated by F1 score against the provided golden answer [9].