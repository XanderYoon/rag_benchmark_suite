Predicting Retrieval Utility and Answer Quality in Retrieval-Augmented Generation Fangzheng Tian , Debasis Ganguly , and Craig Macdonald University of Glasgow, Glasgow, UK f.tian.1@research.gla.ac.uk,debasis.ganguly@glasgow.ac.uk, craig.macdonald@glasgow.ac.uk Abstract.The quality of answers generated by large language models (LLMs) in retrieval-augmented generation (RAG) is largely influenced by the contextual information contained in the retrieved documents. A key challenge for improving RAG is to predict both the utility of retrieved documents—quantified as the performance gain from using context over generation without context—and the quality of the final answers in terms of correctness and relevance. In this paper, we define two prediction tasks within RAG. The first is retrieval performance prediction (RPP), which estimates the utility of retrieved documents. The second is generation performance prediction (GPP), which estimates the final answer quality. We hypothesise that in RAG, the topical relevance of retrieved doc- uments correlates with their utility, suggesting that query performance prediction (QPP) approaches can be adapted for RPP and GPP. Beyond these retriever-centric signals, we argue that reader-centric features, such as the LLM’s perplexity of the retrieved context conditioned on the in- put query, can further enhance prediction accuracy for both RPP and GPP. Finally, we propose that features reflecting query-agnostic docu- ment quality and readability can also provide useful signals to the pre- dictions. We train linear regression models with the above categories of predictors for both RPP and GPP. Experiments on the Natural Ques- tions (NQ) dataset show that combining predictors from multiple feature categories yields the most accurate estimates of RAG performance. Keywords:Large Language Models·Retrieval Augmented Generation ·Query Performance Prediction·Perplexity·Readability. 1 Introduction Retrieval-Augmented Generation (RAG) is a framework that integrates the in- herent parametric knowledge of large language models (LLMs) with retrieved documents to supplement knowledge [38], and mitigate hallucinations [32]. Fig- ure1showshowalistoftop-kdocumentsretrievedforaqueryactsasasourcefor contextualgenerationviaanLLM,aprocessoftencalled“k-shotgeneration” [24]. Although the standard RAG framework follows