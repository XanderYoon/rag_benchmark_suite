[54]. Predicting Retrieval Utility and Answer Quality in RAG 3 However, different from standard IR, where the end user of the retrieved documents is a human, in RAG, the retrieved documents are consumed by an LLM [46]. As a result, RPP and GPP should account not only for topical rele- vance but also for how the context interacts with the LLM [1]. This motivates us to explore indicators of quality other than relevance alone. One of the factors on which the utility of a document depends is the per- plexity, which captures how misaligned the retrieved context is with an LLM’s own internal knowledge and semantics [52]. To illustrate, in response to a sam- ple query from the NQ dataset (as used in our experiments) “Who won the most MVP awards in the NBA?”, a known relevant document’s content –“the award a record six times. He is also the only player...” is topically related. However, as it omits the player’s name, when used as a RAG context, this docu- ment does not lead to the correct answer despite a zero-shot generation correctly outputting “Kareem Abdul-Jabbar”. Because the player’s name is omitted, the LLM cannot reliably associate this relevant document with the target query, which manifests as higher conditional perplexity. This example suggests that the perplexity of the retrieved context, reflecting the LLM’s uncertainty when conditioning on the input query, can serve as a useful predictive signal for both RPP and GPP. Beyond relevance and perplexity,query-agnosticdocument quality charac- teristics such as readability and relevance priors may also influence the utility of retrieved contexts and the quality of generated answers. As an analogy, for human readers, overly complex text can limit usability [25], and low-quality documents may fail to support any query [4]. In this work, we examine two variants of RPP