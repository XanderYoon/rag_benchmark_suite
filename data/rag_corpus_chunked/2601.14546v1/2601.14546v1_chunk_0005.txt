work shows that context utility depends on multiple factors, including the context length [55], relevant document position [39], prompt struc- ture[12],andknowledgeconflictsbetweentheretrieveddocumentandtheLLM’s parametric knowledge [41]. Existing approaches to predicting RAG answer qual- ity mainly focus on uncertainty estimation. Answer-level semantic uncertainty is used to infer generation quality [14,22,47,52], but typically requires multiple sampled generations, limiting its practicality. Token-level uncertainty has also been used to identify unreliable spans in the answer [33,45,59]. In this line of work, [29] defines context utility as the reduction in answer uncertainty; how- ever, this notion is disconnected from factuality and relevance, which are central to downstream evaluation. Beyond uncertainty-based signals, some supervised methods predict whether retrieval is necessary by estimating query complex- ity [31,58]. More recently, [60,61] directly apply LLMs to assess the utility of individual retrieved documents. Query Performance Prediction (QPP).QPP methods estimate retrieval effectiveness by leveraging information from the top-retrieved documents and the query [6,28,43]. Score-based approaches examine retrieval score distribution to assess the separation between relevant and non-relevant documents [11,13,50], and the coherence of top-ranked results [2]. Embedding-based methods exploit dense query–document representations to capture structural topology [21] and inter-document coherence [51,56]. Supervised models directly use the text of the query and the top-retrieved documents to predict IR metrics [3,16,18,20]. Reader-Oriented Document Evaluations.For human readers, document utility is often assessed through readability [27]. Readability measurements typ- ically rely on features such as sentence length, word difficulty, and syntactic complexity [10,36,53]. Beyond readability, the inherent quality of a document may also limit its usefulness, with some documents containing little or no valu- able information [4]. Recent approaches, such as QualT5 [8], explicitly estimate query-agnosticdocumentquality.InRAG,whethertheseconstraintsaffectLLMs as the new “readers” remains underexplored, highlighting potential differences in how humans and LLMs use retrieved text. Research Gap.Prior work has not yet systematically studied context utility and answer quality