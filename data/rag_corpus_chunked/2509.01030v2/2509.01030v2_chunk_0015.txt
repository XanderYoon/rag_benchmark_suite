provides an ordered list of retrieved answers, we report the quality of the returned list by reporting three more metrics: mean reciprocal rank (MRR@k), normalized discounted cumulative gain (nDCG@k), and precision@k. Here, the MRR@k computes the average rank of the ground-truth item within the list of top-k answers; the nDCG@k is similar to hit ratio but penalizes the result if the ground-truth answer is ranked low; the precision@ k (P@k) reports the average number of answers that are related to the query. As benchmark models, the results after the generator are compared to the scores given by gpt-4o-mini and StreetToPerson [9]. 4.3. Experiments Searcher. We only extracted triplets whose language of the object is English or not specified. The Virtuoso SPARQL endpoint for DBpedia was used with a limitation Identifying Origins of Place Names Via Retrieval Augmented Generation Table 1. Statistics of the three datasets used for fine-tuning the indexer-ranker. In GeoNames, overseas territories of a country are considered as an independent entry for countries (e.g. French Guiana and France have two distinct entries), which explains the high number of countries. Dataset Positive pairs Negative pairs Comments DS:QALD9RDF/XML /commen◎s647 64 401 408 questions DS:GeoNamescountry /gl⌢be-asia650 25 751 252 ”countries” DS:GeoNames /gl⌢be-asia/ci◎y320 958 746 938 252 ”countries” + 10 572 cities of ksearcher = 10K triplets. Additionally, we fixed a maximum of 1000 subjects. For reproducibility, the query was executed once, with all extracted knowledge graphs saved locally for all the experiments. Indexer and ranker. We set kranker as 10, meaning that the indexer-ranker will select the top-10 triplets from the 10K triplets retrieved from the searcher. The knowledge graph extracted by the searcher was split by subject in the triple; each chunk is regarded as a document for ColBERTv2. The maximum length for input is set to 256 tokens. As