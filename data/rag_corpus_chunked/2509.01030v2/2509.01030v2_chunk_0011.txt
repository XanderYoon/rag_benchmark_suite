from DBpedia in RDF/XML format. We built this dataset based on QALD9, which includes a range of questions and their corresponding SPARQL queries on DBpedia [10], denoted as {(X,ySPARQL)}. Given the i-th pair consisting question Xi and query ySPARQL,i, we executed the query and extracted the knowledge graph y+ KG,i, forming the new pair {(Xi,y+ KG,i)}. For training, we also identified negative samples y− KG,i by retrieving nodes in DBpedia that contained one keyword (provided by QALD9) in the query, but that are not returned by executing ySPARQL. Identifying Origins of Place Names Via Retrieval Augmented Generation 3.3. Generator From the top-kranker documents, a generative language model, Llama2 [2], was used to choose the top-1 document and generate the final answer. The top-kranker documents were concatenated in the prompt. To concatenate these documents, we experimented with two ways of ordering these documents: ordering by increasing similarity with the query  ↓kranker 1  and decreasing rank  ↓1 kranker  . In our experiments, we observe that positioning the most relevant information near the tail of the prompt improves the results: due to limitations with long-distance dependencies, the model focuses on the most recent input to the language model, i.e., the tail of the prompt. We applied in-context learning into the design of our prompt, where an example of how to solve the task is provided to the language model. The prompt is designed as follows to retrieve the top-kgenerator(kgenerator = 1) document. For [kranker] knowledge graphs, an extract from the RDF/XML file is provided; the names of the knowledge graphs are: [subject 1]. . .[subjectkranker]. We want to find an answer to: “[ANCHOR QUESTION]”. These are the extracts: [KG 1]. . .[KGkranker]”. Give me a simple answer to “[ANCHOR QUESTION]”. [INSTRUCTIONSa] aInstructions: “Only use the provided information. First, choose