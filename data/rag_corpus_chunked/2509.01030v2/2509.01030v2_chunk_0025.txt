C1 C2a C2b C2c (b) ... restricted to the streets where the KG mentions an origin ( n∗ = 93) Figure 3. Evaluation of the hit ratio on the types sem, geo aus and geo vic after the generator for the models C0, C1, C2a, C2b, C2c and comparison with the baselines gpt-4o-mini and StreetToPerson. C0 C1 C2a C2b C2c Model 0.50 0.25 0.00 HR * HR* after the ... ... searcher ... ranker ... generator Figure 4. Relative change ∆HR∗(modulei,modulei−1) with module i ∈ { searcher, ranker, generator }. The more ∆HR → 0, the more relevant information is selected without loss. can be twofold. First, the model does not understand the background knowledge of naming conventions, e.g., some rules abouthow places are named. During the evaluation, the model tends to over-estimate persons from the world of sport, particularly football and rugby. Second, the information ground-truth answer may be absent in the dataset and the model has to infer from the existing knowledge. For example with Athenaeum Place, no extracted information mentionsAthena but surprisingly, the generator correctly Identifying Origins of Place Names Via Retrieval Augmented Generation infers the correct answer from its own knowledge. This capacity of generalization can be improved with a better quality of the top-10 items provided in the dynamic prompt. In Figure 4, we observe that the generator loses less information after the ranker if the top- kranker has good scores on geo vic in C2a [/gl⌢be-asia+ /commen◎s], which compensates lower results after the ranker: through in-context learning, the generator tends to favor answers related to Victoria or Australia. However, there is still a compromise between spatial or semantic scores, as shown in Figure 3. 6. Discussion and conclusions Our experiment underlines the important differences between spatial proximity and semantic proximity. In our GIR architecture,