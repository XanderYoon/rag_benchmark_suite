better scores for geo aus or geo vic as shown in Figure 2 for example. As a surprising result, training on a fine-grained grid of locations with DS:GeoNames does not improve scores on fine locations geo vic but only on coarse locations with geo aus. We propose two explanations to this observation: first, DS:GeoNames contains more mentions to countries that strengthen the similarities between countries and second, the mention of one city is long-tail information that has little impact on the back propagation of the loss. In contrast, only training on DS:GeoNames country at a coarser spatial level keeps a certain capacity for the language models to generalize. In Figure 2, we show that the fine- tuning on spatial pairs does not fully improve the rankings: qualitatively, the distribution of the results still keeps a high entropy. We then assume that a continual learning first on spatial then RDF/XML understanding may lead to “catastrophic forgetting” of spatial knowledge in C2a [/gl⌢be-asia+ /commen◎s] and C2b [/gl⌢be-asia/ci◎y+ /commen◎s]: that is why we define C2c [/gl⌢be-asia/commen◎s] with a unique training on both skills. Finally, C2c [ /gl⌢be-asia/commen◎s] does not necessarily lead to better performance. By consequence, the paradigm of fine-tuning hardly captures both semantic and spatial understanding in a neural information retrieval only based on a language model. 5.3. Generator Figure 3 and Figure 4 present the results for the generator, discussed further below. Comparison with the baselines. StreetToPerson does not generalize to Australia; gpt-4o-mini provides insightful answers only for common knowledge whereas our models offer better and specialized answers in the long-tail knowledge. Indeed, for StreetToPerson, HR...@1 and HR∗ ...@1 are significantly lower than in our approach. With gpt-4o-mini, HRsem@1 is better than StreetToPerson but still lower than our approach; moreover, gpt-4o-mini correctly retrieves an origin for streets that are named after