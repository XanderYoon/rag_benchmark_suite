cases. Only 57.7% of the streets commemorate a person. Of those, 27.4% commemorate a local inhabitant of the area, such as a merchant or a former land owner, even though they are not explicitly named in the document3. The searcher successfully extracts a knowledge graph from DBpedia for 89.5% of the streets: the missing graphs are due to a lexical gap (e.g., Abeckett Street in the gazetteer instead of A’BeckettStreet) or the absence of information in DBpedia. On average, the resulting dataset has 291 objects per knowledge graph. Among the extracted knowledge graphs, only 37.5% (HR@10 k) contain a mention of the origin (see Table 4). A mention does not necessarily mean that there is an explicit link between a candidate and the naming origin; only that the origin appears in the text. This relatively low score for the searcher indicates that not all the required information is easily accessible in DBpedia, first due to the limitations of the SPARQL endpoint and second due to the prevalence of unnamed persons. 5.2. Ranker Table 4 and Figure 2 present the results for the indexer-ranker, where the subscript sem represents evaluation results in terms of semantic understanding and the subscriptgeo vic and geo aus represent results in terms of spatial understanding (i.e., if the answer is respectively at least within Victoria or Australia). General. Initially pre-trained on English, a non fine-tuned ColBERTv2 C0 [ ∅] can already understand the RDF/XML format of knowledge graphs. However, the quality of the top-k candidates is not high with respect to their spatial distribution, particularly with the nDCG compared with fine-tuned models. To evaluate the models in detail, we must distinguish two cases: firstly, can a model retrieve an information that does exist? Secondly, if missing, can a model retrieve information that is at least spatially