focus on the semantic similarity of the retrieved answer and the textual description of the place name Xplace in the gazetteer. Suppose thatXplace is Nancy Adams Placein Melbourne that is named after a local person in Melbourne. We evaluate if the retrieved answer is semantically relevant to Xplace. For example, if the system identifies the origin as Nancy Adams who lived in Victoria, then the answer is semantically overlapped with the query, and hence, it is deemed as semantically correct to the query. In contrast, the botanist Nancy Adams in New Zealand is considered as erroneous. • Spatial relevance: geo aus, geo vic. Similarly, we measure the spatial similarity of the retrieved answer and the place name Xplace in the query. Specifically, if the retrieved origin is related to the spatial context ofXplace (e.g., both retrieved origin and Xplace are related if they share a same spatial parent from a hierarchical spatial representation for containment; the parent can be the “name of the country” – equal to Australia for geo aus – or the “name of the state” – equal to Victoria for geo vic), we treat it as a correct answer. The evaluation is conducted by one annotator. We report the accuracy of each module in our framework in terms of HR@ k: for N data samples in DS:Gazetteer, on average, what is the probability of the ground-truth answer being in the top- k retrieved answers. Since the indexer-ranker provides an ordered list of retrieved answers, we report the quality of the returned list by reporting three more metrics: mean reciprocal rank (MRR@k), normalized discounted cumulative gain (nDCG@k), and precision@k. Here, the MRR@k computes the average rank of the ground-truth item within the list of top-k answers; the nDCG@k is similar to hit ratio but penalizes the result if