embeddings, Table2VecW and Table2VecE, to compute additional semantic matching features. Specifically, each type of embedding contributes four features, for each of the similarity methods in [15]. Given that both the table and query are vectors now, we compute cosine similarity to measure relevance. For comparison purposes, we employ both methods in [15]: early fusion and late fusion. For the former method, query-table relevance is measured between the cen- troid of query term vectors and the centroid of table term vectors. The latter method computes pairwise cosine similarity between table terms (®tj ) and query terms (®qi ) first, and then aggregates those results. Here, query-table relevance is measured using an aggrega- tor function, which can be: (i) maximum of cosine (®qi , ®tj ), (ii) sum of cosine (®qi , ®tj ) (iii) average of cosine (®qi , ®tj ). In this paper, we com- bine all four measures (i.e., early fusion and late fusion using max, sum, and avg aggregators) to yield the final similarity score. For performance comparison, we employ pre-trained Graph2Vec [11] and Word2Vec embeddings [7]. 4 EV ALUATION In this section, we formulate our research questions (Sect. 4.1), discuss our experimental setup (Sect. 4.2), and then present our results and analysis for the three tasks (Sects. 4.3–4.5). 4.1 Research Questions We address the following research questions: RQ1 Can Table2Vec improve table population performance against state-of-the-art baselines? RQ2 Does the training of word embeddings specifically on tables, as opposed to news, affect retrieval performance? RQ3 Which of the semantic representations (entity vs. word em- beddings) performs better in table retrieval? Table 2: Statistics for Table2Vec embeddings. Neg is short for negative sampling (measured in number of words). Embedding Total terms Unique terms Neg Win_size Table2VecW 200,157,990 1,829,874 25 5 Table2VecH 7,962,443 339,433 25 20 Table2VecE 24,863,683 2,159,467