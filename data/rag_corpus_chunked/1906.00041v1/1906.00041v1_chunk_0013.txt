(p < 0.01). The lack of difference between the two indicates that it does not make a difference for the table retrieval task whether word embeddings are trained specifically on tables or not (RQ2). As for the different semantic representations (RQ3), these results show that word embeddings are more beneficial for table retrieval than entity embeddings. 5 CONCLUSION AND FUTURE WORK In this paper, we have introduced Table2Vec, a neural language model for training word and entity embeddings on various table elements. These embeddings have been utilized in three particu- lar table-related tasks, and have been shown to significantly im- prove retrieval effectiveness. We have derived these embeddings particularly from a Wikipedia tables corpus, which contains only high-quality relational tables. In the future, we wish to extend our research to other table corpora, as well as to other types of tables. REFERENCES [1] Julian Eberius Wolfgang Lehner Robert Wrembel Ahmad Ahmadov, Maik Thiele. 2015. Towards a Hybrid Imputation Approach Using Web Tables. In Proc. of BDC ’15. 21–30. [2] Chandra Sekhar Bhagavatula, Thanapon Noraset, and Doug Downey. 2015. TabEL: Entity Linking in Web Tables. In Proc. of ISWC ’15. 425–441. [3] Michael J. Cafarella, Alon Halevy, and Nodira Khoussainova. 2009. Data Integra- tion for the Relational Web. Proc. of VLDB Endow. 2 (2009), 1090–1101. [4] Anish Das Sarma, Lujun Fang, Nitin Gupta, Alon Halevy, Hongrae Lee, Fei Wu, Reynold Xin, and Cong Yu. 2012. Finding Related Tables. In Proc. of SIGMOD ’12. 817–828. [5] Anna Lisa Gentile, Petar Ristoski, Steffen Eckel, Dominique Ritze, and Heiko Paulheim. 2017. Entity Matching on Web Tables: a Table Embeddings approach for Blocking. In Proc. of EDBT ’17. 510–513. [6] Majid Ghasemi-Gol and Pedro A. Szekely. 2018. TabVec: Table Vectors for Classi- fication of Web Tables. CoRR (2018). [7] Tomas Mikolov, Ilya Sutskever, Kai Chen,