0.6588† Baseline + Graph2Vec 0.5764 ◦ 0.6340◦ Baseline + Table2VecW 0.6096 ‡ 0.6505† Baseline + Table2VecE 0.5569 ◦ 0.6161◦ worth pointing out that the performance of this combined method improves more with more seed entities than the baseline BL1, which reaches its peak already after two seed entities. This indicates the seed entities are better utilized in our embedding-based method. 4.4 Column Population Table 4 shows column population performance. We find that the combined method involving Table2VecH significantly outperforms the baseline method ( p < 0.01) in terms of MAP when |L| > 1. For |L| = 3 it achieves substantial and significant improvements (p < 0.01) both in terms of MAP and MRR. Moreover, while the baseline performance does not improve with more seed headings, the combined method can effectively utilize larger input sizes and keeps improving the performance. Combining these findings with the results obtained in Sect. 4.3, we answer RQ1 positively. The in- terpolation parameter (cf. Eq. (5)) that yielded the best performance for the combined method isα= 0.01, which indicates Table2VecH similarity is assigned much higher importance than the baseline. 4.5 Table Retrieval To answer RQ2 and RQ3, we list the table retrieval results in Table 5. For Graph2Vec and Table2VecE, we achieve improvements over the baseline but these are not statistically significant. Table2VecW and Word2Vec have very comparable performance to each other and they outperform all other methods and significantly improve over the baseline method (p < 0.01). The lack of difference between the two indicates that it does not make a difference for the table retrieval task whether word embeddings are trained specifically on tables or not (RQ2). As for the different semantic representations (RQ3), these results show that word embeddings are more beneficial for table retrieval than entity embeddings. 5 CONCLUSION AND FUTURE