exper- iments in [14, 15]. The test inputs and ground truth assessments are obtained for the three tasks as follows: • Row population: we use the test set from [14]. It contains 1000 relational tables, of which each table has at least six rows and four columns. For evaluation, we take entities from the first i rows (i ∈ [1..5]) as seed entities, and the remaining entities as ground truth. The test set contains 21,502 unique entities. • Column population: we use the test set from [14], consisting of 1000 relational tables. Headings from the first j columns (j ∈ [1..3]) are taken as seed headings, while the rest constitute the ground truth. There are a total of 7,216 unique column headings. • Table retrieval: we use a set of 60 queries (two query subsets, QuerySet 1 and QuerySet 2) and corresponding ground truth relevance labels from [15], a total of 3,120 query-table pairs. 4.3 Row Population The row population results are listed in Table 3. The top three lines show the results of the baselines from the literature. The bot- tom three lines are the results of combining the baselines with Table2VecE*. Note that the combination involves a mixture pa- rameterα(cf. Eq. (4)). To understand the potential of using table embeddings, we perform a grid search in steps of 0.1 for the value ofα, and report results using theαvalue that yielded the best MAP score. The best performingαvalues for BL1, BL2, and BL3 are 0.4, 0.0, and 0.1, respectively. This means that the second baseline does not contribute at all to the combination. Overall, we find that the combined methods outperform the respective baselines substantially and significantly (p < 0.01). BL1 + Table2VecE* yields the best performance in terms of MAP. It is Table 3: Row population performance. Statistical