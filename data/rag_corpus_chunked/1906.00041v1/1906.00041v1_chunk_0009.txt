as opposed to news, affect retrieval performance? RQ3 Which of the semantic representations (entity vs. word em- beddings) performs better in table retrieval? Table 2: Statistics for Table2Vec embeddings. Neg is short for negative sampling (measured in number of words). Embedding Total terms Unique terms Neg Win_size Table2VecW 200,157,990 1,829,874 25 5 Table2VecH 7,962,443 339,433 25 20 Table2VecE 24,863,683 2,159,467 25 50 Table2VecE* 5,367,837 1,285,708 25 50 4.2 Experimental Setup For table population, we use Mean Average Precision (MAP) as the main metric and Mean Reciprocal Rank (MRR) as a supplementary metric for performance evaluation. Table retrieval performance is evaluated by Normalized Discounted Cumulative Gain (NDCG) with a cut-off at 10 and 20. To test significance, we use a two-tailed paired t-test and write ◦ to denote not significant, and†/‡ to denote significance at the 0.05 and 0.01 levels, respectively. We use the Wikipedia Tables corpus [14], which contains 1.6 mil- lion high-quality relational tables, both for training the Table2Vec embeddings and for the retrieval experiments. For the word-based embedding, Table2VecW, we filter out empty strings, numbers, HTML tags, and stopwords from the raw text during training to obtain a better representation. For Table2VecH, we employ no nor- malization for the headings, i.e., “year(s), ” “year:, ” and “year” will be treated as different headings in our experiment. Table 2 shows the statistics of different Table2Vec embeddings. DBpedia is used as our knowledge base, which is consistent with the original exper- iments in [14, 15]. The test inputs and ground truth assessments are obtained for the three tasks as follows: • Row population: we use the test set from [14]. It contains 1000 relational tables, of which each table has at least six rows and four columns. For evaluation, we take entities from the first i rows (i ∈ [1..5])