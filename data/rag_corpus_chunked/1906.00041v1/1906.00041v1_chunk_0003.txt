substantial and significant improvements over the baseline. Especially, when the number of seed headings becomes larger, it achieves 40% relative improvement in NDCG@10 over the baseline. The resources developed in this work are made publicly available at https://github.com/iai-group/sigir2019-table2vec. arXiv:1906.00041v1 [cs.IR] 31 May 2019 2 TRAINING TABLE2VEC EMBEDDINGS In this section, we first introduce the neural model for training embeddings (Sect. 2.1), and then detail four variants of table em- beddings (Sect. 2.2). 2.1 Neural Model for Training Embeddings We base the training of our embeddings on the skip-gram neural network model of Word2Vec [7]. It is a computationally efficient two-layer neural language model that learns the meaning of terms from raw sequences and maps those terms to a vector space, such that similar terms close to each other. More formally, given a sequence of training terms t1, t2, . . . ,tn, the objective is to maximize the average log probability: 1 n nÕ i=1 Õ −c ≤j ≤c, j,0 log p(ti+j |ti ) , (1) where c is the size of training context, and the probabilityp(ti+j |ti ) is calculated using the following softmax function: p(to |ti ) = exp( ®v ′ to ⊤ ®vti ) ÍV t =1 exp( ®v ′ t ⊤ ®vti ) , (2) where V is the size of vocabulary, and ®vti and ®v ′ to are the input and output vector representations of term t, respectively. Semantically similar terms share more similar vector representations; accord- ingly, the dot product between those vectors results in higher values, which means higher probabilities after softmax. In our scenario, we consider terms to be words, entities, or head- ings in a table. We also employ negative sampling, to make the training of our models computationally more efficient. 2.2 Four Variants We train four different table embeddings, using