representations; accord- ingly, the dot product between those vectors results in higher values, which means higher probabilities after softmax. In our scenario, we consider terms to be words, entities, or head- ings in a table. We also employ negative sampling, to make the training of our models computationally more efficient. 2.2 Four Variants We train four different table embeddings, using different table el- ements as input; these are summarized in Table 1 and illustrated in Fig. 1. All embeddings are trained using the same neural model, but they differ in (i) what constitutes as a term and (ii) which table elements are used for training. Table2VecW This method takes all the words appearing in a table into consideration. Specifically, it considers the page title, section title, table caption, table headings, and all table cells; see Fig. 1a. Table2VecH Instead of using single words, we further leverage the table structure and represent tables as sequences of headings. Each heading is treated as a single term, as is shown in the shadowed area in Fig. 1b. Table2VecE Tables often contain entities, which are semantically more meaningful than words. Thus, we take sequences of entities as input, by extracting all entities that appear within table cells; see the shadowed area in Fig. 1c. Table2VecE* Relational tables describe a set of entities as well as their attributes in the columns. These entities are placed in the core column. Table2VecE* considers only entities in the core column of the table, as is shown in Fig. 1d. 3 UTILIZING TABLE2VEC EMBEDDINGS In this section, we extend previous table population and retrieval methods by incorporating the Table2Vec embeddings that we in- troduced in Sect. 2. For all tasks, we keep our focus on relational Table 1: Table2Vec embeddings. Method Input Semantic repr. Table2VecW all table data word