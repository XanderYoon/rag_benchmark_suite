both pre- trained ones and task-specific ones. For example, Zhang and Balog [15] use pre-trained word and entity embeddings for table retrieval. Ghasemi-Gol and Szekely [6] develop table embeddings for table classification and Gentile et al. [5] train table embeddings for web table entity matching. However, to the best of our knowledge, no studies have been conducted on training table embeddings specif- ically for table population and retrieval tasks. To fill the gap, we propose Table2Vec, a neural language modeling approach to map different table elements into semantic vector spaces, with specific table-oriented tasks in mind. In this study, we train four variants of table embeddings by utilizing different table elements. Specifically, word embeddings (Table2VecW) consider all the terms within a table, and are lever- aged for table retrieval. The method employing Table2VecW outper- forms a start-of-the-art baseline by over 10% in terms of NDCG@10. Interestingly, this is on par with using pre-trained Word2Vec em- beddings using Google News data. Two different entity embeddings are obtained by considering only core column entities (Table2VecE*) and all table entities (Table2VecE). Table2VecE* is employed for the row population task. We show that it significantly outperforms all baselines. Combining with an effective baseline can lead to further improvements. Table2VecE is employed in table retrieval and can yield minor improvements, albeit those are not statistically sig- nificant. Heading embeddings (Table2VecH) are generated for the column population task by utilizing table headings. Table2VecH re- sults in substantial and significant improvements over the baseline. Especially, when the number of seed headings becomes larger, it achieves 40% relative improvement in NDCG@10 over the baseline. The resources developed in this work are made publicly available at https://github.com/iai-group/sigir2019-table2vec. arXiv:1906.00041v1 [cs.IR] 31 May 2019 2 TRAINING TABLE2VEC EMBEDDINGS In this section, we first introduce the neural model for training embeddings (Sect.