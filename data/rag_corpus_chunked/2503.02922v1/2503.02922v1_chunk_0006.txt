[12], popularized by Neo4J [29] and Lla- maIndex [23], and introduce TREX, a novel and cost-effective al- ternative that captures semantic depth while preserving coherence within each text chunk. Our key contributions are as follows: • We develop TREX, which provides a balanced cost-performance solution suitable for both OLAP and OLTP query types. Optimizing open-domain question answering with graph-based retrieval augmented generation • We benchmark graph-based RAG techniques to identify best practices and optimal scenarios for their application, providing insights into when each approach is most effec- tive. • We introduce new metrics for evaluating faithfulness, show- ing where our method surpasses existing approaches, and report comparative win rates for a clearer understanding of each methodology’s relative performance. • Finally, we demonstrate TREX’s effectiveness in a real- world technical support setting, showcasing its utility with unstructured customer support data. Our extensive benchmarking identifies three key challenges in leveraging LLMs for open-domain question answering: (i) retrieval quality is critical to final performance and presents substantial room for improvement; (ii) state-of-the-art LLMs continue to face difficulties in synthesizing multiple documents without being influ- enced by irrelevant information; and (iii) evaluating RAG systems remains challenging due to the lack of a standardized approach for open-domain QA. These challenges pose promising research directions for devel- oping better systems integrating retrieval and LLMs. 2 RELATED WORK Text summarization is a foundational task in open-domain QA, with methods broadly categorized as extractive (selecting key sentences) and abstractive (generating new sentences). Hybrid approaches blend both techniques [13]. Summarization supports both OLTP- style and OLAP-style queries by condensing large text volumes into accessible gists. While extractive methods suffice for fact-based OLTP queries, OLAP-style queries, requiring synthesis across mul- tiple documents, benefit from recursive and hybrid summarization approaches. Advances in deep learning, particularly large language models (LLMs), have blurred