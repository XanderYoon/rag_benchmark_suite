. . . , ğ‘ ğ‘™=1 ğ‘—ğ‘š ) where ğ‘š is the number of nodes in cluster ğ‘— at level ğ‘™ = 1. This hierarchical process continues until reaching the root node ğ‘ ğ¿, which effectively summarizes the entire document ğ·. The resulting hierarchical tree structure is illustrated in Figure 2 [35]. 3.2.2 Retrieval and Ranking. All summary and leaf nodes are stored in a vector database, in our case, we use Azure AI Search Services [28], enabling retrieval of the most relevant nodes in response to Figure 2: An example of a hierarchical tree structure built from a set of text chunks ending with the root node. The summary nodes generated by a LLM are then inserted into a vector database [46]. user queries. The user query is then encoded using the text-ada- embedding-002 model, and cosine similarity is computed between the query and each summary and leaf node. The top-k nodes are then retrieved and combined with additional nodes from a sec- ondary retrieval mechanism, which uses keyword search on the full text of the query to match terms in the nodes stored in our vector index. The top-k results from this secondary retrieval are then aggregated with the results from the primary retrieval and re-ranked using Reciprocal Rank Fusion (RRF) [28]. RRF is an unsupervised method that combines rankings from multiple systems, shown to outperform other ranking fusion meth- ods such as Condorcet Fuse and CombMNZ [28]. The RRF score for each text chunk, or summary and leaf node, ğ‘‘ is calculated as: RRF(ğ‘‘ âˆˆ ğ·) = âˆ‘ï¸‚ ğ‘Ÿ âˆˆğ‘… 1 ğ‘˜ + ğ‘Ÿ (ğ‘‘) where ğ‘˜ is a constant (set to the systemâ€™s default configuration of 60) to reduce the influence of document rank, and ğ‘Ÿ (ğ‘‘) is the rank of document ğ‘‘ in the initial set of