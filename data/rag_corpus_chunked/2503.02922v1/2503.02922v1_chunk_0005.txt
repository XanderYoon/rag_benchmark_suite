customers that are exclusively interested in broader, activity- driven, OLAP-type, while others are primarily interested in local, data-driven, OLTP-style queries on their data. While ongoing focus groups need to be conducted to better quantify this distribution, recognizing these query types has directly shaped our selection of four benchmarking datasets, as illustrated by Figure 1. Through our engagements, we have gained valuable insights into the types of workloads and queries that customers expect our LLM applications to handle, as well as the pressing need for im- proved methods to track and assess performance regressions. Yet, evaluating RAG systems remains a significant challenge due to the absence of standardized approaches for open-domain QA. Current evaluation methods are largely benchmark-driven; for example, datasets like HotPotQA—a widely used multi-hop QA benchmark derived from Wikipedia—include human-annotated ground-truth answers, allowing for direct accuracy measurement by comparing model responses against verified correct answers [45]. However, other datasets, such as the Kevin Scott Podcasts [12], introduce a different challenge—open-ended questions (e.g., “What is happen- ing in the technology sector?”) where multiple responses are valid. This highlights a critical gap in the industry: the need for robust evaluation frameworks and best practices to consistently judge the quality and correctness of open-domain QA systems [6]. Develop- ing standardized metrics and methodologies will be essential to ensuring reliable and meaningful performance assessments across diverse QA applications. 1.2 Our Contributions We examine two prominent graph-based RAG methodologies, RAP- TOR [35] and GraphRAG [12], popularized by Neo4J [29] and Lla- maIndex [23], and introduce TREX, a novel and cost-effective al- ternative that captures semantic depth while preserving coherence within each text chunk. Our key contributions are as follows: • We develop TREX, which provides a balanced cost-performance solution suitable for both OLAP and OLTP query types. Optimizing open-domain question answering with graph-based retrieval