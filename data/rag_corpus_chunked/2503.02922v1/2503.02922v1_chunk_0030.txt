Q4. https: //www.microsoft.com/en-us/Investor/earnings/FY-2024-Q4/performance? msockid=1e222da9c09667dd09493fd3c1d566de. Accessed: 2024-11-06. [27] Microsoft Corporation. 2024. GraphRAG. https://github.com/microsoft/graphrag. Accessed: 2024-11-06. [28] Microsoft Corporation. 2024. Relevance scoring in hybrid search using Recipro- cal Rank Fusion (RRF). https://learn.microsoft.com/en-us/azure/search/hybrid- search-ranking. Accessed: 2024-11-06. [29] Neo4j. 2024. The GraphRAG Manifesto: Adding Knowledge to GenAI. https: //neo4j.com/blog/graphrag-manifesto/. Accessed: 2024-11-13. [30] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2024. Unifying Large Language Models and Knowledge Graphs: A Roadmap. IEEE Transactions on Knowledge and Data Engineering 36, 7 (July 2024), 3580–3599. https://doi.org/10.1109/tkde.2024.3352100 [31] Stephen Robertson, Hugo Zaragoza, et al . 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333–389. [32] Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, and Saravan Rajmohan. 2024. Exploring llm-based agents for root cause analysis. In Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering . 208–219. [33] Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. 2023. Questions Are All You Need to Train a Dense Passage Retriever. arXiv:2206.10658 [cs.CL] https://arxiv.org/abs/2206.10658 Cahoon et al. [34] Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, and Dhagash Mehta. 2024. HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction. arXiv:2408.04948 [cs.CL] https://arxiv.org/abs/2408.04948 [35] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. 2024. Raptor: Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059 (2024). [36] Kevin Scott. 2024. Behind the Tech with Kevin Scott. https://www.microsoft. com/en-us/behind-the-tech. Accessed: 2024-11-06. [37] Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021. Do long-range language models actually use long-range context? arXiv preprint arXiv:2109.09115 (2021). [38] Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit Iyyer. 2023. Pearl: Prompting large language models to plan and