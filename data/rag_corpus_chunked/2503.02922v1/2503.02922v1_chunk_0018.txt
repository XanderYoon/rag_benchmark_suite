Costs are computed based on the use of GPT-4o ($2.5 per 1 million input and $10 per 1 million output tokens) and text-ada-embedding-002 ($0.10 per 1 million tokens) calls. Note that the costs for GraphRAG does not include the use of cost-effective alternatives like small language models for indexing. Benchmark GraphRAG RAPTOR HybridRAG TREX MSMARCO $0.04 $0.01 $0.01 $0.01 HotPotQA $0.03 $0.01 $0.01 $0.01 Kevin Scott $1.41 $0.01 $0.01 $0.01 Earnings $0.20 $0.10 $0.13 $0.09 Table 2: Comparison of per question average querying costs across the four benchmarks on GraphRAG, RAPTOR, TREX, and HybridSearch methods. Costs are computed based on the use of GPT-4o ($2.5 per 1 million input and $10 per 1 million output tokens) and text-ada-embedding-002 ($0.10 per 1 million tokens) calls. 5.1 OLTP-style Benchmarks 5.1.1 MSMARCO. In terms of accuracy, TREX outperforms Graph- RAG, RAPTOR, and Hybrid Search, achieving approximately 50% accuracy on a sample of 1,000 questions from the MSMARCO bench- mark. When filtering out questions without sufficient context— where the top-10 hyperlinks from the Bing search engine do not provide enough relevant information—we are left with a subset of 735 questions. The ground truth answers for these filtered questions were labeled by human annotators with variations of the response indicating insufficient context to answer the question. Typically, TREX is able to match these non-answers; nevertheless, on the filtered subset of 735, RAPTOR achieves the highest accuracy with 64.3%, followed closely by TREX at 62.7%, Hybrid Search at 61.7%, and GraphRAG at 54.9%. The accuracy results are summarized in Figure 3. In terms of indexing cost, both RAPTOR and TREX demonstrate savings over GraphRAG as shown in Table 1. Querying costs are also lower for RAPTOR and TREX, as indicated in Table 2, suggesting that these methods are better suited for OLTP-style benchmarks. Given that the