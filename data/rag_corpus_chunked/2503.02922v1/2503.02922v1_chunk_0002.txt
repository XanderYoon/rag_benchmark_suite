text, while OLAP-style queries are open-ended, thematic, and require aggregating, synthesizing and abstracting information across multiple documents [47]. Just as operational databases are optimized for OLTP tasks and data warehouses for OLAP workloads, specialized LLM applications are now emerging to address these distinct query types, with tailored approaches for both OLTP-like and OLAP-like QA tasks. Retrieval-augmented generation (RAG) has become a widely adopted LLM-based approach for open-domain QA, leveraging var- ious retrieval strategies that adapt to user input queries. In many commercial applications today, the typical RAG workflow involves indexing large volumes of text, using either inverted indices or dense vector encodings, and then retrieving the most relevant infor- mation, which often includes incorporating re-rankers to refine the retrieved documents [6, 14]. This retrieved context is subsequently used as input for the generation component, which often involves a language model to generate a response. This straightforward yet powerful methodology is effective for extractive tasks or OLTP- style applications, where answers can be located within a single text snippet embedding—in other words, analogous to a database scenario where a simple key-value lookup suffices [47]. However, in OLAP-style settings—where queries are open-ended and require synthesizing information from multiple documents (e.g., “How is artificial intelligence impacting global job markets?” arXiv:2503.02922v1 [cs.IR] 4 Mar 2025 Cahoon et al. or “What are the latest technology trends?”)—existing RAG systems still struggle to retrieve the most relevant information. Although recent advancements in hardware and algorithms have increased the context lengths that models can process [10], the importance of precise context selection remains. Issues such as context stuffing [16], the “lost in the middle” phenomenon [22], and diminishing model performance with singular context persists [37]. While some models [40] achieve both high recall and precision, using such long-context modalities can be costly and slow, underscoring the ongoing need