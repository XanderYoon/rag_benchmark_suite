ğ‘ƒ (ğ‘¥) [25, 35]. However, text embeddings from the text-ada-embeddings-002 model are high-dimensional (ğ‘‘), which can introduce computational inefficiencies and noise. To address this, we apply Uniform Manifold Approximation and Projection (UMAP) as done in [35] to reduce the dimensionality from ğ‘‘ to ğ‘‘â€² [24]. These lower-dimensional representations (ğ‘‘â€²-dimensional vectors) are then used as input to the GMM clustering process. UMAP provides flexible control over neighborhood size, allowing us to capture both global and local structures in the data [25]. The optimal number of clusters ğ‘˜ is selected automatically using the Bayesian Information Criterion (BIC), which balances model complexity and goodness of fit: ğµğ¼ğ¶ = log(ğ‘›)ğ‘ âˆ’ 2 log(ğ¿Ë†) where ğ¿Ë† is the maximized likelihood of the model, ğ‘› is the number of text chunks, and ğ‘ represents the number of model parameters, which is a function of ğ‘˜. By leveraging BIC, we identify the model with the optimal ğ‘˜, ensuring robust clustering performance [35]. After defining clusters, we use an LLM to generate a summary node for each cluster. At the first level, a summary node is created by ğ‘ ğ‘™=1 ğ‘– = ğ¿ğ¿ğ‘€ (ğ‘ğ‘–1, ğ‘ğ‘–2, . . . , ğ‘ğ‘–ğ‘š ) where ğ‘š is the number of chunks in cluster ğ‘– at level ğ‘™ = 0. These summary nodes are subsequently em- bedded and clustered again, creating higher-level summary nodes that recursively summarize the summaries from the previous level: ğ‘ ğ‘™=2 ğ‘— = ğ¿ğ¿ğ‘€ (ğ‘ ğ‘™=1 ğ‘—1 , ğ‘ ğ‘™=1 ğ‘—2 , . . . , ğ‘ ğ‘™=1 ğ‘—ğ‘š ) where ğ‘š is the number of nodes in cluster ğ‘— at level ğ‘™ = 1. This hierarchical process continues until reaching the root node ğ‘ ğ¿, which effectively summarizes the entire document ğ·. The resulting hierarchical tree structure is illustrated in Figure 2 [35]. 3.2.2 Retrieval and Ranking. All summary and leaf nodes are