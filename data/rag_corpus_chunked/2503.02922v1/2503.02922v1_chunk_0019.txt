Search at 61.7%, and GraphRAG at 54.9%. The accuracy results are summarized in Figure 3. In terms of indexing cost, both RAPTOR and TREX demonstrate savings over GraphRAG as shown in Table 1. Querying costs are also lower for RAPTOR and TREX, as indicated in Table 2, suggesting that these methods are better suited for OLTP-style benchmarks. Given that the MSMARCO benchmark includes annotations of the true evidence used to answer each anonymized query, we report precision and recall metrics based on the retrieved context in the fi- nal prompt across the strategies we benchmarked. While substring matching against the ground truth strings provides a basic measure of precision and recall, it is often imprecise for assessing factual correctness due to variations in phrasing and granularity. To ad- dress this, we developed a token-based precision and recall method, where the ground truth context is tokenized by converting all text to lowercase and splitting by non-alphanumeric characters. This approach ensures a more granular and flexible evaluation, account- ing for variations in wording while maintaining alignment with the essential information needed to answer the query accurately. As shown in Table 3, RAPTOR achieves the highest recall, fol- lowed by GraphRAG LocalSearch, HybridSearch, TREX, and finally GlobalSearch. In terms of precision, RAPTOR also ranks highest, followed by TREX, HybridSearch, and then GraphRAG. However, precision values are notably low across all methods due to the na- ture of retrieved context injectionâ€”much of the retrieved content is not explicitly included in the ground truth annotations, leading to an apparent drop in precision. This highlights a fundamental limitation in evaluation: ground truth labels often underrepresent Optimizing open-domain question answering with graph-based retrieval augmented generation the full range of relevant information, making precision an inher- ently conservative metric. These findings emphasize the trade-off between high recall and