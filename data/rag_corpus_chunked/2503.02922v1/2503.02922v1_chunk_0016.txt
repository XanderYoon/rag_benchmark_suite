between the two modalalities via Reciprocal Rank Fusion (RRF) to optimize ranking. Additional features include vector field filtering (e.g., time span, category) and custom re-ranking models. We use default set- tings to evaluate its effectiveness on OLAP and OLTP benchmarks [28]. 4.2.4 Oracle. For HotPotQA and MSMarco, we establish an Or- acle baseline using annotator-highlighted context, evidence they directly used to answer the query at hand. The gap from 100% accu- racy reflects the LLM’s limitations in reasoning and fact synthesis, highlighting areas where the model struggles to fully connect and integrate relevant information [3, 45]. 4.3 Evaluation Ensuring the faithfulness and correctness of LLM-generated an- swers is a key challenge in open-domain QA. A high-quality re- sponse should be topical, accurate, and coherent while aligning with the input query and retrieved context. Evaluating OLTP-style queries is straightforward, as they have well-defined ground-truth answers. However, OLAP-style queries, such as “What is happening in the technology industry?”, introduce subjectivity, as multiple valid responses exist. This variability complicates the definition of correctness and necessitates a more nuanced evaluation approach [12]. Given the scale of our benchmarks, variability in human judg- ments, and time constraints of our domain experts, we rely on LLM-as-a-judge for evaluation. Prior studies have shown 80%+ agreement between human and LLM assessments in MT-Bench and Chatbot Arena [49]. Future work will refine this framework to better align with expert grading criteria, improving evaluation consistency. 4.3.1 LLM-Based Evaluation. To assess answer quality, we apply LLM-as-a-judge for both OLTP and OLAP-style benchmarks: • OLTP Evaluation: Since answers have clear ground truths, we compare model-generated responses against reference answers, achieving 99%+ agreement. To ensure consistency, we apply a logit bias, restricting outputs to “YES” (correct) or “NO” (incorrect). Cahoon et al. Figure 3: Comparison of accuracy of answers generated by TREX, GraphRAG,