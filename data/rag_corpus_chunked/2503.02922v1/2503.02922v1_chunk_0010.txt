into any RAG system that requires integrating information across documents to improve the accuracy of retrieval and response [28]. 3.2.1 Hierarchical Clustering and Summarization. TREX leverages RAPTOR [35] to first construct a hierarchical tree structure, T , from the text chunks ğ· using a hierarchical clustering algorithm, represented as T (ğ·). Each node in this tree is a summary of the child nodes below it. Given a corpus divided intoğ‘› chunks, the tree will haveğ‘› leaf nodes, each representing a text chunk(ğ‘1, ğ‘2, . . . , ğ‘ğ‘›) [35]. At the first level, we apply a Gaussian Mixture Model (GMM) to cluster text chunks into ğ‘˜ clusters. GMMs assume that data arise from a mixture of several Gaussian distributions [25]. Each text chunk ğ‘¥ğ‘– is initially represented as a ğ‘‘-dimensional vector, de- rived from embeddings generated by the text-ada-embeddings-002 model [? ]. The likelihood that a chunk ğ‘¥ğ‘– belongs to cluster ğ‘˜ is given byğ‘ƒ (ğ‘¥ğ‘– |ğœƒğ‘˜ ) = 1 (2ğœ‹ )ğ‘‘/2 |Î£ğ‘˜ |1/2 exp (ï¸‚ âˆ’ 1 2 (ğ‘¥ğ‘– âˆ’ ğœ‡ğ‘˜ )ğ‘‡ Î£âˆ’1 ğ‘˜ (ğ‘¥ğ‘– âˆ’ ğœ‡ğ‘˜ ) )ï¸‚ , where Î£ğ‘˜ and ğœ‡ğ‘˜ represent the covariance matrix and mean vec- tor of cluster ğ‘˜, respectively. The overall probability distribution for ğ‘ƒ (ğ‘¥) is then given by ğ‘ƒ (ğ‘¥) = âˆ‘ï¸ğ¾ ğ‘˜=1 ğœ‹ğ‘˜ ğ‘ƒ (ğ‘¥ |ğœƒğ‘˜ ), where ğœ‹ğ‘˜ is the weight for that ğ‘˜th cluster. The GMM model is then trained to maximize the likelihood of the data, ğ‘ƒ (ğ‘¥) [25, 35]. However, text embeddings from the text-ada-embeddings-002 model are high-dimensional (ğ‘‘), which can introduce computational inefficiencies and noise. To address this, we apply Uniform Manifold Approximation and Projection (UMAP) as done in [35] to reduce the dimensionality from ğ‘‘ to ğ‘‘â€² [24]. These lower-dimensional representations (ğ‘‘â€²-dimensional vectors) are then used as input to the GMM clustering process.