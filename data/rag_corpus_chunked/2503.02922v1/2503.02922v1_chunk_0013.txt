score for each text chunk, or summary and leaf node, ğ‘‘ is calculated as: RRF(ğ‘‘ âˆˆ ğ·) = âˆ‘ï¸‚ ğ‘Ÿ âˆˆğ‘… 1 ğ‘˜ + ğ‘Ÿ (ğ‘‘) where ğ‘˜ is a constant (set to the systemâ€™s default configuration of 60) to reduce the influence of document rank, and ğ‘Ÿ (ğ‘‘) is the rank of document ğ‘‘ in the initial set of rankings ğ‘… from each retreival technique. This method rewards higher-ranked documents while still considering those with lower ranks [28]. For all benchmarking results in Section 4, we use the top five results retrieved by TREX from the fused ranks of keyword and cosine similarity search as context to generate responses to user queries. 4 BENCHMARKING SETUP We benchmark TREX alongside GraphRAG and Azure AI Hybrid Search on four publicly available datasets. This selection reflects the range of question types commonly asked by customers, en- compassing both the OLTP- and OLAP-style type of queries across diverse topics. The chosen QA datasets include questions that re- quire straightforward responses as well as those demanding syn- thesis from multiple documents. A summary of our benchmarks is provided in Figure 1 and described in more detail below. Optimizing open-domain question answering with graph-based retrieval augmented generation 4.1 Datasets These datasets are all common testbeds for knowledge-intensive tasks and known for their construction of complex questions that require synthesizing information from multiple documents. â€¢ HotPotQA: A multi-hop QA dataset from Wikipedia, primar- ily OLTP-style, requiring synthesis across multiple articles [45]. To mitigate data leakage, we use the dev split (7,405 questions), filtering out queries directly answerable with- out any context by GPT-4, resulting in 5,491 benchmark questions. â€¢ MSMarco: A large-scale dataset of crowdsourced queries from Bing search logs [3]. It includes over a million queries with annotated, synthesized answers. We sample 1,000 ques- tions