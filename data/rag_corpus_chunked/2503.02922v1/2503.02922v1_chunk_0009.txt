an open-domain question 洧 and a large body of text 洧냥, we want to develop a system that can efficiently retrieve the most relevant information from 洧냥, or a list of text chunks,Retrieve(洧, 洧냥) = {洧녫1, 洧녫2, . . . , 洧녫洧녲 }, to generate the correct answer 洧냢 given possible answers 洧냢틙 to the question 洧. In other words, we want: 洧냢 = arg max 洧냢틙 S (洧냢틙|洧, Retrieve(洧, 洧냥)) where S is a scoring function that evaluates the quality of the answer based on criteria such as relevance, correctness, or user feedback [35]. In this work, the scoring function is assessed using a LLM judge, as detailed in Section 4.3. 3.2 Our Methodology Recognizing the limitations of standard vector-based RAG as a stan- dalone solution, we develop a more advanced, cost-efficient method capable of integrating and processing long documents for more reli- able responses [22]. We recognize the costs associated with certain advanced RAG solutions and prioritize affordability and scalability, ensuring that TREX remains efficient for large corpora. TREX is also designed to be query-agnostic, handling both OLTP-style fact-based queries and OLAP-style complex, open-ended questions without requiring an external query router [ 9]. Given the range of RAG Cahoon et al. technologies available for large-scale machine reading and compre- hension, we extend the querying modality of RAPTOR, which has demonstrated strong performance across various query types [35]. Due to its simple and modular design, TREX can be incorporated into any RAG system that requires integrating information across documents to improve the accuracy of retrieval and response [28]. 3.2.1 Hierarchical Clustering and Summarization. TREX leverages RAPTOR [35] to first construct a hierarchical tree structure, T , from the text chunks 洧냥 using a hierarchical clustering algorithm, represented as T (洧냥). Each node in this tree is a summary of