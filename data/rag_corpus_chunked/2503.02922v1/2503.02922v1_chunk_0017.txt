answer quality, we apply LLM-as-a-judge for both OLTP and OLAP-style benchmarks: • OLTP Evaluation: Since answers have clear ground truths, we compare model-generated responses against reference answers, achieving 99%+ agreement. To ensure consistency, we apply a logit bias, restricting outputs to “YES” (correct) or “NO” (incorrect). Cahoon et al. Figure 3: Comparison of accuracy of answers generated by TREX, GraphRAG, RAPTOR, Azure AI Hybrid Search as well as Oracle on OLTP benchmarks. • OLAP Evaluation: As no fixed ground truth exists, we adopt the GraphRAG evaluation framework [ 12], assessing re- sponses on: – Comprehensiveness: Depth and thoroughness of infor- mation. – Diversity: Inclusion of multiple perspectives. – Empowerment: How well the answer informs decision- making. In future work, we aim to enhance LLM-based evaluation by en- abling claim-level verification, ensuring a more granular and human- aligned assessment while maintaining scalability. 5 RESULTS We study the cost-performance trade-offs between TREX and the various comparative strategies across the four benchmarks. Our evaluation focuses on the quality of the generated answers and the context retrieved, the cost of indexing, and the overall performance of each strategy. Across all experiments, GPT-4o model version 2024-05-13 is used as the LLM for indexing and querying [42]. Benchmark GraphRAG RAPTOR/TREX HybridSearch MSMARCO $51.37 $5.31 $0.08 HotPotQA $389.12 $36.51 $0.75 Kevin Scott $63.27 $7.03 $0.10 Earnings $116.71 $4.19 $0.05 Table 1: Comparison of indexing cost across the four bench- marks on GraphRAG, RAPTOR, TREX, and HybridSearch methods. Costs are computed based on the use of GPT-4o ($2.5 per 1 million input and $10 per 1 million output tokens) and text-ada-embedding-002 ($0.10 per 1 million tokens) calls. Note that the costs for GraphRAG does not include the use of cost-effective alternatives like small language models for indexing. Benchmark GraphRAG RAPTOR HybridRAG TREX MSMARCO $0.04 $0.01 $0.01 $0.01 HotPotQA