overall performance of the system. Language models that support only a limited number of languages react differently to the input prompt language compared to multilingual models. Specifically, in monolingual models or those with limited support, using a language in which the model has greater proficiency (usually English) leads t o a better understanding of the instructions and assigned tasks. In contrast, in multilingual models like aya and command -r, which were examined in this study, using English as the prompt language can cause confusion, and sometimes appearance of the words from the prompt language in the output. For example, even when emphasizing the production of the answer in Persian, English words may appear among Persian sentences. Therefore, for multilingual models, writing prompts in the target language (Persian) can h elp produce smoother outputs and reduce the generation of irrelevant words. In this study, changing the prompt language from English to Persian led to a significant reduction in the occurrence of English words in the output, indicating the importance of choosing the appropriate language for prompts in multilingual models. • Separation of Prompt Components : The investigation focused on using specific markers to clearly separate different components of the prompt, such as the instructions, retrieved results, and user query. Generally, common separators that language models have been trained on were used, fol lowing templates like: Can be significantly helpful and assist the language model in better understanding. Additionally, utilizing Markdown symbols to separate different parts of the text, rows and columns in tables, and optimizing table styles, according to our team's investigations, results in receiving more accurate and better responses from LLMs. • Adding Metadata: An effective strategy to improve the performance of RAG systems is to include brief contextual information before each retrieved result. This approach helps LLMs better comprehend