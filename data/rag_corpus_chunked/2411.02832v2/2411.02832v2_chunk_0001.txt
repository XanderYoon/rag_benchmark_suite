contextually relevant text. Despite their impressive capabilities, generative language models often tend to provide outdated information or fabricate facts a phenomenon commonly referred to as "Hallucination". This limitation persists even when models are aligned with human preferences through reinforcement learning [1] or style alignment techniques [1-4]. RAG systems have emerged as a promising solution to these challenges. By integrating the strengths of pre -trained models and retrieval mechanisms, RAG provides a powerful framework that enhances model performance and reduces errors in generated content [5]. Additionally, RAG enables the rapid deployment of applications tailored to specific organizations and domains without needing to update the underlying model's parameters, provided that relevant documents are available for retrieval. Several methods have been proposed to enhance LLMs through query -dependent retrieval [5-7]. A typical RAG process includes critical components such as embedding (semantic representation of documents and queries), retrieval (efficient access to relevant documents), and generation (producing responses based on retrieved information). Implementing RAG requires key decisions regarding document segmentation, selecting embedding models for semantic representation, choosing vector databases for efficient storage and retrieval, and optimizing large language models (refer to Figure 1). The inherent compl exity at each stage of this process creates significant challen ges in implementing RAG systems which are more highlighted in a low resource language like Persian . One approach involves using embedding models directly to calculate semantic similarities between queries and documents. These embedding models are often trained adversarially using positive and negative query-response pairs [8, 9]. The choice and combination of techniques at each stage profoundly affect the effectiveness and efficiency of RAG systems. Furthermore, according to the latest reviews, there no research on optimizing RAG implementations for Persian languages, with a focus on core components such as retrieval, embedding, and generative models. Historically, most NLP research and