it is noted that a critical element in RAG systems is the information retrieval component, at the core of which lies the vector embedding process known as "semantic embedding." Their research is focused on studying a set of multilingual semantic embedding models with the aim of effectively enhancing the model's ability to understand and generate Arabic text. This study, conducted using ARCD (the publicly available Arabic Reading Comprehension Dataset), has examined various embedding models, including OpenAI's Ada, Google's Language -Agnostic Sentence Embedding BERT (LaBSE) [10], Cohere Embed-Multilingual-v3.0 [11], MPNet [8], versions one and two of HuggingFace's DistilBERT [12], Meta's SONAR [13] and Microsoft's E5 embedding models [14]. Ultimately, the results of this research indicated that Microsoft's E5 sentence embedding model outperformed all other models on the ARCD dataset, achieving a Recall@10 of over 90%. This study has paid special attention to the embedding process in multilingual RAG (in this case, Arabic) and provides valuable insights into the process of selecting the information retrieval component for multilingual RAG systems, particularly for languages that share the same script as Persian. In this paper, the authors did not use a vector database but opted for the direct use of cosine similarity to match query embeddings against document embeddings. While this approach may slow down the matching process in a real-world environment, it should have little impact on the research findings presented in the paper. Although this article does not, by any means, represent a complete RAG system, it offers a clear pathway for understanding semantic embedding models in designing a multilingual RAG system, and particularly RAG in the Pe rsian language, which shares the same script as Arabic. As the last work reviewed in the Arabic language, we can refer to the research conducted by Ali Mahbub et al. [15]. The paper, focusing