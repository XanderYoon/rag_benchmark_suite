helps identify the tools that perform best. • Domain-Specific Testing: Testing embedders with data relevant to the specific domain ensures they perform well in the desired field, ensuring optimal real-world performance. • Language Support Evaluation: Ensuring the chosen embedder has strong support for the target language, especially for low -resource languages like Persian, is crucial. • Scalability Assessment: Checking the embedder's ability to handle large volumes of data and perform well at scale is essential for industrial and commercial applications where text similarity is high. • Semantic Testing: Conducting tests to ensure the embedder maintains complex semantic relationships in the target language helps guarantee accurate semantic retrieval. These methods can help select the best embedder for low- resource languages like Persian and improve the performance of RAG systems in these areas. Based on evaluations of Persian as the target language, over 40 embedding models, thought to be capable of embedding Persian text, were assessed. Some of them were eliminated in the early stages due to their poor results, and the remaining models were furt her examined. Table I presents these evaluation results, highlighting the performance of different embedders for Persian text processing. TABLE I. SUMMARY OF EMBEDDING MODELS' RESULTS Model Top 1 Top 2 Top 3 Missed paraphrase- multilingual-mpnet- base-v2 67.2 10.8 5.4 3.5 bert-base-parsbert- uncased 69.6 13.3 3.9 4.6 distiluse-base- multilingual-cased- v2 70.2 11 4.2 5.3 dpr-xm 71.4 10.4 4.6 3.5 AviLaBSE 79.3 7.9 3.6 3.7 sentence-embedding- LaBSE 79.3 7.9 3.6 3.7 LaBSE 80.7 9.1 2.5 2.3 LaBSE-sentence- embeddings 80.7 9.1 2.5 2.3 persian-sentence- transformer-news- wiki-pairs-v3 88.1 5.3 2.1 1.3 jina-embeddings-v3 88.8 4.1 1.7 1.9 cohere-embed- multilingual-v3.0 93.5 2.9 1.1 0.3 Most models reviewed were open-source and available on platforms like Hugging Face, except for one from Cohere, which is not open -source. This diversity allowed us to comprehensively