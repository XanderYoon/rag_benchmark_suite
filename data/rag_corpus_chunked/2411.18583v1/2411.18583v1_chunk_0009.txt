is added for the knowledge of the LLM. Now some prompt engineering is performed to produce the required output. Then the LLM results are evaluated using ROUGE SCORE. The overview of the creation of the OpenAI assistant is given in figure 5. Figure 5: Creation of Custom OpenAI Assistant The used prompt: “The user will give you a pdf file as input, similar to the “input” field of the given “data.json” file in your knowledge base. You have to produce a summarized “output” for the given pdf based on the file given to your knowledge. The output will be of max 80 words. Note: You must write in a way that can be considered a literature review of a new research pa per. The user in the future might add more PDFs so try to make the literature review coherent and as per IEEE standards. Please mention the first author’s name and paper title. Don’t write like this “Literature Review of. . . ”.” Figure 6: Pipeline using LLM The next step is to implement a system pipeline by using the LLM to generate a literature review segment automatically. The system takes PDFs of multiple papers as input. It uses the PYPDF2 library to extract the entire text of each PDF. Then it creates a new thread with the extracted text as a message and submits the thread to the assistant with the extracted text as a query. Then the response from the assistant is retrieved and the outputs of each paper are merged for the final literature review segment. The system pipeline overview of the LLM is given in Figure 6. E. The Final System Tool The final system is implemented using the Large Language Model: GPT -3.5-TURBO-0125 as the backend. An aesthetic and simple user interface is created