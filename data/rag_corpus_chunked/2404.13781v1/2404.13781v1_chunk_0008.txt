epochs, incorporating linear warmup for the initial 5% of training steps. The effective batch size is set to 64. Each model is trained using an A100 Nvidia GPU. For document retrieval during training, we utilize BM25 [22] implemented in Pyserini [18] to retrieve 50 documents to augment the input with them. For fast vector search in dense retrieval with Contriever2 [7], we use Faiss [5] flat index. 1https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 2https://huggingface.co/facebook/contriever Evaluating Retrieval Quality in Retrieval-Augmented Generation SIGIR ’24, July 14–18, 2024, Washington, DC, USA. Table 1: The correlation between each evaluation approach and the downstream performance of the LLM. T5-small with FiD with 50 retrieved documents is used. We do not report correlation for the Answers method for FEVER and WOW datasets because the answers to queries do not exist in the document since FEVER is a classification dataset and WoW is long-text generation. For the WoW dataset, we only report correlation on Precision and Hit Ratio because other metrics do not support non-integer relevance labels. Tau is Kendall’s tau and rho is Spearman’s rho. RelevanceAnnotationMetric BM25 Contriever NQ TriviaQA HotpotQA FEVER WoW NQ TriviaQA HotpotQA FEVER WoW tau rho tau rho tau rho tau rho tau rho tau rho tau rho tau rho tau rho tau rho Containing theAnswer MAP 0.349 0.417 0.298 0.364 0.359 0.423 - - - - 0.303 0.366 0.265 0.325 0.379 0.429 - - - - MRR 0.361 0.417 0.313 0.340 0.398 0.449 - - - - 0.301 0.353 0.257 0.292 0.384 0.430 - - - - NDCG 0.357 0.427 0.298 0.365 0.370 0.435 - - - - 0.313 0.378 0.270 0.331 0.385 0.437 - - - - P 0.353 0.411 0.276 0.333 0.396 0.454 - - - - 0.346 0.403 0.283 0.340 0.406 0.449 - - - - R 0.325 0.325 0.232 0.232 0.375 0.375