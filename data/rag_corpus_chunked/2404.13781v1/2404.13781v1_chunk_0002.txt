systemâ€™s behavior. Secondly, it is resource- intensive, consuming significant time and computational power, particularly when dealing with a large set of retrieval results con- sumed by the LLM. To process long input sequences resulting from the utilization of all retrieved documents by the LLM, GPUs with substantial memory capacities are essential for end-to-end evalu- ation. Moreover, many ranking systems rely on interleaving (i.e., replacing one or more documents in the result list) for evaluation and optimization, which further complicates the evaluation, as slight variations in retrieval results necessitate re-computation of the RAG pipeline. Finally, optimizing ranking models often requires document-level feedback, such as user clicks [3, 6]. However, end- to-end evaluation only provides list-level feedback for the retrieval results. That said, this paper studies retrieval evaluation in RAG. Human annotations can be a potential solution for evaluating retrieval models in RAG, however, accurate annotations are often challenging and costly to obtain. More recently, with the emergence of large language models (LLMs) and their advanced capabilities in reasoning and text comprehension, they have been utilized to annotate documents for retrieval evaluation [10, 23]. Nevertheless, these approaches predominantly evaluate the retriever in RAG sys- tems based on human preferences, whereas the primary objective of the retrieval model in RAG is to serve the LLM that leverages the retrieved results [35]. That said, our extensive investigation on a diverse set of RAG systems for open-domain question answer- ing, fact verification, and dialogue systems reveals that employing human annotations, such as theprovenance labels in the KILT bench- mark [20], for evaluating the retrieval models within a RAG system exhibits only a minor correlation with the downstream RAG per- formance. This indicates a lack of meaningful relationship between the evaluated metrics and the downstream performance of RAG. In this paper, we propose eRAG, a new