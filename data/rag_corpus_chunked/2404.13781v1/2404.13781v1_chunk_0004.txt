correlation. Finally, we demonstrate that our approach offers significant computational advantages, consum- ing up to 50 times less memory compared to end-to-end evaluation. To facilitate research in this domain, we make eRAGâ€™s implementa- tion publicly available at: https://github.com/alirezasalemi7/eRAG. 2 EV ALUATING RETRIEVERS IN RAG Generally, two predominant methods are used for obtaining rele- vance labels for retrieval evaluation. The first approach involves human judgment to assess the relevance of a query to documents within a corpus. The main issue with this approach is that human annotation can be costly and is often impractical for evaluating all documents in a corpus [ 28]. Moreover, human annotation relies on human preferences to judge the relevance of documents to a query. However, a document deemed relevant based on human preferences may not be useful for an LLM in fulfilling its task. The second approach utilizes the downstream ground truth out- put associated with the query to provide weak relevance labels. In this method, a retrieved document containing the downstream ground truth is considered relevant [8, 14, 24, 27]. This method also presents its own challenges. This approach is impractical, partic- ularly in scenarios where the task involves long-text generation or text classification, as downstream task labels might not exist within documents. Also, one document can be useful for an LLM in fulfilling its task without containing the ground truth labels. Even though we are not aware any work that use LLMs for evaluating retrieval models in RAG, LLMs can be leveraged to la- bel documents based on their relevance to a query. Inspired by Thomas et al. [30], the LLM functions as a binary classifier, indi- cating whether a document is relevant to the query or not. The mentioned challenges persist even with the judgment of LLMs, espe- cially if the LLM responsible