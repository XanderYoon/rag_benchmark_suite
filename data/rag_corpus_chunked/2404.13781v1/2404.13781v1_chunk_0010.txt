0.021 0.026 -0.002-0.003 MRR 0.060 0.062 0.189 0.196 0.001 0.001 -0.021-0.022-0.008-0.0110.048 0.050 0.143 0.151 0.034 0.038 -0.007-0.007 0.004 0.005 NDCG 0.049 0.060 0.178 0.218 0.032 0.039 0.018 0.022 -0.006-0.0090.036 0.044 0.175 0.214 0.049 0.060 0.022 0.028 0.000 0.000 P 0.028 0.034 0.137 0.166 -0.004-0.006 0.021 0.025 -0.005-0.0080.002 0.003 0.138 0.167 0.010 0.013 0.014 0.017 -0.006-0.010 R 0.014 0.014 0.032 0.032 -0.016-0.016 0.019 0.019 0.003 0.003 0.000 0.000 0.039 0.039 -0.042-0.042-0.017-0.017 0.017 0.021 Hit Ratio0.014 0.014 0.032 0.032 -0.016-0.016 0.019 0.019 0.003 0.003 0.000 0.000 0.039 0.039 -0.042-0.042-0.017-0.017 0.017 0.021 eRAGAnnotations MAP 0.492 0.575 0.474 0.578 0.610 0.694 0.386 0.463 - - 0.467 0.544 0.427 0.519 0.634 0.705 0.399 0.479 - - MRR 0.503 0.577 0.4860.553 0.629 0.695 0.592 0.611 - - 0.466 0.537 0.424 0.495 0.639 0.698 0.481 0.504 - - NDCG 0.505 0.590 0.4860.592 0.612 0.697 0.404 0.484 - - 0.481 0.560 0.440 0.536 0.635 0.705 0.403 0.484 - - Pa 0.5290.598 0.484 0.577 0.594 0.663 0.329 0.391 0.504 0.669 0.5220.5860.4820.571 0.633 0.695 0.378 0.449 0.540 0.712 R 0.519 0.519 0.426 0.426 0.619 0.619 0.301 0.301 - - 0.488 0.488 0.408 0.408 0.631 0.631 0.299 0.299 - - Hit Ratiob 0.519 0.519 0.426 0.426 0.619 0.619 0.301 0.301 0.390 0.532 0.488 0.488 0.408 0.408 0.631 0.631 0.299 0.299 0.414 0.561 aFor non-integer relevance labels, precision is equal to average of the relevance labels.bFor non-integer relevance labels, hit ratio is equal to maximum of the relevance labels. 3.2 Main Findings How do different retrieval evaluation methods correlate with the end-to-end downstream performance in RAG?. To com- pare the different evaluation strategies for evaluating retriever in RAG, we report the correlation between the scores generated for each method and the downstream performance of the LLM (i.e., T5-small with FiD and 50 retrieved documents) in Table 1. The results indicate that eRAG