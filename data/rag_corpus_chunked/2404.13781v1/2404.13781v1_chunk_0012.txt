FEVER for fact-checking, and WoW for long-text generation. The results of this experiment are shown in Figure 1. The outcomes of this experi- ment reveal that irrespective of the quantity of retrieved documents, Figure 1: The correlation between evaluation approaches and the LLM’s downstream performance varying number of retrieved documents by BM25. T5-small with FiD is used. The metric with the highest correlation in Table 1 is used. our suggested evaluation strategy consistently exhibits a higher correlation with the downstream performance of the LLM. Further- more, the results illustrate that augmenting the number of retrieved documents leads to a decline in correlation—a intuitive observation, as all metrics assess each document-relevance label independently for scoring a ranked list, while the LLM uses information from the entirety of these documents to accomplish its task. How does our method correlate with the downstream RAG performance as the size of large language models increases? In addressing this question, we computed the correlation between our retrieval evaluation strategy and the downstream performance of the LMs with two distinct sizes (i.e., T5-small with FiD consisting of 60M and T5-base with FiD consisting of 220M parameters). For the sake of space, we limit our experiments to three datasets: NQ for question answering, FEVER for fact-checking, and WoW for long-text generation. The results illustrated in Figure 2 indicate that, for certain datasets, there is a higher correlation with the smaller LLM, while for others, a higher correlation is observed with the SIGIR ’24, July 14–18, 2024, Washington, DC, USA. Alireza Salemi and Hamed Zamani Figure 2: The correlation between eRAG and the downstream performance of different LLM sizes. In this experiment, T5- small (60M parameters) and T5-base (220M parameters) with FiD are used. The documents are retrieved using BM25. Figure 3: The correlation between eRAG and the downstream