: âˆ€ğ‘‘ âˆˆ Rğ‘˜ (1) where ğ‘¦ is the expected downstream output for the query. We can employ the created Gğ‘ to utilize any ranking metric to evaluate R. Note that the runtime cost of a vanilla transformer [32] scales quadratically with its input length. Consequently, for end-to-end evaluation, the cost of running a transformer on a ranked list with ğ‘˜ documents, with an average length of ğ‘‘, to generate an output with length ğ‘™ is ğ‘‚ (ğ‘™ğ‘˜ 2ğ‘‘2). Conversely, in our approach, as each document is individually fed to the LLM for k times, the cost is ğ‘‚ (ğ‘™ğ‘˜ğ‘‘ 2), proving to be more efficient than end-to-end evaluation. Retrieval Evaluation Metrics. For a ranked list Rğ‘˜, comprising ğ‘˜ retrieved documents generated by a retrieval model R, an evalua- tion metric ER assigns a score ER (Rğ‘˜, Gğ‘) âˆˆ [ 0, 1], by comparing the ranked list with the relevance scoresGğ‘, which is a function that maps each document to a scalar relevance score for the document with respect to the queryğ‘ (i.e., Gğ‘ (ğ‘‘) = ğ‘ ğ‘‘). Various definitions ex- ist for the evaluation metricER; in this paper, we examine Precision (P), Recall (R), Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) [2], Normalized Discounted Cumulative Gain (NDCG) [11], and Hit Rate. Note that when dealing with non-binary rele- vance labels, precision considers the average value of relevance labels, while Hit Ratio considers the maximum value among them. 3 EXPERIMENTS 3.1 Setup Datasets and Evaluation. We use Natural Questions (NQ) [15], TriviaQA [13], HotpotQA [33], FEVER [31], and Wizard of Wikipedia (WoW) [4] datasets from the KILT [20] benchmark. Due to the un- availability of ground truth labels for the test set, we utilize the publicly accessible validation set. As the retrieval corpus, we employ the Wikipedia dump of the