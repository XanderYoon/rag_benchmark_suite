How do different retrieval evaluation methods correlate with the end-to-end downstream performance in RAG?. To com- pare the different evaluation strategies for evaluating retriever in RAG, we report the correlation between the scores generated for each method and the downstream performance of the LLM (i.e., T5-small with FiD and 50 retrieved documents) in Table 1. The results indicate that eRAG attains the highest correlation compared to other evaluation approaches. Furthermore, the results show that regardless of the retrieval model employed, eRAG consistently out- performs others in terms of correlation with the LLM’s downstream performance. Interestingly, the most common approaches, KILT Provenance and Annotation with LLMs, that are, document-level relevance labels and using LLMs to assign a relevance label to each retrieved document, have the lowest correlation with the down- stream performance of the LLM. This finding confirms that the LLM as the consumer of the retrieved results in RAG is the best judge for the performance of the retrieval model. How do different retrieval evaluation methods in RAG per- form as the size of retrieval results increases? To address this, we varied the number of retrieved documents and computed the correlation between the metric with highest correlation for each method in Table 1 at each specified number of retrieved documents and the downstream performance of the LLM given that number of retrieved documents. For the sake of space, we limit our exper- iments to three datasets: NQ for question answering, FEVER for fact-checking, and WoW for long-text generation. The results of this experiment are shown in Figure 1. The outcomes of this experi- ment reveal that irrespective of the quantity of retrieved documents, Figure 1: The correlation between evaluation approaches and the LLM’s downstream performance varying number of retrieved documents by BM25. T5-small with FiD is used. The metric with