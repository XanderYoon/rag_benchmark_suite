740 sec 47.9 GB 6.5 GB 1.5 GB utilization of the entire GPU memory. The results of this experiment are reported in Table 2. The findings indicate that, on average, eRAG is 2.468 times faster than end-to-end evaluation. Further elaborating, the speedup for eRAG ranges from 1.232 to 3.252 times compared to end-to-end evaluation across the datasets, where the least speedup is for the long-text generation task (i.e., WoW). To compare memory consumption between eRAG and end-to- end evaluation, we conducted two experiments. First, we compared the maximum memory required by end-to-end evaluation to assess a query with the maximum memory demanded by eRAG for the same evaluation. To carry out this comparison, we configured the batch size for end-to-end evaluation to 1, while for eRAG, we set it to the same number of documents used for one query by end- to-end evaluation (we call this query-level configuration). In the subsequent experiments, we set both batch sizes to 1 to assess the extent to which eRAG demonstrates superior memory efficiency compared to end-to-end evaluation under the most efficient con- figuration (we call this document-level configuration). The results of these experiments are reported in Table 2. The findings indi- cate that in the query-level configuration, eRAG exhibits between 7 to 15 times greater memory efficiency compared to end-to-end evaluation. Furthermore, in the document-level configuration, this efficiency gap widens, with eRAG demonstrating 30 to 48 times more memory efficiency than end-to-end evaluation across differ- ent dataset. In summary, these experiments suggest that eRAG is more efficient than end-to-end evaluation of a vanilla transformer, excelling in both inference time and memory utilization. 4 CONCLUSION This paper explores various approaches for evaluating retrieval models within a RAG pipeline. Additionally, it introduces eRAG, a novel approach for evaluating retrieval models in the RAG pipeline. eRAG