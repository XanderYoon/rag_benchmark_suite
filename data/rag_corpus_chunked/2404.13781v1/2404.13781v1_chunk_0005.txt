retrieval models in RAG, LLMs can be leveraged to la- bel documents based on their relevance to a query. Inspired by Thomas et al. [30], the LLM functions as a binary classifier, indi- cating whether a document is relevant to the query or not. The mentioned challenges persist even with the judgment of LLMs, espe- cially if the LLM responsible for labeling differs from the LLM in the RAG pipeline. Besides, employing LLMs as judges in this scenario can pose challenges due to the computational cost of running them on a large set of retrieved documents and memory constraints. To mitigate these problems, we propose eRAG, a novel approach that involves utilizing the LLM in RAG system itself as the arbiter for generating labels to evaluate the retrieval model. Using Downstream Large Language Model in RAG as Doc- ument Annotator. Consider a retrieval model R that produces a ranked list Rğ‘˜ with ğ‘˜ documents for the LLM M tasked with performing a specific task, utilizing a downstream evaluation func- tion EM. The LLM M takes a ranked list of documents as its input along with the query ğ‘, and generates an output represented as Â¯ğ‘¦ = M (ğ‘, Rğ‘˜ ). For the documents in Rğ‘˜, we feed each document in- dividually to the LLMM with the query and evaluate the generated answer to create the label for each document, expressed as: Gğ‘ [ğ‘‘] = EM (M (ğ‘, {ğ‘‘ }), ğ‘¦) : âˆ€ğ‘‘ âˆˆ Rğ‘˜ (1) where ğ‘¦ is the expected downstream output for the query. We can employ the created Gğ‘ to utilize any ranking metric to evaluate R. Note that the runtime cost of a vanilla transformer [32] scales quadratically with its input length. Consequently, for end-to-end evaluation, the cost of running a transformer on a ranked list with