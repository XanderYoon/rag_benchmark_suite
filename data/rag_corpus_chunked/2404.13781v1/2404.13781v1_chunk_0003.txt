reveals that employing human annotations, such as theprovenance labels in the KILT bench- mark [20], for evaluating the retrieval models within a RAG system exhibits only a minor correlation with the downstream RAG per- formance. This indicates a lack of meaningful relationship between the evaluated metrics and the downstream performance of RAG. In this paper, we propose eRAG, a new approach for evaluating retrievers in RAG systems, where we apply the LLM in RAG system on each document in the retrieval result list individually and use the LLM’s output to provide document-level annotations. These annotations can be obtained using any arbitrary downstream task metric, such as accuracy, exact match, or ROUGE [17]. We can then apply a set-based or ranking metric as an aggregation function to obtain a single evaluation score for each retrieval result list. We evaluate our proposed approach on question answering, fact- checking, and dialogue generation from the knowledge-intensive arXiv:2404.13781v1 [cs.CL] 21 Apr 2024 SIGIR ’24, July 14–18, 2024, Washington, DC, USA. Alireza Salemi and Hamed Zamani language tasks (KILT) benchmark [20]. Our results demonstrate that our proposed approach achieves the highest correlation with the downstream performance of the RAG system in comparison with the baselines. Specifically, we observe an absolute improvement in Kendall’s tau correlation ranging between 0.168 and 0.494 across the evaluated datasets. Furthermore, we investigate the impact of differ- ent retrieval augmentation methods, the quantity of retrieved docu- ments, and the LLM size on correlation. Finally, we demonstrate that our approach offers significant computational advantages, consum- ing up to 50 times less memory compared to end-to-end evaluation. To facilitate research in this domain, we make eRAG’s implementa- tion publicly available at: https://github.com/alirezasalemi7/eRAG. 2 EV ALUATING RETRIEVERS IN RAG Generally, two predominant methods are used for obtaining rele- vance labels for retrieval evaluation. The first