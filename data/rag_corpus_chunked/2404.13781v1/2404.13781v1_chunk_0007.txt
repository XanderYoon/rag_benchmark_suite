3.1 Setup Datasets and Evaluation. We use Natural Questions (NQ) [15], TriviaQA [13], HotpotQA [33], FEVER [31], and Wizard of Wikipedia (WoW) [4] datasets from the KILT [20] benchmark. Due to the un- availability of ground truth labels for the test set, we utilize the publicly accessible validation set. As the retrieval corpus, we employ the Wikipedia dump of the KILT benchmark and adhere to the pre- processing outlined by Karpukhin et al. [14], where each document is segmented into passages, each constrained to a maximum length of 100 words. The concatenation of the article title and passage is used as a document. The KILT benchmark furnishes document- level relevance labels (called Provenance) for its datasets, and these are employed for evaluating retrieval performance. In line with our preprocessing method, we define all passages within a positive document as positive passages for our evaluation. For relevance evaluation using an LLM, we employ Mistral1 [12] to annotate each document within the retrieved list, determining whether it is rele- vant to the query or not. We adopt the metrics recommended by the KILT benchmark, namely Exact Match (EM) for NQ, TriviaQA, and HotpotQA, Accuracy for FEVER, and F1 for the WoW dataset. Experiments Configuration. In all experiments, unless explicitly stated otherwise, we employ T5-small [21] with Fusion-in-Decoder (FiD) [9] as the LLM. We employ AdamW [19] with a weight decay of 10−2 and a learning rate of 5 × 10−5 for 10 epochs, incorporating linear warmup for the initial 5% of training steps. The effective batch size is set to 64. Each model is trained using an A100 Nvidia GPU. For document retrieval during training, we utilize BM25 [22] implemented in Pyserini [18] to retrieve 50 documents to augment the input with them. For fast vector search in dense retrieval with Contriever2