Retrieval-Augmented Generation. In Proceedings of the 47th Int’l ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’24), July 14–18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION Retrieval-augmented generation (RAG) has emerged as a prominent approach in natural language processing, combining the strengths of retrieval and generation models [35], with use cases in decreas- ing hallucination [ 1, 29], knowledge-grounding [ 9, 16, 34], and personalization [25, 26]. Evaluating RAG systems is important as Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR ’24, July 14–18, 2024, Washington, DC, USA. © 2024 Copyright held by the owner/author(s). ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. https://doi.org/10.1145/nnnnnnn.nnnnnnn it ensures the effectiveness of integrating retrieval-based methods with generative models [10, 23]. Traditionally, RAG evaluation has primarily relied on end-to-end assessment, which entails compar- ing the generated output with one or more ground truth references [20]. While this is crucial, it presents several limitations, especially, for evaluating retrieval models in RAG systems. First, end-to-end evaluation lacks transparency regarding which retrieved document contributed to the generated output, hindering interpretability of the system’s behavior. Secondly, it is resource- intensive, consuming significant time and computational power, particularly when dealing with a large set of retrieval results con- sumed by the LLM. To process long input sequences resulting from the utilization of all retrieved documents by the LLM, GPUs with substantial memory capacities are essential for end-to-end evalu- ation. Moreover, many ranking systems rely