filtering irrelevant ones. Fol- lowing the initial mapping, Agent-4 employs regular expression functions to parse structured output and assess answer complete- ness by categorizing each component as “fully answered, ” “partially answered, ” or “not answered. ” When gaps are identified, Agent-4 initiates a targeted follow-up process: it generates questions to address these gaps and uses the hybrid retrieval system from initial phase, excluding previously retrieved documents to ensure com- plementary information. Agent-4 then generates answers for each follow-up question and integrates the new information into the original answer through answer synthesis. This iterative approach ensures complete coverage of all query components, resulting in a unified final answer that fully addresses the original question. LiveRAG@SIGIR 2025, July 13–17, 2025, Padua, Italy Ines Besrour, Jingbo He, Tobias Schreieder, and Michael Färber 5 Evaluation We evaluate RAGentA with the 500 QA pairs of our synthetic dataset, which we described in Section 3. The evaluation is a two- stage process, where we first assess retrieval performance and then evaluate the generated answers for correctness and faithfulness. 5.1 Evaluation of Retrieval To assess the performance of our retrieval system, we employ two evaluation metrics: Recall@k and Mean Reciprocal Rank (MRR)@k, with k set to 20, as this corresponds to the number of documents provided to the RAGentA framework. Specifically, Recall@20 quan- tifies the proportion of relevant ground-truth evidence documents that are retrieved within the top 20 results, whereas MRR@20 mea- sures the rank of the first relevant ground-truth document in the retrieved set. We compare the performance of our hybrid retrieval system to that of two baseline models, namely BM25 and E5, to evaluate its relative performance. Table 2: Evaluation of Retrieval Performance Retrieval System MRR@20 Recall@20 BM25 0.4205 0.5020 E5 0.3476 0.4920 Hybrid 0.4290 0.5650 The evaluation results are presented in Table 2.