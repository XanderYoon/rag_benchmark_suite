to source documents, thereby enhancing trustworthiness. As described by Huang and Chang [9], LLM attribution is broadly categorized into parametric and non-parametric approaches. Parametric approaches, such as Galactica [22], generate citations only using the modelâ€™s in- ternal knowledge. In contrast, non-parametric methods leverage ex- ternal knowledge sources during generation to provide attribution. Non-parametric approaches can be divided into post-generation and post-retrieval paradigms. Post-generation approaches first gen- erate an answer, then identify supporting evidence. For instance, RARR [6] detects unsupported claims in generated answers and revises them using retrieved documents. Ramu et al. [17] improve attribution in long documents by decomposing answers into coarse- grained segments to better align with evidence. Huang et al . [8] propose fine-grained reward models for training LLMs to gener- ate accurate citations. In this work, we focus on post-retrieval ap- proaches that follow the RAG paradigm, retrieving evidence prior to answer generation. Gao et al. [7] introduced an initial approach and ALCE, a benchmark dataset widely used to evaluate attribution. Fierro et al. [4] propose a planning-based strategy, where models generate citation plans before answering. Berchansky et al . [1] develop CoTAR, applying chain-of-thought reasoning with hier- archical citation to improve attribution granularity. Qi et al. [16] present MIRAGE, which uses internal model representations to trace answer spans back to supporting passages, improving faithful- ness. Finally, Patel et al. [14] demonstrate a method for generating multiple citations per sentence, enabling more nuanced and com- prehensive attribution in long-form generation. 3 Synthetic Dataset Generation We constructed a diverse synthetic QA benchmark for the eval- uation of our RAGentA framework using the DataMorgana plat- form [5]. The dataset comprises 500 QA pairs, each associated with multiple supporting evidence paragraphs. Detailed statistics of the dataset, including the distribution across various question and user categories, are presented in Table