is rated on a four-point scale: 2 The response correctly answers the user question and con- tains no irrelevant content. 1 The response provides a useful answer to the user question, but may contain irrelevant content that does not harm the usefulness of the answer. 0 No answer is provided in the response (e.g., “I don’t know”). -1 The response does not answer the question whatsoever. Faithfulness. This metric assesses whether the answer is grounded in the retrieved documents on a three-point scale, following the methodology proposed by Es et al. [3]: 1 Full support: all answer parts are grounded. 0 Partial support: not all answer parts are grounded. -1 No support: all answer parts are not grounded. Table 3: Evaluation of Correctness and Faithfulness Approach Correctness Faithfulness Standard RAG 0.8256 0.6362 RAGentA (Ours) 0.8346 0.7044 Table 3 presents the average correctness and faithfulness scores computed over the full set of 500 questions. We evaluate the per- formance of RAGentA in comparison to a baseline RAG approach (Standard RAG). Both methods utilize the same set of initially re- trieved documents to ensure a controlled comparison. Standard RAG does not incorporate agent-based reasoning and generates answers using a single prompt. The results show that RAGentA achieves a correctness score of 0.8348, slightly outperforming Stan- dard RAG (0.8256) by +1.1%. For faithfulness, RAGentA reaches 0.7044, exceeding Standard RAG (0.6362) by +10.7%. 6 Conclusion In this work, we introduced RAGentA, a multi-agent RAG frame- work designed to enhance the trustworthiness of attributed QA. Our results demonstrate that a hybrid retrieval system combining BM25 and E5 significantly improves Recall@20 (+12.5%) compared to the best single model. RAGentA outperforms the standard RAG base- line, particularly in faithfulness (+10.7%), showing that providing in-line citations and conducting a second-stage retrieval for answer revision yield more relevant documents