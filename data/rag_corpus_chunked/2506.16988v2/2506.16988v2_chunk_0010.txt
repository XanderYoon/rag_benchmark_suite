the first relevant ground-truth document in the retrieved set. We compare the performance of our hybrid retrieval system to that of two baseline models, namely BM25 and E5, to evaluate its relative performance. Table 2: Evaluation of Retrieval Performance Retrieval System MRR@20 Recall@20 BM25 0.4205 0.5020 E5 0.3476 0.4920 Hybrid 0.4290 0.5650 The evaluation results are presented in Table 2. Our hybrid re- trieval system achieves an MRR@20 of 0.4290, outperforming BM25 (0.4205) by +2.0% and E5 (0.3476) by +23.4%. For Recall@20, the hybrid retrieval reaches 0.5650, exceeding BM25 (0.5020) by +12.5% and E5 (0.4920) by +14.8%. These findings demonstrate the strength of the hybrid retrieval approach, which fuses sparse and dense approaches in the RAGentA framework. 5.2 Evaluation of Correctness and Faithfulness We aligned our evaluation of answer correctness and faithfulness closely with the autoevaluator used in the SIGIR LiveRAG chal- lenge. Instead of the closed-source Claude-3.5 Sonnet model, we use Llama-3.3-70B-Instruct as an LLM-as-a-judge, which receives the predicted answer, the ground-truth answer, and the cited passages. The LLM-as-a-judge assigns a correctness score in the range [−1, 2] and a faithfulness score in the range [−1, 1]. Correctness. This metric consists of two components: coverage, the portion of vital information in the ground-truth answer that is covered by the generated answer, inspired by the work of Pradeep et al. [15], and relevance, the portion of the generated answer that directly addresses the question, regardless of its factual correctness. Correctness is rated on a four-point scale: 2 The response correctly answers the user question and con- tains no irrelevant content. 1 The response provides a useful answer to the user question, but may contain irrelevant content that does not harm the usefulness of the answer. 0 No answer is provided in the response (e.g., “I don’t know”). -1 The response