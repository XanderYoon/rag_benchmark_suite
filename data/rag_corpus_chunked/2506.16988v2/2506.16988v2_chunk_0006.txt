based on BM25, is a widely adopted model that relies on exact term match- ing [18, 19]. Our semantic retrieval is powered by dense embeddings using the intfloat/e5-base-v2 model (E5) with Pinecone as the vector store to capture nuanced semantic relationships [23]. These two systems are fused using a tunable interpolation pa- rameter ğ›¼ âˆˆ [ 0, 1], producing a final document score as: ğ‘†hybrid (ğ‘‘) = ğ›¼ Â·ğ‘†sparse (ğ‘‘) + ( 1 âˆ’ ğ›¼) Â· ğ‘†dense (ğ‘‘) (1) We set ğ›¼ = 0.35 to give slightly higher weight to dense retrieval while maintaining meaningful contribution from sparse retrieval. This choice is motivated by the prior findings of Mosquera et al . [13], where a hybrid retrieval with ğ›¼ = 0.35 yielded the best perfor- mance. Their experiments on regulatory texts demonstrated that this balance improves overall retrieval performance. RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering LiveRAG@SIGIR 2025, July 13â€“17, 2025, Padua, Italy Q1: Where is Mount Everest, and what is its height? Q1-D1-A1 0.7 0.2 0.5 The height of MountEverest is 8,848meters [1][2]. Scoring + Filtering Generating Answer + Inline-Citations Generating Answers 0.7 0.5 Q1-D2-A2 Reformulation Retrieval Agent-4Agent-3 Q': In which country is Mount Everest located? A': Mount Everest islocated in Nepal [3]. + Retrieval Generating Answer Agent-1 Agent-2 The height of MountEverest is 8,848meters [1][2]. The height of Mount Everest is8,848 meters [1][2], and it islocated in Nepal [3]. Query-Document- Answer Triplet Q1-D3-A3 Completeness Check Figure 1: Architecture of the RAGentA framework: (1) A hybrid retriever selects top-20 documents. (2) Agent-1 generates an initial answer. (3) Agent-2 filters Query-Document-Answer triplets. (4) Agent-3 produces a final answer with in-line citations. (5) Agent-4 checks completeness, optionally reformulates the query, and merges both answers. 4.2 Multi-Agent Architecture Our multi-agent architecture builds up on the MAIN-RAG frame- work [2], which employs