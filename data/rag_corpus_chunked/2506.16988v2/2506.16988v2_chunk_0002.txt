Challenge, we propose a multi-agent RAG framework for attributed QA that emphasizes both answer correctness and faithfulness. We extend the MAIN-RAG framework [ 2], which employs three agents to score the relevance of retrieved documents for improved answer correctness. We enhance this architecture by introducing a novel agent that improves the faithfulness of the output through fine- grained attribution and refines the LLM answer when needed. This collaborative setup helps close the gap toward more trustworthy LLM applications for QA. Our key contributions are as follows: • We proposeRAGentA, a collaborative multi-agent RAG frame- work that improves answer faithfulness through fine-grained citations, especially for multi-source questions. • We ensure high retrieval quality through a hybrid sparse- dense approach, reinforced by agent-based relevance scoring to select the most suitable documents for generation. • We test RAGentA against a baseline RAG approach by build- ing a diverse synthetic QA dataset from the FineWeb index to assess retrieval, answer correctness, and faithfulness. 2 Related Work Multi-Agent RAG. Recent developments in RAG have increas- ingly leveraged multi-agent systems to improve performance and arXiv:2506.16988v2 [cs.IR] 1 Sep 2025 LiveRAG@SIGIR 2025, July 13–17, 2025, Padua, Italy Ines Besrour, Jingbo He, Tobias Schreieder, and Michael Färber scalability in QA tasks. Zhao et al. [27] introduced LongAgent, a col- laborative multi-agent framework designed to enable QA over very long documents. In this approach, queries are decomposed into sub- tasks handled by individual agents, each processing a segment of the input. A leader agent coordinates their outputs and synthesizes the final answer. Building on the idea of agent specialization, Zhu et al. [28] proposed ATM, a dual-agent adversarial RAG framework consisting of a generator and an attacker. The attacker injects chal- lenging distractor documents, forcing the generator to distinguish relevant information even in misleading contexts through adver- sarial training.