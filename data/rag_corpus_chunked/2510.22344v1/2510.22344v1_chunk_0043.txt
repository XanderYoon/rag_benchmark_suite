aggregate view is informative, Figure 5 reveals that the distribution of failure modes is highly de- pendent on the nature of the task. For fact-based, single-hop datasets like TriviaQA, failures are over- whelmingly concentrated in the Retrieval stage (47%). However, as query complexity increases in multi-hop datasets (MuSiQue, HotpotQA, 2Wiki- MultihopQA), the burden shifts. In these cases, SEA Errors become significantly more prominent, accounting for 28-32% of all failures. This demon- strates that for complex reasoning tasks, the pri- mary challenge moves beyond simply finding infor- mation to correctly reasoning about and managing it. This trend strongly validates the necessity of a sophisticated strategic control module like SEA in advanced RAG systems. 6 Conclusion In this paper, we introducedFAIR-RAG, a novel, agentic framework designed to address a key limi- tation of existing Retrieval-Augmented Generation systems: their unreliability in handling complex, multi-hop queries. By architecting an evidence- driven, iterative process, FAIR-RAG advances be- yond the static “retrieve-then-read” paradigm. Our core contributions—Adaptive Routing, theItera- tive Refinement Cycle, and the analytical gating mechanism of theStructured Evidence Assess- ment (SEA)module—work in synergy to progres- sively build and validate a comprehensive context before generation. The SEA’s ability to systemati- cally deconstruct queries and identify information gaps provides a precise, actionable signal that di- rectly guides the query refinement process. Our extensive experiments demonstrate that FAIR-RAG’s structured approach achieves leading performance among comparable iterative and adap- tive RAG architectures across challenging multi- hop QA datasets like HotpotQA and 2WikiMul- tiHopQA. These results empirically validate our central hypothesis: that a procedural, multi-stage workflow with explicit evidence assessment is es- sential for achieving high accuracy and faithfulness in knowledge-intensive tasks. 6.1 Limitations Despite its strong performance, FAIR-RAG presents several inherent trade-offs and limitations that warrant discussion: • Dependency on LLM Reasoning Fidelity and Prompt Engineering:The performance of FAIR-RAG’s