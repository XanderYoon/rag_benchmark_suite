sample of 200 unique error instances drawn equally from our four bench- mark datasets: TriviaQA, MuSiQue, HotpotQA, and 2WikiMultihopQA. This analysis employed a hybrid methodology, combining LLM-based cat- egorization with human expert validation to en- sure accuracy and depth. Our taxonomy distin- guishes between two fundamental sources of error: (1)Component-Level Failures, which stem from the inherent limitations of the underlying modules Figure 4: Aggregate Distribution of Failure Sources. Analysis of 200 error samples reveals a primary split between Component-Level Failures (63.5%) and Ar- chitectural Failures (36.5%). While architectural logic offers direct avenues for refinement, the majority of errors stem from the inherent limitations of the founda- tional retrieval and generation models, identifying them as the principal bottleneck for the FAIR-RAG system. (i.e., the retriever and the generator LLM), and (2) Architectural Failures, which are specific to the decision-making logic of the FAIR-RAG frame- work itself (i.e., Query Decomposition, Filtering, Refinement, and SEA). This distinction is critical for understanding the system’s bottlenecks. As shown in Figure 4, a sig- nificant majority of errors (63.5%) are Component- Level, originating from the foundational tools our system is built upon. The remaining 36.5% are Architectural, offering direct targets for refining FAIR-RAG’s internal logic. This distribution un- derscores a key insight: while FAIR-RAG’s itera- tive process is designed to mitigate the weaknesses of its components, the performance of these base components remains the primary limiting factor in overall system accuracy. 1. Component-Level Failures (63.5% of Errors): The Foundational BottleneckThese errors are not caused by FAIR-RAG’s reasoning process but by the fundamental limitations of the tools it or- chestrates. • Retrieval Failure (32.5%):This was the sin- gle largest source of error across all datasets. The retriever’s inability to surface critical doc- uments is a major obstacle, particularly for queries requiring highly specific, long-tail fac- tual knowledge.