the brute-force effect of repeated iterations. 5.2.2 Impact of Iterative Refinement A core hypothesis of this work is that iterative refinement improves answer quality for complex questions. We tested this by running the same set of questions with the maximum number of iterations ranging from 1 to 4. We then used an LLM-as- Judge to rank the resulting answers for each ques- tion. The results, along with efficiency metrics, are presented in Table 4. The data reveals a clear and consistent pattern across the three multi-hop datasets (HotpotQA, 2WikiMultiHopQA, and Musique): • Optimal Performance at 2-3 Iterations: Moving from one to two iterations yields a substantial improvement in answer quality. On 2WikiMultiHopQA, the average quality rank improves from3.08to2.31. The peak performance is generally observed at either the second or third iteration (e.g., an average rank of2.18at iteration 3 for 2WikiMulti- HopQA). This is accompanied by a highIm- provement Rate, with the 2- or 3-iteration an- swer being judged superior to the 1-iteration answer in approximately70%of cases for 2WikiMultiHopQA. • Diminishing Returns:A fourth iteration con- sistently leads to a degradation in average an- swer quality across all complex datasets. This suggests a point of diminishing returns where an additional retrieval cycle is more likely to introduce noisy or tangentially related infor- mation that complicates the final synthesis step. • Cost-Benefit Analysis:Each iteration adds a considerable number of API calls and to- kens, increasing both latency and computa- tional cost. The optimal balance of 2-3 it- erations provides the best balance between answer quality and resource consumption. Conversely, on the simplerTriviaQAdataset, the quality rank degrades with each additional it- eration, confirming that for single-hop queries, the initial retrieval is generally sufficient, and further iterations are unnecessary and even detrimental. This analysis confirms that iteration is crucial for complex reasoning, but an unrestrained