substantially enhancing trustworthiness and reducing hallucination. Our main contributions are as follows: • We introduce a novel agentic RAG architec- ture centered on anIterative Refinement loop. This evidence-centric cycle is governed by an analytical gating mechanism we term Structured Evidence Assessment (SEA). By systematically deconstructing the query and identifying specific information gaps, the SEA module intelligently guides subsequent itera- tions. This process progressively builds and validates a comprehensive context, enabling the system to robustly handle complex, multi- faceted, and multi-hop queries where single- pass retrieval would fail. • We design a sophisticated, two-stage query strategy. It begins withsemantic decomposi- tionto ensure all facets of the initial query are addressed, and more importantly, incorporates anAdaptive Query Refinementmechanism that analyzes evidence gaps to intelligently generate new queries, effectively reasoning about what information is still missing. • We implement an integrated approach to re- source optimization throughdynamic re- source allocation. Our framework employs an initialAdaptive Routingmechanism to bypass the RAG pipeline for simple queries and dynamically assigns LLMs of varying sizes to internal tasks based on their com- plexity, achieving a superior balance between response quality, latency, and computational cost. • We propose a robust, two-pronged approach to guarantee faithfulness. This includes: (1) a pre-generationStructured Evidence Assess- ment (SEA), which performs a final analyt- ical pass to verify that all required findings from the initial query deconstruction are fully supported by the aggregated evidence, and (2) aconstrained generation promptthat en- forces citation and prevents the model from introducing external knowledge. This combi- nation ensures the final answer is both verifi- able and trustworthy. We conducted extensive experiments on a suite of four challenging open-domain QA bench- marks to evaluate the FAIR-RAG framework, en- compassing complex multi-hop reasoning tasks (HotpotQA(Yang et al., 2018),2WikiMulti- HopQA,Musique) and a large-scale single-hop factual dataset (TriviaQA). Our best-performing configuration,FAIR-RAG