adaptive RAG. FAIR- RAG significantly outperforms strong representa- tive baselines from its architectural class across all multi-hop benchmarks.It is critical to note that all comparisons were conducted under a unified and controlled experimental setup, ensuring a fair and reproducible evaluation.Within this rig- orous framework, our results not only demonstrate a consistent advantage in our direct head-to-head comparisons but alsoset a new state-of-the-art performance benchmark for this class of itera- tive methods on these datasets.While we refer- ence previously published results from comparable architectures (Shao et al., 2023; Asai et al., 2023; Jeong et al., 2024), our primary claim of superiority is grounded in re-evaluating these methods under our standardized conditions to eliminate confound- ing variables. The most substantial gains are observed on the multi-hop reasoning benchmarks. OnHotpotQA, our model achieves an F1 score of0.453, an abso- lute improvement of+8.3 pointsover the strongest baseline, Iter-Retgen (0.370). Similarly, on2Wiki- MultiHopQAandMusique, FAIR-RAG achieves F1 scores of0.320and0.264, respectively, outper- forming the next-best method (Self-RAG) by a sig- nificant margin. These results strongly validate the efficacy of our core architectural contributions: the Iterative Refinement Cycleand theStructured Evidence Assessment (SEA). These mechanisms empower FAIR-RAG to systematically deconstruct complex information needs, gather comprehensive evidence, and verify its sufficiency where single- pass or less structured iterative methods fall short. Notably, FAIR-RAG also excels on the single- hop factual benchmark,TriviaQA, achieving an F1 score of0.731. This demonstrates that the frame- workâ€™s sophisticated reasoning machinery does not impose a penalty on simpler retrieval tasks and can effectively streamline its process, largely due to the initialAdaptive Routingmodule. Comparing different variants of our model, we see a consistent performance increase from FAIR- RAG 1 to 4, indicating that the incremental en- hancements contribute positively. The introduction ofAdaptive LLMsfurther boosts performance across the board, confirming the benefits of dynam- ically allocating computational resources