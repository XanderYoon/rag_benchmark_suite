.540 .639 .774 IRCoT .006 .087 .399 .631 .001 .085.433 .488.001 .056 .210 .416 .016 .169 .674 .800 Iterative FAIR-RAG 1 .300 .398 .337 .628 .186 .288 .286 .414 .133 .216 .169 .418 .622 .706 .682 .821 FAIR-RAG 2 .335 .447 .397 .689 .216 .325 .341 .458 .168 .253 .214 .465 .645 .732 .712 .837 FAIR-RAG 3 .332 .447 .404 .697 .183 .305 .338 .450.178 .267 .221 .474 .631 .721 .701 .839 FAIR-RAG 4.344 .456 .401 .697 .209 .333 .357 .486 .175 .266 .228 .475 .644 .728 .710 .837 FAIR-RAG 2 (Adpt.) .331 .436 .384 .673 .183 .296 .313 .444 .158 .241 .207 .447 .640 .722 .704 .828 FAIR-RAG 3 (Adpt.) .338 .453 .399 .694 .206 .320 .350 .452.178 .264 .222 .472 .645 .731 .710 .847 Table 2: Main end-to-end performance comparison of FAIR-RAG against representative baselines across four diverse question-answering benchmarks. The evaluation covers three complex multi-hop datasets (HotpotQA, 2WikiMultiHopQA, Musique) and one open-domain factual dataset (TriviaQA). We report four metrics: Exact Match (EM), F1-Score, script-based Accuracy (ACC), and LLM-as-Judge Accuracy (ACCLLM). Our FAIR-RAG variants consistently achieve state-of-the-art performance, with the most significant improvements on the multi-hop tasks. The best-performing method for each metric is highlighted inbold. Underlined values denote results surpassing the Self-RAG and Iter-Retgen baselines. All scores are based on 1000 test/dev samples. dicated by the patterned bars) consistently achieve the highest F1 scores, particularly on the com- plex reasoning datasets of HotpotQA, 2WikiMulti- HopQA, and Musique. A key trend highlighted by the plots is thestrong positive correlation between the F1 score and the ACCLLM. This suggests that the architectural improvements within FAIR-RAG, which enhance the accuracy of the Large Language Modelâ€™s inter- nal processing and decision-making, are directly responsible for the enhanced final answer accu- racy. This relationship is particularly evident in the HotpotQA and 2WikiMultiHopQA