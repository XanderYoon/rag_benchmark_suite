for character. • F1 Score:A more lenient metric that com- putes the harmonic mean of precision and re- call at the token level. It accounts for partial overlaps and is less sensitive to minor phras- ing differences than EM. 4.3.2 Automated Evaluation using LLM-as-Judge Lexical metrics like EM and F1 are often insuf- ficient for evaluating generative models, as they unfairly penalize semantically correct answers that are phrased differently or contain additional, rele- vant context not present in the ground truth. To overcome this limitation, we employ LLM-as- Judge methodologies for two distinct, nuanced eval- uation purposes, strategically selecting the judge model based on task complexity. • End-to-End Semantic Correctness (ACCLLM):For the large-scale evaluation of our main results (Table 2), we introduce LLM-as-Judge Accuracy (ACCLLM). This metric requires a scalable and consistent binary judgment on whether a final prediction is semantically equivalent to any ground-truth answer. For this task, we use the highly capable“Meta-Llama-3-8B-Instruct” model as the judge. (AI@Meta, 2024) The specific prompt, designed for efficient “Yes” or “No” classification, is detailed in Appendix C.1 (Chiang et al., 2023; Zheng et al., 2023). • Component-Level Quality Score:Given the nuanced, generative nature of interme- diate outputs in our ablation study (e.g., Query Decomposition), a simple binary met- ric is insufficient. Therefore, we employ an LLM-as-Judge methodology (Zheng et al., 2023), building on established frameworks like G-Eval (Liu et al., 2023b) to ensure a scalable and consistent assessment. For this role, we selected Llama-4-Maverick-17B- 128E-Instruct-FP8. (AI@Meta, 2025) This highly capable model was specifically cho- sen not merely for its general performance, but for its demonstrated aptitude in complex reasoning and nuanced instruction following. These capabilities are critical for accurately assessing the quality of our internal compo- nents, where evaluation criteria are intricate and context-dependent. The reliability of this model as a proxy