Faithful'**: The answer contains significant or central factual claims that are not supported by, or actively contradict, the context. Provide your verdict in the following strict JSON format: {{ "faithfulness_verdict": "<One of three strings: 'Fully Faithful','Partially Faithful', or'Not Faithful'>", "reasoning": "<If not fully faithful, specify which claims in the answer are unsupported by the context. Explain if it's an invalid inference or a completely new fact.>" }} """, "iterative_improvement": """ You are an expert AI quality evaluator. For a single question, you are given four answers generated by the same system but with different levels of iterative refinement (1, 2, 3, and 4 iterations). Your task is to rank these answers from best to worst. [User Question]: "{question}" [Answer from 1 Iteration (iter_1)]: "{answer_1}" [Answer from 2 Iterations (iter_2)]: "{answer_2}" [Answer from 3 Iterations (iter_3)]: "{answer_3}" [Answer from 4 Iterations (iter_4)]: "{answer_4}" Rank these three answers from best (Rank 1) to worst (Rank 4). Provide your ranking in the following JSON format: {{ "ranking": ["<ID of the best answer, e.g., 'iter_3'>", "<ID of the second-best answer, e.g., 'iter_4'>", "<ID of the third-best answer, e.g., 'iter_2'>","<ID of the worst answer, e.g.,'iter_1'>"], "reasoning": "<A very brief explanation for your ranking, noting whether more iterations led to a clear improvement>" }} """ } D Failure Mode Analysis Prompt This appendix details the prompts engineered for our LLM-assisted failure mode analysis, as de- scribed in Section 5.2.4. To ensure a systematic and reproducible evaluation, we designed a two- part prompt structure. The PROMPT_SYSTEM prompt establishes the LLM’s persona as an “ex- pert evaluation researcher” and enforces a strict JSON output schema based on a predefined fail- ure taxonomy. The PROMPT_USER_TEMPLATE then provides the data batch and requires the model to ground its diagnosis in specific evidence from the logs, ensuring each classification is structured,