Iter 1) (#) HotpotQA 1 2.73 - 4.97 9,787 22.23 58.50%6.64 14,332 3 2.38 57.00% 7.83 17,281 4 2.66 57.00% 9.01 20,299 2WikiMultiHopQA 1 3.08 - 4.99 9,823 2 2.31 69.30% 7.10 15,413 32.18 70.90%8.79 19,812 4 2.43 67.30% 10.14 23,231 Musique 1 2.89 - 4.98 9,613 22.2063.40% 7.25 15,688 3 2.2863.70%9.01 20,162 4 2.63 61.20% 10.56 24,218 TriviaQA 11.83- 4.97 9,572 2 2.08 28.50% 5.97 12,071 3 2.70 27.20% 6.76 14,003 4 3.39 27.20% 7.25 15,186 Table 4: Ablation study on the impact of the maximum number of refinement iterations on answer quality versus computational cost. The results reveal a clear point of diminishing returns. For complex multi-hop datasets, optimal performance (lowest Avg. Answer Rank) is achieved at 2 or 3 iterations, after which quality degrades while costs (API Calls, Tokens) continue to increase linearly. Conversely, for the simpler fact-based TriviaQA, any iteration beyond the first proves detrimental. This analysis empirically justifies our framework’s default setting of a three-iteration maximum. the British Museum). It might retrieve a general document about the Mona Lisa that lacks architec- tural details, or miss one of the entities entirely. Second, and more fundamentally, standard RAG lacks the procedural logic to deconstruct the query, pursue two parallel reasoning paths, and then syn- thesize the findings into a coherent comparison. It relies on finding a single document that already compares the two museums’ architecture, which is highly improbable. Consequently, a standard RAG would likely produce a disjointed answer focusing on only one of the entities, or fail entirely. Why this query is difficult for other advanced RAGs: • AnITER-RETGENsystem, due to its inher- ently single-threaded nature, might success- fully follow one reasoning path (e.g., find the Louvre, then its style). However, it lacks the mechanism to manage a parallel track simul- taneously, causing it to