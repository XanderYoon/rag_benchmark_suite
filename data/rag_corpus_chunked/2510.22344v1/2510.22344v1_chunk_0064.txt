"Yes" or "No" judgment in a structured JSON format, enabling automated and consistent evaluation of semantic correctness. ACC_PROMPT = """You are an impartial judge. Evaluate whether the model's prediction correctly answers the given question. The prediction is correct if it implies ANY of the ground-truth answers provided. - Question: {question} - Ground-truth Answers (The prediction is correct if it matches ANY of these): {answer} - Prediction: {model_output} Does the Prediction imply any of the Ground-truth Answers? Respond with a JSON object containing a single key "judgment" with a value of "Yes" or "No". Example: {"judgment": "Yes"} """ C.2 Component-Level Quality Scoring This section provides the prompt template used for ourcomponent-level ablation study, with results presented in Table 3. Unlike the binary correct- ness evaluation in Appendix C.1, this prompt is designed for a more nuanced quality assessment. It instructs the LLM-as-Judge to evaluate the out- put of specific generative modules (i.e., Query De- composition and Query Refinement) and assign a quality score on a 1-to-5 Likert scale. This fine- grained analysis allows us to isolate and measure the efficacy of individual components within the FAIR-RAG pipeline. PROMPTS = { "query_decomposition": """ You are an expert AI evaluator specializing in search and query analysis. Your task is to assess the quality of query decomposition. Evaluate the generated sub-queries based on the original user question using the following criteria: 1. **Relevance:** How directly related is each sub-query to the main question? 2. **Coverage:** Do the sub-queries collectively cover all essential aspects of the main question? 3. **Efficiency:** Are the sub-queries concise, focused, and well-formed for a search engine? [User Question]: "{question}" [Generated Sub-Queries]: {sub_queries} Provide your assessment in the following JSON format: {{ "score": <A numeric score from 1.0 (Very Poor) to 5.0 (Excellent) based on the criteria above>, "reasoning": "<A very