selected Llama-4-Maverick-17B- 128E-Instruct-FP8. (AI@Meta, 2025) This highly capable model was specifically cho- sen not merely for its general performance, but for its demonstrated aptitude in complex reasoning and nuanced instruction following. These capabilities are critical for accurately assessing the quality of our internal compo- nents, where evaluation criteria are intricate and context-dependent. The reliability of this model as a proxy for human judgment in our specific tasks is not an unsubstantiated claim; as we will demonstrate in our validation study (Section 5.2.1), our LLM-as-Judge’s ratings show a strong correlation with those of hu- man experts, confirming its suitability for this evaluation. To ensure high-quality, structured feedback and mitigate potential biases, we de- signed custom prompts for each component. These prompts provide the judge with clear task definitions, explicit scoring criteria on a 1-to-5 scale, and illustrative examples. This rigorous, prompt-driven approach, detailed in Appendix C.2, ensures consistent and in- terpretable scores for a credible and multi- faceted analysis of our pipeline’s internal me- chanics. 4.3.3 Reliability of LLM-as-Judge Evaluations To establish the credibility of our dual LLM-as- Judge framework, we conducted two separate hu- man verification studies, one for each evaluation type. • Verification of Binary Semantic Correct- ness (ACCLLM):The reliability of the Llama- 3-8B-Instruct judge, used for the ACC LLM metric, was rigorously validated. A random subset of300question-answer pairs was sam- pled, stratified across all datasets. A human expert,blinded to the LLM’s original deci- sion to prevent bias, annotated these pairs for semantic correctness. The human judgments showed astrong degree of concordancewith the LLM-as-Judge’s binary “Yes/No” outputs, aligning in90%of the cases. This level of agreement is well within the range of typical human inter-annotator agreement for such a nuanced task. Therefore, we conclude that the LLM-as-Judge serves as a reliable and scalable proxy for human evaluation in our experiments.