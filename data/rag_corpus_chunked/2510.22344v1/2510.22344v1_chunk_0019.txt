is built upon the standardized and open-sourceFlashRAGtoolkit, ensuring a fair and reproducible comparison against existing meth- ods. 4.1 Datasets We selected a carefully curated suite of four bench- mark datasets to assess the various capabilities of our framework, with a particular focus on complex, multi-hop reasoning where standard RAG systems often underperform. • Multi-hop QA:We useHotpotQA,2Wiki- MultihopQA, andMusiQue. These datasets are specifically designed to require reason- ing and synthesizing information across multi- ple documents to arrive at an answer, making them ideal for evaluating our iterative refine- ment and adaptive query generation modules. • Open-Domain QA:We also useTriviaQA, a popular dataset for open-domain question an- swering, to ensure our model maintains strong performance on simpler, fact-based queries. For all experiments, we follow the standard prac- tice of using the official test split where available; otherwise, we report results on the development split. Consistent with recent studies on computa- tionally intensive RAG models, and to manage the substantial API and computational costs associated with iterative inference frameworks like ours, all evaluations are conducted on a randomly selected subset of 1000 samples from each dataset.This sample size was deliberately chosen to strike a critical balance between statistical robustness and experimental feasibility.It is large enough to ensure stable and meaningful performance com- parisons while enabling the comprehensive suite of ablation studies and baseline comparisons pre- sented in this work, which would be financially and logistically intractable on the full datasets. 4.2 Baselines We compare FAIR-RAG against a comprehensive set of representative RAG baselines implemented within the FlashRAG framework. These baselines cover different architectural paradigms: Dataset Task Type Source Samples HotpotQA multi-hop QA wiki 1000 2WikiMultiHopQA multi-hop QA wiki 1000 MusiQue multi-hop QA wiki 1000 TriviaQA Open-Domain wiki/web 1000 Table 1: Summary of Datasets • Standard RAG(Lewis et al., 2020): A con- ventional retrieve-then-read