al., 2025) To address this, SELF-RAG (Asai et al., 2023) fine- tunes an LLM to generate special “reflection to- kens”, enabling it to critique its own output for relevance and factual support in an inline fashion during generation. While effective, this approach has two limitations: its reliance on fine-tuning re- stricts applicability to off-the-shelf models, and its evaluation is inherentlytactical, assessing evi- dence on a step-by-step basis as the answer is being composed. FAIR-RAG addresses the faithfulness challenge differently, through an explicit, modular, and more strategicStructured Evidence Assessment (SEA) module. Instead of an inline critique, SEA acts as a distinct analytical gating mechanismbefore final answer generation. It first deconstructs the user’s query into a checklist of required findings. It then performs a holistic audit of theentireevi- dence corpus against this question-centric check- list to identify confirmed facts and explicit “intel- ligence gaps.” This distinction is crucial: whereas SELF-RAG prompts the model to ask, “Is this next piece of evidence useful for my current generation step?”, SEA forces the model to first ask, “Is the entiretyof my evidence sufficient to address all facets of the user’s original query?” The identi- fied gaps from this strategic assessment provide a precise, actionable signal for the subsequent query refinement step, transforming the check from a pas- sive validation into an active steering mechanism. Crucially, unlike fine-tuning-based methods, our modular SEA requires no model training, allowing for greater flexibility and easier integration with various off-the-shelf language models. In summary, while existing works have made substantial advancements, FAIR-RAG provides a novel contribution by synergistically integrating three core principles into a cohesive framework: (1) a structured, gap-aware iterative refinement loop, (2) context-aware, adaptive sub-query generation, and (3) an explicit, modular faithfulness assessment that requires no model fine-tuning. 3 Methodology The FAIR-RAG framework is designed as a multi-