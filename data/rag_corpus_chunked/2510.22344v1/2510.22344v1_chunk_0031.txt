an ablation study using an LLM-as-Judge method- ology on 1000 samples from each dataset. The results, summarized in Table 3, demonstrate the high functionality of each component, justifying its inclusion in the final architecture. For com- ponents likeQuery DecompositionandQuery Refinement, the judge rated the output quality on a 1-to-5 Likert scale. The rating was based on a holis- tic assessment of criteria includingrelevance(how well it addresses the core need),specificity(how focused the query is), andcoverage(whether it cap- tures all necessary facets of the information gap). The average score across 1000 samples is reported. ForEvidence Filtering, we report F1-Score based on the LLM’s ability to correctly classify docu- ments as relevant or irrelevant. ForSEA, we report Accuracy based on its correctness in judging evi- dence sufficiency. The analysis reveals several key insights: • Query Decomposition & Refinement are Highly Effective:The initialQuery De- compositionmodule achieves a high average quality score across all datasets, peaking at 4.33/5.0on TriviaQA. The subsequentQuery Refinementmodule scores even higher, with an average of4.45/5.0on HotpotQA. This Figure 2: Performance comparison of FAIR-RAG variants against baseline methods on four question-answering benchmarks. Each subplot displays the F1 score (bars, left axis) and the LLM-as-Judge Accuracy (ACCLLM, line, right axis). Our FAIR-RAG models are highlighted with a hatched pattern. The results consistently demonstrate the superiority of our framework, especially on complex multi-hop datasets (HotpotQA, 2WikiMultiHopQA, and MusiQue), where it significantly outperforms all baselines in F1 score while maintaining high semantic accuracy. All evaluations are conducted on 1000 samples from each benchmark’s development set. Query Decomp. Evidence Filter SEA Query Refine. Dataset Metric Value Metric Value Metric Value Metric Value HotpotQA Avg. Score 4.19 F1-Score 67.3% Accuracy 72.0% Avg. Score 4.45 2WikiMultiHopQA Avg. Score 4.12 F1-Score 55.5% Accuracy 81.7% Avg. Score 4.39 Musique Avg. Score 4.10 F1-Score 68.5% Accuracy 83.2% Avg. Score 4.42 TriviaQA