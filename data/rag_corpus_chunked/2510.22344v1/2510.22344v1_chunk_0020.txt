compare FAIR-RAG against a comprehensive set of representative RAG baselines implemented within the FlashRAG framework. These baselines cover different architectural paradigms: Dataset Task Type Source Samples HotpotQA multi-hop QA wiki 1000 2WikiMultiHopQA multi-hop QA wiki 1000 MusiQue multi-hop QA wiki 1000 TriviaQA Open-Domain wiki/web 1000 Table 1: Summary of Datasets • Standard RAG(Lewis et al., 2020): A con- ventional retrieve-then-read pipeline serving as a fundamental baseline. • Iterative Methods:We includeIter-Retgen (ITRG)(Shao et al., 2023), which uses the previous generation’s output to retrieve new documents, andIRCoT(Trivedi et al., 2023), which integrates retrieval within a Chain-of- Thought process. • Reasoning-based Methods:We compare againstReAct(Yao et al., 2022), a popular agent-based framework that interleaves rea- soning and action steps to solve problems. • Faithfulness-focused Methods: Self- RAG(Asai et al., 2023) is included as a strong baseline that incorporates explicit reflection and self-critique steps to improve faithfulness. •Branching & Conditional Methods:We in- cludeSuRe(Kim et al., 2024), which gen- erates and ranks multiple candidate answers, andAdaptive-RAG(Jeong et al., 2024), which uses a classifier to conditionally route queries through different execution paths. 4.3 Evaluation Metrics To provide a comprehensive assessment of our sys- tem’s performance, we employ a suite of metrics that capture both lexical accuracy and semantic correctness. 4.3.1 Lexical-Based Metrics Following standard practice for question-answering tasks, we first report two traditional, token-based metrics: • Exact Match (EM):This strict metric mea- sures the percentage of predictions that match one of the ground-truth answers exactly, char- acter for character. • F1 Score:A more lenient metric that com- putes the harmonic mean of precision and re- call at the token level. It accounts for partial overlaps and is less sensitive to minor phras- ing differences than EM. 4.3.2 Automated Evaluation using LLM-as-Judge Lexical metrics like EM and F1 are often insuf- ficient for evaluating generative models, as they