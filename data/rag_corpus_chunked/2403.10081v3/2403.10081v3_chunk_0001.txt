1 Introduction In recent years, large language models (LLMs) have made significant advancements across various natural language processing (NLP) tasks, quickly becoming a critical element in numerous AI appli- cations (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a; Scao et al., 2022; Zhang *swh22@mails.tsinghua.edu.cn †contributed equally ‡Corresponding Author: aiqy@tsinghua.edu.cn 1We have open-sourced all the code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main et al., 2022). Despite their impressive capabilities, these models often produce text that seems coher- ent and plausible but factually incorrect, a problem commonly known as hallucination (Maynez et al., 2020; Zhou et al., 2020; Liu et al., 2021; Ji et al., 2023; Su et al., 2024). To mitigate this issue, Retrieval-Augmented Generation (RAG) has emerged as a prominent solution. RAG enhances LLMs by retrieving and incorporating relevant information from external databases into the LLMs’ inputs. It has demon- strated superior effectiveness across numerous NLP challenges (Khandelwal et al., 2019; Borgeaud et al., 2022; Lewis et al., 2020; Guu et al., 2020; Izacard and Grave, 2020; Jiang et al., 2022; Shi et al., 2023). Traditional methods of RAG typically rely on single-round retrieval, using the LLM’s initial input to retrieve relevant information from external corpora. While this method is effective for straightforward tasks, it tends to fall short for complex multi-step tasks and long-form genera- tion tasks (Jiang et al., 2023). In contrast, dynamic RAG (Trivedi et al., 2022; Borgeaud et al., 2022; Ram et al., 2023; Jiang et al., 2023) performs mul- tiple times of retrieval during the generation pro- cess of LLMs. It includes two steps: identifying the optimal moment to activate the retrieval mod- ule (deciding when to retrieve), and crafting the appropriate query once retrieval is triggered (deter- mining what to retrieve). Depending on when and what to retrieve, a variety