DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models Weihang Su*1, Yichen Tang†1, Qingyao Ai‡1, Zhijing Wu2, Yiqun Liu1 1Department of Computer Science and Technology, Tsinghua University 2School of Computer Science and Technology, Beijing Institute of Technology Abstract Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation pro- cess of Large Language Models (LLMs). There are two key elements of this paradigm: iden- tifying the optimal moment to activate the re- trieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). How- ever, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for decid- ing when to retrieve often rely on static rules. Moreover, the strategies for deciding what to re- trieve typically limit themselves to the LLM’s most recent sentence or the last few tokens, while the LLM’s information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the Information Needs of LLMs. Our framework is specifically de- signed to make decisions on when and what to retrieve based on the LLM’s information needs during the text generation process. We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method1. 1 Introduction In recent years, large language models (LLMs) have made significant advancements across various natural language processing (NLP) tasks, quickly becoming a critical element in numerous AI appli- cations (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a; Scao et al., 2022; Zhang *swh22@mails.tsinghua.edu.cn †contributed equally ‡Corresponding Author: aiqy@tsinghua.edu.cn 1We have open-sourced all the code, data,