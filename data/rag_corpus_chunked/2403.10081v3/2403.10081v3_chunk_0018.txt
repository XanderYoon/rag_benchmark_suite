and effectiveness of DRAGIN in enhancing LLMs’ capabilities. (5) DRAGIN demonstrates more sub- stantial performance improvements in MultihopQA Table 3: Comparison of the frequency of retrieval mod- ule activation in dynamic RAG frameworks across all datasets. 2WMQA, HQA, SQA indicates 2WikiMulti- hopQA, HotpotQA, StrategyQA respectively. 2WMQA HQA SQA IIRC #Num #Num #Num #Num L13B FL-RAG 3.770 3.194 3.626 3.426 FS-RAG 3.131 4.583 4.885 4.305 FLARE 1.592 3.378 0.625 5.521 DRAGIN 2.631 3.505 4.786 2.829 L7B FL-RAG 3.342 3.809 3.757 2.839 FS-RAG 3.833 4.152 4.546 4.210 FLARE 0.941 1.064 1.271 1.095 DRAGIN 2.836 3.013 4.629 2.927 V13B FL-RAG 4.199 3.564 3.591 3.189 FS-RAG 3.720 5.701 6.820 6.032 FLARE 1.093 1.078 1.118 0.335 DRAGIN 2.542 3.184 3.744 3.120 tasks, such as 2WikiMultihopQA and HotpotQA, than in tasks requiring common sense reasoning, like those in the StrategyQA dataset. This differ- ence highlights DRAGIN’s specialized capability in managing complex, multi-step reasoning tasks. 5.2 Efficiency In this section, we investigate the efficiency of various dynamic RAG frameworks across multi- ple datasets. We measure efficiency based on the number of retrieval calls made, as outlined in Ta- ble 3. Due to the special design of FS-RAG, the #NUM for FS-RAG also indicates the average num- Table 4: The influence of the ‘When to Retrieve’ de- cision on various dynamic RAG frameworks, with the IIRC dataset as the evaluation benchmark. The best results are in bold. L13B indicates LLaMA2-13B-Chat, V13B indicates Vicuna-13b-v1.5. We fix the query for- mulation method, the last complete sentence generated by the LLM is selected as the query for all the baselines. EM F1 Prec. L13B FLARE 0.128 0.1599 0.1677 FL-RAG 0.155 0.1875 0.1986 FS-RAG 0.171 0.2061 0.2185 DRAGIN 0.187 0.2242 0.2319 V13B FLARE 0.097 0.1277 0.1324 FL-RAG 0.099 0.1285 0.1324 FS-RAG 0.103 0.1344 0.1358 DRAGIN 0.196 0.2367 0.2476 ber of sentences produced