at position i, which requires external knowl- edge and triggers the retrieval module. The QFS approach then focuses on this specific position to formulate a query. For the token at position i, the QFS method evaluates the attention weights across the preceding token sequence {ti−1, ti−2, ..., t1}. Since the generation of ti by the LLM is based on its interpretation of the entire preceding con- text, the attention weights reflect the model’s self- assessed importance of each token in generating ti. The QFS method prioritizes these tokens based on their attention scores, selecting the top n to- kens to construct the query. The query formula- tion process includes the following steps: (1) Ex- tract the attention scores of the last Transformer layer Ai = {ai,1, ai,2, ..., ai,i−1} for each token ti in T , where ai,j represents the attention score assigned by ti to tj; (2) Sort Ai in descending or- der to identify the top n tokens with the highest attention scores; (3) Find the words corresponding to these tokens from the vocabulary and arrange them according to their original order in the text; (4) Construct the query Qi using the words from these top n tokens, ensuring the query reflects the most relevant aspects of the context as determined by the LLM’s self-attention mechanism. 3.3 Continue Generation after Retrieval Once the RIND module detects the position i that needs external knowledge, the QFS module cre- ates the query and utilizes an off-the-shelf retrieval model (e.g. BM25) to retrieve relevant informa- tion from external knowledge bases. Suppose the retrieved documents are denoted as Di1, Di2, and Di3. Upon successful retrieval, the next step of the dynamic RAG framework is to integrate this exter- nal knowledge into the LLM’s generation process. This integration begins with truncating the LLM’s