timely retrieval activation and QFS for precise query formulation, DRAGIN significantly outperforms existing dynamic RAG methods across various knowledge-intensive bench- Table 7: Comparison of performance between BM25 and SGPT using the LLaMA2-13B-Chat model. The method with better performance is highlighted. retriever EM F1 Prec. 2WMQA BM25 0.304 0.393 0.395 SGPT 0.273 0.356 0.357 HQA BM25 0.314 0.424 0.437 SGPT 0.264 0.371 0.388 IIRC BM25 0.185 0.222 0.235 SGPT 0.169 0.201 0.207 marks. 7 Limitations We acknowledge the limitations of this paper. One of the primary limitations is the reliance on the self- attention mechanism of Transformer-based LLMs for both Real-time Information Needs Detection (RIND) and Query Formulation based on Self- attention (QFS). While self-attention scores are accessible for all open-source LLMs, itâ€™s important to note that our method is not applicable to certain APIs that do not provide access to the self-attention scores. Thus, our future work aims to develop more methods to overcome this constraint. 8 Ethics Statement In conducting this research, we have prioritized eth- ical considerations at every stage to ensure the re- sponsible development and application of AI tech- nologies. Our research does not rely on personally identifiable information or require manually anno- tated datasets. We firmly believe in the principles of open research and the scientific value of repro- ducibility. To this end, we have made all models, data, and code associated with our paper publicly available on GitHub. This transparency not only facilitates the verification of our findings by the community but also encourages the application of our methods in other contexts. Acknowledgments This work is supported by Quan Cheng Laboratory (Grant No. QCLZD202301). References Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Milli- can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving