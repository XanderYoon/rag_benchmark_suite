LLM requires external knowl- edge. In that case, the QFS module is triggered again at position j to generate a new query, re- trieving a new set of documents Dj1, Dj2, and Dj3 to replace Di1, Di2, and Di3. The LLM will then continue generating from position j based on the newly retrieved documents, following the same process. 4 Experimental Setup 4.1 Datasets We choose two MultihopQA datasets 2WikiMul- tihopQA (Ho et al., 2020) and HotpotQA (Yang et al., 2018) to evaluate the RAG framework’s abil- ity to answer complex questions that require multi- hop reasoning. We choose the IIRC (Ferguson 3The specific content of the prompt template is presented in Appendix F. et al., 2020) dataset to evaluate the RAG frame- work’s ability in reading comprehension tasks. Fur- thermore, we utilize the StrategyQA (Geva et al., 2021) dataset to evaluate the commonsense reason- ing capabilities of DRAGIN and other baselines. 4.2 Settings for each Dataset • 2WikiMultihopQA (Ho et al., 2020). We fol- low the setting of (Wang et al., 2022) to generate both chain-of-thought (CoT) reasoning process as well as the final answer. We follow the prompt template of (Trivedi et al., 2022) and (Jiang et al., 2023). For the evaluation metrics, we extract the final answer from the generated output using pat- tern matching techniques. The extracted answer is then compared with the reference answer, uti- lizing methods such as exact match at the answer level, along with token-level measurements of F1 score and precision. • HotpotQA (Yang et al., 2018). We follow the setting and the prompt template of (Trivedi et al., 2022) to generate both chain-of-thought (CoT) reasoning process as well as the final answer. Our evaluation metric on this dataset is the same as 2WikiMultihopQA. • StrategyQA (Geva et al., 2021). We follow the