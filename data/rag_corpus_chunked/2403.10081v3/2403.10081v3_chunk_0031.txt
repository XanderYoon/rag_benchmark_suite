Wikipedia cor- pus, and retrieved 3 documents for context learn- ing. In particular, we excluded questions without answers, so there are a total of 954 questions in IIRC. • StrategyQA. For the question "Is it common to see frost during some college commencements?", the output we aim to generate is "College com- mencement ceremonies can happen in December, May, and June. December is in the winter, so there can be frost. Thus, there could be frost at some commencements. So the answer is yes." We utilized 8 examples enclosed in (Wei et al., 2023), conducted experiments with BM25 as the retriever on the Wikipedia corpus, and retrieved 3 documents for context learning. B Evaluation Details In order to match the answers obtained by the model, we included the paradigm "So the answer is" in the exemplars to encourage the model to gen- erate in this format. Specifically, if "So the answer is" is absent from all of the model’s generations, during the evaluation phase, we append "So the an- swer is" to the end of the model’s output, prompting the model to generate again. Subsequently, we se- lect the words following "So the answer is" as the final answer. C Case Study We select the following question for case study: Question The arena where the Lewiston Maineiacs played their home games can seat how many people? This is a complex question that requires identify- ing the Arena’s name as well as finding the seating capacity of the arena. Initial Output of the LLM The arena where the Lewiston Maineiacs played their home games is the Androscog- gin Bank Colisée. The Androscoggin Bank Colisée has a seating capacity of 4,250. Therefore, the answer is 4,250. During the generation process of LLM, the RIND module identified that the LLM needs ex- ternal