Last Generated Tokens FS-RAG Per Sentence Last Generated Sentence FLARE Any token’s probability below the threshold Last generated Sentence exclude low-probability tokens DRAGIN Generated token’s importance and uncertainty LLM’s attention over the entire context same settings, with the only variation being the timing of triggering retrieval (when to retrieve) and the query formulation method when the retrieval is triggered (what to retrieve). • wo-RAG. LLM provides direct answers to ques- tions without RAG. • SR-RAG (Single-round RAG). Relevant pas- sages are retrieved from an external corpus based on the initial question. The retrieved passages are then added into the LLM’s input. • FL-RAG (Fix Length RAG) (Khandelwal et al., 2019; Borgeaud et al., 2022; Ram et al., 2023). A multi-round retrieval augmentation method that triggers the retrieval module every n tokens. The tokens generated in the previous token window are utilized as the query. • FS-RAG (Fix Sentence RAG) (Trivedi et al., 2022). A multi-round retrieval augmentation method that triggers the retrieval module every sentence. The last generated sentence are utilized as the query. • FLARE (Jiang et al., 2023). A multi-round retrieval augmentation method that triggers re- trieval each time it encounters an uncertain token. When the retrieval module is triggered, the last generated sentence without the uncertain tokens are defines as the query. To illustrate the differences between DRAGIN and other dynamic RAG baselines directly, we present a comparison of retrieval timing and query formation methods for each dynamic RAG frame- works in Table 1. 4.4 Selected LLMs To validate the effectiveness of DRAGIN and other RAG baselines, we conducted experiments with the following LLMs: • LLaMA-2-Chat (Touvron et al., 2023b). LLaMA2 is a collection of pre-trained and fine- tuned LLMs. This series includes fine-tuned LLMs, known as Llama2-Chat, specifically de- signed for optimal performance in dialogue- based