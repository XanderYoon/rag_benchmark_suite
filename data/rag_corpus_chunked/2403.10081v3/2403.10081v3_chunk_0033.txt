Lewiston, Maine, that opened in 1958. The Androscoggin Bank Colisée was built to ...... Thus, after including the retrieved passage to the LLM, it can then generate the correct answer: 3,677. The final revised output is as follows: Final Output The arena where the Lewiston Maineiacs played their home games is the Androscog- gin Bank Colisée. The Androscoggin Bank Colisée has a seating capacity of 3,677. Therefore, the answer is 3,677. In contrast, the FLARE framework decides to trigger retrieval for the first sentence because the probability of the tokens ‘Lewiston’, ‘home’, ‘An- droscoggin’, and ‘Bank’ are all below the FLARE’s threshold: Initial output of the LLM The arena where the Lewiston Maineiacs played their home games is the Androscoggin Bank Colisée. After that, FLARE generates the following query that removes all the tokens below the threshold from the first sentence: The generated query of FLARE The arena where the Maineiacs played their games is the Colisée. Unfortunately, the generated query omitted the correct information from the most recent sentence, leading to an irrelevant passage retrieval for the LLM’s real-time information need. Thus, the LLM did not answer this question correctly. D Error Analysis When the RIND module triggers retrieval, the DRAGIN framework adds multiple passages to the input of the LLM, thereby extending the context length. As a result, LLMs that typically perform poorly with long contexts may become confused during the generation process and mix up the infor- mation from these passages. We select the following case: Question What is the name of the fight song of the uni- versity whose main campus is in Lawrence, Kansas, and whose branch campuses are in the Kansas City metropolitan area? Our framework first detects that the LLM needs to determine which university’s main campus is located in Lawrence, Kansas, and