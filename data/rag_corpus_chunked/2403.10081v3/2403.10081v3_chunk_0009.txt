retrieval decision-making process. Combining uncertainty, significance, and seman- tics, RIND computes a comprehensive score for each token ti: SRIN D(ti) = Hi · amax(i) · si (5) Let T = {t1, t2, . . . , tn} represent the set of tokens already generated by the LLM. The retrieval mod- ule activates when the score SRIN D(ti) for any token exceeds a predefined threshold, θ. 3.2 Query Formulation based on Self-attention Once the position to conduct retrieval augmentation is determined, the next step in the RAG framework is to formulate a query to retrieve necessary infor- mation from external databases for the continued generation of LLMs. In the existing dynamic RAG frameworks, all the query formulation methods limit their focus to the LLM’s most recent sentence or the last few tokens. This narrow scope fails to ad- equately cater to the model’s real-time information needs, which may span across the entire context. To overcome the shortcomings of these approaches, we propose a novel strategy that utilizes the self- attention mechanisms inherent in Transformer- based LLMs. Our method, termed "Query For- mulation based on Self-Attention" (QFS), seeks to ascertain the LLM’s information needs more pre- cisely by examining its understanding of the full context. Consider an output sequence generated by an LLM, denoted as T = {t1, t2, . . . , tn}, with each ti representing an individual token within the se- quence. Suppose the RIND module identifies the token at position i, which requires external knowl- edge and triggers the retrieval module. The QFS approach then focuses on this specific position to formulate a query. For the token at position i, the QFS method evaluates the attention weights across the preceding token sequence {ti−1, ti−2, ..., t1}. Since the generation of ti by the LLM is based on its