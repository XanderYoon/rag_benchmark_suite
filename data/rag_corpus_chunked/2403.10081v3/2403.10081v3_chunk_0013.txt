with token-level measurements of F1 score and precision. • HotpotQA (Yang et al., 2018). We follow the setting and the prompt template of (Trivedi et al., 2022) to generate both chain-of-thought (CoT) reasoning process as well as the final answer. Our evaluation metric on this dataset is the same as 2WikiMultihopQA. • StrategyQA (Geva et al., 2021). We follow the setting of (Wei et al., 2022) to generate both the CoT reasoning process as well as the final answer. We follow the prompt template of (Wei et al., 2022) and (Jiang et al., 2023). For the evaluation metrics, the obtained yes/no response is extracted and compared with the standard correct answer using an exact match approach. • IIRC (Ferguson et al., 2020). We follow the set- ting and the prompt template of (Trivedi et al., 2022) to generate the final answer. Our evalua- tion metric on this dataset is the same as 2Wiki- MultihopQA. Besides the settings introduced in this section, the specific prompt templates corresponding to each dataset are presented in Appendix F. Ap- pendix A provides more detailed descriptions of each dataset’s settings. 4.3 Baselines We choose the following Text Generation base- lines for comparison. Following the setting of FLARE (Jiang et al., 2023), we implemented the existing multi-round RAG frameworks using the Table 1: A comparative overview of our selected Retrieval-Augmented Generation baselines. Timing for Retrieval Query Formulation SR-RAG Before Generation Initial Input FL-RAG Per n Tokens Last Generated Tokens FS-RAG Per Sentence Last Generated Sentence FLARE Any token’s probability below the threshold Last generated Sentence exclude low-probability tokens DRAGIN Generated token’s importance and uncertainty LLM’s attention over the entire context same settings, with the only variation being the timing of triggering retrieval (when to retrieve) and the query formulation method when the retrieval is triggered (what