token ti, we quantify its influ- ence by recording the maximum attention value amax(i), which records the maximum attention from all following tokens 2. The attention matrix 2We choose the attention scores of the last Transformer layer of the LLM. A is computed as follows: A = softmax  mask  QK ⊤ √dk  , (2) where Q is the query matrix, K is the key matrix, dk denotes the dimensionality of a key vector. The mask function is applied to the scaled dot-product matrix M = QK⊤ √dk to prevent each position from attending to subsequent positions. Specifically, for the square matrix M (with dimensions equal to the sequence length), we set Mi,j = −∞ whenever i < j . This effectively masks out the upper triangu- lar part of the matrix. The softmax function is then applied row-wise to produce the attention weights. Following this, the maximum attention value amax(i) for token ti is identified by locating the highest Aj,i for all j > i : amax(i) = max j>i Aj,i (3) Consider the semantic contribution of each to- ken, RIND employs a binary semantic indicator to filter out stopwords, thus concentrating on tokens with significant semantic value: si = ( 0, if ti ∈ S 1, otherwise , (4) where S is the stopwords set,si is the semantic con- tribution score of the token ti. This process ensures that only semantically potent tokens contribute to the retrieval decision-making process. Combining uncertainty, significance, and seman- tics, RIND computes a comprehensive score for each token ti: SRIN D(ti) = Hi · amax(i) · si (5) Let T = {t1, t2, . . . , tn} represent the set of tokens already generated by the LLM. The retrieval mod- ule activates when the score SRIN D(ti) for any token