from the offi- cial Hugging Face repositories for each model, and use the code provided by Hugging Face to conduct text generation. For the generation configuration, we have chosen greedy decoding as the decoding strategy for LLM inference to ensure the repro- ducibility of our experimental results. However, for practical applications, we recommend using the of- ficial default generation configuration provided by each model, as this will yield better performance. 5 Experimental Results 5.1 Overall Results of DRAGIN and Baselines Our experiments comprehensively evaluate the performance of DRAGIN against various base- lines across four benchmark datasets: 2WikiMul- Table 2: The overall experimental results of DRAGIN and other baselines on four benchmarks. The best results are in bold. 2WikiMultihopQA HotpotQA StrategyQA IIRC LLM RAG Method EM F1 EM F1 Accuracy EM F1 Llama2-13b-chat wo-RAG 0.187 0.2721 0.223 0.3097 0.650 0.168 0.2039 SR-RAG 0.245 0.3364 0.263 0.3706 0.654 0.196 0.2303 FL-RAG 0.217 0.3054 0.177 0.2682 0.648 0.155 0.1875 FS-RAG 0.270 0.3610 0.267 0.3715 0.655 0.171 0.2061 FLARE 0.224 0.3076 0.180 0.2756 0.655 0.138 0.1667 DRAGIN (Ours) 0.304 0.3931 0.314 0.4238 0.689 0.185 0.2221 Llama2-7b-chat wo-RAG 0.146 0.2232 0.184 0.2745 0.659 0.139 0.1731 SR-RAG 0.169 0.2549 0.164 0.2499 0.645 0.187 0.2258 FL-RAG 0.112 0.1922 0.146 0.2107 0.635 0.172 0.2023 FS-RAG 0.189 0.2652 0.214 0.3035 0.629 0.178 0.2157 FLARE 0.143 0.2134 0.149 0.2208 0.627 0.136 0.1644 DRAGIN (Ours) 0.220 0.2926 0.232 0.3344 0.641 0.192 0.2336 Vicuna-13b-v1.5 wo-RAG 0.146 0.2232 0.228 0.3256 0.682 0.175 0.2149 SR-RAG 0.170 0.2564 0.254 0.3531 0.686 0.217 0.2564 FL-RAG 0.135 0.2133 0.187 0.3039 0.645 0.0985 0.1285 FS-RAG 0.188 0.2625 0.185 0.3216 0.622 0.1027 0.1344 FLARE 0.157 0.2257 0.092 0.1808 0.599 0.1174 0.1469 DRAGIN (Ours) 0.252 0.3516 0.288 0.4164 0.687 0.2233 0.2652 tihopQA, HotpotQA, StrategyQA, and IIRC. The results, summarized in Table 2, underscore several critical insights: (1) The integration of