the input for the model. Previous research has explored single-round re- trieval augmentation extensively. For instance, RE- PLUG (Shi et al., 2023) treats LLMs as a black box and leverages them to generate training data for the retrieval model. From a different perspective, UniWeb (Li et al., 2023d) proposes an adaptive search engine-assisted learning method that can self-assess whether the LLM requires retrieval aug- mentation. 2.2 Multi-round Retrieval-augmented LLM Single-round retrieval can be relatively effective for simple tasks or cases where user information needs are clear-cut. However, for complex tasks or tasks involving the generation of lengthy text, such as long-form question answering, multi-hop reasoning, chain-of-thought reasoning, etc., rely- ing solely on the userâ€™s initial input for retrieval Input:Pleasegivemea brief introduction to Einstein LLMGenerating: RIND:Real-time Information NeedDetectionContinueGenerationaposition at the Universityofğ“—ğ’Šğ’‚ğ’ğ’‚ğ’™(ğ’Š)ğ’”ğ’Š 0.561.350.030.012.560.020.430.220.260.350.760.15010010 Ã—Ã—Ã—Ã—Ã—Ã—Ã—Ã—Ã—Ã—Ã—Ã— ...... 00.16001.950ğ‘ºğ‘¹ğ‘°ğ‘µğ‘« RetrievalModule QFS:Query Formulation based on Self-attentionPlease givemea brief introduction to Einstein<SEP>Einsteinwas born in the German Empire and moved to Switzerland in 1895. In 1903, he secureda jobat the ğ‘¨ğ’ğ’š ğ‘ºğ‘¹ğ‘°ğ‘µğ‘«>ğœ½ğ‘¨ğ’ğ’ ğ‘ºğ‘¹ğ‘°ğ‘µğ‘«<ğœ½ Query:Einstein1903securedjob LLMContinualGenerationbased on External Knowledge RetrievedExternalKnowledge Einstein was born in the German Empire and moved to Switzerland in 1895. In 1903, he secured a jobat the University of Zurich,where he began to establish himself as a leading physicist. During his time at the University of ZurichI Einstein was born in the German Empire and moved to Switzerland in 1895. In 1903, he secured a jobat the Swiss Patent Officein Bern. This position allowed him to have a stable income I Figure 1: An illustration of our DRAGIN framework. may not adequately cover all the external knowl- edge that the model requires (Jiang et al., 2023). Therefore, some researchers have begun to ex- plore multi-round retrieval augmentation. For ex- ample, RETRO (Borgeaud et al., 2022) and IC- RALM (Ram et al., 2023) trigger retrieval every 4