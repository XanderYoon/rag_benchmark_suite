the necessity of retrieval augmentation not only depends on the generation confidence, but also depends on the importance of the token, the se- mantic of the token, and the influence of each token on subsequent tokens. To address the limitations of the existing approaches, we propose an enhanced approach for triggering retrieval within dynamic RAG frameworks, named Real-time Information Needs Detection (RIND). This method refines the retrieval activation process by evaluating not only the uncertainty of each token, but also its seman- tic contribution and the impact on the following context. RIND begins by quantifying the uncertainty of each token generated during the LLM’s inference process. This is accomplished by recording the en- tropy of the token’s probability distribution across the vocabulary. Consider an output sequence gener- ated by an LLM, denoted as T = {t1, t2, . . . , tn}, with each ti representing an individual token within the sequence at position i. For any token ti, the entropy Hi is computed as follows: Hi = − X v∈V pi(v) log pi(v), (1) where pi(v) denotes the probability of generating the token v over all tokens in the vocabulary V at position i. This measurement of uncertainty serves as the first dimension in our multi-faceted evaluation of tokens. In addition, RIND leverages the self-attention mechanism inherent in Transformer-based LLMs to allocate weights to tokens, which represent the tokens’ impact on the subsequent context. Specifi- cally, for any given token ti, we quantify its influ- ence by recording the maximum attention value amax(i), which records the maximum attention from all following tokens 2. The attention matrix 2We choose the attention scores of the last Transformer layer of the LLM. A is computed as follows: A = softmax  mask  QK ⊤ √dk  , (2) where Q is