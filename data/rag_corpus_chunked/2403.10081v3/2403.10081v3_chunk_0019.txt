mulation method, the last complete sentence generated by the LLM is selected as the query for all the baselines. EM F1 Prec. L13B FLARE 0.128 0.1599 0.1677 FL-RAG 0.155 0.1875 0.1986 FS-RAG 0.171 0.2061 0.2185 DRAGIN 0.187 0.2242 0.2319 V13B FLARE 0.097 0.1277 0.1324 FL-RAG 0.099 0.1285 0.1324 FS-RAG 0.103 0.1344 0.1358 DRAGIN 0.196 0.2367 0.2476 ber of sentences produced by the LLM in response to queries on this dataset. Among the evaluated frameworks, FLARE stood out for its efficiency, re- quiring the fewest retrieval calls across all datasets. DRAGIN followed closely, with fewer retrieval calls than FS-RAG and FL-RAG. 5.3 Timing of Retrieval In this subsection, we investigate the impact of the timing of retrieval on the performance of dy- namic RAG frameworks. Specifically, we fixed the method of query formulation to use the last com- plete sentence generated by the LLM as the query, and varied the timing of retrieval as the only vari- able. We examined DRAGIN alongside three exist- ing frameworks: FLARE, FL-RAG, and FS-RAG on the IIRC dataset. As shown in Table 4, DRA- GIN consistently outperforms all other dynamic RAG methods. This highlights the effectiveness of our novel approach to determining the optimal moment for retrieval. DRAGIN’s superior perfor- mance suggests that its method for detecting the real-time information needs of LLMs and trigger- ing retrieval accordingly is particularly adept at enhancing the quality of the generated text. We also evaluate the impact of varying thresh- old values within the RIND module on the perfor- mance of the DRAGIN framework. We present the experimental results on the HotpotQA dataset in Table 5. Our experimental results show that DRA- GIN’s performance remains stable across threshold settings, indicating a low sensitivity to changes in this hyperparameter. The threshold value is pivotal in determining the retrieval module’s