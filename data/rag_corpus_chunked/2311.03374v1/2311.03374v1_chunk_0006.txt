we use a set of 9048 comments (from Github) with comment text, surrounding code snippets, and a label that specifies whether the comment is useful or not. Sample data has been characterised in Table 1. • The development dataset contains 8048 rows of comment text, surrounding code snippets, and labels (Useful and Not useful). • The test dataset contains 1,000 rows of comment text, surrounding code snippets, and labels (Useful and Not useful). 4. Participation and Evaluation IRSE 2023 received a total of 56 experiments from 17 teams for the two tasks. As this track is related to software maintenance, we received participation from companies like Microsoft, Amazon, Amercian Express, Bosch Research along with several research labs of educational institutes. The various teams with the details of their submissions are characterised in Table 2. Evaluation Procedure: Candidates submited the prediction metrics (precision,recall, F1-Score) fo the classification model with the Gold labels dataset (referred to as the Seed Dataset) and combined dataset (Seed + LLM generated labels - Silver lables dataset). The difference in the F1 score was evaluated by us. Features: Apart from evaluating the prediction metrics, we analysed the types of features the teams have used to devise the machine learning pipeline. The teams have performed routine pre-processing and have retained the significant words or letters only for both the code and comment pairs. Further, some of the teams have also used morphological features of a comment Table 2 Characterizations of the Submissions: test Data Predictions Affiliation Seed data Seed data + LLM-generated data Precision Recall F1Score Precision Recall F1Score DSTI, France 0.8326 0.8626 0.8473 0.844 0.8682 0.8559 0.8948 0.8738 0.884 0.9 0.8707 0.885 0.8807 0.8822 0.8813 0.8871 0.8839 0.8854 SSN-1 (RAM) 0.8 0.8 0.8 0.8021 0.81 0.73 0.72 0.71 0.74 0.7 0.73 0.74 SSN-2 (Aloy) 0.788 0.7363 0.7613