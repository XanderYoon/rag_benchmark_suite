match to find similar words. Vector Space Representations: Code and comments belong- to different semantic granularity which is unified by a vector space representation. The participants have used various pre-trained embeddings to generate vectors for the words like those based on one hot encoding, tf-idf based, word2vec or context aware like ELMo and BERT. Each of the employed embedding models are trained or finetuned using software development corpora. Results: The participants are able to achieve a slight increase (in the range of 2%-4%) in the test prediction metrics and in many cases the performance decrease. However, the increase in bias due to the incorporation of silver standard data reduces the over-fitting of the models. Table 3 Characterizations of the LLM Generated datasets Team name Total entry Useful entry Not useful entry DSTI, France 421 412 9 SSN-1 (RAM) 1238 740 497 SSN-2 (Alloy) 1510 24 1486 IIT (ISM) Dhanbad 199 182 17 SSN 3 (Black) 738 80 658 Microsoft - American Express 233 92 141 DDU-1 8588 4649 3939 DDU-2 332 311 21 IIT KGP-1 334 309 25 SRM 217 196 21 IITKGP-2 263 130 133 DA-IICT 150 65 85 IIT-Goa 543 460 83 TCS 282 61 221 IITKGP-3 570 450 120 IITKGP-3 412 345 67 5. Conclusions The IRSE 2023 track empirically investigates the feasibility of augmenting existing classification models using datasets with labels generated from LLMâ€™s. A total of 17 teams participated and submitted 56 experiments that used various types of machine learning models, embedding spaces, features and different LLMs to generate data. The LLM-generated labels reduce the overfitting of the overall classification model and also improve the F1 score when the combined data from all participants were used to augment the existing data with gold standard labels from industry practitioners. References [1] A. Bosu, M. Greiler, C.