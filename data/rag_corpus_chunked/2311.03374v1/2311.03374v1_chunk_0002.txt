or inconsistencies of these concepts with the related code constructs in a machine learning framework for an overall assessment. The concepts are derived through exploratory studies with developers across 7 companies and from a larger community using crowd-sourcing. The first edition of the IRSE track of FIRE 2023, extends the work in [ 2] and empirically investigates comment quality with a larger set of machine learning solvers and features. The task is based on the quality evaluation of comments into two clusters - ’useful’ and ’not useful’. A ’useful’ comment (refer Table 1) contains relevant concepts that are not evident from the surrounding code design, and thus increases the comprehensibility of the code. The suitability of analysing comment quality using various vector space representations of code and comment pairs along with standard textual features and code comment correlation links are evaluated. Table 1 Useful and Not-Useful comments in context of code comprehension # Comment Code Label 1 /* uses png_calloc defined in pn- griv.h*/ /* uses png_calloc defined in pngriv.h*/ PNG_FUNCTION(png_const_structrp png_ptr) { if (png_ptr == NULL || info_ptr == NULL) return; png_calloc(png_ptr); ...} U 2 /* serial bus is locked before use */ static int bus_reset ( . . . ) /* serial bus is locked before use*/ { .. update_serial_bus_lock (bus * busR); } NU 3 // integer variable int Delete\_Vendor; // integer variable NU U: Useful; NU: Not Useful The 2023 IRSE track extends this challenge to understand the feasibility of using silver standard quality labels generated from the Large Language Models (LLMs) and understand how it augments the classification model in terms of prediction. Developing the gold industry standard for analysing the usefulness of comments that can be relevant for code comprehension in legacy systems can be challenging and time-consuming. However, to scale the model and