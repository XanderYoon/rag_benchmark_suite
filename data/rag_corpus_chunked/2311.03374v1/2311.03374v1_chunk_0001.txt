15ğ‘¡â„ âˆ’ 13ğ‘¡â„ December, 2023 *Corresponding author. â€  These authors contributed equally. /envelâŒ¢pe-âŒ¢penmajumdar.srijoni@gmail.com (S. Majumdar); soumenpaul165@gmail.com (S. Paul); debjyoti93.paul@gmail.com (D. Paul); bandyopadhyay.ayan@gmail.com (A. Bandyopadhyay); samiran.chattopadhyay@jadavpuruniversity.in (S. Chattopadhyay); ppd@cse.iitkgp.ac.in (P. P. Das); p.d.clough@sheffield.ac.uk (P. D. Clough); prasenjit.majumder@gmail.com (P. Majumder) Â© 2023 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings http://ceur-ws.org ISSN 1613-0073 CEUR Workshop Proceedings (CEUR-WS.org) arXiv:2311.03374v1 [cs.SE] 27 Oct 2023 1. Introduction Assessing comment quality can help to de-clutter code bases and subsequently improve code maintainability. Comments can significantly help to read and comprehend code if they are consistent and informative. The perception of quality in terms of the â€™usefulnessâ€™ of the information contained in com- ments is relative and hence is perceived differently based on the context. Bosu et al. [1] attempted to assess code review comments (logged in a separate tool) in the context of their utility in helping developers write better code through a detailed survey at Microsoft. A similar quality assessment model is important to analyse the type of source code comments that can help for standard maintenance tasks but is largely missing. Majumdar et al. [2] proposed a comment quality evaluation framework wherein comments were assessed as â€™usefulâ€™, â€™partially usefulâ€™, and â€™not usefulâ€™ based on whether they increase the readability of the surrounding code snip- pets. The authors analyse comments for concepts that aid in code comprehension and also the redundancies or inconsistencies of these concepts with the related code constructs in a machine learning framework for an overall assessment. The concepts are derived through exploratory studies with developers across 7 companies and from a larger community using crowd-sourcing. The first edition of the IRSE track of FIRE 2023, extends the work in [ 2] and empirically investigates comment quality with