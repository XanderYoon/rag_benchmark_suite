understand the feasibility of using silver standard quality labels generated from the Large Language Models (LLMs) and understand how it augments the classification model in terms of prediction. Developing the gold industry standard for analysing the usefulness of comments that can be relevant for code comprehension in legacy systems can be challenging and time-consuming. However, to scale the model and use it on different languages, it is important to generate more data which we attempt to do with the large language models. The performance of these modes in the context of understating the relations between code and comment can provide an approximation of the data quality generated and how it can be used to scale the existing classification mode. This approach can also be further generalised to any classification model based on software metadata. 2. Related Work Software metadata is integral to code maintenance and subsequent comprehension. A significant number of tools [3, 4, 5, 6, 7, 8] have been proposed to aid in extracting knowledge from software metadata like runtime traces or structural attributes of codes. In terms of mining code comments and assessing the quality, authors [ 9, 10, 11] compare the similarity of words in code-comment pairs using the Levenshtein distance and length of comments to filter out trivial and non-informative comments. Rahman et al. [12] detect useful and non-useful code review comments (logged-in review portals) based on attributes identified from a survey conducted with developers of Microsoft [1]. Majumdar et al. [2, 13] proposed a framework to evaluate comments based on concepts that are relevant for code comprehension. They developed textual and code correlation features using a knowledge graph for semantic interpretation of information contained in comments. These approaches use semantic and structural features to design features to set up a prediction problem for useful and