the Cog- Bench test set, encompasses two primary dimensions: the overall performance of the MARG system and the analysis of the planning procedure within CogPlanner. We utilize the following backbone MLLMs, GPT-4o [9], Qwen2-VL-72B-Instruct [28], Pixtral-Large- Instruct [11], and our fine-tuned Qwen2-7B-VL-Cog. Notably, QVQ- 72B-Preview serves as a representative MLLM for advanced multi- modal reasoning capabilities. 6.1.1 End-to-End MRAG Performance.We conduct six distinct ex- perimental configurations across all selected MLLMs. The base- line configuration employs the original MLLMs, where multimodal queries are processed directly by the MLLM. We then examine two intermediate configurations: one incorporating fixed image retrieval based on visual query components, and another utilizing fixed text retrieval driven by textual query components. Besides, we employ the self-reflective RAG framework4 [2] as a reflective and it- erative competitive framework. The core evaluation focuses on both parallel and sequential modeling implementations of CogPlanner. For performance metrics, we adopt both token-level and claim- level evaluations, inspired by [24]. Token-level evaluation is per- formed using the F1 score, measuring the overlap of common tokens 4https://github.com/langchain-ai/langgraph/tree/main/examples/rag CogPlanner: Unveiling the Potential of Agentic Multimodal Retrieval Augmented Generation with Planning SIGIR-AP 2025, December 7-10, 2025, Xiâ€™an, China Table 2: Performance comparison between CogPlanner and baseline MRAG methodologies on CogBench. Precision and recall are evaluated at the claim level, while the F1 score is assessed at the token level. The diverse planning procedures required by CogBench lead to performance degradation across all fixed pipeline baselines, whereas CogPlanner demonstrates substantial improvements. Model Reasoning-Steps Overall Performance 1-hop 2-hop > 2-hop Precision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1 Origin MLLMs GPT-4o 33.49 37.44 10.01 44.38 59.46 39.84 16.17 24.74 12.83 29.07 38.85 21.21 Pixtral-Large-Instruct 24.30 41.87 5.19 37.65 54.64 34.51 10.77 24.50 7.13 22.45 38.05 15.81 QVQ-72B-Preview 34.15 22.10 10.56 40.51 31.29 21.48 19.42