52.29 38.96 20.95 30.34 11.12 29.15 39.00 19.13 QVQ-72B-Preview 36.38 36.44 48.16 43.90 29.60 24.43 22.19 25.22 26.69 32.29 28.97 30.33 Qwen2-VL-72B-Instruct 35.50 48.77 8.23 45.88 47.08 38.51 21.64 24.00 15.71 32.45 36.90 21.74 CogPlanner with Sequential Modeling GPT-4o 32.92 33.46 10.14 49.67 54.68 43.26 28.03 33.38 15.26 36.21 40.4623.49 Pixtral-Large-Instruct 22.60 36.39 5.57 37.18 54.32 39.60 9.96 29.96 11.28 21.57 39.36 19.51 QVQ-72B-Preview 35.62 45.95 51.69 43.36 44.07 25.37 19.54 21.18 27.27 30.73 33.8431.63 Qwen2-VL-72B-Instruct 36.88 48.86 7.84 42.94 44.45 37.48 21.57 25.01 14.27 31.79 36.33 20.65 between the generated response and the ground truth. Specifically, we use the NLTK tokenizer to segment the generated answers and ground truth. For claim-level evaluation, we utilize both precision and recall, calculated by first extracting claims from both the golden and generated answers using GPT-4o. Precision measures the pro- portion of correct claims within the generated responses, while recall evaluates the proportion of correct claims relative to the ground-truth answer claims. 6.1.2 Planning Procedure Performance.In addition to the overall MRAG performance, we examine the efficiency of CogPlanner’s planning procedure, with emphasis on query reformulation, by comparing its reformulated queries with those annotated by human Table 3: Performance of query reformulation across different MLLMs. Category Model BLEU Rouge F1 Prompting GPT-4o 0.1629 0.4951 0.5375 Parallel GPT-4o 0.1922 0.5266 0.5620 Pixtral-Large-Instruct 0.1678 0.4614 0.5089 Qwen2-VL-72B-Instruct 0.0907 0.4140 0.4472 Sequential GPT-4o 0.1739 0.5050 0.5460 Pixtral-Large-Instruct 0.1773 0.4707 0.5221 Qwen2-VL-72B-Instruct 0.0918 0.4266 0.4643 SIGIR-AP 2025, December 7-10, 2025, Xi’an, China Trovato et al. experts. We evaluate three distinct approaches: parallel modeling, sequential modeling, and direct reformulating query through GPT- 4o with optimized prompt engineering. The comparative analysis employs standard metrics, including BLEU [22], ROUGE [18], and F1 scores to assess the quality and relevance of the reformulated query outputs against established ground truth. 6.1.3 Implementation Details.In