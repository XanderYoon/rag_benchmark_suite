efficiency and reduce the resource requirements of the CogPlanner framework, we aim to achieve a more light- weight integration within MRAG systems. Building upon the Cog- Bench, we introduce a specialized fine-tuning strategy tailored for smaller, resource-efficient MLLMs to broaden the applicability of the CogPlanner framework and mitigate resource constraints. We Entertainment 13.3% Sports 22.2% Politics & History 8.1% Technology & Science 31.1% Business & Economics 6.7% Travel & Transportation 9.6% Architecture & Design 2.2% Culture & Society 5.2% Military 1.5% Double Ring Pie Chart Figure 3: Domain distribution of CogBench. utilize the Qwen2-VL-7B-Instruct as the backbone and employ the CogBench training set as specialized training data. To maintain a balance between the fine-tuning process and the retention of the Qwen2-VL-7B-Instruct modelâ€™s general capabilities, we aug- ment the training dataset with general instruction data at a 1:1 ratio. This ensures that the model benefits from the specialized training required for CogPlanner integration while preserving its broad functionality. The fine-tuned model is referred to as Qwen2- 7B-VL-Cog. This fine-tuning methodology is highly adaptable and capable of being applied to any existing MLLM. In our practice, we find that the CogBench fine-tuning process significantly enhances the MRAG planning capabilities of MLLMs, making them qualified for effective planning experts. Ultimately, this approach facilitates the development of a lightweight integration of CogPlanner, en- hancing MRAG performance while requiring minimal additional computational resources. 6 Experiments 6.1 Experimental Settings Our experimental evaluation of CogPlanner, conducted on the Cog- Bench test set, encompasses two primary dimensions: the overall performance of the MARG system and the analysis of the planning procedure within CogPlanner. We utilize the following backbone MLLMs, GPT-4o [9], Qwen2-VL-72B-Instruct [28], Pixtral-Large- Instruct [11], and our fine-tuned Qwen2-7B-VL-Cog. Notably, QVQ- 72B-Preview serves as a representative MLLM for advanced multi- modal reasoning capabilities. 6.1.1 End-to-End MRAG Performance.We