to enhance the decision-making capabilities of various MLLMs, particularly resource-efficient models, through fine-tuning. In the following subsections, we detail the construc- tion process of CogBench and demonstrate how the benchmark enables lightweight integration of CogPlanner with the Qwen2-VL- 7B-Instruct. 5.1 Query Collection The rapid evolution of MLLMs has underscored the need for evalu- ation on increasingly complex user queries that mirror real-world application scenarios. While existing benchmarks [8, 12] provide valuable groundwork, we recognized the necessity to extend beyond their scope. To this end, we deliberately incorporate more complex queries that demand image-based knowledge augmentation. We acquire authentic user intent through web-sourced screenshots. We curate a diverse array of topics and structure search queries around these topic words, such as"Astro Bot screenshot", and use Google Im- age Search to collect an image corpus. The resultant image corpus is subject to a manual filtration process. To generate realistic queries, we leveraged the Claude-3.5-sonnet API 3 to simulate real users, producing five distinct queries per image that span both factual and open-ended inquiries requiring visual context interpretation. Each query-image pair undergoes manual review and modifica- tion by two senior AI research engineers, each bringing at least three years of domain expertise. The modification process follows several key principles: (1) each query must be distinct, with no repetition—even across different images; (2) queries must be unam- biguous; (3) queries should be meaningful and formulated naturally, resembling how real humans would ask them; and (4) each query should target specific information, asking about concrete aspects of the image. For each image, the 1–5 most compelling queries, which highlight the potential of multimodal retrieval, are retained. 5.2 MRAG Planning and Generation The MRAG planning and generation process is central to the Cog- Planner framework, as detailed in Section 4.2. We employ the GPT- 4o API