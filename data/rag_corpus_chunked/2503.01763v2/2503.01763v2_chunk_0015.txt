23.64 22.39 20.62 5.19 26.75 17.67 19.90 19.50bge-base-en-v1.522.50 6.02 29.96 17.30 17.78 2.92 23.66 22.2725.99 5.71 32.17 24.26 22.09 21.27bge-large-en-v1.524.45 6.66 32.9319.3018.90 3.12 25.7624.4725.72 5.54 32.1824.7923.0222.85 gte-Qwen2-1.5B-inst.♠ 29.17 7.93 38.05 21.49 21.66 3.41 28.89 27.67 36.04 7.89 44.51 35.55 28.96 26.04e5-mistral-7b♠ 26.76 7.25 34.39 21.05 20.01 3.44 28.31 27.10 31.41 6.68 38.47 29.24 26.06 25.80GritLM-7B♠ 25.74 6.85 34.27 21.28 22.02 3.72 30.41 28.8742.31 8.71 49.3438.1730.02 29.44NV-Embed-v1♠ 31.30 8.35 39.1523.0529.64 4.72 40.4538.8840.54 8.25 45.93 34.4433.8332.12 mxbai-rerank-large-v122.99 5.61 30.32 18.38 24.76 3.88 34.86 33.22 26.76 5.91 34.53 26.03 24.84 25.88monot5-base-msmarco28.92 7.70 36.44 19.97 21.61 3.62 30.06 27.88 36.22 7.54 45.11 36.41 28.92 28.09bge-reranker-v2-m332.92 8.73 41.88 25.63 24.28 3.80 32.71 30.94 30.51 7.00 36.03 26.74 29.24 27.77jina-reranker-v2-base35.38 9.25 44.65 26.98 26.47 4.15 35.20 33.94 38.94 8.14 46.06 35.42 33.60 32.11bge-reranker-v2-gemma36.72 9.69 45.9427.8529.89 4.42 38.2336.8239.93 9.06 49.4337.7535.5134.14 Mixtral-8x22B 28.21 8.31 34.1325.4227.41 3.14 34.13 36.98 30.76 5.40 34.12 28.65 28.80 30.35gpt-3.5-turbo-012530.29 8.01 36.0024.22 28.69 4.27 36.25 35.64 29.80 6.39 35.01 28.70 29.60 29.52gpt-3.5-turbo-110631.01 7.86 35.82 23.7628.95 4.44 38.1638.4532.30 6.89 38.3130.8430.7531.01 Table 4: Experiment results in w/o inst. setting (§ 5), where the model takes the query as input to retrieve. We mark the baselines pre-trained on instructional datasets with ♠. We highlight the best performance in each type of model. utilize transformer encoders trained on various IR datasets. We evaluate all-MiniLM-L6-v2, gte (Li et al., 2023c), bge (Xiao et al., 2023a) and e5 (Wang et al., 2022), covering a wide range of sizes. Cross-encoder re-rankers . These models re- rank the initially retrieved documents based on the query-passage relevance. We evaluate: MonoT5 (Nogueira et al., 2020), mxbai-rerank- large, jina-reranker-v2-base, and bge-reranker. LLM agents. These methods leverage general- purpose LLM agents for re-ranking tasks in a zero- shot setting, simulating the tool selection process of tool-use agents. We evaluate the widely used LLM re-ranking framework, i.e.,