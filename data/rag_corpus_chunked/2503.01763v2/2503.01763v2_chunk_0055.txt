of negative tools per input query 5 # Number of target tools (labels) per input query 2.31 Table 7: Basic statistics of the collected large-scaling training set TOOL RET-train. We use the tokenizer from gpt-3.5-turbo in this work. D More experiment details D.1 Baselines We comprehensively evaluate the following mainstream retrieval models on our benchmark, including: • Sparse Retrieval. These methods measure the similarity between tasks and tool documentation based on lexical overlap. We evaluate BM25s (Lù, 2024). • Single-task dense retrieval. These methods employ dual-encoder architecture models trained on conventional IR datasets. We evaluate gtr (Ni et al., 2021a), contriever (Izacard et al., 2021a), and colbertv2.0 (Santhanam et al., 2021a), all trained on MS-MARCO (Nguyen et al., 2016) with relevance criteria. We also evaluate the COLT (Qu et al., 2024a) which is a recently proposed model trained on an ad-hoc tool retrieval dataset. Example of our seed instructions (handcrafted instruction # Query: I would like to generate a video presenting a text-based discussion on the topic of ’The Benefits of Exercise’. # Instruction: Given a "text-to-video" task, retrieve tools that process text inputs to generate coherent textual outputs aligned with the query’s topic and requirements. # Query: I have an audio file ’example.wav’ which is difficult to understand. I would like you to help me transcribe the audio to text. # Instruction: Given a "audio transcription" task, retrieve tools that process audio inputs to produce accurate textual transcriptions aligned with the query’s requirements. # Query: Conduct a two-sample independent t-test with two samples, sample1=[1, 2, 3, 4, 5] and sample2=[6, 7, 8, 9, 10], and a significance level of 0.05. # Instruction: Given a "significance test" task, retrieve tools that perform statistical tests, specifically a two-sample independent t-test, by processing numerical inputs and returning the t-statistic, p-value. # Query: