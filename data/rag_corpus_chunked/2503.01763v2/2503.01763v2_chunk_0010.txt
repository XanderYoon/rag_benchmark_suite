de- tailed pseudo algorithm is provided in Appendix B. After the above three processes, we obtain TOOL RET, which consists of 7.6k tasks, each paired with an instruction, and a corpus of 43k diverse tools, providing a comprehensive testbed and supporting various evaluation settings. 4 Benchmark statistic Table 1 provides the basic statics ofTOOL RET. We observe that there are three mainstream formats of tool documentation: (i) Code, which is a function- level snippet in programming language; (ii) Web API, which elaborates the tool usage in structured JSON format following the Web OpenAPI specifi- cation; (iii) Customized application, which directly describes the tool functionality in free-form nature language. Based on these formats, we categorize 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Rouge-L score between input query and target tool documentation 0.0 2.5 5.0 7.5 10.0 12.5density Frequency distribution for Rouge-L score Normal distribution Kernel density estimation Figure 2: ROUGE-L overlap between the query (input) and the target tools (label). Figure 3: Length distribution of our benchmark. TOOL RET into three subsets accordingly and di- vide the TOOL RET into Code Function, Web API, and Customized App subsets. Below, we report a more detailed analysis of TOOL RET. 4.1 Complexity In tool learning, previous studies have highlighted the necessity of combining multiple tools for task solving (Shi et al., 2024). Thus, we ana- lyze the complexity of our retrieval benchmark from two aspects. First, we calculate the av- erage number of target tools for each retrieval task and compare it with well-known IR bench- marks such as HotpotQA (Yang et al., 2018) and MTEB (Muenni ghoff et al., 2022). As shown in Table 2, TOOL RET requires models to recall more targets, posing a challenge in comprehensive re- trieval. Second, we compute the lexical overlap, i.e., ROUGE-L, between the input