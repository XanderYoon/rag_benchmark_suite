al., 2023) https://huggingface.co/intfloat/e5-mistral-7b-instruct GritLM-7B (Muennighoff et al., 2024) https://huggingface.co/GritLM/GritLM-7B NV-Embed-v1 (Lee et al., 2024) https://huggingface.co/nvidia/NV-Embed-v1 Cross-encoder re-ranking mxbai-rerank-large-v1 https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1 monot5-base (Nogueira et al., 2020) https://huggingface.co/castorini/monot5-base-med-msmarco bge-reranker-v2-m3 (Li et al., 2023a; Chen et al., 2024a)https://huggingface.co/BAAI/bge-reranker-v2-m3 jina-reranker-v2 https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual bge-reranker-v2-gemma (Li et al., 2023a; Chen et al., 2024a)https://huggingface.co/BAAI/bge-reranker-v2-gemma LLM agent RankGPT https://github.com/sunnweiwei/RankGPT -Mixtral-8x22B https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1 -GPT-3.5-turbo-1106 https://openai.com/chatgpt/overview/ -GPT-3.5-turbo-0125 https://openai.com/chatgpt/overview/ Table 9: The public link or endpoint of the baselines in our experiments. • Multi-task Embedding Models. These methods utilize transformer encoders trained on various annotated IR datasets. We evaluate gte (Li et al., 2023c), bge (Xiao et al., 2023a), and e5 (Wang et al., 2022), covering a wide range of parameter sizes. Additionally, we evaluate all-MiniLM-L6-v26 from the Sentence Transformers platform. • Cross-encoder Re-rankers. These models re-rank the initially retrieved documents based on the query-passage relevance using bidirectional or unidirectional transformers. We evaluate MonoT5- Base and three re-rankers trained on diverse tasks: (i) mxbai-rerank-large-v17, (ii) jina-reranker-v2- base8, and (iii) BGE-reranker. • LLM Agents. These methods leverage general-purpose LLM agents for re-ranking tasks in a zero- shot setting, simulating the tool selection process of tool-use agents. We evaluate the widely used LLM re-ranking framework, i.e., RankGPT (Sun et al., 2023), with various LLMs as backbone. We highlight that the initial tools for LLM agent and Re-ranking baselines are retrieved by NV-embedd-v1 model . Details about these baselines are provided in Table 9. D.2 Compare with conventional IR tasks To further investigate the complexity of tool retrieval tasks, we conducted a comparative analysis of model performance between our proposed benchmark (TOOL RET) and the conventional Information Retrieval (IR) task benchmark, specifically the Massive Text Embedding Benchmark (MTEB). The relationship between these two benchmarks is visually presented in Figure 7. Our analysis reveals two significant findings. First , we observe a strong positive correlation between the two