tools per task. While recent information retrieval (IR) techniques such as semantic matching (Qu et al., 2024a; Xu et al., 2024), can assist with tool retrieval, they are of- ten trained on ad-hoc tool-use datasets, lacking comprehensive evaluation on diverse scenarios, es- pecially for unseen tasks. To further explore the importance of tool retrieval, we conduct a pilot experiment on ToolBench (Qin et al., 2023). As shown in Figure 1, we observe that (i) the agentâ€™s performance substantially drops when replacing the officially annotated toolset with the retrieved tools; and (ii) even strong retrievers like colbertv2 (San- thanam et al., 2021a), struggle to retrieve target tools effectively. These findings highlight the ne- cessity to (i) systematically evaluate IR models on diverse tool retrieval tasks; and (ii) analyze the impact of retrieval on the end-to-end task pass rate. In this work, we introduce TOOL RET, the first arXiv:2503.01763v2 [cs.CL] 26 May 2025 large-scale tool retrieval benchmark comprising 7.6k diverse retrieval tasks and a corpus of 43k tools, which comprehensively evaluates IR mod- els across diverse retrieval scenarios. Specifically, we collect query-tool datasets from the following sources: (i) Tool-use agent benchmarks from pub- lished research papers in AI conferences, such as ACL and NeurIPS; (ii) Related conference re- sources such as AppBench in EMNLP and Tool- Lens in CIKM; and (iii) Other publicly available datasets from the open-source community, e.g., HuggingFace. The collected data is carefully cu- rated to cover a wide range of practical tool require- ments, comprising diverse types of tool documen- tation, domains, and varying query lengths. Then, we standardize the format of all the collected tasks, aligning them with retrieval tasks similar to the for- mat in MTEB, where each retrieval task contains a query and target tools (e.g., labels). To support the instructional retrieval (Weller et al.,