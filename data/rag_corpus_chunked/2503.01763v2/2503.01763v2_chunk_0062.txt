gtr-t5-large all-MiniLM-L6-v2 e5-base-v2 e5-large-v2 gte-base-en-v1.5 bge-base-en-v1.5 bge-large-en-v1.5 e5-mistral-7b-inst. gte-large-en-v1.5 gte-base-en-v1.5 NV-Embed-v1 Figure 7: Correlation between the score on our benchmark and MTEB (retrieval subset). conventional IR benchmark. This discrepancy suggests that while our benchmark shares fundamental characteristics with conventional IR tasks, it presents additional challenges that make it more demanding for existing models. Second , our experimental results demonstrate that state-of-the-art IR models, particularly those trained with relevance-oriented optimization criteria (e.g., Contriever), exhibit substantially degraded performance on TOOL RET. This performance gap underscores the necessity for target-aware reasoning capabilities in our benchmark, which goes beyond traditional relevance matching. The unique challenges of TOOL RET are further elaborated in ยง 4.1, where we identify two key distinguishing factors: (1) the presence of Model TOOLRET-Web T OOLRET-Code T OOLRET-Customized Avg. N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 C@10 Conventional sparse and dense models BM25S 51.79 13.78 63.35 45.4638.74 5.87 52.65 51.39 59.7213.5571.8360.38 50.08 52.41 ColBERT 51.70 13.56 61.92 41.01 38.60 6.0555.0754.05 53.91 12.10 66.85 55.29 48.07 50.11 contriever-msmarco53.23 14.55 65.96 46.59 35.97 5.79 52.32 50.8456.9413.21 72.11 61.17 48.71 52.87 gtr-t5-base 51.65 14.13 64.55 44.56 33.98 5.51 48.88 47.77 54.28 13.24 70.95 60.82 46.64 51.05 gtr-t5-large 56.6215.0669.7750.26 37.40 5.85 52.41 51.27 56.24 13.31 71.2561.40 50.09 54.31 Embedding models all-MiniLM-L6-v2 48.49 13.54 62.35 43.63 34.40 5.62 50.15 48.71 58.08 13.22 72.72 62.17 46.99 51.50 e5-small-v2 54.40 14.60 66.47 46.76 35.18 5.67 50.70 49.19 56.85 13.45 73.43 60.86 48.81 52.27 e5-base-v2 55.42 14.92 67.90 48.10 38.35 6.23 56.08 54.90 59.96 14.18 76.18 66.55 51.24 56.52 e5-large-v2 54.32 14.81 67.69 47.89 40.24 6.23 56.33 54.85 59.40 13.79 72.96 60.37 51.32 54.37 gte-base-en-v1.5 56.4815.4070.0750.96 39.46 6.27 56.5855.24 64.0014.2378.0366.93 53.31 57.71 gte-large-en-v1.555.39 14.89 68.33 48.98 38.23 6.22 56.16 54.95 57.88 13.86 75.12 65.15 50.50 56.36 bge-base-en-v1.5 56.17 14.86 68.30 49.05 38.71 6.05