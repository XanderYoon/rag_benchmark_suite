and scientific re- trieval (Ajith et al., 2024), which substantially aug- ments the downstream models. However, tool re- trieval, a crucial step for tool-use agents, remains underexplored. Compared with traditional IR tasks, retrieving useful tools is more challenging since solving a task typically requires the combination of multiple tools (Qu et al., 2024b). Most exist- ing benchmarks simplify this retrieval process by manually annotating a small set of tools that fit the LLMsâ€™ context, which is far from reality with a large toolset. In this work, we evaluate IR models on diverse tool retrieval tasks and contribute over 200k training data to facilitate future research. 3 Benchmark construction 3.1 Data collection To build a comprehensive benchmark for tool re- trieval evaluation, we collect data from the follow- ing well-known sources: (i) Tool-use LLM bench- marks: A wide range of benchmarks published in leading AI conferences such as ACL and NeurIPS; (ii) Conference Resources: Datasets from resource tracks in IR and NLP conferences (e.g., CIKM and EMNLP); and (iii) Other high-quality dataset: We identify related datasets released on open-source platforms like HuggingFace and their technique re- ports can be found in public submissions like arXiv. We include them to enrich TOOL RET. Given the rapid development of benchmarks from these sources, we collect datasets released between the August 2023 to December 2024 in this version.3 We download these data from official channels based on their usage requirements and totally collect more than 30 datasets. Since the data sources are diverse and their original formats vary substantially, we perform necessary data cleaning operations such as deduplication and text normal- ization to ensure consistency and quality. We observe that most of the collected datasets are originally designed to evaluate the tool-use ca- pability of LLMs, where the LLM is required to cor- rectly