9ToolBench consists of three subsets: ToolBench-G1, ToolBench-G2, and ToolBench-G3. Model TOOLRET-Web T OOLRET-Code T OOLRET-Customized Average N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 C@10 Sparse and dense models bm25 26.47 4.04 34.51 33.22 20.61 5.06 26.35 15.87 38.48 8.51 48.24 37.09 28.52 28.73 COLT 22.23 3.75 31.66 30.15 21.65 5.36 29.15 19.12 36.12 7.99 47.63 37.64 26.67 28.97 Colbert 23.58 3.84 32.64 31.19 23.50 5.70 28.82 16.03 33.71 6.64 40.85 32.01 26.93 26.41 contriever-msmarco19.11 3.16 26.52 24.97 21.84 5.93 27.83 15.19 35.01 7.57 44.07 34.71 25.32 24.96 gtr-t5-base 21.19 3.48 29.05 27.90 18.18 4.45 25.09 16.32 35.95 8.10 45.77 36.50 25.11 26.91 gtr-t5-large 24.48 3.91 33.55 32.44 23.49 5.64 30.79 19.31 38.88 8.89 49.55 38.64 28.95 30.13 Embedding models all-MiniLM-L6-v2 18.07 3.10 25.13 23.75 13.71 3.49 18.73 11.78 31.61 7.12 40.66 30.03 21.13 21.85 e5-small-v2 20.10 3.14 26.47 25.12 21.02 5.33 27.70 16.88 32.58 7.35 39.85 30.33 24.57 24.11 e5-base-v2 20.96 3.54 29.43 28.23 21.25 5.43 27.59 16.42 32.90 7.33 43.10 33.51 25.04 26.05 e5-large-v2 22.93 3.49 29.53 28.09 20.16 5.19 27.26 16.68 39.45 8.81 49.07 38.19 27.51 27.65 gte-base-en-v1.5 24.50 3.98 33.85 32.48 24.32 6.46 33.01 19.91 37.86 8.10 49.44 38.94 28.89 30.44 gte-large-en-v1.5♠ 23.00 3.90 33.07 31.93 23.84 6.19 31.38 19.15 35.34 8.24 45.43 35.28 27.39 28.79 bge-base-en-v1.5 23.44 3.86 32.79 31.47 24.17 6.37 31.80 18.68 36.59 8.47 47.44 36.87 28.07 29.01 bge-large-en-v1.523.12 3.76 31.93 30.67 27.14 7.24 35.77 21.16 35.51 7.82 45.71 36.39 28.59 29.40 gte-Qwen2-1.5B-inst.♠ 28.99 4.57 39.58 38.16 31.85 8.37 39.95 22.69 45.47 10.02 55.68 43.85 35.44 34.90 e5-mistral-7b 25.38 4.13 34.81 33.47 28.22 7.60 35.99 22.28 42.31 9.03 51.34 40.24 31.97 32.00 GritLM-7B♠ 29.67 4.82 41.30 39.77 30.86 8.18 39.99 25.25 48.92 10.31 59.01 46.55 36.48 37.19 NV-Embed-v1♠ 35.50 5.54 48.36 46.72 33.08 8.77 41.08 24.83 51.2811.1361.49 48.05