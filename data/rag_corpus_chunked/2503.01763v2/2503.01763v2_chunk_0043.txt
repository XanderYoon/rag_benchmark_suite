code functions, customized apps are described using free-form natural language documentation Algorithm 1: The pseudo algorithm for our target-aware strategy in automatically constructing instructions for evaluation tasks. Input: A set of N seed instructions S = {si | i ∈ [N ]} manually crafted by human experts; A powerful LLM M (e.g., GPT-4o); Collected tasks T = {ti|i ∈ [|T |]} Initialize an instruction pool I ← S; for i ∈ |T | do Sample k examples {s′ 1, s′ 2, ..., s′ k} from I; // Generate a new instruction si using M through in-context learning: si ← M(prompt with {s′ 1, s′ 2, ..., s′ k}); //Append new instruction to pool: I = I ∪ { si} ; Apply heuristic filtering to remove low-quality instructions from I; Output: A set of high-quality instructions I = {s1, s2, ..., s|T |} rather than structured formats. Specifically, we include the following datasets: ToolACE (Liu et al., 2024a), GPT4Tools (Yang et al., 2024),TaskBench (Shen et al., 2023), ToolAlpaca, ToolBench-sam (Xu et al., 2023), ToolEmu (Ruan et al., 2023), and TooLink (Qian et al., 2023). B.5 Task format The final benchmark, TOOL RET, integrates the above datasets and reformats all test cases into a unified format, similar to conventional IR benchmarks such as BEIR and MTEB, to evaluate IR models in tool retrieval tasks. Each reformatted task consists of: an input query, an instruction, and the corresponding target tools (e.g., labels). Each tool is assigned a unique identifier and is paired with detailed documentation describing its functionality. Below, we present a concrete example from TOOL RET. # An example of an evaluation task in our proposed benchmark - Query : I need to find a grocery store near 123 Main Street , Downtown District that has a good selection of limes for