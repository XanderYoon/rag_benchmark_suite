instruction fully and accurately describe the features of all target tools mentioned in the query ? ( Are there any important features of the target tools that are missing or inadequately described in the instruction ?) 3. Accuracy of Tool Feature Description : Does the instruction correctly describe the features of the target tools ? ( Key question to ask : Are the descriptions of the target tools technically accurate and consistent with their actual functionality ?) 4. Relevance to Input Query : Is the instruction directly relevant to the original input query ? ( Key question to ask : Does the instruction address the specific needs or context provided in the input query , or does it deviate from the query ' s intent ?) ## Detailed annotation Process For each instruction , evaluate it based on the four aspects above . 1. If the instruction meets all criteria ( no hallucination , comprehensive , accurate , and relevant ) , mark it as correct . 2. If the instruction fails to meet any of the criteria , mark it as incorrect and provide a brief explanation of the issue ( e . g . , " contains hallucination ," " missing key tool features ," or " irrelevant to query ") . For incorrect instructions , `` revise `` them to ensure they meet all quality criteria . The goal of this annotation process is to ensure that all instructions in our benchmark are of high quality and faithfully grounded in the original queries and target tools . C Large-scaling training dataset: T OOL RET-train We extend the data collection process from TOOL RET to incorporate the training sets of three mainstream tool-use datasets: ToolACE (Liu et al., 2024a), ToolBench (Qin et al., 2023), and APIGen (Liu et