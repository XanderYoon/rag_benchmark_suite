LLMs Statistic # size of retrieval task 7,615 - # ofweb APIretrieval task 4,916 - # ofcode functionretrieval task 950 - # ofcustomized appretrieval task 1,749 # size of tool 43,215 - # ofweb API 36,978 - # ofcode function 3,794 - # ofcustomized app 2,443 avg. query / instruction length (tokens) 46.87 / 43.43 avg. tool documentation length (token) 174.56 Table 1: Basic statistics of our benchmark TOOL RET. Ours NQ MSMARCO HotpotQA MTEB # Average number of tar- gets for an input query.2.171.00 1.00 2.00 2.57 # ROUGE-L overlap be- tween query and targets.0.060.31 0.34 0.11 0.27 Table 2: Comparison with conventional IR benchmarks. to automate this process. Specifically, we first in- vite three human experts with strong NLP and IR backgrounds to manually craft 100 seed in- structions. In line with the well-defined format from Asai et al., our instruction outlines the rel- evance criteria by bridging the query intent and the functionality of the target tools. For example, for the transcribing the audio to text task, the in- struction is presented as “retrieve tools that process audio inputs to produce accurate textual transcrip- tions aligned with the user requirements ”. Next, we employ a powerful LLM, i.e., GPT-4o, as an automatic instruction generator and guide it to gen- erate instruction for each task through in-context learning. To enhance the diversity, we randomly sample in-context examples from the pool of both the generated and handcrafted instructions. A de- tailed pseudo algorithm is provided in Appendix B. After the above three processes, we obtain TOOL RET, which consists of 7.6k tasks, each paired with an instruction, and a corpus of 43k diverse tools, providing a comprehensive testbed and supporting various evaluation settings. 4 Benchmark statistic Table 1 provides the basic statics ofTOOL RET. We observe that there are