log-likelihood of the target tools. The loss function L is formulated as: − 1 |T | X ti∈T log esim(I⊕q,ti) esim(I⊕q,ti) + P ˆtj ∈ bT esim(I⊕q,ˆtj ) . The I ⊕ q indicates concatenation of instruction and query with a special token. During the training, we set the K to 10 and the learning rate to 5e-5. Improvement from retrieval. As shown in Fig- ure 6, all IR models trained on TOOL RET-train achieve substantial improvement in NDCG@10 metric. We further evaluate the task pass rate of two tool-use LLMs: GPT-3.5 and ToolLlama (Qin et al., 2023). When equipped with the improved IR models, both LLMs exhibit substantial gains in pass rate, confirming the critical role of retrieval in downstream tasks. As part of future work, we suggest adapting the IR models to better augment the tool-use LLMs, which offers a efficient plug- and-play solution compared with training LLMs. We further conduct an ablation study by remov- ing the instruction I from the loss function L. The results show that this variant shows improvements compared with the non-tuned counterparts, but un- derperforms compared with their instruction-tuned counterparts (See Appendix D). These validate the effectiveness of our instructional training data in enhancing tool retrieval performance. 8 Discussion In this section, we discuss the following three open questions related to the design and reliably of our benchmark construction, as well as potential exten- sion for future work. Fine-grained functional differences of tools In this work, we construct the overall tool corpus by merging tools from various existing tool-use benchmarks. This heterogeneous construction strat- egy inevitably leads to overlapping functionality among different tools, which may raise concerns about unreliable evaluations, such as the one-to- many problem5. However, the following reasons 5The one-to-many problem arises because our dataset com- bines multiple existing datasets.