2024. Bigcodebench: Benchmarking code genera- tion with diverse function calls and complex instruc- tions. arXiv preprint arXiv:2406.15877. A Data Card Following previous work (Bender and Friedman, 2018; Gebru et al., 2021; Zhuo et al., 2024), we provide the datacard for TOOL RET, where we tend to summarize and centralize all information that might be relevant for the benchmark analysis. (i) The purpose of this benchmark : This benchmark is proposed to comprehensively evaluate the information retrieval (IR) models on tool retrieval tasks. On top of TOOL RET, we find that existing IR models, despite achieving strong performance in conventional IR benchmarks such as MTEB and BEIR, still suffer from substantial challenges in tool retrieval tasks. The poor retrieval quality further degrades the end-to-end task pass rate of tool-use LLMs. Thus, we believe that the TOOL RET reveals the importance of tool retrieval in building better tool-use LLMs, and can be used as a comprehensive and fair benchmark in facilitating the development of tool retrieval models. (ii) How will the dataset be distributed (e.g., Tarball on Website or Github)?The proposed benchmark TOOL RET will be released to the public, and hosted on GitHub and Hugging Face. The TOOL RET will be managed and maintained by our research team. (iii) Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? Yes. If we include more tasks or find any errors, we will correct the dataset hosted on Hugging Face and GitHub and update the results in the leaderboard accordingly. It will be updated on our website. (iv) Will the training dataset TOOL RET-train will be released publicly. Yes, the proposed training dataset TOOL RET-train will be released to the public, and hosted on GitHub and Hugging Face. B Details of Benchmark B.1 Dataset collections TOOL RET is