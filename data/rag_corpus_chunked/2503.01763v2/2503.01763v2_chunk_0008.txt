the corresponding toolset and randomly sample one task from each cluster. If the toolset size exceeds the number of queries, we retain all queries. For example, the orig- inal ToolEyes (Ye et al., 2024a) dataset contains 500 queries and 95 tools; Thus, we set the cluster number as min(500, 95) = 95 for clustering. Toolset sampling. To eliminate redundancy, we manually review the documentation of each raw dataset to identify and merge identical toolsets. For example, since the COLT (Qu et al., 2024a) toolset overlaps with the Toolbench (Qin et al., 2023) , we merge their intersecting tools. Ultimately, we merge all toolsets from the 34 datasets to form the final corpus, resulting in a total of 43k tools. Each tool is assigned a unique identifier. After sampling, we obtain 7.6k retrieval tasks and a corpus of 43k tools. 3.3 Instruction construction Instructional information retrieval (Sun et al., 2024; Weller et al., 2024) is an active research area, where an additional instruction is paired with the input query to guide IR models in retrieving target in- formation. This instruction-following capability is especially critical in tool retrieval, as IR models are often used to augment LLM agents and receive additional context from the agents beyond the input query. To support this instructional IR scenario, we construct the instructions as part of TOOL RET. Considering manually writing instructions is cost-intensive and challenging to scale, we intro- duce a target-aware strategy using powerful LLMs Statistic # size of retrieval task 7,615 - # ofweb APIretrieval task 4,916 - # ofcode functionretrieval task 950 - # ofcustomized appretrieval task 1,749 # size of tool 43,215 - # ofweb API 36,978 - # ofcode function 3,794 - # ofcustomized app 2,443 avg. query / instruction length (tokens) 46.87 / 43.43 avg. tool documentation length (token)