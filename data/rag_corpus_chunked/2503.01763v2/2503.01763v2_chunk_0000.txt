Retrieval Models Arenâ€™t Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models Zhengliang Shi1 Yuhan Wang1 Lingyong Yan2 Pengjie Ren1 Shuaiqiang Wang2 Dawei Yin2 Zhaochun Ren3* 1Shandong University, Qingdao, China 2Baidu Inc., Beijing, China 3Leiden University, Leiden, The Netherlands /gtbTool-Retrieval-Benchmark shizhl@mail.sdu.edu.cn z.ren@liacs.leidenuniv.nl Abstract Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) mod- els to select useful tools from large toolsets is a critical initial step. However, the perfor- mance of IR models in tool retrieval tasks re- mains underexplored and unclear. Most tool- use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world sce- narios. In this paper, we propose TOOL RET, a heterogeneous tool retrieval benchmark com- prising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on TOOL - RET. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on TOOL RET. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we con- tribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.1 1 Introduction Large language models (LLMs) have demonstrated remarkable progress across various natural lan- guage processing (NLP) tasks, such as text sum- marization (Chang et al., 2023). However, they suffer from inherent inabilities to interact with the physical world and access vast, up-to-date knowl- edge (Qin et al., 2024). To alleviate these draw- backs, tool learning is proposed to equip LLMs with external tools, augmenting them as agents to manipulate tools for practical task-solving