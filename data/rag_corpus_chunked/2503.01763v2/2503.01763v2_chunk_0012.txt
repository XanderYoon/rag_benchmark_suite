user behavior, as users tend to input brief queries with minimal effort. Additionally, most tool documentation is under 200 tokens, which is similar to the chunk length in standard IR document retrieval corpus, such as Wikipedia dump (Karpukhin et al., 2020). 4.3 Quality So far, we have demonstrated the complexity and quantity of our benchmark while the quality of the LLM-generated instructions remains uncertain. To investigate this, we ask 5 human experts to label the quality based on four aspects listed in Table 3. Our evaluation reveals that 89.2% of the generated instructions correctly cover the feature of the target tools and are faithfully grounded on the original queries. For the remaining 10.8% instructions that mismatch the query or the target tools, we ask ex- perts to revise them. This re-check mechanism ensures the high quality of instructions in TOOL - RET, making it a reliable evaluation benchmark. To explain more intuitively, we list a number of seed instructions, high-quality and low-quality instruc- tions in Table 8. Annotation guidance is also pro- vided in Appendix B to promote our transparency. 4.4 Instruction diversity We further analyze how the generated instructions differ from the seed instructions used to prompt the generation. For each generated instruction, we compute its highest ROUGE-L overlap with the 100 seed instructions. We plot the distribution of these ROUGE-L scores in Figure 4. The results 4We use the tokenizer from gpt-3.5-turbo in this work. 0.1 0.3 0.5 0.7 Rouge-L score between generated instruction and seed instruction 0 2 4 6density Frequency distribution for Rouge-L Normal distribution Kernel density estimation Figure 4: ROUGE-L overlap between the handcrafted seed instructions and model-generated instructions. indicate a decent number of new instructions are generated, which have low overlap with the seeds. 5 Benchmark evaluation setup 5.1 Evaluation protocol We use three