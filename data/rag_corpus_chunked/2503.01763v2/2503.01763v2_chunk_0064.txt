characteristics collectively contribute to a more complex retrieval environment that requires advanced reasoning and understanding capabilities from retrieval models. D.3 Results of controlled experiment Since TOOL RET integrates multiple datasets, we also conduct controlled experiments where IR models retrieve tools exclusively within the toolset of each individual dataset instead of the overall tool corpus. Table 11 presents the results under the setting that the IR models only take the query to retrieve, i.e., the w/o inst setting. Table 10 presents the results under the setting that the IR models take the query and additional instruction to retrieve, i.e., the w/ inst setting. D.4 Results of in-subset retrieval TOOL RET contains three subsets, including TOOL RET-web, TOOL RET-code and TOOL RET-customized. The tool in each subset diverges by its documentation format, domain, and functionality. For a compre- hensive evaluation, we also conduct an in-subset retrieval experiment, where IR models retrieve tools exclusively within the toolset of each subset instead of the overall tool corpus. Table 13 presents the results under the setting that the IR models only take the query to retrieve, i.e., the w/o inst setting. Table 12 presents the results under the setting that the IR models take the query and additional instruction to retrieve, i.e., the w/ inst setting. D.5 Results of trained IR mdels Experimental results on TOOL RET reveal that even IR models with strong performance on conventional IR benchmarks such as MTEB and BEIR struggle significantly in tool retrieval tasks. A key factor contributing to this performance degradation is the lack of a large-scale training dataset specifically tailored for tool retrieval. To address this gap, we introduce TOOL RET-train, a diverse training dataset comprising more than 200k tool retrieval tasks. Each example in TOOL RET-train consists of an input query, an instruction generated using our