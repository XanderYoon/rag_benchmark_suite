generic search API without language constraints. In line with prior work (Gao et al., 2024; Qin et al., 2023; Qu et al., 2025a), our benchmark en- courages models to retrieve semantically and func- tionally appropriate tools, not merely those with similar surface forms. Reason for our format-based categorization Unlike existing tool retrieval benchmarks (Qu et al., 2024a), we introduce a novel task categorization by dividing the overall TOOL RET dataset into three subsets: Web APIs, Code Functions , and Customized Applications. This division is moti- vated by two reasons: (i) It naturally reflects the structural features of tools collected from over 30 diverse datasets. This categorization aligns with conventions established in prior research on tool- augmented LLMs, where these three formats are commonly used as the primary categories for ex- ecutable tools. (ii) This format-based taxonomy provides a clear, interpretable, and effective frame- work for analyzing model behavior across different tool representations. Beyond format-based category In addition to the format-based categorization presented in this work, we believe that other dimensions of cate- gorization can also offer valuable insights into re- trieval behavior. Therefore, we propose extending the category structure in future work to include three additional dimensions for more comprehen- sive and customized evaluations: (i) Query Length (Input Complexity): Tasks are grouped based on used as the label, potentially leading to unreliable evaluations. the query token length, with categories such as 0–25, 25–50, 50–75, and 75+ tokens. (ii) Number of Target Tools (Multi-Label Complexity): Queries are grouped by the number of relevant ground-truth tools, categorized as 1, 2–3, or ≥ 4 tools. (iii) Lex- ical Overlap (ROUGE-L) Between Query and Tool Documentation: Tasks are grouped by the de- gree of lexical overlap, categorized as low (<0.05), medium (0.05–0.2), and high ( ≥ 0.2). A lower overlap indicates a higher need