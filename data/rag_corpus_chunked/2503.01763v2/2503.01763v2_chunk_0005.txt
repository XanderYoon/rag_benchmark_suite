al., 2024). The former prepends the description of candidate tools in the LLMsâ€™ context, prompting them to se- lect and invoke tools (Huang et al., 2023). The latter enables LLMs to learn the usage of each tool through training on synthetic data (Liu et al., 2024a; Gao et al., 2024). However, both two paradigms struggle when facing the large-scale toolset in prac- tice (Qu et al., 2024b; Liu et al., 2024b). First, real-world toolsets are typically massive, making it less possible to incorporate all tools within the limited context of LLMs. For example, the Rapi- dAPI platform contains more than 52k tools while the PyPI2 hosts over 600k frequently updated pack- ages. Second, since tools are frequently updated, it is cost-intensive to re-train the LLMs to memo- rize all tools (Qu et al., 2025a). Although recent studies address this challenge using semantic re- trievers (Qin et al., 2023; Wang et al., 2024c), these solutions are typically ad-hoc and lack systematic evaluation across diverse tool retrieval scenarios. To fill this gap, we present thefirst comprehensive tool retrieval benchmark with systematic analysis. Information retrieval benchmark. Conventional information retrieval (IR) benchmarks are typically designed for information-seeking tasks, such as Nature Question (Kwiat kowski et al., 2019) for question answering and MS-MARCO (Nguyen 2https://pypi.org/ et al., 2016) for passage re-ranking. Recent work also explores the IR technique in various down- stream tasks, such as table retrieval (Chen et al., 2024b; Zhang and Balog, 2020) and scientific re- trieval (Ajith et al., 2024), which substantially aug- ments the downstream models. However, tool re- trieval, a crucial step for tool-use agents, remains underexplored. Compared with traditional IR tasks, retrieving useful tools is more challenging since solving a task typically requires the combination of multiple tools (Qu et al., 2024b). Most exist- ing benchmarks simplify this retrieval