e5-large-v2 61.72 15.90 73.27 52.84 56.21 8.42 75.2573.14 69.8815.0181.13 71.30 62.60 65.76 gte-base-en-v1.5 64.3516.5575.80 57.3859.18 8.77 76.95 74.45 71.79 14.53 81.90 70.07 65.11 67.30 gte-large-en-v1.560.67 15.46 72.30 52.41 54.11 8.22 73.35 71.37 68.59 14.36 80.41 69.82 61.12 64.53 bge-base-en-v1.5 65.05 16.37 75.72 57.30 54.55 7.72 69.22 67.48 71.21 14.71 83.13 72.53 63.60 65.77 bge-large-en-v1.566.2516.4875.84 57.75 58.61 8.41 74.91 72.74 71.1914.20 80.44 69.2765.35 66.59 gte-Qwen2-1.5B-inst.67.57 16.93 78.14 60.81 58.12 8.51 75.41 73.39 71.73 15.34 83.03 73.39 65.81 69.19 e5-mistral-7b 69.5117.3779.34 62.48 58.15 8.37 75.12 72.79 72.52 14.68 81.79 71.49 66.73 68.92 GritLM-7B 69.43 17.25 78.97 61.67 62.78 9.22 78.74 77.5976.0415.44 85.55 74.3569.42 71.21 NV-Embed-v1 66.04 16.88 77.19 59.0663.46 9.40 81.79 79.82 75.3915.7588.48 78.37 68.30 72.42 Cross-encoder re-ranking models mxbai-rerank-large-v157.48 14.60 68.65 49.54 50.37 7.75 69.59 67.88 62.24 13.32 73.26 61.24 56.70 59.55 monot5-base-msmarco54.57 14.23 64.38 46.12 50.00 8.05 68.76 66.80 64.50 13.28 75.80 67.84 56.36 60.25 bge-reranker-v2-m370.42 17.75 80.33 65.49 64.22 9.37 80.60 79.48 75.70 16.15 88.87 78.65 70.11 74.54 bge-reranker-v2-gemma75.6718.6384.07 71.00 69.59 9.78 84.18 83.47 77.1716.5588.34 79.80 74.14 78.09 Table 10: Results of control experiment where each IR models is evaluated from the toolset of each integrated dataset in w/ inst. setting. 16 18 20 22 24 26 28 30 32 Evaluation score on our benchmark 40 45 50 55 60Evaluation score on MTEB benchmarkbm25 e5-small-v2 contriever-msmarco gtr-t5-base gtr-t5-large all-MiniLM-L6-v2 e5-base-v2 e5-large-v2 gte-base-en-v1.5 bge-base-en-v1.5 bge-large-en-v1.5 e5-mistral-7b-inst.gte-large-en-v1.5 gte-base-en-v1.5 NV-Embed-v1 Pearson coefficient = 0.790 Spearman coefficient = 0.441 bm25 e5-small-v2 contriever-msmarco gtr-t5-base gtr-t5-large all-MiniLM-L6-v2 e5-base-v2 e5-large-v2 gte-base-en-v1.5 bge-base-en-v1.5 bge-large-en-v1.5 e5-mistral-7b-inst. gte-large-en-v1.5 gte-base-en-v1.5 NV-Embed-v1 Figure 7: Correlation between the score on our benchmark and MTEB (retrieval subset). conventional IR benchmark. This discrepancy suggests that while our benchmark shares fundamental characteristics with conventional IR tasks, it presents additional challenges that make it more demanding for existing models. Second , our experimental results demonstrate that