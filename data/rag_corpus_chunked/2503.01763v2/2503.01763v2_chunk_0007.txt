than 30 datasets. Since the data sources are diverse and their original formats vary substantially, we perform necessary data cleaning operations such as deduplication and text normal- ization to ensure consistency and quality. We observe that most of the collected datasets are originally designed to evaluate the tool-use ca- pability of LLMs, where the LLM is required to cor- rectly call a sequence of target tools given an input query. To facilitate retrieval evaluation in TOOL - RET, we align the format of all collected tasks with the well-established IR benchmark like BEIR and MTEB. Specifically, each task consists of a query as 3Our team will maintain and update the benchmark. input and target tools as label (a.k.a, ground truth), where a tool is identified by a unique identifier and paired with detailed documentation to describe its functionality. Endpoints of the collected datasets and concrete examples of our formatted dataset are provided in Appendix B. 3.2 Data sampling After collecting the datasets, we observe data size imbalances across different datasets. Besides, some datasets are extremely large with substantial redun- dant content, making comprehensive model evalu- ation both inefficient and unnecessary. Therefore, we streamline them through effective data sampling while maintaining its evaluation integrity. Task sampling. For each collected dataset, we encode the tasks using the embedding model, i.e., NV-embedd-v1, and apply the K-means clustering algorithm on the text embeddings. We set the num- ber of clusters to the size of the corresponding toolset and randomly sample one task from each cluster. If the toolset size exceeds the number of queries, we retain all queries. For example, the orig- inal ToolEyes (Ye et al., 2024a) dataset contains 500 queries and 95 tools; Thus, we set the cluster number as min(500, 95) = 95 for clustering. Toolset sampling. To eliminate redundancy, we