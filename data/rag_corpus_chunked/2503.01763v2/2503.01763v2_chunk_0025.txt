Target Tools (Multi-Label Complexity): Queries are grouped by the number of relevant ground-truth tools, categorized as 1, 2–3, or ≥ 4 tools. (iii) Lex- ical Overlap (ROUGE-L) Between Query and Tool Documentation: Tasks are grouped by the de- gree of lexical overlap, categorized as low (<0.05), medium (0.05–0.2), and high ( ≥ 0.2). A lower overlap indicates a higher need for deep semantic matching between the query and the correct tools. 9 Conclusion In this work, we introduce TOOL RET, the first di- verse tool retrieval benchmark comprising 7.6k queries, each paired with an instruction, and a corpus of 43k tools. TOOL RET is a heteroge- neous benchmark, constructed by aggregating ex- isting tool-use datasets and aligning them into a unified format, similar to conventional IR bench- marks such as MTEB. We evaluate state-of-the-art IR models on TOOL RET and uncover a surpris- ing finding: even models with strong performance on conventional IR benchmarks struggle in tool retrieval. This low retrieval quality significantly degrades the end-to-end task pass rate of tool-use LLMs. Inspired by this, we further propose TOOL - RET-train, a large-scale training set containing over 200k retrieval tasks. Results show that IR models trained on TOOL RET-train exhibit substantial im- provement and also enhance the pass rate of tool- use LLMs by 10%-20%. In the future, we plan to extend the TOOL RET into multimodal scenarios. Acknowledgements This work was supported by the Natural Science Foundation of China (Grant No. 62472261), the Shandong Province Key Research and Develop- ment Program (2024CXGC010108), and the Shan- dong Province Technology Innovation Guidance Program (YDZX2024088). Limitation The limitations of this work include the lack of exploration in multilingual retrieval settings. Cur- rently, our benchmark is confined to the English language and focuses exclusively on text retrieval. To address this limitation, we plan