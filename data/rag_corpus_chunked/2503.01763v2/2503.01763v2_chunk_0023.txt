tools In this work, we construct the overall tool corpus by merging tools from various existing tool-use benchmarks. This heterogeneous construction strat- egy inevitably leads to overlapping functionality among different tools, which may raise concerns about unreliable evaluations, such as the one-to- many problem5. However, the following reasons 5The one-to-many problem arises because our dataset com- bines multiple existing datasets. For example, for a query from dataset A, the ground truth may not be limited to the single annotation provided in A. Similar tools in dataset B might also provide valid solutions to the same query. However, in the evaluation process, only the ground truth from dataset A is support the reliability of our approach: (i) In real- world scenarios, many different but functionally similar tools exist, but typically, only the most suit- able one is chosen. In this work, we focus on such real-world scenarios, where the ground truth from the original dataset is considered the most appropriate. We argue that models should have the fine-grained discrimination ability to identify the most appropriate tool; (ii) Even if tools appear to have overlapping high-level functionality (e.g., Bing Search vs. Google Search), they often differ in important dimensions such as input parameters (e.g., support for language-specific filtering) and specific application scope (e.g., medical search vs. general news search). For example, for a query like search news articles in Chinese , a tool like bing_search_with_lang_param is a more precise match than a generic search API without language constraints. In line with prior work (Gao et al., 2024; Qin et al., 2023; Qu et al., 2025a), our benchmark en- courages models to retrieve semantically and func- tionally appropriate tools, not merely those with similar surface forms. Reason for our format-based categorization Unlike existing tool retrieval benchmarks (Qu et al., 2024a), we introduce a