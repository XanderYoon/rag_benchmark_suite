of target tools for each retrieval task and compare it with well-known IR bench- marks such as HotpotQA (Yang et al., 2018) and MTEB (Muenni ghoff et al., 2022). As shown in Table 2, TOOL RET requires models to recall more targets, posing a challenge in comprehensive re- trieval. Second, we compute the lexical overlap, i.e., ROUGE-L, between the input query and cor- responding retrieval targets (tool documentation in TOOL RET and passage in IR benchmarks). We find that this overlap is substantially lower inTOOL RET. It indicates that, for neural IR models, TOOL RET requires more heavily on the semantic representa- tion rather than simple lexical matching. Therefore, the retrieval task in TOOL RET is more challenging. Quality Review Question Yes or No % Whether the instruction is relevant to the orig- inal input query? 90.1% / 9.9% Whether the instruction describes the feature of target tools 92.3% / 8.7% Whether the instruction comprehensively de- scribe the feature of all target tools 89.2% / 10.8% Whether the instruction contains hallucination about the target tools or input query?5.9% / 94.1% Table 3: The quality review for our generated instruc- tions, which is conducted by five human experts with 0.743 Kappa statistics. 4.2 Length statistics Figure 3 illustrates the length distribution of the query, instruction, and tool documentation in TOOL RET.4 We find that most queries are concise, typically containing fewer than 60 tokens (about 25 words), which aligns with real-world user behavior, as users tend to input brief queries with minimal effort. Additionally, most tool documentation is under 200 tokens, which is similar to the chunk length in standard IR document retrieval corpus, such as Wikipedia dump (Karpukhin et al., 2020). 4.3 Quality So far, we have demonstrated the complexity and quantity of our benchmark while the quality of the