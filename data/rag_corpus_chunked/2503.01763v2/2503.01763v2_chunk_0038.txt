Hugging Face and GitHub and update the results in the leaderboard accordingly. It will be updated on our website. (iv) Will the training dataset TOOL RET-train will be released publicly. Yes, the proposed training dataset TOOL RET-train will be released to the public, and hosted on GitHub and Hugging Face. B Details of Benchmark B.1 Dataset collections TOOL RET is a heterogeneous benchmark that integrates a wide range of well-established tool-use datasets and aligns them into a unified format, similar to standard information retrieval (IR) benchmarks such as BEIR and MTEB, to facilitate tool retrieval evaluation. In tool learning, we observe that previous work primarily focuses on three mainstream types of tools: (i) Web APIs: These tools are encapsulated in the OpenAPI format (standard JSON documentation) and can be directly invoked via HTTP requests. Web APIs are typically used to access, manipulate (e.g., add, delete, edit, or query), or retrieve private data or information from specialized databases, covering a wide range of domains such as movies, music, and sports. (ii) Code Functions: These tools are represented by source code containing function signatures and im- plementation details. Code functions primarily focus on low-level computations or atomic operations, such as tensor calculations, calling Hugging Face models, or utilizing PyTorch libraries. (iii) Customized Apps: These tools are paired with free-form natural language descriptions. They are typically user-oriented or personalized, enabling tasks such as sending emails or other custom applications. These tool types differ in functionality and documentation format, reflecting diverse scenarios for tool-use LLMs. For IR models, retrieving different types of tools may present varying levels of difficulty. Therefore, we categorize the collected datasets into these three types based on their paired toolset formats, resulting in three subsets of TOOL RET: TOOL RET-web, TOOL RET-code, and TOOL RET-customized. During evaluation, we report the