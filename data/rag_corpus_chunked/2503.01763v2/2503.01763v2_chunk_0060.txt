retrieval tasks, we conducted a comparative analysis of model performance between our proposed benchmark (TOOL RET) and the conventional Information Retrieval (IR) task benchmark, specifically the Massive Text Embedding Benchmark (MTEB). The relationship between these two benchmarks is visually presented in Figure 7. Our analysis reveals two significant findings. First , we observe a strong positive correlation between the two benchmarks, as evidenced by a Pearson’s correlation coefficient of β = 0 .790, indicating a similar performance trend across models. However, we observe that the absolute performance scores in TOOL RET are consistently lower than those in the 6huggingface.co/all-MiniLM-L6-v2 7huggingface.co/mxbai-rerank-large-v1 8huggingface.co/jina-reranker-v2-base-multilingual Model REST API Code Function Customized tool Avg. N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 P@10 R@10 C@10 N@10 C@10 Conventional sparse and dense models BM25s 62.0915.6872.98 58.06 56.98 8.24 73.95 72.81 68.4814.7880.51 69.55 62.51 66.81 ColBERT 56.73 14.59 67.86 47.87 53.56 7.82 71.66 70.09 64.49 13.05 76.35 64.85 58.26 60.94 contriever-msmarco57.81 15.33 70.77 50.87 49.66 7.29 65.99 64.5668.8614.67 83.05 73.45 58.77 62.96 gtr-t5-base 57.06 14.54 68.26 49.75 49.38 7.29 65.76 64.09 70.48 14.29 81.60 70.96 58.97 61.60 gtr-t5-large 60.32 15.27 72.05 54.03 52.79 7.41 67.28 65.79 72.03 14.69 84.3973.95 61.72 64.59 Embedding models all-MiniLM-L6-v2 53.92 14.47 66.77 48.64 50.14 7.58 68.39 66.74 68.31 14.20 81.57 71.14 57.46 62.17 e5-small-v2 61.95 15.84 72.91 53.34 51.45 7.76 68.16 65.46 69.13 14.24 79.69 68.33 60.85 62.38 e5-base-v2 62.90 15.90 73.98 53.83 55.81 8.44 74.17 72.53 69.96 14.9883.63 73.94 62.8966.77 e5-large-v2 61.72 15.90 73.27 52.84 56.21 8.42 75.2573.14 69.8815.0181.13 71.30 62.60 65.76 gte-base-en-v1.5 64.3516.5575.80 57.3859.18 8.77 76.95 74.45 71.79 14.53 81.90 70.07 65.11 67.30 gte-large-en-v1.560.67 15.46 72.30 52.41 54.11 8.22 73.35 71.37 68.59 14.36 80.41 69.82 61.12 64.53 bge-base-en-v1.5 65.05 16.37 75.72 57.30 54.55 7.72 69.22 67.48 71.21 14.71 83.13 72.53 63.60 65.77 bge-large-en-v1.566.2516.4875.84 57.75 58.61 8.41 74.91 72.74 71.1914.20