range of practical tool require- ments, comprising diverse types of tool documen- tation, domains, and varying query lengths. Then, we standardize the format of all the collected tasks, aligning them with retrieval tasks similar to the for- mat in MTEB, where each retrieval task contains a query and target tools (e.g., labels). To support the instructional retrieval (Weller et al., 2024) setting of our benchmark, we also introduce a target-aware strategy to supplement each query with an instruc- tion using the powerful LLMs (i.e., gpt-4o). We systematically evaluate five types of IR mod- els such as embedding models and LLM re-ranking, under various experimental settings. Our results re- veal that even the best model (i.e., NV-embedd-v1) that demonstrates strong performance in conven- tional IR benchmarks, achieves an nDCG@10 of only 33.83 in our benchmark. This highlights the challenges of the tool retrieval tasks. We identify two key factors contributing to this performance gap: (i) Lower term overlap between queries and target tools in tool retrieval tasks, which demands higher representation abilities for IR models to ac- curately match query intent with the correct tools; and (ii) Task shift from conventional information- seeking tasks (e.g., document retrieval) to tool re- trieval, leading to suboptimal performance of IR models that are not explicitly optimized. To enhance the retrieval performance and enable IR models to augment tool-use agents, we further propose the TOOL RET-train, a large-scale train- ing dataset containing more than 200k retrieval tasks. We extend our data collection process from TOOL RET to include the training set of three main- stream tool-use datasets, including ToolACE (Liu et al., 2024a), APIGen (Liu et al., 2024b) and Tool- Bench (Qin et al., 2023). To enable the training, we pair each retrieval task with 10 negative tools re- trieved by the NV-embed-v1. Finally, each