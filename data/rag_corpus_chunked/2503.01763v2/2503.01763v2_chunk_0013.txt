0.7 Rouge-L score between generated instruction and seed instruction 0 2 4 6density Frequency distribution for Rouge-L Normal distribution Kernel density estimation Figure 4: ROUGE-L overlap between the handcrafted seed instructions and model-generated instructions. indicate a decent number of new instructions are generated, which have low overlap with the seeds. 5 Benchmark evaluation setup 5.1 Evaluation protocol We use three widely used IR metrics to evaluate the retrieval performance: (i) NDCG@K (N@K): evaluates ranking quality based on the relevance of retrieved tools; (ii) Recall@K (R@K): evaluates the proportion of target tools successfully retrieved within the top-K results; and (iii) Precision@K (P@K): evaluates the accuracy of the retrieved tools within the top-K results. We also use Complete- ness@K (C@K) from COLT (Qu et al., 2024b), which specifically evaluates the retrieval complete- ness in tool retrieval tasks. The C@K is 1 if all target tools are included in the top-k retrieved tools; otherwise, it is 0. We mainly evaluate IR models under two set- tings: (i) w/o inst.: The model take the query alone as input; and (ii) w/ inst.: The model takes the con- catenation of the query and instruction as input to retrieve. This allows us to analyze the impact of instructions on retrieval performance. 5.2 Model to Evaluate We comprehensively evaluate the following main- stream IR models on our benchmark. Sparse retrieval. These methods measure the simi- larity between query and tool documentation based on lexical overlap. We evaluate BM25s (LÃ¹, 2024). Single-task dense retrieval . These methods use dual-encoder models trained on conven- tional IR datasets. We evaluate gtr (Ni et al., 2021a), contriever (Izacard et al., 2021a), and col- bertv2.0 (Santhanam et al., 2021a), all trained on MS-MARCO (Nguyen et al., 2016). We also eval- uate COLT (Qu et al., 2024a), a recently proposed model trained on