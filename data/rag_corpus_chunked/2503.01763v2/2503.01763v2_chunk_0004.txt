retrieval tasks. We extend our data collection process from TOOL RET to include the training set of three main- stream tool-use datasets, including ToolACE (Liu et al., 2024a), APIGen (Liu et al., 2024b) and Tool- Bench (Qin et al., 2023). To enable the training, we pair each retrieval task with 10 negative tools re- trieved by the NV-embed-v1. Finally, each training example contains the query, an generated instruc- tion, the target tools, and the negative tools. Results show that the IR models trained over TOOL RET- train, exhibit significant improvements in the re- trieval process, leading to a higher end-to-end task pass rate when integrated with tool-use LLMs. Our contributions are summarized as follows: (i) We introduce TOOL RET, the first evaluation bench- mark for tool retrieval tasks. (ii) We evaluate the tool retrieval performance of various IR models and analyze the impact of retrieval on the end-to- end task pass rate of tool-use LLMs; and (iii) We contribute to a large-scale training dataset that en- hances the performance of IR models, improving their ability to augment tool-use LLMs effectively. 2 Related work Tool learning with foundation models.Tool learn- ing aims to equip LLMs with tools, such as web API (Song et al., 2023) and python packages (Wang et al., 2024d), expanding their utility (Qin et al., 2023). Existing work teaching LLMs to use tools can be broadly classified into tuning-free (Lu et al., 2023) and tuning-based methods (Gao et al., 2024). The former prepends the description of candidate tools in the LLMsâ€™ context, prompting them to se- lect and invoke tools (Huang et al., 2023). The latter enables LLMs to learn the usage of each tool through training on synthetic data (Liu et al., 2024a; Gao et al., 2024). However, both two paradigms struggle when facing the large-scale toolset