in-domain datasets, they are much more challenging to implement in Bayes-optimal GR models. Specifically, as we will discuss, achieving this concentrated structure requires additional computational and data resources beyond those needed to obtain the Bayes-optimal model itself. Detailed discussions are shown in the following. 5.1 Aggregation introduces redundancy Although the corpus size |D | = ğ‘˜ğ‘š is sufficiently large, the model does not necessarily fully utilize the entire code space. Here we study the entropy of the corpus distribution Pr(Â·), which is de- fined as H(ğ’…) = âˆ’Ã ğ‘‘ âˆˆ D Pr(ğ‘‘) log Pr(ğ‘‘). It can be decomposed into the entropy of the marginal distribution at each step, i.e., H(ğ’…) = Ãğ‘š ğ‘–=1 H(ğ’…ğ‘– | ğ’…<ğ‘– ). When the distribution is uniform, the en- tropy is maximized, i.e., H(ğ’…) = log |D | = ğ‘š log ğ‘˜. If we introduce external prior knowledge to aggregate the relevant documents, the relevant branches will stand out at a very early stage of generation, and the entropy at that layer H(ğ’…ğ‘– | ğ’…<ğ‘– ) will be low and even ap- proaching zero. The more aggregated the relevant documents are, the lower the entropy becomes. In other words, for the same corpus size, the model will waste more code space on redundant structures. In practice, this approach for concentration is often realized by conducting hierarchical clustering on the corpus, which has been shown to be effective in small scale retrieval tasks. The trade-off between concentration and redundancy can be ignored to some extent when the corpus is small. However, if we would like to build a Bayes-optimal GR model, learning a sufficiently large code space is already expensive, see Appendix A. Effective concentration will introduce more redundancy, which is computationally inefficient. 5.2 Amplification requires high-quality data We treat the amplification strategy as an approximation