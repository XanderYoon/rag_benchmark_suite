within a narrow branch, and (ii) amplification, i.e., amplifying the scores of relevant documents to be significantly large. Both directions are essentially meant to provide a high concentration prior on the relevance distribution. We show that although they are empirically beneficial for small scale corpus, difficulty will arise if we aim to construct a Bayes-optimal GR. Our results provide theoretical grounding of constrained beam search in GR, and shows the inherent limitations of this retrieval strategy towards a reliable Bayes-optimal GR model. Our results also imply the importance of balancing concentration during model training for mitigating this problem. Contributions. Our main contributions are as follows: (i) We pro- vide theoretical results concerning Bayes-optimal GR to determine how constrained beam search affects the generalization. (ii) We decompose the negative effect from two angles, constraints and beam search, and identify a trade-off factor, i.e., the concentration of relevance distribution. (iii) Our theoretical results are verified by experiments on synthetic and real-world datasets. 2 Related work Generative retrieval (GR) is an emerging direction in neural in- formation retrieval, exploring the possibility of replacing traditional index structures in retrieval systems with a single large-scale neural networks [27, 54]. It leverages generative models to directly gen- erate the relevant docIDs given a query. This paradigm originated with Cao et al . [9], Metzler et al . [34] and has garnered consid- erable attention [4, 30, 38, 46â€“49, 52, 55, 57, 61, 66, 67, 70] in the information retrieval community. Generalization remains a challenge for GR, especially when ap- plied to out-of-distribution corpora [2, 11, 22, 27, 31, 32, 46]. Pre- vious research attributes this challenges to limited model capac- ity [22, 60], lack of learning in the docID construction [46, 57, 66], and difficulties in learning semantic representations [ 47, 53]. In contrast, our work