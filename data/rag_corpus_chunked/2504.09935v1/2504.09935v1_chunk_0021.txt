case of a uniform relevance distribution, as discussed in Section 4.1. Since the lower bound is expressed in terms of the size of the downstream corpus, we vary its size to observe the behavior across different vocabulary sizes. We choose a sufficiently largeğ‘š so that the complete corpus of sizeğ‘˜ğ‘š is much larger than the downstream corpus. For a downstream corpus of sizeğ‘›, the sampling probability is given by ğ‘ = ğ‘› ğ‘˜ğ‘š . The simulation results are shown in Figure 1. As illustrated, the KL divergence for different ğ‘˜ values exhibits a consistent decreasing trend. For a fixed downstream corpus size, a smaller ğ‘˜ results in significantly lower error. This is expected, as a smaller ğ‘˜ leads to each branch covering a larger number of selected documents, thereby reducing variance. In this case, the model-predicted marginal distribution closely follows the actual one. General relevance distribution. Next, we simulate the case of general relevance distributions, as described in Section 4.1. For sim- plicity, we ensure that each branch has the same Simpson diversity index. We assign random weights to documents within the range [1, ğ‘’100], where the assignment probability decreases exponentially with larger weights. The concentration of the distribution is con- trolled by varying the rate of decrease: slower rates result in less concentrated distributions, corresponding to smaller Simpson di- versity indices. When the Simpson diversity index approaches 1, our lower bound no longer holds. The simulation results are shown in Figure 2. Compared to the uniform distribution case, the KL divergence is significantly larger and decreases more slowly as the downstream corpus size increases. For data with a Simpson diversity index approaching 1, the KL divergence reaches approximately 6. In contrast, the uniform case consistently maintains a very low KL divergence. For example, the lowest Simpson diversity index in