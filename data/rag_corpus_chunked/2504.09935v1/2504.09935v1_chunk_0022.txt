in Figure 2. Compared to the uniform distribution case, the KL divergence is significantly larger and decreases more slowly as the downstream corpus size increases. For data with a Simpson diversity index approaching 1, the KL divergence reaches approximately 6. In contrast, the uniform case consistently maintains a very low KL divergence. For example, the lowest Simpson diversity index in the figure is around1ğ‘’ âˆ’06, which matches the magnitude of the uniform distribution, 1 220 â‰ˆ 1ğ‘’ âˆ’ 06, and the corresponding KL divergence is approximately 0.19, as seen in Figure 1. 212 214 216 218 220 222 224 Size of Downstream Corpus 0 1 2 3 4 5 6 7KL Divergence 0.19 A = 1.00 A = 0.04 A = 3.27e â–¡ 04 A = 1.91e â–¡ 05 A = 1.43e â–¡ 06 Lower Bound Figure 2: The KL divergence error in the first generation step on several synthetic relevance distribution data with differ- ent degrees of concentration. The vocabulary size is 210, the docID length is 3. ğ´ is the Simpson diversity index of the rele- vance distribution. As for highly concentrated distributions, e.g., ğ´ = 1, and ğ´ = 0.04, the Lyapunovâ€™s condition will no longer hold (see Theorem B.3 for more details), we do not draw their lower bounds. 0.05 0.1 0.2 0.8 1.5 Î» 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Recall@#Relevant Branches Precision@1 is 1.0 for all kk = 2 8 k = 2 10 k = 2 12 k = 2 14 0.5 + max(0, 0.65 â–¡ 0.15 Î» ) Figure 3: The recall of relevant branches cut off at the total number of relevant branches in the first generation step. The synthetic relevance distribution is constructed as Section 4.2. The total number of sampled relevant document is ğœ†ğ‘˜. The size of each