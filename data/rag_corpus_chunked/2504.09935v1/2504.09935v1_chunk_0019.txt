concentration and redundancy can be ignored to some extent when the corpus is small. However, if we would like to build a Bayes-optimal GR model, learning a sufficiently large code space is already expensive, see Appendix A. Effective concentration will introduce more redundancy, which is computationally inefficient. 5.2 Amplification requires high-quality data We treat the amplification strategy as an approximation of the max-heap structure discussed in Li et al. [24]. In this structure, the value of each node is the maximum value of its children instead of the sum, i.e., ğ‘‰ (ğ‘‘ğ‘– | ğ‘‘<ğ‘– ) = maxğ‘‘>ğ‘– Pr(ğ‘‘ğ‘–, ğ‘‘>ğ‘– | ğ‘‘<ğ‘– ). One can prove that this structure can achieve perfect recall performance by preserving the relevant documents in the top-ğ‘› branches [71]. Note that this structure is no longer a chain decomposition of the joint distribution, and the distribution at each step is different from the original distribution. The new distribution can still be learned through empirical risk minimization by carefully filtering the train- ing data. Considering the ğ‘–-th step, the negative log-likelihood loss is as follows: Lğ‘– (ğœƒ ) = âˆ’ âˆ‘ï¸ ğ‘‘ âˆˆ ËœD log Pr(ğ‘‘ğ‘– | ğ‘‘<ğ‘– ; ğœƒ ), (13) where ËœD is the training set, and ğœƒ is the model parameter to be optimized. In order to learn the max-heap structure, the predicted distribution Pr(ğ‘‘ğ‘– | ğ‘‘<ğ‘– ; ğœƒ ) should be proportional to ğ‘‰ (ğ‘‘ğ‘– | ğ‘‘<ğ‘– ). Therefore the loss function should filter out the non-maximum successors in the training set, i.e., Lğ‘– (ğœƒ ) = âˆ’ âˆ‘ï¸ ğ‘‘ âˆˆ ËœD I[ğ‘‘ âˆˆ arg max Pr(ğ‘‘>ğ‘– | ğ‘‘ â‰¤ğ‘– )] log Pr(ğ‘‘ğ‘– | ğ‘‘<ğ‘– ; ğœƒ ). (14) As we can see, only the most relevant successors are allowed to contribute to the loss function. It not only needs to throw away