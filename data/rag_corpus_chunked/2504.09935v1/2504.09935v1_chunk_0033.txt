set ğ›¿ = ğ‘œ (1) and ğ›¿ 2ğ‘˜ = ğ‘˜ğ‘‚ (1), and have ğ‘‘TV (ğ‘ƒ, ğ‘„) â‰¥ | ğ‘ƒ (ğ´ğ‘›) âˆ’ ğ‘„ (ğ´ğ‘›)| (25) â‰¥ ğ‘›ğ‘Œğ‘› ğ‘ âˆ’ ğ‘Œğ‘› ğ‘Œ1 (26) â‰¥ (1 âˆ’ ğ›¿)ğ‘›E[ğ‘Œğ‘›] (1 + ğ›¿)ğ‘˜ğ‘šğ‘ âˆ’ (1 + ğ›¿)E[ğ‘Œğ‘›] (1 âˆ’ ğ›¿)E[ğ‘Œ1] (27) â‰ƒ 1 âˆ’ Î¦(1)âˆšğ‘šğ‘ . (28) where ğ‘‘TV (ğ‘ƒ, ğ‘„) = supğ´âŠ† [ğ‘˜ ] |ğ‘ƒ (ğ´) âˆ’ ğ‘„ (ğ´)| is the total variation distance of two distributions. Lastly, we use Pinskerâ€™s inequality [15] to give the asymptotic lower bound in Eq. 17, KL [ğ‘ƒ âˆ¥ ğ‘„] â‰¥ 2ğ‘‘TV (ğ‘ƒ, ğ‘„)2 â‰³ 0.05 ğ‘šğ‘ . â–¡ Theorem B.2 (Lower bound of KL divergence for uniform relevance distribution.). Let ğ‘Ÿ = ğ‘š âˆ’ 1 and the sampling proba- bility ğ‘ = 1 ğ‘˜ğ‘Ÿ âˆ’ğ‘  . Pr(Â·) is a uniform distribution. We have, for the KL divergence in Eq. 7, KL [Pr(Â·|ğ¶) âˆ¥ Pr(Â·|ğ¶ğ‘– )] â‰³ 0.05 ğ‘˜ğ‘  . (29) Here ğ‘  is small and hence the right hand side converges slowly with respect to ğ‘˜. When ğ‘  = 0, we have a constant lower bound 0.05. Proof. We consider each possible tokenğ‘‘1 âˆˆ [ ğ‘˜] at first position. Since the selection of each document follows Bernoulli(ğ‘), the number of documents selected with first token being ğ‘‘1 follows Binomial(ğ‘˜ğ‘Ÿ , ğ‘). As the GR model can only consider the constraint in the current step, it will return a uniform distribution over the valid ğ‘‘1 tokens. By revoking Lemma B.1, we have the lower bound. â–¡ Theorem B.3 (Lower bound of KL divergence for general relevance distribution). Let ğ‘†ğ‘– ğ‘— be a weighted Bernoulli random variable, with parameter ğ‘ and weight ğ‘¤ğ‘– ğ‘—, where ğ‘– âˆˆ [ ğ‘˜] and ğ‘— âˆˆ [ğ‘›]. Suppose for some ğ›¿ > 0, {ğ‘†ğ‘– ğ‘— | ğ‘— âˆˆ [ ğ‘›]} satisfy Lyapunovâ€™s condition