models using binary cross-entropy loss. They showed that pseudo- labeling during training does not guarantee that beam search will get the most relevant targets. In our work, we analyze the marginal distribution of an auto-regressive distribution and provide a the- oretical result on the top- 1 and top-ğ‘˜ performance under sparse relevance situations. Zhuo et al. [71] also provide a Bayes-optimal tree structure, which is often called max-heap assumption [ 24], and we will discuss the difficulty of enforcing this assumption in our setting in Section 5. In GR, some work have reached the same conclusion that beam search is not sufficient for retrieval as it is likely to prune the relevant docIDs and the model is not able to recover from this [26, 28, 62]. They propose to use a hybrid retrieval strategy to help bypassing this problem. We instead focus on under- standing the root cause of this problem, i.e., the usage of marginal distribution. 3 Preliminaries We formulate GR and introduce key notations in this section. Generative retrieval. Following Tay et al. [49], we formulate GR where the mapping from documents anddocIDs is one-to-one func- tion. A corpus, denoted as D, is a set of documents ğ‘‘, with each document represented as a sequence of tokens, i.e.,ğ‘‘ = (ğ‘‘1, . . . , ğ‘‘ğ‘š), where ğ‘š is the length. In this paper, we assume all documents have the same length. A generative retrieval model ğ‘“ , typically imple- mented using a sequence-to-sequence architecture such as T5 [43] or BART [23], generates a ranked list of the most relevant docIDs in D for the given query. The ranking list is computed through beam search during generation. To ensure the model reliably gen- erates valid docIDs from the corpus, a constrained auto-regressive decoding process ğ‘” together with the beam