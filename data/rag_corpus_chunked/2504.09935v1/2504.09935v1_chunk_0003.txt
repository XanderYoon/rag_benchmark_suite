51, 56, 56, 59]. Similar to the scaling principle in large lan- guage models (LLMs) [7], many dense retrieval approaches consider training on a high-resource dataset and then evaluated on different domains [18, 39, 44, 50, 51]. In particular, GTR [39] demonstrates significant improvements in out-of-domain performance by success- fully scaling up the model size and the training corpus. Unlike dense retrieval, GR, as a retrieval paradigm itself, is much more concerned with the generalization abilities to unseen corpora [2, 12, 26, 46]. Bayes-optimal generative retrieval. Given the success of gen- erative LLMs, GR is expected to capture the universal relevance distribution when trained on sufficiently large retrieval datasets. While extensive studies have already proposed effective training strategies at large scale [ 12, 26, 61, 62], the performance of an ideal, viz. a Bayes-optimal GR model, on unseen corpora has not been systematically studied. In this paper, we study an amortized Bayes-optimal auto-regressive GR model that fully encapsulates the underlying relevance distribution over the complete corpus containing all possible documents. On top of this, we apply the constrained auto-regressive generation process to provide a valid docID in any given downstream corpus which is a subset of the complete one. Constrained beam search and generalization. The core com- ponents of GR are the differentiable index and the generation of docID [54]. In this paper, we focus on how the generation process affects the generalization of GR. Most existing GR models adopt constrained beam search to generate the top-ùëò docIDs as the default retrieval strategy [46, 47, 49, 52]. However, Zeng et al. [62] point out the pitfalls of this strategy, i.e., a greedy local search algorithm is likely to prune false negative docIDs and may thus not be suffi- cient for developing effective GR models. While these effects are typically entangled