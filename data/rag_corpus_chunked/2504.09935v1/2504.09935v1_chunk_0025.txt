models on the full MS MARCO passage corpus [3]. The codebook size is set to 256. Datasets. We evaluate our approach using the MS MARCO V1 passage corpus [3], which contains 8.8 million passages, along with three evaluation datasets: (i) MS MARCO-dev, consisting of 7k queries; (ii) TREC DL 2019 [14], with 43 queries; and (iii) TREC DL 2020 [13], with 54 queries. Relevance distribution. While recent advance improves GR per- formance [61], there remains a gap compared to state-of-the-art models. We use SLIM++ [25] to compute relevance scores due to its strong overall performance on the MS MARCO V1 passage re-ranking leaderboard.2 We use the top-10, 000 scores and extrap- olate the remaining scores by taking 1% of the lowest score from the ranked list, adding small Gaussian perturbations. 7.2 Effect of constraints We examine two sampling strategies: uniform sampling, as dis- cussed in previous sections, and sampling directly based on the 2MS MARCO V1 Passage Regressions are available at Pyserini. 1 10 50 75 90 100 Ratio of selected docs (%) 0.0 0.1 0.2 0.3 KL Divergence (a) Uniform sampling 1 10 50 75 90 100 Ratio of selected docs (%) 0.0 0.1 0.2 0.3 0.4 dl20 dl19 dev (b) Non-uniform sampling Figure 5: The KL divergence error in the first generation step on MS MARCO V1 passage corpus. A subset of top- 10, 000 rank list from SLIM++ [ 25] is treated as the complete corpus, and the downstream corpus is (a) uniformly sampled, or (b) sampled with the relevance distribution. 1 11 33 56 79 Ratio of selected docs (%) 0.00 0.01 0.02 0.03 0.04 KL Divergence (a) Uniform sampling 1 11 33 56 79 Ratio of selected docs (%) 0.0 0.2 0.4 0.6 0.8 dl20 dl19 dev (b) Non-uniform sampling Figure 6: The KL divergence