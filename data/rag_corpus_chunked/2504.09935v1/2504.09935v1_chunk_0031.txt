the vocabulary size of the GR model is ğ‘˜, the maximum aver- age information in a single token is log2 ğ‘˜ bits. Each docID has ğ‘š Constrained Auto-Regressive Decoding Constrains Generative Retrieval SIGIR â€™25, July 13â€“18, 2025, Padua, Italy tokens, so there will beğ‘š log2 ğ‘˜ bits in total. One would expect that the information content of the docID should be larger than the one of the document, i.e., ğ‘š log2 ğ‘˜ â‰¥ ğ‘›ğ›¼, which implies ğ‘˜ â‰¥ 2 ğ‘›ğ›¼ ğ‘š . From OpenAI [42] and several publications [ 6, 29], the bpb is usually around 1. For a document of length512 bytes, and our docID length ğ‘š is 32, the vocabulary size ğ‘˜ should be about 216ğ›¼ â‰ˆ 65, 536. This is about the same size of the vocabulary in a language model. The size of the complete corpus is astronomically high, and for a regular size downstream corpus, the sampling probability is approaching zero, and will raise large KL divergence according to Section 4.1. This is similar to the case where we want to use the language model as GR model and some textual content as the docID. B Lower bound of KL divergence Lemma B.1 (Lower bound of KL divergence between Bino- mial and uniform distribution). Let ğ‘ºğ‘– âˆ¼ Binomial(ğ‘š, ğ‘), where ğ‘– âˆˆ [ ğ‘˜], and ğ‘ = Ãğ‘˜ ğ‘–=1 ğ‘†ğ‘– . We define a normalized distribution ğ‘ƒ as ğ‘ƒğ‘– B ğ‘†ğ‘– ğ‘ , ğ‘– âˆˆ [ ğ‘˜], (15) and a uniform distribution ğ‘„ on supp(ğ‘ƒ) B {ğ‘– |ğ‘†ğ‘– > 0} as ğ‘„ğ‘– B ( 1 |supp(ğ‘ƒ ) | , if ğ‘†ğ‘– > 0 0, otherwise (16) Then, we have a lower bound of the KL divergence between ğ‘ƒ and ğ‘„ for large ğ‘˜, KL [ğ‘ƒ âˆ¥ ğ‘„] = âˆ‘ï¸ ğ‘– ğ‘ƒğ‘– ln ğ‘ƒğ‘– ğ‘„ğ‘– â‰³