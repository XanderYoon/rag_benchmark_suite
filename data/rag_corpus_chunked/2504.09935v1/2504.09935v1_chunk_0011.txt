𝑑<𝑖. We use “constraints”, and “downstream corpus” to represent the result of sampling interchangeably. The constrained generation process 𝑔𝑐 𝑖 is applied at the 𝑖-th step of 𝑓 . It first zeros out the invalid tokens and then re-normalizes the remaining probabilities. 4 Theoretical analysis We investigate the inherent limitations of GR arising from con- straints and beam search individually. Our analysis disentangles these factors to isolate their individual effects. For constraints, we analyze its effect on the marginal distribution at each generation step. For beam search, we analyze how it independently degrades re- call performance on the complete corpus, disregarding constraints. These results are asymptotic with respect to the vocabulary size 𝑘. In Appendix A, we examine the required magnitude of 𝑘 for a sufficiently expressive model and show that it needs to be exponen- tially large in terms of the ratio of raw document length and docID length. 4.1 Constraints cause marginal distribution mismatch We begin by studying the effects of applying constraints to the model performance. We first identify the factor that causes the error during the generation. Then we quantitatively analyze the magnitude of this error in (i) uniform, and (ii) general relevance distribution given some query 𝑞. We consider the first generation SIGIR ’25, July 13–18, 2025, Padua, Italy Shiguang Wu et al. step without loss of generality. Other cases can be reduced to it by adjusting the document length 𝑚 or vocabulary size 𝑘. Unawareness of future constraints. Since 𝑓 𝑐 is unaware of the constraints in the future steps, there may exist biases between the distributions of complete corpus D and downstream corpus D𝑐. Specifically, after applying constrained decoding 𝑔𝑐, 𝑓 𝑐 (𝑞) returns Pr(𝒅1 | 𝑞, 𝐶1) = 𝑔𝑐 [Pr(𝒅1 | 𝑞)] (1) ∝ I[𝑑1 ∈ D 𝑐 1 ] ∑︁ ∀𝑑>1