is obtained when Pr(Â·) is uniform. Let the selected corpus size be ğ‘˜ğ‘Ÿ â‰ˆ ğ‘ |D |, we have the same error shown in Eq. 9. 0.05ğ´ ğ‘ â‰ˆ 0.05ğ‘˜ |D |ğ‘ = 0.05 ğ‘˜ğ‘Ÿ âˆ’1 . (12) Note that for concentrated distribution, the Simpson diversity index considerably exceeds ğ‘, resulting in a corresponding larger error. In conclusion, we infer a lower bound of the KL divergence be- tween the predicted and ground-truth marginal distribution. The bound is proportional to the degree of concentration of the under- lying relevance distribution. 4.2 The impact of beam search on recall Beam search often fails to retain the correct top-ğ‘˜ prefix candidates, resulting in the exclusion of relevant documents during genera- tion [62]. We attribute this limitation to the usage of conditional decomposition of the joint distribution over the corpus. To formalize this analysis, we model the decomposition as a tree where: (i) each node at layer ğ‘– represents a token ğ‘‘ğ‘– generated under a specific prefix ğ‘‘<ğ‘–, (ii) each sub-tree represents possible continuations of a prefix, and (iii) the value of node ğ‘‘ğ‘– with prefix ğ‘‘<ğ‘–, denoted ğ‘‰ (ğ‘‘ğ‘– | ğ‘‘<ğ‘– ), equals Ã ğ‘‘>ğ‘– Pr(ğ‘‘ â‰¥ğ‘– | ğ‘‘<ğ‘– ), i.e., the mar- ginal probability Pr(ğ‘‘ğ‘– | ğ‘‘<ğ‘– ). The property of this structure is that node values represent marginal probabilities aggregated over all possible future paths. However, the objective of the retrieval model requires identifying specific document with maximal joint proba- bility Pr(ğ‘‘). This creates a fundamental mismatch: nodes with high values ğ‘‰ (ğ‘‘ğ‘– | ğ‘‘<ğ‘– ) may not contain the hightest-joint-probability documents in their sub-trees. Non-relevant branches overtaking relevant ones. We setup a scenario with a sparse relevance distribution to elucidate this issue. We present how branches containing relevant documents can be overtaken by non-relevant peer branches during generation. At