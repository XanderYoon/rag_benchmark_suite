also related to the ranking performance. Several publications have showed that the cross-entropy is a bound of several commonly used metrics, e.g., Normalized Discounted Cumulative Gain (nDCG) and Mean Reciprocal Rank (MRR) [8, 58] for binary relevance score. KL divergence and cross-entropy only differ by the entropy. valid first tokens, but the ground-truth should be proportional to the number of valid documents in each valid branch. Due to the variance of the sampling, the ground-truth distribution will not be exactly uniform. General relevance distribution. Following the same idea of the uniform case, we further give the result for general relevance dis- tributions. The key is the Simpson diversity index [45], which is used for measuring the degree of concentration. We introduce the average Simpson diversity index. It is computed as the squared expectation of root sum of squared probabilities Pr(ğ‘‘ | ğ‘‘1), i.e., E2 ğ‘‘1 ï£®ï£¯ï£¯ï£¯ï£¯ï£° âˆšï¸„âˆ‘ï¸ ğ‘‘ Pr(ğ‘‘ | ğ‘‘1)2 ï£¹ï£ºï£ºï£ºï£ºï£» . (10) Recall that each document is selected with probability ğ‘, and let ğ´ be the average Simpson diversity index, we have an asymptotic lower bound of the KL divergence for large ğ‘˜ in Eq. 11. KL [Pr(Â· | ğ¶) âˆ¥ Pr(Â· | ğ¶ğ‘– )] â‰³ 0.05ğ´ ğ‘ . (11) For detailed illustration of the proof, see Theorem B.3. We also show that the lower bound reaches its minimum in the uniform relevance distribution case. Note that we have ğ´ â‰¥ ğ‘˜ | D |, where the equality is obtained when Pr(Â·) is uniform. Let the selected corpus size be ğ‘˜ğ‘Ÿ â‰ˆ ğ‘ |D |, we have the same error shown in Eq. 9. 0.05ğ´ ğ‘ â‰ˆ 0.05ğ‘˜ |D |ğ‘ = 0.05 ğ‘˜ğ‘Ÿ âˆ’1 . (12) Note that for concentrated distribution, the Simpson diversity index considerably exceeds ğ‘, resulting in a corresponding larger error. In conclusion, we infer