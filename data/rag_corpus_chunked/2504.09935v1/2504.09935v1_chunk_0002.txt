to beam search. This paper aims to improve our theoretical understanding of the generalization capa- bilities of the auto-regressive decoding retrieval paradigm, laying a foundation for its limitations and inspiring future advancements toward more robust and generalizable generative retrieval. CCS Concepts •Information systems → Retrieval models and ranking . Keywords Generative retrieval; Constrained decoding; Beam search ACM Reference Format: Shiguang Wu, Zhaochun Ren, Xin Xin, Jiyuan Yang, Mengqi Zhang, Zhumin Chen, Maarten de Rijke, and Pengjie Ren. 2025. Constrained Auto-Regressive Decoding Constrains Generative Retrieval. In Proceedings of the 48th Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’25), July 13–18, 2025, Padua, Italy. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3726302.3729934 1 Introduction The advent of generative models has catalyzed the emergence of generative retrieval (GR) as a new paradigm in information retrieval. GR provides a potential way to replace the conventional index struc- ture, such as inverted index and vector-based index, with a single large-scale neural network [34]. By integrating the retrieve-then- rank pipeline into an end-to-end framework, GR offers the promise of enhanced efficiency. Typically,GR adopts auto-regressive gen- erative models, e.g., BART [ 23] and T5 [ 43], trained to generate document identifiers (docIDs) given a query. arXiv:2504.09935v1 [cs.IR] 14 Apr 2025 SIGIR ’25, July 13–18, 2025, Padua, Italy Shiguang Wu et al. Generalization in neural information retrieval. Generaliza- tion is a key problem in neural information retrieval models [ 16, 18, 50, 51, 56, 56, 59]. Similar to the scaling principle in large lan- guage models (LLMs) [7], many dense retrieval approaches consider training on a high-resource dataset and then evaluated on different domains [18, 39, 44, 50, 51]. In particular, GTR [39] demonstrates significant improvements in out-of-domain performance by success- fully scaling up the model size and the training corpus. Unlike