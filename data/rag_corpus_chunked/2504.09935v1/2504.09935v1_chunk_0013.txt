see that the KL divergence is the gap between the average proportion of constraints locally within each branch Dğ‘ â‰¥1 and the global average constraints. This indicates that the varia- tion across branches may contribute the mismatch. Recall that we construct our constraints via an i.i.d. sampling, and we have the expectation of the constrained marginal distribution Pr(ğ‘‘1 | ğ‘, ğ¶1) as follows: Eğ¶ [Pr(ğ‘‘1 | ğ‘, ğ¶)] âˆ E[I[ğ’… âˆˆ D ğ‘ ]] Pr(ğ‘‘1 | ğ‘) âˆ Pr(ğ‘‘1 | ğ‘). (8) Therefore, the downstream corpus will follow the same distribution as the complete corpus on average. However, as we will see in the next, the gap is not concentrated at zero with high probability in terms of KL divergence. Uniform relevance distribution. We first give an example case when the relevance distribution Pr(ğ’… | ğ‘) is a uniform distribution. If the size of the downstream corpus is ğ‘˜ğ‘Ÿ â‰ª ğ‘˜ğ‘š = |D |, we derive an asymptotic lower bound of the error for large ğ‘˜ as follows KL [Pr(Â· | ğ¶) âˆ¥ Pr(Â· | ğ¶1)] â‰³ 0.05 ğ‘˜ğ‘Ÿ âˆ’1 . (9) In particular, we have a constant error 0.05 if the size is ğ‘‚ (ğ‘˜). For detailed illustration of the proof, see Theorem B.2. Here is the intuition: GR model will predict a uniform distribution over the 1The reason we adopt the KL divergence is that it is not only part of the training loss, i.e., empirical cross-entropy, but also related to the ranking performance. Several publications have showed that the cross-entropy is a bound of several commonly used metrics, e.g., Normalized Discounted Cumulative Gain (nDCG) and Mean Reciprocal Rank (MRR) [8, 58] for binary relevance score. KL divergence and cross-entropy only differ by the entropy. valid first tokens, but the ground-truth should be proportional to the number of