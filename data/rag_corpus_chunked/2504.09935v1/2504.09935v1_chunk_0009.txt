imple- mented using a sequence-to-sequence architecture such as T5 [43] or BART [23], generates a ranked list of the most relevant docIDs in D for the given query. The ranking list is computed through beam search during generation. To ensure the model reliably gen- erates valid docIDs from the corpus, a constrained auto-regressive decoding process ğ‘” together with the beam search is used. Bayes-optimal generative retrieval. We first assume the com- plete corpus D contains all possible documents of length ğ‘š. Any downstream corpus is therefore a subset of D. We denote ğ‘“ as the Bayes-optimal GR model on D which has the ability to predict the ground-truth relevance distribution over D given any query. The Bayes-optimal model ğ‘“ is considered as an ideal and oracle prototype model which helps us understand the behavior of the generation process. When ğ‘“ is applied to a downstream corpus Dğ‘ âŠ‚ D , it uses the corresponding constrained decoding process ğ‘”ğ‘ to predict relevant documents in Dğ‘. This induced GR model on Table 1: Glossary. Symbol Description ğ‘˜, ğ‘š the vocabulary size and document length D the complete corpus, i.e., [ğ‘˜]ğ‘š Dğ‘, ğ¶ downstream corpus and constraints ğ‘“ , ğ‘“ ğ‘ Bayes-optimal and induced downstream GR Pr(Â· | ğ‘) relevance distribution given query ğ‘ ğ‘‘ = (ğ‘‘1, . . . , ğ‘‘ğ‘š) document in D Dğ‘ is denoted as ğ‘“ ğ‘. Note that a similar setting has recently been proposed as zero-shot indexing [2]. Notation. Table 1 lists the main notation used in the paper. For an integer ğ‘›, we denote the set {1, . . . , ğ‘›} by [ğ‘›]. We use bold face to denote random variables. Tokens in a document ğ‘‘ are integers from [ğ‘˜], making ğ‘‘ âˆˆ [ ğ‘˜]ğ‘š, and ğ‘˜ is the vocabulary size. We set the complete