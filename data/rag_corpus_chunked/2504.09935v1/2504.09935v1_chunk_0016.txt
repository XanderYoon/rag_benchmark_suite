bility Pr(ğ‘‘). This creates a fundamental mismatch: nodes with high values ğ‘‰ (ğ‘‘ğ‘– | ğ‘‘<ğ‘– ) may not contain the hightest-joint-probability documents in their sub-trees. Non-relevant branches overtaking relevant ones. We setup a scenario with a sparse relevance distribution to elucidate this issue. We present how branches containing relevant documents can be overtaken by non-relevant peer branches during generation. At the first generation step, the model have to choose within ğ‘˜ nodes, each associated with a sub-tree of ğ‘˜ğ‘šâˆ’1 documents (leaves). We Constrained Auto-Regressive Decoding Constrains Generative Retrieval SIGIR â€™25, July 13â€“18, 2025, Padua, Italy assign a logit uniformly sampled from [âˆ’1, 1] to each document in the corpus. A subset of ğœ†ğ‘˜ â‰ª |D | documents is randomly selected as relevant, and each is assigned a logit within ğ‘‚ (log ğ‘˜ Â± log logğ‘˜). The exponential of each logit is the final relevance score of the document, i.e., the score of the corresponding root-to-leaf path. We prove that the recall of the top-ğœ†ğ‘˜ valued branches is upper- bounded by 0.5+ğ‘œ (1) with high probability. This indicates that many relevant branches are excluded from the highest-valued branches. However, the top-1 branch is highly likely to contain the most rel- evant documents. A detailed formal statement and proof of this result is provided in Theorem C.1 in Appendix C. The thickness of the tail distribution or the sharpness of relevant branches de- termines the probability of overtaking. If the scores for relevant branches are sufficiently high, this issue becomes less pronounced. In summary, GR models relying on the sum of sub-tree values for ranking branches struggle to achieve high recall performance while maintaining top-1 precision. 4.3 Concentration as a trade-off factor We find a common dependence on the concentration of relevance distribution from both components, i.e., the Simpson diversity in- dex,