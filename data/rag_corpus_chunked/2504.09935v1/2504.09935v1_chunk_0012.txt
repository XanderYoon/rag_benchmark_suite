Unawareness of future constraints. Since ğ‘“ ğ‘ is unaware of the constraints in the future steps, there may exist biases between the distributions of complete corpus D and downstream corpus Dğ‘. Specifically, after applying constrained decoding ğ‘”ğ‘, ğ‘“ ğ‘ (ğ‘) returns Pr(ğ’…1 | ğ‘, ğ¶1) = ğ‘”ğ‘ [Pr(ğ’…1 | ğ‘)] (1) âˆ I[ğ‘‘1 âˆˆ D ğ‘ 1 ] âˆ‘ï¸ âˆ€ğ‘‘>1 âˆˆ D>1 Pr(ğ‘‘1ğ‘‘>1 | ğ‘), (2) where the ğ¶1 constraint is satisfied through ğ‘”ğ‘, and I[ğ‘‘1 âˆˆ D ğ‘ 1 ] means ğ‘‘1 is a valid first token in the downstream corpus. In the contrary, the ground-truth marginal distribution only sum over the documents in Dğ‘, so we have Pr(ğ’…1 | ğ‘, ğ¶1, ğ¶>1) = ğ‘”ğ‘ [Pr(ğ’…1 | ğ‘, ğ¶>1)] (3) âˆ I[ğ‘‘1 âˆˆ D ğ‘ 1 ] âˆ‘ï¸ âˆ€ğ‘‘>1 âˆˆ Dğ‘ >1 Pr(ğ’…1ğ‘‘>1 | ğ‘), (4) where Pr(ğ’…1 | ğ‘, ğ¶1, ğ¶>1) = Pr(ğ’…1 | ğ‘, ğ¶). Here we use the red mark to highlight the differences from Eq. 2. Note that this gap would not arise if the downstream corpus were preset, with both training and inference performed on it, as the model would learn Pr(ğ’… | ğ‘, ğ¶) directly. We then analyze the Kullbackâ€“Leibler (KL) divergence as follows,1 KL [Pr(Â· | ğ¶) âˆ¥ Pr(Â· | ğ¶1)] (5) = Eğ’…1âˆ¼Pr(Â· |ğ¶ )  log Pr(ğ’…1 | ğ¶) Pr(ğ’…1 | ğ¶1)  (6) = E [log Pr(ğ¶>1 | ğ’…1)] âˆ’ log Pr(ğ¶>1 | ğ¶1). (7) In Eq. 7, we see that the KL divergence is the gap between the average proportion of constraints locally within each branch Dğ‘ â‰¥1 and the global average constraints. This indicates that the varia- tion across branches may contribute the mismatch. Recall that we construct our constraints via an i.i.d. sampling, and we have the expectation of the constrained marginal distribution Pr(ğ‘‘1 | ğ‘,