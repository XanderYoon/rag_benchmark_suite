high probability, at least 0.15(ğ‘˜ âˆ’ ğœ†ğ‘˜) non-relevant branches will exceed one of the low-score relevant branches. Thus at most 0.5ğœ†ğ‘˜ + max{0.5ğœ†ğ‘˜ âˆ’ 0.15(ğ‘˜ âˆ’ ğœ†ğ‘˜), 0} relevant branches will be in the top-ğœ†ğ‘˜. Then the recall is at most 0.5 + max{0.65 âˆ’ 0.15/ğœ†, 0}. For the top-1 precision, the maximum score of the relevant doc- uments can approach ğ›¿ + Î” w.h.p. And for a normal distribution ğ‘« with mean 0 and variance ğ›¿ 2 = 0.8ğ‘›, Pr  ğ‘« ğ›¿ â‰¥ ğœ–  â‰¤ ğ‘‚ ( 1 ğœ– ğ‘’ âˆ’ğœ– 2/2). If we let ğœ– = ğ‘’Î”, we have that the probability there is a non-relevant branch exceeding by ğ‘’Î”âˆš 0.8ğ‘› is ğ‘œ (1). Then w.h.p. the top-1 preci- sion is 1. â–¡ Remark C.2. In fact, the recall will be lower than 0.5 if ğœ† is small enough because more non-relevant branches will be much higher than the barrier. The result mainly comes from the carefully designed score of relevant documents which is linear in ğ‘›. It may not hold for extremely skewed distribution of scores, e.g., the relevant score is exponentially large, which actually corresponds to the â€œamplificationâ€ discussed in Section 5. Constrained Auto-Regressive Decoding Constrains Generative Retrieval SIGIR â€™25, July 13â€“18, 2025, Padua, Italy References [1] Kareem Ahmed, Kai-Wei Chang, and Guy Van den Broeck. 2023. Semantic Strengthening of Neuro-Symbolic Learning. In Proceedings of The 26th Interna- tional Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research, Vol. 206), Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent (Eds.). PMLR, 10252â€“10261. https://proceedings.mlr.press/v206/ahmed23a. html [2] Arian Askari, Chuan Meng, Mohammad Aliannejadi, Zhaochun Ren, Evan- gelos Kanoulas, and Suzan Verberne. 2024. Generative Retrieval with Few- shot Indexing. CoRR abs/2408.02152 (2024). doi:10.48550/ARXIV.2408.02152 arXiv:2408.02152 [3] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao,