Lemma B.1, we have the lower bound. â–¡ Theorem B.3 (Lower bound of KL divergence for general relevance distribution). Let ğ‘†ğ‘– ğ‘— be a weighted Bernoulli random variable, with parameter ğ‘ and weight ğ‘¤ğ‘– ğ‘—, where ğ‘– âˆˆ [ ğ‘˜] and ğ‘— âˆˆ [ğ‘›]. Suppose for some ğ›¿ > 0, {ğ‘†ğ‘– ğ‘— | ğ‘— âˆˆ [ ğ‘›]} satisfy Lyapunovâ€™s condition [5], i.e., limğ‘›â†’âˆ ğ‘ğ‘2+2ğ›¿ + ğ‘2+2ğ›¿ğ‘ Ã ğ‘— ğ‘¤ 2 ğ‘– ğ‘—ğ‘ğ‘ 1+ğ›¿ âˆ‘ï¸ ğ‘— ğ‘¤ 2+2ğ›¿ ğ‘– ğ‘— = 0. (30) We define ğ‘ƒ and ğ‘„ similar to Theorem B.2, as ğ‘ƒ [ğ‘–] = ğ‘†ğ‘– ğ‘ , ğ‘„ [ğ‘–] = ğ‘¤ğ‘– ğ‘Š , (31) where ğ‘ = Ãğ‘˜ ğ‘–=1 ğ‘†ğ‘– and ğ‘Š = Ãğ‘˜ ğ‘–=1 ğ‘¤ğ‘– . We have a lower bound of the KL divergence between ğ‘ƒ and ğ‘„ for large ğ‘˜, KL [ğ‘ƒ âˆ¥ ğ‘„] â‰³ 0.05E2 [ğ´ğ‘– ] ğ‘ , (32) where ğ´2 ğ‘– = Ã ğ‘— ğ‘¤ 2 ğ‘– ğ‘—/ğ‘¤ 2 ğ‘– . Proof. Here we use the Lyapunov central limit theorem [5], to approximate the distribution of ğ‘Š â€² ğ‘– . We have ğ‘†ğ‘– âˆ¼ N ğ‘¤ğ‘–ğ‘, ğ‘ğ‘ğ‘¤ 2 ğ‘– ğ´2 ğ‘–  = N (ğœ‡ğ‘–, ğœ2 ğ‘– ), ğ‘ + ğ‘ = 1. (33) As we have done in Theorem B.2, we choose a subset of [ğ‘˜] to compute a lower bound of total variation distance. Let ğ¼ = {ğ‘– | ğ‘†ğ‘– â‰¥ SIGIR â€™25, July 13â€“18, 2025, Padua, Italy Shiguang Wu et al. ğœ‡ğ‘– + ğœğ‘– }, ğ›¿ = ğ‘œ (1) and ğ‘˜ğ›¿ 2 = ğ‘˜ğ‘‚ (1), we have, with probability at least 1 âˆ’ 3 exp âˆ’2ğ›¿ 2ğ‘˜ 2 ğµ2 , where ğµ = Ã ğ‘– âˆˆ [ğ‘˜ ] ğ‘¤ 2 ğ‘– , ğ‘‘TV (ğ‘ƒ, ğ‘„) â‰¥ | ğ‘ƒ (ğ¼ ) âˆ’ ğ‘„ (ğ¼ )| (34) â‰¥ Ã ğ‘– âˆˆğ¼ ğ‘†ğ‘–