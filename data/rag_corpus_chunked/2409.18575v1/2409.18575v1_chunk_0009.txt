intents, facets or aspects of query ğ‘. The goal of asking a good clarification question becomes directly related to finding the possible search intents (facets) ğ¹ that query ğ‘ engulfs. To find such search intents ğ¹, it is important to take into account the document collection ğ¶ the user searches through. In failing to do so, we risk presenting search engine users with facets that are not found in ğ¶. RAG models. Retrieval Augmented Generation models have be- come the standard choice for Knowledge-Intensive tasks where generation is required. Their strength lies on combining LLMs, that are proficient in text generation, with retrieval, that allows them to access information from a large non-parametric memory like a document collection [ 3]. That also motivates their use for our task, end-to-end and Corpus-informed clarifying question genera- tion. Specifically, we use a Fusion-in-Decoder (FiD) model [12] as a question generator, due to its efficiency and effectiveness. FiD is an encoder-decoder model, but its encoder models input documents independently (no cross-attentions), producing individual embed- dings. The decoder fuses the information from those embedding, generating an output answer. Due to the lack of cross-attentions be- tween documents, FiD models can model longer context, ie. multiple retrieved documents with the same GPU memory requirements. Retrieval. We experiment with various types of retrieval tech- niques, such as lexical ( ğµğ‘€ 25), semantic (ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ), as well as the documents that originated from the BING SERP provided with the MIMICS dataset (ğµğ¼ ğ‘ğº). For ğµğ‘€ 25 search, we use the Pyserini toolkit [17] using the default parameters. For semantic retrieval, we use the Contriever architecture that has been jointly pretrained with the ğ´ğ‘¡ğ‘™ğ‘ğ‘  checkpoints, but observe in our preliminary experiments that retrieval performance of the retriever jointly pretrained with ğ´ğ‘¡ğ‘™ğ‘ğ‘  models is sub-optimal. To this end, we initialize