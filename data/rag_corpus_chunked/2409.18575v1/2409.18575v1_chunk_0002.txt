clarifying questions, ie. presenting users with options or facets that do not exist in the collection. Some research explicitly models the underlying corpus by rely- ing on pipeline methods that (i) extract keywords or features from the document collection and (ii) generate questions based on these features [30, 32, 36, 45–47]. However, this separation prevents the joint modeling of the query and its ambiguity, the information in the corpus and the various aspects present therein, and the clarifica- tion questions. In this work we advocate for Retrieval Augmented Generation of Clarifying Questions, in a way that jointly models queries and the retrieval corpus to generate questions. Another line of work achieves state-of-the-art performance using text genera- tion models without intermediate feature extraction [21, 29], but is limited by the size of the retrieval pool. This is a crucial factor, as it determines the breadth of the possible clarifications. Since the generation model cannot read the entire corpus, Re- trieval Augmentation helps to inform the generator of the possible information needs present in the collection. Hence, along with the generator, a retriever is responsible for obtaining a representative sample that covers the uncertainty of the user’s query in the col- lection. To apply Retrieval Augmentation effectively, it is crucial to (a) select a sample of the corpus that is representative of users’ potential information needs, and (b) account for as many relevant documents as possible to cover those. For this reason, we propose using Fusion-in-Decoders (FiD), a family of models [12] that are com- putationally efficient in modelling multiple evidence documents. We demonstrate their effectiveness in simultaneously modeling the collection (user queries and retrieved documents) and generating questions, in a way that eliminates the need for an intermediate step of facet or keyword extraction and increases the ability to model larger