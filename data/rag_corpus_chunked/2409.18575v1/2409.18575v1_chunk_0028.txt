0 and 1, where higher values of ğœ† give more importance to relevance than novelty. Second, we experiment with training the dense retriever (Contriever-FT) with a diversification objective that uses knowledge distillation from the question generator [11]. This method has been commonly used in Question Answering, allowing retrievers to be trained on question-answer pairs, rather than query-document relevance pairs. However, we investigate this in a different task, where we seek to optimize retrieval for novelty or diversity of retrieved passages. In practice, our retriever here is trained on a distillation signal from the reader, which shows which documents were attended to generate the ground truth facets. In this way, we try to bias the retriever towards retrieving the ground truth facets. Introducing novelty using MMR. The first part of Table 6 shows that inducing diversity through MMR in the evidence pool does not make the ground truth facets more likely to be retrieved and therefore extracted. Specifically, we see that increasing diversifi- cation results in lower performance. This is consistent with the findings of Samarinas et al. [29], that show that diversifying a list of extracted facets with MMR does not lead to improvements in MIMICS. Interestingly, in the related area of Multi-Answer retrieval, other works also report that MMR fails to improve results, since it increases diversity but hurts relevance significantly [18]. Introducing novelty using Retriever-Reader Knowledge Distilla- tion. In the last part of Table 6, we experiment with finetuning the retriever jointly with the reader, using knowledge distillation from the ground truth facets. As usual, the retriever is initialised from the checkpoint finetuned on MSMarco. For initialising the reader, we have two different variants: ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿ âˆ’ ğ¹ğ‘‡ âˆ’ ğ‘‡ ğ‘Ÿğ‘ğ‘–ğ‘› is initialised directly from the Atlas checkpoint, pre-trained with the unsupervised Masked Language Modelling objective, while ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿ âˆ’ğ¹ğ‘‡