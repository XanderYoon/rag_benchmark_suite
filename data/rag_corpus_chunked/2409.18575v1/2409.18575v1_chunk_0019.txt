the clarifying question generation (Section 5.2.2), (c) do clarifying generation model remain faithful to given evidence documents and how does training affect that (Section 5.2.3), and (d) does increasing the document pool help in recovering missing facets 5.2.4)? 5.2.1 Measuring alignment between target facets and evidence docu- ments. First, we investigate to what extent the evidence set (ğµğ¼ ğ‘ğº snippets) we used in Section 5.1, and is commonly used in the litera- ture [9, 10, 29], is aligned with the target ground truth facets we try to generate. As a proxy for relevance, we use Term-Overlap-Recall that measures how many of all facet words appear in the evidence pool, and Exact-Match-Recall that measures whether the entire facet appears verbatim. In Table 2, we measure the alignment of the provided Bing- snippets as well as evidence pools retrieved using lexical (BM25) and semantic (Contriever) methods. For Bing snippets, we observe that only 50% of facet words appear in the evidence pool. When retrieving documents from the MSMarco passage collection [20] using the query, alignment is even lower with 43% and 45% of the facet words appearing in the evidence set. Last when we use both the original query and corresponding ground truth facets to construct the evidence pool (by expanding the original query with a single facet and interleaving the top-K results of all facets into a Model evidence (train&test) Term Overlap Exact Match Set-BERT Set-BLEU Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 BLEU1 BLEU2 BLEU3 BLEU4 BART [Bing] 0.3003 0.3199 0.2984 0.0770 0.0694 0.0711 0.4452 0.4673 0.4516 0.3994 0.3355 0.3000 0.2778 FiD [Bing] ğ‘ 0.3418ğ‘ 0.3061ğ‘ 0.3093ğ‘ 0.0854 0.0677 0.0723 0.4322 0.4571 0.4400 0.3858 0.3310ğ‘ 0.3020ğ‘ 0.2826ğ‘ FiD [BM25|Q,F] 0.4782 0.4104 0.4220 0.1605 0.1154 0.1290 0.4599 0.4920 0.4700 0.4114 0.3726 0.3504 0.3329 FiD [Contriever|Q,F] 0.4714 0.4338 0.4326 0.1674