query-facet patterns are heavily repeated within the MIMICS dataset. This can be attributed to the dataset construction process, during which facets were extracted based on search engine query logs and a taxonomy defined by the authors. To detect such patterns, we extract the top-20 frequent facet words (excluding stop-words) and inspect a couple of queries containing them. Based on those, we report the most frequent detected patterns in Table 7 and try to quantify to what extent this taxonomy bias affects the dataset. Our conservative estimate suggests that at least 1/5-th of dataset queries are biased towards the predetermined taxonomy. We arrive to this conclusion given that roughly 20% of dataset queries contain facets from this incomplete taxonomy (defined just on the top-20 facet keywords). The implications of this for evaluation are important. Our ex- periments and related work, show that model performance has an upper-bound of 5% âˆ’ 15% in Exact Match metrics (depending on the type of Retrieval Augmentation). Yet, 20% of the dataset is (at least to some extent) described by a fixed taxonomy. In such a setting, it is reasonable to assume that models over-fitting to the patterns of Table 7 might outperform models that produce more diverse but equally reasonable query facets. Further, models trained on this dataset are very likely to be biased towards the underlying taxonomy rather than being Corpus-informed, raising questions regarding their ability to generalize to open-domain settings. This, in combination with the finding that many of the facets are irretrievable (Table 2) suggests that current test collections and evaluation frameworks are not suitable for evaluating Corpus- Informed clarifying question generation. Qualitative analysis. In Table 8 we analyse a handful of random queries, presenting their ground-truth and generated facets. We show facets generated by our aligned model (train. evidence:Contr- FT|Q,F), and