while the topic of the documents is not specifically related to those. Another observation that stands out is that the ground-truth facets are not exhaustive and other equally good or better facets can be generated. Such examples include episodes of penny appearing in the big bang theory, or referring to the leiden thrombophilia disorder rather than aspects related to the city. Given those static ground truths, evaluation metrics would be very low, despite the overall high quality. For the queries “network drive”, “leiden”, “sickle cell anemia” and “consumer price index”, both Precision and Recall would be zero, despite the quality and greater diversity of the produced facets. This highlights the the presence of multiple ground truths, which are in fact ignored in the current evaluation framework. This analysis highlights an important gap in current datasets and evaluation protocols, that prohibits the generation of Corpus- informed clarifying questions: Ground truth facets are constructed from query reformulations and static taxonomies, but do not reflect the facets that are often present in search collections. 7 CONCLUSIONS AND FUTURE DIRECTIONS In this paper, we investigated the task of generatingCorpus-Informed Clarifying Questions end-to-end, based on Retrieval Augmented Generation models. We showed that Fusion-in-Decoder models are able to effectively and efficiently model queries and evidence docu- ments when generating clarifying questions. This efficiency advan- tage allows them to model larger parts of the corpus when asking clarification questions, potentially improving question quality. Further, we investigated the role of retrieval in this task, show- ing that retrieval quality is more important for the generator than finding exact lexical matches of the facets to be generated. We query facets (ground truth) facets (generated) network drive set up, remove windows 7, windows 8, windows 10 leiden what time is it, zip code, things to do, weather leiden thrombophilia penny