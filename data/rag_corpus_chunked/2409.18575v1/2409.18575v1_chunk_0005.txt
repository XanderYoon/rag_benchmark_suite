evaluation methodology for automatic question generation. Below we discuss several types of clarifying questions datasets. Clarifying Questions for Web Seach were first introduced in Qulac [2], an open-domain information-seeking conversational search dataset. This work builds upon faceted or ambiguous queries from the TREC Web Track dataset [7, 8]. The authors crowd-sourced clarifying questions to be asked from a search system to the user, as well as answers to these questions from users with different intents. The quality of automatically generated clarifying question was evaluated by the relevance of the retrieved documents. ClariQ [1] extended Qulac with additional topics and built synthetic multi-turn conversations from single-turn clarifications. MIMICS introduces a clarification pane on the search engine result page (SERP) [40, 41]. This pane includes a template-based question (eg. “Who are you shopping for”) with multiple candidate answers or search intents (eg. "men, women, kids"). Intents are extracted from query reformulation logs and a defined taxonomy. The offline evaluation framework compares ground truth intents to the generated ones [9, 29, 35], while later work also introduces an online evaluation setup based on user interactions [33, 34, 42]. Another line of research investigates clarification in community forums like StackExchange [ 16, 24, 25, 27, 34]. In these datasets, clarifying questions are written by expert users and the task is often defined as clarifying question retrieval from a pool of ques- tions. Last, there are efforts to create clarifying questions on the product search domain, either towards assisting users in product search [44, 48], but also to clarify ambiguities originating from product descriptions [28, 45]. These works differ from our line of work, since they often revolve around structured product metadata. For a more comprehensive overview on clarifying questions datasets we refer readers to Rahmani et al. [26]. 2.2 Clarifying questions generation for web