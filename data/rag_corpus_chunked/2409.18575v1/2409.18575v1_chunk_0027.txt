sets at inference time and measure performance of models trained in different ways. We observe that differences in performance metrics are minimal within groups, while models trained and tested with evidence pools from the same evidence distribution as the test distribution is somewhat better. This comes in contrast to the large differences observed in Table 3, where alignment brought more than double increases in Exact Match metrics, as well as Table 4 that showed that aligned generators are much more effective in extracting the facets present in the evidence. Therefore, our hypothesis remains that aligned generators do not perform better in those evidence sets because the facets do not exist there, and seek to improve this in the following Section. 5.3.2 Evidence diversification. Our results in the previous subsec- tion showed that grounded question generators cannot find the ground truth facets when the retrieval is not bounded towards those facets. In this section we explore whether introducing novelty in the evidence set can help. To induce novelty in the inference evidence pool, we use two sets of methods. First, we use the Maximal Marginal Relevance (MMR) algorithm [5] to rerank documents from an initially retrieved evidence pool ofùëõ = 50 documents. MMR reranks documents taking into account their relevance with respect to a query, but also how similar they are to the rest of the ranking list. In practice, this is achieved with a ùúÜ parameter that is bounded between 0 and 1, where higher values of ùúÜ give more importance to relevance than novelty. Second, we experiment with training the dense retriever (Contriever-FT) with a diversification objective that uses knowledge distillation from the question generator [11]. This method has been commonly used in Question Answering, allowing retrievers to be trained on question-answer pairs, rather than query-document relevance pairs. However,