facets in clarifying question 5.2 Building evidence sets to support Corpus-informed Retrieval-Augmented Generation of Clarifying Questions In this section, we focus on the role of retrieval and evidence docu- ments, as those are fundamental to generating good clarifications. Our hypothesis here is that if evidence documents and ground truth facets are not aligned during training, that is if ground truth facets do not appear in the evidence documents, then models learn to produce these facets out of their internal model knowledge and eventually stop relying on the retrieved evidence [15]. This means that the model becomes corpus-agnostic and static, with a high risk to "hallucinate", ie. present facets to the user that are either irrelevant to her query or do not exist in the retrieval corpus. Either of these cases would have detrimental effects for user experience . This would happen if users are presented with irrelevant facets, or facets that are irretrievable on this search collection ùê∂, caus- ing the ultimate goal of search (retrieving a relevant document) to fail. Hence, we emphasize the importance of generating Corpus- informed Clarifying Questions. Overall, we aim to answer RQ2, that is how to build evidence sets that can supportCorpus-informed Clarifying Question generation models. Specifically we look into the following questions: (a) does the corpus contain information on all ground truth facets, and can retrieval algorithms bring them up (Section 5.2.1), (b) what is the effect of missing or irretrievable facets in the clarifying question generation (Section 5.2.2), (c) do clarifying generation model remain faithful to given evidence documents and how does training affect that (Section 5.2.3), and (d) does increasing the document pool help in recovering missing facets 5.2.4)? 5.2.1 Measuring alignment between target facets and evidence docu- ments. First, we investigate to what extent the evidence set (ùêµùêº ùëÅùê∫ snippets)