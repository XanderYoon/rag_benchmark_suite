the MIMICS dataset (ğµğ¼ ğ‘ğº). For ğµğ‘€ 25 search, we use the Pyserini toolkit [17] using the default parameters. For semantic retrieval, we use the Contriever architecture that has been jointly pretrained with the ğ´ğ‘¡ğ‘™ğ‘ğ‘  checkpoints, but observe in our preliminary experiments that retrieval performance of the retriever jointly pretrained with ğ´ğ‘¡ğ‘™ğ‘ğ‘  models is sub-optimal. To this end, we initialize the retriever from the unsupervised pretraining of Contriever (ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ ) and the checkpoint finetuned on MSMarco[20] (ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ âˆ’ ğ¹ğ‘‡ )2 Ad- ditionally, we consider two retrieval variants, namely non-aligned (âˆ—|ğ‘„) or facet-aligned (âˆ—|ğ‘„, ğ¹ ). For the former, we use only the origi- nal user query to retrieve, while in case of the latter we do multiple retrieval rounds using the query and each facet and interleave the retrieved documents (without replacing duplicates). When experimenting with novelty methods (Section 5.3), we train the retriever with knowledge distillation from the question generator [11]. In practice, this method uses attention scores of document embeddings in the decoder as a proxy for document rele- vance. In Question Answering, this signal corresponds to detecting relevant passages for a question, while in our setting it promotes novel documents that contain the ground truth facets. 4 EXPERIMENTAL SETUP In this section we outline our experimental setup. 4.1 Datasets and Evaluation As discussed in Section 2.1, a number of Clarifying Question datasets and setups exist. In this work, we focus on Web Search Clarifying Questions and perform our experiments on the MIMICS dataset. Our choice is based on a number of factors, namely: (a) the presen- tation of clarifying questions that happens via a clarification pane on a SERP. In our view, this presentation is less intrusive than a conversational system that interrupts the userâ€™s journey without presenting results to ask a question, and hence less likely