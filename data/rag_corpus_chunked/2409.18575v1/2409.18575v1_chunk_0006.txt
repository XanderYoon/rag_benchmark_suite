domain, either towards assisting users in product search [44, 48], but also to clarify ambiguities originating from product descriptions [28, 45]. These works differ from our line of work, since they often revolve around structured product metadata. For a more comprehensive overview on clarifying questions datasets we refer readers to Rahmani et al. [26]. 2.2 Clarifying questions generation for web search A number of approaches have been proposed for generating clarify- ing questions for web search results. Broadly, these works use a com- bination of rule-based systems, keyword extraction or topic mod- elling approaches, and Large Language Models (LLMs). Hashemi et al. [9] proposed NMIR, a transformer architecture that learns multiple intent representations for web queries by matching differ- ent document clusters to query intents. Later works found that a transformer encoder-decoder approach based on the BART model can outperform NMIR [29]. Other research tried combining LLMs with facet extraction methods. Sekulić et al. [30] constructed the ClariQ-FKW (Facet KeyWords) dataset and used it to guide GPT-2 in generating clarifications. They find that using facet keywords to guide the LLM helped with grounding and generating useful ques- tions. In follow-up work, Sekulić et al. [32] try to improve upon the facet/keyword extraction part by using part-of-speech tags, entities and LDA topics. They combine those approaches by ranking their output using entropy-based methods and generate a template-based question using the top ranked keyword. In contrast to our work, (a) these methods only generate questions addressing one facet and (b) keep the facet extraction part disconnected from question generation. Samarinas et al . [29] combines various facet extrac- tion methods, such as autoregressive generation, sequence tagging, extreme multi-labelling classification and LLM prompting in an ensemble, concluding that these methods are often complementary. Another line of work tries to inform the CQ generation