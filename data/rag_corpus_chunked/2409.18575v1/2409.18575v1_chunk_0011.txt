our experiments on the MIMICS dataset. Our choice is based on a number of factors, namely: (a) the presen- tation of clarifying questions that happens via a clarification pane on a SERP. In our view, this presentation is less intrusive than a conversational system that interrupts the userâ€™s journey without presenting results to ask a question, and hence less likely to harm user experience [ 4], and (b) a more straightforward and robust evaluation method that directly evaluates generated facets [ 14]. In contrast, the QuLaC [2] evaluation framework involves a user answering the clarifying question and ranking documents to judge question quality. However, evaluation can be particularly noisy, since both of these parts are challenging. User answers largely de- pend on their cooperativeness [31], while ranking documents with clarification-based queries is particularly challenging [14]. Following prior work on ğ‘€ğ¼ ğ‘€ğ¼ğ¶ğ‘† , we train on ğ‘€ğ¼ ğ‘€ğ¼ğ¶ğ‘† âˆ’ ğ¶ğ‘™ğ‘–ğ‘ğ‘˜ and test on ğ‘€ğ¼ ğ‘€ğ¼ğ¶ğ‘† âˆ’ ğ‘€ğ‘ğ‘›ğ‘¢ğ‘ğ‘™ [9, 10, 29]. Evaluation metrics. Following previous research [9, 10, 29, 46], we focus on a number of lexical and semantic metrics that measure overlap of generated facets to the ground truth ones. Given a se- quence of ground truth facets, ğº = ğ‘”1, ğ‘”2, ğ‘”ğ‘›, and a sequence of generated facets, ğ¹ = ğ‘“1, ğ‘“2, ..., ğ‘“ğ‘š, we assess aspect generation qual- ity using: â€¢ Term Overlap (ğ‘ƒ, ğ‘…, ğ¹ 1): measures lexical overlap at the word level, i.e. overlap between words in ğº and ğ¹. â€¢ Exact Match (ğ‘ƒ, ğ‘…, ğ¹ 1): measures exact lexical match at the facet level, i.e. whether ğ‘“ğ‘– = ğ‘”ğ‘— . â€¢ Set-BERT [9]: measures semantic overlap at the facet level based on BERT-score [43] â€¢ Set-BLEU-ngram[9, 23]: measures n-gram overlap at the facet level 2https://huggingface.co/facebook/contriever https://huggingface.co/facebook/contriever-msmarco Model Co-Gen Term Overlap Exact Match Set-BERT Set-BLEU Prec. Rec.