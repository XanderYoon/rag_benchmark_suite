rather than the lexical one. Therefore, we conclude that ğ¹ğ‘–ğ· can successfully extract and paraphrase facets from evidence pools without the need to find them verbatim in the evidence, and verify that retrieval quality is of great importance when generating clarifications. 5.2.3 Faithfulness of Question Generators towards evidence docu- ments. Next, we test the faithfulness of question generators to the evidence pool. We emphasize the importance of this property to ensure clarifications are grounded in the retrieval corpus, where search will take place. To probe model faithfulness, we design a leave-one-out (LOO) evaluation, where documents corresponding to a facet are included or dropped from the evidence. We start from an evidence pool containing all facets (similar to âˆ—|ğ‘„, ğ¹ ) and measure ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ of a random ground truth facet before or after re- moving its documents. We probe the generators trained on the Bing 10 30 50 # of evidence documents 0.00 0.05 0.10 0.15 0.20 0.25Exact Match F1 retriever bm25_Q bm25_QF ContrFT_QF ContrFT_Q Figure 1: Effect of increasing the number of evidence docu- ments in performance snippets evidence set, the non-diversified ğµğ‘€ 25|ğ‘„ and the aligned generator ğµğ‘€ 25|ğ‘„, ğ¹ . As we can see on Table 4, aligned Fusion-in-Decoders are much more likely to capture the facet if the evidence documents contain it, in contrast to the non aligned ones (ğ‘‡ ğ‘’ğ‘Ÿğ‘šğ‘‚ğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘ and ğ¸ğ‘¥ğ‘ğ‘ğ‘¡ğ‘€ğ‘ğ‘¡ğ‘â„ - ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ ). ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ âˆ’ ğ¿ğ‘‚ğ‘‚ performance is roughly the same across models, with facet recall of aligned models being slightly lower.6 At the same time, the drop in Recall of the facet that is removed from the evidence set (Î”ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ ) is much larger in aligned models, demonstrating substantially more faithful clarifications towards the evidence documents. This hints that previous question generators lack sufficient evidence grounding, which is essential for creating Corpus-informed Clarifications.