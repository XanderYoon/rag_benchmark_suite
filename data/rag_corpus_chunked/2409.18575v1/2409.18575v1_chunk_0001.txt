INTRODUCTION Open-domain search and Question Answering (QA) systems make the best effort to respond to any user’s question or query. Re- cent work attempts to quantify the uncertainty in the question- answering models in order to defer from answering a question they are uncertain about [6, 38]. In a parallel line of research, lim- ited work in open-domain conversational and ad-hoc search sys- tems has investigated enabling them to ask Clarifying Questions (CQs) [2, 19, 41]. The majority of this work employs static models of clarifying question generation, i.e. models that generate a clarifying question independent of the ability of the system to locate the right answer in the underlying corpus, or the potential different answers present therein [2, 9, 10, 40, 47]. In this work, we emphasize the importance of generatingcorpus- informed clarifying questions, that are dynamic with respect to the collection. We argue that it is critical to defer the ambiguity of the user query by modeling the aspects1 of it in the document collection. Since the primary goal of search is to retrieve relevant information, clarifying questions should be generated as a function of the corpus and the relevant information therein. Failing to be dynamic and corpus-informed poses a risk to user experience, (a) due to the disruption caused by asking generic questions that offer no relevant information, and more severely (b) due to "hallucinations" while 1Intents, facets and aspects are used interchangeably in this paper. generating clarifying questions, ie. presenting users with options or facets that do not exist in the collection. Some research explicitly models the underlying corpus by rely- ing on pipeline methods that (i) extract keywords or features from the document collection and (ii) generate questions based on these features [30, 32, 36, 45–47]. However, this separation prevents the joint modeling of the