also in terms of evaluation and result reliability. 5.2.2 Effect of retriever in generation performance. In Table 3 we use these different evidence pools to explore how they affect down- stream performance. It is evident that aligning evidence and gen- eration (âˆ—|ğ‘„, ğ¹ ) during training and inference brings a big boost and significant performance improvements across metrics. Most notably the strict Exact Match metric that increases from âˆ¼ 6-7% to up to âˆ¼ 16% when our best retriever (ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ âˆ’ ğ¹ğ‘‡ ) is used. Overall, facet extraction scores follow the trend of the alignment statistics of Table 2. Using only the query (âˆ—|ğ‘„) to retrieve evidence has slightly smaller facet alignment than the Bing snippets and this same trend is reflected in facet extraction performance. We observe that using better retrievers (ie. the finetuned Con- triever) only results in gains in the (âˆ—|ğ‘„, ğ¹ ) setting. This shows that retrievers have to be biased towards the ground truth facets to find them, and suggests the presence of other prevalent facets within the collection. We explore this issue further in Sections 5.3.2 and 6. It is also noteworthy that the generator benefits significantly more from high-quality semantic retrieval (ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘’ğ‘Ÿ âˆ’ ğ¹ğ‘‡ ) than lexical (ğµğ‘€ 25), even when measuring lexical metrics. This is important because the semantic pool of documents has less lexical overlap with the facets (0.345 vs 0.380, Table 2), yet ğ¹ğ‘–ğ· benefits more from this retriever pool, rather than the lexical one. Therefore, we conclude that ğ¹ğ‘–ğ· can successfully extract and paraphrase facets from evidence pools without the need to find them verbatim in the evidence, and verify that retrieval quality is of great importance when generating clarifications. 5.2.3 Faithfulness of Question Generators towards evidence docu- ments. Next, we test the faithfulness of question generators to the