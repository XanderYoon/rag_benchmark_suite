and present results on Table 1. Fol- lowing prior work, we only report facet generation performance so results are comparable across different methods. We experiment with three variants ofFiD, depending on whether the model only predicts facets ( FiD-AspGen) or jointly generates facets and the template-based questions of MIMICS (FiD-CQGen, FiD-RevGen). FiD-CQGen predicts question and facets in the orig- inal order of MIMICS (question followed by the aspects), while FiD-RevGen first outputs the aspects. We find that all variants are competitive, with FiD-RevGen performing best across Exact Match metrics while being able to co-generate questions and aspects. For this reason, in the rest of our experiments we use this variant, unless stated differently. Next we compare FiD models with BART, the SOTA seq2seq baseline. We observe that results are mixed, with significant differ- ences occurring for both models on different metrics. ùêπùëñùê∑ signif- icantly outperforms in Term Overlap Precision metrics, but loses in Term Overlap Recall and Set-BERT Precision, Overall, BART seems to be better in terms of recall, and also when compared us- ing the Set-BERT metric, which computes facet-to-facet semantic similarities. The Set-BERT metric, although used in relevant lit- erature [9, 10, 29, 43], has significant shortcomings, as it favors method that generate a large number of facets. This is attributed to the fact that our methods generate less facets on average compared to the baselines. The FiD method is also on par with BART when using Set-BLUE, outperforming BART for larger n-grams. In gen- eral, BART achieves higher Recall while FiD is stronger in terms of Precision. This comparison between FiD and BART is impor- tant, because FiD fuses document embeddings in the decoder and therefore can model lengthier or more evidence documents than BART, with the same GPU memory footprint. The benefits of this efficiency are