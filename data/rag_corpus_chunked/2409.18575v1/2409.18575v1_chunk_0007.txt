only generate questions addressing one facet and (b) keep the facet extraction part disconnected from question generation. Samarinas et al . [29] combines various facet extrac- tion methods, such as autoregressive generation, sequence tagging, extreme multi-labelling classification and LLM prompting in an ensemble, concluding that these methods are often complementary. Another line of work tries to inform the CQ generation us- ing descriptions of queries and lists of attributes from retrieved web pages [47]. Those are extracted with heuristics and filtering, and ranked using learning-to-rank. The top ranked are given to a seq2seq model (QLM [40]) to generate the final clarifying question. Similarly, Zhao et al. [46] focuses on complementing web-search results with relational information (eg. “Windows”→ Operating system) extracted from web search results to inform the generation. Wang et al. [36] introduces another approach that uses LLMs and extracted keywords to generate clarifications. Instead of prompting, they use Neurologic decoding, a constrained generation approach that biases the generation towards these keywords. Mltiple clarifica- tions are generated and then ranked with a CQ ranking system. In contrast to those our work focuses on modelling the collection and generating a question in an end-to-end way, without the need for intermediate steps. Finally, another line of work focuses on ranking clarifying questions from a large pool of existing questions [1, 22]. However, such question pools do not exist in an open-domain set- ting and we do not discuss those further. Those approaches include many different components (eg. keyword extraction, generation) that work in a disconnected way from each other. Last there is work that focuses on enhancing the training of au- toregressive models for generating clarification facets, which are by nature unordered. Next token prediction objectives could unfairly penalize the models for predicting facets in a different ordering. To tackle this, Hashemi