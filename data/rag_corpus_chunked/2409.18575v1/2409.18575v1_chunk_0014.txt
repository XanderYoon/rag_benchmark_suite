do not compare with baselines that focus on optimising the train- ing objectives for generating set-based predictions, since those are orthogonal to our work and beyond the scope of this paper [21]. 4.3 Implementation details We initialize the question generator (FiD) model from anğ´ğ‘¡ğ‘™ğ‘ğ‘  âˆ’ğ‘ğ‘ğ‘ ğ‘’ checkpoint, which is pretrained with an unsupervised language modelling objective and exhibits good few-shot abilities [13]5. Un- less stated differently, we use a training batch size of32 to maximize our GPU memory usage, and a maximum generation length of 64 to fit the longest generation output. The maximum length of the encoded documents is also set to 64, since our collection consists of 3See footnote 6 of Samarinas et al. [29] 4https://github.com/algoprog/Faspect/ 5https://github.com/facebookresearch/atlas/ small bing snippets and MSMarco-passage. We do early stopping while optimizing for Exact Match F1 (chosen due to reliability and computational efficiency) on a held out validation set, taken from MIMICS-Click. We perform our experiments on one Nvidia RTX A6000 GPU. 5 RESULTS In this Section, we discuss our experimental results and the answers to our Research Questions. 5.1 Can we generate end-to-end Clarifications with Retrieval Augmented Generation? In this section, we try to answer RQ1, that is whether we can gen- erate Clarifying Questions end-to-end with Retrieval Augmented Generation models. In this part, we use as Retrieval Augmenta- tion (evidence) the BING snippets provided along with the MIMICS dataset [40]. We compareFusion-in-Decoder (FiD) models with other RAG and closed book models and present results on Table 1. Fol- lowing prior work, we only report facet generation performance so results are comparable across different methods. We experiment with three variants ofFiD, depending on whether the model only predicts facets ( FiD-AspGen) or jointly generates facets and the template-based questions of MIMICS (FiD-CQGen, FiD-RevGen). FiD-CQGen predicts question and facets in the orig- inal