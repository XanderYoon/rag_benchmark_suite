document length, which neural models may be sensitive to: once over the entire topN doc- uments and once over the ﬁrst 500 terms of the top N documents. This yields 16 features total. In cases where the linear regression model that serves as our meta-learner predicts values for α outside of the range [0, 1], we round the value to 0 or 1 as appropriate. Given that M1 and M2 may produce scores in diﬀer- ent ranges, we ﬁrst normalize the scores before combining them. We do so by dividing the scores by the absolute value of the result set’s average score. Fixed alpha baselines. In order to determine whether the gains achieved by our meta-learners are due to query-level alpha predictions or are simply due to the simple combination of diﬀerent retrieval models, we consider baselines which use a ﬁxed alpha value for all queries. For these ﬁxed alpha baselines, we compute the optimal α that maximizes the performance on the entire training set. We varyα from [0, 1] in 0.1 intervals as done with the meta-learners. We then use thisα to compute the performance on all queries in the test set. Since this model performs no query speciﬁc computations, its performance can be considered to signify the gain that can be achieved by simply combining two ranking methods without considering any query-level features. Oracles. In order to understand the theoretical maximum gain that can be achieved by the meta-learners, we additionally report results using query-level oracle models. For each query in the test, we report the results using the optimal alpha. As before we vary α from [0, 1] in 0.1 intervals. Thus oracle results reveal the performance of a perfect meta learner. These results signify the improvements in retrieval that can be achieved by using query level