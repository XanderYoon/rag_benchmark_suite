the terms New and York is important. On the other hand, for the query Bidgely Data Science Company , the occurrence of Bidgely is most important, and documents talking about a diﬀerent Data Science Company are unlikely to be relevant. Thus, for the latter query, we would like a retrieval function to weight the occurrence of rare uni- grams higher than the occurrence of ordered bigram or trigram matches. This behavior may not be ideal for the former query. Diﬀerent retrieval methods are generally sensitive to diﬀerent retrieval ax- ioms, especially in the case of neural ranking methods. [6,24] Many neural rank- ing methods are not sensitive to document length normalization, for example, ⋆ This work was conducted during an internship at MPII. arXiv:1904.05737v1 [cs.IR] 11 Apr 2019 2 S. Arora and A. Yates and others are not sensitive to term discrimination because they do not consider IDF. This observation motivates our eﬀort to combine scores from diﬀerent re- trieval method based on a given query’s retrieval needs. Determining the optimal behavior for a given query (or even domain) is inherently diﬃcult, however, and axioms cannot yet describe a retrieval method’s optimal behavior on a per-query level. In this work we aim to reduce this gap by investigating query-level meta- learning in order to select an optimal combination of retrieval methods for a given query. Meta-learning in Information Retrieval is most common in the context of Query Prediction Performance (QPP), which share some similarities with this work. The goal of QPP is to predict a retrieval model’s performance for a given query. Prior work in this area has used these predictions to select a retrieval algorithm [33] or to weight an ensemble of models [32]. We follow this line of research by investigating axiom-inspired features for diﬀerentiating between the performance