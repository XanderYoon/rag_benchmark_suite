128 batches each. Meta-learner training. We instantiate one meta-learning method for each pair of models considered and train each meta-learner using the same approach as with the neural IR models. That is, the meta-learner is trained on three out of ﬁve folds, and its single hyperparameter N is chosen using the validation fold from the following values: 20, 50, 100, 200, 500. Each meta-learner’s ranking 5 https://code.google.com/archive/p/word2vec/ Investigating Retrieval Method Selection 7 methods M1 andM2 are trained using the same training and validation folds as the meta-learner is. Each meta-learner is trained to predict the optimal value of α for a given query based on the features described in the previous section. To determine the optimal values of α, we vary α from [0, 1] in 0.1 intervals. For each query we choose the value of α that maximizes the performance of the two methods as measured by nDCG@20 and use this value as the ground truth when training. When calculating the seven features that require an initial result set, we identify the top N documents using the strongest bag-of-words ranking method con- sidered by the meta-learner. In cases where both the ranking models consider n-grams, we depend on BM25’s top N documents to compute the features (i.e., we use KNRM for the KNRM+BM25, KNRM+PACRR, KNRM+DeepTileBar, KNRM+ConvKNRM pairs and we use BM25 for the remaining pairs). We cal- culate these seven features twice in order to consider the impact of document length, which neural models may be sensitive to: once over the entire topN doc- uments and once over the ﬁrst 500 terms of the top N documents. This yields 16 features total. In cases where the linear regression model that serves as our meta-learner predicts values for α outside of the range [0, 1], we round the value to