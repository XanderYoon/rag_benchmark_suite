only the document or query. For example, Nie et al. [22] showed that combining relevance scores with page importance scores calculated using PageRank and HITS can improve performance. He et al. [10] tried to incorpo- rate topic of user’s interest and other characteristics of user to improve retrieval process. Linguistic features, such as the number of adjectives in a paragraph, have also been considered. [35] The Letor Benchmark [23] includes many pre- computed features like relevance scores from a range of retrieval models over diﬀerent ﬁelds, the document’s PageRank, and features derived from the URL. In terms of LTR models, a variety of algorithms have been proposed and can be group into three broad categories indicating how documents are compare to one another: pointwise, pairwise, and listwise approaches. While this work shares some similarity with LTR approaches, our approach diﬀers in that we combine models’ retrieval scores directly in order to produce an improved rank- ing, whereas LTR approaches use these scores as features to predict a ranking for a set of documents. In addition, our features are mostly based on properties of an initial result set rather than on relevance scores. F ederated Search. In the area of federated search there has been much work on combining results from various algorithms and document collections [28], such as using the presence of a document in an external result set to predict relevance. [5] More recently, some neural models for ad-hoc retrieval have tried to implicitly combine signals from multiple relevance models by incorporating the scores as features that are fed into the model. [26,27] This work diﬀers from ours because the scores considered are constant regardless of the query, whereas we perform algorithm selection on retrieval models trained independently and weight the models’ scores based on a query. 3 Methodology