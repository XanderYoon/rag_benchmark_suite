approaches. Pre-retrieval approaches use linguistic features of the query as well as other features that can be computed without computing relevance scores for the collection. As a result, the pre-retrieval approaches are usually more eﬃcient. Mothe et al. [20] use linguistic features, such as part of speech tags and polysemy information obtained from Wordnet, to predict query diﬃculty. They found a signiﬁcant correlation between these features and performance for a query. He et al. [9] use corpus statistics like average query length, IDF of the query, and query scope to predict query performance. Query ambiguity, which was estimated by considering the coherence between documents containing query terms, has also been observed to be a useful feature. [11] Hauﬀ et al. [8] provide a comprehensive overview of pre-retrieval predictors. Post-retrieval approaches use the ranked list for a given query to predict query diﬃculty. They have been found to outper- form pre-retrieval approaches. Townsend et al. [3] use predicted relevance scores to estimate query ambiguity. Zhou and Croft [38,39] estimate performance by measuring how robust the ranked list is to perturbations. The retrieval score dis- tribution can also give crucial insight into query performance. [29] More recently, neural approaches with weak supervision has been employed for this task. [37] There has also been some work in using these query performance prediction fea- tures for meta learning. Yom-Tov et al. [36] query diﬀerent datasets and compute the query’s diﬃculty for each dataset. This query diﬃculty is used to weight the scores from each dataset to produce a ﬁnal combined ranked list. Winaver et al. [32] used a query clarity measure to predict the best performing language model from a pool of language models with diﬀerent parameters. In [33], the authors use the ranked results produced by systems submitted to TREC and predict the