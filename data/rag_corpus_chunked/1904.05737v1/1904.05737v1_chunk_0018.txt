The document length feature favors PACRR, DeepTileBar, and KNRM over BM25 even though both these ranking methods do not consider document length as an explicit signal. This may be related to the observation that BM25 sometimes overpenalizes long documents. [15]. The document length feature always favors KNRM over other models, indicating that summing query term scores can help KNRM to con- sider document length. The query coverage feature tends to favor models that sum query term scores rather than combining them with a fully connected layer (i.e., KNRM and BM25 are preferred over PACRR). Query coverage seems to strongly favour BM25 over DeepTileBar, whereas DeepTileBar is favoured over KNRM and PACRR, which may indicate that DeepTileBar’s bagging with dif- ferent kernel sizes is a more eﬃcient mechanism for query coverage. Additionally, DeepTileBar is strongly preferred over BM25 for both ordered and unordered matches. Regarding the ConvKNRM meta-learners, which are empirically the best-performing, bigram matches seem to favour BM25 and PACRR whereas unordered matches seem to favour ConvKNRM in both meta learners. It may be that ConvKNRM’s cross matching CNN layers capture unordered matches more eﬃciently than PACRR’s approach. DeepTileBar and KNRM are preferred over ConvKNRM for unordered matches, whereas ConvKNRM is preferred over bigram matches for DeepTileBar and trigram matches for KNRM. In Figure 1 we analyze the distribution of per-query weights predicted by several meta-learners and compare them to the query weights selected by the ﬁxed alpha baseline. 6 The KNRM+ConvKNRM meta-learner is an interesting case. The baseline always gives zero weight to KNRM and exclusively uses Con- vKNRM’s predictions. However, the meta-learner uses ConvKNRM exclusively 6 The baseline’s weights are ﬁxed for each test set but vary across diﬀerent test sets. Investigating Retrieval Method Selection 11 0.0 0.2 0.4 0.6 0.8 1.0 Weight given to model ( )