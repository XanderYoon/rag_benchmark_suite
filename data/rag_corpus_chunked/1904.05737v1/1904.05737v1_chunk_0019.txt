The KNRM+ConvKNRM meta-learner is an interesting case. The baseline always gives zero weight to KNRM and exclusively uses Con- vKNRM’s predictions. However, the meta-learner uses ConvKNRM exclusively 6 The baseline’s weights are ﬁxed for each test set but vary across diﬀerent test sets. Investigating Retrieval Method Selection 11 0.0 0.2 0.4 0.6 0.8 1.0 Weight given to model ( ) 0.0 0.2 0.4 0.6 0.8 1.0Fraction of queries Favours KNRM Favours ConvKNRM Metalearner Baseline 0.0 0.2 0.4 0.6 0.8 1.0 Weight given to model ( ) 0.0 0.2 0.4 0.6 0.8 1.0Fraction of queries Favours BM25 Favours ConvKNRM Metalearner Baseline 0.0 0.2 0.4 0.6 0.8 1.0 Weight given to model ( ) 0.0 0.2 0.4 0.6 0.8 1.0Fraction of queries Favours BM25 Favours PACRR Metalearner Baseline 0.0 0.2 0.4 0.6 0.8 1.0 Weight given to model ( ) 0.0 0.2 0.4 0.6 0.8 1.0Fraction of queries Favours ConvKNRM Favours PACRR Metalearner Baseline Fig. 1: Weight (α) distributions for meta-learners and ﬁxed alpha baselines. 12 S. Arora and A. Yates for only 38% of the queries, which yields a signiﬁcant improvement. The dif- ference in alphas is even larger for the BM25+ConvKNRM meta-learner: the baseline chooses an alpha of at least 0.9 the vast majority of the time, whereas the meta-learner chooses alphas between 0.4 and 0.6 about 50% of the time. Our empirical evaluation demonstrates that both meta-learners signiﬁcantly improve over the ﬁxed alpha baselines. Similar analysis holds true for Con- vKNRM+PACRR; the baseline chooses alpha less than or equal to 0.1 for 60% of total queries whereas the meta-learner chooses alpha between 0.2-0.5 for 75% of queries. It is not the case that the meta-learner simply favors the models that per- form better. For example, with PACRR+BM25, the baseline chooses a value of alpha greater than 0.6 about 50%