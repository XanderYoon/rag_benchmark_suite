query diﬃculty is used to weight the scores from each dataset to produce a ﬁnal combined ranked list. Winaver et al. [32] used a query clarity measure to predict the best performing language model from a pool of language models with diﬀerent parameters. In [33], the authors use the ranked results produced by systems submitted to TREC and predict the performance of each of these systems. They use this predicted performance to categorize input systems as good, fair, or bad. This categorization is used to weight results from the input systems and produce a ﬁnal ranking. While our approach shares some similarities with this prior work, we build upon it by predicting the retrieval systems’ weights directly and attempting to characterize the systems’ strengths in terms of axiom-related features. Learning to Rank. Another area of research closely aligned to ours is learn- ing to rank. In learning to rank (LTR), multiple features are computed for each query-document pair and considered by a supervised model to produce a docu- ment ranking. Relevance scores from diﬀerent retrieval functions are commonly used, making LTR an eﬀective way to combine scores for diﬀerent retrieval func- tions. Corpus statistics (e.g., TF, IDF) and their combinations may also be used as features [21,1]. Nallapati et al. [21] compute these features separately from the entire text of document, the anchor text, and the title. LTR features may also 4 S. Arora and A. Yates be based on only the document or query. For example, Nie et al. [22] showed that combining relevance scores with page importance scores calculated using PageRank and HITS can improve performance. He et al. [10] tried to incorpo- rate topic of user’s interest and other characteristics of user to improve retrieval process. Linguistic features, such as the number of adjectives in a paragraph,