order to gain further insight about the meta-learning methods, we consider the weights they assign to features. In order to mitigate the impact of the features’ varying scales, we scale the feature values to zero mean and unit variance before training. These feature weights are shown in Table 2 and 3. Negative weights indicate that the meta-learner favors the second ranking method, whereas positive weights indicate the ﬁrst ranking method is favored (e.g., given PACRR+KNRM, a negative weight means the feature favors KNRM over PACRR). Note that the two types of document features can sometimes cancel each other out. To remove the impact of such cancellation on our analysis, we train two separate meta-learners, with each using only one type of document feature. We then choose the meta learner that achieved better performance and used its feature weights in the analysis. In this table, several features weights are related to behavior described by the IR axioms. Features related to the frequency of query terms generally do not favor PACRR, which may be related to the fact that PACRR’s k-max pool- ing considers only the k strongest matches for each query term. This violates TFC1, because it makes the model oblivious to the diﬀerence in relevance of a document with more than k matches as compared to a document with exactly k matches. The unordered match feature favors PACRR over BM25 and DeepTile- Bar but prefers KNRM and ConvKNRM over PACRR. The document length feature favors PACRR, DeepTileBar, and KNRM over BM25 even though both these ranking methods do not consider document length as an explicit signal. This may be related to the observation that BM25 sometimes overpenalizes long documents. [15]. The document length feature always favors KNRM over other models, indicating that summing query term scores can help KNRM to