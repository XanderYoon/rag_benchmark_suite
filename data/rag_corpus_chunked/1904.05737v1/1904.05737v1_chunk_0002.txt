which share some similarities with this work. The goal of QPP is to predict a retrieval model’s performance for a given query. Prior work in this area has used these predictions to select a retrieval algorithm [33] or to weight an ensemble of models [32]. We follow this line of research by investigating axiom-inspired features for diﬀerentiating between the performance of two ranking functions and predicting how to combine their scores in order to improve retrieval performance. This additionally shares some simi- larities with Learning to Rank (LTR) [23], where scores from diﬀerent ranking functions are considered by a model in order to predict an improved ranking. However, we focus on learning when one model should be preferred over another for a given query instead of attempting to produce a ranking directly. In this work we propose performing a query-dependent weighted combination of retrieval models’ scores in order to improve retrieval performance. Inspired by IR axioms, we identify a set of nine feature types upon which to base this linear combination of relevance scores. This proposed meta-learner predicts the weights that should be given to the scores from two retrieval models M1 and M2 on a per-query basis. We consider several pairs of retrieval models, which consist of both BM25 and four neural re-ranking models. Our contributions are: (1) the proposal of a meta-learner using nine feature types to predict how to best produce relevance scores for a given query; (2) an evaluation of the proposed approach against the base models themselves; and (3) an analysis of the weights given to the meta-learner’s features and the model weights predicted by the meta-learner. 2 Related Work Instance level meta learning, as deﬁned in [2], refers to the task of selecting the best algorithm or appropriately combining a pool of algorithms for