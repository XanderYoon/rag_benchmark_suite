and max query IDF ). 3 We did not observe substantial improvements when using more powerful models. Investigating Retrieval Method Selection 5 The remaining seven features consider interactions between the query and the top N documents returned by an initial ranker. Average query IDF and max query IDF. These feature consider the sat- isfaction of Term Discrimination Constraints (TDC) [6], which state that terms more popular in a collection should be penalized. A query with a low average IDF may not beneﬁt from a model’s ability to satisfy TDC, whereas retrieval performance on a query with a high IDF is expected to improve when a retrieval model satisﬁes this axiom. 4 F requency of query terms. This feature is computed as the average frequency of query terms normalized by document length. It is used as a proxy for Term Frequency Constraints (TFC1) [6], which requires a retrieval function to give higher a score to document with more query term matches, and for TF-LNC [6], which requires the retrieval method to balance the interaction between term frequency and document length. Neural IR models that truncate documents to a ﬁxed size, such as PACRR, are not capable of normalizing term matches by the document length. F requency of highest IDF query term. This feature is also normalized. Document length. This feature is averaged over the top N documents. It is related to Length Normalization Constraints (LNCs). [6]. Query coverage. This feature is calculated as the average percentage of query terms that occur in the top N documents for the query. It is closely related to TFC3 [6], which requires a retrieval method to give a higher score to a document with more distinct query terms. Bigram and trigram matches. These features are the average numbers of bigram matches and trigram matches in