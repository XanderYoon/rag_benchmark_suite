the high computational costs of hyperparameter tuning, we keep most of the models’ parameters at their default values. We set PACRR’s k-max pooling parameter to 2, replace its RNN with a fully connected layer of size 32 as in prior work [13], and keep PACRR’s other parameters at their default values (as described in the original paper). Following prior work [13], we add a fully connected layer of size 30 with a tanh nonlinearity to KNRM. We leave KNRM’s other parameters at their default values. For DeepTileBar, We use all parameters set to their default values as provided in [30] (i.e., α = 20 and β = 6 for text tiling, nq = 5, nb = 30, l = 10, number of units in LSTM to 3 and MLP with 2 hidden layers with 32 and 16 units each). We perform TextTiling using NLTK’s implementation. We change the loss function from ranknet loss to hinge loss in DeepTileBar and our empirical evaluation show no diﬀerence in performance. For ConvKNRM, we used all default parameters but freeze the embeddings. For ConvKNRM, KNRM, and PACRR we set the maximum document length to 800 and the maximum query length to 4; we truncate or zero pad to reach these lengths. All models are trained using a pairwise ranking hinge loss and the Adam optimizer [14] with its default parameters. We use a batch size of 32 and train for 150 iterations consisting of 128 batches each. Meta-learner training. We instantiate one meta-learning method for each pair of models considered and train each meta-learner using the same approach as with the neural IR models. That is, the meta-learner is trained on three out of ﬁve folds, and its single hyperparameter N is chosen using the validation fold from the following values: 20, 50, 100,