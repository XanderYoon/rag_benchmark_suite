Yates using Terrier. [16] We instantiate our approach using every pair of the following models to serve as M1 and M2: BM25 [25], KNRM [34], PACRR [12], Deep- TileBar [30] and ConvKNRM [4] . These ﬁve models additionally serve as our baselines. We re-rank the TREC qrels (i.e., all judged documents) in order to remove the eﬀects of an initial ranking method. All methods are evaluated using the common nDCG@20 (normalized discounted cumulative gain), MAP (mean average precision), and P@30 (precision at 30) metrics. We create ﬁve folds cor- responding to years 2010–2014 of the Web Track and use them for training, testing, and validation in a round robin manner. Three folds are used for train- ing, one fold for validation (i.e., hyperparameter and epoch selection), and the remaining fold for testing. We consider all combinations of these folds, resulting in 20 testing folds for each method evaluation. We consider nDCG@20 on the validation set. Hyperparameters. We tune BM25’s parametersk1 andb on the concatenation of the training and validation folds, ﬁxing the values that performed best across folds. We choose the value of k1 from [0.1, 4.0] in intervals of 0 .1 and b from [0.1, 1.0] in intervals of 0.1. We use pre-trained word2vec embeddings 5 [19] with the neural IR models (i.e., KNRM, PACRR, DeepTileBar, and ConvKNRM) and train them further on our collection to avoid missing terms. We freeze the embeddings during training with all models. Given the high computational costs of hyperparameter tuning, we keep most of the models’ parameters at their default values. We set PACRR’s k-max pooling parameter to 2, replace its RNN with a fully connected layer of size 32 as in prior work [13], and keep PACRR’s other parameters at their default values (as described in the original paper). Following prior work