retrieval have tried to implicitly combine signals from multiple relevance models by incorporating the scores as features that are fed into the model. [26,27] This work diﬀers from ours because the scores considered are constant regardless of the query, whereas we perform algorithm selection on retrieval models trained independently and weight the models’ scores based on a query. 3 Methodology Our algorithm selection approach consists of a supervised meta-learner and a pair of retrieval methods M1(q,d ) and M2(q,d ). The meta-learner is trained to combine the scores from both retrieval methods to produce a ranking. That is, given a query q and features calculated over the top N documents returned by an initial ranking method, the meta-learner’s goal is to predict a value α∈ [0, 1] that maximizes the retrieval performance of the query-document ranking functionscore(q,d ) =αM1(q,d )+(1−α)M2(q,d ). In this section we describe the meta-learner and its features. We instantiate the approach with speciﬁc retrieval methods M1 and M2 in the next section. The meta-learner consists of a regression model for predicting α based on a training set of queries and documents. In this work we use a linear regression since this allows for interpretable feature weights. 3 The meta-learner’s predic- tions are based on nine features that were inspired by prior work studying how IR axioms relate to retrieval methods’ performance. [6] Of these nine features, two consider only the query terms (i.e., average query IDF and max query IDF ). 3 We did not observe substantial improvements when using more powerful models. Investigating Retrieval Method Selection 5 The remaining seven features consider interactions between the query and the top N documents returned by an initial ranker. Average query IDF and max query IDF. These feature consider the sat- isfaction of Term Discrimination Constraints (TDC) [6],