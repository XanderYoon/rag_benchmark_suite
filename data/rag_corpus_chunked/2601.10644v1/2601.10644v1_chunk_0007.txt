ex- ample, a pipeline could indicate that results from two first-stage retrieval engines should be fused and then reranked. Pipelines can be composed of available Engines on the fly. 3.1 Retrieval Engines Each retrieval model or system should be wrapped as an Engine, which imple- ments the interface for serving queries. Each engine can provide one or more of the four core retrieval capabilities: (a) index searching, (b) query-passage scoring (for reranking), (c) query rewriting (or generation), and (d) result fusion. Allowing each engine instance to provide multiple capabilities can minimize the memory footprint when different tasks share the same underlying models. For instance, most bi-encoder models can provide both first-stage retrieval that results in a ranked list of documents, and query-passage scoring (reranking) that enables fast passage selection when a RAG pipeline [6, 15, 18] needs to compress the document input to the most relevant passages to feed to a downstream gener- ation step. To improve retrieval effectiveness, an Engine can provide a reranker, such as monoT5 [37], Qwen3 Reranker [53], or Rank1 [44]. Rerankers take a query and a list of strings (e.g. documents) as input and output a ranking over the input list. InRoutIR, modules can be initiated in Python as a standalone model instance to provide a unified interface; this is similar to what PyTerrier extensions such aspyterrier-colbertprovide. However, the primary benefit of this thin wrapper is that it provides a robust and flexible online search service. Queries as inputs to the Engine are batched (as detailed in Section 3.2) to provide better service throughput; this is helpful because most bi-encoders and cross-encoders can score multiple queries and documents in the same matrix multiplication, leading to better GPU utilization. The common measurement for query-time efficiency has been query latency measured by sequentially issu- ing queries to