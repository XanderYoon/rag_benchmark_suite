service. Queries as inputs to the Engine are batched (as detailed in Section 3.2) to provide better service throughput; this is helpful because most bi-encoders and cross-encoders can score multiple queries and documents in the same matrix multiplication, leading to better GPU utilization. The common measurement for query-time efficiency has been query latency measured by sequentially issu- ing queries to the model during offline evaluation [38]. However, when serving multiple users or queries as a service,RoutIRoptimizes for throughput (i.e., the number of queries served in a fixed period of time) since queries can often be served asynchronously. Especially for a RAG pipeline that issues multiple 6 E. Yang et al. queries simultaneously [11, 49], all retrieval results must arrive before genera- tion begins; this suggests the need for high-throughput rather than low-latency (although the two qualities are usually correlated). Multi-Server Request Routing.A special type of Engine is aRelayâ€“ an Engine that relays requests to anotherRoutIRendpoint. This capability is par- ticularly useful when computing resources are divided among multiple machines or if some compute nodes are not exposed to a public IP (a common setup in academic research clusters). This is similar to the proxy service inLiteLLM 11 that relays LLM requests to compute endpoints without exposing multiple ma- chines to the end users. WhileRoutIRdoes not offer load-balancing at the request level (which may be included in future versions), it offers triage at the model level to direct requests to models with different resource requirements to different machines.RoutIRalso supports importing services from a list of endpoints to simplify configuration (more on this in Section 4.2). This feature provides the backbone for collectively serving multiple retrieval models with one endpoint in a distributed computing environment, which is crucial to facilitate complex retrieval pipelines (more on this later in this section). 3.2 Query