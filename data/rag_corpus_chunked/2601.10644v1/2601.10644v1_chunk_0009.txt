with different resource requirements to different machines.RoutIRalso supports importing services from a list of endpoints to simplify configuration (more on this in Section 4.2). This feature provides the backbone for collectively serving multiple retrieval models with one endpoint in a distributed computing environment, which is crucial to facilitate complex retrieval pipelines (more on this later in this section). 3.2 Query Processor Each Engine is further wrapped in a Processor class, which handles caching and queuing of input search queries. When the processor receives a query, it is added to the service queue for batching. The queue dispatches a batch of queries to the engine whenever the batch is full (size configurable) or the maximum wait time is reached (typically 50 to 100 ms; also configurable). When a set of subqueries is generated by a RAG system and sent to the endpoint individually through asynchronous HTTP requests, they are usually batched on the server side to allow them to be processed together by the Engine. The end user can also simultaneously process multiple top-level queries in a RAG pipeline and use the batching capability of the retrieval server. This exploits the asynchronous nature of HTTP requests. With the native support of asynchronous operations in Python, firing multiple retrieval and LLM requests to an external server without blocking the program from advancing to other operations until the results are actually needed is a key ingredient to accelerate the speed of RAG toolkits such as LangGraph,12 AutoGen,13 DSPy [19], and GPT Researcher [12]. While batching adds some overhead in gathering queries, it prevents queries from being processed sequentially. This results in greater throughput when han- dling multiple queries. This is generally not handled by offline IR toolkits such as PyTerrier [32] and Anserini [50], since online serving is not the primary use case