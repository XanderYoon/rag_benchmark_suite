RoutIR: Fast Serving of Retrieval Pipelines for Retrieval-Augmented Generation Eugene Yang1 , Andrew Yates1 , Dawn Lawrie1 , James Mayfield1 , and Trevor Adriaanse2 1 Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore, MD 21211, USA {eugene.yang,andrew.yates,lawrie,mayfield}@jhu.edu 2 Johns Hopkins University, Baltimore, MD 21211, USA tadriaa1@jhu.edu Abstract.Retrieval models are key components of Retrieval-Augmented Generation (RAG) systems, which generate search queries, process the documents returned, and generate a response. RAG systems are often dynamic and may involve multiple rounds of retrieval. While many state- of-the-art retrieval methods are available through academic IR platforms, these platforms are typically designed for the Cranfield paradigm in which all queries are known up front and can be batch processed of- fline. This simplification accelerates research but leaves state-of-the-art retrieval models unable to support downstream applications that require online services, such as arbitrary dynamic RAG pipelines that involve looping, feedback, or even self-organizing agents. In this work, we in- troduceRoutIR, a Python package that provides a simple and efficient HTTP API that wraps arbitrary retrieval methods, including first stage retrieval, reranking, query expansion, and result fusion. By providing a minimal JSON configuration file specifying the retrieval models to serve, RoutIRcan be used to construct and query retrieval pipelines on-the- fly using any permutation of available models (e.g., fusing the results of several first-stage retrieval methods followed by reranking). The API automatically performs asynchronous query batching and caches results by default. While many state-of-the-art retrieval methods are already supported by the package,RoutIRis also easily expandable by imple- menting theEngineabstract class. The package is open-sourced and publicly available on GitHub:http://github.com/hltcoe/routir. Keywords:search service路retrieval-augmented generation路asynchronous query batching路multi-stage retrieval路online evaluation 1 Introduction Information retrieval research typically requires system comparison using a fixed set of queries and documents, following the Cranfield paradigm [7]. Such exper- iments can be conducted through