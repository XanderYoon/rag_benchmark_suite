example, a standalone Lucene instance searching a batch of queries may block the Python process from accepting an API request. We provide some implementation guidance on how these concurrency issues can be overcome in the documentation. However, such engineering issues are model 14 Rank1 [44], a pointwise reasoning reranker, integration script can be found inhttps: //github.com/hltcoe/routir/blob/main/examples/rank1_extension.py. RoutIR: Fast Serving of Retrieval Pipelines for RAG 11 T able 2.Effetiveness and efficiency on the NeuCLIR 2023 MLIR Task. Effectiveness Batched Throughput Seq. Latency Model nDCG@20 (query/sec)↑(sec/query)↓ Multi-Vector: PLAID-X [48] 0.402 7.05 0.24 LSR: MILCO [34] 0.413 3.27 2.46 Dense: Qwen3 Embedding [53] 0.430 9.60 1.23 and toolkit-dependent. They can generally be solved by hosting different models in separateRoutIRinstances and combining them viaserver importsto a joint endpoint. 5 Experiment and Analysis To demonstrate the adaptability ofRoutIR, we report effectiveness and effi- ciency using the TREC 2023 NeuCLIR MLIR task [22], which has 76 queries and about 10 million web documents in Chinese, Persian, and Russian extracted from CommonCrawl News. We experiment with the following three multilingual models with distinct architectures and stacks: –Multi-vector dense using PLAID-X [48]. The PLAID-X model was reported as the state of the art in 2023 during TREC. The model is based on XLM- RoBERTa-Large [28] and is served using an NVIDIA TITAN RTX 24G with the PLAID-X implementation. –Learned-sparse retrieval using MILCO [34] with Anserini [50]. The MILCO model is also based on XLM-RoBERTa-Large and is served with the same GPU using the Huggingface Transformer. The index is served with Anserini via PyJNIus. –Dense retrieval using Qwen3 Embedding [53] with a FAISS index [10] using vLLM.15 The Qwen3 Embedding model is served with vLLM with parame- ters cast to FP16 to fit on the TITAN RTX GPU. The document embeddings are indexed with FAISS using product quantization of