such as LangGraph,12 AutoGen,13 DSPy [19], and GPT Researcher [12]. While batching adds some overhead in gathering queries, it prevents queries from being processed sequentially. This results in greater throughput when han- dling multiple queries. This is generally not handled by offline IR toolkits such as PyTerrier [32] and Anserini [50], since online serving is not the primary use case for those tools.RoutIRprovides the essential wrappers to serve retrieval mod- 11 https://github.com/BerriAI/litellm 12 https://www.langchain.com/langgraph 13 https://microsoft.github.io/autogen RoutIR: Fast Serving of Retrieval Pipelines for RAG 7 1{ 2" pipeline " : " { qwen3 - neuclir , plaidx - neuclir } RRF % 3" c o l l e c t i o n " : " neuclir " , 4" query " : " where is Taiwan " 5} Fig. 2.Example Pipeline Request. The pipeline issues the query toqwen3-neuclir andplaidx-neuclirengines, fuses the results with reciprocal rank fusion, takes the top 50 documents from the fused result, and finally reranks using Rank1 [44] reranker. T able 1.Operators for pipeline construction string. Please refer tohttps:// github.com/hltcoe/routir/blob/main/src/routir/pipeline/parser.py#L7for the full context-free grammar. Operator Operation Description e1 >> e2 Pipe Pass the retrieval results ofe1to a downstream enginee2, such as a reranker. e1%k Limit Only retain the topkretrieved documents frome1 {e1, e2 Parallel Pipelines Pass the upstream results or query (if at the begin- ning of a pipeline) to a list of parallel pipelines (e1 ande2). xx{e1,e2 Query Generation Generate multiple sub-queries with methodxxand issue them to all parallel pipelines. }xx Result Fusion Fuse retrieval results from parallel pipelines with methodxx. els, including those supported by PyTerrier and Anserini, to efficiently embed them in a RAG system pipeline. Furthermore, processors also cache retrieval results to prevent duplicate re- quests to the Engine instance.RoutIRsupports both in-memory and Redis caches, which provides flexibility to support different cache