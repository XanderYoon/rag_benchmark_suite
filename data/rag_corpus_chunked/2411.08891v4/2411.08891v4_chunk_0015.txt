the 6 CT CT-Probe Linguistic Number CalibRAG CalibRAG-multi 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (a) BM25 w/ NQ 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (b) BM25 w/ WebQA 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (c) Contriever w/ NQ 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (d) Contriever w/ WebQA Figure 3:Evaluation results of the baselines and CalibRAG using two retriever models: BM25 and Con- triever on NQ and WebQA.We report four metrics—1-AUROC, 1-ACC, ECE, and BS—wherelower values indicate better performance. decoding temperaturetselected in Stage 2. This decision is compared with the correct answery ∗ byGto determine its accuracy. 4 Experiments Implementation detail.For all experiments, following Sec. 3.3, we collect a total of 20,870 sam- ples for training and 4,125 for validation. All evaluations are conducted in azero-shotsetting on held-out tasks that are disjoint from both the training and validation sets. Unless otherwise speci- fied, we useLlama-3.1-8B[3] as both the RAG modelMand decision modelU. However, we also conduct ablation studies diverse model structures, and the results are presented in Sec. D. To evaluate long-form generated answers, we employGPT-4o-minias the evaluation modelG. Baselines.We compare CalibRAG with the following relevant baselines. •Uncertainty calibration baselines:(1)Calibration Tuning[21] methods employ a model that outputs probabilities for answering “Yes” or “No” to the question, “Is the proposed answer true?” These probabilities allow us to measure the uncertainty calibration of the response. As baselines, we consider two variants:CT-probe, which adds a classifier head to estimate the probability of correctness, andCT-LoRA, which utilizes the normalized token probability between the tokens “Yes” and “No.” (2)Verbalized Confidence Fine-tuning[40, 41, 6] utilizes verbalized tokens to represent the model’s confidence. In this case, we also consider two baseline variants:Number- LoRA,