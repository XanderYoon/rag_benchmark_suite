[53]. These approaches are highly effective in improving retrieval relevance and downstream QA perfor- mance. However, they are fundamentally designed to rank documents by relevance, not to assess how the retrieved information influences the correctness of the final user decision based on the LLM- generated answer. Thus, the resulting scores, although often normalized between 0 and 1, are not calibrated probabilities of correctness and cannot be directly used for decision calibration. Similarly, Self-RAG [43] introduces the notion of utility scores for retrieved documents to identify potentially helpful content. While this provides a signal for filtering noisy documents, the utility score reflects plausibility rather than empirical correctness. As such, these scores are neither opti- mized for nor aligned with standard calibration metrics such as ECE, NLL, or Brier Score. In contrast, our approach directly addresses this gap by training a forecasting function to output calibrated confidence scores that reflect the actual correctness of decisions made by a surrogate user model. We explicitly supervise the forecasting function using binary labels that indicate whether the model’s final prediction is correct, and optimize this function using strictly proper scoring rules. This ensures that the predicted confidence scores match the empirical likelihood of correctness, thus enabling true decision calibration rather than merely relevance estimation. This fundamental difference in supervisionsignal(relevance vs. correctness) andobjective(ranking vs. calibration) delineates the core novelty of our work from prior reranking-based approaches. By aligning the model’s confidence estimates with empirical decision accuracy, our method offers a principled and interpretable framework for improving trustworthiness in RAG systems. B Experimental details Our implementation builds on key libraries such as PyTorch 2.1.2 [54], Hugging Face Trans- formers 4.45.1 [55], and PEFT 0.7.1, 3 providing a robust foundation for experimentation. We employ theLlama-3.1-8B-Instructmodel, an open-source multilingual LLM available on Hugging Face. 4 Our experiments are conducted