produced by LLMs may not consistently be reliable for important human decision-making processes, potentially overlooking key or hidden details. Additionally, LLMs frequently provide inaccurate or misleading information with a high degree of confidence, a phenomenon referred to ashallucination[8, 9], which can lead humans to make flawed decisions. In addition, Zhou et al. [7] has empirically demonstrated that human users often over-rely on LLM outputs during decision-making processes, and this over-reliance tends to increase in proportion to the model’s confidence. Here, the model’s confidence refers to the expression of how certain the model is when asked how confident it is in its answer. Specifically, †Co-corresponding authors 39th Conference on Neural Information Processing Systems (NeurIPS 2025). arXiv:2411.08891v4 [cs.IR] 15 Oct 2025 they have found that for answers with high confidence, users show strong over-reliance regardless of whether the answer is correct or not. These findings highlight that utilizing LLMs without proper calibration of their responses and addressing the frequent occurrence of hallucinations can lead to incorrect decisions in high-stakes tasks such as medical diagnosis and legal reasoning, potentially resulting in severe consequences [10, 11, 12]. Retrieval Augmented Generation (RAG) [13, 14, 15] has emerged as a promising method to address hallucinations, which is one of the two key issues when using LLMs in decision-making [16, 17]. Instead of generating answers directly, RAG retrieves relevant documents from external databases and uses them as an additional context for response generation. This approach supplements the in- formation that LLMs lack, resulting in more accurate and reliable responses. However, the database cannot encompass all information, and the knowledge from world is continuously being updated. In such cases, the retriever may retrieve irrelevant documents, which can distract the LLM and lead to the generation of incorrect answers to the question [18]. Moreover, as described in Sec. 2.2,