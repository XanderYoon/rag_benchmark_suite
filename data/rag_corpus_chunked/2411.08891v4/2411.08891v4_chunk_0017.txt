documents at inference. The forecasting function is evaluated by marginalizing over a set of six decoding temperatures t∈ {1.0,1.1, . . . ,1.5}. This is because, in the absence of explicit information about a user’s prefer- ence, the forecasting function cannot accurately model the user’s behavior under a single decoding temperature. To reflect this variation, we approximate marginalization by averaging over these six values, as exact integration over all possibletis infeasible. Building on this, CalibRAG-multi ex- tends CalibRAG to a multi-class setting by modeling the correctness histogram across bins (0-10). Evaluation metrics.We evaluate all the models in terms of accuracy, AUROC, and various cali- bration metrics such as Expected Calibration Error [ECE; 44], Brier Score [BS; 45]. For clarity and 7 CT CT-Probe Linguistic Number CalibRAG 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (a) MedCPT w/ BioASQ-Y/N 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (b) MedCPT w/ MMLU-Med 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (c) MedCPT w/ PubMedQA Figure 4:Evaluation results of the baselines and CalibRAG utilizing MedCPT on BioASQ-Y/N, MMLU- Med, and PubMedQA.We report four metrics—1-AUROC, 1-ACC, ECE, and Brier Score—wherelower values indicate better performance. Table 1: Comparison of RAG variants across datasets. (a) Comparison of reranking RAG methods. Baseline reranking scores are treated as confidence. Data Method AUROC(↑)ACC(↑)ECE(↓)BS(↓) HotpotQACross-encoder 60.74 34.98 0.477 0.477LLM-rerank 60.57 38.52 0.248 0.297 CalibRAG72.47 42.37 0.106 0.206 (b) Comparison of robust RAG methods using Llama-2-7B asM. Data Method AUROC(↑)ACC(↑)ECE(↓)BS(↓) NQ SelfRAG 48.4 36.2 0.522 0.545CalibRAG63.5 37.4 0.258 0.287 WebQASelfRAG 51.9 39.7 0.478 0.503CalibRAG68.8 40.5 0.217 0.262 consistency, we adopt the 1-AUROC and 1-ACC notation in plots so that all metrics can be inter- preted under the convention that lower values indicate better performance. Details regarding these metrics can