our method offers a principled and interpretable framework for improving trustworthiness in RAG systems. B Experimental details Our implementation builds on key libraries such as PyTorch 2.1.2 [54], Hugging Face Trans- formers 4.45.1 [55], and PEFT 0.7.1, 3 providing a robust foundation for experimentation. We employ theLlama-3.1-8B-Instructmodel, an open-source multilingual LLM available on Hugging Face. 4 Our experiments are conducted on NVIDIA RTX 3090 and RTX A6000 GPUs. Additionally, we utilize the officialfacebookresearch-contrieverrepository 5 and theelastic-research-bm25repository 6 for our retrieval model. We also useMedMCT 3https://github.com/huggingface/peft 4https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct 5https://github.com/facebookresearch/contriever 6https://www.elastic.co/ 22 based on the MedRAG framework. 7 For training calibration tuning baselines, we reference the calibration-tuningrepository. 8 B.1 Datatsets Train DatasetsSQuAD [56, 38] is a reading comprehension dataset sourced from Wikipedia, containing questions answered by text spans from the articles. WikiQA [39] is a question-sentence pair dataset from Wikipedia, designed for open-domain question answering and includes unanswer- able questions for research on answer triggering. TriviaQA [37] is a reading comprehension dataset with questions authored by trivia enthusiasts, paired with evidence documents from Wikipedia and other web sources. We randomly sampled 10,000 data points each from TriviaQA and SQuAD2.0, and collected all 873 training samples from WikiQA. In addition, we incorporated non-overlapping samples from SQuAD1.0, resulting in a combined training dataset of 61,886 examples after dedu- plication. For the validation set, we gathered 2,000 samples each from TriviaQA and SQuAD, along with 126 samples from WikiQA, and added non-overlapping samples from SQuAD1.0, yielding a total of 12,643 validation data points. All null values were removed prior to finalization. We downloaded all these datasets in Hugging Face datasets 9. For the construction of the labeled datasetSused to train the forecasting function of CalibRAG, we sample a temperaturetâˆ¼Uniform[1.0,2.0]for each queryqand retrieved documentd. For each triplet(t, q, d), we perform user decoding 10 times and assign