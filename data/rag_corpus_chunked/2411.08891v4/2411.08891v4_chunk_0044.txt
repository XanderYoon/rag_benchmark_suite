in this research does not involve LLMs as any important, original, or non-standard components. • Please refer to our LLM policy (https://neurips.cc/Conferences/ 2025/LLM) for what should or should not be described. 21 A Related Works A.1 Uncertainty Calibration in Language Models Traditional calibration methods rely on token-level log probabilities [25], but modern LLMs gen- erate text autoregressively by multiplying conditional probabilities [4]. Estimating semantic-level probabilities would require marginalization over all possible sequences, which is computationally intractable. As a result, token-level probabilities often fail to provide reliable confidence estimates for long-form text generation. Prompt-based approaches aim to address this problem by eliciting verbalized confidence scores [40, 52]. For example, a model can be prompted with:“Express your confidence as a number between 0 and 100. ”If it responds with“90”, this value is interpreted as its confidence level. However, LLMs often exhibit overconfidence in zero-shot settings, resulting in poorly calibrated outputs [9]. Although RAG can mitigate this issue, when the retrieved context is unreliable, LLM may still demonstrate overconfidence, leading to misleading conclusions. Addressing this challenge remains essential for improving LLM reliability in complex decision-making tasks. A.2 Methods for Enhancing RAG Robustness Recent advancements in reranking for RAG have largely focused on enhancing the relevance of retrieved documents with respect to the input query. For example, LLM-based rerankers leverage semantic representations to reorder documents based on their relevance [42], while cross-encoder- based rerankers jointly encode query-document pairs to model their interaction more precisely [53]. These approaches are highly effective in improving retrieval relevance and downstream QA perfor- mance. However, they are fundamentally designed to rank documents by relevance, not to assess how the retrieved information influences the correctness of the final user decision based on the LLM- generated answer. Thus, the resulting scores, although often normalized between 0 and 1, are not calibrated