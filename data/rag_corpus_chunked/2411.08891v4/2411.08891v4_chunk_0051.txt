information, and thereby improving accuracy. However, this increase can potentially lead to higher calibration errors. Specifically, while better retrieval enhanced predic- tion accuracy, the confidence scores for these predictions only increased marginally. This mismatch between improved accuracy and relatively low confidence resulted in underconfident predictions, which contributed to a slight increase in calibration error. To assess the impact of differentϵvalues on model performance, we conducted experiments on the BioASQ dataset. Based on these observations, we selectedϵ= 0.5as a reasonable compromise to balance accuracy improvements with calibration reliability. D.2 Evaluation on BEIR Benchmark To provide a more comprehensive evaluation, we conducted experiments using two datasets from the BEIR benchmark: SciFact and TREC-COVID. These evaluations aim to validate the effective- ness of CalibRAG beyond its primary focus on well-calibrated decision-making, which predicts the probability of a correct decision when a user relies on the generated guidance to solve a given problem. While CalibRAG is not specifically designed as a reranking method to optimize retrieval performance, it inherently supports both calibration and retrieval. 25 Table 5: Evaluation results on TREC-COVID and SciFact datasets, a subset of the BEIR benchmark. The evaluation metric is Normalized Discounted Cumulative Gain (NDCG@K). Model Dataset NDCG@5 NDCG@10 Cross-Encoder TREC-COVID 0.7655 0.7576 SciFact 0.6668 0.6914 CalibRAG TREC-COVID 0.7863 0.7660 SciFact 0.6872 0.7114 Table 6: Results of Verbalized Confidence Fine-Tune Evaluation on the MMLU Dataset using Llama-3.1-8B-Instruct. Evaluation metrics are ACC and ECE. Case ACC ECE Continuous-Number 43.63 0.3190 Discrete-Number 44.96 0.1605 Linguistic 45.03 0.1585 For the experiments, we followed the standard retrieval pipeline, retrieving documents using BM25 and reranking the top-100 results. We compared CalibRAG with the Cross-Encoder baseline, and the results, presented in Table 5, demonstrate that CalibRAG consistently outperforms the Cross- Encoder. These findings validate that CalibRAG not only enables well-calibrated decision-making but also enhances retrieval performance,