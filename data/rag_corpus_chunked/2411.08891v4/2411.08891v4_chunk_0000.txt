Reliable Decision-Making via Calibration-Oriented Retrieval-Augmented Generation Chaeyun Jang KAIST jcy9911@kaist.ac.kr Deukhwan Cho KAIST macro_boomin@kaist.ac.kr Seanie Lee KAIST lsnfamily02@kaist.ac.kr Hyungi Lee† Kookmin University lhk2708@kookmin.ac.kr Juho Lee† KAIST juholee@kaist.ac.kr Abstract Recently, Large Language Models (LLMs) have been increasingly used to support various decision-making tasks, assisting humans in making informed decisions. However, when LLMs confidently provide incorrect information, it can lead hu- mans to make suboptimal decisions. To prevent LLMs from generating incorrect information on topics they are unsure of and to improve the accuracy of gener- ated content, prior works have proposed Retrieval Augmented Generation (RAG), where external documents are referenced to generate responses. However, previ- ous RAG methods focus only on retrieving documents most relevant to the input query, without specifically aiming to ensure that the human user’s decisions are well-calibrated. To address this limitation, we propose a novel retrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG), which ensures that decisions informed by RAG are well-calibrated. Then we empirically validate that CalibRAG improves calibration performance as well as accuracy, compared to other baselines across various datasets. 1 Introduction Large language models [LLMs; 1, 2, 3, 4] have demonstrated remarkable performance on numerous downstream natural language processing (NLP) tasks, leading to their widespread integration into various decision-making processes [5, 6, 7]. However, even with significant increases in model size and the expansion of training datasets, it remains infeasible for LLMs to encode all possible knowledge within their parameters. As a result, the outputs produced by LLMs may not consistently be reliable for important human decision-making processes, potentially overlooking key or hidden details. Additionally, LLMs frequently provide inaccurate or misleading information with a high degree of confidence, a phenomenon referred to ashallucination[8, 9], which can lead humans to make flawed decisions. In addition, Zhou et al. [7] has empirically demonstrated that human users often