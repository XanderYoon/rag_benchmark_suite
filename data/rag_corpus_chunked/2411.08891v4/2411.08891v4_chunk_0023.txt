Evaluation(IITP) grant funded by the Korea government(MSIT) (No.RS- 2025-02219317, AI Star Fellowship(Kookmin University)). This work was supported by Artifi- cial intelligence industrial convergence cluster development project funded by the Ministry of Sci- ence and ICT(MSIT, Korea) & Gwangju Metropolitan City. This research was supported by the MSIT(Ministry of Science, ICT), Korea, under the Global Research Support Program in the Digital Field program(IITP-2024-RS-2024-00417958) supervised by the IITP(Institute for Information & Communications Technology Planning & Evaluation). 10 References [1] Albert Q Jiang et al. “Mistral 7B”. In:arXiv preprint arXiv:2310.06825(2023) (cit. on p. 1). [2] Hugo Touvron et al. “Llama 2: Open foundation and fine-tuned chat models”. In:arXiv preprint arXiv:2307.09288(2023) (cit. on p. 1). [3] Abhimanyu Dubey et al. “The llama 3 herd of models”. In:arXiv preprint arXiv:2407.21783 (2024) (cit. on pp. 1, 7). [4] Josh Achiam et al. “GPT-4 technical report”. In:arXiv preprint arXiv:2303.08774(2023) (cit. on pp. 1, 22). [5] Rishi Bommasani et al. “On the opportunities and risks of foundation models”. In:arXiv preprint arXiv:2108.07258(2021) (cit. on p. 1). [6] Neil Band et al. “Linguistic Calibration of Long-Form Generations”. In:Forty-first Interna- tional Conference on Machine Learning. 2024 (cit. on pp. 1–5, 7, 34). [7] Kaitlyn Zhou et al. “Relying on the Unreliable: The Impact of Language Models’ Reluctance to Express Uncertainty”. In:arXiv preprint arXiv:2401.06730(2024) (cit. on p. 1). [8] Terry Yue Zhuo et al. “Exploring ai ethics of chatgpt: A diagnostic analysis”. In:arXiv preprint arXiv:2301.1286710.4 (2023) (cit. on p. 1). [9] Theodore Papamarkou et al. “Position paper: Bayesian deep learning in the age of large-scale ai”. In:arXiv preprint arXiv:2402.00809(2024) (cit. on pp. 1, 22). [10] Margaret Li et al. “Don’t say that! making inconsistent dialogue unlikely with unlikelihood training”. In:arXiv preprint arXiv:1911.03860(2019) (cit. on p. 2). [11] Wei Li et al. “Faithfulness in natural language generation: A systematic survey of analysis, evaluation