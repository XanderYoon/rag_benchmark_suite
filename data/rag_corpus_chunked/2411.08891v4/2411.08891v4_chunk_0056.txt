of zero-shot evaluation of calibration baselines onNQandWebQAdatasets using Contriever (dense) retrieval. Results are averaged over three random seeds. Methods NQ WebQA AUROC ACC ECE BS AUROC ACC ECE BS CT-LoRA69.89±4.9439.93±1.260.2800±0.05850.3008±0.043569.81±6.8237.83±1.250.2646±0.05100.2860±0.0394 CT-probe63.84±6.1437.92±2.800.3225±0.06340.3343±0.049862.65±8.1036.43±4.030.3072±0.06700.3180±0.0565 Linguistic-LoRA57.05±3.9141.50±0.370.4368±0.02670.4252±0.029056.30±2.7039.76±0.770.4657±0.01240.4477±0.0162 Number-LoRA71.16±0.6135.99±0.540.1827±0.01240.2214±0.001673.47±1.0135.61±0.120.1754±0.01240.2141±0.0040 CalibRAG73.89±1.5046.55 ±2.450.0312±0.00730.2074±0.006276.24±0.3744.19±2.600.0970±0.01220.2095±0.0062 CalibRAG-multi72.73±0.0849.42±0.070.1656±0.00190.2375±0.001372.95±0.0846.78±0.020.1901±0.00120.2488±0.0009 indicate that the learned calibration itself, without requiring reranking, still provides significant ben- efit, demonstrating the robustness of CalibRAG’s alignment mechanism. D.7 Using Uncertainty baseline confidence scores for reranking In this section, we investigate the effectiveness of using uncertainty baseline confidence scores for reranking in the RAG pipeline. As described in the main paper, these confidence scores are derived from verbalized scalar predictions generated by the LLM, typically representing values from 0 to 100. While such scalar confidence values can be used to rerank retrieved documents, this approach in- curs significant computational overhead. Specifically, the Number baseline requires generating full guidancezfor every(q, d)pair before estimating confidence, as the model conditions on both the query and document to generate scalar outputs. In contrast, CalibRAG directly estimates confidence from the(q, d)pair using a lightweight forecasting functionf(q, d), thus avoiding this expensive intermediate generation. Despite this additional cost, we performed an ablation to compare the reranking performance of Number-based confidence scores versus CalibRAG. As shown in Table 9, CalibRAG consistently outperforms the baseline across both BM25 and Contriever retrievers on the WebQA dataset. These results demonstrate that CalibRAG not only provides better-calibrated decisions but does so more efficiently without requiring guidance generation for every document candidate. This high- lights the dual advantage of CalibRAG in both performance and computational cost. 28 Table 12: Comparison of zero-shot evaluation of calibration baselines onBioASQ-Y/N,MMLU-Med, and PubMedQAdatasets. Results are averaged over three random seeds. Methods Dataset AUROC ACC ECE BS CT-LoRABioASQ-Y/N 65.20±2.3254.31±0.730.5167±0.0120.5099±0.0146 MMLU-Med 66.94±0.6847.20±1.520.4293±0.00880.4262±0.0084 PubMedQA 56.67±3.1643.80±0.910.4307±0.00990.4300±0.0094 CT-probeBioASQ-Y/N 59.73±4.2757.98±1.450.5664±0.0090.5630±0.0094 MMLU-Med 55.39±3.2449.49±5.000.4771±0.03840.4758±0.0375 PubMedQA 54.56±0.6146.60±1.880.4506±0.0120.4510±0.0121 Linguistic-LoRABioASQ-Y/N 48.24±2.2657.82±0.500.3193±0.00270.3464±0.0030 MMLU-Med 51.30±0.9355.43±0.940.3262±0.00780.3544±0.0049 PubMedQA 49.13±0.7947.13±2.250.4047±0.03360.4225±0.021 Number-LoRABioASQ-Y/N 52.43±2.1953.72±1.690.4664±0.03550.4659±0.0332 MMLU-Med