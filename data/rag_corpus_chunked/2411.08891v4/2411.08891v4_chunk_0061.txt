the user modelUin making decisions based on the LLM-generated guidancez and confidencec. Prompt that generates open-ended queryqfrom the decision taskx You are an automated assistant tasked with rephrasing specific questions into open-ended queries to encourage detailed exploration and discussion of the key topics mentioned. Your goal is to prompt someone to write a paragraph exploring the topic without directly revealing the answer. Examples for Guidance: Example 1: Question 1: Which sea creature is the world’s largest invertebrate? Question 2: Write a paragraph about the world’s largest invertebrate. ... Now, please rephrase the following question: Question 1: {question} Question 2: (b) This prompt was first suggested by Band et al. [6], and we have modified part of the proposed prompt for our use here. We use this prompt as an input when generating the queryqbased on the decision taskx. Guidancezgeneration prompt Directly state the answer without phrases like ’the correct answer is. Given the retrieved context, answer the question as accurately as possible. Question: {question} Retrieved Context: {title} - {context} Answer: (c) This prompt guides the LLMMto provide direct, concise guidancezbased on a given retrieved document d. Figure 14: Prompt used for (a) user model making decisions, (b) generatingqfromx, and (c) generatingz. 34 Evaluation prompt The problem is: {question} The correct answer for this problem is: {ground-truth} A student submitted the answer: {prediction} The student’s answer must be correct and specific but not overcomplete (for example, if they provide two different answers, they did not get the question right). However, small differences in formatting should not be penalized (for example, ’New York City’ is equivalent to ’NYC’). Did the student provide an equivalent answer to the ground truth? Please answer yes or no without any explanation: (a) Prompt used to evaluate the long-form generated answer. Query regeneration prompt. You are a