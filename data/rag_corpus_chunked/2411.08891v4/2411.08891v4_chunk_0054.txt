and WebQA (b, d). Here, we utilizePhi-4[58] as our user modelU. We report four metrics—1-AUROC, 1-ACC, ECE, and Brier Score—where lower values indicate better performance. CT CT-Probe Linguistic Number CalibRAG 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Deepseek: BM25 with NQ (a) 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Deepseek: BM25 with WebQA (b) 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Deepseek: Contriever with NQ (c) 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Deepseek: Contriever with WebQA (d) Figure 7: Evaluation results of the baselines and CalibRAG utilizing two retriever models: BM25 (a, b) and Contriever (c, d) on NQ (a, c) and WebQA (b, d). Here, we utilizeDeepSeek-Distill[59] as our user modelU. We report four metrics—1-AUROC, 1-ACC, ECE, and Brier Score—where lower values indicate better performance. Table 7: Comparison of fine-tuned RAG reranking methods using our synthetic training data on HotpotQA. Methods AUROC(↑)ACC(↑)ECE(↓)BS(↓) Cross-encoder 60.74 34.98 0.477 0.477 Cross-encoder (Fine-tuned) 61.55 32.540.0082.555 CalibRAG 72.47 42.370.1060.206 Table 8: Evaluation metrics of CalibRAG without reranking on WebQA using BM25 Methods AUROC(↑)ACC(↑)ECE(↓)BS(↓) Number 69.38 ±2.8436.04 ±0.500.1931 ±0.01310.2293 ±0.0102 CalibRAG w/o Rerank 75.73±0.0041.99±0.030.0780±0.03120.1981±0.0025 D.5 Ablation on Fine-Tuning for Reranking Baselines To ensure a fair comparison between CalibRAG and the reranking baseline, we also evaluated a fine-tuned reranker model which fine-tuned using our synthetic datasets. However, as discussed in § 3.2, training was challenging due to the difficulty in feature extraction without using an embedding model to generate the guidance variablez. And this difficulty let fine-tuned model underfit