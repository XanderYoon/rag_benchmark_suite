documents that are highly relevant to the decision problemx, allowing us to confirm that the guidance generated from these retrieved doc- uments is well-predicted to be helpful for decision-making. Additionally, we can confirm that when the document with the highest rank does not aid in decision-making forx, CalibRAG successfully assigns a lower confidence level, helping to prevent the user from over-relying on the guidance. Figure 10:CalibRAG vs Linguistic-LoRA.In the case of CalibRAG, a document about the person in question is retrieved with a confidence level of 83.93%. In contrast, the document retrieved by the base retrieval model is related to Donald Trump but does not match the specific person in the query. Nevertheless, the Linguistic- LoRA model trust the document confidently. Figure 11:CalibRAG vs Number-LoRA.In the case of CalibRAG, an accurate document about the location following North Africa was retrieved, allowing the user model to make a correct decision. In contrast, the base retrieval model brought a different document. Nevertheless, Number-LoRA model set this context with a confidence level of 6 out of 10, leading the user to ultimately make an incorrect decision. 32 Figure 12:CalibRAG vs Number-LoRA.The base retrieval model focused solely on the word ’impeached’ and retrieved a related document, missing the context of ’first.’ Despite this, the Number-LoRA model set a confidence level of 9 out of 10, causing the user to make an incorrect decision. In contrast, CalibRAG retrieved an accurate document that, while not explicitly containing ’first impeached,’ included the phrase ’first being.’ It set a confidence level of 92.32%, allowing the user to arrive at the correct answer. Figure 13:CalibRAG vs CT-LoRA.In the case of CalibRAG, the top-20 confidence score is 20.95 for incorrect information, causing the user to hesitate in making a decision. However, with the CT-LoRA model, incorrect information is assigned a