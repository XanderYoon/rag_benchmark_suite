in Sec. D. For domain specific evaluation, we assess CalibRAG on the BioASQ-Y/N, MMLU-Med, and Pub- MedQA datasets from the MIRAGE benchmark [49], which focus on the medical domain. Since domain-specific retrievers are necessary in this field, we utilize MedCPT [50], a retriever trained on user click logs on PubMed corpus. Note that the rest of CalibRAG model, including the LLMM and the forecasting function, which have been trained on TriviaQA, SQuAD2.0, and WikiQA, re- mains fixed. Thus, this setup evaluates robustness of CalibRAG when applied to an unseen retriever and out-of-domain datasets. The results in Fig. 3 clearly demonstrate that CalibRAG outperform other baselines across all metrics, even in specialized domain scenarios. Similar to the general do- main, CalibRAG significantly improves calibration metrics across all datasets, showing its ability to effectively calibrate the decision-making process even with an unseen retriever and dataset. Comparison with reranking and robust RAG baselines.CalibRAG retrieves the topKdoc- uments during inference and selects the one most likely to lead to a correct decision, using it to generate guidance and confidence for a well-calibrated decision-making process. To further evalu- ate its effectiveness, we compare CalibRAG against reranking and robust RAG baselines in Table 1a and Table 1b. 8 1.0 1.1 1.2 1.3 1.4 1.5 User model decoding temperature (t) 0.05 0.10 0.15 0.20 0.25 0.30ECE f(t,q,d) f(q,d) (a) Ablation oftconditioning 5 10 20 40 Number of Retrievals (K) 36 38 40 42 44ACC (%) CalibRAG Number + Reranking (b) Number of retrievals BM25 NQ BM25 WebQA Contriever NQ Contriever WebQA 40 42 44 46 48 50ACC (%) Calibrag Calibrag (c) Effect of query reformulation Figure 5:(a)Calibration with and without temperature conditioning on the NQ dataset using Contriever.(b) Effect of the number of retrieved documents on reranking performance on the WebQA dataset using BM25.(c) Impact of query