for Reranking Baselines To ensure a fair comparison between CalibRAG and the reranking baseline, we also evaluated a fine-tuned reranker model which fine-tuned using our synthetic datasets. However, as discussed in § 3.2, training was challenging due to the difficulty in feature extraction without using an embedding model to generate the guidance variablez. And this difficulty let fine-tuned model underfit to the training dataset. As shown in Table 7, the reranker model underperforms compared to the zero-shot setting. Therefore, in theComparison with reranking and robust RAG baselinesexperiments in Sec. 4.1, we evaluated the CalibRAG model alongside zero-shot reranker models. D.6 Ablation on CalibRAG without reranking To isolate the effect of reranking in our confidence calibration framework, we evaluated CalibRAG without using any reranking, where the model directly uses retrieved contexts without reordering them based on predicted confidence. As shown in Table 8, even without reranking, CalibRAG sub- stantially outperforms the Number baseline in both accuracy and calibration metrics. These results 27 Table 9: Evaluation metrics of Number + Rerank and CalibRAG on WebQA Retriever Methods AUROC(↑)ACC(↑)ECE(↓)BS(↓) BM25 Number + Rerank 75.06 ±0.0042.42 ±0.010.2075 ±0.01670.2397 ±0.0109 CalibRAG 77.29±0.4243.77±0.540.0567±0.03320.1983±0.0045 Contriever Number + Rerank 76.84±0.0043.08 ±0.000.2088 ±0.01270.2390 ±0.0083 CalibRAG 76.24 ±0.3744.19±2.600.0997±0.01220.2095±0.0062 Table 10: Comparison of zero-shot evaluation of calibration baselines onNQandWebQAdatasets using BM25 (lexical) retrieval. Results are averaged over three random seeds. Methods NQ WebQA AUROC ACC ECE BS AUROC ACC ECE BS CT-LoRA73.51±1.6537.70±0.280.2479±0.0240.2709±0.013374.36±1.1738.09±0.280.2487±0.03030.2681±0.0200 CT-probe60.92±0.9437.59±3.030.3490±0.02360.3536±0.022358.52±2.5137.75±4.390.3491±0.03290.3539±0.0332 Linguistic-LoRA57.12±4.3539.42±0.940.4529±0.02230.4362±0.028456.44±1.9340.58±1.180.4536±0.00710.4385±0.0091 Number-LoRA67.48±1.4234.38±0.710.1922±0.01650.2294±0.007669.38±2.8436.04±0.500.1931±0.01310.2293±0.0102 CalibRAG77.29±0.1242.66 ±0.970.0600±0.00390.1983±0.001777.29±0.4243.77±0.540.0567±0.03320.1983±0.0045 CalibRAG-multi76.73±0.2246.16±0.050.1397±0.00220.2138±0.001676.40±0.2845.84±0.250.1372±0.00070.2175±0.0008 Table 11: Comparison of zero-shot evaluation of calibration baselines onNQandWebQAdatasets using Contriever (dense) retrieval. Results are averaged over three random seeds. Methods NQ WebQA AUROC ACC ECE BS AUROC ACC ECE BS CT-LoRA69.89±4.9439.93±1.260.2800±0.05850.3008±0.043569.81±6.8237.83±1.250.2646±0.05100.2860±0.0394 CT-probe63.84±6.1437.92±2.800.3225±0.06340.3343±0.049862.65±8.1036.43±4.030.3072±0.06700.3180±0.0565 Linguistic-LoRA57.05±3.9141.50±0.370.4368±0.02670.4252±0.029056.30±2.7039.76±0.770.4657±0.01240.4477±0.0162 Number-LoRA71.16±0.6135.99±0.540.1827±0.01240.2214±0.001673.47±1.0135.61±0.120.1754±0.01240.2141±0.0040 CalibRAG73.89±1.5046.55 ±2.450.0312±0.00730.2074±0.006276.24±0.3744.19±2.600.0970±0.01220.2095±0.0062 CalibRAG-multi72.73±0.0849.42±0.070.1656±0.00190.2375±0.001372.95±0.0846.78±0.020.1901±0.00120.2488±0.0009 indicate that the learned calibration itself, without requiring reranking, still provides significant ben- efit, demonstrating the robustness of CalibRAG’s alignment mechanism. D.7 Using Uncertainty