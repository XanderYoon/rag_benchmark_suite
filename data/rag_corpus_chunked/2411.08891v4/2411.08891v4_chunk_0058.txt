captures the full context, retrieving documents specifically about the gypsy moth, which uses the sweetgum as a host plant, and correctly assigns a confidence level of 81.41. This demonstrates the capability of CalibRAG to find a relevant docu- ment and assign a confidence level correlated with the accuracy of the downstream surrogate user. Additional examples can be found in § E. E Data Examples Fig. 9 shows the top 20 examples of queries and their corresponding labels. The full set of data examples will be released upon publication of the paper. Fig. 9 shows that the ranking of the retrieved documents is not correlated with the accuracy of the user decision. As seen in this example, the top-ranked document is not helpful for the user model in decision-making, whereas the second- ranked document provides information that can lead the user model to make a correct decision. This illustrates the importance of CalibRAG’s forecasting functionfin effectively modeling the probability that a decision made using documentdis correct, emphasizing the need for reranking documents based on this modeling. 30 Figure 9: Top-20 retrieved document examples. 31 F Qualitative Examples Here, we present additional qualitative examples for comparison with other baselines. In Fig. 10, Fig. 11, Fig. 12, and Fig. 13, the examples demonstrate that while the baselines retrieve documents that provide incorrect answers to the queries, they still assign high confidence to the retrieved docu- ments. In contrast, CalibRAG effectively reranks and retrieves documents that are highly relevant to the decision problemx, allowing us to confirm that the guidance generated from these retrieved doc- uments is well-predicted to be helpful for decision-making. Additionally, we can confirm that when the document with the highest rank does not aid in decision-making forx, CalibRAG successfully assigns a lower confidence level, helping to prevent the user from