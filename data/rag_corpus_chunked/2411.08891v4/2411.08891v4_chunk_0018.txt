Comparison of robust RAG methods using Llama-2-7B asM. Data Method AUROC(↑)ACC(↑)ECE(↓)BS(↓) NQ SelfRAG 48.4 36.2 0.522 0.545CalibRAG63.5 37.4 0.258 0.287 WebQASelfRAG 51.9 39.7 0.478 0.503CalibRAG68.8 40.5 0.217 0.262 consistency, we adopt the 1-AUROC and 1-ACC notation in plots so that all metrics can be inter- preted under the convention that lower values indicate better performance. Details regarding these metrics can be found in Sec. B.3.1. 4.1 Main Results Comparison with uncertainty calibartion baselines.Fig. 3 and Fig. 4 present a comparison of uncertainty-based baselines in both general and specific domain tasks. For the general domain, we evaluated CalibRAG on the Natural QA (NQ) [46] and WebQA [47] datasets. To demonstrate that our method performs well across different retrievers, we conducted experiments using both Contriever [29] and BM25 [48] retrievers. The results in Fig. 3 clearly show that CalibRAG and CalibRAG-multi outperform other baselines in all metrics. In particu- lar, it significantly reduces calibration metrics across all datasets and retriever settings, indicating that CalibRAG effectively calibrates the decision-making process compared to other baselines. Ad- ditionally, while CalibRAG is not explicitly designed to improve accuracy, it naturally identifies documents that are more likely to lead to correct decisions. As a result, it also enhances accuracy compared to other baselines. We also evaluate CalibRAG without reranking, which still shows im- proved calibration metrics compared to both the baselines and the setting where baseline confidence scores are used for reranking. Detailed results can be found in Sec. D. For domain specific evaluation, we assess CalibRAG on the BioASQ-Y/N, MMLU-Med, and Pub- MedQA datasets from the MIRAGE benchmark [49], which focus on the medical domain. Since domain-specific retrievers are necessary in this field, we utilize MedCPT [50], a retriever trained on user click logs on PubMed corpus. Note that the rest of CalibRAG model, including the