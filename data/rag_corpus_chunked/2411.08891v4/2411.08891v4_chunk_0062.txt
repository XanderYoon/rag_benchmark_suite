answers, they did not get the question right). However, small differences in formatting should not be penalized (for example, ’New York City’ is equivalent to ’NYC’). Did the student provide an equivalent answer to the ground truth? Please answer yes or no without any explanation: (a) Prompt used to evaluate the long-form generated answer. Query regeneration prompt. You are a language model assistant who specializes in improving queries for document search systems. Your task is to highlight and clarify the important parts of a given query to make it more precise and help retrieve relevant documents. Please take the original search query below and rewrite it by emphasizing the important words. Do not add any new information not included in the original query. Original Retrieval Query: {query} Please generate the new retrieval query without any explanation: (b) This prompt assists in rewriting search queries to enhance precision and relevance for document retrieval, emphasizing the crucial elements without adding extraneous information. Figure 15: Prompt used for (a) evaluation and (b) query regeneration. 35 Calibration tuning prompt Is the proposed answer correct? Choices: (i): no (ii): yes Answer: (a) This prompt was first suggested by Kapoor et al. [21]. It poses a straightforward question to verify the correctness of a proposed answer with binary choices for evaluation. We used this prompt when training our baselines. Linguistic calibration prompt Provide the certainty level of answer using the given 11 certainty levels. Give ONLY your certainty level, no other words or explanation. Certainty Levels: Unlikely, Doubtful, Uncertain, Ambiguous, Probable, Likely, Possible, Specified, Confirmed, Certain, Inevitable. For example: Certainty: <ONLY the certainty level that Answer is correct, without any extra commentary whatsoever; just the certainty level!> Certainty: (b) This prompt requires the model to evaluate the certainty of an answer using a predefined set of