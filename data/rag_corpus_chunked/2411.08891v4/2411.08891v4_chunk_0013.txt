variation during sampling. To estimate the reliability of the generated decision, we sampleR= 10fromUat the same temperature and evaluate each using the correctness functionG, which compares the decision with the ground truth answery. The soft labelb∈[0,1]is then computed as the proportion of correct responses among theRsamples. Thus, for each(x, y)pair, we generate multiple training quadruples(t, q, d, b), each corresponding to a differenttsetting and document retrieved. Refer to Sec. E for examples of our synthetic data. 3.4 Inference After finishing the training of the forecasting functionf, we perform inference for a new decision taskx ∗ through the following four stage process: Stage 1: Initial retrieval of documents. Given an open-ended queryq ∗, derived from the orig- inal questionx ∗, we begin the document retrieval process using the retrieval model. Similarly to the training data generation process, we retrieve the topKrelevant documents from the external database, denoted byD ∗ :={d ∗ i }K i=1. The goal of this stage is to construct a diverse set of candidate documents that may contain valuable information for producing the correct answery. Stage 2: Scoring and selection of documents.Once theKcandidate documents are retrieved, we estimate the decision confidence of each document with our trainedt-conditioned forecasting functionf. At inference time, the user may choose $t$ to reflect their decision preference, with lower values for cautious, consistent decisions and higher values for exploratory reasoning. Regardless of the original retrieval similarity score, each document is then reranked by its predicted confidence. Concretely, we sort the documents in descending order of the probabilities{f(t, q ∗, d∗ i )}K i=1, which represent the chance that the user will reach a correct decision if guidancezis generated from each document. The top-ranked document is selected for the next stage. Here, if the predicted probability for the highest-ranked documentd ∗ falls below a