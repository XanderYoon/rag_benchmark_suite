better modeling of real user behavior and allows for more effective confidence calibration, ultimately resulting in a safer and more trustworthy decision-making system. 3.2 Modeling and Training To model the forecasting functionf, it is essential to have the capacity to sufficiently analyze the impact of the generated guidancezon the actual decision. For this reason, we use a pre-trained LLMM, responsible for generatingz, as the base feature extractor modelf feat. For details of the feature-extraction process, please refer to Sec. B.4. To incorporate the temperature parametert, we apply a Fourier positional encoding [33] that maps the scalart∈Rto a2N-dimensional vector as follows: PE(t) =  sin(ω1t),cos(ω 1t), . . . ,sin(ωN t),cos(ω N t)  , ω n = 2n 2π tmax −t min .(2) whereNis the number of frequencies in the encoding. The resultingPE(t)∈R 2N is projected with a learnable matrixW p ∈R h×2N and then added element-wise to the base featuref feat(q, d). Additionally, to model the probability thatU(x, z, f(t, q, d))leads to a correct decision, we place a linear classification head on top of the extractorf feat derived from the frozen LLMM. For parameter-efficient fine-tuning and to avoid abrupt representation shifts, we keepMfrozen and train only the LoRA adapters [LoRA; 34] and the lightweight head. The resulting forecasting function is defined as: f(t, q, d) :=σ W ⊤ head (ffeat(concat[q, d];W LoRA) +W p PE(t)) +b head  ,(3) whereσ(x) = 1/(1 + exp(−x))is the sigmoid function. Here,W head,b head,W p, and the LoRA parametersW LoRA are all learnable. This formulation allowsfto condition its prediction on both the semantic compatibility of theq-dpair and the user-specific behavior encoded byt, providing an uncertainty-aware estimate of decision correctness. Then we trainfby minimizing the following log-likelihood loss: L=− 1 |S| X (t,q,d,b)∈S [blogf(t, q, d) + (1−b) log(1−f(t, q, d))](4) whereSis the