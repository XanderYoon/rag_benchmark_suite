a total of 12,643 validation data points. All null values were removed prior to finalization. We downloaded all these datasets in Hugging Face datasets 9. For the construction of the labeled datasetSused to train the forecasting function of CalibRAG, we sample a temperaturetâˆ¼Uniform[1.0,2.0]for each queryqand retrieved documentd. For each triplet(t, q, d), we perform user decoding 10 times and assign a soft labelbindicating the ratio of generated answers that contain the ground truth. The final datasetSthus consists of tuples in the form(t, q, d, b).The dataset will be made publicly available upon the publication of this work. Evaluation DatasetsFor zero-shot evaluation, we employ several datasets covering diverse do- mains and question types. HotpotQA [51] is a multi-hop question-answering dataset requir- ing reasoning across multiple supporting documents from Wikipedia to find answers, emphasiz- ing a more complex retrieval and reasoning process. WebQA [47] is an open-domain question- answering dataset consisting of natural, conversational questions paired with web documents, target- ing real-world, context-rich scenarios. Natural Questions (NQ) [46] is another large-scale question- answering dataset, designed to answer questions based on Wikipedia articles, containing both long- form and short-form answers. These datasets are used without additional training, providing a ro- bust evaluation of the generalization capabilities of CalibRAG across different domains and question types. We also evaluate domain-specific datasets, including BioASQ [57], a biomedical QA dataset con- taining factoid, list, and yes/no questions derived from PubMed articles, as well as Medical Infor- mation Retrieval-Augmented Generation Evaluation (MIRAGE) [49] and a textbook corpus. B.2 Hyperparameters Table 2 outlines the hyperparameters used for training the base model and LoRA, including key parameters such as learning rate, batch size, and LoRA-specific settings like rank and alpha. B.3 Evaluation metrics To evaluate long-form text, we utilizedgpt-4o-minito compare the ground-truth answers with the predicted answers in all cases.