L.; and Gal, Y . 2024. Detect- ing hallucinations in large language models using semantic entropy. Nature, 630(8017): 625–630. Findley, K. A. 2011. Adversarial inquisitions: Rethinking the search for the truth. NYL Sch. L. Rev., 56: 911. Flores, C.; and Woodard, E. 2023. Epistemic norms on evidence-gathering. Philosophical Studies , 180(9): 2547– 2571. Gao, L.; Dai, Z.; Pasupat, P.; Chen, A.; Chaganty, A. T.; Fan, Y .; Zhao, V .; Lao, N.; Lee, H.; Juan, D.-C.; and Guu, K. 2023. RARR: Researching and Revising What Language Models Say, Using Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers) , 16477–16508. Toronto, Canada: Association for Computational Linguis- tics. Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W. 2020. REALM: retrieval-augmented language model pre- training. In Proceedings of the 37th International Confer- ence on Machine Learning, ICML’20. JMLR.org. He, H.; Zhang, H.; and Roth, D. 2022. Rethinking with retrieval: Faithful large language model inference. arXiv preprint arXiv:2301.00303. He, J.; Neubig, G.; and Berg-Kirkpatrick, T. 2021. Effi- cient Nearest Neighbor Language Models. In Moens, M.-F.; Huang, X.; Specia, L.; and Yih, S. W.-t., eds.,Proceedings of the 2021 Conference on Empirical Methods in Natural Lan- guage Processing, 5703–5714. Online and Punta Cana, Do- minican Republic: Association for Computational Linguis- tics. Ho, X.; Duong Nguyen, A.-K.; Sugawara, S.; and Aizawa, A. 2020. Constructing A Multi-hop QA Dataset for Com- prehensive Evaluation of Reasoning Steps. In Scott, D.; Bel, N.; and Zong, C., eds., Proceedings of the 28th Inter- national Conference on Computational Linguistics , 6609– 6625. Barcelona, Spain (Online): International Committee on Computational Linguistics. Huang, J.; Gu, S. S.; Hou, L.; Wu, Y .; Wang, X.; Yu, H.; and Han, J. 2022. Large language models can self-improve. arXiv preprint arXiv:2210.11610. Huang, L.;