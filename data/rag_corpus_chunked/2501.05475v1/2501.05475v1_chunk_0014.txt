answering (QA) datasets. Experimental Setup Datasets and Evaluation Metrics We conduct exper- iments on two multi-hop question answering datasets: HotpotQA(Yang et al. 2018) and 2WikiMQA(Ho et al. 2020). Since both of the datasets are constructed based on Wikipedia documents, we use the same document cor- pus and retrievers to provide external references for LLMs. Due to the constraints of experimental costs, following(Zhou et al. 2024), we sub-sample 500 questions from the valida- tion set of each dataset for experiments. For evaluation metrics, we use exact match (EM) as our standard metrics at answer-level, to measure whether the predicted answer is completely consistent with the standard answer. And we use token-level F1, precision (Pre) and re- call (Rec) for comprehensive evaluation at token-level, to evaluate the proportion of correct answer tokens in the over- all tokens. Baselines We compare our RetroRAG to recent baseline approaches: Standard Prompting(Brown et al. 2020), Chain- of-Thought(Wei et al. 2023), Standard RAG(Lewis et al. 2020), ReAct(Yao et al. 2023), Self-Ask(Press et al. 2023), IR-COT(Trivedi et al. 2023), SearChain(Xu et al. 2024), and MetaRAG(Zhou et al. 2024). We comprehensively describe each baseline in Appendix B.1 and explain the rationale be- hind selecting these specific baselines. Settings We choose GLM4-9B-chat(THUDM 2024) LLM as the base LLM for all baseline and our RetroRAG ap- proach with the temperature setting of 0.01, except the SC- generator of our RetroRAG whose temperature is set to 1.00. We utilize the Wikipedia dump(Karpukhin et al. 2020) as the document corpus for both datasets, where articles are seg- mented into passages of 100 tokens. We employ the BM25 algorithm(Robertson, Zaragoza et al. 2009) and SimLM re- triever(Wang et al. 2023) to retrieve the top 5 relevant pas- sages to be the external knowledge for all approaches. And we set a default judgment threshold for