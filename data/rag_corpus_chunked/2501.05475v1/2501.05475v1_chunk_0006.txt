or specific do- main knowledge of LLMs(He, Zhang, and Roth 2022; Gao et al. 2023; Siriwardhana et al. 2023; Ram et al. 2023). Be- sides, some methods enhance the LLMs to better perceive factual information by fine-tuning the model with the exter- nal knowledge (Lee et al. 2022; Tian et al. 2023). Retrieval Augmented Language Model Many studies have demonstrated the impressive perfor- mance of the retrieval-augmented language model (RALM) in various natural language tasks, which is enhanced by the provision of detailed and specific external knowledge to sup- plement LLMs(Lewis et al. 2020; Guu et al. 2020; Shi et al. 2024). These models typically employ a retriever to obtain a set of relevant documents from a knowledge corpus, such as Wikipedia, to enhance the effectiveness of the answers. While initial RALM performs the single-time retrieval strat- egy, which extracts knowledge once based on the userâ€˜s ini- tial query(He, Neubig, and Berg-Kirkpatrick 2021; Izacard and Grave 2021; Ram et al. 2023), recent studies have fo- cused on multi-time retrieval models to overcome the issues of insufficient knowledge, due to retriever may focus only on parts of the query when addressing multi-hop complex questions. Some models decompose the initial query into multiple sub-questions, then iteratively retrieve knowledge and answer these sub-questions, until the original query can be finally resolved(Yao et al. 2023; Press et al. 2023; Shao et al. 2023; Xu et al. 2024); while the others construct an iterative process of holistic thinking, continuously increas- ing the amount of knowledge retrieved based on unsolved questions, until effective reasoning can be achieved(Trivedi et al. 2023; Zhou et al. 2024), and more recent studies have explored fine-tuning certain components to enhance the reli- ability of retrieval process(Yan et al. 2024; Liu et al. 2024). All these approaches are proven to be effective.