series of experiments to investigate the impact of the number of evidence. As depicted in Figure 4, we find that on both datasets, increasing the numbers of evidence from 1 to 3 can significantly improve the performance of RetroRAG, which reflects that under the condition of insuf- ficient information, it is difficult for LLMs to perform effec- tive reasoning. The accuracy of RetroRAG reaches the peak at the number of 5 pieces of evidence, and then slowly de- creases as the increase of the evidence. This suggests that although the filtering and updating mechanism can suppress irrelevant information to some extent, an excessively large evidence window can still introduce noise and impair per- formance. Different similarity thresholds Although threshold t > 0.5 signifies that the LLM evaluator considers the result to be valid, a higher t represents the LLM has more confidence in the determination. This could lead to a more reliable mon- itor but might also result in overly stringent requirements for the results, causing misjudgments and excessive itera- tions. As illustrated in Figure 4, we find that for HotpotQA dataset, t = 0 .5 can provide a good discriminative effect, but for 2WikiMQA, LLMs need a higher threshold which is about 0.7 to ensure the effectiveness. And for both datasets, excessively high thresholds lead to a decline in performance. This is also related to the over-reasoning issue. Case Study Due to factually related irrelevant documents from the in- herent flaws of the current retrieval system, previous ap- proaches cause issues of insufficient reasoning and over- reasoning, respectively. To address these issues, we intro- duce inferential evidence as a form of knowledge caching. By summarizing and updating past relevant information, this approach enables retroactive awareness when new knowl- edge is introduced, while simultaneously reducing inter- ference from irrelevant information.