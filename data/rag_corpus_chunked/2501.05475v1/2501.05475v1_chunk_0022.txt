We believe this because the re- trieval knowledge is extensively relevant and mutually cor- roborative, enhancing the LLM’s performance. Furthermore, our work enhances the LLM’s understanding of knowledge by generating Inferential Evidence, thereby further improv- ing the reliability of its responses. Additionally, the absence of Inferential Evidence generation may be due to the sparse distribution of required knowledge within individual docu- ments. In such cases, decomposing sub-questions allows the LLM to better understand and answer the questions. More- over, our work ensures the accumulation of effective infor- mation through Source Evidence updating, thereby also en- hancing the LLM’s performance. Conclusion In this paper, we point out the threat from the unidirec- tional forward reasoning paradigm inherent in traditional RAG methods, within which any errors produced during rea- soning steps are irreversible and affect the whole reason- ing chain. We then introduce RetroRAG, a novel framework that uses a detective-like retroactive reasoning paradigm that can revise and reconstruct the reasoning chain, ensuring it on the correct direction. Through the evidence-collation- discovery framework, RetroRAG can search, generate, and update credible evidence, empower the model to perceive existing information, and seek out more necessary evidence to complete the reasoning process. Experimental results on two multi-hop QA datasets demonstrated that RetroRAG performs better than all baselines. In the future, we aspire to explore the possibility of allowing LLMs to independently learn the aforementioned evidence-collation-discovery pro- cess through methods such as fine-tuning or pre-training. References Asai, A.; Wu, Z.; Wang, Y .; Sil, A.; and Hajishirzi, H. 2024. Self-RAG: Learning to Retrieve, Generate, and Cri- tique through Self-Reflection. In The Twelfth International Conference on Learning Representations. Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,