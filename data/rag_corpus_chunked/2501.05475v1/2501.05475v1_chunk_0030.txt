Liu, Z.; Jin, J.; Nie, J.-Y .; and Dou, Z. 2024. Metacognitive Retrieval-Augmented Large Language Mod- els. In Proceedings of the ACM on Web Conference 2024 , 1453–1463. New York, NY , USA: Association for Comput- ing Machinery. A. Prompt Detail We show the prompt used in experiment on both datasets in Figure 7 to Figure 13. In this work, we constructed the few- shot CoT prompt, and the declarative assessor prompt by referencing (Zhou et al. 2024), and design different prompt for the corresponding functions. For all calculations as de- tailed in the Methodology section, we quantified the results by generating probability distributions of the ’yes’ and ’no’ tokens. Figure 7: Prompt MD for generating the monitoring answer. Figure 8: Prompt Msc for calculating the Self-Consistency score to determine if the answer is reliable. Figure 9: Prompt Me for calculating the score of relevance between evidence and query for evidence updating. Figure 10: Prompt MIE for generating inferential evidence. Figure 11: Prompt Mqr for calculating the Question- Relevance score. Figure 12: Prompt Mra for calculating the Reference- Attribution score. Figure 13: Prompt MR for generating re-query. B. Experimental Details Baselines We compare our proposed model with several state-of-theart baselines listed as follows: • Standard Prompting(Brown et al. 2020): Standard Prompting directs LLM to answer the queries with a sim- ple question-answer prompt. • Chain-of-Thought(Wei et al. 2023): Chain-of-Thought provides a few-shot prompt to LLM, make it can answer with a reasoning process. • Standard RAG(Lewis et al. 2020): Standard RAG first retrieves multiple documents by query, then inject these documents into prompts to LLM for answering. • ReAct(Yao et al. 2023): ReAct introduces a reasoning and acting paradigm, alternately executing reasoning and task-specific actions to complete the QA task. • Self-Ask(Press et al. 2023): Self-Ask introduces a paradigm that