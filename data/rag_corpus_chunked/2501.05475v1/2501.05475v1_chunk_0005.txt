cannot reverse the error in preced- ing reasoning steps, RetroRAG uses a retroactive reason- ing paradigm that can revise and reconstruct the reason- ing chain through two types of evidence, provides effec- tive answers with less hallucination. • To the best of our knowledge, this is the first time an evidence-collation-and-discovery framework has been proposed and used in a retrieval-augmented framework, which generates and updates the evidence to support the reasoning process, significantly enhances knowledge re- trieval performance on question-answering tasks. RELA TED WORK Hallucination in Large Language Model Currently, hallucination is referred as generated content that either does not align with real-world facts or deviates from the source material and self-consistency(Huang et al. 2023b; Ye et al. 2023). In the context of question-answering tasks, hallucination specifically manifests as the generation of ar- bitrary, and incorrect answers. This phenomenon occurs be- cause, in cases of hallucination, the internal consistency of the generation process in LLMs is unstable. (Manakul, Liusie, and Gales 2023; M¨undler et al. 2024; Farquhar et al. 2024). Some studies consider addressing the hallucination problem based on the tendencies of generation from LLMs, they generate multiple outputs and then employ a majority voting strategy to obtain relatively reliable answers(Wang et al. 2022; Huang et al. 2022). More studies consider that the inconsistency generation of LLMs stems from the lack of knowledge, therefore, they introduce reliable exter- nal knowledge through the Retrieval-Augmented Generation (RAG) framework, to enhance the factual or specific do- main knowledge of LLMs(He, Zhang, and Roth 2022; Gao et al. 2023; Siriwardhana et al. 2023; Ram et al. 2023). Be- sides, some methods enhance the LLMs to better perceive factual information by fine-tuning the model with the exter- nal knowledge (Lee et al. 2022; Tian et al. 2023). Retrieval Augmented Language Model Many studies have demonstrated