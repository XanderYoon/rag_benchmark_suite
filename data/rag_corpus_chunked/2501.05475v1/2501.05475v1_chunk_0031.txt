a reasoning process. • Standard RAG(Lewis et al. 2020): Standard RAG first retrieves multiple documents by query, then inject these documents into prompts to LLM for answering. • ReAct(Yao et al. 2023): ReAct introduces a reasoning and acting paradigm, alternately executing reasoning and task-specific actions to complete the QA task. • Self-Ask(Press et al. 2023): Self-Ask introduces a paradigm that decomposes questions into sub-questions, continuously engaging in self-questioning until the final answer is obtained. • IR-COT(Trivedi et al. 2023): IR-COT iteratively alter- nates between using COT reasoning to guide retrieval, and utilizing retrieval results to enhance CoT reasoning, continuing executing until the final answer is obtained. • SearChain(Xu et al. 2024): SearChain introduces the concept of ”search-in-chain,” which corrects the reason- ing process through the interaction between Information Retrieval (IR) and Chain-of-Query (COQ), let IR por- vides the knowledge that LLM really needs. • MetaRAG(Zhou et al. 2024): MetaRAG combines the RAG process with metacognition, allowing LLM to ex- ecute different actions based on the reliability of inter- nal and external knowledge, identify the sufficiency of knowledge and potential errors during reasoning. Additional Case Studies We present additional cases to further demonstrate the ef- fectiveness of our proposed RetroRAG approach. As shown in Figure 14, we find that RetroRAG not only leverages the effective information from inferential evidence to make ac- curate reasoning, but can also compensates for insufficient inferential evidence by retrieving complementary source ev- idence through re-query, thereby enabling correct reasoning, which also highlights the necessity of leveraging both infer- ential evidence and source evidence. Figure 14: Additional Case Studies. C. Assessment using LLM-judge across Various Scenarios To validate the transferability of our approach across differ- ent linguistic contexts, and its reliability in simple question- answering scenarios, we conducted performance evaluations on the knowledge question-answering segments of a