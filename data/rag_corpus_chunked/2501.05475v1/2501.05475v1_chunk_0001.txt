evaluations show that RetroRAG significantly outperforms existing methods. Introduction Large language models (LLMs), such as ChatGPT(OpenAI 2022) and ChatGLM(THUDM 2024), have demonstrated outstanding performance across a wide range of natural lan- guage processing tasks. However, despite the vast amount of knowledge stored during training, these models still exhibit a tendency to generate hallucinatory content, resulting in un- verified or factually incorrect answers(Huang et al. 2023a; Wu, Wu, and Zou 2024). To address this issue, the Retrieval- Augmented Generation (RAG) framework is leveraged to acquire and subsequently inject relevant external source knowledge into the LLM’s prompt, significantly enhancing the accuracy and reliability of LLM’s responses(Lewis et al. 2020; Mao et al. 2021; Chen et al. 2024). *This work was completed during an internship at Xiaomi Cor- poration. †*Corresponding authors Figure 1: An example of previous RAG approaches causes hallucinatory content due to their unidirectional forwards reasoning paradigm, and how RetroRAG address this issue. Traditional retrieval-augmented models typically retrieve and extract knowledge documents once based on the ini- tial query, these approaches struggle with addressing multi- hop complex questions due to insufficient knowledge. To tackle this issue, recent studies have transformed the sin- gle retrieval-generating into a dynamic multiple retrieval- generating process. These approaches decompose the com- plex question into several sub-questions, and obtain the fi- nal output by answering all these sub-questions(Shao et al. 2023; Trivedi et al. 2023; Press et al. 2023; Yao et al. 2023). Even though the latest approaches have been pro- posed to improve the effectiveness of knowledge documents retrieval(Asai et al. 2024; Xu et al. 2024), current RAG frameworks are susceptible to the threat of insufficient rea- soning from the documents which are factual, and related, but irrelevant due to the inherent flaws of current retrieval systems(Wu et al. 2024), and cause external hallucination. As illustrated