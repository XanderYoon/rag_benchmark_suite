Improving the Do- main Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering. Transac- tions of the Association for Computational Linguistics , 11: 1–17. THUDM. 2024. GLM-4. https://github.com/THUDM/ GLM-4. Tian, K.; Mitchell, E.; Yao, H.; Manning, C. D.; and Finn, C. 2023. Fine-tuning language models for factuality. arXiv preprint arXiv:2311.08401. Trivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal, A. 2023. Interleaving Retrieval with Chain-of-Thought Rea- soning for Knowledge-Intensive Multi-Step Questions. In Proceedings of the 61st Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers). Toronto, Canada: Association for Computational Linguis- tics. Wang, L.; Yang, N.; Huang, X.; Jiao, B.; Yang, L.; Jiang, D.; Majumder, R.; and Wei, F. 2023. SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval. In Rogers, A.; Boyd-Graber, J.; and Okazaki, N., eds., Pro- ceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2244– 2258. Toronto, Canada: Association for Computational Lin- guistics. Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang, S.; Chowdhery, A.; and Zhou, D. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023. Chain-of- Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903. Wu, K.; Wu, E.; and Zou, J. 2024. ClashEval: Quantifying the tug-of-war between an LLM’s internal prior and external evidence. arXiv:2404.10198. Wu, S.; Xie, J.; Chen, J.; Zhu, T.; Zhang, K.; and Xiao, Y . 2024. How Easily do Irrelevant Inputs Skew the Responses of Large Language Models? arXiv preprint arXiv:2404.03302. Xu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-S. 2024. Search-in-the-Chain: Interactively Enhancing Large Lan- guage Models with Search for Knowledge-intensive Tasks. In Proceedings of the ACM