the current knowledge context, and deter- mines if a consistent response can be generated within the current knowledge context. (2) ELLERY obtains documents DQ from the retrieval corpus D = {di}|D| i=1(with Wikipedia dumps served as the primary data source in this study) re- lated to the question, as the source evidence, and by which generates credible evidence E, and re-queries qr based on q, k, DQ, and the last reasoning chain r. ELLERY contin- ues this process of collating and discovering evidence until a definitive answer is obtained by Answerer. The details of the prompts we designed and used will be introduced in Section A of the Appendix. Answerer The main target of Answerer is to generate a reliable an- swer to the question. To achieve this, the Answerer first generates pseudo-answer a and corresponding reason r with the current quantities of information E through an answer- generator, which utilizes COT-prompt MC with a low- temperature parameter to obtain a more fixed output. To assess whether the current knowledge context is suf- ficient, we refer Self-Consistency (SC) concept that outputs of LLMs should converge towards the correct answer un- der a strong knowledge context, with which LLMs are capa- ble of constructing reliable reasoning and answers. To this end, we employ an SC-generator with a direct-answering- prompt MD and high-temperature parameter to obtain mon- itoring answer asc with more divergent thinking pattern and the same knowledge content ofa, and the similarity between these two outputs can assess the self-consistency score and the degree of hallucination. We designed an LLM-based evaluator S to convert scores, e.g., similarity or relevance, into the probability of generating indicative tokens (e.g., ’yes’ or ’no’). We calculate the similarity scoresssc through LLM-based evaluator Ssc as the following formulation: Figure 3: Overview of our RetroRAG structure.