Retrieval-Augmented Generation by Evidence Retroactivity in LLMs Liang Xiao1*, Wen Dai2, Shuai Chen 2, Bin Qin 2, Chongyang Shi 1†, Haopeng Jing 1, Tianyu Guo1 1Beijing Institute of Technology, 2Xiaomi Corporation {patrickxiao, cy shi, jinghp, guotianyu}@bit.edu.cn, {daiwen, chenshuai3, qinbin}@xiaomi.com Abstract Retrieval-augmented generation has gained significant atten- tion due to its ability to integrate relevant external knowl- edge, enhancing the accuracy and reliability of the LLMs’ responses. Most of the existing methods apply a dynamic multiple retrieval-generating process, to address multi-hop complex questions by decomposing them into sub-problems. However, these methods rely on an unidirectional forward reasoning paradigm, where errors from insufficient reason- ing steps or inherent flaws in current retrieval systems are irreversible, potentially derailing the entire reasoning chain. For the first time, this work introducesRetroactive Retrieval- Augmented Generation (RetroRAG), a novel framework to build a retroactive reasoning paradigm. RetroRAG revises and updates the evidence, redirecting the reasoning chain to the correct direction. RetroRAG constructs an evidence- collation-discovery framework to search, generate, and refine credible evidence. It synthesizes inferential evidence related to the key entities in the question from the existing source knowledge and formulates search queries to uncover addi- tional information. As new evidence is found, RetroRAG con- tinually updates and organizes this information, enhancing its ability to locate further necessary evidence. Paired with an Answerer to generate and evaluate outputs, RetroRAG is ca- pable of refining its reasoning process iteratively until a re- liable answer is obtained. Empirical evaluations show that RetroRAG significantly outperforms existing methods. Introduction Large language models (LLMs), such as ChatGPT(OpenAI 2022) and ChatGLM(THUDM 2024), have demonstrated outstanding performance across a wide range of natural lan- guage processing tasks. However, despite the vast amount of knowledge stored during training, these models still exhibit a tendency to generate hallucinatory content, resulting in un- verified or factually