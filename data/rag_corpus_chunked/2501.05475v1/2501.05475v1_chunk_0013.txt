process of evaluators SEi can be formulated as: sEi = SEi ([E(L−1) i ∪ e(L) ic , q], Me) (4) We select the top-K inferential evidence as current inferen- tial evidence E(L) i in the same way, through which achiev- ing the revising and reconstructing of reasoning nodes. And, E(L) i would be sent to Answerer as referenced evidence in the (L+1)-th round. It is important to note that since E(L) i ∈ E(L) s ∪ E(L−1) i , and current quantities of information E(L) = E(L) s ∪ E(L−1) i , no new information is brought in after the updat- ing of Ei. If Answerer fails to provide an effective answer with the knowledge context E(L), it is necessary to retrieve more source evidence to fill the knowledge gap. To achieve this, we should first know the information E(L) LLM al- ready have right now, and the reason r why LLM can‘t (or wrongly) answer the question based on these information, then generate a new query to further retrieve information from the corpus to answer the initial question q. Hence, we construct a LLM-based re-query generator GR with cus- tomized prompt MR, to generate search query q(L) s to de- duce what information LLM needs to answer the question: q(L) s = LLM ([E(L) s , E(L) i , r, q], MR) (5) Experiments In this section, we evaluate the effectiveness of our proposed model on two multi-hop question answering (QA) datasets. Experimental Setup Datasets and Evaluation Metrics We conduct exper- iments on two multi-hop question answering datasets: HotpotQA(Yang et al. 2018) and 2WikiMQA(Ho et al. 2020). Since both of the datasets are constructed based on Wikipedia documents, we use the same document cor- pus and retrievers to provide external references for LLMs. Due to the constraints of experimental