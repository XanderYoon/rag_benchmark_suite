enabling correct reasoning, which also highlights the necessity of leveraging both infer- ential evidence and source evidence. Figure 14: Additional Case Studies. C. Assessment using LLM-judge across Various Scenarios To validate the transferability of our approach across differ- ent linguistic contexts, and its reliability in simple question- answering scenarios, we conducted performance evaluations on the knowledge question-answering segments of a Chi- nese hallucination evaluation dataset HalluQA(Cheng et al. 2023), which contain single-hop question build from Baidu Baike. We utilize the Baidu Baike dump as the document corpus for HalluQA datasets, where articles are segmented into passages of 100 tokens, and we employ the BM25 algo- rithm to retrieve the top 5 relevant documents to be the exter- nal knowledg.Given that the golden truth in HalluQA con- tains detailed descriptions, making it challenging to quan- tify performance using token-level metrics, we employed the GPT4 as the LLM-Judge, with the same setting of (Cheng et al. 2023), to assess the answer semantic accuracy. Additionally, we simultaneously applied the same LLM- Judge to evaluate the performance on HotpotQA and 2WikiMQA dataset from the semantic perspective. Table 3: LLM-Judge on three datasets. HotpotQA 2WikiMQA HalluQA LLM-Judge LLM-Judge LLM-Judge Standard 29.4 29.2 27.7 COT 32.0 32.4 37.0 RAG 47.4 38.6 45.2 ReAct 42.3 35.3 36.6 Self-Ask 52.0 39.7 30.4 IR-COT 53.4 42.5 56.8 SearChain 56.8 47.9 48.3 MetaRAG 56.2 40.9 57.8 RetroRAG 68.2 51.2 64.3 As shown in Table 3, our RetroRAG outperforms all base- lines in semantic perspective on all three datasets, This in- dicates that our approach demonstrates more stable and re- liable performance across various scenarios. Also, we ob- served that some baselines with unidirectional forward rea- soning paradigm, exhibit performance degradation when en- countering simple questions with single-hop. For instance, Self-Ask may degrade to standard prompting, ReAct may degrade to CoT,