Deductive Reasoning: we utilize an LLM-based evidence generating prompt MIE to generate inferential evidence candidates e(L) ic related to k from E(L) s , to obtain as muchdeductive inference to the initial question as possible, from the current retrieval documents. (2) Hy- pothesis Testing: we design two specified LLM-based gated prompt Mqr and Mra to calculate the Question-Relevance (QR) score and the Reference-Attribution (RA) score with their LLM-based evaluator Sqr and Sra , to ensure the ef- fectiveness of each inference. The specific functions of these two evaluation scores are as follow: • Question-Relevance (QR): On knowing the last inferen- tial evidence E(L−1) i , if e(L) ic could be directly related to answering the matching query q(L) m : • Reference-Attribution (RA): If the claim of e(L) ic can be directly found in any claims of E(L) s . Through these step, we only reserve the useful and con- firmed inference as evidence, and this process can be for- mulated as: sqr = Sqr([e(L) ic , E(L−1) i , q(L) m ], Mqr)) sra = Sra([e(L) ic , E(L) s ], Mra) e(L) ic = ((sqr > 0.5) ∩ (sra > 0.5))e(L) ic (3) Next, we mergee(L) ic and last inferential evidenceE(L−1) i . Following the updating process of source evidence, we ap- ply the same evaluator Se score each inferential evidence candidate. Since inferential evidence should align with the initial question for a long term memory, the scoring process of evaluators SEi can be formulated as: sEi = SEi ([E(L−1) i ∪ e(L) ic , q], Me) (4) We select the top-K inferential evidence as current inferen- tial evidence E(L) i in the same way, through which achiev- ing the revising and reconstructing of reasoning nodes. And, E(L) i would be sent to Answerer as referenced evidence in