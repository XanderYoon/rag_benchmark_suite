et al. 2020) as the document corpus for both datasets, where articles are seg- mented into passages of 100 tokens. We employ the BM25 algorithm(Robertson, Zaragoza et al. 2009) and SimLM re- triever(Wang et al. 2023) to retrieve the top 5 relevant pas- sages to be the external knowledge for all approaches. And we set a default judgment threshold for our answering eval- uation mechanism at 0.7 to ensure consistency of answers. The maximum number of both iterations and the size of the evidence repository are set to 5. Main Results Performance on multi-hop question answering datasets is shown in Table 1. It can be observed that: (1) Our proposed RetroRAG consistently surpasses all baseline methods across two datasets. At answer-level, the performance improvement on EM is +8.8 on HotpotQA and +5.2 on 2WikiMQA compared to the best baseline results; At token-level, the performance improvement on F1 is+10.6 on HotpotQA and +4.0 on 2WikiMQA compared to the best baseline results. This suggests that when directly employ- ing LLM, without additional pre-training or fine-tuning, our approach exhibits optimal performance. (2) When compared to SearChain, which adapts the uni- directional forward reasoning paradigm but verifies each node in COT and outperforms other methods using the same paradigm, such as COT, ReACT, Self-Ask, etc., RetroRAG shows an improvement of +11.6 on HotpotQA and +5.2 on 2WikiMQA. This reflects that the retroactive reasoning paradigm RetroRAG uses can solve the issue of local insuf- ficient reasoning, and provides more comprehensive reason- ing, thereby improving the performance markedly. (3) When compared to IR-COT and MetaRAG, which also do not adhere to the paradigm of linear reasoning, but increase the quantity of retrieved documents to re-generate the answer, RetroRAG shows an improvement of +8.8 on HotpotQA and +9.2 on 2WikiMQA. This reflects that the evidence-collation-and-discovery framework RetroRAG