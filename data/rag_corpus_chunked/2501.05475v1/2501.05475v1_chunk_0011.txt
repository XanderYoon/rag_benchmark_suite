q(L−1) s ,which is designed to specifically target missing information, to re- trieve relevant passages D(L) C . And concatenate q(L−1) s with q to obtain the matching query q(L) m = [ q, q(L−1) s ], which is designed to match the most relevant evidence within the current knowledge context, while avoiding the issue of devi- ating from the initial question by focusing too much on the generated search query. Specifically, we set q(0) s = q(0) m = q as the initialization. After obtaining D(L) C , we merge them with the last stored source evidence E(L−1) s , and apply the customized prompt Me with LLM to individually score each source evidence candidate, ranging from 0 to 1, to assess the contribution of source evidence to answering the question. Since source evidence should be updated to ensure the pro- gression of current answering process, the scoring process of evaluators SEs can be formulated as: sEs = SEs ([E(L−1) s ∪ D(L) C , q(L) m ], Me)) (2) Based on sEs, we can extract the top-N source evidence as the current source evidence E(L) s , which would be sent to Answerer to help the answering process in the L-th round, and be used to generate the inferential evidence and the re- query in the failure answering case. Evidence Discovery Follow the actual detective, we uses Deductive method to generates inferential evidence, which contains two steps: (1) Deductive Reasoning: we utilize an LLM-based evidence generating prompt MIE to generate inferential evidence candidates e(L) ic related to k from E(L) s , to obtain as muchdeductive inference to the initial question as possible, from the current retrieval documents. (2) Hy- pothesis Testing: we design two specified LLM-based gated prompt Mqr and Mra to calculate the Question-Relevance (QR) score