reasoning, and provides more comprehensive reason- ing, thereby improving the performance markedly. (3) When compared to IR-COT and MetaRAG, which also do not adhere to the paradigm of linear reasoning, but increase the quantity of retrieved documents to re-generate the answer, RetroRAG shows an improvement of +8.8 on HotpotQA and +9.2 on 2WikiMQA. This reflects that the evidence-collation-and-discovery framework RetroRAG uses can address the issue of over-reasoning, and mitigate the irrelevant and noisy information from knowledge docu- ments, thereby improving the performance significantly. (4) Compared with Standard Prompting and COT ap- proaches, both the idea of decomposing the initial query into multiple sub-questions and iteratively increasing the amount of knowledge retrieved can doubtlessly improve the ability of reasoning of LLMs. When multi-hop questions Table 1: Evaluation results on two multi-hop question answering datasets. ’*’ denotes the result outperforms baseline models in t-test at p < 0.05 level. The best results are in bold, and the second best results are underlined. HotpotQA 2WikiMQA Methods EM F1 Pre Rec EM F1 Pre Recall Standard Prompting(Brown et al. 2020) 14.2 21.7 23.2 21.2 21.4 27.3 28.8 26.9 Chain-of-Thought(Wei et al. 2023) 16.8 24.9 26.1 24.9 23.6 30.4 31.1 30.5 Standard RAG(Lewis et al. 2020) 23.4 35.6 36.6 36.6 22.8 26.8 27.2 28.0 ReAct(Yao et al. 2023) 20.6 29.4 29.6 32.1 21.2 28.5 28.2 30.3 Self-Ask(Press et al. 2023) 24.8 35.1 36.5 36.4 29.4 36.7 36.4 38.2 IR-COT(Trivedi et al. 2023) 30.4 40.1 41.6 41.0 25.6 30.9 31.0 32.1 SearChain(Xu et al. 2024) 29.6 41.2 41.5 43.4 33.4 42.6 42.5 44.8 MetaRAG(Zhou et al. 2024) 32.4 44.3 45.5 45.6 28.8 36.0 35.7 38.4 RetroRAG 41.2 * 54.9* 56.2* 58.3* 38.6* 46.6* 46.9* 49.5* present a clearer progressive structure, such as samples in 2WikiMQA dataset, the approaches of decomposing sub- problems will perform better, so