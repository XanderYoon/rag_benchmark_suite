the whole utterance, as shown by BM25+resp2ctxt lu’s (2b) higher recall values. In the MANtIS dataset the R@10 goes from 0.309 when using the model trained to predict the dialogue context to 0.325 when using the one trained to p redict only the last utterance of the dialogue context. We thus ﬁnd that F2 generalizes to response retrieval for dialogues, especially when predi cting only the last utterance of the context 16. In order to understand what the response expansion methods ar e doing most—term re-weighting or adding novel terms—we present the pe rcentage of novel terms added by both methods in Table 4. The table shows that resp2ctxtlu does more term re-weighting than adding new words when compared to resp2ctxt 16 As future work, more sophisticated techniques can be used to determine which parts of the dialogue context should be predicted. From Document and Passage Retrieval to Response Retrieval f or Dialogues 11 Table 4. Statistics of the augmentations for resp2ctxt and resp2ctx tlu. New words are the ones that did not exist in the document before. MANtIS MSDialog UDC DSTC8 Context avg length 474.12 426.08 76.95 Response avg length 42.58 71.38 11.06 Aug. avg length - resp2ctxt 494.23 596.99 202.3 Aug. avg length - resp2ctxt lu 138.5 135.29 72.57 % new words - resp2ctxt 71% 69% 71% % new words - resp2ctxt lu 59% 37% 63% (53% and 70% on average are new words respectively and thus 47% v s 30% are changing the weights by adding existing words), generating overall smaller aug- mentations (115.45 vs 431.17 on average respectively). F3 ✓ Sparse retrieval outperforms zero-shot dense retrieval [3 4,41]. Sparse retrieval models are more eﬀective than the majority of ze ro-shot dense models, as shown by the comparison of rows (1a–b), and (2a–b) wit h rows (3a–