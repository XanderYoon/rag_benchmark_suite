roblem is by using query expansion methods. RM3 [1] is a competitive [49] query ex pansion technique that uses pseudo-relevance feedback to add new term s to the queries followed by another ﬁnal retrieval step using the modiﬁed query. Context for F2 A supervised sparse retrieval model can take advantage of the eﬀectiveness of transformer-based language models by chan ging the terms’ weights from collection statistics to something that is learned. Docu ment ex- pansion with a learned model can be considered a learned sparse ret rieval ap- proach [19]. The core idea is to create pseudo documents that hav e expanded terms and use them instead when doing retrieval. Doc2query [25] is a strong su- pervised sparse retrieval baseline that uses a language model to p redict queries that might be issued to ﬁnd a document. The predictions of this mode l are used to create the augmented pseudo documents. Context for F3 and F4 Supervised dense retrieval models 7, such as ANCE [46] and coCodenser [7], represent query and documents in a small ﬁxed -length space, for example of 768 dimensions. Dense retrieval models without acce ss to target data for training—known as the zero-shot scenario—have underperformed sparse methods ( F3). For example, the BEIR benchmark [41] showed that BM25 was superior to dense retrieval from 9–18 (depending on the model) ou t of the 18 datasets in the zero-shot scenario. In contrast, when having ac cess to enough supervision from target data, dense retrieval models have shown to consistently outperform strong sparse baselines [7,15,34] ( F4). 7 A distinction can also be made of cross-encoders and bi-enco ders, where the ﬁrst encode the query and document jointly as opposed to separate ly [40]. Cross-encoders are applied in a re-ranking step due to their ineﬃciency and t hus