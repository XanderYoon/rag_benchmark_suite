a mixed-initiative conversation through natural language interaction, rather than through the traditional search engine r esults page. A popular approach to conversational search is retrieval-based [3]: given an on- going conversation and a large corpus of historic conversations, r etrieve the response that is best suited from the corpus [45,48,28,47,11]. Due to the ef- fectiveness of heavily pre-trained transformer-based language models such as BERT [4], they have become the predominant approach for convers ation re- sponse re-ranking [28,43,53,8,42]. The most common evaluation procedure for conversation respons e re-ranking consists of re-ranking a limited set of n candidate responses (including the 1 https://github.com/Guzpenha/transformer_rankers/tree/full_rank_retrieval_dialogues. 2 Gustavo Penha and Claudia Hauﬀ ground-truth response(s)), followed by measuring the number o f relevant re- sponses found in the ﬁrst K positions—Recalln@K [52]. Since the entire collec- tion of available responses is typically way bigger 2 than such a set of candidates, this setup is in fact a selection problem, where we have to choose the correct response out of a few options. This evaluation overlooks the ﬁrst- stage retrieval step, which retrieves a set of n responses to be re-ranked. If the ﬁrst-stage model, e.g. BM25, fails to retrieve relevant responses, the entire pipeline f ails. Motivated by a lack of research on the ﬁrst-stage retrieval step , we are inter- ested in answering in our replicability study whether the considerable knowledge obtained on document and passage retrieval tasks generalizes to the dialogue do- main. Unlike document and passage retrieval where the documents are generally longer than the queries, in response retrieval for dialogues the qu eries (dialogue contexts) tend to be longer than the documents (responses). A second important diﬀerence is the structure induced by the dialogue as seen in Table 1. Table 1. Comparison between passage retrieval and response retriev al for dialogues.