have Stack Exchange data ( MSMarco) in their ﬁne-tuning, bi-encoder bert−base (3c), improves the eﬀectiveness with statistical signiﬁcance from 0.092 R@10 to 0.2 05 R@10. F5 ✓ Hard negative sampling is better than random sampling for training dense retrieval models [46,51]. Surprisingly we ﬁnd that naively using more eﬀective models to select negative candidates is detrimen tal to the ef- fectiveness of the dense retrieval model (see Hard negative sam pling in Table 5). We observe this phenomenon when using diﬀerent language models, w hen switch- ing intermediate training on or oﬀ for all datasets, and when using an alternative contrastive loss [10] that does not employ in-batch negative samp ling19. After testing for a number of hypotheses that might explain why ha rder negatives do not improve the eﬀectiveness of the dense retrieval model, we found that false negative samples increase signiﬁcantly when using better negative sampling methods. False negatives are responses that are potent ially valid for the context. Such relevant responses lead to unlearning relevant matches between context and responses as they receive negative labels. See below a n example of a false negative sample retrieved by the bi-encoder model (row 3d o f Table 3): Dialogue context ( U ): hey... how long until dapper comes out? [U] 14 days [...] [ U] i thought it was coming out tonight Correct response ( r+): just kidding couple hours F alse negative sample ( r−): there is a possibility dapper will be delayed [...] mean- while, dapper discussions should occur in ubuntu+1 Denoising techniques try to solve this problem by reducing the numbe r of false negatives. We employ a simple approach that instead of using the top -ranked responses as negative responses, we use the bottom responses of the top-ranked responses as negatives 20. This decreases