Ri, Yi)}M i=1 be a data set consisting of M triplets: dialogue context, response candidates and response re levance labels. The dialogue context Ui is composed of the previous utterances {u1, u2, ..., uτ } at the turn τ of the dialogue. The candidate responses Ri = {r1, r2, ..., rn} are either ground-truth responses r+ or negative sampled candidates r−, indicated by the relevance labels Yi = {y1, y2, ..., yn}. In previous work, the number of candidates is limited, typically n = 10 [29]. The ﬁndings we replicate here come from passage and document retrieval tasks where there is no limit t o the number of documents or passages that have to be retrieved. Thus, in all o f our ﬁrst-stage retrieval task experiments n is set to the size of the entire collection of responses in the corpus. The number of ground-truth responses is one, the observed re- sponse in the conversational data. The task is then to learn a rank ing function f (.) that is able to generate a ranked list from the entire corpus of res ponses Ri based on their predicted relevance scores f (U, r). F1: Unsupervised Sparse Retrieval We rely on classic retrieval methods, for which the most commonly used baseline is BM25. One of the limitation s of sparse retrieval is the vocabulary mismatch problem. Expansion te chniques are able to overcome this problem by appending new words to the dialogue contexts and responses. For this reason, we here translate a query expan sion technique to the dialogue domain and perform dialogue context expansion with RM3 [1], a competitive unsupervised method that assumes that the top-ra nked responses by the sparse retrieval model are relevant. From these pseudo- relevant responses, words are selected and an expanded dialogue context is created