the dense retrieval model performs. Th e results indicate that a good zero-shot retrieval model needs to go through inter mediate training on a large set of training data coming from multiple datasets to gener alize well to diﬀerent domains and outperform strong sparse retrieval bas elines. F4 ✓ Dense models with access to target training data outperform sparse models [7,15,34]. First, we see that ﬁne-tuning the dense retrieval model, which has gone through intermediate training already, with ra ndom sampling—row (4a) in Table 3—achieves the best overall eﬀectivenes s in two of the three datasets. This result shows that F4 generalizes to the task of 17 For the full description of the intermediate data see https://huggingface.co/sentence-transformers/all-mpnet-base-v2 . 12 Gustavo Penha and Claudia Hauﬀ response retrieval for dialogues when employing intermedi ate train- ing18. Having access to the target data as opposed to only the intermed iate training data means that the representations learned by the mode l are closer to the true distribution of the data. We hypothesize that ﬁne-tuning the bi-encoder for MANtIS (4a) is harmful because the intermediate data contains Stack Exchange respons es. In this way, the set of dialogues of Stack Exchange that MANtIS encompasses might be serving only to overﬁt the intermediate representations. As evidence for this hypothe- sis, we found that (I) the learning curves ﬂatten quickly (as oppos ed to other datasets) and (II) ﬁne-tuning another language model that doe s not have Stack Exchange data ( MSMarco) in their ﬁne-tuning, bi-encoder bert−base (3c), improves the eﬀectiveness with statistical signiﬁcance from 0.092 R@10 to 0.2 05 R@10. F5 ✓ Hard negative sampling is better than random sampling for training dense retrieval models [46,51]. Surprisingly we ﬁnd that naively using more eﬀective models to select negative candidates is detrimen tal to the ef-