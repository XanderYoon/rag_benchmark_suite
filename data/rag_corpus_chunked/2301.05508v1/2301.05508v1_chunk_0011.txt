ef- fective family of models 9. A zero-shot model is one that is not trained on the target data. Target data is data from the same distribution, i.e. da taset, of the evaluation dataset. One way of improving the representations of a heavily pre-trained la nguage model for the zero-shot setting is to ﬁne-tune it with intermediate data [33]. Such intermediate data contains triplets of query, relevant document, and negative document and can include multiple datasets. The advantage of addin g this step before employing the representations of the language model is to r educe the 8 For example, while the TREC-DL-2020 passage and document retrieval tasks the queries have between 5–6 terms on average and the passages an d documents have over 50 and 1000 terms respectively, for the information-se eking dialogue datasets used here the dialogue contexts (queries) have between 70 an d 474 terms on average depending on the dataset while the responses (documents) ha ve between 11 and 71. 9 See for example the top models in terms of eﬀectiveness from t he MSMarco bench- mark leaderboards https://microsoft.github.io/msmarco/. From Document and Passage Retrieval to Response Retrieval f or Dialogues 7 gap between the pre-training and the downstream task at hand [3 1,26,30]. In Table 2 we clarify the relationship between pre-training, intermediat e training and ﬁne-tuning. Table 2. The diﬀerent training stages and data, their purposes, exam ples of datasets, and the type of dense model obtained after each stage. Pre-training data Intermediate data T arget data Purpose Learn general represen- tations Learn sentence representations for ranking Learn representations for tar- get distribution Model is Zero-shot Zero-shot Fine-tuned Example Wikipedia MSMarco MANtIS The intermediate training step learns to represent pieces of text ( query and documents) by applying a mean pooling function over the transform er’s