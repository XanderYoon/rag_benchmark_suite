a possibility dapper will be delayed [...] mean- while, dapper discussions should occur in ubuntu+1 Denoising techniques try to solve this problem by reducing the numbe r of false negatives. We employ a simple approach that instead of using the top -ranked responses as negative responses, we use the bottom responses of the top-ranked responses as negatives 20. This decreases the chances of obtaining false positives and if k << |D| we will not obtain random samples. Our experiments in Table 5 reveal that this denoising technique, row (3b), increases the eﬀe ctiveness for harder negative samples, beating all models from Table 3 for two of t he three 18 Our experiments show that when we do not employ the intermedi ate training step the ﬁne-tuned dense model does not generalize well, with row (3d ) performance dropping to 0.172, 0.308 and 0.063 R@10 for MANtIS, MSDialog and UDCDSTC8 respectively. 19 The results are not shown here due to space limitations 20 For example, if we retrieve k = 100 responses, instead of using responses from top positions 1–10, we use responses 91–100 from the bottom of th e list. From Document and Passage Retrieval to Response Retrieval f or Dialogues 13 datasets. The results indicate that F5 generalizes to the task of respo nse retrieval for dialogues only when employing a denoising tec hnique. Table 5. Results for the generalizability of F5—with and without a de noising strategy for hard negative sampling. Superscripts indicate statist ically signiﬁcant improvements using Students t-test with Bonferroni correction . †=signiﬁcance against the random sampling baseline, ‡=signiﬁcance against hard negative sampling without denoi sing. MANtIS MSDialog UDC DSTC8 R@10 R@10 R@10 Baseline (1) Bi-encoder Random 0.307 0.387 0.128 Hard negative sampling (2a) Bi-encoder BM 25 0.271 0.316 0.087 (2b) Bi-encoder Bi−encoder 0.146 0.306 0.051