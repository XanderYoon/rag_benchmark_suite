there are more d atasets and previous research—and study how to adapt and improve them for r etrieval-based conversational search. 2 Related Work In this section we ﬁrst discuss current research in retrieval-base d systems for conversational search, followed by reviewing the major ﬁndings of (un)supervised sparse and dense retrieval in the domains of passage and documen t retrieval. 2.1 Ranking and Retrieval of Responses for Dialogues Early neural models for response re-ranking were based on match ing the repre- sentations of the concatenated dialogue context and the repres entation of a re- sponse in a single-turn manner with architectures such as CNN and L STM [23,14]. More complex neural architectures matching each utterance with the response were also explored [54,9,22]. Heavily pre-trained language models such as BERT were ﬁrst shown to be eﬀective by Nogueira and Cho [24] for re-ran king. Such models quickly became a predominant approach for re-ranking in IR [ 21] and were later shown to be eﬀective for re-ranking responses in conve rsations [42,28]. In contrast, the ﬁrst-stage retrieval of responses for a dialog ue received rela- tively little attention [29]. Lan et al. [17] and Tao et al. [38] showed tha t BERT- based dense retrieval models outperform BM25 for ﬁrst-stage r etrieval of re- sponses for dialogues. A limitation of their work is that strong spars e retrieval 3 ✗ indicates that the ﬁnding does not hold in our domain whereas ✓ indicates that it holds in our domain followed by the necessary condition or exception . 4 For example in Table 1 the last utterance is u3. 5 A zero-shot is a model that does not have access to target data , cf. Table 2. 6 Target data is data from the same distribution, i.e. dataset , of the evaluation dataset. 4