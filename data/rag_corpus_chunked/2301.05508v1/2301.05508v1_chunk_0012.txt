obtained after each stage. Pre-training data Intermediate data T arget data Purpose Learn general represen- tations Learn sentence representations for ranking Learn representations for tar- get distribution Model is Zero-shot Zero-shot Fine-tuned Example Wikipedia MSMarco MANtIS The intermediate training step learns to represent pieces of text ( query and documents) by applying a mean pooling function over the transform er’s ﬁnal layer, which is then used to calculate the dot-product similarity. The loss function employs multiple negative texts from the same batch to learn the rep resentations in a constrastive manner, also known as in-batch negative sampling. Such a procedure learns better text representations than a naive appr oach that uses the [CLS ] token representation of BERT [33,2]. The function f (U, r) is then dot(η(concat(U)), η(r)), where η is the repre- sentation obtained by applying the mean pooling function over the las t layer of the transformer model, and concat(U) = u1 | [U ] | u2 | [T ] | ... | uτ , where | indicates the concatenation operation. The utterances from the context U are concatenated with special separator tokens [ U ] and [ T ] indicating end of utter- ances and turns 10. The eﬀectiveness of a zero-shot bi-encoder model in the domain of dialogues is the third ﬁnding we validate. F4: Fine-tuned Dense Retrieval The standard procedure is to ﬁne-tune dense models with target data that comes from the same dataset t hat the model will be evaluated. Since we do not have labeled negative responses, a ll the remain- ing responses in the dataset can be thought of as non-relevant to the dialogue context. Computing the probability of the correct response over all other re- sponses in the dataset would give us P (r | U ) = P (U ,r)