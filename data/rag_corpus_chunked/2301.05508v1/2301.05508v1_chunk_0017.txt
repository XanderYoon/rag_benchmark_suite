not improve over BM25 (1a) on any of the three conversational datasets analyzed . We performed thorough hyperparameter ﬁne-tuning and no combination of the R M3 hyperpa- rameters outperformed BM25. This indicates that F1 does not hold for the task of response retrieval for dialogues. A manual analysis of the new terms appended to a sample of 60 dialogu e contexts by one of the paper’s authors revealed that only 18% of t hem have at least one relevant term added based on our best judgment. Unlike w eb search where the query is often incomplete, under-speciﬁed, and ambiguo us, in the information-seeking datasets employed here the dialogue context (query) is quite detailed and has more terms than the responses (documents). We hypothesize that because the dialogue contexts are already quite descriptive, the task of expansion is trickier in this domain and thus we observe many dialogues for which the added terms are noisy. 14 The alternative models we considered are those listed in the model overview section at https://www.sbert.net/docs/pretrained_models.html. 15 The standard evaluation metric in conversation response ra nking [50,8,39] is recall at position K with n candidates Rn@K. Since we are focused on the ﬁrst-stage retrieval we set n to be the entire collection of answers 10 Gustavo Penha and Claudia Hauﬀ Table 3. Results for the generalizability of F1–F4. Bold values indi cate the high- est recall for each type of approach. Superscripts indicate statistically signiﬁcant im- provements using Students t-test with Bonferroni correcti on. †=other methods from the same group 1=best from unsupervised sparse retrieval ; 2=best from supervised sparse retrieval; 3=best from zero-shot dense retrieval. For example, in F3 † indicates that row (3d) improves over rows (3a–c), 1 indicates that it improves over row (1a) and 2 indicates it improves over row (2b). MANtIS