enough supervision from target data, dense retrieval models have shown to consistently outperform strong sparse baselines [7,15,34] ( F4). 7 A distinction can also be made of cross-encoders and bi-enco ders, where the ﬁrst encode the query and document jointly as opposed to separate ly [40]. Cross-encoders are applied in a re-ranking step due to their ineﬃciency and t hus are not our focus. From Document and Passage Retrieval to Response Retrieval f or Dialogues 5 Context for F5 In order to train neural ranking models, a small set of negative (i.e. non-relevant) candidates are necessary as it is prohibitively ex pensive to use every other document in the collection as negative sample for a q uery. A limitation of randomly selecting negative samples is that they might be t oo easy for the ranking model to discriminate from relevant ones, while for n egative documents that are harder the model might still struggle. For this reason hard negative sampling has been shown to perform better than random s ampling for passage and document retrieval [46,36,51]. 3 First-stage Retrieval for Dialogues In this section we ﬁrst describe the problem of ﬁrst-stage retriev al of responses, followed by the ﬁndings we want to replicate from sparse and dense a pproaches. Problem Deﬁnition The task of ﬁrst-stage retrieval of responses for dialogues, concerns retrieving the best response out of the entire collection given the di- alogue context. Formally, let D = {(Ui, Ri, Yi)}M i=1 be a data set consisting of M triplets: dialogue context, response candidates and response re levance labels. The dialogue context Ui is composed of the previous utterances {u1, u2, ..., uτ } at the turn τ of the dialogue. The candidate responses Ri = {r1, r2, ..., rn} are either ground-truth responses r+ or negative sampled candidates