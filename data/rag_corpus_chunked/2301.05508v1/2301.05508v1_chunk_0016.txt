negatives, and also uses the remaining in-ba tch random negatives to train the model. We use a total of 10 negative samples f or dialogue context. We ﬁne-tune the dense models for a total of 10k steps, and every 100 steps we evaluate the models on a re-ranking task that selects the relevant re- sponse out of 10 responses. We use the re-ranking validation MAP t o select the best model from the whole training to use in evaluation. We use a batc h size of 5, with 10% of the training steps as warmup steps. The learning rate is 2e-5 and the weight decay is 0.01. We use FAISS [13] to perform the similarity search. Evaluation To evaluate the eﬀectiveness of the retrieval systems we use R@K. We thus evaluate the models’ capacity of ﬁnding the correct respo nse out of the whole possible set of responses 15. We perform Students t-tests at the 0.95 conﬁdence level with Bonferroni correction to compare statistic al signiﬁcance of methods. Comparisons are performed across the results for eac h dialogue context. 5 Results In this section, we discuss our empirical results along with the ﬁve ma jor ﬁnd- ings from previous work (Section 1) in turn. Table 3 contains the main results regarding F1 to F4. Table 5 contains the results for F5. F1 ✗ Query expansion via RM3 leads to improvements over not using query expansion [1,18,49,21]. BM25+RM3 (row 1b) does not improve over BM25 (1a) on any of the three conversational datasets analyzed . We performed thorough hyperparameter ﬁne-tuning and no combination of the R M3 hyperpa- rameters outperformed BM25. This indicates that F1 does not hold for the task of response retrieval for dialogues. A manual analysis of the new terms appended to a sample of 60 dialogu e