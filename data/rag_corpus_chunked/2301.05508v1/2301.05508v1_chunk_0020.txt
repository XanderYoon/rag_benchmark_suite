v s 30% are changing the weights by adding existing words), generating overall smaller aug- mentations (115.45 vs 431.17 on average respectively). F3 ✓ Sparse retrieval outperforms zero-shot dense retrieval [3 4,41]. Sparse retrieval models are more eﬀective than the majority of ze ro-shot dense models, as shown by the comparison of rows (1a–b), and (2a–b) wit h rows (3a– c). However, a dense retrieval model that has gone through inte rmediate training on large and diverse datasets including dialogues is more eﬀective tha n a strong sparse retrieval model, as we see by comparing row (3d) with row (2 b) in Table 3. For example, while the zero-shot dense retrieval models based only on the MSMarco dataset (3a–b) perform on average 35% worse than the strong s parse baseline (2b) in terms of R@10 for the MSDialog dataset, the zero-shot model trained with 1.17B instances on diverse data (3d) is 68% better than the sparse baseline (2b). When using a bigger amount of intermediate training da ta17, we see that the zero-shot dense retrieval model (3d) is able to outp erform the sparse retrieval baseline by margins of 33% of R@10 on average across dat asets. We thus show that F3 only generalizes to response retrieval f or dialogues if we do not employ a large set of diverse intermedi ate data. As expected, the closer the intermediate training data distribution is to the eval- uation data, the better the dense retrieval model performs. Th e results indicate that a good zero-shot retrieval model needs to go through inter mediate training on a large set of training data coming from multiple datasets to gener alize well to diﬀerent domains and outperform strong sparse retrieval bas elines. F4 ✓ Dense models with access to target training data outperform sparse models