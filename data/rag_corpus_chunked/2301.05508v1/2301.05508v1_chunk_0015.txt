products; MANtIS [27] contains 1.3 million context-response pairs built from con- versations of 14 Stack Exchange sites, such as askubuntu and travel; UDCDSTC8 [16] contains 184k context-response pairs of disentangled Ubuntu IR C dialogues. Implementation Details For BM25 and BM25+RM313 we rely on the pyserini implementations [20]. In order to train resp2ctxt expansion method s we rely on the Huggingface transformers library [44], using the t5-base model. We ﬁne- tune the T5 model for 2 epochs, with a learning rate of 2e-5, weight decay of 0.01, and batch size of 5. When augmenting the responses with resp2ctx t we follow docT5query [25] and append three diﬀerent context predictions, using sampling and keeping the top-10 highest probability vocabulary tokens. For the zero-shot dense models, we rely on the SentenceTransformers [33] model releases. The library uses Hugginface’s transformers for the pre-trained models such as BERT [4] and MPNet [37]. For the bi-encoder models, we use the 12 MSDialog is available at https://ciir.cs.umass.edu/downloads/msdialog/; MANtIS is available at https://guzpenha.github.io/MANtIS/; UDCDSTC8 is available at https://github.com/dstc8-track2/NOESIS-II . 13 We perform hyperparameter tuning using grid search on the nu mber of expansion terms, number of expansion documents, and weight. From Document and Passage Retrieval to Response Retrieval f or Dialogues 9 pre-trained all-mpnet-base-v2 weights which were the most eﬀective in our initial experiments, compared with other pre-trained models 14. When ﬁne-tuning the dense retrieval models, we rely on the MultipleNegativesRankingLoss, which ac- cepts a number of hard negatives, and also uses the remaining in-ba tch random negatives to train the model. We use a total of 10 negative samples f or dialogue context. We ﬁne-tune the dense models for a total of 10k steps, and every 100 steps we evaluate the models on a re-ranking task that selects the relevant re- sponse out of 10 responses. We