dialogue contexts and responses. For this reason, we here translate a query expan sion technique to the dialogue domain and perform dialogue context expansion with RM3 [1], a competitive unsupervised method that assumes that the top-ra nked responses by the sparse retrieval model are relevant. From these pseudo- relevant responses, words are selected and an expanded dialogue context is created an d subsequently employed by the sparse retrieval method to rank the ﬁnal list of re sponses. The eﬀectiveness of RM3 in the domain of dialogues is the ﬁrst ﬁnd ing that we validate . 6 Gustavo Penha and Claudia Hauﬀ F2: Learned Sparse Retrieval Alternatively, we can expand the responses in the collection with a learned method. To do so we “translate” doc2qu ery [25] into our domain, yielding resp2ctxt. Formally, we ﬁne-tune a generative transformer model G for the task of generating the dialogue context Ui from the ground-truth response r+ i . This model is then used to generate expansions for all responses in the collection, ri = concat(ri, G(ri)). These expansions are appended to the responses and the collection is indexed again—the sparse retrieval method it- self is not modiﬁed, i.e. we continue using BM25. This approach (which w e coin resp2ctxt) leads to two improvements: term re-weighting (adding terms that already exist in the document) and dealing with the vocabulary mismat ch prob- lem (adding new terms). The eﬀectiveness of doc2query in the domain of dialogues is the second ﬁnding that we validate . Unlike passage and document retrieval where the queries are smalle r than the documents, for the retrieval of responses for dialogues the que ries are longer than the documents 8. This is a challenge for the generative model, since generating larger pieces of text is a more diﬃcult problem than smaller