that it holds in our domain followed by the necessary condition or exception . 4 For example in Table 1 the last utterance is u3. 5 A zero-shot is a model that does not have access to target data , cf. Table 2. 6 Target data is data from the same distribution, i.e. dataset , of the evaluation dataset. 4 Gustavo Penha and Claudia Hauﬀ baselines that have shown to be eﬀective in other retrieval tasks, e.g. BM25 with dialogue context expansion [25] or BM25 with response expansion [49], were not employed for dense retrieval. We do such comparisons here and tes t a total of ﬁve major ﬁndings that have been not been evaluated before by previo us literature on the ﬁrst-stage retrieval of responses for dialogues. 2.2 Dense and Sparse Models for Passage and Document Retriev al Context for F1 Retrieval models can be categorized into two dimensions: su- pervised vs. unsupervised and dense vs. sparse representation s [19]. An unsuper- vised sparse representation model such as BM25 [35] represents eac h document and query with a sparse vector with the dimension of the collection’s v ocabu- lary, having many zero weights due to non-occurring terms. Since t he weights of each term are entirely based on term statistics they are considere d unsupervised methods. Such approaches are prone to the vocabulary mismatch problem [6], as semantic matches are not considered. A way to address such a p roblem is by using query expansion methods. RM3 [1] is a competitive [49] query ex pansion technique that uses pseudo-relevance feedback to add new term s to the queries followed by another ﬁnal retrieval step using the modiﬁed query. Context for F2 A supervised sparse retrieval model can take advantage of the eﬀectiveness of transformer-based language models by chan ging