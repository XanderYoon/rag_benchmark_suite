of monoT5, the most eﬀective model in our experiments, for diﬀerent training set sizes, tested only on topics with leaked queries. For small train- ing sets, monoT5 achieves rather low nDCG@10 values and cannot exploit the leakage. The nDCG@10 increases with more training data on all benchmarks, peaking at 16,000 or 32,000 instances. At the peaks, monoT5 trained with leak- age is more eﬀective than without, and training on test leakage leads to a slightly higher nDCG@10 than leakage from MS MARCO / ORCAS (MSM). However, the diﬀerence between test and MSM leakage is larger for Robust04 (with some documents published as early as 1989) compared to the newer Common Core tracks(withdocumentspublishedclosertothepublicationdateofMSMARCO). On the Common Core data, MSM leakage is almost as eﬀective as test leakage. Leakage-Induced Eﬀectiveness Improvements for Other Models.We employ a ﬁve-fold cross-validation setup for Duet, KNRM, monoBERT, monoT5, and PACRR to study whether leakage-induced eﬀectiveness improvements can also be observed for other models when a grid search in the cross-validation setup can choose the training set size with the highest leakage eﬀect for each model. We report the eﬀectiveness of the models as nDCG@10, Precision@1, and the 8 Fröbe et al. T able 3. Eﬀectiveness on Robust04 (R04) as nDCG@10, mean ﬁrst rank of a rel- evant document (MFR), and Precision@1 (Prec@1) in a ﬁve-fold cross-validation setup on all test topics. Models are trained with no leakage (No), leakage from MS MARCO / ORCAS (MSM), or leakage from the test data (Test). Highest scores in bold;† marks Bonferroni-corrected signiﬁcant diﬀerences to the no-leakage baseline (Student’s t-test,p = 0.05). Model order swaps induced by MSM leakage in red. Model nDCG@10 on R04 MFR on R04 Prec@1 on R04 No MSM Test No MSM Test No MSM Test Duet [33] 0.201 0.198 0.224† 2.420 2.682 2.340 0.297 0.261