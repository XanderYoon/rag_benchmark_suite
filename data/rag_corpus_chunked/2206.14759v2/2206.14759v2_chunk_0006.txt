BERT [11] is not suited for paraphrase detection—both, with respect to eﬃciency and accuracy. Sentence-BERT [38] solves this issue by adopting a BERT-based triplet network structure and a con- trastive loss that attempts to learn a global and a local structure suited for detecting semantically related sentences. We therefore use Sentence-BERT in a version speciﬁcally trained for paraphrase detection to identify leaking queries. 3 Identifying Leaking Queries To examine the impact of possible leaks from MS MARCO / ORCAS to the TREC datasets Robust04 and Common Core 2017 and 2018, we compare the for- mer’s queries (367,013 plus 10 million) to the 275 topics of the latter three. Since lexical similarity may not be suﬃcient, as indicated by the ELI5 issue [24] men- tioned above, we compute semantic similarity scores using Sentence-BERT [38].8 WestoretheSentence-BERTembeddingsofallMSMARCOandORCASqueries in two Faiss indexes [23] and query them for the 100 nearest neighbors (exact; cosine similarity) of each topic’s title, description, and query variants. To determine the threshold for the Sentence-BERT similarity score beyond which we consider a query a source of leakage for a topic, one human annotator assessed whether a query is leaking for a TREC topic (title, description, query variants)forastratiﬁedsampleof100pairsofqueriesandtopicswithasimilarity above 0.8. Against these manual judgments, a similarity threshold of 0.91 is the lowest that yields a 0.9 precision for deciding that a query is leaking for a topic. Table 1 shows the number of topics for which queries above this threshold can be found. From MS MARCO and ORCAS combined, 3,960 queries are leakage candidates for one of 181 Robust04 topics (72% of the 250 topics). From the two Common Core tracks, 37 and 38 topics have leakage candidates (76% of the 50 topics, respectively)—high similarities mostly against the query variants. 7 https://trec.nist.gov/data/wapost/ 8 Of the available pre-trained Sentence-BERT models, we use