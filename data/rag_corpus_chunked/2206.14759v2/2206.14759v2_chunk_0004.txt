2018 Common Core tracks [1] intentionally reused topics from Robust04 to allow participants to use the relevance judgments for training. Indeed, approaches trained on the Robust04 judgments were more eﬀective than others [1]. In this paper, we study whether a similar eﬀect can be observed for unintentional leakage from the large MS MARCO and ORCAS datasets. Training retrieval models on MS MARCO and applying them to another corpus is a form of transfer learning [34]. Transfer learning is susceptible to train–test leakage since the train and test data are often generated indepen- dently without precautions to prevent leaks [6]. Research on leakage in transfer learning focuses on membership inference [35, 41] (predicting if a model has seen an instance during training) and property inference [2, 17] (predicting properties of the training data). Both inferences rely on the observation that neural models may memorize some training instances to generalize through interpolation [5, 7] and to similar test instances [15, 16]. It is unclear whether and how neural re- trieval models in a transfer learning scenario are aﬀected by leakage. Memorized relevant instances might reduce eﬀectiveness for diﬀerent test queries while im- proving it for similar queries, like the examples in Figure 1. We take the ﬁrst steps to investigate the eﬀects of such a train–test leakage. When the target corpus contains only few training instances, transferred retrieval models are often more eﬀective without ﬁne-tuning, in a zero-shot set- ting [47]; for instance, when training on MS MARCO and testing on TREC datasets [34, 36, 47]. A frequently used target TREC dataset is Robust04 [42] with 250 topics and a collection of 528,155 documents published between 1989 and 1996 by the Financial Times, the Federal Register, the Foreign Broad- cast Information Service, and the LA Times.5 Later, the TREC Common Core track