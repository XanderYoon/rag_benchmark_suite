be found. From MS MARCO and ORCAS combined, 3,960 queries are leakage candidates for one of 181 Robust04 topics (72% of the 250 topics). From the two Common Core tracks, 37 and 38 topics have leakage candidates (76% of the 50 topics, respectively)—high similarities mostly against the query variants. 7 https://trec.nist.gov/data/wapost/ 8 Of the available pre-trained Sentence-BERT models, we use the para- phrase detection model: https://huggingface.co/sentence-transformers/ paraphrase-MiniLM-L6-v2 How Train–Test Leakage Aﬀects Zero-shot Retrieval 5 T able 1. Number of topics (T) in Robust04 and the TREC 2017 and 2018 Common Core tracks for which similar queries (number as Q) in MS MARCO (MSM) and the union of MSM and ORCAS (+ORC) exist in terms of the query having a Sentence- BERT score > 0.91 against the topic’s title, description, or a query variant. Candidates Robust04 Core 2017 Core 2018 MSM + ORC MSM + ORC MSM + ORC T Q T Q T Q T Q T Q T Q Title 33 83 140 1,775 2 12 23 176 2 2 21 110 Description 2 3 8 50 0 0 0 0 0 0 1 2 Variants 45 116 167 3,356 6 16 38 602 9 26 38 973 Union 53 151 181 3,960 7 18 38 645 9 26 38 973 Some of these leakage candidates still are false positives (threshold precision of 0.9). To only use actual leaking queries in our train–test leakage experiments, two annotators manually reviewed the 5 most similar candidates per topic above the 0.91 threshold (a total of 827 candidates; not all topics had 5 candidates). Given the title, description, and narrative of a topic, the annotators labeled the similarity of a query to the topic title according to Jansen et al.’s reformula- tion types [22]: a query can beidentical to the topic title