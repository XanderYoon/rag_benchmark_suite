leakage from the test data (Test). Highest scores in bold;† marks Bonferroni-corrected signiﬁcant diﬀerences to the no-leakage baseline (Student’s t-test,p = 0.05). Model order swaps induced by MSM leakage in red. Model nDCG@10 on R04 MFR on R04 Prec@1 on R04 No MSM Test No MSM Test No MSM Test Duet [33] 0.201 0.198 0.224† 2.420 2.682 2.340 0.297 0.261 0.304 KNRM [44] 0.194 0.214 † 0.309† 2.348 2.309 1.976† 0.293 0.313 0.329 monoBERT [37] 0.394 0.373 † 0.396 1.688 1.725 1.639 0.434 0.454 0.414 monoT5 [36] 0.461 0.457 0.478† 1.443 1.416 1.417 0.562 0.578 0.590 PACRR [21] 0.382 0.364 † 0.391 1.663 1.604 1.579† 0.458 0.462 0.502 mean ﬁrst rank of a relevant document (MFR) [18].9 While eﬀectiveness scores measured via nDCG@10 and Precision@1 have the property that higher values are better (a score of 1 indicates “best” eﬀectiveness), for MFR, lower scores are better—but still a score of 1 is the best case indicating that the document on rank 1 always is relevant. In all eﬀectiveness evaluations, we conduct signiﬁ- cance tests via Student’s t-test (p = 0.05) with Bonferroni correction for multiple testing as implemented in PyTerrier [32]. Table 3 shows the ﬁve-fold cross-validated eﬀectiveness on Robust04 for the ﬁve models when optimizing each fold for nDCG@10, MFR, or Precision@1 in a grid search. Models trained on test leakage almost always are more eﬀective than their no-leakage counterparts (exception: Precision@1 of monoBERT) and actual test leakage usually helps more than leakage from MS MARCO / ORCAS (MSM; exceptions: MFR of monoT5 and Precision@1 of monoBERT). Overall, on Robust04, models trained with MSM leakage are often less eﬀective than their no-leakage counterparts (e.g., the nDCG@10 of monoBERT). A possible expla- nation might be the large time gap between the Robust04 document publication dates (documents published between 1987 and