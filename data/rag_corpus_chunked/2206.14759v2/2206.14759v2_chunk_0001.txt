MS MARCO data, is currently the most eﬀective model for the Ro- bust04 document ranking task.3 Furthermore, the reference implementations of monoT5 and monoBERT [37] in retrieval frameworks such as PyTerrier [32] or Pyserini / PyGaggle [26] all use models trained only on MS MARCO by default. However, when MS MARCO was oﬃcially split into train and test data, cross- benchmark use was not anticipated, so that MS MARCO’s training queries may overlap with the test queries of other much smaller datasets (e.g., Robust04). In this paper, we investigate the impact of such a train–test leakage by training neural models on MS MARCO document ranking data with diﬀerent propor- tions of controlled leakage to Robust04 and the TREC 2017 and 2018 Common Core tracks as test datasets. 3 https://paperswithcode.com/sota/ad-hoc-information-retrieval-on-trec-robust04 arXiv:2206.14759v2 [cs.IR] 30 Aug 2022 2 Fröbe et al. Robust04 Topic 441 Title: lyme disease Description: How do you prevent and treat Lyme disease? Narrative: Documents that discuss current prevention and treatment techniques for Lyme disease are relevant. Reports of research on new treatments of the disease are also relevant. Query variants: lyme disease treatments prevent lyme disease ... MS MARCO + ORCAS lyme disease how to treat lyme disease how to prevent lyme disease lyme disease treatment prevent lyme disease ...... 0.95 0.95 1.0 1.0 1.0 SBERT Figure 1. MS MARCO / ORCAS queries with high Sentence-BERT (SBERT) simi- larity to Robust04 Topic 441. To identify probably leaking queries, we run a semantic nearest-neighbor search using Sentence-BERT [38] and compare each MS MARCO / ORCAS query to the title, description, and manual query variants [3, 4] of the topics in Robust04 and the TREC 2017 and 2018 Common Core tracks. Figure 1 illus- trates this procedure for Topic 441 (lyme disease) from Robust04. Our manual review of the leakage candidates