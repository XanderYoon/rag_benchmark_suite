K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for language understanding. In: Proc. of NAACL 2019, pp. 4171–4186, Association for Computational Linguistics, Minneapolis, Minnesota (2019) 14 Fröbe et al. [12] Dolan, W.B., Brockett, C.: Automatically constructing a corpus of senten- tial paraphrases. In: Proc. of the Third International Workshop on Para- phrasing (IWP 2005) (2005) [13] Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., Auli, M.: ELI5: Long form question answering. In: Proc. of ACL 2019, pp. 3558–3567, ACL (2019) [14] Fan, Y., Guo, J., Lan, Y., Xu, J., Zhai, C., Cheng, X.: Modeling diverse relevance patterns in ad-hoc retrieval. In: Proc. of SIGIR 2018, pp. 375– 384, ACM (2018) [15] Feldman, V.: Does learning require memorization? A short tale about a long tail. In: Proc. of STOC 2020, pp. 954–959, ACM (2020) [16] Feldman, V., Zhang, C.: What neural networks memorize and why: Dis- covering the long tail via inﬂuence estimation. In: Proc. of NeurIPS 2020 (2020) [17] Fredrikson, M., Jha, S., Ristenpart, T.: Model inversion attacks that exploit conﬁdence information and basic countermeasures. In: Proc. of CCS 2015, pp. 1322–1333, ACM (2015) [18] Fuhr, N.: Some common mistakes in IR evaluation, and how they can be avoided. SIGIR Forum51(3), 32–41 (2017) [19] He, H., Garcia, E.: Learning from imbalanced data. IEEE Trans. Knowl. Data Eng. 21(9), 1263–1284 (2009) [20] Hofstätter, S., Zlabinger, M., Hanbury, A.: Interpretable & time-budget- constrained contextualization for re-ranking. In: Proc. of ECAI 2020, Fron- tiers in Artiﬁcial Intelligence and Applications, vol. 325, pp. 513–520, IOS Press (2020) [21] Hui, K., Yates, A., Berberich, K., Melo, G.: PACRR: A position-aware neu- ral IR model for relevance matching. In: Proc. of EMNLP 2017, pp. 1049– 1058, ACL (2017) [22] Jansen, B., Booth, D., Spink, A.: Patterns of query reformulation during web searching. J.