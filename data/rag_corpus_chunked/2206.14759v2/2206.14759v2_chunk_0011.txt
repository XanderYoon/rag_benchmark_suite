single large one. Since the training is not deterministic, How Train–Test Leakage Aﬀects Zero-shot Retrieval 7 Training Instances 0.1 0.2 0.3 0.4 0.5nDCG@10 Robust04 1k 2k 4k 8k 16k 32k 64k 128k Training Instances 1k 2k 4k 8k 16k 32k 64k 128k Common Core 2017 Training Instances 1k 2k 4k 8k 16k 32k 64k 128k Common Core 2018 Training Dataset Test Leakage MSM Leakage No Leakage Figure 2. Eﬀectiveness of monoT5 measured as nDCG@10 on the topics with leakage (172 topics for Robust04, 37 and 38 for the 2017 and 2018 editions of the Common Core track). Models trained on datasets of varying size with no leakage (No), leakage from MS MARCO / ORCAS (MSM), or leakage from the test data (Test). each model is trained on each of the 72 training sets ﬁve times for one epoch with varying seeds (used to shuﬄe the training queries; conﬁgured in PyTorch). We useir_datasets [31] for data wrangling and, following previously suggested training regimes [36, 37, 45], pass the relevant and the non-relevant document of a query consecutively to a model in the same batch during training. Dur- ing inference, all models re-rank the top-100 BM25 results (Capreolus, default conﬁguration) and we break potential score ties in rankings via alphanumeric ordering by document ID (with random IDs, this leads to a random distribution for other document properties such as text length [27]). Leakage-Induced nDCG Improvements for MonoT5.Figure 2 shows the average nDCG@10 of monoT5, the most eﬀective model in our experiments, for diﬀerent training set sizes, tested only on topics with leaked queries. For small train- ing sets, monoT5 achieves rather low nDCG@10 values and cannot exploit the leakage. The nDCG@10 increases with more training data on all benchmarks, peaking at 16,000 or 32,000 instances. At the peaks, monoT5 trained with leak-