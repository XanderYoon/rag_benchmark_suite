train–test leakage from MS MARCO / OR- CAS to Robust04 and TREC 2017 and 2018 Common Core. We analyze the models’ eﬀectiveness in ﬁve-fold cross-validation experiments, report detailed results for varying training set sizes for monoT5 (which has the highest eﬀec- 6 Fröbe et al. T able 2.Statistics of the 827 manually annotated leakage candidate queries. (a) Num- ber of true and false candidates. (b) Annotated query reformulation types. (a) Manually annotated candidates. Corpus Candidates Queries T opics Robust04 true 648 172 false 93 53 Core 2017 true 138 37 false 21 11 Core 2018 true 157 38 false 19 7 (b) Reformulation types. Type Queries Identical 187 Generalization 124 Specialization 228 Reformulation 182 Diﬀerent Topic 106 tiveness in our experiments), and study the eﬀects of leaked instances on the retrieval scores and the resulting rankings. Training Datasets. For each of the three test scenarios (Robust04 and the two Common Core scenarios), we construct three types of training datasets: (1) ‘No Leakage’ with random non-leaking queries (balanced between MS MARCO and ORCAS as in previous experiments [8]), (2) ‘MSM Leakage’ with a ﬁxed num- ber of random manually veriﬁed leaking queries from MS MARCO / ORCAS (500 queries for Robust04, 100 queries for Common Core) supplemented by no- leakage queries till a desired size is reached, and (3) ‘Test Leakage’ with a ﬁxed number of queries from the actual test data (500 for Robust04, 100 for Common Core; oversampling: each topic twice (but diﬀerent documents) to match ‘MSM Leakage’) supplemented by no-leakage queries till a desired size is reached. ‘Test Leakage’ is meant as an “upper bound” for any train–test leakage eﬀect. For each type, we construct datasets with 1,000 to 128,000 instances (500 to 64,000 queries; each with one relevant and one non-relevant document). Since MS MARCO / ORCAS