How Train–Test Leakage Aﬀects Zero-shot Retrieval Maik Fröbe,1 Christopher Akiki,2 Martin Potthast,2 Matthias Hagen1 1 Martin-Luther-Universität Halle-Wittenberg 2 Leipzig University Abstract Neural retrieval models are often trained on (subsets of) the millionsofqueriesofthe MSMARCO/ORCASdatasetsandthentested on the 250 Robust04 queries or other TREC benchmarks with often only 50 queries. In such setups, many of the few test queries can be very similar to queries from the huge training data—in fact, 69% of the Ro- bust04 queries have near-duplicates in MS MARCO / ORCAS. We inves- tigate the impact of this unintended train–test leakage by training neural retrievalmodelsoncombinationsofaﬁxednumberofMSMARCO/OR- CAS queries that are highly similar to the actual test queries and an increasing number of other queries. We ﬁnd that leakage can improve eﬀectiveness and even change the ranking of systems. However, these ef- fects diminish the smaller and, thus, the more realistic the amount of leakage is among all training instances. Keywords: Neural information retrieval; Train–test leakage; BERT; T5 1 Introduction Training transformer-based retrieval models requires large amounts of data un- available in many traditional retrieval benchmarks [34]. Data-hungry training regimes became possible with the 2019 release of MS MARCO [10] and its 367,013 queries that were subsequently enriched by the ORCAS click log [8] with 10 million queries. Fine-tuning models trained on MS MARCO to other benchmarks or using them without ﬁne-tuning in zero-shot scenarios is often very eﬀective [34, 36, 47]. For example, monoT5 [36], which has been trained only on MS MARCO data, is currently the most eﬀective model for the Ro- bust04 document ranking task.3 Furthermore, the reference implementations of monoT5 and monoBERT [37] in retrieval frameworks such as PyTerrier [32] or Pyserini / PyGaggle [26] all use models trained only on MS MARCO by default. However, when MS MARCO was oﬃcially split into train and test data, cross-