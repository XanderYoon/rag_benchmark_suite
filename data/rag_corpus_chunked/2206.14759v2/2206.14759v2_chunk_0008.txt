the 5 most similar candidates per topic above the 0.91 threshold (a total of 827 candidates; not all topics had 5 candidates). Given the title, description, and narrative of a topic, the annotators labeled the similarity of a query to the topic title according to Jansen et al.’s reformula- tion types [22]: a query can beidentical to the topic title (diﬀerences only in inﬂection or word order), be ageneralization (subset of words), aspecialization (superset of words), areformulation (some synonymous terms), or it can be on a diﬀerent topic. An initial kappa test on 103 random of the 827 candidates showed moderate agreement (Cohen’s kappa of 0.59; 103 queries: we aimed for 100 but included all queries for a topic when one was selected). After discussing the 103 cases with the annotators, they agreed on all cases and we had them each independently label half of the remaining 724 candidates. Table 2 shows the annotation results: 172 topics of Robust04 (i.e., 69%) have manually veriﬁed leaking queries (648 total), as well as 37 topics of Common Core 2017 (74%) and 38 of Common Core 2018 (76%). A large portion of the true-positive leaking queries are identical to or specializations of a topic’s title (57.5% of 721). In our below train–test leakage experiments, we only use manually veriﬁed true-positive leaking queries as the source of leakage from MS MARCO / ORCAS. 4 Experimental Analysis Focusingonzero-shotsettings,wetrainneuralretrievalmodelsonspeciﬁcallyde- signed datasets to assess the eﬀect of train–test leakage from MS MARCO / OR- CAS to Robust04 and TREC 2017 and 2018 Common Core. We analyze the models’ eﬀectiveness in ﬁve-fold cross-validation experiments, report detailed results for varying training set sizes for monoT5 (which has the highest eﬀec- 6 Fröbe et al. T able 2.Statistics of the 827 manually annotated leakage candidate queries. (a) Num- ber of