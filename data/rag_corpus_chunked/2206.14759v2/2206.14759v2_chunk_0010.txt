topic twice (but diﬀerent documents) to match ‘MSM Leakage’) supplemented by no-leakage queries till a desired size is reached. ‘Test Leakage’ is meant as an “upper bound” for any train–test leakage eﬀect. For each type, we construct datasets with 1,000 to 128,000 instances (500 to 64,000 queries; each with one relevant and one non-relevant document). Since MS MARCO / ORCAS queries only have annotated relevant documents, we follow Nogueira et al. [36] and sample “non-relevant” instances from the top- 100 BM25 results for such queries. For the ‘Test Leakage’ scenario, we use the actualTRECjudgmentstosamplethenon-/relevantinstances.Inour72training datasets (3 test scenarios, 3 types, 8 sizes), the number of leaked instances is held constant to analyze the eﬀect of a decreasing (and thus more realistic) ratio of leakage. Larger training data would result in more costly training, but our chosen sizes already suﬃce to observe a diminishing eﬀect of train–test leakage. Trained Models.For our experimental analyses, we use models based on mono- BERT [37] and monoT5 [36] as implemented in PyGaggle [26], and models based on Duet [33], KNRM [44], and PACRR [21] as implemented in Capreolus [45] (default conﬁgurations each). In pilot experiments with 32,000 ‘No Leakage’ in- stances, these models had higher nDCG@10 scores on Robust04 than CEDR [30], HINT [14], PARADE [25], and TK [20]. Following Nogueira et al. [36], we use the base versions of BERT and T5 to spend the computational budget on training many models instead of a single large one. Since the training is not deterministic, How Train–Test Leakage Aﬀects Zero-shot Retrieval 7 Training Instances 0.1 0.2 0.3 0.4 0.5nDCG@10 Robust04 1k 2k 4k 8k 16k 32k 64k 128k Training Instances 1k 2k 4k 8k 16k 32k 64k 128k Common Core 2017 Training Instances 1k 2k 4k 8k 16k 32k 64k 128k Common Core 2018 Training Dataset