instance, when training on MS MARCO and testing on TREC datasets [34, 36, 47]. A frequently used target TREC dataset is Robust04 [42] with 250 topics and a collection of 528,155 documents published between 1989 and 1996 by the Financial Times, the Federal Register, the Foreign Broad- cast Information Service, and the LA Times.5 Later, the TREC Common Core track 2017 [1] reused 50 of the 250 Robust04 topics on the New York Times An- notated Corpus [39]6 (1,864,661 documents published between 1987 and 2007) 5 https://trec.nist.gov/data/cd45/index.html 6 https://catalog.ldc.upenn.edu/LDC2008T19 4 Fröbe et al. and the Common Core track 2018 reused another 25 Robust04 topics (and in- troduced 25 new topics) on the Washington Post Corpus7 (595,037 documents published between 2012 and 2017). A total of 311,410 relevance judgments were collected for the Robust04 topics, 30,030 for the TREC 2017 Common Core track, and 26,233 for the TREC 2018 Common Core track. Interestingly, every Robust04 topic and every topic from the Common Core tracks 2017 and 2018 was augmented with at least eight query variants compiled by expert searchers, and made available as an additional resource [3, 4]. Research on paraphrase detection [12, 43] and semantic question match- ing [40] is of great relevance to the identiﬁcation of potentially leaking queries between training and test data. Reimers and Gurevych [38] and Lin et al. [28] showed that pooling or averaging the output of contextual word embeddings of pre-trained transformer encoders like BERT [11] is not suited for paraphrase detection—both, with respect to eﬃciency and accuracy. Sentence-BERT [38] solves this issue by adopting a BERT-based triplet network structure and a con- trastive loss that attempts to learn a global and a local structure suited for detecting semantically related sentences. We therefore use Sentence-BERT in a version speciﬁcally trained for paraphrase detection to