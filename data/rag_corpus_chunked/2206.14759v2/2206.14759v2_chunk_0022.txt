neural models. At ﬁrst glance, this overlap might seem alarming since train–test leakage is known to invalidate experimental evaluations. We thus analyzed train–test leakage eﬀects for ﬁve neural models (Duet, KNRM, monoBERT, monoT5, and PACRR) in scenarios with diﬀerent amounts of leakage. While our experiments show leakage-induced eﬀectiveness improvements that may even lead to swaps in model ranking, our overall results are reassuring: the eﬀects on nDCG@10 are rather small and not signiﬁcant in mostcases.Theyalsobecomesmallerthesmaller(andmorerealistic)theamount of leakage among all training instances is. Still, even if only a few nDCG@10 How Train–Test Leakage Aﬀects Zero-shot Retrieval 13 diﬀerences were signiﬁcant, we noticed a memorization eﬀect: the rank oﬀset between leaked relevant and non-relevant documents increased on all scenarios. Train–test leakage should thus still be avoided in academic experiments but the practical consequences for real search engines might be diﬀerent. The ob- served increased rank oﬀset might be a highly desirable eﬀect when presuming that queries already seen during training are submitted again after a model has been deployed to production. An interesting direction for future research is to enlarge our experiments to investigate more of the few cases where train–test leakage slightly reduced the eﬀectiveness. Bibliography [1] Allan, J., Harman, D., Kanoulas, E., Li, D., Gysel, C., Voorhees, E.: TREC 2017 Common Core track overview. In: Proc. of TREC 2017, vol. 500-324, NIST (2017) [2] Ateniese, G., Mancini, L., Spognardi, A., Villani, A., Vitali, D., Felici, G.: Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classiﬁers. Int. J. Secur. Networks10(3), 137–150 (2015) [3] Benham, R., Gallagher, L., Mackenzie, J., Damessie, T., Chen, R., Scholer, F., Moﬀat, A., Culpepper, J.: RMIT at the 2017 TREC CORE track. In: Proc. of TREC 2017, NIST Special Publication, vol. 500-324, NIST (2017) [4] Benham, R., Gallagher, L., Mackenzie, J.,