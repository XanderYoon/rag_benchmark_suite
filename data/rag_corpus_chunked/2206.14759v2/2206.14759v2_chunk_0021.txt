0.076 ±17.19 0.369 ±20.04 3.886 ±20.39 0.980 ±17.44 3.497 ±25.98 monoT5 0.443 ±8.60 0.390 ±9.28 0.789 ±9.91 3.443 ±19.96 2.242 ±9.84 1.819 ±10.98 PACRR 0.043 ±19.30 0.764 ±10.93 0.452 ±12.38 1.952 ±17.71 0.271 ±16.96 0.753 ±14.16 Table 8 shows the macro-averaged increase in the rank diﬀerence of the leaked relevant and non-relevant documents between models trained with and without leakage. The leakage increases the rank oﬀset for all ﬁve analyzed models (e.g., 6.4 ranks for Duet on Robust04 with MSM leakage). Interestingly, an in-depth inspection showed that most of the increased diﬀerences are caused by lowered ranks of the leaked non-relevant documents (e.g., 2 ranks lower for monoT5) while the leaked relevant documents improve their ranks only slightly (e.g., 0.3 ranks higher for monoT5). Discussion. Overall, our results in Tables 6–8 indicate that memorization hap- pens but has little impact. Larger memorization eﬀects might be desirable in practical scenarios where a retrieval system that memorizes good results can simply present them when the same query is submitted again. However, for empirical evaluations in scientiﬁc publications or at shared tasks, (unintended) leakage memorization at a larger scale might still lead to unwanted outcomes. 5 Conclusion Our study of train–test leakage eﬀects for neural retrieval models was inspired by the observation that 69% of the Robust04 topics, a dataset often used to test neural models, have very similar queries in the MS MARCO / ORCAS datasets, that are often used to train neural models. At ﬁrst glance, this overlap might seem alarming since train–test leakage is known to invalidate experimental evaluations. We thus analyzed train–test leakage eﬀects for ﬁve neural models (Duet, KNRM, monoBERT, monoT5, and PACRR) in scenarios with diﬀerent amounts of leakage. While our experiments show leakage-induced eﬀectiveness improvements that may even lead to swaps in model ranking, our overall