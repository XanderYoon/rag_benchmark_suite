the training data becomes smaller and more representative of realistic training scenarios. Nonetheless, our experiments on the eﬀects of leaked instances on search results and the resulting system rankings show that leakage eﬀects occur even when improvementsineﬀectivenessarestatisticallynegligible—astrongargumentthat train–test leaks should be avoided in academic experiments.4 4 All code and data is publicly available athttps://github.com/webis-de/SPIRE-22. How Train–Test Leakage Aﬀects Zero-shot Retrieval 3 2 Background and Related Work Disjoint training, validation, and test datasets are essential to properly evaluate the eﬀectiveness of machine learning models [7]. Duplication between training and test data can lead to incorrectly high “eﬀectiveness” by memorizing instances rather than learning the target concept. In practice, though, train and test data often still contain redundancies. For text data, paraphrases, synonyms, etc., can be especially problematic, resulting in train–test leaks [19, 24, 29]. For instance, the training and test sets of the ELI5 dataset [13] for question answering were created using TF-IDF as a heuristic to eliminate redundancies between them. This proved insuﬃcient as 81% of the test questions turned out to be paraphrases of training questions, which clearly favored models that memorized the training data [24]. Recently, Zhan et al. [46] found that 79% of the TREC 2019 Deep Learning track topics have similar or duplicated queries in the training data and proposed new data splits to evaluate the interpolation and extrapolation eﬀec- tiveness of models. However, not all types of train–test leaks are unintentional. The TREC 2017 and 2018 Common Core tracks [1] intentionally reused topics from Robust04 to allow participants to use the relevance judgments for training. Indeed, approaches trained on the Robust04 judgments were more eﬀective than others [1]. In this paper, we study whether a similar eﬀect can be observed for unintentional leakage from the large MS MARCO and ORCAS datasets. Training retrieval models on