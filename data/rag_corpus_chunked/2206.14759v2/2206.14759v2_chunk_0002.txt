run a semantic nearest-neighbor search using Sentence-BERT [38] and compare each MS MARCO / ORCAS query to the title, description, and manual query variants [3, 4] of the topics in Robust04 and the TREC 2017 and 2018 Common Core tracks. Figure 1 illus- trates this procedure for Topic 441 (lyme disease) from Robust04. Our manual review of the leakage candidates shows that 69% to 76% of the topics have near- duplicates in MS MARCO / ORCAS. To analyze the eﬀect of this potential train–test leakage on neural retrieval models, we create three types of train- ing datasets per test corpus, in variants with 1,000 to 128,000 training instances (query + (non-)relevant document): (1) a ﬁxed number of instances derived from test queries from the test corpora (1000 for Robust04 and 200 for each of the two Common Core tracks), augmented by other random non-leaking MS MARCO / ORCAS instances to simulate an upper bound on train–test leakage eﬀects, (2) a ﬁxed number of leaking MS MARCO / ORCAS instances (1000 for Robust04 and 200 each for the two Common Core tracks) supplemented by other ran- dom non-leaking instances, and (3) random MS MARCO / ORCAS instances, ensuring that no train–test leakage candidates are included. Inourexperiments,weobserveleakage-inducedimprovementsineﬀectiveness for Robust04 and the two Common Core tracks, which can even change the ranking of systems. However, the average improvements in overall eﬀectiveness are often not signiﬁcant and decrease as the proportion of leakage in the training data becomes smaller and more representative of realistic training scenarios. Nonetheless, our experiments on the eﬀects of leaked instances on search results and the resulting system rankings show that leakage eﬀects occur even when improvementsineﬀectivenessarestatisticallynegligible—astrongargumentthat train–test leaks should be avoided in academic experiments.4 4 All code and data is publicly available athttps://github.com/webis-de/SPIRE-22. How Train–Test Leakage Aﬀects Zero-shot Retrieval