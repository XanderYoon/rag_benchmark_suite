Context relevance assesses how well the retrieved doc- uments match the query, ensuring that the documents are contextually appropriate and useful for generating an accurate answer. Let Rctx denote context relevance, calculated in the equation 15: Rctx = 1 N NX i=1 Relevance(R(qi), qi) (15) Where: • R(qi) is the set of documents retrieved for query qi. • Relevance(R(qi), qi) evaluates how well the set of re- trieved documents R(qi) addresses the query qi. The relevance function for context is further defined in the equation 16: Relevance(R(qi), qi) = P|R(qi)| j=1 P(Rj(qi), qi) |R(qi)| (16) where P (Rj(qi), qi) is a measure which indicates the po- tential of Rj(qi) in the set of retrieved documents R(qi) for answering the query qi , VI. R ESULTS A. Pipeline Performance On UQB Our pipeline’s performance was evaluated on a dataset of 300 questions and answers using the test set of the UniversityQuestionBench (UQB) dataset. We used our base model, Dorna, to compute the evaluation metrics. As previ- ously discussed, the evaluation focused on three key metrics: faithfulness, answer relevance, and context relevance. The calculated results are demonstrated in Table I. • Faithfulness: – Faithfulness measures the factual accuracy of the responses, reflecting the system’s ability to generate outputs consistent with the underlying data source. – The highest faithfulness score (0.8497) is achieved by GPT-3.5-turbo with OpenAI Embeddings , un- derscoring the robustness of general-purpose embed- dings in generating accurate responses. – The performance of Dorna with Dorna Embed- dings (0.839) is competitive, highlighting the capa- bility of localized embeddings specifically designed for Persian-language content. – Models utilizing Persian-Sentence-Embedding-V3 (e.g., GPT-3.5-turbo, 0.8113 ) exhibit slightly re- duced faithfulness compared to OpenAI Embed- dings, suggesting limitations in the current iteration of these embeddings for ensuring strict fidelity to source information. • Answer Relevancy: –