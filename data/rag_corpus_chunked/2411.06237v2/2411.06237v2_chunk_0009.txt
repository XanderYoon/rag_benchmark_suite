total number of data items extracted. Step 2: Identify the type of question and determine the relevant department using the DORNA Model, which is a fine-tuned version of Llama-3 on Presian data [25], [26]. Specifically, we employed the 8-bit quantized version of Dorna utilizing QLoRA [27]. This approach enables us to load our base model with significantly reduced memory requirements. Let Tdept(q) be the function that assigns a question q to a specific department. This can be represented in the Equation 7: Tdept(q) = DORNAclassify(q), (7) Step 3: Split paragraphs from the texts of each department. Let P denote the set of paragraphs split from D, The formula can be written in the Equation 8: P = {paraj}M j=1, (8) where para j represents each paragraph and M is the total number of paragraphs. Step 4: Use FAISS to find the similarity function. FAISS is a library for efficient similarity search and clustering of dense vectors, crucial for retrieving similar texts [28]. We utilize the Persian embedding model named persian-sentence-transformer-news- wiki-pairs-v3 for embedding the paragraphs. Let E(Â·) be the embedding function provided by the Persian sentence transformer. The embeddings for the paragraphs are in the Equation 9: E(P) = {E(paraj)}M j=1, (9) Step 5: For a given query q, its embedding is denoted as E(q). We then retrieve the first 3 closest documents based on the text similarity between the question and the retrieved contents. This similarity search is represented in the Equation 10: R(q) = TopK(FAISS(E(q), E(P)), 3), (10) where R(q) represents the set of top 3 retrieved paragraphs similar to the query q. Step 6: Create a prompt template and pass it to the LLMs. Our LLM, DORNA, is a fine-tuned version on Llama-3 of persian data. The prompt template T is designed to incorporate the retrieved paragraphs