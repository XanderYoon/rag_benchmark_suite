99.5 98.1 92.5 99.5 98.2 92.5 BEIT-3 95.4 84.8 56.1 95.6 88.3 60.7 99.3 97.5 94.3 99.2 97.5 94.6 X-VLM 95.0 86.1 n/a 94.6 83.4 n/a 99.5 96.7 n/a 99.0 96.9 n/a X-VLMNF 95.8 87 58.4 95.8 88.8 60.3 99.3 97.7 94.5 99.3 97.6 94.5 ViLT 63.5 42.6 32.9 62.8 42.5 33.0 97.2 92.0 88.7 94.6 90.1 88.7 Fine-Grained IMRAM 94.5 86.2 57.9 94.5 86.2 57.9 98.8 96.8 93.6 98.9 96.9 93.6 VSRN 95.1 83.5 45.8 94.7 83.5 46.6 98.8 95.7 91.6 98.8 95.7 91.6 SCAN 94.9 87.1 55.5 95.0 87.5 58.2 98.9 96.7 93.4 98.9 96.8 93.5 ADV2048bit - - - - - - 98.6 96.3 92.2 98.4 95.9 92.1 Coarse-Grained ADV64bit - - - - - - 98.4 95.6 91.7 98.7 95.7 91.8 UCCH 86.2 81.5 60.7 86.6 82.1 60.2 95.1 92.1 89.3 90.7 88.2 88.8 DADH 84.8 78.4 58.7 75.6 71.6 60.5 98.6 97.8 96.3 96.7 96.1 95.7 T able 4: Comparison of category-level retrieval performance across v arious Vision-Language Pre-trained Fine-grained (VLP FG) models, non-pretrained Fine-Grained (FG) m odels, and Coarse-Grained (CG) models on the MS-COCO and Flickr30K datasets. Results are report ed as mean Average Precision (mAP) at k (k=10,100,N) for both (i → t) and (t → i) tasks. Note that the fusion steps of BLIP-2 and X-VLM discard all samples that are left out of the fusion reranking step (k =128 and k=256 for Flickr30K and MS-COCO respectively). Therefore, mAP@N is not applicable (n/a) f or these fusion-based models due to the number of relevant samples often exceeding the top-k of th eir reranking range. Hence, results for the no-fusion (NF) variants BLIP-2 and X-VLM are also provided retrieval prohibitively expensive computationally. This computational burden has driven the ﬁeld towards dual-encoder architectures, which avoid query-time attention by computing and storing embeddings independently for