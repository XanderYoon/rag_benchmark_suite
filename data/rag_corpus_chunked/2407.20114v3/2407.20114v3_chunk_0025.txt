sets. We additionally measure query-time attention costs of applicable models to assess the trade-oﬀ in real-time performance which attention-based models face. We incrementally duplicate the Flickr30K evaluation set, generating four test sets: 1K/5K, 10K/50K, 100K/500K and 1M/5M image/text samples. By conducting these experiments on an NVIDIA A100 80GiB GPU and Intel Xeon Platinum 8480+ CPU, we aim to pro- vide insights into the practical trade-oﬀs between model complexity, computational resources, and retrieval performance at scale. Encoding time and storage cost. Table 5 shows the encoding time and storage require- ments for increasing data volumes for the selected models. The experiments were constrained by an 80GiB GPU memory threshold; runs exceed- ing this limit due to the size of the ﬁnal or intermediate embeddings did not ﬁnish (DNF). For instance, BEIT-3, despite having the most compact ﬁnal embeddings among VLP models, required 8.12GiB per 1K/5K images/captions to compute intermediate embeddings, causing its 10K/50K encoding run to exceed available GPU memory. These experiments used original model implementations without custom batching to ensure consistency and reﬂect practical limita- tions users might encounter with similar hardware. This consideration highlights the storage eﬃciency of cross-modal hashing models, which require only 0.36GiB to store 1M/5M image/text embeddings. For query-time attention models, Table 5 includes storage costs for original image and text features, which must be retained for the attention step; an important consideration when assessing the practicality of query-time attention in memory- constrained applications. Encoding time generally increased by a factor of 10 from CG to FG meth- ods and again to VLP models, with X-VLM and ADV as exceptions. The rapid encoding times of UCCH and DADH demonstrate the computa- tional eﬃciency of hash function encoders. ViLT’s end-to-end fusion architecture computes similar- ity scores directly without encoding intermediate embeddings, requiring full model inference for each