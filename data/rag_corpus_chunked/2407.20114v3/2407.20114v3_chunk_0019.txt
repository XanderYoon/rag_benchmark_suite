across all evaluation metrics compared to the full fusion-based ViLT, demonstrating that competitive retrieval performance can be attained without extensive inter-modal interaction at query time. Similarly, among non-pretrained models, the cross-attention model IMRAM outperforms the global-representation model VSRN by a modest R@1 diﬀerence of 4.2 for the MS-COCO (i → t) task and 1.6 for (t → i) task. Whether these moderate improvements in retrieval performance justify the use of query-time attention is a consid- eration that the experiments in Section 5.3 aim to inform. 8 Model Approach Q-Time Attn Dim Img Feat Txt Feat DType Fine-Grained Vision-Language Pretrained BLIP-2 Dual+Fusion Fusion Rerank 256D 1408x677D 768 × 40D f32 BEIT-3 Dual None 768D - - f32 X-VLM Dual+Fusion Fusion Rerank 256D 1024 × 145D 768 × 40D f32 ViLT Fusion Full Fusion n/a 768x217D 768 × 40D f32 Fine-Grained IMRAM Local+Global Cross-Attn 1024D 1024 × 36D 1024 × 70D f64 VSRN Global None 2048D - - f64 SCAN Local+Global Cross-Attn 1024D 1024 × 36D 1024 × 70D f64 Coarse-Grained ADV Dual None 64D/2048D - - i8 UCCH Dual Hash None 64D - - i8 DADH Dual Hash None 64D - - i8 T able 1: Architectural details and computational characteristics of the evaluated image-text retrieval models. For models using query time attention, the image and text fe atures used are shown as these are necessary during query time computation for their respective att ention steps Dataset structure impact. The dataset structure can explain the superior (i → t) retrieval performance across both datasets and all models compared to the (t → i) task. Each image query has ﬁve relevant captions to choose from, whereas each caption query has only one relevant image, making the (i → t) task inherently easier. 5.2 Category-level retrieval results This experiment aims to empirically compare