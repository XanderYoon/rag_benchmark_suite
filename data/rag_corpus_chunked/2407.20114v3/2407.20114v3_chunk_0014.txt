a broader deﬁnition of relevance, where a large amount of samples in the retrieval set may be considered relevant to a given query based on shared category labels. 5) Evaluation metrics. With the retrieval results produced by the retrieval tasks, for instance-level retrieval, we use recall at k (R@k) as the primary retrieval performance metric, deﬁned as follows: R@k = number of relevant items in top k results total relevant items (1) Recall is particularly suitable for instance- level retrieval as it measures whether speciﬁc, individual items have been found. For category- level retrieval, we use mean average precision at k (mAP@k) as the standard evaluation metric. Precision at k (P @k) is deﬁned as follows: P @k = number of relevant items in top k results number of retrieved items (2) For a single query, AP@ k averages the preci- sion values at all available ranks within the top k. For a query set, mAP@k computes the mean of the AP@k scores of all the queries within the query set. mAP@k is well-suited for evaluating category- level retrieval where queries typically have a large number of relevant items, as it measures a model’s ability to include many relevant results towards the top of a large retrieved set. Addition- ally, we implement the 11-point precision-recall curve for the category-level retrieval task, which plots interpolated precision at recall levels R = {0. 0, 0. 1, . . . , 1. 0}. 4 Experiment methodology The following is the experiment methodology for the comparative experiments conducted in Section 5. Datasets. We use MS-COCO and Flickr30K as primary benchmark datasets. MS-COCO con- tains 123 287 images, each with ﬁve human- annotated captions and native multi-label anno- tations across 80 semantic categories, making it particularly suitable for evaluating both instance- level and category-level retrieval