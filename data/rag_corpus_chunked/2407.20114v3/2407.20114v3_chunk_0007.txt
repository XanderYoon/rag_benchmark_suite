between modalities by employing a uniﬁed encoder that jointly processes image and text inputs. Although the fusion encoder approach may potentially lead to higher-quality embeddings, the embeddings must be computed at query time due to the requirement to process all possible query/retrieval sample combinations jointly. Building on these two approaches, models such as X2-VLM [ 31] and BLIP-2 [ 32] adopt a hybrid methodology. These hybrid methods ﬁrst leverage a dual-encoder step to independently compute embeddings to boost eﬃciency. Then, to further improve the embedding quality, they employ a fusion-encoder reranking step that allows for modality interaction. Speciﬁ- cally, the top candidates retrieved from the initial dual-encoder step are reranked using the fusion- encoder during query time. Eﬃciency in FG ITR. To address eﬃ- ciency challenges inherent to FG ITR, recent approaches have aimed to optimise inference speeds by proposing lightweight implementa- tions of the dual-encoder architecture. Lightning- DOT [ 33] tackles this by simplifying pre-training tasks, maximising the amount of computations that can be done oﬄine, and promoting the use of lightweight encoders. VLDeformer [ 34] pro- poses a two-stage retrieval process, ﬁrst using a transformer learning stage followed by more eﬃ- cient indexing-based retrieval in the second stage. HiVLP [ 35] uses coarse-grained screening as a ﬁrst step, followed by a ﬁne-grained rerank in the second step. However, rather than a hash-based approach, HiVLP generates features for the coarse step using early layers of the transformer stack, whereas fully inferred features from later layers are used for the ﬁne-grained rerank. 2.2 Coarse-grained image-text retrieval CG methods, similarly to FG methods, also aim to learn joint visual and textual representations. However, in contrast to the instance-level align- ment of FG methods, CG methods aim to align relevant samples at a broader semantic category level. By using a broader criterion,