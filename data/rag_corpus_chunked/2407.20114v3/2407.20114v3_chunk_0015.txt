4 Experiment methodology The following is the experiment methodology for the comparative experiments conducted in Section 5. Datasets. We use MS-COCO and Flickr30K as primary benchmark datasets. MS-COCO con- tains 123 287 images, each with ﬁve human- annotated captions and native multi-label anno- tations across 80 semantic categories, making it particularly suitable for evaluating both instance- level and category-level retrieval approaches. MS- COCO’s adoption as a standard benchmark across both FG and CG communities [ 7, 8] fur- ther enables fair cross-methodology comparisons. Flickr30K comprises 31K images, also with ﬁve human-annotated captions per image, focusing on human-object interactions. To enhance the com- parability of the results, the toolkit provided through our FiCo-ITR library implements the Karpathy [ 51] split uniformly across all evalu- ated models for consistency. This split allocates 5K images for testing and 5K for validation in MS-COCO, with the remainder used for train- ing. For Flickr30K, 1K images are designated for testing and validation each, with the remaining 29K samples used for training. Due to Flickr30K not containing semantic category labelling, we employ the label generation module of FiCo-ITR enabled by Q2L [ 49] to generate semantic cate- gory labelling for it. These generated labels are available for use within the provided FiCo-ITR repository. Model selection. The model selection pro- cess considered both academic impact and archi- tectural diversity while ensuring practical repro- ducibility. From recent surveys and benchmarks [ 7, 8], we identiﬁed representative models across the key architectural approaches in ITR (as detailed in Table 1). Our model selection thus aims to span the architectural spectrum: For VLP models, we include the fusion-encoder ViLT [ 26], dual-encoder BEiT-3 [ 28], and hybrid dual+fusion rerank models BLIP-2 [ 32] and X- VLM [ 52]. Traditional FG models are represented through both local+global attention (IMRAM [ 9],