retrieval. Therefore, for retrieval applications where stor- age concerns can be addressed, dual-encoder VLP models which do not have query-time attention represent the most practical architecture type for instance-level retrieval. Search scalability insights. Scalability experiments revealed that the potential eﬃciency advantages of CG models, particularly in terms of bitwise operations, do not always translate into practical performance gains for query latency. This was evidenced when applying F AISS-based ANNS indexing, where query latency of CG and FG embeddings was equalised, yet FG embeddings achieved considerably higher recall performance. This outcome highlights the signiﬁcant impact of hardware optimisations and similarity search implementations on search performance. Without custom implementations of hardware-level opti- mised bitwise operations, CG embeddings do not improve query-time latency over FG ones when using a standard similarity search implementation such as F AISS. Further research into standardised optimisations of CG search for modern hardware architectures is recommended to fully leverage their potential eﬃciency advantages. Storage eﬃciency . In terms of storage eﬃ- ciency, CG models oﬀer signiﬁcant advantages. The 64-bit embeddings used for the evaluated CG models were over 15.8 times smaller than the most compact FG embeddings evaluated (256D). For category-level retrieval tasks with large amounts of relevant retrieval samples where FG models do not yield improvements in retrieval perfor- mance, the storage savings of CG models become particularly compelling. 15 6.1 Limitations Our proposed FiCo-ITR library and experiments identify new insights into the tradeoﬀs of FG and CG ITR models, however, we acknowledge cer- tain limitations. CG models are commonly bench- marked on datasets which contain hashtag collec- tions as text samples (e.g. MIR-Flickr25K [ 59] or NUS-WIDE [ 60]). In contrast, FG models require full sentences to enable instance-level retrieval. There are currently no benchmark ITR datasets that contain both full sentences and hashtag col- lections