share category labels with the query. mAP@N challenge and model size. In contrast to the relative ease of scoring well on the mAP@10 metric, the mAP@N metric presents a more substantial challenge. Unlike instance-level retrieval, performance on this metric does not appear to align with the size of the model being used. For example, on the MS-COCO dataset, the state-of-the-art FG VLP BLIP-2 NF model achieves the second lowest category-level mAP@N score of 47.9% on the (i → t) task, while the coarse model UCCH attains the highest score of 60.7% (Table 4). This suggests that FG low-level seman- tics may introduce noise when high-level category semantics is all that is needed for the task. Precision-recall trade-oﬀs. The precision- recall curves in Figure 3 illustrate that the CG unsupervised model UCCH and, in particular, the supervised model DADH trained with learning objectives targeting category-level retrieval better maintain their precision as the proportion of rele- vant samples to be retrieved increases. In contrast, FG models, which are not optimised for retrieving large numbers of broadly relevant samples, show a sharper decline in precision with increases in recall requirement. F usion-encoder global similarity limita- tion. The fusion-encoder approach implemented 9 Instance-Level Retrieval Task MS-COCO Flickr30K Image to Text Text to Image Image to Text Text to Image Recall@k @1 @5 @10 @1 @5 @10 @1 @5 @10 @1 @5 @10 Fine-Grained Vision-Language Pretrained BLIP-2 85.4 97.0 98.4 68.2 87.2 92.6 97.6 100.0 100.0 89.7 98.1 98.9 BLIP-2NF 74.3 94.2 97.4 63.5 86.1 91.8 90.8 99.6 99.9 85.7 97.6 99.1 BEIT-3 79.0 94.3 97.2 61.3 84.6 90.7 96.3 99.7 100.0 86.1 97.6 98.8 X-VLM 81.0 95.1 98.0 63.0 85.7 91.5 96.8 99.8 100.0 86.1 97.4 98.7 X-VLMNF 71.4 91.9 96.3 54.4 81.5 88.8 90.2 99.0 99.7 78.4 95.2 97.8 ViLT 61.6 86.3 92.7