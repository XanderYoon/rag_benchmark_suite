based models like ViLT, while eﬀective at speciﬁc 10 Instance Retrieval Recall@k: Flickr30K Image to Text Text to Image @50 @100 @200 @50 @100 @200 Coarse-Grained ADV64bit 93 96.4 98.2 82.9 90.2 95.7 UCCH 68.4 81.3 88.5 68.7 80.6 89.5 DADH 8.6 16.3 28.0 8.3 15.4 26.3 T able 3 : Extended top-k retrieval results (k=50, 100, 200) for CG models on the Flickr30K dataset instance-level retrieval, perform poorly at ranking more generally similar items at the category level. Dataset diﬃculty comparison. Across all models and tasks, the average mAP@N was 92.9% for Flickr30K compared to 56% for MS- COCO, indicating that the MS-COCO category- level retrieval benchmark presents a more chal- lenging task. This diﬀerence in diﬃculty can be attributed to the datasets’ composition. MS- COCO features more diverse and less overlapping labels, resulting in a more complex retrieval task at the category level. In contrast, Flickr30K has a narrower focus, with a majority of samples con- taining the ‘person’ label due to its emphasis on human-object interactions. As a result, Flickr30K oﬀers a more homogeneous set of images and labels, while MS-COCO provides a broader range of scenes and objects, leading to more varied queries and a more challenging test of model performance across diverse visual scenarios. 5.3 Scaling encoding, storage, and attention This experiment aims to evaluate the scalabil- ity of the selected models by measuring encoding time and embedding storage cost across progres- sively larger test sets. We additionally measure query-time attention costs of applicable models to assess the trade-oﬀ in real-time performance which attention-based models face. We incrementally duplicate the Flickr30K evaluation set, generating four test sets: 1K/5K, 10K/50K, 100K/500K and 1M/5M image/text samples. By conducting these experiments on an NVIDIA A100 80GiB GPU and Intel Xeon Platinum 8480+ CPU, we aim to pro- vide