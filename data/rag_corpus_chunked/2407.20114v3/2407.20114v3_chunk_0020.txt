structure can explain the superior (i → t) retrieval performance across both datasets and all models compared to the (t → i) task. Each image query has ﬁve relevant captions to choose from, whereas each caption query has only one relevant image, making the (i → t) task inherently easier. 5.2 Category-level retrieval results This experiment aims to empirically compare CG and FG models in category-level retrieval tasks, where their relative performance is not well- established in the literature. The category-level mAP@k evaluation results for the selected mod- els on the Flickr30K and MS-COCO datasets are reported in Table 4. The 11-point interpolated precision-recall curves of the evaluated models for both datasets are shown in Figure 3. From these results, the following observations can be made: mAP@10 performance comparison. Given the majority of the queries within both the Flickr30K and MS-COCO evaluation sets have hundreds to thousands of retrieval candidates which are considered relevant at the category level, scoring well on the mAP@10 metric is the least challenging aspect of this task. Nevertheless, the no fusion variant of BLIP-2 achieves an (i → t) mAP@10 score of 96.2% and a (t → i) mAP@10 score of 95.9% on the MS-COCO dataset, whereas the coarse model UCCH achieves scores of 86.2% and 86.6%, respectively (Table 4). The superior performance of FG models on the mAP@10 met- ric can be attributed to instance-level matches being retrieved at the top ranks, which inherently share category labels with the query. mAP@N challenge and model size. In contrast to the relative ease of scoring well on the mAP@10 metric, the mAP@N metric presents a more substantial challenge. Unlike instance-level retrieval, performance on this metric does not appear to align with the size of the model being used. For example, on the MS-COCO dataset, the state-of-the-art