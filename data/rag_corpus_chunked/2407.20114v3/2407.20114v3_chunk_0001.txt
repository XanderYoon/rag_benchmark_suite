Fine-Grained, Coarse-Grained, Vi sion-Language Pretraining, Cross-modal hashing 1 Introduction Cross-Modal Retrieval (CMR) involves using one type of data, such as text, to search for another type of data, such as images. Unlike general multi-modal tasks, CMR speciﬁcally focuses on bridging the gap between diﬀerent modalities to enable retrievals across modalities. CMR has gained prominence over the past decade due to its success in various applications, including e- commerce [ 1, 2], content-based retrieval [ 3, 4], video surveillance [ 5], and recommendation sys- tems [ 6]. When the retrieval task speciﬁcally involves images and text, it is referred to as Image-Text Retrieval (ITR). There are two dis- tinct subﬁelds within ITR: Fine-Grained (FG) and Coarse-Grained (CG) ITR. FG ITR aims to ﬁnd instance-level matches, retrieving the image that directly corresponds to a detailed text query, and vice versa. State-of- the-art FG methods employ large-scale Vision- Language Pretraining (VLP) followed by retrieval 1 arXiv:2407.20114v3 [cs.IR] 16 Jan 2026 ﬁne-tuning, often through the use of contrastive learning techniques [ 7]. CG ITR focuses on category-level retrieval, where the retrieved samples should broadly belong to the semantic category which the query is searching for, rather than aiming for speciﬁc exact matches. State-of-the-art CG methods implement Cross-Modal Hashing [ 8], which train hash func- tions to map image and text samples onto a common Hamming subspace for eﬃcient bitwise similarity comparisons. Figure 1 illustrates the dif- ferent search criteria between FG and CG search approaches. Despite sharing the same overarching retrieval task, the subﬁelds of FG and CG ITR have evolved independently with limited integration, leading to the following three major challenges: 1) Because of methodological diﬀerences between the two subﬁelds, obtaining informative quanti- tative results which are directly comparable is a non-trivial task. 2) Researchers in one subﬁeld rarely benchmark against works from