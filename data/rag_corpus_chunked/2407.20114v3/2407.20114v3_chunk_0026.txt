generally increased by a factor of 10 from CG to FG meth- ods and again to VLP models, with X-VLM and ADV as exceptions. The rapid encoding times of UCCH and DADH demonstrate the computa- tional eﬃciency of hash function encoders. ViLT’s end-to-end fusion architecture computes similar- ity scores directly without encoding intermediate embeddings, requiring full model inference for each new query and preventing the possibility of oﬄine embedding storage and reuse. Con- sequently, ViLT’s computational costs are not represented in Table 5, which focuses on one- time encoding costs for oﬄine storage; instead, its query-time costs are analyzed in the following segment on attention mechanisms. Attention time. Attention in this context refers to end-to-end fusion for ViLT, fusion- based reranking for BLIP-2 and X-VLM and cross-attention for IMRAM and SCAN; processes which are computed at query time. Due to the end-to-end fusion and cross-attention mechanisms used by ViLT, IMRAM and SCAN attending to all possible image-text pairs, the computa- tional complexity of the mechanism is O(n × m), where n is the number of queries and m the number of retrieval candidates, quickly making 11 Category-Level Retrieval Task mAP@k MS-COCO Flickr30K Image to Text Text to Image Image to Text Text to Image mAP @10 @100 @N @10 @100 @N @10 @100 @N @10 @100 @N Fine-Grained Vision-Language Pretrained BLIP-2 93.7 79.4 n/a 94.7 86.7 n/a 99.4 96.8 n/a 99.3 97.3 n/a BLIP-2NF 96.2 87.8 47.9 95.9 88.5 51.1 99.5 98.1 92.5 99.5 98.2 92.5 BEIT-3 95.4 84.8 56.1 95.6 88.3 60.7 99.3 97.5 94.3 99.2 97.5 94.6 X-VLM 95.0 86.1 n/a 94.6 83.4 n/a 99.5 96.7 n/a 99.0 96.9 n/a X-VLMNF 95.8 87 58.4 95.8 88.8 60.3 99.3 97.7 94.5 99.3 97.6 94.5 ViLT 63.5 42.6 32.9 62.8 42.5 33.0 97.2 92.0 88.7 94.6 90.1 88.7 Fine-Grained IMRAM