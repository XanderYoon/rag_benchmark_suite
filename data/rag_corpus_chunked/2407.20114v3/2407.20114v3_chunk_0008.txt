fully inferred features from later layers are used for the ﬁne-grained rerank. 2.2 Coarse-grained image-text retrieval CG methods, similarly to FG methods, also aim to learn joint visual and textual representations. However, in contrast to the instance-level align- ment of FG methods, CG methods aim to align relevant samples at a broader semantic category level. By using a broader criterion, CG methods can place more emphasis on computational eﬃ- ciency. The most prominent approach within CG ITR is Cross-Modal Hashing (CMH) [ 8], which is primarily characterised by the use of bit hash codes to represent their encoded data. The use of hash codes aims for lower storage costs and faster retrieval speeds, due to the bit hash format being inherently lightweight and the associated bitwise operations being less computationally complex than continuous embedding operations. Recent CMH methods use deep neural networks to learn hash functions that map image and text samples to binary hash codes. During retrieval, the hash codes are compared using Hamming distance, an eﬃcient similarity measure for bit strings which counts the number of diﬀering bits between two equal-length bit hash codes. Supervised cross-modal hashing. Super- vised CMH methods leverage multi-category labelling to train the hash function for each modality. DCMH [ 36] pioneered this approach by proposing an end-to-end deep learning CMH framework. Leveraging Generative Adversarial Networks (GAN), SSAH [ 37] implements label information as network input to strengthen cat- egory alignment in the hash space. AGAH [ 38] uses label information directly in its loss function, implementing a multi-labeling map. DADH [ 39] adopts a weighted cosine triplet-margin con- straint for ranking-based similarity preservation. 4 DCHUC [ 40] introduces a four-step iterative opti- misation process that allows simultaneous learn- ing of uniﬁed hash codes for database samples and modality-speciﬁc hashing functions for unseen queries.