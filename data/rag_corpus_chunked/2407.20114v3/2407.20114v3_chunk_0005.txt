(IMRAM [ 9] and UCCH [ 10]) feature representations from image-text pairs dur- ing training. Establishing the joint space enables retrieval across modalities, where the relative dis- tance of items in the shared space determines their relevance. While FG methods share the same fun- damental principles as CG ones, they diﬀer in the level of scrutiny with which the processed sam- ples are analysed. Speciﬁcally, FG methods focus on low-level features and object-level relationships to achieve an in-depth understanding of the scene within a given sample. The format of the encoded samples is typically real-valued continuous embed- dings. These embeddings enable calculating the similarity between samples using vector distance measurements such as Cosine distance [ 14]. T raditional ﬁne-grained methods. Early deep learning-based FG methods typically employed Long Short-Term Memory (LSTM) networks for text encoding and convolutional neural networks (CNNs) such as AlexNet and VGGNet for image feature extraction [ 15]. VSE++ [ 14] built on this approach by experi- menting with VGG19 [ 16] and ResNet152 [ 17] for image encoding along with a GRU-based text encoder and implementing hard negative mining and reranking loss functions. SCAN [ 18] achieved a breakthrough in performance through its proposed bottom-up attention module align- ing image regions and words based on Faster R-CNN object detection [ 19]. CAMP [ 20] aggre- gates salient messages between image regions and words via attention to handle negative pairs before directly predicting matching scores. VSRN [ 21] introduced hierarchical reasoning to capture region-concept interactions and was later upgraded to VSRN++ [ 22] using BERT [ 23] text features. KASCE [ 24] expands image concepts using “common-sense” relationships from scene graphs and selects the most relevant expansions. CAAN [ 25] employs context-aware attention to selectively attend to fragments based on inter- and intra-modal semantics. IMRAM [ 9]