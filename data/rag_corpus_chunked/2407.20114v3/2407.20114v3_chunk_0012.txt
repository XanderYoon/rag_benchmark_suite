embeddings, as well as direct similarity matrices; 3) Similarity Measures, implementing four distance measures for unifor m similarity calculation; 4) Retrieval Tasks, implementing instance- and category-level retrieval for b oth (i → t) and (t → i); and 5) Evaluation Metrics, reporting Recall@K for instance-level retrieval and mAP@ K with P/R curves for category-level retrieval dataset management, the toolkit provides func- tionality to split samples into pre-deﬁned splits along their IDs, ensuring that when diﬀerent models encode the same test set, they process exactly the same samples in the same order. For output handling, the framework supports three types of encoded outputs: Binary hash codes from CG models (typically 64-bit to 2048-bit lengths), continuous embeddings from FG models (typi- cally 256 to 2048 dimensions), and directly com- puted similarity matrices from models employing query-time attention. This ﬂexible interface design enables fair comparisons while respecting each model’s native encoding process. 3) Similarity measures. Given a set of eval- uation embeddings, we implement four similarity measures optimised for diﬀerent embedding types: • Cosine similarity for continuous embeddings, computed as the normalised dot product between vectors x and y: x·y ∥x∥∥y∥ , with values ranging from [-1, 1] • Euclidean similarity, transformed to a (0, 1] range through 1 / (1 + d + ǫ) where d is the Euclidean distance √ ∑(x − y)2 and ǫ = 10 − 8 for numerical stability • Hamming similarity for binary hash codes, com- puted as the negative normalised count of dif- fering bits: − ∑(x ̸= y)/n where n is the bit length • Inner product similarity x ·y, particularly suit- able for embeddings trained with dot-product- based loss functions For traditional binary hash codes (0s and 1s), we implement Hamming distance which calcu- lates the number of diﬀering bits between two hash codes