21] introduced hierarchical reasoning to capture region-concept interactions and was later upgraded to VSRN++ [ 22] using BERT [ 23] text features. KASCE [ 24] expands image concepts using “common-sense” relationships from scene graphs and selects the most relevant expansions. CAAN [ 25] employs context-aware attention to selectively attend to fragments based on inter- and intra-modal semantics. IMRAM [ 9] progressively aligns fragments through a recurrent attention unit to capture diﬀerent semantics at each step, alongside a memory unit to accumulate cues. 3 Fine-grained vision-language pretrain- ing methods. Following the success of the attention mechanism in BERT [ 23], the trans- former architecture has been extensively adapted for vision-language tasks. Moreover, ﬁrst pio- neered by the model ViLT [ 26], state-of-the-art performance is achieved by using transformer encoder stacks end-to-end, without the need for additional pre-processing steps such as object detection feature extraction predominant in tra- ditional FG models. This shift most notably involves leveraging pretraining on large-scale data to obtain task-agnostic features for subsequent ﬁne-tuning onto downstream tasks such as ITR. Initial VLP methods are categorised into two main encoder architectures: Dual-encoder and fusion-encoder architectures. Dual-encoders, such as ALIGN [ 27] and BEIT-3 [ 28], employ separate image and text encoders, allowing the indepen- dent computation and oﬄine storage of embed- dings for each modality. This approach eliminates the need to compute embeddings at query time. Fusion-encoder models, such as UNITER [ 29] and METER [ 30], enhance interactivity between modalities by employing a uniﬁed encoder that jointly processes image and text inputs. Although the fusion encoder approach may potentially lead to higher-quality embeddings, the embeddings must be computed at query time due to the requirement to process all possible query/retrieval sample combinations jointly. Building on these two approaches, models such as X2-VLM [ 31] and BLIP-2 [ 32]