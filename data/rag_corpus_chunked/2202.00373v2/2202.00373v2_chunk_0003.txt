for ﬁne-tuning BERT could optimize the model for document-level representation [9]. Following Devlin et al [12], we use the ﬁnal hidden state corresponding to the [ CLS] token to encode the query q and the document d into their representations rq, and rd: rq =BERT ( [CLS]q [SEP ] )[CLS ] rd =BERT ( [CLS]d [SEP ] )[CLS ] (2) Pairwise Ranking Loss. In Learning-To-Rank tasks, pairwise loss minimizes the average number of pairwise errors in a ranked list [5,6]. Here, we aim to optimize Multi-task optimization for BERT-based QBD retrieval 3 Fig. 1. The ﬁne-tuning process. The same training triples ( q,d +,d−) are used in each step. The BERT re-rankers are the same, and the BERT encoder is shared between the ranking and representation learning tasks. the BERT re-ranker with a pairwise cross-entropy softmax loss function [5]: lrank =−log escore(q,d+) escore(q,d+) +escore(q,d−) (3) where thescore function represents the degree of relevance between a query and a document computed as described in Eq. 1. In fact, this pairwise loss frames the ranking task as a binary classiﬁcation problem in which, given a query ( q) and a pair of relevant (d+) and non-relevant (d−) documents, the ﬁne-tuned ranking model predicts the relevant one. However, at inference time the model is used as a point-wise score function. Triplet Representation Learning Loss. In the context of representation learning with pre-trained transformers, a triplet loss function ﬁne-tunes the weights of the model such that given an anchor query q, the representations of the query rq and the documentrd (obtained as described in Eq. 2) are closer for a relevant document d+ than for a non-relevant document d−: lrepresentation =max{(f(rq, rd+) − f(rq, rd−) + margin ), 0} (4) Here, f indicates a distance metric and and margin ensures that d+ is at