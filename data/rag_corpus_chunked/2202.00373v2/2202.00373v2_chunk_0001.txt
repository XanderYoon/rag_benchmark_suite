provide more context for ranking models [16]. Transformer-based ranking models have proven to be highly eﬀective at taking advantage of context [10,11,24], but the long query documents pose challenges because of the maximum input length for BERT-based ranking models. Recent work showed that transformer- based models which handle longer input sequences are not necessarily more eﬀective when being used in retrieval tasks on long texts [3]. We, therefore, direct our research towards improving retrieval eﬀectiveness while acting within the input length limitation of ranking models based on large scale pre-trained BERT models [12]. We posit that the representations learned during pre-training arXiv:2202.00373v2 [cs.IR] 23 May 2022 2 A. Abolghasemi, S. Verberne, L. Azzopardi have been tailored toward smaller sequences of text – and additional tuning the language models to better represent the documents in this speciﬁc domain and query setting, could lead to improvements in ranking. To investigate this, we ﬁrst focus on the task of Case Law Retrieval (i.e. given a legal case ﬁnd the related cases), and employ multi-task optimisation to improve the standard BERT-based cross-encoder ranking model [15] for QBD retrieval. We then explore the generalizability of our approach by evaluating our approach on four QBD retrieval tasks in the academic domain. Our approach draws upon multi-task learning to rank – where a shared structure across aux- iliary, related tasks is used [1,20,29,8]. Speciﬁcally, in our method, we employ document-level representation learning as an auxiliary objective for multi-task ﬁne-tuning (MTFT) of the BERT re-ranker. To our knowledge, there is no prior work on using representation learning directly as an auxiliary task for ﬁne-tuning a BERT re-ranker. We show that optimizing the re-ranker jointly with document- level representation learning leads to consistently higher ranking eﬀectiveness over the state-of-the-art with greater eﬃciency i.e., with the same training in- stances