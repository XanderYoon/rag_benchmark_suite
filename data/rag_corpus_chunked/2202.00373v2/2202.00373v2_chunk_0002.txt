multi-task ﬁne-tuning (MTFT) of the BERT re-ranker. To our knowledge, there is no prior work on using representation learning directly as an auxiliary task for ﬁne-tuning a BERT re-ranker. We show that optimizing the re-ranker jointly with document- level representation learning leads to consistently higher ranking eﬀectiveness over the state-of-the-art with greater eﬃciency i.e., with the same training in- stances on the same architecture. 2 Preliminaries BERT-based Ranking. Pre-trained transformer-based language models [12] have shown signiﬁcant improvement in ranking tasks [10,11,24,18]. In this work, we use the BERT re-ranker proposed by Nogueira and Cho [26], which is a pre- trained BERT model followed by a projection layer Wp on top of its [ CLS] token ﬁnal hidden states. The BERT re-ranker, which is a cross-encoder neural ranking model, uses the concatenation of a query and candidate document as the input to a ﬁne-tuned pre-trained BERT model. The output of the model is used to indicate the relevance score s of the document d for the input query q, such that: s(q, d) =BERT ([CLS]q [SEP ]d [SEP ])[CLS ]∗Wp (1) BERT-based Representation Learning. BERT was originally pre-trained on two tasks, namely Masked Language Modeling and Next Sentence Prediction [12]. These tasks, however, are not meant to optimize the network for document- level information representation [9] which may make the model less eﬀective in representation-focused [14] downstream tasks [10]. Previous works have shown that leveraging a Siamese or triplet network structure for ﬁne-tuning BERT could optimize the model for document-level representation [9]. Following Devlin et al [12], we use the ﬁnal hidden state corresponding to the [ CLS] token to encode the query q and the document d into their representations rq, and rd: rq =BERT ( [CLS]q [SEP ] )[CLS ] rd =BERT ( [CLS]d [SEP ] )[CLS ] (2)