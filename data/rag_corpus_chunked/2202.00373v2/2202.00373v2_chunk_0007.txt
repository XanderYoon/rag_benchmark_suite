BM25optimized 17.44† 29.99† 22.05† T able 2. Ranking results on the SciDocs benchmark. HF is Huggingface. † indicates the statistically signiﬁcant improvements according to a paired t-test (p <0.05). Model Co-view Co-read Cite Co-cite MAPnDCGMAPnDCGMAPnDCGMAPnDCG SPECTER [9] 83.6% 0.915 84.5% 0.924 88.3% 0.949 88.1% 0.948 SPECTER w/ HF[36]83.4% 0.914 85.1% 0.927 92.0% 0.966 88.0% 0.947 BM25 75.4% 0.874 75.6% 0.881 73.5% 0.876 76.3% 0.890 BM25optimized 76.26% 0.877 76.09%0.881 75.3% 0.884 77.41% 0.896 BERT 85.2% 0.925 87.5% 0.940 94.0% 0.975 89.7% 0.955 MTFT-BERT 86.2%† 0.930† 87.7%0.94094.2%0.97691.0%† 0.961† we use LegalBERT [7], and SciBERT[4], which are domain-speciﬁc BERT mod- els pre-trained on the legal and scientiﬁc domains respectively. We train our neural ranking models for 15 epochs with a batch size of 32, and AdamW opti- mizer [22] with a learning rate of 3 × 10−5. All of our models are implemented and ﬁne-tuned using PyTorch [27] and the HuggingFace library [36]. 5 Results and Analysis Ranking quality. Table 1 displays the ranking quality of the MTFT-BERT re- ranker in comparison to BM25, TLIR[23], BM25optimized, and the original BERT re-ranker on COLIEE 2021. The cut-oﬀ k for all rankers is set to 5 during both validation and test since the train queries in COLIEE 2021 have 5 relevant docu- ments on average. We report precision and recall besides F1, which is the oﬃcial metric used in the COLIEE competition. It can be seen that the BERT re-ranker and the MTFT-BERT re-ranker can both achieve better quality over BM25 with default parameters as initial ranker. In contrast, when we use BM25 optimized as the initial ranker, the BERT re-ranker fails to yield improvement, while MTFT- BERT outperforms the state-of-the-art BM25 optimized [3] by a statistically sig- niﬁcant margin of 8.3% relative improvement. For comparability reasons on the SciDocs benchmark, we have included both