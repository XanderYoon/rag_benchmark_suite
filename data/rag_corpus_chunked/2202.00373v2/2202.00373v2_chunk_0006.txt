academic papers, we take the validation set provided for each task and use 85% of it as training set and the rest as the validation set for tuning purposes. Implementation. We use Elasticsearch1 to index and retrieve the initial rank- ing list using a BM25 ranker. It was shown in prior work that BM25 is a strong baseline [32], and it even holds the state-of-the-art in case law retrieval on COL- IEE 2021 [3]. Therefore, to make our work comparable, we use the conﬁguration provided by [3] to optimize the BM25 with Elasticsearch for COLIEE 2021 case law retrieval. For query generation, following the eﬀectiveness of term selection using Kullback-Leibler divergence for Informativeness (KLI) in prior work in case law retrieval [21,3], we use the top-10% of a query document terms scored with KLI 2 as the query for BM25 in our experiments. As the BERT encoders, 1 https://github.com/elastic/elasticsearch 2 Implementation from https://github.com/suzanv/termprofiling/ Multi-task optimization for BERT-based QBD retrieval 5 T able 1. The reranking results with BM25 and BM25 optimized as initial rankers for the COLIEE 2021 test data. † indicates the statistically signiﬁcant improvements over BM25optimized according to a paired t-test (p <0.05). TLIR achieved the highest score in the COLIEE 2021 competition. Model Initial RankerPrecision%Recall%F1% BM25 - 8.8 16.51 11.48 TLIR [23] - 15.33 25.56 19.17 BM25optimized[3] - 17.00 25.36 20.35 BERT BM25 10.48 18.80 13.46 MTFT-BERT BM25 12.08 21.59 15.49 BERT BM25optimized 14.40 24.63 18.17 MTFT-BERT BM25optimized 17.44† 29.99† 22.05† T able 2. Ranking results on the SciDocs benchmark. HF is Huggingface. † indicates the statistically signiﬁcant improvements according to a paired t-test (p <0.05). Model Co-view Co-read Cite Co-cite MAPnDCGMAPnDCGMAPnDCGMAPnDCG SPECTER [9] 83.6% 0.915 84.5% 0.924 88.3% 0.949 88.1% 0.948 SPECTER w/ HF[36]83.4% 0.914 85.1% 0.927 92.0% 0.966 88.0% 0.947 BM25 75.4% 0.874 75.6% 0.881