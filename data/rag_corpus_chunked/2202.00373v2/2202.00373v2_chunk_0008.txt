both achieve better quality over BM25 with default parameters as initial ranker. In contrast, when we use BM25 optimized as the initial ranker, the BERT re-ranker fails to yield improvement, while MTFT- BERT outperforms the state-of-the-art BM25 optimized [3] by a statistically sig- niﬁcant margin of 8.3% relative improvement. For comparability reasons on the SciDocs benchmark, we have included both the original paper results and the results reported in their oﬃcial code reposi- tory3, which is achieved using Huggingface models like our implementations. As 3 https://github.com/allenai/specter 6 A. Abolghasemi, S. Verberne, L. Azzopardi Fig. 2. The evaluation results of MTFT-BERT+BM25 optimized with various λ in the COLIEE 2021 case law retrieval task. λ = 0 indicates the BERT re-ranker. Table 2 shows, while both the BERT re-ranker, and the MTFT-BERT re-ranker yield improvement over the SPECTER method, the MTFT-BERT re-ranker out- performs the BERT re-ranker which conﬁrms the eﬀectiveness of our method in an additional domain-speciﬁc QBD retrieval setting. Robustness to varying λ. Task weighting is a widely used method in multi- task learning algorithms [19,17] where a static or dynamic weight is assigned to the loss of diﬀerent tasks. Figure 2 displays the ranking quality of the MTFT- BERT re-ranker over diﬀerent values ofλ on the COLIEE test set, using BM25optimized as the initial ranker. We bound λ at 1 since our target task is ranking, and we do not want the representation loss rate to have higher impact in the training. We can see that our model quality is relatively consistent across diﬀerent values above 0.5 which indicates the robustness of our model in tuning this parameter. Eﬀect of re-ranking depth. We experimented with the ranking depth, i.e., number of documents re-ranked from the initial ranker result, by increasing it from 15 to 100 in steps of 5.