Improving BERT-based Query-by-Document Retrieval with Multi-Task Optimization Amin Abolghasemi 1, Suzan Verberne1, and Leif Azzopardi 2 1 Leiden University, The Netherlands {m.a.abolghasemi,s.verberne}@liacs.leidenuniv.nl 2 University of Strathclyde, UK leif.azzopardi@strath.ac.uk Abstract. Query-by-document (QBD) retrieval is an Information Re- trieval task in which a seed document acts as the query and the goal is to retrieve related documents – it is particular common in profes- sional search tasks. In this work we improve the retrieval eﬀectiveness of the BERT re-ranker, proposing an extension to its ﬁne-tuning step to better exploit the context of queries. To this end, we use an additional document-level representation learning objective besides the ranking ob- jective when ﬁne-tuning the BERT re-ranker. Our experiments on two QBD retrieval benchmarks show that the proposed multi-task optimiza- tion signiﬁcantly improves the ranking eﬀectiveness without changing the BERT re-ranker or using additional training samples. In future work, the generalizability of our approach to other retrieval tasks should be further investigated. Keywords: Query-by-document retrieval· BERT-based ranking· Multi- task optimization. 1 Introduction Query by document (QBD) [38,37], is a widely-used practice across professional, domain-speciﬁc retrieval tasks [33,35] such as scientiﬁc literature retrieval [25,9], legal case law retrieval [2,3,30,34], and patent prior art retrieval [13,28]. In these tasks, the user’s information need is based on a seed document of the same type as the documents in the collection. Taking a document as query results in long queries, which can potentially express more complex information needs and provide more context for ranking models [16]. Transformer-based ranking models have proven to be highly eﬀective at taking advantage of context [10,11,24], but the long query documents pose challenges because of the maximum input length for BERT-based ranking models. Recent work showed that transformer- based models which handle longer input sequences are not necessarily more eﬀective when being used in