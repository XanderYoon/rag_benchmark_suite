in the training. We can see that our model quality is relatively consistent across diﬀerent values above 0.5 which indicates the robustness of our model in tuning this parameter. Eﬀect of re-ranking depth. We experimented with the ranking depth, i.e., number of documents re-ranked from the initial ranker result, by increasing it from 15 to 100 in steps of 5. We then analyzed the MTFT-BERT re-ranking quality relative to depth. We found that the ranking quality decreases rapidly after the lower ranking depths, to F 1 = 17.3 at 100, which is lower than the original BM25 optimized ranking. While MTFT-BERT can improve over BM25 with a shallow re-ranking set, we conﬁrm the ﬁndings by previous studies that BM25 is a strong baseline for case law retrieval [3,32]. 6 Conclusion This paper shows that it is possible to improve the BERT cross-encoder re-ranker quality using multi-task optimization with an auxiliary representation learning task. We showed that the resulting model named MTFT-BERT re-ranker ob- tains consistently better retrieval quality than the original BERT re-ranker using the same training instances and structure. While our focus was on query-by- document retrieval in professional search domains (legal and academic), as a future work, it would be interesting to study the eﬀectiveness of MTFT-BERT re-ranker in other retrieval tasks where we have shorter queries. Multi-task optimization for BERT-based QBD retrieval 7 7 Acknowledgments This work is funded by the DoSSIER project under European Union’s Horizon 2020 research and innovation program, Marie Sk lodowska-Curie grant agreement No. 860721. References 1. Ahmad, W.U., Chang, K.W., Wang, H.: Multi-task learning for document ranking and query suggestion. In: International Conference on Learning Representations (2018) 2. Althammer, S., Hofst¨ atter, S., Sertkan, M., Verberne, S., Hanbury, A.: Paragraph aggregation retrieval model (parm) for dense document-to-document retrieval. In: Advances in Information