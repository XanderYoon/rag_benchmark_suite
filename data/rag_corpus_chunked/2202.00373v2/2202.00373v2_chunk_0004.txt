such that given an anchor query q, the representations of the query rq and the documentrd (obtained as described in Eq. 2) are closer for a relevant document d+ than for a non-relevant document d−: lrepresentation =max{(f(rq, rd+) − f(rq, rd−) + margin ), 0} (4) Here, f indicates a distance metric and and margin ensures that d+ is at least margin closer to q than d− [31]. 3 Multi-task ﬁne-tuning of the BERT re-ranker Our proposed re-ranker aims to jointly optimise both thelrank andlrepresentation – we shall refer to our BERT re-ranker model as MTFT-BERT . As shown in Figure 1, the Multi Task Fine Tuning (MTFT) is achieved by providing training instances consisting of triples (q,d +,d −). To do so, we ﬁrst feed the concatenation ofq andd+, and the concatenation of q andd− separately to the MTFT-BERT 4 A. Abolghasemi, S. Verberne, L. Azzopardi re-ranker, as described in section 2, to compute the pairwise loss lrank following Eq. 3. In the next step, we feed each of q, d+, and d− separately to the shared encoder of the re-ranker to compute the lrepresentation following Eq. 4. As dis- tance metric f we use the L2-norm and we set margin = 1 in our experiments. The shared encoder is then ﬁne-tuned with the aggregated loss as shown in Eq. 5 while the ranking head is only ﬁne-tuned by the ﬁrst term: laggregated = lrank + λl representation (5) The λ parameter balances the weight between the two loss functions. Later, we investigate the stability of our model under diﬀerent values ofλ. Since ranking is the target task, and the ranking head is only optimized by the ranking loss, we assign the regularization weight (0 <λ< 1) only to the representation loss. It is noteworthy that at inference time,