corpus which is used to measure the utility of the defense under benign setting. As for dense retrievers we have considered4 retrievers namely, contriever (Izacard et al., 2022), ANCE Xiong et al. (2020), multilingual e5 (Wang et al., 2024) and GTE large Li et al. (2023). Evaluation metrics: We measure two evaluation metrics to measure both the robustness of the defense under attacks and the utility of the it under benign settings. The success of an attack is measured by the attack success rate (ASR)which measures the number of times the retriever retrieved atleast one poison or malicious document for test queries. The utility, on the other hand, is measured by thesuccess rate (SR) that counts the number of times the retriever retrieved atleast one golden document for test queries. We measure the average drop in the SR as measure of the drop in utility (lower the better) and the average drop in ASR as a measure of the robustness of a defense (higher the better). Attacks: We consider both gradient-based and interpretable attacks as candidates for adversaries. Similar to the works of (Zou et al., 2024, Zhong et al., 2023) as a candidate for the gradient based attack we consider 7 RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation the Hotfip (Ebrahimi et al., 2018) style attacks which generate poison by searching for successful adversarial tokens in the token space guided by the gradient of the attackerâ€™s objective. We also consider a variant of this attack where the attacker instead of adding an adversarial token in a certain part of the text has the ability to spread out the tokens throughout the document which we call as HotFlip (spread out). As a candidate for interpretable attacks similar to Zou et al. (2024), we add the query