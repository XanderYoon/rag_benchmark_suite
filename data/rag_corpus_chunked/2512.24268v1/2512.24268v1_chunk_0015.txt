attacker’s objective. We also consider a variant of this attack where the attacker instead of adding an adversarial token in a certain part of the text has the ability to spread out the tokens throughout the document which we call as HotFlip (spread out). As a candidate for interpretable attacks similar to Zou et al. (2024), we add the query itself in the document as a poison which we call as query as poison. Furthermore we also propose a modfied version of AdvBDGen Pathmanathan et al. (2024) as mentioned in Section 3 as an additional interpretable attacks. Baseline defenses: Similar to the work of (Zou et al., 2024), we consider paraphrase and perplexity based defenses as the baseline. The idea behind paraphrase based defense is that certain poisons added to a document can be broken by paraphrasing the document before retrieval. We use LLama 3.3 70B (Grattafiori et al., 2024) as the paraphraser. Perplexity-based rely on the idea that the addition of poisons in a document can increase the perplexity of the document which can inturn be used to remove the adverserial document. We measure the perplexity here using a GPT2 model (Radford et al., 2019). 5. Results Table 1: Perplexity-based Defense: This table shows that, similar to paraphrase-based defenses, perplexity-based defenses are effective at detecting gradient-based attacks. However, they fail to distinguish poisoned documents from benign ones in the case of interpretable attacks such as Query-as-Poison and AdvRAGgen, and therefore perform worse than the proposed defenses. We evaluate perplexity using four retrievers—Contriever Izacard et al. (2022), ANCE Xiong et al. (2020), Multilingual E5 Wang et al. (2024), and GTE Large Li et al. (2023). In the table, we report perplexity scores and highlight detections in red when the defense correctly identifies a poisoned document as malicious, and in green