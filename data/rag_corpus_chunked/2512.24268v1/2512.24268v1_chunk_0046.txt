early experiments, optimizing with only the first two feedback signals caused the generator to copy the query into the document, effectively making the generated text identical to the query in the poison setting. To prevent this, we penalize such cases by measuring the Rouge-L score (Lin, 2004) between the query and the generated document, applying a penalty when the score is high. Generator Trained with the feedback Malicious document similar to the query Query Malicious document Similarity in retrieverâ€™s embedding space Semantic similarity with the malicious document 2. Generation condition feedback 1. Retrieval condition feedback Rouge L score between the query and the document 3. Feedback to prevent the trivial solution -+ Figure 9: Pipeline of AdvRAGgen:The figure illustrates the three feedback signals used to train AdvRAGgen. The generation conditionensures similarity between the paraphrased and original malicious documents, theretrieval conditionenforces similarity between the query and the paraphrased document to enable successful retrieval, and the Rouge-L score serves as aregularizerto prevent the trivial solution of copying the query as the poison. 25 RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation B.2. Ablation Results Table 6: ASR Hotflip with iterations (Ablation study): This table shows that for certain stronger models such as multilingual e5 (Wang et al., 2024) more iterations of the hotflip method is needed to produce an efficient poison and still the proposed defenses are capable of defending against the attack. Natural Questions (NQ)Kwiatkowski et al. (2019) ASR (%) Retriever Defense itr=30 itr=40 itr=50 itr=60 itr=70 itr=80 itr=90 itr=100 No Defense 16 50 50 78 82 88 88 90 RAGPart 0 0 0 0 0 0 0 2 (N = 5, k=1) Multilingual E5 RAGPart 0 0 0 0 0 0 1 0 Wang et al. (2024)(N = 5, k=3) RAGMask 2 2 0 2 6 2