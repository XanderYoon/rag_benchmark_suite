therefore perform worse than the proposed defenses. We evaluate perplexity using four retrievers—Contriever Izacard et al. (2022), ANCE Xiong et al. (2020), Multilingual E5 Wang et al. (2024), and GTE Large Li et al. (2023). In the table, we report perplexity scores and highlight detections in red when the defense correctly identifies a poisoned document as malicious, and in green when it incorrectly classifies it as benign. Dataset Retriever No Poison HotFlip HotFlip (spread out) Query-as- Poison AdvRAGgen Natural Questions (NQ) Contriever 143 989 1827 119 74 ANCE 143 5726 12021 119 74 Multilingual E5 143 113 392 119 74 GTE Large 143 224 447 119 74 FiQA Contriever 143 274 631 100 53 ANCE 143 466 1095 100 53 Multilingual E5 143 86 252 100 53 GTE Large 143 113 303 100 53 Due to space constraints, we present both the ASR and SR (utility) results as averages over the four retrievers considered. For fine-grained results, refer to the Appendix. In the Appendix, we further provide hyperparameter analysis, motivating the choice ofN,kin RAGPart andδ,min RAGMask. Ineffectiveness of the baseline defenses: As seen in Figure 8 and Table 1, both the paraphrase-based and perplexity-based defenses fail to defend against interpretable poisoning attacks, even though they are effective against gradient-based attacks. Effect of intersection-based aggregation and majority vote-based aggregation: Intersection-based 8 RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation Intersection Majority Vote0 5 10 15 20 25 30 35 40 45 Different types of Aggregation % drop in SR (a) Naive: Intersection-based aggregation vs majority vote-based aggregation Intersection Majority Vote0 10 20 30 40 50 60 Different types of Aggregation % drop in SR (b) RAGPart: Intersection-based aggregation vs majority vote-based aggregation Figure 5: Drop in success rate (SR) on FiQA dataset(lower is better)— Comparison of aggregation methods: