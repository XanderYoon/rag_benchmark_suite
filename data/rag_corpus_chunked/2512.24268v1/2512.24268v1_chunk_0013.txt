& RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation 2. The similarity between the query and the paraphrase in the target retriever’s embedding space. This ensures the paraphrase is retrievable, satisfying the retrieval condition. 3. The negative ROUGE-L score between the query and the paraphrase. This discourages trivial attacks that involve simply inserting the query into the adversarial document. Generator Trained with the feedback Malicious document similar to the query Query Malicious document Similarity in retriever’s embedding space Semantic similarity with the malicious document 2. Generation condition feedback 1. Retrieval condition feedback Rouge L score between the query and the document 3. Feedback to prevent the trivial solution -+ Figure 4: AdvBDGen style attack on retrieval: This figure shows the overview of the poison generator’s training framework inspired by AdvBDGen (Pathmanathan et al., 2024) For further details, refer to the Appendix. 4. Experiments DatasetandModels: Weusedtwoquestionansweringdatasets, namelyNaturalQuestion(NQ)(Kwiatkowski et al., 2019) and Financial Opinion Mining and Question Answering (FIQA) Maia et al. (2018). NQ dataset is made of corpus of2, 681, 468documents, while the FIQA is made of a corpus of57, 638 documents. From the queries, we randomly select512 queries and use them as our training set. For each query, we pick3 randomly pick documents and treat them as irrelevant documents. The goal of the attacker is to craft poisons, whose addition will make these documents retrievable. Each of the queries has10< relevant queries, which we call golden documents in the corpus which is used to measure the utility of the defense under benign setting. As for dense retrievers we have considered4 retrievers namely, contriever (Izacard et al., 2022), ANCE Xiong et al. (2020), multilingual e5 (Wang et al., 2024) and GTE large Li et al. (2023). Evaluation metrics: We measure two evaluation metrics to measure both the robustness of the