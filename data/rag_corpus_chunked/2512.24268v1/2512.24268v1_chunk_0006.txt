3 RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation based on the access the attacker has to the retriever model. The goal of the attacker is to either create a retrievable adversarial passage that can cause a harmful generation when added to the context of an LLM or craft poisons whose addition into the adversarial passage can make them retrievable for a given query. Works such as (Zou et al., 2024, Zhong et al., 2023) have crafted white-box poisons by exploiting the gradient of the retrievers, which when added to an adversarial passage can fool the retriever into retrieving the passage. In black-box settings, the works of (Zou et al., 2024) have proposed adding the query itself to the adversarial passage to make it retrievable. Defenses: Early defenses, such as those by Weller et al. (2024), proposed paraphrasing queries to retrieve multiple robust passages and thereby mitigate misinformation at the retrieval stage. Although these defenses can handle weaker adversaries, they often fail against stronger attacks Zou et al. (2024) and robust retrievers capable of preserving semantic meaning across paraphrases. Another line of work (Xiang et al., 2024) proposes certified defenses against corpus poisoning at the generation stage (rather than at retrieval) by aggregating responses generated from each of the top-p retrieved documents. However, these generation- stage defenses rely on strong assumptionsâ€”each golden document must independently suffice for generation, and retrievers must reliably retrieve an overwhelming number of golden documents. In practice, these conditions rarely hold, and such approaches are computationally expensive because long-context LLMs must be invoked multiple times per inference. 3. Method 3.1. Defense: RAGPart Most state-of-the-art dense retrievers Wang et al. (2024), Izacard et al. (2022), Li et al. (2023) are pre-trained using a large-scale contrastive loss Khosla et al. (2021) and then fine-tuned on