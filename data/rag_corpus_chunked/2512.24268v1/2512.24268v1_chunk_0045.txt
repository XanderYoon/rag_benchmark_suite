document from the training set, the generator is prompted to paraphrase the document into a retrievable form. Initially, the generator lacks knowledge of what is considered retrievable for a given query. To address this, we fine-tune the generator using three types of feedback (listed below), applying direct preference optimization (DPO) (Rafailov et al., 2024). Specifically, two paraphrases are generated and scored based on the feedback signals, and one is labeled as preferred. These preference pairs are then used to fine-tune the generator in an online DPO setup. The feedback signals are: • Generation condition: To ensure the generation condition is satisfied, the original malicious document must preserve its content when paraphrased. Therefore, we measure the semantic similarity between the original document and the generated response, and use this similarity score as a feedback signal for the generation objective. • Retrieval condition: For the malicious document to be successfully retrieved, it must be semantically similar to the query. To enforce this, we measure the similarity between the query and the document in the retriever’s embedding space and use this as a retrieval condition. While this constitutes a white-box attack by definition, we observe that the poisoned examples generated using one retriever model are transferable to others. In our experiments, we generate poisons using the Contriever model Izacard et al. (2022) and find that they yield high attack success rates (ASR) even when applied to other retrievers. • Preventing trivial solution: In early experiments, optimizing with only the first two feedback signals caused the generator to copy the query into the document, effectively making the generated text identical to the query in the poison setting. To prevent this, we penalize such cases by measuring the Rouge-L score (Lin, 2004) between the query and the generated document, applying a penalty when the score