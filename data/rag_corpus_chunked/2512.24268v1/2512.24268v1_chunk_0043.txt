with those guarantees, the adversary can end up getting chosen in the topp documents even though it is not the topmost relevant document. While we can restrict ourselves to the top 1 document, it can severely hurt the performance of the system due to the existence of multiple documents and the retriever’s deficiency. 2. What are the limitations of the proposed defenses? In what scenarios can the defense not defend and what is your argument against such scenarios? The scenario we consider the most here is the scenario where there is an adversarial document and it is generally not retrievable for the retriever, and a poison is added to make it retrievable. If the adversarial document itself is retrievable, then our defenses may fail. For example, consider the following scenario. Given a query “Where is Mount Everest located?” a golden document can be “Mount Everest is located between Nepal and Tibet,” and an adversarial document can be “Mount Everest is located at Spain.” An important point to note here is that both these documents are semantically similar, i.e., both the documents talk about the location of Mount Everest, and one document is adversarial due to the fact that it contains misinformation or can induce the generation to contain misinformation. If the retriever is retrieving this document, that means the retriever doesn’t have the knowledge about the factual error in the document. Thus, there is no way to defend against such a poison at the retrieval stage; rather, we argue that one should consider a generation-level defense as this is due to the deficiency of the retriever. If one is to solve this problem in a retrieval defense, then they should consider enhancing the retriever with more hard negatives (a document that is getting retrieved but is not actually a relevant