fragments may be poisoned. This method is illustrated in Figure 1. 2. Naive combination of fragments baseline: In this approach, we first select combinations ofk out ofN document fragments (without first embedding them) and concatenate the raw text. The resulting text is then passed through the embedding model. A toy example containing a comparison between the two methods is shown later in Figure 2. Since embedding happens after mixing the content, poisoned fragments can—by design—have a significant influence on the final embedding. Once a new set of(N k )embeddings is formulated to represent each document, we can build(N k )⋅D embedding databasesforagivencorpusof Ddocuments. Thesedatabasesareusedtoretrieve (N k )setsoftop- pdocuments. The retrieved results can then be aggregated to form the final top-p documents to be passed to the generator. We explore two aggregation strategies below: 1. Intersection-based aggregation: From the(N k )sets of top-p documents, we select only those docu- ments that appear in all of the sets. If no document appears across all sets, we randomly choosep documents from the union of the(N k )⋅p retrieved documents. While this approach can reduce the chance of retrieving adversarial documents, it often severely impacts the success rate (SR). As a result, we do not use this strategy in most of our experiments. 2. Majority vote-based aggregation: From the(N k )sets of top-p documents, we select thep documents that appear most frequently. It is important to note that the idealk condition for robustness in (Sun et al., 2022), which assumesnp < N−1 2 , no longer guarantees robustness in this context. This is because the aggregation now involves selecting the top-p documents by majority vote across combinations, rather than making a decision per combination. Thus, even if the adversarial document is not the top-ranked in most combinations, it may still be retrieved due to