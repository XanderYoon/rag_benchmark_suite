the correctness of the generated responses as a signal for membership inference test. Probe Question. Next, we generate a set of questions that are highly aligned with the content of ğ‘‘âˆ—. Drawing inspiration from doc2query tasks in the IR literature, we adopt a few-shot prompting strategy [8] that instructs an LLM to create natural, information- seeking queries based on ğ‘‘âˆ—. By default, these questions follow a yes/no structure, which simplifies validation and aggregation in later stages. This process yields a set of candidate Probe Questions: P = {ğ‘1, ğ‘2, . . . , ğ‘ğ‘› }. The exact prompt used, along with further examples, is detailed in the Appendix (Figure 14). Combining Summaries and Questions. Finally, we concatenate each probe question ğ‘ğ‘– with the single Retrieval Summary ğ‘ âˆ— to form the final query set ğ‘„ = {ğ‘1, . . . , ğ‘ğ‘› }, with ğ‘ğ‘– = ğ‘ âˆ— âˆ¥ğ‘ğ‘–, (4) This two-part structure fulfills both retrieval and membership in- ference objectives simultaneously. An example of our generated queries is shown in Figure 3. 5.2 Ground Truth Answer Generation After obtaining our queries, ğ‘„ = {ğ‘1, . . . , ğ‘ğ‘› }, we generate their corresponding ground truth answers using a shadow LLM. Con- cretely, we provide the text of the target documentğ‘‘âˆ— as a reference, prompting this LLM to produce accurate answers for each query ğ‘ğ‘–. Since the questions are framed in a way that elicits binary re- sponses, extracting answers from LLM outputs is straightforward. Let ğº = {ğ‘”1, ğ‘”2, . . . , ğ‘”ğ‘› } denote the resulting ground truth answers. These answers serve as baselines for evaluating the correctness of the RAG systemâ€™s responses and, ultimately, for deriving member- ship signals. 5.3 Membership Inference We submit the queries ğ‘„ to the RAG system by issuing standard inference requests through