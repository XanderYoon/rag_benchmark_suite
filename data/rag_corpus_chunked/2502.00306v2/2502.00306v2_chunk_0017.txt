Lakera can easily detect attempts to infer retrieved documents. Our attack achieves near-zero detection rate, unlike prior attacks that are almost always detected. We aim to craft natural queriesâ€”those resembling ordinary user inputsâ€”yet highly specific to a target document. The premise here is that such a document contains information that is uniquely specific, often the rationale for employing RAG in the first place. To lever- age this specificity, we design questions likely to be answerable only in the documentâ€™s presence. Increasing the number of queries would help cover multiple descriptive aspects of the document, en- hancing coverage and specificity for membership inference. These queries should be natural, relevant, and easy to validate, ensuring effectiveness and plausibility. When aggregated, they yield reliable membership signals without arousing suspicion. Our attack (IA) has three main stages: generating queries (Sec- tion 5.1) , generating ground-truth answers for these queries (Sec- tion 5.2), and finally aggregating model responses for membership inference (Section 5.3). 5.1 Query Generation We begin by creating a set of queries that are highly specific to the target document ğ‘‘âˆ—. The overarching goal is to produce questions that are natural in formâ€”thus undetectableâ€”and highly relevant to ğ‘‘âˆ—, making them effective probes for membership. Concretely, each query must simultaneously: (i) ensure retrieval of the target document ğ‘‘âˆ— (if present in the RAG) by incorporating keywords or contextual clues, and (ii) probe with questions that can only be accurately answered with the target document ğ‘‘âˆ— as relevant context. We achieve this by designing a two-part query format consisting of a Retrieval Summary and a Probe Question, as described below. Retrieval Summary. We first craft a dedicated prompt, denoted Psum, to guide an LLM in producing a short, natural-sounding de- scription ğ‘ âˆ—. This summary, generated onlyonce per target document, includes key terms fromğ‘‘âˆ— and mimics