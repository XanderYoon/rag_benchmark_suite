to bypass a model’s guardrails and induce it to perform unintended actions. To coun- teract such vulnerabilities, many LLM deployments incorporate countermeasures like detection tools to selective reject such queries. Several prior works on membership inference and data extrac- tion for RAG systems rely on prompting the model to either re- gurgitate its context directly or answer questions indirectly tied to the content. For instance, Zeng et al. [56] explore targeted and untargeted information extraction by designing queries that trigger the retrieval of specific documents, paired with a command suffix intended to induce the generative model to repeat its context and, consequently, the retrieved documents. Similarly, Anderson et al. [2] propose directly querying the RAG system to determine whether a target document is included in the model’s context. On the other hand, some related works [7, 42] employ adversarial prompts to coax the generator into regurgitating data from its context. However, these adversarial (or even unnatural) queries heav- ily rely on prompt injection techniques. Prompt injection [40] is a broader concept that refers to an LLM vulnerability where attackers craft inputs to manipulate the LLM into executing their instructions unknowingly. In the specific case of these prompt injection attacks, known as context probing attacks, the adversary attempts to extract information from the hidden context provided to the LLM. There- fore, it is crucial to analyze the effectiveness of existing inference attacks that rely on prompt injection to determine how successful their queries are in bypassing current detection filters—an area currently underexplored in the literature. To evaluate the ability of current attacks to bypass detection methods, we adopt two different approaches. First we utilize Lak- eraGuard, a commercial off-the-shelf guard model designed for detecting prompt-injection and jailbreak attempts [24], to evaluate queries from different attacks. While this tool can detect queries