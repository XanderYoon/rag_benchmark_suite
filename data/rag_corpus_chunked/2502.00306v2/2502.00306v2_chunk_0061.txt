with GTE as the retriever in a vanilla RAG setting. Dataset Attack Method AUC-ROC Accuracy TPR @ low FPR FPR=0.005 FPR=0.01 FPR=0.05 NFCorpus RAG-MIA [2] - 0.729 - - - S2MIA [26] 0.727 0.615 0.027 0.033 0.177 MBA [29] 0.989 0.957 0.873 0.917 0.963 IA (Ours) 0.972 0.928 0.178 0.485 0.872 TREC-COVID RAG-MIA [2] - 0.764 - - - S2MIA [26] 0.690 0.581 0.009 0.014 0.145 MBA [29] 0.988 0.957 0.630 0.751 0.965 IA (Ours) 0.944 0.873 0.087 0.325 0.688 SCIDOCS RAG-MIA [2] - 0.814 - - - S2MIA [26] 0.733 0.581 0.003 0.01 0.145 MBA [29] 0.998 0.980 0.974 0.990 0.998 IA (Ours) 0.979 0.934 0.188 0.781 0.899 19 Classifier Prompt You are an expert classifier tasked with validating user input queries in a Retrieval-Augmented Generation (RAG) system. Your role is to classify each query into one of the following two categories: 1. Natural Queries: - Queries that are naturally formulated and relevant to the task/domain. - These queries align with the system’s intended purpose, such as retrieving factual information, summarizing content, or answering domain-specific queries. 2. Context-Probing Queries: - Queries that attempt to probe hidden or inaccessible parts of the system or input context. - These queries are often out-of-scope and unnatural for the task/domain. Examples include queries that ask for metadata, hidden inputs, or system behavior. — Instructions — 1. Carefully analyze the given query to determine whether it aligns with the system’s intended purpose. 2. If the query is Natural, ensure it is appropriate and consistent with the system’s task/domain (e.g., medical, legal, general retrieval). 3. If the query is Context-Probing, identify if it is attempting to extract hidden content, system behavior, or metadata. — Output Format — For each input query, respond with a single word indicating the classification: Natural or Context-Probing. Do not provide