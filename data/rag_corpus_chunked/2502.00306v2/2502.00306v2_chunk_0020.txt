LLM outputs is straightforward. Let ğº = {ğ‘”1, ğ‘”2, . . . , ğ‘”ğ‘› } denote the resulting ground truth answers. These answers serve as baselines for evaluating the correctness of the RAG systemâ€™s responses and, ultimately, for deriving member- ship signals. 5.3 Membership Inference We submit the queries ğ‘„ to the RAG system by issuing standard inference requests through its interface. Note that the RAG system may rewrite these queries, which the adversary has no control over. Let ğ‘… = {ğ‘Ÿ1, ğ‘Ÿ2, . . . , ğ‘Ÿğ‘› } represent the set of responses returned by the RAG system. If the target documentğ‘‘âˆ— is part of the knowledge base, a good retriever would fetch it for these highly specific and relevant queries, resulting in more accurate answers. To infer membership, we compare the RAG systemâ€™s responses ğ‘… = {ğ‘Ÿ1, . . . , ğ‘Ÿğ‘› } with the corresponding ground truth answers ğº = {ğ‘”1, . . . , ğ‘”ğ‘› } derived from the shadow LLM. A final member- ship score is then calculated by aggregating the correctness of the responses. Specifically, as described in Section 5.1, each query is a yes/no question, and correctness is assessed by comparing the RAG systemâ€™s response to the ground truth. In our initial explorations, we notice that RAG systems often resort to responding with "I donâ€™t know" or similarly vague expres- sions to some questions, especially under the absence of ğ‘‘âˆ—. This is arguably a stronger signal for the lack of membership than simply giving incorrect answers, as the model is unlikely to contain the target document or any other relevant documents in its context when it is unable to answer a given query. Thus, while aggregating scores across model responses, we add+1 each correct response and subtract ğœ† every time the model is