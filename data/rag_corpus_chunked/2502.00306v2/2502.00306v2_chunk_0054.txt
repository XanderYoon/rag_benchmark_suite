in the specified format:\n“[Mask_i]: answer_i\n" for each mask: Interrogation Attack (Ours) I want to ask about Comprehensive survey of PageRank issues, models, solution methods, and future research areas. Does the paper act as a companion or extension to the “Inside PageRank" paper by Bianchini et al.? Please answer with “Yes, " “No, " or “I don’t know" Table 4: Example exact queries generated by each of the baseline attacks (RAG-MIA, S 2-MIA, MBA and our attack (IA) for the same target document (before query rewriting). 100 75 50 25 0 25 50 75 100 MIA Score 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 Percentage of Samples RAG (Non-Members) LLM (Non-Members) (a) Gemma2-2B 100 75 50 25 0 25 50 75 100 MIA Score 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 Percentage of Samples (b) Llama3.1-8B 100 75 50 25 0 25 50 75 100 MIA Score 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 Percentage of Samples (c) Phi4-14B Figure 10: Distribution for MIA scores for non-member documents for TREC-COVID, using the RAG’s generator directly without any context (LLM), and when using the RAG normally (RAG). We observe peculiar behavior for the Llama model, where the model’s ability to answer questions deteriorates significantly in the presence of unrelated documents. using the RAG setup (RAG) and querying the underlying LLM without providing any context (LLM). Ideally, the model’s ability to answer questions related to a target document should improve when that document is available, as this justifies the use of retrieval- augmented generation. For non-member documents, an interesting trend emerges (Fig- ure 10). The Gemma2 and Phi4 models exhibit similar MIA scores regardless of context presence, as expected, since the provided docu- ments are unrelated. However, the Llama model behaves peculiarly: not only does it successfully