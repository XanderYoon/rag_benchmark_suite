the RAG’s generator directly without any context (LLM), and when using the RAG normally (RAG). The Llama model can answer most questions correctly even when the relevant document is absent from context, suggesting that it has seen similar documents in its training. Table 5: Performance comparison of the three query genera- tion methods using the metrics of Attack Success Rate (ASR), Retrieval Recall, and Semantic Diversity. Method ASR Retrieval Recall Semantic Diversity Instruction Only 0.894 0.837 0.55 Few-Shot Prompting 0.907 0.863 0.537 Iterative Generation 0.894 0.893 0.475 during training. However, without precise knowledge of its training corpus, we can only speculate. More importantly, our findings highlight that users of RAG systems should benchmark whether the underlying model truly benefits from additional context. While our attack is designed as a MIA, it can be adapted for analyses like ours to assess whether incorporating external documents meaningfully enhances model performance. D RAG Without Query-Rewriting As mentioned in Section 6.1, in addition to the RAG setting with query rewriting, we also evaluate the vanilla RAG setting, where the input query is sent directly to the retriever without any modification. For this evaluation, we use LLaMA 3.1-8B as the generator and GTE as the retriever. The results are presented in Table 8. In the vanilla setting, without any detection filter or query rewriting, the MBA attack demonstrates better performance compared to our attack, although our attack achieves high AUC across all settings. However, it is important to note that, in a realistic scenario, the MBA attack’s queries are unlikely to pass detection filters, limiting its practical applicability. E Making Prior Works Stealthy Existing membership inference attacks against RAG models were de- veloped assuming a plain, minimal RAG setup, without accounting for the additional components often present in real-world systems, such as detection mechanisms. This