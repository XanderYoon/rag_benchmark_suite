easily using auxiliary models (Section 4). Several recent works have also proposed context leakage and integrity attacks, where the adversary has the capability of injecting malicious documents into RAG knowledge database [6, 21] or can poison the RAG system direcly [39]. This threat model is different than ours as we do not assume any RAG poisoning or knowledge base contamination for our MIA. 3 Threat Model Adversaryâ€™s Objective.Given access to a RAG system utilizing a certain set of documents D, the adversary wants to infer whether a given document ğ‘‘âˆ— is part of this set of documents being utilized in the given RAG system. More formally, the adversaryâ€™s goal is to construct a membership inference function A such that, given access to the RAG system S: A (ğ‘‘âˆ—, S) = ( 1, if ğ‘‘âˆ— âˆˆ D 0, if ğ‘‘âˆ— âˆ‰ D The very use of a RAG system implies that the generative modelâ€™s knowledge is not wholly self-contained. This reliance often stems from the need to reference specific, potentially sensitive informa- tion or to incorporate detailed factual knowledge that is not part of the systemâ€™s pre-trained model. Depending on the nature of the documents used, successful inference can lead to significant implications while posing unique challenges: â€¢ PII-Containing Documents: Documents that contain per- sonally identifiable informationâ€”such as internal user records, support tickets, financial transactions, or health-related formsâ€” may not need to be leaked in full for privacy to be compro- mised. The mere confirmation that a particular document is part of the retrieval corpus can reveal that an individual engaged with a system, received a specific service, or ap- pears in a sensitive internal context. Such inferences may already constitute privacy violations under data protection regulations like GDPR, particularly when tied to specific individuals. â€¢ Factual Knowledge Sources: Internal documentation