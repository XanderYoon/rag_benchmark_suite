queries for each target document. There are several approaches to achieve this by prompt- ing GPT-4o, and we consider three distinct strategies: (1) Instruction Only: Provide a detailed instruction to GPT-4o to generate the queries. (2) Few-Shot Prompting: In addition to the detailed instruc- tion, include an example of a text along with multiple exam- ple queries based on the text. (3) Iterative Generation: Use the same instruction and exam- ples but execute the query generation in three stages. In each stage, we generate five queries, and in subsequent stages, we add the previously generated queries to the prompt and instruct the model to generate new, non-redundant queries. This ensures the final set of queries is diverse and avoids duplication. To compare these strategies, we consider three metrics. A good set of queries for each document should be diverse, achieve a high retrieval score (i.e., the target document is successfully retrieved from the database), and lead to better attack performance. Thus, the metrics we use are: • Attack Success Rate (ASR): The effectiveness of the attack using the generated queries. • Retrieval Recall: Described in Section 6.2, measuring whether the target document is retrieved. • Semantic Diversity: Calculated as the average cosine dis- tance, representing the diversity of the queries for each doc- ument based on their semantic embeddings. We conducted a small experiment with 250 members and 250 non- members from the TREC-COVID dataset, with Llama 3.1 Instruct-8B as both the shadow model and generator to evaluate the ASR, with ColBERT as the retriever model. For semantic similarity, we used the all-MiniLM-L6-v2 model to compute embeddings. As shown in Table 5, few-shot prompting achieves higher ASR and retrieval recall compared to the other two methods. The third generation strategy performed the worst across all three metrics. Consequently, we adopt