document ğ‘‘âˆ—, if present, is consistently retrieved by the RAG system during its operation. Additionally, the adversary must craft queries in a manner that not only distinguishes the target document from other potentially related documents in D but also bypasses any intermediate processes employed by the RAG system (as discussed in Section 4) that may limit inference success. Adversaryâ€™s Capabilities.We operate under a black-box access model where the adversary can query the target RAG system, but possesses no information about its underlying models or compo- nents. We assume the adversary has access to an auxiliary LLM, which it leverages to generate queries and interpret answers. The adversary lacks knowledge of the retriever and generator models used by the victim, including their hyperparameters (e.g., ğ‘˜ for top- ğ‘˜ retrieval, temperature settings for generation, etc.). The adversary also lacks knowledge of system instructions used in the victim RAG system, or query-rewriting strategies (if any) employed. Like in a typical membership inference scenario, the adversary owns a set of non-member documents from the same data distribution, which it uses to establish thresholds for predicting membership. Unlike some prior work [6] that assumes the ability to inject poisoned doc- uments, the adversary in this setup has no read or write access to the data used by the victimâ€™s RAG system. 4 Limitations of Existing Inference Attacks on RAG Systems A well-established issue in deploying LLM-based systems is jail- breaking, where adversarial prompts are used to bypass a modelâ€™s guardrails and induce it to perform unintended actions. To coun- teract such vulnerabilities, many LLM deployments incorporate countermeasures like detection tools to selective reject such queries. Several prior works on membership inference and data extrac- tion for RAG systems rely on prompting the model to either re- gurgitate its context directly or answer questions indirectly tied