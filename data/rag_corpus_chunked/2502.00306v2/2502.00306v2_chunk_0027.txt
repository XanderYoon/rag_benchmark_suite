MBA evades detection, its performance is inconsistent across different LLM generators in the RAG system. In contrast, our attack maintains strong performance while slipping past detection filters. Among the baselines, S2MIA consistently performs the worst, highlighting its limitations in this evaluation. Additionally, the TREC-COVID dataset poses more challenges for our attack, with lower performance metrics (AUC, accuracy, and TPR@low FPR) compared to NFCorpus and SCIDOCS. This suggests that the dataset’s complexity or the diversity of its queries and documents may intro- duce extra difficulties for inference attacks. While IA shows slightly lower TPRs, this trade-off is intentional, prioritizing undetectability. In contrast, MBA and similar attacks prioritize performance over stealth, making them more suitable for illustrative purposes than practical use. We posit that the superior performance of our attack stems from the utilization of multiple, diverse questions per document, in con- trast to prior methods that typically rely on a single query. This mul- tiplicity allows for a broader exploration of the document’s content, enhancing the likelihood of uncovering exploitable information (as detailed in Section 6.3.1). Similarly, the mask-based attack (MBA) benefits from analyzing multiple masked words within a single doc- ument, which might help explain its stronger performance relative to the other baselines. Retrieval Recall. In addition to directly measuring inference suc- cess, we consider retrieval recall as another metric. A good attack query is expected to retrieve the target document if it is a mem- ber. In Table 3, we present the retrieval recall for all attacks across three datasets using both BGE and GTE as retrievers, before and after query rewriting. All attacks demonstrate high recall (≥ 0.9) in all settings, indicating their effectiveness in retrieving the target document. It is not surprising that some baselines achieve a perfect recall of 1.000, often outperforming our attack. This is