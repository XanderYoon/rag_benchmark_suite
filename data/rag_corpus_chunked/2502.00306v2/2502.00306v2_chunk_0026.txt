True Positive Rates (TPRs) at low False Positive Rates (FPRs), which provide valuable insights into the success of our attack in inferring membership. Since RAG-MIA only produces a binary membership label for each target document, https://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024 we report accuracy for that attack and compute accuracy for other attacks by using a threshold corresponding to FPR= 0.1. 6.2 Results As shown in Section 6.2, our attack outperforms all baselines in both AUC and accuracy across various settings, including all datasets and RAG generator types. In particular, for the TREC-COVID dataset with Gemma2-2B as the generator, there is a noticeable performance gap in AUC between our attack and the baselines, demonstrating the the robustness of our method. In terms of TPR@low FPR, our attack generally achieves higher performance in most settings (Fig- ure 4). However, the MBA baseline shows better TPR in some cases, specifically when using LlamA 3.1 as the RAG generator. On the other hand, our attack is robust to changes in the generator. Lakera and GPT4-based detection methods are highly effective at spotting queries corresponding to MBA, with detection rates of 0.974 and 0.928, respectively, and high confidence levels (aver- age confidence of 0.964). This means attacks like MBA would typically fail to bypass these detection models in a RAG sys- tem. For comparison, we hypothetically assume in our evaluations that MBA and other attacks could evade detection—though they do not—while our attack (IA) successfully bypasses detection. Even if MBA evades detection, its performance is inconsistent across different LLM generators in the RAG system. In contrast, our attack maintains strong performance while slipping past detection filters. Among the baselines, S2MIA consistently performs the worst, highlighting its limitations in this evaluation. Additionally, the TREC-COVID dataset poses more challenges for our attack, with lower performance metrics (AUC, accuracy, and TPR@low FPR)