ğ‘‘âˆ— as relevant context. We achieve this by designing a two-part query format consisting of a Retrieval Summary and a Probe Question, as described below. Retrieval Summary. We first craft a dedicated prompt, denoted Psum, to guide an LLM in producing a short, natural-sounding de- scription ğ‘ âˆ—. This summary, generated onlyonce per target document, includes key terms fromğ‘‘âˆ— and mimics realistic user queries (e.g., â€œI have a question about . . . â€). Including these keywords increases the likelihood of retrieving ğ‘‘âˆ—, assuming it resides in the RAG systemâ€™s knowledge base. The exact prompts used to generateğ‘ âˆ— are detailed in the Appendix (Figure 16). 5 You are a helpful assistant, below is a query from a user and some relevant contextsâ€¦ Contexts ğ·ğ‘˜: Query : à·ğ‘ğ‘– à·ğ‘ğ‘– gn g1 g2 rn ğ‘Ÿ r2 RAG System ğ’® LLM (GPT-4o) Shadow LLM (GPT-4o mini) Target Document ğ’…âˆ— System Instruction Generator ğ’¢ Private Database ğ’Ÿ Retriever â„› Ground-truth Answers ğ‘® RAGâ€™s Responses ğ‘¹ 1 ğ‘› à· ğ•€ ri = gi âˆ’ ğœ†ğ•€[ri = UNK] MIA Score .â€¦ .â€¦ .â€¦ qn q1 q2 Generated Queries ğ‘¸ .â€¦ Query Rewriting Malicious Input Detection Figure 2: Overview of the problem setting and our Interrogation attack. Given black-box access to a RAG system S, the adversary wants to infer membership of a given target document in the RAGâ€™s private database. Our method uses auxiliary LLMs to generate benign queries in the form of natural questions, and uses the correctness of the generated responses as a signal for membership inference test. Probe Question. Next, we generate a set of questions that are highly aligned with the content of ğ‘‘âˆ—. Drawing inspiration from doc2query tasks in the IR literature, we adopt a few-shot prompting strategy [8] that instructs an LLM to create natural, information- seeking queries based on ğ‘‘âˆ—.