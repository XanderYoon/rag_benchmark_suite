easily or lack potency, we achieve successful inference while remaining virtu- ally indistinguishable from natural queries, with detection rates as low as 5%, compared to upwards of 90% for most inference at- tacks against RAG. Finally, we analyze our attackâ€™s failure cases (Section 7) and find that RAG may often be unnecessary: in many instances, the underlying LLM can answer questions about a given document without direct access to it, thereby questioning the ne- cessity of a RAG-based system for such scenarios. Code for our ex- periments is available at https://github.com/ali7naseh/RAG_MIA. 2 Background and Related Work In this section, we describe the components of a RAG system (Sec- tion 2.1), revisit membership inference for machine learning (Sec- tion 2.2), and discuss recent works on privacy leakage in RAG systems in (Section 2.3). 2.1 Retrieval Augmented Generation (RAG) Let G be some generative LLM, with some retriever model R, and D denote the set of documents part of the RAG system S. Most real-world systems that deploy user-facing LLMs rely on guardrails [10] to detect and avoid potentially malicious queries. One such technique that also happens to benefit RAG systems [3, 28, 31, 35, 50] is â€œquery rewriting", where the given query ğ‘ is transformed before being passed on to the RAG system. Query rewriting is helpful in dealing with ambiguous queries, correcting typographical errors, providing supplementary information, in addition its utility in circumventing some adversarial prompts [19]. Ë†ğ‘ = rewrite(ğ‘). (1) For the transformed query Ë†ğ‘, the retrieverğ‘… begins by producing an embedding for Ë†ğ‘ and based on some similarity function (typically cosine similarity), fetching the ğ‘˜ most relevant documents ğ·ğ‘˜ = arg top-ğ‘˜ğ‘‘ âˆˆ Dsim( Ë†ğ‘, ğ‘‘), (2) where sim() represents the similarity function, and arg top-ğ‘˜ se- lects the top-ğ‘˜ documents with the highest similarity scores. The generator