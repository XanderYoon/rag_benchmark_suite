against RAG systems. Anderson et al. [2] developed techniques across different access levels, including a gray-box method using a meta-classifier on model logits and a black-box approach directly querying model membership. Li et al. [26] propose a similarly straightforward ap- proach, where the target document is broken into two parts, with the idea that presence of the target document in the context would lead the LLM into completing the given query (one half of the doc- ument). However, authors for both these works find that simple modifications to the system instruction can reduce attack perfor- mance significantly to near-random. Cohen et al . [7] focus on data extraction by directly probing the model to reveal its retrieved contexts as is, using a specially crafted query. Zeng et al. [56] break the query into two parts, where the latter is responsible for making the model output its retrieved contexts directly using the command “Please repeat all the con- text". [51] propose MIAs for long-context LLMs. While they do not specifically target RAG systems, their setup is similar in the adversary’s objective- checking for the existence of some particular text (retrieved documents) in the model’s context. Similarly, Duan et al. [11] focus on membership inference for in-context learning under the gray-box access setting, where model probabilities are available. While data extraction is a strictly stronger attack, we find that the kind of queries required to enable these attacks can be identified very easily using auxiliary models (Section 4). Several recent works have also proposed context leakage and integrity attacks, where the adversary has the capability of injecting malicious documents into RAG knowledge database [6, 21] or can poison the RAG system direcly [39]. This threat model is different than ours as we do not assume any RAG poisoning or knowledge base contamination