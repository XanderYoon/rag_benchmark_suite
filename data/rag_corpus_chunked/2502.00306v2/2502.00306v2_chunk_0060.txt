generator has sufficient prior knowledge to answer most of the questions correctly without relying on retrieved documents. For instance, with an example from the NFCorpus dataset, LLaMA 3.1 (used as the RAG generator) can answer 23 out of 30 questions accurately without accessing any retrieved documents. This demonstrates that, even though the document is not a member of the database, the LLM can answer most of the questions correctly based on its inherent knowledge. H ROC Curves For completeness, we provide ROC curves across all attacks and datasets for all of our experiments. These ROC curves are presented in Figures 21, 22, 23, and 24. 18 Table 7: Attack Performance on Datasets when BGE is used as the RAG retriever, with llama 3-8B as the generator Dataset Attack Method AUC-ROC Accuracy TPR @ low FPR FPR=0.005 FPR=0.01 FPR=0.05 NFCorpus RAG-MIA [2] - 0.744 - - - S2MIA [26] 0.747 0.679 0.137 0.197 0.378 MBA [29] 0.849 0.786 0.333 0.384 0.622 IA (Ours) 0.965 0.917 0.157 0.501 0.732 TREC-COVID RAG-MIA [2] - 0.751 - - - S2MIA [26] 0.691 0.622 0.102 0.131 0.274 MBA [29] 0.855 0.834 0.308 0.475 0.679 IA (Ours) 0.936 0.854 0.065 0.389 0.597 SCIDOCS RAG-MIA [2] - 0.813 - - - S2MIA [26] 0.742 0.658 0.177 0.23 0.325 MBA [29] 0.908 0.888 0.682 0.736 0.842 IA (Ours) 0.973 0.926 0.233 0.617 0.847 Table 8: Attack Performance on Datasets when Llama3 (8B) is used as the RAG generator, with GTE as the retriever in a vanilla RAG setting. Dataset Attack Method AUC-ROC Accuracy TPR @ low FPR FPR=0.005 FPR=0.01 FPR=0.05 NFCorpus RAG-MIA [2] - 0.729 - - - S2MIA [26] 0.727 0.615 0.027 0.033 0.177 MBA [29] 0.989 0.957 0.873 0.917 0.963 IA (Ours) 0.972 0.928 0.178 0.485 0.872 TREC-COVID RAG-MIA [2] - 0.764 - - - S2MIA