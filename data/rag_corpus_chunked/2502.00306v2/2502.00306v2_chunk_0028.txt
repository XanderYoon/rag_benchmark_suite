we present the retrieval recall for all attacks across three datasets using both BGE and GTE as retrievers, before and after query rewriting. All attacks demonstrate high recall (≥ 0.9) in all settings, indicating their effectiveness in retrieving the target document. It is not surprising that some baselines achieve a perfect recall of 1.000, often outperforming our attack. This is because these The drop primarily stems from the model’s ability to answer questions correctly without any context. See Appendix C for details. 8 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0True Positive Rate TREC-COVID 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0True Positive Rate SCIDOCS 0.0 0.2 0.4 0.6 0.8 1.0 False Positive Rate 0.0 0.2 0.4 0.6 0.8 1.0True Positive Rate NFCorpus RAG-MIA S2-MIAAUC=0.687 MBAAUC=0.741 IA (Ours)AUC=0.991 Figure 4: ROC curves for Command-R (7B) as generator, GTE as retriever, across various datasets. Our attack (IA) achieves near-perfect inference across multiple datasets. ROC curves for other RAG configurations, can be found in Appendix H. baselines typically integrate the entire target document or signifi- cant portions of it directly into the query. In contrast, our queries are general yes/no questions derived from the target document, making them less explicit. As expected, retrieval recall after paraphrasing is generally simi- lar to or slightly lower than without paraphrasing, but it remains high overall. It is important to note that the retrieval recall for our attack reflects the average proportion of queries that successfully retrieve the target document. For example, a retrieval recall of 0.930 in the paraphrased setting on the TREC-COVID dataset using GTE indicates that, on average, 93% of the 30 questions for each target document successfully retrieve it. This is sufficient to distinguish members from non-members effectively.