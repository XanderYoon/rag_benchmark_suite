It is worth noting that we excluded RAG-MIA from this study, as it does not produce AUC scores for direct comparison. 6.3.3 Distribution of Questions. Prior work shows that social and implicit biases can push LLMs toward generating affirmatively an- swered questions [25]. To measure this tendency, we analyze the distribution of generated questions, ground-truth answers, and the outputs of the RAG system for these questions. We find a clear imbalance: for instance, with Gemma on TREC-COVID, questions answered with â€œyes" appear nearly 12 times more often than those answered with â€œno. " Accuracy is also skewed: 61% for "yes" re- sponses and 39% for â€œno", suggesting a bias toward affirmative answers. To address this, we adapted the attack and scoring func- tions such that the model is given multiple answers to choose from, with only one being correct. Despite the format change, attack effectiveness remains comparable. For example, we sampled 100 members and 100 nonmembers from SCIDOCS and tested the attack against LLaMA 3.1 (without query rewriting). We found that attack performance remained almost unchanged: the AUC is 97.7% for yes/no questions and 97.8% for multiple-choice questions. 0 2 4 6 8 Lambda 0.94 0.95 0.96 0.97 0.98 0.99 AUC TREC-COVID NFCorpus SCIDOCS Figure 7: Attack performance (AUC) as a function of the UNK penalty ğœ†. Performance increases steadily with higher ğœ† values before leveling off. 6.3.4 UNK Response Penalty ( ğœ†). As described in (5), ğœ† is a hyper- parameter that penalizes the RAG system when it cannot answer a question. We set ğœ† to a value greater than one (5) to reflect this intuition, but find that performance remains stable as long as it is reasonably large. For example, Gemma-2 on TREC-COVID shows an increase in attack AUC from 0.938 at ğœ† = 0 to 0.954 at