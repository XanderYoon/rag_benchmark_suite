the retrieval recall for our attack reflects the average proportion of queries that successfully retrieve the target document. For example, a retrieval recall of 0.930 in the paraphrased setting on the TREC-COVID dataset using GTE indicates that, on average, 93% of the 30 questions for each target document successfully retrieve it. This is sufficient to distinguish members from non-members effectively. Impact of Retriever. Apart from GTE [27], we also experiment with BGE [27] as a retriever. Table 3 compares the retrieval rates for both retrievers across various attacks, with or without query rewriting. Although GTE and BGE differ slightly in terms of re- call, all attacks maintain consistently high retrieval rates overall. We also evaluate the end-to-end RAG after replacing GTE with BGE, under the same settings as Section 6.2, with Llama3.1 as the generator. We observe similar performance trends (Table 7) for this setup, confirming our primary conclusion: despite operating more stealthily, our attack achieves performance on par with (often surpassing) baselines. Regarding query rewriting, Table 3 shows that each attack’s recall rate–including IA–does not significantly degrade after rewrit- ing. However, MBA exhibit a noticeable performance drop under rewriting (see Section 6.2 and Table 8), while IA is minimally af- fected. This observation suggests that with query rewriting, per- formance decline for MBA is not driven by lower retrieval rates. Instead, even when the target document is successfully retrieved, MBA often relies on verbatim queries rather than knowledge-focused probing, rendering it more vulnerable to modifications in query phrasing. 6.3 Ablation Study Here we evaluate the impact of varying several aspects of the RAG system and our attak. 5 10 15 20 25 30 Number of Questions (n) 0.86 0.88 0.90 0.92 0.94 0.96 0.98AUC SCIDOCS NFCorpus TREC-COVID Figure 5: Changes in attack performance (AUC) for our attack as the