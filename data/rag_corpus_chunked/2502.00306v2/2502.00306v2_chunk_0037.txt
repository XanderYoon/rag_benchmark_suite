scores. False Negatives. The fact that we observe high retrieval recall for our attack rules out the possibility of the target document being absent from the context provided to the RAG generator. The RAGâ€™s inability to answer the question properly can thus have two po- tential reasons. On rare occasions, GPT-4o fails to paraphrase the userâ€™s query accurately (see Appendix G for an example), which reflects a shortcoming in the RAG systemâ€”not being able to para- phrase a normal, benign query. For other cases, the RAG generator may struggle to answer the question even when the appropriate document is present in the provided context. Similarly, this can be attributed to the RAGâ€™s generator lacking capabilitiesâ€”especially given the fact that the question, by design, can be answered by GPT-4o-mini under the presence of the target document. False Positives. The RAG answering our queries correctly implies that the target document (corresponding to the query) is not re- quired specifically as context to respond correctly. This can happen if similar documents with the necessary context are fetched by the retriever, or if the generator already possess sufficient knowledge to answer the question without relying on any context (see Ap- pendix G for examples). To better understand this failure case, we compute the similarity between a non-member documentğ‘‘ and the document actually retrieved as context for a query corresponding to that non-memberğ‘‘, across multiple non-member documents and their corresponding queries generated for our attack. In Figure 9, we look at ğ‘›-gram overlap and cosine similarity between retriever embeddings, and visualize them with respect to MIA scores for our attack. We observe that above some certain meaningful threshold (0.2 for 4-gram overlap, 0.9 for embedding cosine similarity), there is a positive correlation between how "similar" the non-member documents are to documents already present in the