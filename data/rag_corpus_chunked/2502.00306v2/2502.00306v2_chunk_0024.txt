Llama 3.1 Instruct-8B [14], Command-R-7B, Mi- crosoft Phi-4 [1], and Gemma-2-2B [48]. Shadow LLM. As described, the shadow LLM is employed to gen- erate ground-truth answers for the questions created based on the target documents. In all experiments, we use GPT-4o-mini as the shadow model because it is fast and cost-efficient, and it belongs to a different family of LLMs compared to the RAG‚Äôs generator. This ensures adherence to the black-box setting scenario, where the adversary has no knowledge of the RAG‚Äôs generator. Query Generation Setting. For IA, we employ few-shot prompt- ing with GPT-4o to generate 30 queries based on the target docu- ment. We also use GPT-4o to generate a short description of the target document, summarizing its main idea and keywords. For details of different prompting strategies and the corresponding prompts for each stage, see Appendix B and Appendix F. RAG Setting. As described in Section 2.1, we evaluate our attack in a more realistic setting compared to previous works, where the RAG system employs query-rewriting on the user‚Äôs query. We implement query-rewriting using a simple query-paraphrasing prompt via GPT-4o. We setùëò = 3 for retrieval and investigate the impact of this hyperparameter across all attacks in Section 6.3.2. These retrieved documents are then provided as context to the generator via a system prompt. Details on both the query-paraphrasing and system prompts are presented in Appendix F. To demonstrate the impact of query-rewriting on inference, we also evaluate attacks in a vanilla RAG setup where query-rewriting is disabled (Appendix D). Baselines. We compare our attack with three prior black-box MIAs against RAG systems: RAG-MIA [2], ùëÜ2MIA [26], and MBA [29]. RAG-MIA takes a simpler approach by directly probing the RAG system to ask if the target document appears in the context. ùëÜ2MIA uses the first half of