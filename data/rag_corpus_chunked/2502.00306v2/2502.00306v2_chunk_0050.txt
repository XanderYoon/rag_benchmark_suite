Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, et al. 2024. The good and the bad: Exploring privacy issues in retrieval-augmented generation (rag). arXiv preprint arXiv:2402.16893 (2024). [57] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023. Retrieve anything to augment large language models. arXiv preprint arXiv:2310.07554 (2023). A Details for Detection Setup Baselines. A robust detection method should also perform well against natural user queries. To evaluate this, we include two QA datasets: SQuAD and AI Medical Chatbot. These datasets allow us to assess how each detection method behaves when faced with standard, benign queries. Datasets. We consider three datasets: three from the BEIR bench- mark, including NFCorpus, TREC-COVID, and SCIDOCS, as well as the HealthCareMagic dataset. From each dataset, we select 125 samples and integrate them into the attack prompt templates, re- sulting in a total of 500 samples for each attack. For the RAG-MIA attack, which includes multiple templates, we distribute the selected samples evenly across the different templates. Metrics. We evaluate the detection methods against these attacks using the detection rate, which measures the proportion of samples identified as "context probing" by the GPT-4o-based classifier or as "prompt injection" by the Lakera detection method. We also include the exact attack queries for our attack (IA) and three baseline attacks (RAG-MIA, S2-MIA, MBA for a fixed docu- ment in Table 4. B Query Generation Setting As mentioned, we utilize GPT-4o to generate queries for each target document. There are several approaches to achieve this by prompt- ing GPT-4o, and we consider three distinct strategies: (1) Instruction Only: Provide a detailed instruction to GPT-4o to generate the queries. (2) Few-Shot Prompting: In addition to the detailed instruc- tion, include an example of a text along with multiple exam- ple queries based on the