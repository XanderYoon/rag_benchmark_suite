document should improve when that document is available, as this justifies the use of retrieval- augmented generation. For non-member documents, an interesting trend emerges (Fig- ure 10). The Gemma2 and Phi4 models exhibit similar MIA scores regardless of context presence, as expected, since the provided docu- ments are unrelated. However, the Llama model behaves peculiarly: not only does it successfully answer most questions generated as part of our attack (as indicated by most MIA scores being > 0), but its performance drops when unrelated documents are provided as context. This suggests that Llama possesses the necessary knowl- edge to answer these questions but is easily confused by irrelevant context. A comparable pattern appears in the distribution of scores for member documents (Figure 11). The Llama model can answer most questions without context, but when the relevant document is included via RAG, its accuracy improves. This implies that Llama has likely encountered the TREC-COVID dataset (or similar data) 16 100 75 50 25 0 25 50 75 100 MIA Score 0.000 0.005 0.010 0.015 0.020 0.025 0.030 Percentage of Samples RAG (Members) LLM (Members) (a) Gemma2-2B 100 75 50 25 0 25 50 75 100 MIA Score 0.00 0.02 0.04 0.06 0.08 Percentage of Samples (b) Llama3.1-8B 100 75 50 25 0 25 50 75 100 MIA Score 0.00 0.01 0.02 0.03 0.04 0.05 Percentage of Samples (c) Phi4-14B Figure 11: Distribution for MIA scores for member documents for TREC-COVID, using the RAGâ€™s generator directly without any context (LLM), and when using the RAG normally (RAG). The Llama model can answer most questions correctly even when the relevant document is absent from context, suggesting that it has seen similar documents in its training. Table 5: Performance comparison of the three query genera- tion methods using the metrics of Attack Success Rate