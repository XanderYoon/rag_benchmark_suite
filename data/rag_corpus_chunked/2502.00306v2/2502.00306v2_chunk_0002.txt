Generation (RAG) as a plausible remedy to ground model outputs. RAG involves retriev- ing relevant text from a knowledge base for a given query using a retrieval model. These retrieved documents are then incorporated into the model’s prompt as context, augmenting its knowledge. RAG offers a promising approach to grounding model outputs while en- abling flexible, domain-specific knowledge customization without the need for expensive model retraining. However, this advantage of parameter-free customization introduces a significant vulnerabil- ity: exposure to adversaries aiming to extract sensitive information from the underlying set of documents. Apart from adversaries that can inject their own documents via poisoning [6], prompt-stealing adversaries [18] may be able to infer the presence of retrieved doc- uments present in the model’s context via membership inference [46], or extract them directly via data-extraction [5]. Membership inference attacks (MIAs) in machine learning at- tempt to discern if a given record was part of a given model’s training data. MIAs thus have great utility for privacy auditing, copyright violations [32], and test-set contamination [38]. While MIAs generally relate to the information contained in the model’s ∗ Equal Contribution † Correspondence to anaseh@cs.umass.edu parameters (with the model having seen some data during train- ing), inferring the presence of particular documents in a RAG’s data-store is different as the knowledge is not directly contained in model parameters. RAG systems introduce unique risks: even without exposing full content, simply confirming that a document is indexed can compromise privacy or reveal sensitive internal con- text. For example, documents containing PII might disclose that a user interacted with a system, while the presence of internal guide- lines or strategy papers can hint at organizational priorities. These inferences carry implications for privacy, IP exposure, and regu- latory compliance. Since RAG systems ingest data at deployment, membership inference can serve