questions accurately based on private data- base knowledge. This makes devising countermeasures without negatively impacting performance challenging. Figure 8 provides valuable insights for considering defensive strategies against our attack. The core reason our attack is effective lies in the distinguish- able distributions of MIA scores for members and non-members. Any effective countermeasure must focus on making these two dis- tributions less distinguishable, either by moving members’ scores closer to non-members’ or vice versa. Moving members towards non-members implies that the RAG system would deliberately answer questions related to docu- ments in the database incorrectly. However, this approach would degrade the overall performance and utility of the RAG system, undermining its primary purpose. Moving non-members towards members would require the RAG system to answer questions accurately even when the related document is not in the database. While this could be a promising defense against membership inference, but then it also undermines the necessity of the RAG system if the generator is consistently able to answer questions without relying on the retrieved context. We already observe something similar with Llama, where the generator can answer several queries successfully without any provided con- text, but refuses to answer under the presence of irrelevant queries (Appendix C). Both approaches present significant trade-offs, highlighting the difficulty of defending against our attack without compromising either the system’s performance or its utility. 8 Conclusion In this work, we introduced Interrogation Attack (IA), a member- ship inference attack targeting Retrieval-Augmented Generation (RAG) systems. Unlike prior methods, IA leverages natural, topic- specific queries that remain undetectable by existing defense mech- anisms while maintaining high effectiveness. Through extensive experiments across diverse datasets and RAG configurations, we demonstrated the robustness of our attack, achieving superior infer- ence performance with minimal cost and low detection rates. Our analysis highlights the vulnerabilities inherent