injection detector, including detailed instructions and examples, is presented in Figure 13. The few-shot prompt used to generate 30 yes/no questions with GPT-4o is shown in Figure 14. Following question generation, the prompt for generating the gen- eral description of each target document with GPT-4o is provided in Figure 16. Additionally, the short prompt for rewriting the input query of the RAG system is illustrated in Figure 15. This prompt is a modified version of the best-performing prompt reported in [22]. Finally, the RAG system prompt and the prompt used to gen- erate ground-truth answers are presented in Figures 17 and 18, respectively. G Failed Cases Examples As described in Section 7.2, one potential reason a member receives a low MIA score is when GPT-4o fails to paraphrase the question accurately. While this is a rare occurrence, it can impact overall performance. In Figure 19, we provide an example of this type of failure. For non-members misclassified as members due to high MIA scores, we identify two main potential reasons. The first occurs when, although the non-member document is not in the RAG data- base, there exists at least one similar document in the database that the LLM uses to answer the questions. An example of this case, taken from the SCIDOCS dataset, is shown in Figure 20. For all 30 questions, the same similar document is consistently retrieved from the database. The second potential reason arises when the RAG generator has sufficient prior knowledge to answer most of the questions correctly without relying on retrieved documents. For instance, with an example from the NFCorpus dataset, LLaMA 3.1 (used as the RAG generator) can answer 23 out of 30 questions accurately without accessing any retrieved documents. This demonstrates that, even though the document is not a member of the database,