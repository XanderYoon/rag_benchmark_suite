rendering it more vulnerable to modifications in query phrasing. 6.3 Ablation Study Here we evaluate the impact of varying several aspects of the RAG system and our attak. 5 10 15 20 25 30 Number of Questions (n) 0.86 0.88 0.90 0.92 0.94 0.96 0.98AUC SCIDOCS NFCorpus TREC-COVID Figure 5: Changes in attack performance (AUC) for our attack as the number of questions ( ğ‘›) increases, when the RAGâ€™s generator is LLaMA 3.1. We observe improvement in perfor- mance across all three datasets. 6.3.1 Number of Questions ( ğ‘›). While we use 30 questions as the default for our attack, we vary this number (ğ‘›) to understand its impact on attack inference. Our evaluations show that the num- ber of questions significantly impacts the attack AUC, with more questions improving performance. As shown in Figure 5, increas- ing the number of questions consistently results in higher AUC values, across all three datasets. Notably, with just 5 questions, the AUC of our attack outperforms the baselines. However, we observe diminishing returns at higher question counts, with AUC improvement stabilizing at a saturation point. This suggests that while adding more questions generally enhances performance, the marginal benefit reduces as the number of questions increases. 6.3.2 Number of Retrieved Documents ( ğ‘˜). While our attack demon- strates robustness across different retrievers and generator models, certain other aspects of a RAG system, such as the number of docu- ments retrieved as context (ğ‘˜), are not under the adversaryâ€™s control. This optimal value of ğ‘˜ can vary across different tasks and datasets. While we set this hyperparameter to 3 in our experiments, we conduct an ablation study to examine the effect of the number of retrieved documents (ğ‘˜) on the attack AUCs. As shown in Figure 6, 9 Dataset Generator Attack Method AUC Accuracy TPR@FPR