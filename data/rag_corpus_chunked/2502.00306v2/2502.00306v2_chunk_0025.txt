evaluate attacks in a vanilla RAG setup where query-rewriting is disabled (Appendix D). Baselines. We compare our attack with three prior black-box MIAs against RAG systems: RAG-MIA [2], ùëÜ2MIA [26], and MBA [29]. RAG-MIA takes a simpler approach by directly probing the RAG system to ask if the target document appears in the context. ùëÜ2MIA uses the first half of the target document as a query and calculates the semantic similarity score (i.e., BLEU) between the RAG system‚Äôs responses and the original document as the membership score. They hypothesize that if a document is present in the database, the RAG system‚Äôs responses will exhibit high semantic similarity to the original content. MBA uses a proxy-LM to selectively mask words in the target document, followed by querying the RAG to predict those masked words. The number of successfully predicted masked words is used as the membership score. In our experiments, we use Qwen-2.5-1.5B [54] as the proxy LM. In line with our black-box assumptions, we configure each attack so that the adversary has access only to the final generated answers, without any logit-level data. Concretely, for RAG-MIA and ùëÜ2MIA, we focus on their black-box versions, which rely solely on the out- puts rather than logits/perplexity. We describe the exact prompting strategies for RAG-MIA and ùëÜ2MIA, along with an example of the format used for MBA, in Table 1. Metrics. Following previous works, we evaluate our attack using the AUC-ROC score and True Positive Rates (TPRs) at low False Positive Rates (FPRs), which provide valuable insights into the success of our attack in inferring membership. Since RAG-MIA only produces a binary membership label for each target document, https://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024 we report accuracy for that attack and compute accuracy for other attacks by using a threshold corresponding to FPR= 0.1. 6.2 Results As shown