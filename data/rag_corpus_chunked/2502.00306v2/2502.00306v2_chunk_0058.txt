Table 6: Evaluating the effectiveness of malicious-input de- tection against baseline attacks for membership inference. E.2 Adapting Queries to Elicit Binary Responses While RAG-MIA already constructs queries that correspond to a â€˜yesâ€™ or â€˜noâ€™ response, other baselines like S2-MIA rely on the RAG system â€˜autocompletingâ€™ some given text. We therefore experiment with a variant of the S 2-MIA attack by adapting the prompt to explicitly ask for a â€˜yesâ€™ or â€˜noâ€™ response. Specifically, for a document ğ‘‘ = ğ‘‘1||ğ‘‘2, whereğ‘‘1 and ğ‘‘2 are the two halves, we modify the query to require a yes/no answer, as illustrated in Figure 12. We find that this modified variant is still easily detected, with a detection score of 0.998 using a GPT-4 based classifier. However, Lakera detection scores decrease slightly, dropping from 0.070 to 17 Modified S2-MIA Prompt [Text 1] â€œğ‘‘1"\n\n [Text 2] â€œğ‘‘2â€\n\n Is Text 2 the com- pletion of Text 1? Answer â€œyes" or â€œno" only. Figure 12: Attack Prompt for the modified S 2-MIA attack, for some document ğ‘‘ = ğ‘‘1||ğ‘‘2. 0.036 for this modified variant. This is notable, as the Lakera clas- sifier appears less effective at detecting queries framed as yes/no questions, consistent with the lower detection scores observed for both RAG-MIA and the modified S2-MIA attack. F Prompts for Experimental Stages In this section, we document the exact prompts used at various stages of our experimental setup. The prompt used to deploy GPT- 4o as a prompt injection detector, including detailed instructions and examples, is presented in Figure 13. The few-shot prompt used to generate 30 yes/no questions with GPT-4o is shown in Figure 14. Following question generation, the prompt for generating the gen- eral description of each target document with GPT-4o is provided in Figure 16. Additionally, the short prompt for rewriting the input query of