can be efficiently offloaded to a distributed array of near-memory accelerators that each oper- ate in parallel on a shard of corpus data with a low-overhead top-K aggregation phase at the end. Leveraging these unique features, we design, implement, and evaluate Intelligent Knowledge Store (IKS), a memory expander with a scale-out near-memory acceleration architec- ture, uniquely designed to accelerate vector database search in future scalable RAG systems. IKS is designed with three re- quirements in mind: (1) The memory capacity of IKS should be cost-effective and scalable because the size of vector databases for RAG applications is several tens or hundreds of gigabytes and is likely to increase. (2) The near-memory accelerators should be managed in userspace as the cost of context switches and kernel overhead would reduce the benefits of offloads. (3) The near-memory accelerators and host CPU should imple- ment a shared address space; otherwise, explicit data move- ments between the CPU and near-memory accelerator ad- dress spaces will negate the benefits of near-memory offloads; another issue that GPU acceleration of ENNS suffers from. Moreover, a partitioned address space requires rewriting the entire vector database application, as ENNS is just one opera- tion we want to accelerate near the memory, while other data manipulation operations, such as updates, should be managed by the host CPU. We designed IKS, a type-2 CXL memory expander/accelerator, to meet all these requirements. Our rationale for choosing CXL over DDR-based (or DIMM-based) [4; 35; 62; 111] near- memory processing architecture is that DIMM-based near- memory processing (1) requires sophisticated mechanisms to share the address space between near-memory accelerators and the host [61], (2) limits per-rank memory capacity and compromises the memory capacity of the host CPU when used as an accelerator, and (3) has limited compute and ther- mal capacity. Instead, IKS relies