[31]. To overcome the challenges presented by LLMs, a potential solution is to enhance them with non-parametric knowledge, where the LLM is augmented with information retrieved from a knowledge source (e.g., text documents). These approaches have recently gained considerable attention in the machine learning communities [22; 26; 41; 49; 58; 73; 83], and have played key roles in some recent breakthrough applications in the tech industry, such as Google Gemini [90], Microsoft Copi- lot [86], and OpenAI ChatGPT with Retrieval Plugins [56]. Retrieval-Augmented Generation (RAG) is the term that is arXiv:2412.15246v1 [cs.CL] 14 Dec 2024 Derrick Quinn et al. used to refer to systems that adopt this approach in the context of LLMs. A RAG application includes two key components: a re- trieval model and an LLM for text generation, called the gen- erative model. When a query is received, the retrieval model searches for relevant items (e.g., documents) and the top re- trieved items, together with the input, are sent to the genera- tive model. Current state-of-the-art retrieval approaches use bi-encoder neural networks (called dense retrieval) [30] for learning optimal embedding for queries and documents. Each item is then encoded into a high-dimension vector (called embedding vectors) and stored in a vector database. Such approaches use K-nearest neighbor algorithms for retrieving the top “K" items from the vector database. Figure 1 provides an overview of a RAG application. The accuracy of generated output in RAG hinges on the quality of the retrieved item list. Conducting an Exact Near- est Neighbors Search (ENNS) to retrieve the precise top K relevant items involves scanning all the embedding vectors in the vector database, which is costly in today’s memory bandwidth-limited systems. For example, in a RAG applica- tion with a 50GB (using a 16-bit floating point representation) vector database running on an