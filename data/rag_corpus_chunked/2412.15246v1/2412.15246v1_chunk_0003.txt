of the retrieved item list. Conducting an Exact Near- est Neighbors Search (ENNS) to retrieve the precise top K relevant items involves scanning all the embedding vectors in the vector database, which is costly in today’s memory bandwidth-limited systems. For example, in a RAG applica- tion with a 50GB (using a 16-bit floating point representation) vector database running on an Intel Xeon 4416+ with 8×DDR5- 4000 memory channels, and a generative model running on an NVIDIA H100 GPU, ENNS takes up to 97% of the end-to-end inference time (§3). One strategy to mitigate the retrieval cost is to employ Approximate Nearest Neighbor Search (ANNS), and opt for a faster, but lower-quality search configuration. While lower- quality retrieval can improve search time, our extensive ex- periments on Question Answering applications demonstrate that a lower-quality search scheme should provide signifi- cantly more items to the language model in order to match the end-to-end RAG accuracy of ENNS or a higher-quality, but slower ANNS configuration. This virtually negates any benefits gained during the retrieval phase and even increases the end-to-end inference time. In this paper, we extensively profile the execution pipeline of RAG, demystifying the complex interplay between var- ious hardware and software configurations in RAG appli- cations [106]. Motivated by the need for high-performance, low-cost, high-quality search and the limitations of current commodity systems, we contribute the Intelligent Knowledge Store (IKS), a cost-optimized, purpose-built CXL memory ex- pander that functions as a high-performance, high-capacity vector database accelerator. IKS offloads memory-intensive dot-product operations in ENNS to a distributed array of low- profile accelerators placed near LPDDR5X DRAM packages. IKS implements a novel interface atop the CXL.cache pro- tocol to seamlessly offload exact vector database search oper- ations to near-memory accelerators. IKS is exposed as a mem- ory expander that disaggregates its internal DRAM