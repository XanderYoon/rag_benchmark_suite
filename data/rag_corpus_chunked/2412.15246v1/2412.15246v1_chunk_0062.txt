Zhou. 2019. Response Generation by Context-Aware Prototype Edit- ing. Proceedings of the AAAI Conference on Artificial Intelligence 33, 01 (Jul. 2019), 7281–7288. https://doi.org/10.1609/aaai.v33i01.33017281 [100] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien De- mouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In Inter- national Conference on Machine Learning . PMLR, 38087–38099. https://doi.org/10.5555/3618408.3619993 [101] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024. Efficient Streaming Language Models with Attention Sinks. InThe Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=NG7sS51zVF [102] Jitao Xu, Josep Crego, and Jean Senellart. 2020. Boosting Neural Machine Translation with Similar Translations. InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 1580–1590. https://doi.org/10.18653/v1/2020.acl-main.144 [103] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective Retrieval Augmented Generation. arXiv (2024). https://doi.org/10.48550/arXiv.2401.15884arXiv:2401.15884 [cs.CL] [104] Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Inference with reference: Lossless acceleration of large language models. arXiv preprint arXiv:2304.04487 (2023). https://doi.org/10.48550/arXiv.2304.04487 [105] Yifan Yuan, Jinghan Huang, Yan Sun, Tianchen Wang, Jacob Nelson, Dan R. K. Ports, Yipeng Wang, Ren Wang, Charlie Tai, and Nam Sung Kim. 2023. Rambda: RDMA-driven Acceleration Framework for Memory-intensive µs-scale Datacenter Applications. In 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 499–515. https://doi.org/10.1109/HPCA56546.2023.10071127 [106] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Ghodsi. 2024. The Shift from Models to Compound AI Systems. Online; accessed 2024-12-13. https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/ [107] Hamed Zamani and Michael Bendersky. 2024. Stochastic RAG: End-to- End Retrieval-Augmented Generation through Expected Utility Maxi- mization. InProceedings of the 47th International ACM SIGIR Conference on Research and Development