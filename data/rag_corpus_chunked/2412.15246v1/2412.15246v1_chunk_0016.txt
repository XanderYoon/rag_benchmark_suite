Flops/Byte and 592 Int8 Ops/Byte Derrick Quinn et al. 83212851220488192 1/8 1/22832128512 Performance (GFlop/s)Operational Intensity (Flops/Byte) 16 Core RooflineBatch Size 1Batch Size 16Peak Compute: 2625GFlop/s Peak DRAM Bandwidth:187.3 GB/s Fig. 4. Roofline model for ENNS using Batch Size 1 and 16. See Section 6 for the experimental setup. 4 Case for Near-Memory ENNS Acceleration ENNS is characterized by the following features: • ENNS operations exhibit no data reuse for pair-wise similarity score calculations between corpus vectors and a query vector. • ENNS operations consist of simple vector-vector dot- products coupled with top-K logic. • ENNS has a regular and predictable memory access pattern. • ENNS is highly parallelizable, allowing the corpus to be distributed across different processors with a simple aggregation of top-K similarities at the end. These features make ENNS a prime candidate for near- memory acceleration due to the following reasons: (1) Deep cache hierarchies are not beneficial for ENNS and can even cause slowdown due to the complex cache maintenance and coherency operations managed by the hardware. This is evi- dent from the roofline model in Figure 4 as ENNS running on the CPU cannot saturate the available DRAM bandwidth. (2) The limited data reuse with huge data set size enables low over- head software-managed cache coherency implementation between the host CPU and near-memory accelerators. (3) The regular memory access pattern of ENNS enables coarse-grain virtual to physical address translation on near-memory accel- erators. (4) ENNS operations can be efficiently offloaded to a distributed array of near-memory accelerators that each oper- ate in parallel on a shard of corpus data with a low-overhead top-K aggregation phase at the end. Leveraging these unique features, we design, implement, and evaluate Intelligent Knowledge Store (IKS), a memory expander with a scale-out near-memory acceleration architec- ture, uniquely designed to accelerate vector