35; 62; 111] near- memory processing architecture is that DIMM-based near- memory processing (1) requires sophisticated mechanisms to share the address space between near-memory accelerators and the host [61], (2) limits per-rank memory capacity and compromises the memory capacity of the host CPU when used as an accelerator, and (3) has limited compute and ther- mal capacity. Instead, IKS relies on asynchronous CXL.mem and CXL.cache protocols to safely share the address space and independently scale the local and far memory capacity of the host CPU, implement a low-overhead interface for of- floading from the userspace, and eliminate the limitations on the compute or thermal capacity of the PCIe-attached IKS card. In Section 5, we explain the architecture of IKS and its interface to the host CPU, and how we used it to accelerate end-to-end RAG applications. 5 Intelligent Knowledge Store 5.1 Overview Figure 5 provides an overview of the Intelligent Knowledge Store (IKS) architecture. IKS incorporates a scale-out near- memory processing architecture with low-profile accelera- tors positioned near the memory controllers of LPDDR5X packages. While IKS can function as a regular memory ex- pander, it is specifically designed to accelerate ENNS over the embedding vectors stored in its LPDDR5X packages. As shown in Figure 5b, IKS utilizes eight LPDDR5X pack- ages, each directly connected to a Near-Memory Accelerator (NMA) that implements both LPDDR5X memory controllers and accelerator logic. Each package contains 512Gb LPDDR5X DRAM with eight 16-bit channels, similar to CXL-PNM [57] and MTIA [16]. One of the key differences between IKS and these architectures is the scale-out near-memory accelera- tion architecture. IKS distributes the NMA logic over multiple chips, each providing high-bandwidth and low-energy access to its local LPDDR5X package. Why Scale-Out NMA Architecture?The rationale for such a scale-out NMA architecture is to keep the area of the NMA