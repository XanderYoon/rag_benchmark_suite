significantly speed up ENNS, as the corpus size increases, the cost of offloading ENNS to GPUs in- creases significantly. For example, to fit the 50 GB and 512 GB corpus sizes tested in Table 1, we need to use 1 and 8 H100 GPUs, respectively. One of the key contributors to the cost of GPUs is the high-bandwidth memory (HBM) used to im- plement GPU main memory, which is several times more expensive than DDR or LPDDR-based memories [54]. Lastly, GPUs provision huge amounts of compute relative to memory bandwidth2, meaning that a large GPU die is poorly utilized by the primarily memory-bound workload of ENNS [24]. 3.5 Summary The analysis presented in this section, using various software and system configurations for RAG applications, led to the following takeaways: • Generation accuracy, time to interactive, and through- put of RAG applications can be improved by using a slower but higher-quality retrieval scheme. • When high-quality retrieval is used, the retrieval phase accounts for a significant portion of end-to-end run- time, regardless of whether the search is performed via ENNS or high-quality ANNS. • Using GPUs to accelerate ENNS is expensive, and GPUs are not able to accelerate high-quality ANNS effectively or affordably. • New accelerators for ANNS are highly complex and task-specific due to the unique requirements of ANNS algorithms, while ENNS relies on a very simple scheme, making ENNS simpler to accelerate than ANNS. 2NVIDIA H100 80GB provisions 296 Flops/Byte and 592 Int8 Ops/Byte Derrick Quinn et al. 83212851220488192 1/8 1/22832128512 Performance (GFlop/s)Operational Intensity (Flops/Byte) 16 Core RooflineBatch Size 1Batch Size 16Peak Compute: 2625GFlop/s Peak DRAM Bandwidth:187.3 GB/s Fig. 4. Roofline model for ENNS using Batch Size 1 and 16. See Section 6 for the experimental setup. 4 Case for Near-Memory ENNS Acceleration ENNS is characterized by the following