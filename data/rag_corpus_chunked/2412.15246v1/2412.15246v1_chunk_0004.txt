high-performance, high-capacity vector database accelerator. IKS offloads memory-intensive dot-product operations in ENNS to a distributed array of low- profile accelerators placed near LPDDR5X DRAM packages. IKS implements a novel interface atop the CXL.cache pro- tocol to seamlessly offload exact vector database search oper- ations to near-memory accelerators. IKS is exposed as a mem- ory expander that disaggregates its internal DRAM capacity and shares it with vector database applications and other co-running applications through CXL.mem and CXL.cache protocols. Instead of building a full-fledged vector database accelerator, IKS co-designs the hardware and software to implement a minimalist scale-out near-memory accelerator architecture. This design relies on software to map data into the internal IKS DRAM and scratchpads while performing the final top-K aggregation. In summary, we make the following contributions: • We demystify RAG by profiling its execution pipeline. We explore various hardware, system, and application- level configurations to assess the performance and ac- curacy of RAG. • We demonstrate that RAG requires high-quality re- trieval to perform effectively; nonetheless, current RAG applications are bottlenecked by a high-quality re- trieval phase. • We introduce Intelligent Knowledge Store (IKS), which is a specialized CXL-based memory expander equipped with low-profile accelerators for vector database search. IKS leverages CXL.cache to implement a seamless and efficient interface between the CPU and near-memory accelerators. • We implemented an end-to-end accelerated RAG ap- plication using IKS. IKS accelerates ENNS for a 512GB knowledge store by 13.4–27.9×, leading to a 1.7–26.3× end-to-end inference speedup for representative RAG applications. 2 Background 2.1 Information Retrieval in RAG Recent advancements in RAG indicate superior outcomes when employing dense retrieval over other methods, for uni- modal [25; 26; 41] and multi-modal [ 19; 71; 74] scenarios. Consequently, our emphasis in this study centers on dense retrieval models, exploring their efficiency-related aspects. In the context