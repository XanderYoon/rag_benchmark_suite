shared AVX 2x AVX-512 FMA units (164 GFlop/s/core) OS Ubuntu 22.04.3 Kernel Linux 5.15.0-88-generic Memory 512 GB DDR5-4000 across 8 channels (256 GB/s) AMX – Intel AMX (BFloat16, 500 GFlop/s/core) IKS (emulated) – 1.1 TB/s, 69.9 TFlop/s GPU GPU Model NVIDIA H100 SXM: 3.35 TB/s, 1979 TFlop/s Table 2. Processing Element Options. Memory configuration for Intel AMX is the same as for CPU. may be read and processed dimension-by-dimension. Conse- quently, each NMA will access up to 136 bytes per cycle from the memory controller read queue, comprising one element from 68 distinct embedding vectors. As discussed in Section 5.3, the host CPU will fill the query scratchpads with query vectors before an offload starts. The query vectors are stored in sequential addresses within the query scratchpads, as illustrated in Figure 8. This data layout inside DRAM and query scratchpads sim- plifies the address generation as well as the network-on-chip architecture of the NMAs. We modified the memory allocation scheme in the vector database application to implement the block data mapping of embedding vectors inside IKS DRAM as shown in Figure 7. Derrick Quinn et al. 6 Experimental Methodology 6.1 Experimental Setup To evaluate the performance of the IKS, we developed a simu- lator (see appendix A) and fed ENNS traces into it to obtain the retrieval time of IKS. The simulator is a cycle-approximate per- formance model that utilizes timing parameters from the RTL synthesis, LPDDR5X access timing, PCIe/CXL timing [43; 81], along with calculations of real software stack overhead (top-K aggregation and umwait() overhead). It emulates an IKS as a CXL device running on a remote CPU socket. We imple- mented the end-to-end RAG application described in Section 3 (i.e., FiDT5, Llama-8B, and Llama-70B), including the APIs for distributing queries to the NMA query scratchpad and reducing partial