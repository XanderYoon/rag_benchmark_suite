95]. While lower-quality ANNS algorithms could possibly provide or- ders of magnitude faster nearest neighbor search compared with ENNS, high-quality ANNS algorithms are shown to pro- vide only a modest speedup [45; 94]. For example, ANNS-2, which is the best performing ANNS configuration in Figure 2, offers only a 2.5Ã— speedup compared with ENNS. In fact, all the Pareto frontier configurations that provide high genera- tion accuracy in Figure 2 are ENNS. Therefore, in the rest of this section, we focus on understanding how to optimize and accelerate RAG applications with ENNS. 3.3 End-to-End RAG Performance with ENNS In this subsection, we profile time-to-interactive (also known as time to first token) [ 87] for the FiDT5, Llama-8B, and Llama-70B RAG applications and report latency ratios for the retrieval and generation phases. For all experiments, re- trieval uses ENNS and runs on the CPU, while generation runs on a single NVIDIA H100 GPU. We select CPU as the baseline for ENNS retrieval, rather than GPU. This decision is made based on the high cost of using GPU memory As we discussed in Section 3.2, the generation accuracy of RAG applications directly depends on the retrieval accuracy. However, as shown in Figure 3a, utilizing ENNS for retrieval can quickly become an end-to-end bottleneck in RAG applica- tions, even for large models. Although it is possible to compen- sate for the retrieval accuracy by increasing K (in case of using Accelerating Retrieval-Augmented Generation 0%25%50%75%100% 50 GB200 GB512 GB1024 GB2048 GB50 GB200 GB512 GB1024 GB2048 GB50 GB200 GB512 GB1024 GB2048 GBFiDT5Llama-3-8BLlama-3-70B Retrieval (ENNS on CPU)Generation (H100) 0.62s1.24s3.11s6.23s12.46s0.62s1.24s3.11s6.23s16.84s0.62s1.24s3.11s6.23s16.86s (a) Sensitivity to corpus size. All configurations use K=16. 0%25%50%75%100% K = 1K = 4K = 16K = 32K = 1K = 4K = 16K = 32K = 1K = 4K = 16K = 32FiDT5Llama-3-8BLlama-3-70B Retrieval (ENNS