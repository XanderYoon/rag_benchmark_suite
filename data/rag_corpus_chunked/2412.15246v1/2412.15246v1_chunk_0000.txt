Accepted for publication at ASPLOS 2025 Accelerating Retrieval-Augmented Generation Derrick Quinn Cornell University Ithaca, NY, USA dq55@cornell.edu Mohammad Nouri Cornell University Ithaca, NY, USA mn636@cornell.edu Neel Patel Cornell University Ithaca, NY, USA nmp83@cornell.edu John Salihu University of Kansas Lawrence, KS, USA jsalihu@ku.edu Alireza Salemi University of Massachusetts Amherst Amherst, MA, USA asalemi@cs.umass.edu Sukhan Lee Samsung Electronics Hwasung, Republic of Korea sh1026.lee@samsung.com Hamed Zamani University of Massachusetts Amherst Amherst, MA, USA zamani@cs.umass.edu Mohammad Alian Cornell University Ithaca, NY, USA malian@cornell.edu Abstract An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce infer- ence time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG. In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent inter- face between the host CPU and near-memory accelerators. IKS offers 13.4–27.9× faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search per- formance translates to 1.7–26.3× lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to pre- vent DRAM – which is the most expensive component in today’s servers – from being