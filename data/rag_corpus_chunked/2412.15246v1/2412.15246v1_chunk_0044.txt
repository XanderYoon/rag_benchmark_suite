Intel Sapphire Rapids accelerators, leading to 2.0-49× lower end- to-end RAG inference time. Acknowledgments This work was supported in part by NSF grant numbers 2239020, 1565570, and 2402873, in part by ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corpo- ration (SRC) program sponsored by DARPA, in part by the Office of Naval Research contract number N000142412612, and in part by the Center for Intelligent Information Retrieval. Any opinions, findings, conclusions, and recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsors. We thank Jae-sun Seo and Yuan Liao from Cornell University for their help in synthesizing the near-memory accelerators on 16nm TSMC technology. A Artifact Appendix A.1 Abstract This appendix describes two artifacts: 1–The cycle-approximate simulator for IKS, which models IKS using timing data gath- ered from RTL synthesis. 2–FAISS modified for fast ENNS on Intel CPUs. All artifacts are available via Github. A.2 Artifact check-list • Simulator: https://github.com/architecture-research- group/iks_simulator • Optimized Faiss:https://github.com/architecture-research- group/ae-asplo25-iks-faiss/tree/main • Compilation: Please refer to each program’s repos- itory. • OS requirement: Modern Linux kernel • Hardware requirement: Intel 4th Gen Xeon Scalable Processors or newer, with AMX equipped and enabled. • Software requirement: Intel MKL Installed • Publicly available?: Yes. References [1] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwa- tra, Bhargav S. Gulavani, and Ramachandran Ramjee. 2023. SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills. Microsoft Research Blog (September 2023). https://doi.org/10.48550/arXiv.2308.16369 [2] Yeonchan Ahn, Sang-Goo Lee, Junho Shim, and Jaehui Park. 2022. Retrieval-Augmented Response Generation for Knowledge-Grounded Conversation in the Wild. IEEE Access 10 (2022), 131374–131385. https://doi.org/10.1109/ACCESS.2022.3228964 [3] Meta AI. 2024. Llama 3. Online; accessed 2024-12-13. https://llama.meta.com/llama3/ [4] Mohammad Alian, Seung Won Min, Hadi Asgharimoghaddam, Ashutosh Dhar, Dong Kai Wang, Thomas Roewer, Adam McPad- den, Oliver O’Halloran, Deming