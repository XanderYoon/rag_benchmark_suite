Once the NMA computation is complete, the NMA updates the context buffers with the partial list of similarity scores and physical addresses of the corresponding embedding vectors. Lastly, the NMA writes into the doorbell register, and the host gets notified of the completion of the offload through theumwait() mechanism (step 11). Our experimental results on a two-socket Sapphire Rapids CPU show that communicating the offload context through cache-coherent shared memory provides 1.6Ã— higher through- put compared with using non-temporal writes that mimic the PCIe MMIO datapath (i.e., CXL.io). Using a cache-coherent in- terconnect to implement the notification mechanism through the producer/consumer-style doorbell register eliminates the need for expensive interrupt or polling mechanisms. 5.4 NMA Architecture As shown in Figure 5c, each NMA implements 64 process- ing engines to accommodate similarity score calculations for up to 64 query vectors in parallel. Each processing engine includes a query scratchpad, dot-product unit, Top-K unit, and output scratchpad. There is a central control unit in each NMA that generates memory accesses, controls data move- ment within the NMA, and activates processing engines based on the number of query vectors provided by the host CPU. The network-on-chip implements a fixed broadcast network from DRAM to all the processing engines to reuse data when multiple processing engines are active and evaluate similarity scores against different query vectors. As shown in Figure 5d, the dot-product unit includes 68 MAC units, each operating at a 1 GHz frequency and pro- viding 68 GFLOPS (16-bit floating point multiply-accumulate operations) compute throughput; therefore saturating the 136 Accelerating Retrieval-Augmented Generation GBps memory bandwidth of the LPDDR5X channels. Each MAC unit evaluates the similarity score between the query (stored in the query scratchpad) and an embedding vector that is read from DRAM inVD (Vector Dimension) cycles. All the processing engines operate on