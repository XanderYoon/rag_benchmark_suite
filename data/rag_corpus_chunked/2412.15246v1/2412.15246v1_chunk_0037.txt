do not evaluate deployments of IKS spanning multiple nodes. Table 3 reports the absolute time breakdown of ENNS re- trieval on IKS. We break down the retrieval time of IKS into four components: transfer time of query vectors over the CXL interconnect to the NMAs, time for performing dot-products (both computation and DRAM accesses), updating the top-k score lists in parallel on all NMAs, and time for reducing the partial top-32 lists into a single one on the CPU. The retrieval time of IKS does not change with the value of K (with a maxi- mum K value of 32). This is because IKS always returns 32 top similarity scores, and it is up to the retriever model to pass between 1 to 32 of them to the generative model. As shown in the table, the majority of time is spent on computations and DRAM accesses, and the overhead of initiating offload over the cache-coherent interconnect and aggregating top-K documents on the CPU is negligible. 7.2 End-to-End Performance Figure 10 compares the end-to-end inference time ofFiDT5, Llama-8B, and Llama-70B when CPU and IKS are used for ENNS retrieval for various batch sizes, document counts, and corpus sizes. As shown, for large corpus sizes or large batch sizes, the inference time of the RAG applications with CPU retrieval exceeds several seconds, which is not accept- able for user-facing question-answering applications. IKS significantly reduces the ENNS retrieval time for the appli- cations. The end-to-end inference time speedup provided by IKS ranges between 5.6 and 25.6× for FiDT5, between 5.0 and 24.6× for Llama-8B, and between 1.7 and 16.8× for Llama-70B for various batch sizes, corpus sizes, and document counts. To gain a comprehensive understanding of how the perfor- mance and accuracy of RAG applications with IKS accelera- tion compare across various configurations,