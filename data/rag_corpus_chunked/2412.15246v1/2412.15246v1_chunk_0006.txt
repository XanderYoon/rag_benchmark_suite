pairwise distance matrix be- tween embedding and query vectors. In ANNS, however, strategies such as Product Quantization (PQ) [28], Inverted File with Product Quantization (IVFPQ) [7], and Hierarchical Navigable Small World (HNSW) [52], are employed to reduce the search space, seeking to trade off a small amount of search accuracy for higher search efficiency. 2.2 Applications of RAG RAG has proven beneficial for various tasks in natural lan- guage processing [36; 44; 108], including dialogue response generation [2; 8; 10; 83; 91; 92; 96; 97; 99], machine transla- tion [18; 21; 102; 109], grounded question answering [25; 26; 41; 66; 67; 76–78; 85; 107], abstractive summarization [58; 64], code generation [20; 49], paraphrase generation [32; 89], and personalization [37; 72; 73; 75]. Additionally, RAG’s applica- tion extends to multi-modal data tasks like caption generation from images, image generation, and visual question answer- ing [11; 12; 15; 19; 70; 71; 80]. It is noteworthy that commercial LLM systems employing RAG are typically proprietary, and as such, their implementa- tions are not openly accessible. Nevertheless, insights into the implementation of these systems can be gleaned from open- source releases by research labs within commercial entities. We adhere to a methodology akin to the approach outlined by Izacard and Grave[26] and Lewis et al. [41], both of which are contributions from Meta AI. Our implementations closely align with the depicted pipeline in Figure 1. Specifically, we employ a dense document retrieval model as the retriever and leverage a language model for answer generation, consistent with the aforementioned work. Additionally, for efficient vec- tor search capabilities, we utilize the Faiss [27] library, similar to the aforementioned works. 3 Demystifying RAG In this section, we profile the end-to-end execution of three representative long-form question-answering RAG applica- tions and quantify both the execution time breakdown and