the retriever and leverage a language model for answer generation, consistent with the aforementioned work. Additionally, for efficient vec- tor search capabilities, we utilize the Faiss [27] library, similar to the aforementioned works. 3 Demystifying RAG In this section, we profile the end-to-end execution of three representative long-form question-answering RAG applica- tions and quantify both the execution time breakdown and the generation accuracy of RAG with different hardware and software configurations: FiDT5, where we use the T5-based Fusion-in-Decoder [26; 68] as the generative model, as well as Llama-8B, and Llama-70B, where we use 4-bit-Quantized Llama-3-8B-Instruct and Llama-3-70B-Instruct [3] as the gen- erative models, respectively. The knowledge source for all workloads is Wikipedia, and a trained BERT base (uncased) model is used to generate embedding vectors for documents. We assume 16-bit number format and test with various vector database sizes (corpus size) that store the embedding vectors. The documents themselves are stored in the CPU memory. In FiDT5, the documents are presented via the Fusion-in- Decoder approach, where documents are encoded by the encoder stage of a T5 model, and these encoded representa- tions are combined for the decoder stage. InLlama-8B and Llama-70B, retrieved documents are presented as plaintext in the prompt. For more information about the methodology, see Section 6. In the following subsections, we discuss both the accuracy of an end-to-end RAG system and the retrieval model on its own. Retrieval accuracy is discussed in terms of recall, where ENNS is considered to be perfect, and the recall score of an ANNS algorithm is the proportion of relevant documents retrieved by both ENNS and ANNS algorithm compared to the total number of relevant documents retrieved by ENNS. Generation accuracy refers to how well an end-to-end RAG system answers questions. For details on the evaluation of generation accuracy, see Section