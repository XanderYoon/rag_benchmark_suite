don’t know” when facing questions beyond their knowledge scope [10, 8]. 2 2.2 Technological Pathways to Address RAG Challenges To tackle these challenges, recent studies have proposed various technical approaches. Preference modeling and gain assessment have emerged as promising strategies for improving paragraph fil- tering accuracy, exemplified by GainRAG, which quantifies the contribution of each paragraph to the correct answer [11]. To address knowledge conflict and factual consistency issues, FaithfulRAG and RPO introduce self-knowledge verification mechanisms and reinforcement learning strategies, respectively, to enhance the reliability of generated results [1, 12]. In complex tasks like multi- hop QA, reasoning chain optimization and structured organization have become mainstream di- rections. RankCoT, DualRAG, and KiRAG improve reasoning capabilities through CoT ranking, dual-channel architecture, and iterative knowledge triplet utilization [13, 14, 6]. Meanwhile, docu- ment filtering and noise suppression mechanisms are widely explored, with MAIN-RAG introducing a multi-agent scoring filtering mechanism and Lexical Diversity-aware methods using contrastive learning to remove irrelevant content [3, 4]. Furthermore, dynamic retrieval and trigger control mechanisms are evolving, with DioR proposing early and real-time classifiers to enable on-demand retrieval and significantly improve system responsiveness [7]. 2.3 Reasoning Guidance and Knowledge Distillation An increasing number of studies are leveraging reasoning guidance and knowledge distillation to improve the intelligence of RAG systems. For example, Rationale Distillation utilizes LLMs to generate “rationales” as signals for re-ranking documents and fine-tuning the reranker to better align with the generator’s preferences [15, 16]. Memory-inspired iterative frameworks use multi-agent collaboration to integrate historical retrieval results, dynamically adjust queries, and filter noise, thus improving retrieval efficiency and quality [17]. Additionally, self-refinement mechanisms play a role in formalization tasks, with LTRAG building a thought-guided knowledge base to assist the formalization process and integrating symbolic solvers for dynamic optimization [18]. 2.4 Graph Enhancement and Structured Knowledge Modeling Structured knowledge