FilterVisualContent GenerateSections ClusterBySections Phase 4: Section Writing CreateDraft GetSectionContent RefineDraft ComputeRelevance CollectMultimodalContent OptimizePlacement FindSource GenerateCitation AddCitation AddImagePaths ValidateCitations FormatDocument Multimodal Content Factual Grounding Fine-grained Citations Figure 3: The Framework of DeepWriter The report generation process is divided into several stages. First, DeepWriter retrieves contex- tual information through DocFinder by leveraging vector search, ensuring that the retrieved content is relevant and semantically aligned with the query. It then integrates this content, combining the query-driven information with the LLM to generate the report. Visual elements like charts, tables, and images are placed within the report based on their relevance to the text, optimizing the inte- gration of multimodal data. Finally, the ReportProcessor ensures that the generated document is formatted in Markdown and includes all necessary citations. The report is thoroughly fact-checked, with each claim or visual element accurately cited, providing transparency and verifiability. This robust framework enhances the quality and reliability of the generated reports, supporting complex tasks across diverse domains. 4 Experiment The experimental evaluation of the HetaRAG system is divided into three main parts, each targeting a different aspect of the systemâ€™s capabilities. Section 3.1, RAG-Challenge Evaluation, focuses on assessing the retrieval results. Section 3.2 evaluates the effectiveness of multi-hop reasoning when handling complex questions that require reasoning across multiple information sources. Section 3.3 is dedicated to evaluating the quality of multimodal report generation, examining how well the system generates structured reports that integrate both textual and visual content. Each section provides valuable insights into the strengths and potential improvements of the system. 8 4.1 RAG-Challenge 4.1.1 Experimental Setup Dataset.To assess the retrieval and reasoning capabilities of HetaRAG, we adopt the dataset and evaluation protocol from the Enterprise RAG Challenge Round 2[49]. The benchmark comprises domain-specific queries paired with gold-standard answers annotated by experts, along with relevant document contexts.Evaluation follows