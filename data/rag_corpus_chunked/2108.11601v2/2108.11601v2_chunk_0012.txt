repositories on GitHub. By default, we exclude the target code/summary from the retrieval database. Implementations As mentioned in Section 3, REDCODER has two disjoint components. First, the dense retriever SCODE-R is implemented adopting DPR (Karpukhin et al., 2020) and the encoders in DPR are initialized from GrpahCode- BERT available in the Huggingface API (Wolf et al., 2020). In addition, we implement a baseline BM25 retriever. We use the ofﬁcial codebase of PLBART (Ahmad et al., 2021) and set max epoch to 15, patience to 5, learning rate to 2× 10−5. We tune the batch size in {8, 16, 32, 64, 72} and the k value for top-k retrieval up to 10 for code gen- eration and in range {10, 30, 50, 100} for code summarization. As some candidate code and sum- maries are short in length, we tune with this upper bound of k to accommodate as many candidates as possible within PLBART’s maximum input length. 4.2 Evaluation Metrics BLEU Following prior works (Ahmad et al., 2021; Feng et al., 2020a), we compute the cor- pus level BLEU (Papineni et al., 2002) and the smoothed BLEU-4 (Lin and Och, 2004) scores for code generation and summarization tasks. CodeBLEU To demonstrate syntactic and seman- tic data ﬂow correctness of code generation models, we report CodeBLEU (Ren et al., 2020). Code- BLEU is a weighted average of lexical, abstract syntax tree, and data ﬂow match. Exact Match (EM) indicates the percentage of output sequences that exactly match the references. 4.3 Baseline Methods We compare REDCODER w.r.t.a number of state- of-the-art code models. We classify them into two categories: (i) retrieval based models and (ii) gen- erative models. We study both generative models that are trained from scratch and are pre-trained on programming and natural languages. Method Java Python Type Name EM BLEU