output sequences that exactly match the references. 4.3 Baseline Methods We compare REDCODER w.r.t.a number of state- of-the-art code models. We classify them into two categories: (i) retrieval based models and (ii) gen- erative models. We study both generative models that are trained from scratch and are pre-trained on programming and natural languages. Method Java Python Type Name EM BLEU CodeBLEU EM BLEU CodeBLEU Retrieval BM25 0.00 4.90 16.00 0.00 6.63 13.49 Based SCODE-R 0.00 25.34 26.68 0.00 22.75 23.92 Generative CodeBERT 0.00 8.38 14.52 0.00 4.06 10.42 GraphCodeBERT 0.00 7.86 14.53 0.00 3.97 10.55 CodeGPT-adapted 0.00 7.10 14.90 0.01 3.11 11.31 PLBART 0.00 10.10 14.96 0.00 4.89 12.01 Retrieval BM25 + PLBART 0.10 11.37 15.52 0.03 6.99 13.89 Augmented REDCODER 8.95 26.92 31.15 8.88 22.74 28.93 Generative REDCODER-EXT 10.21 28.98 33.18 9.61 24.43 30.21 Table 2: Results on code generation on CodeXGLUE (Lu et al., 2021). Methods EM BLEU CodeBLEU Retrieval based methods BM25 0.0 20.3 23.7 SCODE-R 0.0 32.6 36.5 Generative methods Seq2Seq 3.1 21.3 26.4 Guo et al. (2019) 10.1 24.4 29.5 Iyer et al. (2019) 12.2 26.6 - GPT-2 17.4 25.4 29.7 CodeGPT-2 18.3 28.7 32.7 CodeGPT-adapted 20.1 32.8 36.0 CodeBERT 18.0 28.7 31.4 GraphCodeBERT 18.7 33.4 35.9 PLBART 18.6 36.7 38.5 Retrieval augmented generative methods BM25+PLBART 21.4 40.2 41.8 REDCODER 23.4 41.6 43.4 REDCODER-EXT 23.3 42.5 43.4 Table 3: Code generation results on Concode dataset. SCODE-R was initialized with CodeBERT. Graph- CodeBERT initialized results are similar. Retrieval based models We examine two re- triever baselines and consider the top-1 retrieved candidate as the prediction. • Dense Retriever We consider DPR as the dense retriever baseline. We evaluate both the ofﬁcially released models trained on the natural language open-domain QA task and a variant called DPR (code) that we ﬁne-tune on the evaluation datasets. • Sparse