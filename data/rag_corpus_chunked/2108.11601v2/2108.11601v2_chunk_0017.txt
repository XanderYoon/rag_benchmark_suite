and Concode, the gains in BLEU are 18.88, 19.54, and 5.8. Com- paring REDCODER to REDCODER-EXT shows that BLEU scores on Concode and all metrics on CodeXGLUE are improved by∼1%. These results conﬁrm our conjecture that complementing input with paired summaries of the retrieved code help code generation. We provide a qualitative exam- ple in the Appendix to explain how the retrieved information helps PLBART in generation. 5.2 Code Summarization We compare REDCODER with three sets of base- line methods for code summarization, and Table 4 shows the results. Among the two retrieval base methods, SCODE-R performs signiﬁcantly well, conﬁrming the advantages of dense retrieval over its sparse counterpart. Out of the generative meth- ods, PLBART excels on code summarization as it leverages an extensive collection of natural lan- guage descriptions during pre-training. As antici- pated, retrieval augmented generative methods out- perform the other two sets of models. We see that the “BM25 + PLBART” model improves over PLBART, conﬁrming our conjecture that retrieval augmented techniques have the promise to improve code summarization. Our proposed framework REDCODER and its variant REDCODER-EXT outshine “BM25 + PLBART”, surpassing its per- formance by∼1.5 and∼3.2 points for Python and Java languages, respectively. 6 Analysis In this Section, we analyze REDCODER’s perfor- mance on the following points. Figure 6: Recall@K for CodeR and BM25. CodeR refers to SCODE-R used for source code retrieval. Retrieval database includes the target sequence Table 5 shows the code generation results when we did not ﬁlter the target from the retrieval (summa- rization results are in Appendix). As expected, SCODE-R performances are much better than those in Table 2, 3, and 4. In all cases, RED- CODER gets more enhanced when target is present in the retrieval database. For the code generation task, we plot the recall@k curve for k