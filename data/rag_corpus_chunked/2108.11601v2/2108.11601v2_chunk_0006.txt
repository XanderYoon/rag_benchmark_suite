retrieval methods en- code documents into a ﬁxed-size representations and retrieve documents via maximum inner prod- uct search (Sutskever et al., 2014; Guo et al., 2016). Particularly of interests, Karpukhin et al. (2020) propose a Dense Passage Retriever (DPR) model for open-domain question answering (QA). It con- sists of two encoders (Q(.) and P(.)) that encode queries and passages, respectively. The similarity of a query q and a passage p is deﬁned by the in- ner product of their encoded vectors sim(p, q)= Q(q)T ⋅ P(p). Given a query q, a positive (rele- vant) passage p+, and a set of n irrelevant passages p− i , DPR optimizes the classiﬁcation loss: L=− log esim(q,p+) esim(q,p+)+ ∑n i=1 esim(q,p− i ). Karpukhin et al. (2020) propose to ﬁne-tune DPR using in-batch negatives (Gillick et al., 2019; Yih et al., 2011) with curated “hard” negatives us- Figure 3: An example retrieved code that is relevant yet does not match the reference. ing BM25 (candidates with high BM25 scores but contain no sub-string that match the target). We refer to Karpukhin et al. (2020) for details. 2.3 Generator: PLBART PLBART (Ahmad et al., 2021) is a sequence-to- sequence Transformer model (Vaswani et al., 2017) that is pre-trained on a huge collection of source code and natural language descriptions via denois- ing autoencoding. PLBART has shown promise in several software engineering applications, includ- ing code generation and summarization. We adopt PLBART as the generator module in our proposed framework, REDCODER. 3 Proposed Framework: REDCODER Our proposed code generation and summarization framework, REDCODER generates the target code or summary by augmenting the input x with rele- vant code snippets or summaries. We build our re- triever module by training a DPR model differently from (Karpukhin et al., 2020). With an intelligent scheme, we then augment