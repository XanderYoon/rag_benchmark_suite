summary gener- ation. Various designs of retriever and generator models can be incorporated into this framework. Existing cross-encoder code retrievers being computationally expensive, their applicability to retrieve from a large database is limited (Humeau et al., 2020). A natural choice would be to use sparse term based retrievers such as TF-IDF or BM25 (Robertson and Zaragoza, 2009). However, the retriever module in REDCODER should ex- hibit a good understanding of source code and pro- grammers’ natural language, which is a non-trivial task due to the syntactic and semantic structure of the source code (Guo et al., 2021; Ahmad et al., 2021). Such an expectation of searching for se- mantically similar code and summary may not be attainable by a sparse token level code retriever (e.g., BM25). To that end, we design the retriever module in REDCODER based on programming languages (PL) and natural languages (NL) under- standing models (e.g., GraphCodeBERT (Guo et al., 2021)). This retriever module extends the state-of- the-art dense retrieval technique (Karpukhin et al., 2020) using two different encoders for encoding the query and document. As for the generator, REDCODER can handle retrieval databases consisting of both unimodal (only code or natural language description) and bi- modal instances (code-description pairs) and makes the best usage of all the auxiliary information that 1The database could be open source repositories (e.g., GitHub) or developers’ forums (e.g., Stack Overﬂow). Figure 2: Example input/output for the code generation and summarization tasks. are available. Yet, to incorporate information, we augment the retrieved information only in the in- put level. It does not modify the underlying archi- tecture of the generator module —preserving its model agnostic characteristics. We evaluate the effectiveness of REDCODER on two popular programming languages (Java and Python) on both code generation and code sum- marization tasks. The empirical results