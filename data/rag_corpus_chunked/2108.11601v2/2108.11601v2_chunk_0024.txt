the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020a. CodeBERT: A pre-trained model for programming and natural languages. In Findings of the Associa- tion for Computational Linguistics: EMNLP 2020 , pages 1536–1547, Online. Association for Compu- tational Linguistics. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020b. CodeBERT: A pre-trained model for programming and natural languages. In Findings of the Associa- tion for Computational Linguistics: EMNLP 2020 , pages 1536–1547, Online. Association for Compu- tational Linguistics. Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, and Diego Garcia-Olano. 2019. Learning dense repre- sentations for entity retrieval. In Proceedings of the 23rd Conference on Computational Natural Lan- guage Learning (CoNLL) , pages 528–537, Hong Kong, China. Association for Computational Lin- guistics. Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep api learning. In Proceed- ings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, pages 631–642. Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Jian Yin, Daxin Jiang, et al. 2021. Graphcodebert: Pre- training code representations with data ﬂow. In International Conference on Learning Representa- tions. Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, and Jian Yin. 2019. Coupling retrieval and meta- learning for context-dependent semantic parsing. In Proceedings of the 57th Annual Meeting of the As- sociation for Computational Linguistics, pages 855– 866, Florence, Italy. Association for Computational Linguistics. Ruiqi