func- tion name MuxerStream mentioned in the in- put summary does not match the function name DeMuxerStream of the rank-3 retrieved code, it only adapts one line containing cPtr from rank-3 retrieved code (line #3) and takes the rests includ- ing the function deﬁnition (i.e., line #1) from the rank-1 retrieved code. Now when REDCODER- EXT is allowed to leverage the summaries of the retrieved code, it can match the summary of the rank-3 retrieved code with the input, and that is why it produces the MuxerStream class object but with the throw exceptions from the rank-3 re- trieved code. B Performance Difference of PLBART on CodeXGLUE and Concode Concode is a relatively easier dataset for code gen- eration and retrieval due to several pre-processing steps taken by its authors. Along with additional contexts (environment variables and methods) in the input summary, Concode artifacts the target code by replacing the speciﬁc variable names with generic tokens. 1 void function(Element arg0, 2 Formula arg1) { 3 arg0.addElement( 4 "concode_string").setText( 5 arg1.getText()); 6 } Therefore, we suspect that due to this, PLBART achieves good EM score for Concode but not for the generation of real code in CodeXGLUE. Analogously for the retrieval models, code re- trieved by BM25 have also a large word overlap- ping with the targets in Concode in contrast to CodeXGLUE (1st row in Table 2 and 3). Con- sequently, BM25 retrieval boosts PLBART (i.e., BM25 + PLBART) more in Concode than that in CodeXGLUE (3rd row for the bottom in Table 2 and 3). Overall, we anticipate all these skewness in model performances are due to the dataset char- acteristics. Dataset Lang. Task Retrieval Database |Size| |Nonparallel|CSNet CCSD Concode CodeXGLUE Python Gen.    1.2M 504K Sum.    1.1M 833K Java Gen.   