tively. First, we compare REDCODER with the state-of-the-art code generation models. They are transformers models pre-trained with differ- ent objectives using external resources of differ- ent sizes. Among them, the relatively strong base- line PLBART has an EM score of 18 on the Con- code dataset while it rarely generates any code that matches the real target code in CodeXGLUE (See Table 2) (more discussion on this is in Appendix). The BLEU and CodeBLEU scores are also low. Such result indicates that automated code lacks quality and correctness without the proper supervi- sion in the input to the generator. Among the retriever-only models, SCODE-R signiﬁcantly outperforms BM25 (more comparison is in § 6). As expected, the EM is zero as targets are ﬁltered from the retrieval, and CodeBLEU scores are high as they are real code. However, although the retrieved code does not exactly match the target code, they are quite relevant (e.g., Figure 3; more in Appendix). When comparing retrieval-only models to generative models, it is interesting to note that SCODE-R surpasses PLBART by a large margin on CodeXGLUE (Table 2), suggesting that retrieved code has high overlapping with target code that can beneﬁt the generation. Overall, the retrieval augmented generative mod- els excel in code generation. Our proposed frame- work REDCODER outperforms PLBART by a large margin, validating the advantage of reusing existing codebases to help code generation. The REDCODER-EXT gains are even higher. For CodeXGLUE (Java, Python) and Concode, the gains in BLEU are 18.88, 19.54, and 5.8. Com- paring REDCODER to REDCODER-EXT shows that BLEU scores on Concode and all metrics on CodeXGLUE are improved by∼1%. These results conﬁrm our conjecture that complementing input with paired summaries of the retrieved code help code generation. We provide a qualitative exam- ple in the Appendix to explain how