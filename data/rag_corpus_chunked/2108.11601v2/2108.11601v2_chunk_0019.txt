Figure 8 shows that typically the performance improves more with more retrievals on both tasks. However, roughly 5 Figure 7: (Python) Code gen. BLEU vs target len. CodeXGLUE (Java) gen. CodeXGLUE (Python) gen. CodeXGLUE (Java) sum. CodeXGLUE (Python) sum. Figure 8: Code gen. and sum. performance vs #re- trievals. In general performance improves with higher number of augmented candidates. code and 30 summaries work sufﬁciently well. Human evaluation Finally, we evaluate the qual- ity of code generated by SCODE-G using human evaluation. In Table 7, we perform a human eval- uation for code generation task on a subset of the test set in CodeXGLUE (Python). In this study, we compare REDCODER generated code with the code retrieved by SCODE-R. Note that both RED- CODER and SCODE-R using the same retrievers, but REDCODER generates code using SCODE- G, while SCODE-R outputs code written by real programmers. We sample 30 instances where RED- CODER generated code has a lower BLEU score than that of the SCODE-R and investigate whether the quality of code generated by them are signiﬁ- cantly different on these cases. As programming requires a speciﬁc skill, we do not evaluate the quality of the code generation us- ing the mass crowd workers. We recruit 7 Ph.D. students studying in computer science as volun- teers2 to score (1 to 5) code based on three criteria 2Before participating in the evaluation process, all the participants are informed that it is a voluntary task and it may Model Human Evaluation Automatic Metric Similarity Relevance Compilability BLEU EM CodeBLEU SCODE-R 2.09 3.00 3.16 11.56 0.00 16.66 REDCODER 2.06 2.94 3.10 10.70 0.07 18.31 Table 7: Human evaluation on code generation (CodeXGLUE-Python). REDCODER (SCODE-R + SCODE-G) achieves similar scores as SCODE-R that directly retrieves developers’ written code which suggests that the quality of the