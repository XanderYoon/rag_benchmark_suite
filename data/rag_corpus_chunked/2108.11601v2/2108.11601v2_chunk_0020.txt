voluntary task and it may Model Human Evaluation Automatic Metric Similarity Relevance Compilability BLEU EM CodeBLEU SCODE-R 2.09 3.00 3.16 11.56 0.00 16.66 REDCODER 2.06 2.94 3.10 10.70 0.07 18.31 Table 7: Human evaluation on code generation (CodeXGLUE-Python). REDCODER (SCODE-R + SCODE-G) achieves similar scores as SCODE-R that directly retrieves developers’ written code which suggests that the quality of the code generated by SCODE-G are competitive with real code from programmers’ perspective. (i) similarity, and (ii) relevancew.r.t.the target code; (iii) the compilability of the generated code. The ratings show that both models receive simi- lar scores, with a slightly higher score for SCODE- R in terms of similarity to the target code, relevancy, and compilability. This shows that the quality of the code generated by SCODE-G are competitive with real code from programmers’ perspective. In- terestingly, REDCODER achieves higher scores than SCODE-R in CodeBLEU and Exact Match even on the cases where its BLEU score is lower. 7 Related Works Code Summarization. In recent years, source code summarization attracted a lot of attention (Iyer et al., 2016; Liang and Zhu, 2018; Allamanis et al., 2016; Hu et al., 2018b; Ahmad et al., 2020). Many of these works view code as a sequence of to- ken. Other approaches leverage the structural prop- erties of code using Tree based model (Shido et al., 2019; Harer et al., 2019; Hu et al., 2018a; LeClair et al., 2019). In literature, several retrieval-based methods were proposed that leverage retrieved in- formation along with the input code. For example, Zhang et al. (2020) retrieves similar code snippet and use those as an auxiliary input for summa- rization. On the other hand, Hayati et al. (2018) retrieves related summaries for augmenting sum- marization input. Different from these approaches, REDCODER leverages both the retrieved code and its summary