formation Xj complements the input sequence x and verify its effectiveness in the experiments. Note that retrieve candidates could be a mix of singleton and pairs. In case of a singleton candi- date, we simply replace Xj or Yj with an empty string. We refer this setting as REDCODER-EXT. Although, REDCODER-EXT is a more general setting which includes “Case 1”, we study them separately to understand how these two retrieval settings beneﬁt the target tasks. We illustrate an example on code generation in Figure 5. In both Dataset Gen. Sum. Lang. Train Valid Test ∣Code∣ ∣ Summary∣ CodeXGLUE   Java 164,923 5,183 10,955 97 12 (Lu et al., 2021) Python 251,820 13,914 14,918 99 14 Concode (Iyer et al., 2018)   Java 100,000 2,000 2,000 27 72 Table 1: Dataset Statistics. Gen., and Sum. refers to code generation and summarization tasks respectively. Sum- mary denotes a natural language description paired with each code. For Concode, the input summary includes the corresponding environment variables and methods. All lengths are computed and averaged before tokenization. cases, the augmented input x′ is truncated to match PLBART’s maximum input length 512. 4 Experiment Setup In order to investigate the effectiveness of our framework, we perform a comprehensive study and analysis on code generation and summarization in two programming languages, Java and Python. 4.1 Datasets and Implementations Datasets We perform evaluation on both the tasks using the code summarization dataset from CodeXGLUE (Lu et al., 2021). It is curated from CodeSearchNet (Husain et al., 2019) by ﬁltering noisy examples. In addition, we conduct code generation experiments in Java using the Concode benchmark (Iyer et al., 2018). The dataset statistics are summarized in Table 1. Retrieval Databases To generate a source code given its natural language description or a sum- mary given the code,