that do not exactly match the reference) during ﬁne-tuning. is summary and vice versa). SCODE-R returns the the top-k output sequences {Y1, Y2, . . . ,Yk}, where sim(x, Yi)≥ sim(x, Yj)∀j > i. Training We ﬁne-tune SCODE-R using a set of parallel examples (xi, yi) of code and summaries. As mentioned in Section 2.2, DPR originally pro- posed to be ﬁne-tuned usingin-batch negatives and curated “hard” negatives from BM25 retrieved pas- sages for open-domain QA. The key idea behind “hard” negatives is to ﬁne-tune DPR to distinguish the target passage from relevant passages that do not contain the target answer. However, unlike open-domain QA, a retrieved code or summary that is not the target could still beneﬁt code generation or summarization (veriﬁed in Section 6). We pro- vide an example in Figure 3; although the retrieved code does not match the target one but can facilitate generating it. Therefore, we ﬁne-tune SCODE-R without any “hard” negatives. Speciﬁcally, for each training instance (xi, yi), the corresponding output yi is considered as positive and the other in-batch outputs (i.e., the outputs of other instances in the same batch - y1, . . . , yi−1, yi+1, . . . , ybsz) as nega- tives. Figure 4 shows an example of SCODE-R ﬁne-tuning for code generation task. 3.2 Generator: SCODE-G We adopt PLBART as discussed in Section 2.3 as the generator module of REDCODER and call it SCODE-G (Summary and CODE Generator). The input sequence x is concatenated with the top-k re- Figure 5: REDCODER-EXT input for code generation. trieved sequences to form the augmented input se- quence, x′= x⊕Y1⊕Y2 . . .⊕Yk. The augmented input x′ is fed to PLBART to estimate pgen(y∣x′). Note that a source code often consists of doc- strings, comments that can be extracted to form code