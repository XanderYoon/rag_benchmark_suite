et al., 2021). It is curated from CodeSearchNet (Husain et al., 2019) by ﬁltering noisy examples. In addition, we conduct code generation experiments in Java using the Concode benchmark (Iyer et al., 2018). The dataset statistics are summarized in Table 1. Retrieval Databases To generate a source code given its natural language description or a sum- mary given the code, our proposed approach RED- CODER ﬁrst retrieves prospective candidates from an existing code or summary database. We form the code retrieval database using the deduplicated source code (on average 1.4M functions in Java and Python) that consists of both paired (59%) and monolingual code, released in CodeSearch- NET (Husain et al., 2019). As for building the summary retrieval database, we extract the high quality natural language summaries from the paired instances in the training sets of CodeSearchNET. As many of the summaries are duplicated, we also consider the training sets in the other four avail- able languages Ruby, Javascript, Go, and PHP. We then further enlarge it by aggregating the ad- ditional summaries from the CCSD corpus (Liu et al., 2021). After performing deduplication, we retain 1.1M unique code summaries and for evalu- ating REDCODER-EXT, 20% of them can be used as pairs with the corresponding Java and Python source code. We provide the statistics of the re- trieval databases in Appendix. Note that the re- trieval databases contain code and summaries that are curated from real developers’ open sourced repositories on GitHub. By default, we exclude the target code/summary from the retrieval database. Implementations As mentioned in Section 3, REDCODER has two disjoint components. First, the dense retriever SCODE-R is implemented adopting DPR (Karpukhin et al., 2020) and the encoders in DPR are initialized from GrpahCode- BERT available in the Huggingface API (Wolf et al., 2020). In addition, we