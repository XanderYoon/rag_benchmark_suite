, Cn in code generation, X = C1, . . . , Cn, Y = S1, . . . , Sn in summary generation ). We as- sume that we have access to a retrieval database consisting of an extensive collection of source code (e.g., aggregated from GitHub or Stack Overﬂow) or summaries ( e.g., docstrings, code comments) (YR). Note that, target sequences (Y ) may or may not be present in the retrieval database (YR). Now, given an input x ∈ X, a retriever retrieves the top-k relevant output sequences from the database: Y1, Y2, . . . ,Yk ∈ YR. Then the input sequence x is augmented with the retrieved sequences to form x′ = x⊕ Y1⊕ Y2 . . .⊕ Yk, where⊕ denote the concatenation operation. Finally, a generator gen- erates the target output y ∈ Y given x′. In the following, we ﬁrst discuss the base retriever and generator modules used in REDCODER and then how we improve these components is in Section 3. 2.2 Retriever: DPR Information retrieval (IR) systems or retriever mod- els are designed to retrieve the top-k relevant doc- uments that presumably best provide the desired information (Manning et al., 2008). Term-based retrieval methods, a.k.a. sparse retrieval models, such as TF-IDF or BM25 (Robertson and Zaragoza, 2009) use sparse vector representations to perform lexical matching and compute relevance scores to rank the documents based on a query. On the other hand, dense retrieval methods en- code documents into a ﬁxed-size representations and retrieve documents via maximum inner prod- uct search (Sutskever et al., 2014; Guo et al., 2016). Particularly of interests, Karpukhin et al. (2020) propose a Dense Passage Retriever (DPR) model for open-domain question answering (QA). It con- sists of two encoders (Q(.) and P(.)) that encode queries and passages, respectively.