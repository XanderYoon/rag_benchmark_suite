similar. Retrieval based models We examine two re- triever baselines and consider the top-1 retrieved candidate as the prediction. • Dense Retriever We consider DPR as the dense retriever baseline. We evaluate both the ofﬁcially released models trained on the natural language open-domain QA task and a variant called DPR (code) that we ﬁne-tune on the evaluation datasets. • Sparse Retriever The second baseline is a sparse retriever that uses the BM25 algorithm to compute relevance scores. Generative models The generative models work in a sequence-to-sequence (Seq2Seq) fashion. • RoBERTa, RoBERTa (code) RoBERTa mod- els (Liu et al., 2019) pre-trained on natural lan- guage corpora, and source code from CodeSearch- Net (Husain et al., 2019) respectively. Methods Python Java Retrieval based methods BM25 1.92 1.82 SCODE-R 14.98 15.87 Generative methods Seq2Seq 15.93 15.09 Transformer 15.81 16.26 RoBERTa 18.14 16.47 CodeBERT 19.06 17.65 GraphCodeBERT 17.98 17.85 PLBART 19.30 18.45 Retrieval augmented generative methods BM25 + PLBART 19.57 19.71 REDCODER 21.01 22.94 REDCODER-EXT 20.91 22.95 Table 4: Evaluation BLEU-4 score for code summa- rization on CodeXGLUE. Baseline results are reported from Ahmad et al. (2021). • CodeBERT (Feng et al., 2020a) is pretrained with a hybrid objective incorporating masked lan- guage modeling (Devlin et al., 2019) and replaced token detection (Clark et al., 2020). • GraphCodeBERT (Guo et al., 2021) is pre- trained by modeling the data ﬂow graph of source code. GraphCodeBERT holds the state-of-the-art results on code search using CodeSearchNet. • GPT-2, CodeGPT-2, and CodeGPT-adapted are GPT-style models that are pre-trained on natural language (Radford et al., 2019) and code corpora CodeXGLUE (Lu et al., 2021). • PLBART (Ahmad et al., 2021) is the generator module of our proposed framework. In addition, we train an LSTM based Seq2Seq model with attention mechanism (Luong et al., 2015) and a Transformer model