Similarly, REDCODER retrieves existing code or summaries and adapts them to generate the target code or summary. In contrast, Hashimoto et al. (2018) optimizes a joint objective; Zhang et al. (2020); Liu et al. (2021) do not consider any decoder pre-training, Lewis et al. (2020) ﬁne-tunes both of the retriever and the generator end-to-end. For open domain QA, Izac- ard and Grave (2021) propose a similar model of alternative generator (multi-encoder uni-decoder). 8 Conclusion We propose REDCODER to automate developers’ writing of code and documentation by reusing what they have written previously. We evaluate RED- CODER on two benchmark datasets and the results demonstrate a signiﬁcant performance boost with the help of the retrieved information. In the future, we want to extend REDCODER to support other code automation tasks such as code translation. Acknowledgments We thank anonymous reviewers for their helpful feedback. We also thank the UCLA NLP group for helpful discussions, comments, and participating voluntarily in the human evaluation. This work was supported in part by NSF OAC-1920462, SHF- 2107405, SHF-1845893, IIS-2040961, IBM, and VMWare. Any opinions, ﬁndings, and conclusions expressed herein are those of the authors and do not necessarily reﬂect those of the US Government. References Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020. A transformer-based ap- proach for source code summarization. In Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4998–5007, Online. Association for Computational Linguistics. Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Uniﬁed pre-training for program understanding and generation. In Pro- ceedings of the 2021 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics. Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big code and naturalness. ACM Com- puting