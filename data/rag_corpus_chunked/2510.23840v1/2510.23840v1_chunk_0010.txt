loaded into the game environment and Unity workspace where sets of geometric space transformation (distortion treatment) are deployed. Virtual models are placed into the scene in relation to the physical room to project reconstructed geometry. This allows our system to extend the virtual world from the real world, as if the room is transforming, by rendering the physical world within the scene. To simulate the changing geometric environment through the projected walls of the physical room, the synchronization be- tween the user’s head position, digital twin, and the real world is crucial. The live reconstruction of the physical environment is done using four RGB-D cameras (Microsoft Kinect v2) that are placed in each corner of the room. The real-time geometric representation of the world is then directly placed in the virtual game environment that warps and enlarges while tracking the head position to reflect the user’s perspective. 3.1 System Infrastructure The AR projected room of approximately 4.5 × 5.5 meters incorpo- rates four Microsoft Kinect v2 depth cameras in each corner of the ceiling line. We placed the furniture objects as we would in our own living room, making a close representation of the living room where the user would use our system. Five wide field-of-view projectors render a 360◦ Spatial AR system, projecting to surfaces within the room and fully utilizing the objects inside the room including the furniture and moving objects. Virtual objects are presented from the user’s viewpoint as found in RoomAlive system [18]. The distorting 3D geometry model is placed and oriented in the physical room and the scene is constructed based on the viewpoint of the user. As the projection is rendered in a view-dependent manner, the participant is free to walk around the room without a headset. As the user in the room is