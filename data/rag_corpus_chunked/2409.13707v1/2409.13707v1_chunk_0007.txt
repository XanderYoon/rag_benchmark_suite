0.62 and recall from 0.75 to 0.98. The lower perfor- mance can be explained in part by the varying class imbal- ance across products (positive class proportion from 11% to 44%) as well as the products’ differing inter-annotator agreement (See Section 6). Despite this, the model still substantially outperforms random guessing of the classes. Hyper-parameters including batch size, learning rate, and dropout were determined based on a small grid search. We then evaluated the fine-tuned classifier on about 1,400 cases from three additional products that were not in the training set to validate if the model generalized to other products. The resulting F1 of 0.65, precision of 0.54, and recall of 0.80 for the three products suggests that classifier model generalizes well to products not seen during training even with substantially different class balance. Query Generator Model BertScore F1 ROUGE-L F1 Falcon-40B* 0.91 0.40 Mistral-Large-2 0.91 0.38 Mixtral-8x7B-Instruct 0.91 0.36 Granite-13B-Chat-v2 0.89 0.28 Table 2: Comparison of BertScore F1 and ROUGE-L F1 for different models on query generation task. BertScore is based on roberta-large embeddings. Falcon-40B scores are not comparable to the other models because the prompt used was different, and it was used to create the initial questions that were validated or edited by SMEs to create the ground truth. In order to create a concise query that could be used by the retriever, we generated a single sentence question based on the case subject and description. Our experiments over various open-source generative models (Table 2) and model availability in the client’s services led us to choose Mixtral- 8x7b-Instruct as the model for query generation which reli- ably reproduced the ground truth queries despite being a rel- atively small model with no domain knowledge. Note that the results are skewed for Falcon-40B (Almazrouei et al. 2023) as Falcon-40B generated