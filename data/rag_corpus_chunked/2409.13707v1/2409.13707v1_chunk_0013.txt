= Mostly Correct: Most details are accurate, with only minor gaps or irrelevant information. • 1 = Correct: Includes only relevant details. In Table 5, we present the final score for each model as the average of human annotators’ scores across 40 question-answer pairs of which InstructLab-IT emerged as the best model. While Llama-3.1-8b-Instruct performed slightly better than GPT-4o, the improvement in results with InstructLab-IT was very noticeable over both models. This is especially significant considering the model sizes: GPT- 4o (over 1 trillion parameters and 1.5 TB), Llama-3.1-8b- Instruct (8 billion parameters and 16 GB), and InstructLab- IT (7 billion parameters and 28 GB). These results signal that a smaller, domain-specific model tuned for a specific set of use cases may better meet client requirements. 5 Deployment The tool is currently integrated into the ticketing system but silently deployed (not visible to agents) for testing purposes. Model BertScore (roberta-large) F1 BertScore (deberta-xlarge-mnli) F1 ROUGE-L F1 GPT-4o (2024-08-06) 0.86 0.62 0.34 Mistral-Large-2 0.86 0.62 0.37 Mixtral-8x7B-Instruct 0.87 0.64 0.41 Granite-13B-Chat-v2 0.86 0.58 0.32 Table 4: Comparison of BertScore F1 and ROUGE-L F1 for different models performing the answer generation task. BertScore based on roberta-large embeddings Figure 3: Working mockup of online deployment UI of single system result with feedback items. Model Score GPT-4o 0.68 Llama-3.1-8b-Instruct 0.70 InstructLab-IT 0.76 Table 5: Comparison of analytic rubric scores for different models on answer generation task. We are currently working on refinements and integration and plan to deploy the system before the end of the year. We will incorporate feedback buttons for the tool once it is de- ployed online and visible to agents. In the customer support portal user interface, for a given case, support agents will see a suggested solution and link. This will include five-star ratings for accuracy and readability as