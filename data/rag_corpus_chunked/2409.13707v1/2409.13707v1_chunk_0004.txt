Research 2024) that was fine- tuned on almost 14,000 examples labeled by subject matter experts. If the case is predicted to be single turn, the case continues to the next step in the pipeline. Question Generator: The question generator summarizes the often vague case subjects and verbose, convoluted de- scriptions into concise text queries suitable for the retrieval system. The system prompts a large language model, Mix- tral 8x7B Instruct (Jiang et al. 2024), to generate a concise question based on the provided case subject and descrip- tion. Additional post-processing keeps only the first gener- ated sentence in case the generative model does not follow instructions and generates additional sentences beyond the first question. Retriever: Our documentation is retrieved from multiple data collections in a Milvus vector database, all indexed with the standard Slate-30M embedding. If the search stage re- quires top-3 documents, we search for 3 from each of these indexes, and then merge-sort based on the score before re- taining the top-k from the combined set. Having retrieved the top-3 documents, as ranked by a general-purpose embedding, we re-rank them using a Bi- Encoder model that has been fine-tuned using application- specific training data. Re-ranker scores are computed as co- sine similarities of a combination of the original case and the derived question, compared with the same passages (with re- calculated vectorizations) that were matched in the first pass. Answer Generator: For each of the top three documents retrieved, the answer generator produces an answer to the previously generated query by leveraging the top three re- ranked passages from the document. First, we split the re- trieved document content into 2500-token chunks with 250- token overlaps and use cosine similarity to pick the three most relevant chunk contexts. The answer generator prompts a large language model, Mixtral, to