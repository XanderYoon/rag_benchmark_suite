spending time early on to understand how stakeholders will interact with the system, knowing that changes and evolution in the actual workflow may cause a decrease in system performance. Inter-Annotator Agreement Product Classifier Question Link Product 1 0.80 (20) 0.75 (16) 0.50 (16) Product 2 0.50 (20) 0.50 (10) 0.70 (10) Product 3 0.15 (20) 1.00 (3) 1.00 (3) Product 4 0.65 (20) 0.85 (13) 0.69 (13) Product 5 0.65 (20) 0.54 (13) 0.69 (13) Product 6 0.25 (20) 1.00 (4) 1.00 (4) Total 0.50 (120) 0.72 (59) 0.67 (59) Table 6: Inter-Annotator Agreement: Proportion of labels that 3 annotators agreed on. Total N in parenthesis. For ques- tion and link labels, proportions only calculated based on cases for which all 3 annotators labeled as single turn and evaluated quality of corresponding question and link. Three SMEs labeled a subset of twenty cases to determine inter-annotator agreement. The results in Table 6 show that labeling cases as single vs. multi-turn is not a trivial task and for most products, SMEs disagreed widely. Of the cases in which all three annotators agreed to be single-turn, agree- ment on the question and link quality was better but still raises questions about the validity of the training and evalu- ation data. In particular, the low agreement of the provided links can be explained by the fact that more than one link can potentially solve the same question and so neither annota- tor is necessarily wrong. This suggests that for ground truth data, we should consider a list of correct links instead of a single ground truth link for each question. The low agree- ment of single-turn vs. multi-turn labels also potentially ex- plains the lower performance of the classifier model if the model is attempting to learn from potentially conflicting in- formation. RAG