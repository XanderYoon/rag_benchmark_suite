of our action generation task for AIOps domain, we employ the knowledge infusion methodology described in (Sudalairaj et al. 2024). First, we manually created a seed dataset with six tuples, each con- taining context and four related question-answer pairs. Then, we randomly selected fifteen documents from the corpus to guide synthetic data generation, using the seed dataset to replicate similar artifacts for each document. Using this seed dataset, we created 14,000 synthetic samples with the Mixtral-8x7B-Instruct model as the teacher. IBM’s Gran- ite 7B IL-Internal-Granite-7B-Base, a much smaller model, was fine-tuned with IT domain data to cater to the specific task of answer generation for the IT Support use case re- sulting in the InstructLab-IT model with domain knowledge infusion. To evaluate the quality improvement, we conducted a user study with 6 technical experts and forty test question-answer pairs per model. For each question, we retrieved context from a 1200-document corpus of six software products and used it to prompt each model separately for an answer. Our user study used a 0 to 1 rubric to evaluate answer correctness with clear descriptions for each level: • 0 = Incorrect: irrelevant or fails to answer the question. • 0.25 = Mostly Incorrect: Some details are correct, but key details are missing, fabricated, or mostly irrelevant. • 0.5 = Partially Correct: Most details are correct, but some key details are missing, fabricated, or include a lot of ir- relevant information. • 0.75 = Mostly Correct: Most details are accurate, with only minor gaps or irrelevant information. • 1 = Correct: Includes only relevant details. In Table 5, we present the final score for each model as the average of human annotators’ scores across 40 question-answer pairs of which InstructLab-IT emerged as the best model. While Llama-3.1-8b-Instruct performed slightly better than GPT-4o, the