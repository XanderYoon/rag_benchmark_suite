and plan to deploy the system before the end of the year. We will incorporate feedback buttons for the tool once it is de- ployed online and visible to agents. In the customer support portal user interface, for a given case, support agents will see a suggested solution and link. This will include five-star ratings for accuracy and readability as well as a drop-down menu for feedback including the options: “useful”, “some- what useful”, “no useful suggestion”, and “need more client info”. See Figure 3 for a working mock-up of the user inter- face. 6 Lessons Learned Use Case Formation Defining the proper use case is probably the most critical step in developing a proper RAG recommendation applica- tion. Because of the expense of data collection, annotation, and development, any confusion or change in the exact use case and capabilities of the model can result in substantial delays and costs. For example, the first dataset that we considered for this use case was synthetic data for which subject matter ex- perts crafted questions based on support document titles, and then provided corresponding answers. When we compared this data to actual customer cases, we found the genuine questions to be more verbose and to contain more off-topic “noise.” Thus we decided to use the more challenging actual support ticket data for training and validation, as it appeared better suited to our final deployment than the cleaner syn- thetic data. We recommend spending time early on to understand how stakeholders will interact with the system, knowing that changes and evolution in the actual workflow may cause a decrease in system performance. Inter-Annotator Agreement Product Classifier Question Link Product 1 0.80 (20) 0.75 (16) 0.50 (16) Product 2 0.50 (20) 0.50 (10) 0.70 (10) Product 3 0.15 (20) 1.00 (3) 1.00 (3) Product