(yellow) and without (orange) re-ranking vs. direct Google search via SerpApi (grey) for 1729 customer issues over six products and a link provided by a subject matter expert. The source of the link was randomized as Source A or Source B so that, for example, Source A could be either our tool or an SME for any given case. The support agents were asked to rate each link as shown in Table 3 and to pick the better of the two solutions. The results for Product A show that support agents gave higher ratings to and preferred the links suggested by the tool over those from a SME. The results for Product B how- ever show higher scores for links provided by the SME but about half of the cases were still rated as having a somewhat helpful or complete link provided by our tool. Additionally, when directly asked to compare the recommendations, sup- port agents reported that the tool was more helpful or just as helpful as the SME link 69% of the time for Product A and 35% of the time for Product B. Answer Generator The final step of our solution takes in the generated query and top three most relevant retrieved passages as context to prompt the answer generator. In particular, the prompt asks the model to use the information within the provided con- texts to generate an answer. Additionally, if the context is insufficient, then the model is instructed to state that an ac- curate answer cannot be provided. To evaluate the answers, we used the subject matter ex- pertâ€™s annotated ground truth answers and ground truth doc- uments verified to contain the answer to the question. We compared the answers generated by the answer extractor us- ing the ground truth document to the ground