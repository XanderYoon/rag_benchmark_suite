retrieved, the answer generator produces an answer to the previously generated query by leveraging the top three re- ranked passages from the document. First, we split the re- trieved document content into 2500-token chunks with 250- token overlaps and use cosine similarity to pick the three most relevant chunk contexts. The answer generator prompts a large language model, Mixtral, to answer the query us- ing the three contexts. Finally, the system returns the three retrieved links with their corresponding generated answers. The results can then be displayed to the support agent in a graphic interface. 3 Data We collected almost 19,000 real support cases across nine software products: six to serve as training and three to serve as validation. Each case included a case subject and descrip- tion originally drafted by a customer when opening the case. Additionally, the three indices for documentation leveraged for retrieval had a total corpus of over 5 million documents. For each product, we asked five support agents who were singled out as product subject matter experts to carry out the following tasks for each of the nine products: 1) anno- tate single-turn vs. multi-turn label, 2) validate silver ground truth query based on case subject and description and pro- vide updated query as necessary, 3) provide link to relevant document in support corpus, and finally 4) copy and paste solution to query as found in the relevant support document. Tasks #2 through #4 were carried out only for cases that had been labeled as single-turn. After cleaning and removing missing annotations, we created a dataset of almost 19,000 support cases, with almost 3,000 cases for each training product and over 400 cases for each evaluation product. 4 Development and Validation Case Turn Classifier As our solution is meant to supply answers before involv- ing