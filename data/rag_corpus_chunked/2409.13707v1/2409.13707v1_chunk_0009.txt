scores, using a threshold of 0.90 as sufficiently similar to count as identical. For retrieval, a dedicated team is already responsible for maintaining indexed collections of the software product doc- umentation. This saves our project from gathering, maintain- ing, and indexing all of these documents, and its base Slate- 30M embedding returns a good first set of results. Re-ranking this first set allows us to use fine-tuning to improve performance without maintaining a parallel set of indexes. For this final task-specific fine-tuning stage, we used training data based on 1,430 questions with up to three matching passages per question extracted from documents identified in user interactions, together with negative exam- ples found using BM25 search. The resulting IBM Slate- 125M model was then distilled into the deployed IBM Slate- 30M model. To evaluate its effectiveness, we present recall before and after re-ranking with Google Search as a baseline in Figure 2. 0 - Completely 1 - somewhat 2 - Solution Rating irrelevant relevant/helpful in link Product A SME 58% (11) 0% (0) 42% (8) Tool 26% (5) 5% (1) 68% (13) Product B SME 20% (12) 48% (28) 28% (17) Tool 48% (29) 37% (22) 12% (7) Table 3: AB testing human evaluation of retrieved links We also conducted an AB test in which support agents of two products were provided with a link retrieved by the tool Figure 2: Retriever recall for top x (log scale), compar- ing with (yellow) and without (orange) re-ranking vs. direct Google search via SerpApi (grey) for 1729 customer issues over six products and a link provided by a subject matter expert. The source of the link was randomized as Source A or Source B so that, for example, Source A could be either our tool or an SME for any given case. The