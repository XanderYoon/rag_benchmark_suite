the model is instructed to state that an ac- curate answer cannot be provided. To evaluate the answers, we used the subject matter ex- pert’s annotated ground truth answers and ground truth doc- uments verified to contain the answer to the question. We compared the answers generated by the answer extractor us- ing the ground truth document to the ground truth answer using BertScore (Zhang et al. 2020) and ROUGE-L F1. We evaluated different models and prompts to find the optimal combination and present the results of the models assessed in Table 4. While BertScore (roberta-large) F1 is rather low (in practice it ranges between 0.85-0.95), ROUGE-L F1, tra- ditionally a rather strict metric, shows promising results for Mixtral-8x7b-Instruct with a score of 0.41. Mixtral-8x7b- Instruct’s outperforms of GPT-4o, included as a baseline for larger models, in all three metrics, despite having substan- tially less parameters. Likewise, Granite-13B-Chat-v2 is not far behind GPT-4o despite its merely 13 billion parameters compared to GPT-4o’s rumored hundreds of billions or even trillions of parameters. This suggests that the RAG approach of smaller models leveraging retrieved context is a viable so- lution for IT incident resolution recommendation systems. Knowledge Infusion for Answer Generation Directly applying general foundational models to AIOps for answer generation tasks often does not yield optimal re- sults. The knowledge infusion approach involves adapting these pre-trained models to specific tasks through additional training on task-specific data. To further boost the results of our action generation task for AIOps domain, we employ the knowledge infusion methodology described in (Sudalairaj et al. 2024). First, we manually created a seed dataset with six tuples, each con- taining context and four related question-answer pairs. Then, we randomly selected fifteen documents from the corpus to guide synthetic data generation, using the seed dataset to replicate similar