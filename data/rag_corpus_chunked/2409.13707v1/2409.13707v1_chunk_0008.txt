over various open-source generative models (Table 2) and model availability in the client’s services led us to choose Mixtral- 8x7b-Instruct as the model for query generation which reli- ably reproduced the ground truth queries despite being a rel- atively small model with no domain knowledge. Note that the results are skewed for Falcon-40B (Almazrouei et al. 2023) as Falcon-40B generated the first pass of silver ground truth queries that were then edited by subject matter experts. Retriever Support experts supplied us with a collection of cases with one ground truth link each that a “correct” solution should reference; our evaluation is based on whether this link is contained in the top n links returned (for various values of n). Implementing this evaluation presented several chal- lenges: • URL Duplication: A single page of documentation often has several different URLs to identify it. • Subtle Content Variations: Documentation for the same topic in different versions of a product may have subtly different titles, like “How to update a list” and “Lists: Updating”. • Identical Content Across Different Documents: Docu- mentation for the same topic in different versions may be identical in which case results from different versions are still valid. Mitigating the first of these challenges, many of our docu- mentation pages include a “canonical link” in their metadata. In many cases, this allows us to identify identical links. The two issues with documentation evolution between versions are addressed with Rouge-1 scores, using a threshold of 0.90 as sufficiently similar to count as identical. For retrieval, a dedicated team is already responsible for maintaining indexed collections of the software product doc- umentation. This saves our project from gathering, maintain- ing, and indexing all of these documents, and its base Slate- 30M embedding returns a good first set of results. Re-ranking this