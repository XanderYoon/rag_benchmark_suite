carried out only for cases that had been labeled as single-turn. After cleaning and removing missing annotations, we created a dataset of almost 19,000 support cases, with almost 3,000 cases for each training product and over 400 cases for each evaluation product. 4 Development and Validation Case Turn Classifier As our solution is meant to supply answers before involv- ing a support agent, we needed to develop a method for classifying incoming cases as single-turn vs. multi-turn. The classifier model is a binary encoder-only IBM Slate 125m model fine-tuned on the single-turn/multi-turn labels of al- most 7,000 unique cases across six software products. The final training data is around 14,000 cases as it includes two copies of a given case: one with tokens from both the case subject and description fields, and another with tokens only from the case subject.. Positive Train Eval Class % F1 P R In-Domain 13964 3512 25% 0.46 0.31 0.89 Out-of-Domain - 1375 53% 0.65 0.54 0.80 Table 1: Single-Turn classifier model performance on six in- domain training products and three out-of-domain evalua- tion products For our application, we prioritized correctly predict- ing single-turn cases (positive class) versus multi-turn cases (negative) emphasizing recall over precision. Table 1 presents the final fine-tuned model performance on the held-out evaluation set of the six products when using a classification threshold of 0.1. We found that performance varies widely depending on the product, ranging from F1 of 0.27 to 0.62 and recall from 0.75 to 0.98. The lower perfor- mance can be explained in part by the varying class imbal- ance across products (positive class proportion from 11% to 44%) as well as the productsâ€™ differing inter-annotator agreement (See Section 6). Despite this, the model still substantially outperforms random guessing of the classes. Hyper-parameters including batch size, learning rate,