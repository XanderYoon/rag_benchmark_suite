One labeler marked to play sound 265 0.1281 Two labelers marked to play sound 210 0.1015 All three labelers marked to play sound 336 0.1624 Table 3: Distribution of sentences in terms of labels. Though simple, we prove that these intuitive features produce safer results than plain retrieval results. In our experiments, we em- ploy two popular machine learning approaches, SVM and XGBoost, as our basic classifier. 4 EXPERIMENTS In this section, we build a dataset and conduct experiments to validate the effectiveness of our proposed models. 4.1 Setup Cross-modal retrieval relies highly on the precision of tags that de- scribe the content of the sound effects. Therefore, one key step is to establish a sound-tag dataset with rich keywords. We take the scene- trigger sound effect as the example and collect the sound effects data under corresponding categories and their descriptions from a famous Chinese public sound effect website 4. We pre-process the descriptions of sound effects by removing all the words except for verbs and nouns, and use them as the basic tags. Next, we extend these tags with similar words from both synonym tools 5 and pre- trained word embeddings space. Our preliminary dataset includes sound effects and multiple synonymous tags for each of them. In addition to the sound-tag dataset, we need another radio story dataset with ground truth for training and evaluation. We select the 16 most common scene-triggered sound effects and use their tags as queries to retrieve the candidate sentences from 632 new stories. Note that the top 16 categories of the scene triggers account for around 83.88% of all scene-triggered sound effects. Focusing on these categories helps alleviate the noise caused by those long- tailed sound effects. The 16 kinds of scenes are "forest", "mountain", "river", "sea", "rain", "wind", "thunder", "park", "party",