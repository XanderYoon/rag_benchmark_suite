the imagery but also in increasing their attention span [9]. However, producing one radio story with sound effects takes onerous human work. With the mature development of text-to-speech technology [11] and its successful application on storytelling [3, 6, 7], it is interesting to study the process of automatically adding sound effects to radio stories. Consequently, an end-to-end pipeline could be deployed for radio story generation as shown in the Figure 1. In this paper, we explore the application of cross-modal retrieval methods on this problem, namely the inevitable difficulties in keyword search scenarios faced by a naive model and the potential solutions. Cross-modal retrieval has been studied in depth [13], especially between text and images [1, 4]. The basic retrieval models between text and sound also have a rich history, especially on the content- based audio retrieval [ 2, 5, 10, 12]. The quality of such models arXiv:1908.07590v1 [cs.IR] 20 Aug 2019 Story Database Annotated Story Sound-Tag Database Text-to-Speech Synthesis Semantic Inference Model Music Database Sound Effects Background Music High-quality Radio Story Figure 1: A sketch of radio story production pipeline. Note that the procedures indicated by the solid lines are consid- ered in the task of this paper. depends on the richness and accuracy of sound descriptions for training, which are designed deliberately and aligned perfectly with the sounds. In this paper, we focus on tag-based retrieval methods for the preliminary results of adding sound effects to radio stories. However, unexpected difficulties need to be addressed first under such a practical setting. To be specific, texts are often loosely related to the sounds and contain lots of noise, and simple keywords cannot be used consistently as reliable queries to retrieve sound effects. Datasets of public sound effects with descriptive tags are com- mon such as Adobe Audition Sound Effects