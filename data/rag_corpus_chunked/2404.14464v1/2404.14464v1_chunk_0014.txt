for few-shot demonstrations, and adopted a 3-shot setting for all baselines and ours. In the main experiment, the depth of TOR is set to 3, with the number of nodes in each layer be- ing 5, 3, 3. We adopted the evidence-based fusion method, missing paragraph completion expansion strategy, and two kinds of pruning strategies. CoR: To compare the differences between the tree and chain structures, we designed an experiment using the same prompt as TOR but providing only a single reasoning path. The model chooses an action in each iteration and generates new queries based on all retrieved paragraphs. See Table 13 for details. 4.5 Main Result As shown by Table 1 and Table 2, our methodTOR achieves nearly optimal performance in both re- trieval metrics and generation metrics across three datasets under two different base models. In the experiments with GPT-4-Turbo as the base model 6 for the three datasets, the retrieval metrics outper- form the best-performing baseline method ITER- RETGEN by 13.3%, 12.0%, and 1.5%, respectively. Meanwhile, the F1 values for response generation surpass the highest values among the various base- line methods, with improvements of 9.2%, 10.3%, and 0.3%, respectively. We consider three reasons for achieving these re- sults: 1) TOR allows the model to explore multiple reasoning paths, effectively mitigating the cascad- ing errors caused by single reasoning path mis- takes. IRCoT, ITER-RETGEN, and CoR (intro- duced in Section 4.4) are all based on chain-of- thought reasoning, and their final response quality is constrained by the accuracy of retrieval and rea- soning at each step along the path. In contrast,TOR employs a tree structure to expand into different paths, sharing the risk of retrieval and reasoning failures. 2) The TOR structure can effectively re- duce the interference of useless information. IR- CoT, ITER-RETGEN, and CoR