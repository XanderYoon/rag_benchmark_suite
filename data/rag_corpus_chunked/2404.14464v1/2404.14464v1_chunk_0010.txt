48.5 Table 1: Paragraphs recall@15 on multi-hop question answering datasets. We highlight the best results in bold and underline the best results among other methods. quently prunes the paths expanded from irrelevant paragraphs. Repetitive pruning is conducted after retrieval, where it matches the paragraph ID of re- trieved paragraphs and received paragraphs in the evidence pool. If any retrieved paragraph is already in the evidence pool, it is pruned. Effective expansion aims to optimize the effec- tiveness and diversity of paragraphs review block initiating new queries. We adopt CoT Expansion and Missing Paragraph Completion Expansion (MPC). CoT expansion allows the model to think step by step, identify missing information in cur- rent paragraphs and generate a new query based on this missing information. MPC expansion enables the model to complete the missing information in paragraphs using its internal knowledge and to use the newly generated paragraph as a new query. See Table 7 and Table 9 for details. 4 Experiments 4.1 Datasets We conducted experiments on three multi-hop rea- soning datasets: HotpotQA with Fullwiki setting (Yang et al., 2018), 2Wiki-MultiHopQA (Ho et al., 2020), and the answerable subset of MuSiQue (Trivedi et al., 2022). For HotpotQA and 2Wiki- MultiHopQA, we used the Wikipedia dump from December 2018 as the retrieval source, while for MuSiQue, we used the Wikipedia dump from De- cember 2021. Following the work of previous researchers (Shao et al., 2023), we used the first 500 questions from the development sets of these datasets for retrieval and response generation per- formance evaluation in Table 1 and Table 2. Then, randomly selected 100 questions from the remain- ing part for hyperparameter tuning in Table 3 to Table 6. 4.2 Evaluate Setting We evaluatedTOR from retrieval quality and gener- ation quality. For the retrieval metric, we followed Trivedi et al.