frame- work calls the LLM at each node, which improves retrieval performance but introduces additional computational overhead. Although we have de- signed two different pruning strategies to alleviate this issue, an average of 16 LLM calls still exists. In future work, we plan to optimize the framework in the following ways: 1) Implement a more fine- grained repetitive pruning strategy, which involves pruning repetitive paragraphs from multiple per- spectives, such as semantic similarity. 2) Develop a more powerful retriever: the experimental results show that reducing tree depth and width can ef- fectively decrease the number of calls, and a more powerful retriever can recall relevant paragraphs more effectively, allowing for a reduction in tree depth and width. 3) Introduce an early termina- tion mechanism: the framework would dynami- cally choose to terminate the tree search early when the LLM believes sufficient evidence has already been obtained. Moreover, akin to several other baseline methods with which we have drawn comparisons, our exper- iments employed the OpenAI LLM API. Owing to the deprecation of the text-davinci-002 API em- ployed by IRCoT and the text-davinci-003 API em- ployed by ITER-RETGEN, we could not employ identical models for a fair comparison. To contrast their approaches, we conducted experiments us- ing the gpt-4-1106-preview and gpt-3.5-turbo-0125 APIs. Although we used the prompts reported in the baseline studies, the issues about prompt trans- ferability precluded a guarantee of fully replicating the effects of their methods. Recognizing that the APIs we have employed may also be deprecated at some point in the future, we intend to release all prompts and code to make our research easier to replicate for future study. Lastly, the performance of TOR on other com- plex reasoning tasks still requires further verifi- cation. We have only validated the effectiveness of the TOR framework on