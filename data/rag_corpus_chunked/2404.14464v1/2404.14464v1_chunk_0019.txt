increases non-linearly. This is because our framework gener- ates more feasible paths through node expansion. As this expansion grows exponentially with the in- crease in tree depth, we need to reasonably limit the depth of the tree to ensure search efficiency. 2) The effective call rate decreases with the deepening of the tree depth. Even though repetitive pruning reduces the repetitive calls of accepted paragraphs, it cannot avoid some unaccepted paragraphs be- ing reviewed multiple times. This phenomenon is amplified with the increase in tree depth. 3) By expanding the breadth of each tree layer, our frame- work can retrieve more paragraphs and obtain more evidence while ensuring an effective call rate. No- tably, its retrieval metrics decrease while its genera- tion metrics improve. We think that the evidenceâ€™s proportion of ground truth paragraphs decreases as the breadth increases, leading to fewer recalled ground truth paragraphs at a specific quantity. How- ever, the reader can add more evidence (more than 15) for response generation, thus improving the generation metrics. 4) To balance performance and time cost, we ultimately chose a depth of 3 and widths of 5, 3, and 3. 5 Conclusion This paper proposes TOR, a tree-structured dy- namic retrieval framework for multi-hop question- answering tasks. This framework leverages the tree structure and the chain-of-thought capability of Large Language Models(LLMs) to dynamically explore multiple feasible reasoning paths. Experi- mental results demonstrate that the method effec- tively explores more diverse reasoning paths while 8 reducing ineffective path expansion. We believe that TOR can serve as a robust baseline model for future research in multi-hop question-answering tasks. Moreover, we hope our framework can be extended to more complex reasoning tasks. Limitations TOR has requirements for the capabilities of the base models, including 1) The model should have zero-shot or few-shot CoT