prompt- ing (OneR-Direct/ CoT) augments Direct/CoT 1In this work if the number of retrieved paragraphs exceeds 15, we re-rank the evidence in the evidence pool based on the similarity between the evidence and the final response. We select the top 15 paragraphs according to their similarity scores. 5 Method GPT-3.5-Turbo GPT-4-Turbo HotpotQA 2WikiMQA MuSiQue HotpotQA 2WikiMQA MuSiQue EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 Without Retrieval Direct 28.2 37.7 27.6 31.8 9.6 18.2 40.6 52.2 38.4 47.6 25.9 36.0 CoT 27.8 38.8 26.7 33.6 17.2 25.1 39.6 53.3 42.2 51.8 24.0 36.8 With Retrieval OneR-Direct 25.0 33.4 20.6 23.8 5.0 10.3 40.2 53.6 32.6 42.4 26.2 37.4 OneR-CoT 24.8 32.1 14.0 18.7 5.0 11.2 39.6 52.3 36.6 47.0 22.8 34.9 ReAct 25.8 37.2 14.6 24.3 2.2 7.7 34.8 47.5 37.7 48.2 26.4 38.0 Self-AsK 23.4 32.6 15.4 22.3 5.6 12.5 39.1 51.3 41.1 52.6 28.8 40.6 ITER-RETGEN 25.8 36.7 16.6 23.0 9.4 16.7 46.2 58.9 39.8 51.0 31.4 43.3 IRCoT 29.8 38.9 26.4 30.4 8.2 15.0 42.8 53.9 39.2 49.3 28.6 40.1 CoR 30.6 39.8 25.2 28.8 9.2 16.8 41.7 55.3 43.6 54.1 26.6 38.4 ToR 38.2 50.4 29.0 37.0 13.2 22.1 49.2 63.1 51.0 62.9 30.9 43.6 Table 2: Answer EM and F1 results on multi-hop question answering datasets. We highlight the best results in bold and underline the best results among other methods. Prompting with paragraphs retrieved by the re- triever. ReAct/Self-Ask (Yao et al., 2023b; Press et al., 2023) iteratively execute the following steps : (i) Initiate retrieval using the follow-up question gen- erated by the LLM, returning relevant paragraphs, and (ii) Respond to the follow-up question, sub- sequently deciding whether to generate the next question or finalize the answer. The primary dis- tinction between ReAct and Self-Ask in our imple- mentation