path is either accepted, rejected, or reaches its maximum search depth. 3.3 Evidence Fusion The evidence pool contains some pieces of evi- dence. The QA reader will generate a response according to the evidence pool. We propose three simple methods for evidence fusion: Analysis-based Fusion: The reader generates a response only according to the brief analysis. Paragraph-Based Fusion: The reader generates a response only according to the paragraphs. Evidence-based fusion: The reader generates a response according to both of them. See Table 10 to 12 for details. 3.4 Tree-Based Search Optimization Although the tree structure can explore more di- verse reasoning paths and reduce failures caused by a single reasoning path, it introduces longer time overheads. Therefore, we propose pruning and effective expansion to reduce redundancy and irrelevant expansion in the search process while guaranteeing expansion diversity. Pruning aims to reduce the initiation of in- valid searches. We propose two methods: Rele- vance Pruning and Repetitive Pruning. Rele- vance pruning is conducted at the paragraphs re- view block, where the model judges whether the paragraphs are relevant to the question and subse- 4 Method GPT-3.5-Turbo GPT-4-Turbo HotpotQA 2WikiMQA MuSiQue HotpotQA 2WikiMQA MuSiQue OneR 44.3 45.8 23.2 44.3 45.8 23.2 ReAct 44.6 48.0 25.2 51.3 46.1 35.5 Self-AsK 44.0 50.7 25.9 52.9 59.5 37.2 ITER-RETGEN 50.6 51.1 27.2 60.5 67.4 47.0 IRCoT 46.0 46.5 25.2 53.3 53.9 36.5 CoR 47.9 47.6 25.8 61.0 62.4 39.4 ToR 53.1 51.8 29.5 73.8 79.4 48.5 Table 1: Paragraphs recall@15 on multi-hop question answering datasets. We highlight the best results in bold and underline the best results among other methods. quently prunes the paths expanded from irrelevant paragraphs. Repetitive pruning is conducted after retrieval, where it matches the paragraph ID of re- trieved paragraphs and received paragraphs in the evidence pool. If any retrieved paragraph