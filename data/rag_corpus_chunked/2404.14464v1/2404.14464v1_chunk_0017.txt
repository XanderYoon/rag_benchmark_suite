the MPC strategy exhibits the best performance, which may be attributed to the extensive knowledge stored in recent LLMs. This confirms that utilizing both parametric and non-parametric information during the retrieval process can improve retrieval and gen- eration performance (Yu et al., 2023; Sun et al., 2023). The pruning strategies ensure performance while reducing time cost. As shown by Table 5, repetitive pruning improves the effective call rate, significantly reducing the time of API calls for the same paragraph and lowering the time cost. Without repetitive pruning, the framework can re- trieve more different paragraphs and obtain more evidence through node expansion, which leads to 7 Method #API #Doc Rate #Evidence Recall@15 EM F1 ToR 16.9 15.7 92.9 2.9 79.3 51.6 63.7 w/o repetitive pruning 33.5 18.3 54.6 3.7 76.4 51.4 63.8 w/o relevance pruning 29.1 24.6 84.5 3.3 73.2 48.9 59.3 w/o both 65.0 31.8 48.9 4.4 72.9 49.1 59.4 Table 5: Results of different pruning strategies on 2WikiMQA, #API represents the average number of GPT API calls. #Doc represents the average number of different paragraphs retrieved. Rate = #Doc/#API, which means the number of reviewed paragraphs per API call, where a higher value indicates more effective API calls (the higher, the better).#Evidence represents the average number of evidence in the evidence pool. Other metrics are introduced in Section 4.2. TOR use both repetitive pruning and relevance pruning. w/o repetitive pruning only uses relevance pruning, and w/o relevance pruning only uses repetitive pruning. w/o, both donâ€™t use any pruning strategies. Depth width #API Rate #Doc #Evidence Recall@15 EM F1 2 5,3 10.3 10.0 97.1 1.8 69.7 44.3 55.6 3 5,3,3 16.9 15.7 92.9 2.9 79.3 51.6 63.7 4 5,3,3,3 36.3 27.6 76.0 4.7 79.7 51.8 64.4 3 10,5,3 41.3 39.2 94.9 6.8 75.4 52.4 66.0 Table 6: Results for