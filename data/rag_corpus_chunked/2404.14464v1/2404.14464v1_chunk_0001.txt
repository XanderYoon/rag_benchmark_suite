We conducted experiments on three different multi-hop question answering datasets. The results show that compared to the baseline methods, TOR achieves state-of-the- art performance in both retrieval and response generation. In addition, we propose two tree- based search optimization strategies, pruning and effective expansion, to reduce time over- head and increase the diversity of path exten- sion. We will release our code. 1 Introduction Large Language Models (LLMs) have demon- strated the capacity for multi-step reasoning (Wei et al., 2022). This is achieved by generating in- termediate reasoning steps, a process known as Figure 1: The chain-like iterative retrieval process faces the issue of error accumulation. The example shows how irrelevant retrieval results affect subsequent retrievals and the generation of new queries, ultimately leading to incorrect answers. the chain of thoughts (CoT) (Kojima et al., 2022). However, despite their advanced reasoning capabil- ities, LLMs are sometimes prone to generating in- correct reasoning steps. These inaccuracies can be attributed to the lack of current knowledge within their parameters or the erroneous retrieval of in- formation encoded in their weights (Maynez et al., 2020). In response to this issue, arguments for LLMs with knowledge from external data sources have emerged as a promising approach, attract- ing increased attention from researchers (Shi et al., 2023; Jiang et al., 2023; Trivedi et al., 2023). In some typical question-answering tasks, retrieval-augmented language models utilize a one- time retrieval method (Izacard et al., 2023; Lewis et al., 2020). However, these methods are not satisfied for multi-hop questions, necessitating a more nuanced approach to acquiring comprehen- sive knowledge. Such questions often involve in- direct facts that may exhibit minimal lexical or semantic correlation with the question but are es- 1 arXiv:2404.14464v1 [cs.CL] 22 Apr 2024 sential for reaching accurate answers. For instance, to answer the question,â€˜According to