thought reasoning, and their final response quality is constrained by the accuracy of retrieval and rea- soning at each step along the path. In contrast,TOR employs a tree structure to expand into different paths, sharing the risk of retrieval and reasoning failures. 2) The TOR structure can effectively re- duce the interference of useless information. IR- CoT, ITER-RETGEN, and CoR utilize all retrieved paragraphs during the reasoning process, and the useless information contained therein may lead to reasoning errors. We reduce the impact of useless information on retrieval and reasoning along the path by two pruning strategies. 3) TOR enhances the generation quality by improving the quality of retrieval results. Combining the results from Ta- ble 1 and Table 2, we find that retrieval metrics are positively correlated with generation metrics. Therefore, our method improves the final gener- ation quality by enhancing the systemâ€™s retrieval performance. Although we adopted the same prompts and ex- perimental settings as in the baseline papers, the results of some baselines on GPT-3.5-Turbo still do not perform well. We speculate that the main reason for this performance gap is the scale of the model parameter. According to the API call prices, GPT-3.5-Turbo costs $0.5/1M tokens for input and $1.5/1M tokens for output, and text-davinci-003 costs $20.0/1M tokens. Based on this, we can infer that the parameter scale of gpt-3.5-turbo is much smaller than that of text-davinci-003. The evidence fusion strategies can enhance the performance of the reader. As shown by Ta- ble 3, generating the final answer based on both retrieved paragraphs and analysis yields optimal performance, demonstrating the effectiveness of our search process. A significant gap exists be- tween performance derived from analysis and those from paragraphs, indicating that when there are Method HotpotQA 2WikiMQA MuSiQue Analysis 56.8 52.0 40.4 Paragraph 64.6 62.3 46.0