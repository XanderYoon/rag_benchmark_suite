the development sets of these datasets for retrieval and response generation per- formance evaluation in Table 1 and Table 2. Then, randomly selected 100 questions from the remain- ing part for hyperparameter tuning in Table 3 to Table 6. 4.2 Evaluate Setting We evaluatedTOR from retrieval quality and gener- ation quality. For the retrieval metric, we followed Trivedi et al. (2023), allowing different retrieval systems to return up to 15 paragraphs and calculat- ing the recall of golden paragraphs; this is referred to as recall@151. We used exact match (EM) and F1 score for the generation metric. 4.3 Baselines Given the disparity in retriever, reader, and test samples used by the baseline methods, a fair com- parison becomes challenging. Therefore, we fol- lowed Shao et al. (2023) used Contriver (Izacard et al., 2023) as our retriever. GPT-3.5-Turbo(gpt- 3.5-turbo-0125) and GPT-4-Turbo(gpt-4-1106- preview) (Ouyang et al., 2022; OpenAI, 2023) were used as the base models to implement the follow- ing baseline methods. The format of prompts and few-shot settings are adopted as presented in their papers. We retrieved top-5 paragraphs for each query, and for baselines involving multi-turn itera- tions, we set the maximum number of turns to 3. Direct Prompting (Brown et al., 2020) prompts a Language Language Model (LLM) to generate the final answer directly. CoT prompting (Wei et al., 2022)prompts a Lan- guage Language Model (LLM) to generate the final answer step by step. One-step Retrieval with Direct/ CoT prompt- ing (OneR-Direct/ CoT) augments Direct/CoT 1In this work if the number of retrieved paragraphs exceeds 15, we re-rank the evidence in the evidence pool based on the similarity between the evidence and the final response. We select the top 15 paragraphs according to their similarity scores. 5 Method GPT-3.5-Turbo GPT-4-Turbo HotpotQA 2WikiMQA MuSiQue HotpotQA 2WikiMQA MuSiQue EM F1 EM