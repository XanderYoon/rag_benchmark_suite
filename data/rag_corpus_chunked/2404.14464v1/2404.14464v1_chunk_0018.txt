only uses repetitive pruning. w/o, both donâ€™t use any pruning strategies. Depth width #API Rate #Doc #Evidence Recall@15 EM F1 2 5,3 10.3 10.0 97.1 1.8 69.7 44.3 55.6 3 5,3,3 16.9 15.7 92.9 2.9 79.3 51.6 63.7 4 5,3,3,3 36.3 27.6 76.0 4.7 79.7 51.8 64.4 3 10,5,3 41.3 39.2 94.9 6.8 75.4 52.4 66.0 Table 6: Results for different tree depths and widths on 2WikiMQA. a decrease in the Recall@15 metric. This is be- cause repetitive paragraphs do not provide infor- mation gain through node expansion, and the ob- tained evidence cannot offer additional effective paragraphs, potentially introducing invalid para- graphs that lower retrieval metrics. Relevance pruning filters out irrelevant para- graphs, reducing ineffective expansion. Without relevance pruning, the framework initiates node expansion for each paragraph. Although this ap- proach can retrieve more different paragraphs, the evidence obtained does not significantly increase, as the retrieval initiated by irrelevant paragraphs does not directly contribute to problem-solving. Additionally, introducing such misleading informa- tion may cause the model to generate erroneous reasoning, decreasing Recall@15, EM, and F1 met- rics. The depth and width of the tree affect the per- formance. As shown by Table 6, we conducted the experiment at different tree depths and widths and drew the following conclusions: 1) As the tree depth increases, our framework retrieves more paragraphs and obtains more evidence, leading to an improvement in retrieval and generation met- rics. However, the number of calls also increases non-linearly. This is because our framework gener- ates more feasible paths through node expansion. As this expansion grows exponentially with the in- crease in tree depth, we need to reasonably limit the depth of the tree to ensure search efficiency. 2) The effective call rate decreases with the deepening of the tree depth. Even though repetitive pruning reduces the