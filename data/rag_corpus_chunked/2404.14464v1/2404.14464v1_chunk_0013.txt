et al., 2023b; Press et al., 2023) iteratively execute the following steps : (i) Initiate retrieval using the follow-up question gen- erated by the LLM, returning relevant paragraphs, and (ii) Respond to the follow-up question, sub- sequently deciding whether to generate the next question or finalize the answer. The primary dis- tinction between ReAct and Self-Ask in our imple- mentation lies in the positioning of the retrieved paragraphs. ITER-RETGEN (Shao et al., 2023) iteratively ex- ecute the following steps for several turns:(i) Ini- tiate retrieval using the original question and re- sponse generated by the LLM, returning relevant paragraphs, and (ii) Answer the original question with the current turn retrieval paragraphs. Finally, take the last round’s response as an answer. IRCoT (Trivedi et al., 2023) iteratively execute the following steps: (i) Initiate retrieval using the CoT sentence generated by the LLM, returning relevant paragraphs, and (ii) Generate a new CoT sentence using historical information until a special trigger word is produced or the maximum number of turns is reached. Finally, use the historical retrieval para- graphs to answer the original question. 4.4 Implementation Details We also employed Contriver, GPT-3.5-Turbo, and GPT-4-Turbo for our experiments. We adopted a greedy decoding strategy to ensure the stability of the output. We set the maximum length to 4096 and added as much evidence from the pool without exceeding this limit. We randomly sampled sev- eral data from each dataset’s training set, manually annotated them for few-shot demonstrations, and adopted a 3-shot setting for all baselines and ours. In the main experiment, the depth of TOR is set to 3, with the number of nodes in each layer be- ing 5, 3, 3. We adopted the evidence-based fusion method, missing paragraph completion expansion strategy, and two kinds of pruning strategies. CoR: To compare the differences