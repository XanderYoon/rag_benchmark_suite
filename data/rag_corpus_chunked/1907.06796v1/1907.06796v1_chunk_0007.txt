ob- jects in the viewﬁnder, making them appear to be part of the real-world scene. Our key insight to enable this is to decou- ple the camera’s translation and rotation estimation, treating them instead as independent optimization problems. We employ our image-based region tracker to estimate translation and relative scale differences. The result mod- els the 3D translation of a tracked region w.r.t. the camera (using a simple pinhole camera model). Separately, the device’s 3D rotation (roll, pitch, and yaw) is retrieved from the built-in gyroscope. The local orienta- tion is calculated relative to a canonical global orientation, Figure 4: AR sticker effect: tracking a moving, non-planar region across a change in camera perspective. which we compute on initialization. Using the fused grav- ity vector from the accelerometer sensor, the “up” direction is observable, which yields a sensible initial orientation if we further assume initial object placement on a relatively horizontal surface. This ﬁnal assumption is purely based on how we imagine users will place virtual objects, and works well in practice. To make the effect more robust, we also allow for limited region tracking outside the camera’s ﬁeld of view–enabling a virtual object to reappear in approximately the same spot when panning away and back again. Combining visual in- formation for 3D translation and IMU data for 3D rota- tion lets us track and render virtual content correctly in the viewﬁnder with 6DoF, with the initial object scale being set by the user. With this parallelization, the system is fast and efﬁcient, can track moving or static regions, and further- more requires no calibration–it works on any device with a gyroscope. 4. Results and applications to AR In ﬁg. 3 we demonstrate planar target tracking results for a real-world image from two different perspectives. Note that the tracking is