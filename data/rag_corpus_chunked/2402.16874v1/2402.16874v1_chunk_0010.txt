our three retrieval methods alongside prompt augmentation for response generation. For the augmentation process, we selected Orca2 7b, a Small Language Model recognized for its outstanding performance compared to other Large Language Models. This choice was influenced by its impressive results combined with minimal GPU memory requirements, ensuring efficiency in our experimentation. As the source for retrieval, we first thought of using the Jeopardy dataset, we could have randomly selected the questions to measure our retriever capacity as well as our generator ability to understand the given context and generates answers, but we realized that the Jeopardy dataset only provides short answer, not a sentence, therefore if we wanted to efficiently measure our system we should have change the answer structure into full sentence (example: for the question “1912 Olympian; football star at Carlisle Indian School; 6 MLB seasons with the Reds, Giants & Braves”, the answer should be “Jim Thorpe is a 1912 Olympian, a football star at Carlisle Indian School, he played 6 MLB seasons with the Reds, Giants & Braves” and not just only “Jim Thorpe”). Therefore, we decided to only use a book, “The Cognitive Neuropsychology of Schizophrenia (Classic Edition)” by 12 Frith, C. D. edition 2015. Later to test the efficiency of the information gain made by our system, we implemented another article dealing on the prevalence of alcohol use for persons having schizophrenia, wrote by Koskinen et al. [16] and an article explaining through experiment on diverse patients that paranoia, a symptom of schizophrenia, is the product of grandiosity and guilt, and can also be associated with bipolarity, written by Lake C. [17]. Employing resources like those underscores our system's capability to deliver fine-tuned results, by using RAG alone. Regarding our evaluation metric and the intricacy of assessing Large Language Models (LLM) answers –