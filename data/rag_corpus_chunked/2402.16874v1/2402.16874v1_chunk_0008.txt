similarity between the query vector and all document vectors. • BERT: time complexity o (n * e + n * d2) when using dimension reduction o Where n is the number of documents, e is the time complexity of encoding a single document with BERT, d is the number of dimensions in the UMAP space, it scales quadratically with the amount of dimension, and we decided to only use 2 here. The time complexity for BERT encoding is o (n⋅e), and for UMAP, it's approximately o (n⋅d). o If we weren’t using dimension reduction, then the complexity would be the following: o (n⋅l⋅h⋅h′+n⋅n⋅h′). 10 o Where l is the maximum sequence length, h is the number of attention heads, and H' is the hidden size. This complexity considers the encoding of each document o(n⋅l⋅h⋅h′) and the pairwise similarity calculation o(n⋅n⋅h′). We highlighted the different time complexities for the retrieval each method, but there is to consider the precision to evaluate its efficiency, as well as the training time. While some documents are fast to encode, like TF-IDF, some takes longer time, which is the case for BERT, that ended being 44 times slower. In the context of our research, we implemented a strategic approach to enhance document retrieval. When using PDF, we broke it down into sentences, focusing on those longer than 15 characters. During the retrieval process, we deliberately selected not only the targeted sentence (n) but also the preceding (n-1) and succeeding (n+1) sentences. This decision assumed that relevant information and contextual clues are often dispersed before and after the identified sentence, contributing to a more comprehensive understanding of the content. 4.4 Generator In our framework, the generator functions as an answer generation tool centered around documents, powered by a language model generator. When presented with a user