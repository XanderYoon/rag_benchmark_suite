to retrieve the desired information with similar data rather than through a question or question/answer training. In the context of augmented generation for our research, query augmentation is not limited to providing the right answers; it's about designing queries that guide the AI model to understand and reflect the semantic nuances of the user's demand, ultimately leading to more relevant and contextually appropriate document retrieval. 4 By enhancing the initial steps of the retrieval process through augmented queries, we enable AI models to sift through extensive data repositories with greater accuracy and relevance. This approach not only elevates the efficiency of document retrieval systems but also enriches the user experience by ensuring that the information retrieved closely matches their search intent. 2. Related works Natural Language Processing (NLP) has witnessed significant advancements with the emergence of LLMs. Despite their prowess in downstream tasks, these models face challenges in accessing and manipulating knowledge, leading to suboptimal performance on knowledge-intensive tasks. A groundbreaking approach to address these limitations is Retrieval-Augmented Generation (RAG). In Lewis et al.'s seminal work (2020) [2], the authors introduce RAG models that combine pre-trained parametric and non-parametric memory for language generation. By leveraging a pre-trained seq2seq model as parametric memory and a dense vector index of Wikipedia as non-parametric memory, RAG achieves superior performance on a spectrum of knowledge-intensive NLP tasks. It outshines parametric seq2seq models and task-specific architectures, setting new benchmarks in open-domain question answering (QA) tasks. Furthermore, RAG models exhibit enhanced language generation capabilities, producing more specific, diverse, and factual language compared to state-of-the-art parametric-only seq2seq baselines. In the realm of information retrieval (IR), recent research has made remarkable strides. Hambarde and Proenca (2023) [7] provide an extensive overview of IR models, discussing the state-of-the-art methods, including those based on terms, semantic retrieval, and neural approaches.