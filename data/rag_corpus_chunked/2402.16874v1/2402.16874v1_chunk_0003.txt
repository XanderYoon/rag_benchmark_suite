Furthermore, RAG models exhibit enhanced language generation capabilities, producing more specific, diverse, and factual language compared to state-of-the-art parametric-only seq2seq baselines. In the realm of information retrieval (IR), recent research has made remarkable strides. Hambarde and Proenca (2023) [7] provide an extensive overview of IR models, discussing the state-of-the-art methods, including those based on terms, semantic retrieval, and neural approaches. Karpukhin et al. (2020) [8] revolutionize open-domain QA by introducing dense passage retrieval, demonstrating its practical implementation using only dense representations. 5 This approach outperforms traditional sparse vector space models, leading to state-of-the-art performance on multiple open-domain QA benchmarks. Qu et al. (2020) [9] further optimize dense passage retrieval with RocketQA, addressing challenges such as training-inference discrepancy and limited training data. RocketQA significantly outperforms previous state-of-the-art models on well-established datasets, showcasing its effectiveness in improving end-to-end QA systems. Advancements in similarity search, as demonstrated by Johnson et al.'s (2019) [10], their work, optimizing k-selection and addressing memory hierarchy challenges, sets new benchmarks in various similarity search scenarios. The ability to construct a high-accuracy k-NN graph on a massive dataset underscores the ongoing effort to enhance scalability and efficiency in information retrieval systems, this system is better known as FAISS. The evolution of text representation methods has been pivotal in advancing NLP capabilities. Le and Mikolov's Doc2Vec algorithm (2014) [11] overcomes the limitations of bag-of-words models by learning distributed representations of sentences and documents. This approach outperforms traditional models in text classification and sentiment analysis tasks, achieving new state-of-the-art results. Devlin et al.'s BERT (2018) introduces bidirectional pre-training, empowering the model to achieve state-of-the-art results across various NLP tasks. These methods lay the foundation for effective text representation, enabling models to capture complex semantic relationships within language. Advancements in query augmentation strategies play a crucial role in improving information retrieval effectiveness.