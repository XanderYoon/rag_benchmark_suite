RAG & query augmentation 3.6 3.5 3.8 Table 1. Score based on retrieval methods Conclusion After carefully checking Tab 1. and the extra tables in Annex A, a clear improvement in the initial language model's performance is noticeable. RAG contributes to refining its capabilities, similar to fine-tuning. The prompt augmenter plays a crucial role in delivering factual responses, especially evident in the query "Can animals have schizophrenia," where the prompt augmenter stands out as the only method providing the correct answer. The retriever capabilities showed consistency despite using different encodings, confirming our intuition about retrieving similar documents through those generated by our Language Model. The use of UMAP for BERT proves to be a successful strategy, simplifying retrieval complexity while maintaining consistent and reliable results, marking another achievement in our research. By incorporating other resources, we aimed to fine-tune the results obtained solely through RAG. The efficiency of our system was further highlighted by introducing another article on the prevalence of alcohol use disorders in schizophrenia by Koskinen et al. The utilization of these additional articles showcased the capacity of our system to yield refined and information-enriched outcomes, affirming the significance of augmenting retrieval with well- 15 curated and relevant documents. Those incorporations showed limited success; it also led to misunderstanding when the query lacks precision, it also exponentially increases the BERT retrieving time, which was expected when looking at the time complexity equations. Overall, the performance difference with each system highlights their known strengths and weaknesses. BERT shows the best results, but the computation time is the highest, making it difficult to apply on a large-scale system. TF-IDF stands here as the best ratio performance/cost, while being the fastest it is also the one that benefit the most from our augmented query, being more reliable than Doc2Vec, due to