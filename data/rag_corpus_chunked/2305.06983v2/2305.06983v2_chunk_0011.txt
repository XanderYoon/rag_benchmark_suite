answer is “a law degree” What university did Joe Biden attend?What degree did Joe Biden earn? implicit query by maskingexplicit query by question generationJoe Biden attended , where he earned . LM such as ChatGPT Figure 3: Implicit and explicit query formulation. To- kens with low probabilities are marked with underlines. ing information. We propose two simple methods to overcome this issue as illustrated in Figure 3. Masked sentences as implicit queries. The first method masks out low-confidence tokens inˆst with probabilities below a threshold β ∈ [0, 1], where a higher β results in more aggressive masking. This removes potential distractions from the sentence to improve retrieval accuracy. Generated questions as explicit queries. An- other method is to generate explicit questions that target the low-confident span in ˆst. For example, if the LM is uncertain about “the University of Penn- sylvania”, a question like “Which university did Joe Biden attend?” can help retrieve relevant in- formation. Self-ask (Press et al., 2022) achieved this by manually inserting follow-up questions into downstream task exemplars as shown later in Prompt D.2, which requires task-specific annota- tion efforts. Instead, we developed a universal ap- proach that generates questions for low-confidence spans without additional annotation. Specifically, We first extract all spans from ˆst with probabilities below β. For each extracted span z, we prompt gpt-3.5-turbo to generate a question qt,z that can be answered with the span: Prompt 3.2: zero-shot question generation User input x. Generated output so far y≤t. Given the above passage, ask a question to which the answer is the term/entity/phrase “z”. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on ˆst as follows: qt = ( ∅ if all tokens of