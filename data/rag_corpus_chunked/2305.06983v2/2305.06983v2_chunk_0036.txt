which are longer than most sen- tences, and use NLTK sentence tokenizer 5 to ex- tract the first sentence and discard the rest. Efficiency As shown in subsection 6.2, on aver- age retrieval is triggered for 30% ∼ 60% of sen- tences depending on downstream tasks. In compar- ision, KNN-LM (Khandelwal et al., 2020) retrieves every token, RETRO or IC-RALM (Borgeaud et al., 2022; Ram et al., 2023) retrievers every 4∼32 to- kens, and IRCoT (Trivedi et al., 2022) retrieves every sentence. Compared to single-time retrieval, however, interleaving retrieval and generation with a naive implementation indeed increases overheads, which we discuss in the limitation section (sec- tion 9). B Datasets and Settings Datasets, metrics, and experimental settings are summarized in Table 7. 5https://www.nltk.org/api/nltk.tokenize. PunktSentenceTokenizer.html Multihop QA For “Why did the founder of Ver- sus die?”, the output we aim to generate is “The founder of Versus was Gianni Versace. Gianni Ver- sace was shot and killed on the steps of his Miami Beach mansion on July 15, 1997. So the answer is shot.” We use 8 exemplars from Trivedi et al. (2022) listed in Prompt D.4 for in-context learn- ing, BM25 as the retriever, and Wikipedia articles as the retrieval corpus. Similar to the observation in Trivedi et al. (2022), we found incorporating retrieval results for exemplars improves the per- formance, we use the input x of each exemplar to retrieve several documents and then add them using the format in Prompt D.1. We found increasing the number of retrieval documents often increases per- formance. Therefore, we use the maximum number of documents that can fit within the input length limit of text-davinci-003, which is 2 for 2Wiki- MultihopQA. Commonsense Reasoning For “Would a pear sink in water?”, the output we aim to generate is “The density of a pear