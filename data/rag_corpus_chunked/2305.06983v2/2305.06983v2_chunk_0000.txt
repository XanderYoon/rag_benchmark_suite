Active Retrieval Augmented Generation Zhengbao Jiang1∗ Frank F. Xu1∗ Luyu Gao1∗ Zhiqing Sun1∗ Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend