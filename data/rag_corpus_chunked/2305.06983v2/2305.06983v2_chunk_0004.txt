et al., 2022). We ask the following question: can we create a simple and generic retrieval augmented LM thatac- tively decides when and what to retrievethroughout the generation process, and are applicable to a va- riety of long-form generation tasks? We provide a generalized view of active retrieval augmented gen- eration. Our hypothesis regarding when to retrieve is that LMs should retrieve information only when they lack the required knowledge to avoid unneces- sary or inappropriate retrieval that occurs in passive retrieval augmented LMs (Khandelwal et al., 2020; Borgeaud et al., 2022; Ram et al., 2023; Trivedi et al., 2022). Given the observation that large LMs tend to be well-calibrated and low probability/con- fidence often indicates a lack of knowledge (Ka- davath et al., 2022), we adopt an active retrieval strategy that only retrieves when LMs generate low- probability tokens. When deciding what to retrieve, it is important to consider what LMs intend to gen- erate in the future, as the goal of active retrieval is to benefit future generations. Therefore, we propose anticipating the future by generating a temporary next sentence, using it as a query to retrieve rel- evant documents, and then regenerating the next sentence conditioning on the retrieved documents. Combining the two aspects, we propose Forward- Looking Active REtrieval augmented generation (FLARE), as illustrated in Figure 1. FLARE iter- atively generates a temporary next sentence, use it as the query to retrieve relevant documents if it contains low-probability tokens and regenerate the next sentence until reaches the end. FLARE is applicable to any existing LMs at inference time without additional training. Con- sidering the impressive performance achieved by GPT-3.5 (Ouyang et al., 2022) on a variety of tasks, we examine the effectiveness of our meth- ods on text-davinci-003. We evaluate FLARE on 4 diverse tasks/datasets involving