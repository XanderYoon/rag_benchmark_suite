gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable. There- fore, we propose a more direct way of forward- looking active retrieval that uses the next sentence to decide when and what to retrieve. 3.2.1 Confidence-based Active Retrieval As shown in Figure 1, at step t, we first generate a temporary next sentence ˆst = LM([x, y<t]) with- out conditioning on retrieved documents. Then we decide whether to trigger retrieval and formulate queries based on ˆst. If the LM is confident aboutˆst, we accept it without retrieving additional informa- tion; if not, we use ˆst to formulate search queries qt to retrieve relevant documents, and then regen- erate the next sentence st. The reason we utilize sentences as the basis of our iteration is due to their significance as semantic units that are neither too short nor too lengthy like phrases and paragraphs. However, our approach can also utilize phrases or paragraphs as the basis. Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022), we actively trigger retrieval if any token of ˆst has a probability lower than a threshold θ ∈ [0, 1]. θ = 0 means retrieval is never triggered, while θ = 1 triggers retrieval every sentence. yt = ( ˆst if all tokens of ˆst have probs ≥ θ st = LM([Dqt, x, y<t]) otherwise where the query qt is