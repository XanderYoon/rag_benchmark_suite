as queries. As shown in Table 3, using the next sentence to retrieve is clearly better than using the previous sentence, confirming our hypothesis. We also run previous-window approaches using different numbers of past tokens as queries. As shown in Table 4, using too many tokens (> 32) in %steps/sentences with retrieval 0.0 20.0 40.0 60.0 80.0 0.0 25.0 50.0 75.0 100.0 2WikiMultihopQA StrategyQA Figure 5: Performance (EM) of FLARE with respect to the percentage of steps/sentences with retrieval on 2WikiMultihopQA and StrategyQA. the past hurts the performance, further confirming our hypothesis that previous context might not be relevant to intent of future generations. Importance of active retrieval. Next, we inves- tigate how active retrieval threshold θ affects per- formance. To alter our method from not retrieving to retrieving every sentence, we adjust the confi- dence threshold θ that determines when to trigger retrieval from 0 to 1. We then calculate the pro- portion of steps/sentences where retrieval is acti- vated, and present the performance based on it. As shown in Figure 5, on 2WikiMultihopQA, the per- formance plateaus when the retrieval percentage exceeds 60%, indicating that retrieval when LMs are confident is not necessary. On StrategyQA, the performance drops when the retrieval percentage exceeds 50%, indicating that unnecessary retrieval can introduce noise and impede the original gen- eration process. We found triggering retrieval for 40%-80% of sentences usually leads to a good per- formance across tasks/datasets. Effectiveness of different query formulation methods We study implicit query formation by masking and explicit query formulation through question generation. In Table 5, we compare the performance of FLARE with different masking β EM F 1 Prec. Rec. 0.0 0.488 0.576 0.571 0.605 0.2 0.498 0.588 0.582 0.616 0.4 0.510 0.597 0.591 0.627 0.6 0.506 0.593 0.586 0.622 Table 5: Performance of FLARE