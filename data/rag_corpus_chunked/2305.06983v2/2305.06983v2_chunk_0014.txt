queries might not reflect what LMs intend to generate in the future. (2) Retriev- ing information at a fixed interval can be inefficient because it might occur at inappropriate points. (3) Question decomposition approaches require task- specific prompt engineering, which restricts their generalizability in new tasks. 5 Experimental Setup We evaluate the effectiveness of FLARE on 4 di- verse knowledge-intensive tasks using few-shot in- context learning (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023). We follow previous works (Trivedi et al., 2022) to sub-sample at most 500 examples from each dataset due to the cost of run- ning experiments. Datasets, metrics, and settings are summarized in Table 7 of Appendix B. The hyperparameters of FLARE are selected based on the development set and listed in Table 9. FLARE refers to FLAREdirect if not specifically stated. Multihop QA The goal of multihop QA is to answer complex questions through information re- trieval and reasoning. We use 2WikiMultihopQA (Ho et al., 2020) which contains 2-hop complex 4Since KNN-LM uses the contextualized representation corresponding to the current decoding position to retrieve rel- evant information which encodes all previous tokens. Strictly speaking, qt should be y<t. questions sourced from Wikipedia articles that re- quire composition, comparison, or inference, e.g., “Why did the founder of Versus die?” We follow Wang et al. (2022) to generate both the chain-of- thought and the final answer. Experimental setting details are included in Appendix B. We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g.,