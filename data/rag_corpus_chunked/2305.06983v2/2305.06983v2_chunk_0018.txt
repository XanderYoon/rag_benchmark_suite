et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets StrategyQA ASQA ASQA-hint WikiAsp Metrics EM EM D-F 1 R-L DR EM D-F 1 R-L DR UniEval E-F 1 R-L No retrieval 72.9 33.8 24.2 33.3 28.4 40.1 32.5 36.4 34.4 47.1 14.1 26.4 Single-time retrieval 68.6 40.0 27.1 34.0 30.4 43.2 34.8 37.4 36.0 52.4 17.4 26.9 Multi-time retrieval Previous-window 71.2 39.9 27.0 34.3 30.4 43.7 35.7 37.5 36.6 51.8 18.1 27.3 Previous-sentence 71.0 39.9 27.9 34.3 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2 FLARE (ours) 77.3 41.3 28.2 34.3 31.1 46.2 36.7 37.7 37.2 53.4 18.9 27.6 Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1. 2WikiMultihopQA ASQA-hint EM