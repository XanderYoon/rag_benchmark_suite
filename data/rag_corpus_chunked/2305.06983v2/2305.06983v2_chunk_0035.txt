United Arab Emirates, December 7-11, 2022, pages 2023–2038. Association for Computa- tional Linguistics. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvininejad. 2021. Detecting hallucinated content in conditional neural sequence generation. In Find- ings of the Association for Computational Linguis- tics: ACL-IJCNLP 2021, pages 1393–1404, Online. Association for Computational Linguistics. A FLARE Implementation Details FLAREinstruct implementation details We found that LMs can effectively combine retrieval and downstream task-related skills and generate meaningful search queries while performing the task. However, there are two issues: (1) LMs tend to generate fewer search queries than necessary. (2) Generating excessive search queries can disrupt answer generation and adversely affect performance. We address these issues using two methods respectively. First, we increase the logit of the token “[” by 2.0 to improve the chances of LMs generating “[Search(query)]”. Second, whenever LMs generate a search query, we use it to retrieve relevant information, promptly remove it from the generation, and generate the next few tokens while forbidding “[” by adding a large negative value to the logit of “[”. The initial query of FLARE. FLARE starts with the user input x as the initial query to re- trieve documents to generate the first sentence ˆs1 = LM([Dx, x]) to bootstrap the iterative gener- ation process. For the following steps, the tempo- rary forward-looking sentence is generated without retrieved documents. Sentence tokenization. For each step t, we gen- erate 64 tokens which are longer than most sen- tences, and use NLTK sentence tokenizer 5 to ex- tract the first sentence and discard the rest. Efficiency As shown in subsection 6.2, on aver- age retrieval is triggered for 30% ∼ 60% of sen- tences depending on downstream tasks. In compar- ision, KNN-LM (Khandelwal et al., 2020) retrieves every token, RETRO or IC-RALM