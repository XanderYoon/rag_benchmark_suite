x. Generated output so far y≤t. Given the above passage, ask a question to which the answer is the term/entity/phrase “z”. We retrieve using each generated question and interleave the returned documents into a single ranking list to aid future generations. In summary, queries qt are formulated based on ˆst as follows: qt = ( ∅ if all tokens of ˆst have probs ≥ θ mask(ˆst) or qgen(ˆst) otherwise 3.3 Implementation Details Base LM We validate our method on one of the most advanced GPT-3.5 LMs text-davinci-003 by iteratively querying their API.2 Document corpus and retrievers. Since we fo- cus on the integration of retrieval and generation, we use off-the-shelf retrievers that take queries as inputs and return a list of relevant documents. For datasets that mainly rely on knowledge from Wikipedia, we use the Wikipedia dump from Karpukhin et al. (2020) and employ BM25 (Robert- son and Zaragoza, 2009) as the retriever. For datasets that rely on knowledge from the open web, we use the Bing search engine as our retriever.3 Retrieved document formatting. Multiple re- trieved documents are linearized according to their ranking and then added to the beginning of the user input using Prompt D.1. Other implementation details such as sentence to- kenization and efficiency are included Appendix A. 4 Multi-time Retrieval Baselines Existing passive multi-time retrieval augmented LMs can also be formulated using our framework (subsection 2.3). In this section, we formally in- troduce three baseline categories based on when and what to retrieve. These baselines are not exact reproductions of the corresponding paper because many design choices differ which makes direct comparisons impossible. We implemented them using the same settings, with the only variation being when and what to retrieve. Previous-window approaches trigger retrieval every l tokens, where l represents the window size. Generated tokens