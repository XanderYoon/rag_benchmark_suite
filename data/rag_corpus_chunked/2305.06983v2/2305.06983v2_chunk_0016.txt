to.” Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion from open web (Giorgi et al., 2022). We use WikiAsp (Hayashi et al., 2021) which aims to gen- erate aspect-based summaries about entities from 20 domains in Wikipedia, e.g., “Generate a sum- mary about Echo School (Oregon) including the 0.0 20.0 40.0 60.0 80.0 2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp No ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE) Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp. following aspects: academics, history.” Experimen- tal setting details are included in Appendix B. Metrics include ROUGE, named entity-based F1, and UniEval (Zhong et al., 2022) which measures factual consistency. 6 Experimental Results We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task’s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it