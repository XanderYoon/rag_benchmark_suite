We use regular expressions to extract the final answer from the output and compare it with the ref- erence answer using exact match (EM), and token- level F1, precision, and recall. Commonsense reasoning Commonsense reason- ing requires world and commonsense knowledge to generate answers. We use StrategyQA (Geva et al., 2021) which is a collection of crowdsourced yes/no questions, e.g., “Would a pear sink in wa- ter?” We follow Wei et al. (2022) to generate both the chain-of-thought and the final yes/no answer. Details are included in Appendix B. We extract the final answer and match it against the gold answer using exact match. Long-form QA Long-form QA aims to generate comprehensive answers to questions seeking com- plex information (Fan et al., 2019; Stelmakh et al., 2022). We use ASQA (Stelmakh et al., 2022) as our testbed where inputs are ambiguous questions with multiple interpretations, and outputs should cover all of them. For example, “Where do the Philadel- phia Eagles play their home games?” could be asking about the city, sports complex, or stadium. We found in many cases it is challenging even for humans to identify which aspect of the question is ambiguous. Therefore, we created another set- ting (ASQA-hint) where we provide a brief hint to guide LMs to stay on track when generating an- swers. The hint for the above case is “This question is ambiguous in terms of which specific location or venue is being referred to.” Experimental setting details are included in Appendix B. We use metrics from Stelmakh et al. (2022), in- cluding EM, RoBERTa-based QA score (Disambig- F1), ROUGE (Lin, 2004), and an overall score com- bining Disambig-F1 and ROUGE (DR). Open-domain summarization The goal of open- domain summarization is to generate a comprehen- sive summary about a topic by gathering informa- tion