granularity of generation and retrieval strategies. Adaptive retrieval has been studied in single-time retrieval scenarios based on either question pop- ularity or generation probabilities (Mallen et al., 2022; Li et al., 2023), while we focus on long-form generation requiring active information access. Browser-enhanced LMs WebGPT (Nakano et al., 2021) and WebCPM (Qin et al., 2023) train LMs to interact with browser to enhance factuality using reinforcement learning or supervised train- ing where multiple queries can be triggered before generation. FLARE is built on text-based retrievers but can be combined with a browser to potentially improve retrieval quality. 8 Conclusion To aid long-form generation with retrieval aug- mentation, we propose an active retrieval aug- mented generation framework that decides when and what to retrieve during generation. We imple- ment this framework with forward-looking active retrieval that iteratively uses the upcoming sentence to retrieve relevant information if it contains low- confidence tokens and regenerates the next sen- tence. Experimental results on 4 tasks/datasets demonstrate the effectiveness of our methods. Fu- ture directions include better strategies for active retrieval and developing efficient LM architectures for active information integration. 9 Limitations We also conduct experiments on Wizard of Wikipedia (Dinan et al., 2019) and ELI5 (Fan et al., 2019), and found that FLARE did not provide sig- nificant gains. Wizard of Wikipedia is a knowledge- intensive dialogue generation dataset where the out- put is relatively short (âˆ¼20 tokens on average) so retrieving multiple disparate pieces of information might not be necessary. ELI5 (Fan et al., 2019) is a long-form QA dataset requiring in-depth an- swers to open-ended questions. Due to issues men- tioned in Krishna et al. (2021) such as difficulties of grounding generation in retrieval and evalua- tion, both single-time retrieval and FLARE did not provide significant gains over not using retrieval. From