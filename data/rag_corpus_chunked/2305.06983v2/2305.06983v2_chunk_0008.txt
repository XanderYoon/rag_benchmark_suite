...” When working with GPT-3.5 models that Search results: !![1]: …[2]: … Joe Biden attended Search results: !"![1]: …[2]: …Search results: !""[1]: …[2]: … [Search(Joe Biden University)] [Search(Joe Biden degree)]the University of Pennsylvania, where he earned a law degree. Generate a summary about Joe Biden.Input$ &$ &#%$ &%%%Generation Retriever $ %$ %% Figure 2: An illustration of forward-looking active re- trieval augmented generation with retrieval instructions (FLAREinstruct). It iteratively generates search queries (shown in gray italic) to retrieve relevant information to aid future generations. offer only API access, we elicit such behavior by few-shot prompting (Brown et al., 2020). Specifically, for a downstream task, we place the search-related instruction and exemplars at the beginning as skill 1, followed by the instruction and exemplars of the downstream task as skill 2. Given a test case, we ask LMs to combine skills 1 and 2 to generate search queries while performing the task. The structure of the prompt is shown in Prompt 3.1, and full details can be found in Prompt D.3. Prompt 3.1: retrieval instructions Skill 1. An instruction to guide LMs to generate search queries. Several search-related exemplars. Skill 2. An instruction to guide LMs to perform a specific downstream task (e.g., multihop QA). Several task-related exemplars. An instruction to guide LMs to combine skills 1 and 2 for the test case. The input of the test case. As shown in Figure 2, when the LM generates “[Search(query)]” (shown in gray italic), we stop the generation and use the query terms to retrieve relevant documents, which are prepended before the user input to aid future generation until the next search query is generated or reaches the end. Additional implementation details are included in Appendix A. 3.2 Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct