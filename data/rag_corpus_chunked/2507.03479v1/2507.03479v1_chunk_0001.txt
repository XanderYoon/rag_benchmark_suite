55], where decisions based on inaccurate or misleading information can have serious consequences. For instance, if doctors rely on incorrect agent responses to epilepsy [27] or cancer-related [37] queries, they may make harmful treatment decisions [22]. Similarly, businesses that base their strategies on misleading information risk significant financial losses [12]. Therefore, building trust in these systems is essential. To this end, researchers argue that the search results underlying the agents’ answers must be made more transparent, interpretable, and ultimately explainable [4, 11]. To achieve this, research on explainable AI (XAI) and explain- able information retrieval (XIR) has introduced methods to help users interpret model decisions [2, 43]. XAI techniques aim to pro- vide insights into the behaviour of models [ 38]. Local XAI tech- niques explain model decisions given specific input instances to users. Famous examples include feature attribution methods such as LIME [35] or SHAP [31] or counterfactual explanations [44, 47]. Similarly, XIR research focuses on making search systems more transparent and interpretable [2, 39]. This includes explaining how the system interprets user queries [3, 46, 54], developing evalua- tion metrics to assess explainability [7, 20], and providing expla- nations for search results [ 6, 28–30]. Methods for search result explanations range from explaining the ranking of retrieved evi- dences [6, 29, 34, 52] to providing additional information on the retrieved sources to enhance transparency and build trust in the sys- tem [28]. Explanations can take the form of visualisations or textual descriptions [1, 28] offering users varying levels of detail [1, 33, 51]. Most existing research focuses on domain-agnostic use cases. Search system explainability in specific domains, however, remains underexplored. While some studies have examined explainable IR in healthcare and medical applications [22], other critical fields, such as finance, law, and audit, have received little attention. Studying domain-specific