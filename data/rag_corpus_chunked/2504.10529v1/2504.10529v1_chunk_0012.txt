46.30% 73.00% 70.01% 69.00% 67.62% HeteRAG78.00% 76.60% 86.00% 75.33% 87.00% 77.30% 82.00% 75.97% E5 Naive 67.00% 57.03% 63.00% 54.75% 58.00% 51.62% 55.00% 51.66% Late 57.00% 46.50% 61.00% 34.16% 59.00% 49.70% 60.00% 51.28% HeteRAG69.00% 63.23% 68.00% 61.99% 56.00% 57.32% 55.00% 54.96% MedEmb Naive 57.00% 60.77% 67.00% 65.51% 67.00% 67.03% 75.00% 72.14% Late 73.00% 66.19% 75.00% 46.41% 72.00% 65.37% 76.00% 67.32% HeteRAG79.00% 74.58% 81.00% 76.17% 87.00% 79.47% 83.00% 78.81% Table 1: Evaluation of different chunk representation methods on retrieval tasks. HeteRAG significantly improves retrieval accuracy in the majority of settings. 2019) is a collection of real user queries paired with Wikipedia passages. SQuAD (Rajpurkar et al., 2018) is a widely-used benchmark dataset for machine comprehension, consisting of questions on a set of Wikipedia articles. TriviaQA (Joshi et al., 2017) contains 95K trivia-based QA pairs, while HotpotQA (Yang et al., 2018) offers 113K Wikipedia QA pairs for multi-hop reasoning chal- lenges. We used three state-of-the-art open-source LLMs as generative models: Llama3-8b-Instruct (Dubey et al., 2024), Mistral-8B-Instruct (Jiang et al., 2024), and Gemma-9b-Instruct (Team et al., 2024). The end-to-end RAG code implementation refers to Jin et al. (2024). Next, we introduce the experimental settings. For retrieval experiments, we use commonly used ranking metrics ndcg@1 and ndcg@10. For the adaptive tuning process, we conducted fine-tuning using the training partition of the SciFact dataset, followed by performance evaluation on the desig- nated test partition. For generation experiments, we use commonly used metrics in QA systems, namely EM (Exact Match) and token-level F1. The token-level F1 metric refers to the harmonic mean of token-level precision and recall, calculated by comparing shared tokens between the response and golden answer. In the retrieval corpus, we choose the widely-used Wiki2018 corpus, which is com- patible with the five QA datasets used in the experi- ment. To streamline the experiments, we