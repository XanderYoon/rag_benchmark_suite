con- tributing complementary perspectives for robust retrieval. For a knowledge chunk C(j) i in document Dj, We formulate the modeling procedure at the 3 retrieval side as follows: e(j) i = R h ψ(C(j) i ) ⊕ψctx({C(j) ictx}) ⊕ψmeta(M(j) i ) i (2) where M (j) i represents the global metadata of C(j) i in Dj, including but not limited to sub- ject, abstract, document title, section title, subsec- tion title, related keywords, etc. And {C(j) ictx} = {C(j) t , C(j) s , {C(j) i±k}} represents the multi-granular contextual signals, which can provide the retrieval model with contextual information at different lev- els. ψ(·), ψctx(·), and ψmeta(·) are the semantic encoders for different components, and ⊕ denotes the fusion operation. The generation side aims to keep the representa- tion of the knowledge chunk as concise as possible for the sake of efficiency and precision, avoiding re- dundant or unnecessary information. Therefore, in contrast to the retrieval side, we only provide C(j) i itself to the generative model on the generation side to maintain task-specific precision: Ans = G  T (Q, C(j) i )  (3) Where T (·, ·) refers to the prompt template accus- tomed to the generative language model G. In this way, the representations are decoupled between retrieval and generation. 3.3 Adaptive Prompt tuning Strategy In many cases where RAG systems are applied to specific domains, the retrieval embedding model is fine-tuned to adapt to the corresponding domain. To specialize the retrieval model for heterogeneous document structures, we introduce an adaptive fine- tune strategy. Prompt tuning (Lester et al., 2021) has been widely adopted in various fields, as it uses task- specific instructions to improve performance on targeted tasks. To enable the retrieval model to better leverage contextual signals and structured metadata, as well as