62 68 74 chunk chunk + contextual signals chunk + metadata HeteRAG Figure 3: Effect of contextual signals and structured metadata in HeteRAG framework. The ablation results show that both contribute significantly to the retrieval performance of HeteRAG. implement contrastive learning-based fine-tuning for fair comparison on baseline methods follow- ing the same protocol. Fine-tuned variants consis- tently outperform their non-fine-tuned counterparts across all datasets. When trained with identical op- timization steps, HeteRAG achieves superior per- formance compared to fine-tuned baseline meth- ods, confirming the benefits of our proposed fine- tuning strategy. These findings demonstrate that HeteRAG maintains compatibility with standard embedding model fine-tuning strategies, exhibiting strong adaptation capabilities. 4.4 End-to-End RAG Performance The experimental results of our end-to-end RAG framework, as shown in Table 3, demonstrate con- sistent performance improvements across three generative language models (Llama3-8b-Instruct, Mistral-8B-Instruct, and Gemma-9b-Instruct) and five benchmark datasets (NQ, PopQA, SQuAD, TriviaQA, and HotpotQA). The table presents the results of retrieval top-5 knowledge chunks from the Wiki corpus. HeteRAG significantly outper- forms other baseline methods across all models and datasets. These gains might be attributed to Wikipediaâ€™s inherent tree-like hierarchical struc- ture, which enables HeteRAG to holistically model document-level dependencies as metadata. 4.5 Ablation Study We evaluate the effectiveness of contextual signals and structured metadata through ablation studies by 7 1 3 5 10 15 T op K Value 25 30 35 40 45F1 Score (%) Naive RAG (Llama3-8b) HeteRAG (Llama3-8b) Naive RAG (Mistral-8b) HeteRAG (Mistral-8b) Naive RAG (Gemma-9b) HeteRAG (Gemma-9b) PopQA HotpootQA TriviaQA Squad NQ0 20 40 60F1 Score(%) T opK=1 T opK=3 T opK=5 T opK=10 T opK=15 Figure 4: The RAG results under varying retrieval numbers (top-k). The left side shows the results of three LLMs on the HotpotQA dataset as they vary with top-k, using both naive RAG and HeteRAG. The