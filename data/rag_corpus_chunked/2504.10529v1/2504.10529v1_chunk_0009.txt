the corresponding domain. To specialize the retrieval model for heterogeneous document structures, we introduce an adaptive fine- tune strategy. Prompt tuning (Lester et al., 2021) has been widely adopted in various fields, as it uses task- specific instructions to improve performance on targeted tasks. To enable the retrieval model to better leverage contextual signals and structured metadata, as well as to adapt to the characteris- tics of different corpora, we propose a fine-tuning strategy based on prompt tuning. Specifically, we prepend instructions to different information units of a certain chunk. For a chunk C(j) i with contex- tual signals {C(j) i±k} and global metadata M (j) i , we formulate the instruction input as: ˜Ch = [INSTh] ⊕ C (4) where [INSTh] denotes the instruction embedding specific to hierarchy level h, implemented as soft prompts through continuous token vectors. The retrieval model R then encodes both the original query Q and prompted chunks { ˜Ch} into an adap- tive embedding space: q = R(Q), ˜eh = R( ˜Ch) (5) Following the conventional paradigm of con- trastive learning, we construct positive and negative samples. Given a user query Q, the positive pair (Q, C+) is directly derived from human-annotated relevance data. For negative pairs (Q, C−), both in-batch negatives {C− j }j̸=iand random negatives C− rand are employed for training. Similarity between Q and C is measured by scaled cosine similarity: ϕ(Q, C) = q⊤˜eh ∥q∥∥˜eh∥ · τ −1 (6) where τ denotes the temperature hyperparameter controlling the softness of the similarity distribu- tion. The model is trained using an InfoNCE loss (Oord et al., 2018): L=−1 N NX i=1 log es(Qi,C+ i ) PN j=1es(Qi,C+ j ) +PK k=1es(Qi,C− k ) (7) 4 Experiments and Analysis 4.1 Experimental Datasets and Settings We utilize three information retrieval datasets for evaluation in