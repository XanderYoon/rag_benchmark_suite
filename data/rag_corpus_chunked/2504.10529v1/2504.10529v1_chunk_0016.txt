HeteRAG EM F1 Recall EM F1 Recall EM F1 Recall Llama3-8b PopQA 18.70% 22.96% 25.80% 24.00% 39.75% 58.66% 32.70% 52.25% 76.19% HotpotQA 19.70% 28.03% 28.06% 21.70% 30.56% 32.53% 30.80% 42.48% 43.32% TriviaQA 51.60% 58.94% 60.47% 52.40% 61.04% 63.65% 58.70% 68.56% 71.87% Squad 20.40% 27.09% 28.48% 28.90% 36.49% 40.11% 32.60% 40.34% 44.17% NQ 22.40% 32.61% 37.45% 29.80% 40.25% 47.01% 36.10% 48.24% 57.46% Mistral-8b PopQA 20.10% 22.51% 22.69% 32.70% 45.77% 58.94% 46.20% 61.40% 76.04% HotpotQA 18.60% 26.63% 26.30% 26.80% 37.21% 36.96% 36.60% 47.99% 47.91% TriviaQA 47.30% 53.90% 54.67% 55.40% 63.51% 64.87% 61.30% 69.53% 71.48% Squad 15.50% 21.75% 22.78% 33.30% 40.40% 42.52% 37.20% 44.24% 46.35% NQ 17.00% 24.68% 27.91% 33.00% 42.38% 47.22% 40.20% 51.54% 56.69% gemma-9b PopQA 15.00% 16.20% 16.40% 38.60% 48.16% 58.58% 52.00% 63.27% 75.51% HotpotQA 16.70% 24.39% 23.85% 25.10% 33.74% 33.07% 34.90% 45.40% 44.67% TriviaQA 52.40% 58.01% 58.09% 58.10% 64.79% 65.41% 63.60% 71.31% 72.22% Squad 16.30% 21.23% 22.03% 34.50% 39.63% 40.67% 37.90% 43.36% 44.61% NQ 21.60% 31.02% 32.70% 33.20% 42.72% 45.71% 39.80% 50.34% 54.22% Table 3: The performance evaluation of different methods on five datasets and three LLMs. Across all datasets and models, HeteRAG demonstrates higher QA accuracy on all evaluation metrics. Size=16 Size=32 Size=64 Size=12855 59 63 67nDCG@10 (SciFact) Jina Size=16 Size=32 Size=64 Size=12855 60 65 70 75 BGE Size=16 Size=32 Size=64 Size=12850 55 60 65 70 MedEmb Size=16 Size=32 Size=64 Size=12850 55 60 65 70nDCG@10 (TrecCOVID) Size=16 Size=32 Size=64 Size=12850 56 62 68 74 Size=16 Size=32 Size=64 Size=12850 56 62 68 74 chunk chunk + contextual signals chunk + metadata HeteRAG Figure 3: Effect of contextual signals and structured metadata in HeteRAG framework. The ablation results show that both contribute significantly to the retrieval performance of HeteRAG. implement contrastive learning-based fine-tuning for fair comparison on baseline methods follow- ing the same protocol. Fine-tuned variants consis- tently outperform their non-fine-tuned