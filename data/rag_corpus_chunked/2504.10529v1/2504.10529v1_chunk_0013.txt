EM (Exact Match) and token-level F1. The token-level F1 metric refers to the harmonic mean of token-level precision and recall, calculated by comparing shared tokens between the response and golden answer. In the retrieval corpus, we choose the widely-used Wiki2018 corpus, which is com- patible with the five QA datasets used in the experi- ment. To streamline the experiments, we select the first 1,000 samples from the test or development set of all QA datasets. For vector database index 5 Dataset Emb Method chunk size=16 chunk size=32 chunk size=64 chunk size=128 ndcg@1 ndcg@10 ndcg@1 ndcg@10 ndcg@1 ndcg@10 ndcg@1 ndcg@10 SciFact Jina Naive 47.33% 61.87% 52.67% 64.70% 51.00% 66.06% 51.33% 65.53% Late 55.67% 70.89% 56.00% 70.70% 56.33% 70.33% 54.33% 69.76% HeteRAG 56.67% 70.21% 58.00% 71.98% 59.00% 72.01% 60.00% 71.87% BGE Naive 49.00% 65.15% 58.67% 70.78% 62.67% 73.91% 63.33% 74.93% Late 61.33% 73.40% 62.00% 73.67% 61.67% 73.45% 61.00% 73.22% HeteRAG 64.67% 77.11% 65.33% 77.34% 65.00% 77.59% 65.00% 77.46% e5 Naive 47.67% 63.05% 53.00% 68.01% 55.33% 70.40% 58.00% 71.66% Late 56.67% 70.11% 56.00% 69.65% 55.33% 69.41% 55.00% 69.12% HeteRAG 59.33% 73.06% 61.67% 74.61% 63.00% 74.54% 62.67% 74.85% Table 2: Evaluation of our proposed adaptive fine-tune strategy on retrieval tasks. While fine-tuning generally enhances retrieval task performance, HeteRAG still achieves superior results compared to the fine-tuned baselines. building, we employ the Faiss library (Douze et al., 2024). All experiments were conducted on four RTX 5000 GPUs. 4.2 Performance Evaluation We conduct comprehensive experiments to evaluate the effectiveness of HeteRAG on the BeIR bench- mark, comparing against two baseline retrieval methods: naive RAG and late chunking. Late chunking method (GÃ¼nther et al., 2024) embeds all tokens in a document before applying chunk- ing with a long text embedding model, to preserve full contextual information and improve retrieval performance. For the Jina model, since it is