These re- trieved chunks are subsequently incorporated into the prompt for the LLM, allowing it to generate contextually informed responses. In typical RAG architectures, the retrieval and generation phases demonstrate distinct require- ments regarding knowledge chunk granularity. As interactive objects of the retriever, knowledge chunks are required to accurately match user queries to help the retrieval model find the most relevant information. Therefore, the retrieval step requires semantically complete information to en- sure retrieval accuracy. Conversely, excessively long chunks may introduce redundant or irrelevant information. This may potentially induce hallu- cinations in LLMs (Huang et al., 2023), thereby compromising the efficacy and efficiency of the generation process. Hence, knowledge chunks are expected to provide the most precise information to answer the userâ€™s questions. However, most exist- ing RAG methods employ identical representations of knowledge chunks for both retrieval and genera- 1 arXiv:2504.10529v1 [cs.IR] 12 Apr 2025 tion, and thus face challenges in jointly optimizing the performance of both stages caused by the identi- cal granularity of knowledge chunk representation. To address this problem, we propose HeteRAG, a heterogeneous RAG framework that decouples the representations of knowledge chunks for re- trieval and generation stages. As shown in Fig 1, we employ a context-enriched modeling strategy at retrieval side to integrate both multi-granular contextual signals and global structured metadata, enhancing the retrieval accuracy. Meanwhile, we utilize standalone knowledge chunks for the gen- eration process, enabling LLMs to generate with high efficiency and precision. This architecture facilitates joint optimization of both stages. Build- ing on this, we further propose an adaptive prompt tuning strategy that enables the retrieval model to dynamically align with our context-enriched mod- eling strategy. It facilitates the specialization of off-the-shelf embedding models, allowing them to effectively handle diverse, structurally complex real-world knowledge corpus. We conduct ex- tensive experiments on