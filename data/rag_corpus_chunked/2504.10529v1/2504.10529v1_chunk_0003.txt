precision. This architecture facilitates joint optimization of both stages. Build- ing on this, we further propose an adaptive prompt tuning strategy that enables the retrieval model to dynamically align with our context-enriched mod- eling strategy. It facilitates the specialization of off-the-shelf embedding models, allowing them to effectively handle diverse, structurally complex real-world knowledge corpus. We conduct ex- tensive experiments on retrieval tasks and end-to- end RAG pipelines to evaluate the effectiveness of HeteRAG. Experimental results demonstrate that HeteRAG achieves significant improvements compared to baselines. The consistent gains in retrieval and QA accuracy confirm HeteRAG ef- fectively resolves the two-stage optimization con- flict, thereby enhancing the real-world applicabil- ity of RAG. Our codes are available at: https: //anonymous.4open.science/r/HeteRAG/. Our contributions can be summarized as follows: • We introduce a novel heterogeneous RAG framework that decouples knowledge repre- sentations for retrieval and generation step. • We design a prompt tuning strategy that adap- tively aligns pre-trained models with the het- erogeneous RAG process. • Extensive experiments on 3 knowledge bases, 5 datasets, 4 retrieval model, and 3 foundation models demonstrate that HeteRAG effectively outperforms baseline RAG methods. 2 Related Works 2.1 Retrieval Models Retrieval models aim to retrieve relevant informa- tion from a corpus based on queries. Modern ap- proaches predominantly employ transformer-based pre-trained embedding models for dense retrieval, a paradigm that learns latent space representations of queries and chunks through neural encoding. Recent progress features several impactful em- bedding models that demonstrate state-of-the-art performance across various benchmarks. The E5 family (Wang et al., 2022) train text embeddings in a contrastive manner using weak supervision from a large-scale text pair dataset. Jina Embed- dings (Günther et al., 2023) focus on long text input and extend token limits, effectively handling long documents without the need for truncation or paragraph splitting. BGE embedding family