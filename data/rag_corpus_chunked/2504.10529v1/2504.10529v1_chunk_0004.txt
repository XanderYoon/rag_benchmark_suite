demonstrate state-of-the-art performance across various benchmarks. The E5 family (Wang et al., 2022) train text embeddings in a contrastive manner using weak supervision from a large-scale text pair dataset. Jina Embed- dings (Günther et al., 2023) focus on long text input and extend token limits, effectively handling long documents without the need for truncation or paragraph splitting. BGE embedding family (Xiao et al., 2024; Chen et al., 2024a) is a versatile em- bedding model trained through multi-stages that exhibits highly competitive performance in multi- lingual and cross-lingual retrieval tasks. These ver- satile embedding models are capable of uniformly supporting a variety of tasks, providing support for multiple applications, including RAG. Note that our work is orthogonal to these embedding models; it can be implemented in any embedding model to enhance their performance in retrieval tasks. 2.2 Retrieval Augmented Generation Since the RAG framework was first proposed (Lewis et al., 2020; Guu et al., 2020), it has be- come an important supporting technology in the real-world applications of LLMs. By providing re- liable and up-to-date external knowledge to LLMs, RAG effectively enhances their generation perfor- mance. In recent years, many works have improved the retrieval stage of RAG through various opti- mization methods. Yu et al. (2023) introduce an augmentation-adapted retriever which is trained to learn unseen LLMs’ preferences from a known source language model. Shi et al. (2024) append re- trieved documents to the input of a frozen language model, differentiating itself from previous methods that train language models to adapt to retrievers. Overall, these works still employ identical repre- sentations of knowledge chunks for both retrieval and generation stages. Some works have decoupled retrieval and gen- eration representations to a certain extent. For ex- ample, late chunking method (Günther et al., 2024) utilizes long context embedding models to first