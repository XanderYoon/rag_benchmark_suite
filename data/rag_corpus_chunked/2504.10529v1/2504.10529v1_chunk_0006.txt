maintains standalone chunk usage. models or in scenarios involving extremely long documents. Raina and Gales (2024) introduce a zero-shot adaptation of dense retrieval by decom- posing chunks into atomic statements and generat- ing synthetic questions for improved chunk recall. Chen et al. (2024b) propose a multi-document QA framework with cascading metadata integration and multi-route retrieval for multi-document environ- ments. Anthropic (2024) present a method for gen- erating contextualized chunk embeddings by using a large language model (LLM) to augment chunk text with relevant context from the entire document before embedding. These works might face latency caused by the generation of KG or summaries by large models, which limits their effectiveness in online settings and with larger corpora. 3 Methods In this section, we first give a problem formulation of retrieval and generation process of RAG. Then we elaborate on HeteRAG framework in detail. 3.1 Problem Formulation Given a document corpus {D1, ..., DM } and user query Q, an RAG system operates through three coordinated phases: document chunking, dense vector retrieval, and conditional generation. A chunking strategy is used to first decompose each document Dj into text chunks through a chunk- ing strategy C: {C(j) 1 , ..., C(j) Nj } = C(Dj) ∀j ∈ {1, ..., M}. C(j) i denotes the i-th chunk from docu- ment Dj, resulting in a global chunk collectionSM j=1{C(j) 1 , ..., C(j) Nj }. Then the retriever R en- codes both the query and all chunks into a shared embedding d-dimensional vector space: q = R(Q), e(j) i = R(C(j) i ) (1) The system computes pairwise similarity scores ϕ(q, e(j) i ) between the query embedding and chunk embeddings, typically implemented as cosine simi- larity. The top-k most relevant chunks are passed to LLM G to generate the final response. 3.2 Knolwedge