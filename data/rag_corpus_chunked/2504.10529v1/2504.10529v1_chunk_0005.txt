differentiating itself from previous methods that train language models to adapt to retrievers. Overall, these works still employ identical repre- sentations of knowledge chunks for both retrieval and generation stages. Some works have decoupled retrieval and gen- eration representations to a certain extent. For ex- ample, late chunking method (GÃ¼nther et al., 2024) utilizes long context embedding models to first em- bed all tokens before applying chunking, resulting in chunk embeddings that preserve full contextual information and improve performance on retrieval tasks. However, the effectiveness of late chunk- ing is limited when using conventional embedding 2 Retrieval Embedding Model â„› Generative Language Model ğ’¢ğ’¢ Identical Representations for ğ“¡ğ“¡and ğ“–ğ“– ğ‘„ğ‘„ Decoupling Representations for ğ“¡ğ“¡and ğ“–ğ“– ğœ™ğœ™(ğªğª, ğğğ‘–ğ‘– (ğ‘—ğ‘—)ï¼‰ ğªğª ğ¶ğ¶ğ‘–ğ‘– (ğ‘—ğ‘—): ğ‘–ğ‘–-th Chunk of Doc. ğ‘—ğ‘— Retrieval Embedding Model â„› Generative Language Model ğ’¢ğ’¢ ğ‘„ğ‘„ ğğğ‘–ğ‘– (ğ‘—ğ‘—) ğªğª ğœ™ğœ™(ğªğª, ğğğ‘–ğ‘– (ğ‘—ğ‘—) ï¼‰ ğ¶ğ¶1 (ğ‘—ğ‘—) ğ¶ğ¶ğ‘ğ‘ğ‘—ğ‘— (ğ‘—ğ‘—) ğ·ğ·ğ‘—ğ‘—: ğ‘—ğ‘—-th Doc. in Corpus ğ¶ğ¶ğ‘–ğ‘– (ğ‘—ğ‘—) â€¦â€¦ Multi-Granular Contextual Signals ğœ“ğœ“ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘{ğ¶ğ¶ğ‘–ğ‘–ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘—ğ‘—} Global Structured Metadata of ğ·ğ·ğ‘—ğ‘— ğ·ğ·ğ‘—ğ‘—: Title Sub-title 1 Sub-title ğ‘–ğ‘–ğ‘†ğ‘† Sub-title ğ‘ğ‘ğ‘†ğ‘†â€¦ â€¦ Paragraph 1 Paragraph ğ‘–ğ‘–ğ‘ƒğ‘ƒ Paragraph ğ‘ğ‘ğ‘ƒğ‘ƒâ€¦ â€¦ ğœ“ğœ“ğ‘šğ‘šğ‘šğ‘šğ‘ğ‘ğ‘šğ‘šğ‘€ğ‘€ğ‘–ğ‘– ğ‘—ğ‘— ğ¶ğ¶t (ğ‘—ğ‘—) ğ¶ğ¶s (ğ‘—ğ‘—) {ğ¶ğ¶ğ‘–ğ‘–Â±ğ‘˜ğ‘˜ (ğ‘—ğ‘—) } ğğğ‘–ğ‘– (ğ‘—ğ‘—) = â„› ğœ“ğœ“ğ¶ğ¶ğ‘–ğ‘– ğ‘—ğ‘—âŠ• ğœ“ğœ“ğ‘šğ‘šğ‘šğ‘šğ‘ğ‘ğ‘šğ‘šğ‘€ğ‘€ğ‘–ğ‘– ğ‘—ğ‘—âŠ• ğœ“ğœ“ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘{ğ¶ğ¶ğ‘–ğ‘–ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘—ğ‘—}ğğğ‘–ğ‘– (ğ‘—ğ‘—) = â„›(ğ¶ğ¶ğ‘–ğ‘– (ğ‘—ğ‘—)) Chunking Figure 2: The overall framework of HeteRAG. The left shows naive RAG using identical representations of knowledge chunks for retrieval and generation. The right depicts HeteRAGâ€™s framework: retrieval incorporates global metadata and multi-granular context, while generation maintains standalone chunk usage. models or in scenarios involving extremely long documents. Raina and Gales (2024) introduce a zero-shot adaptation of dense retrieval by decom- posing chunks into atomic statements and generat- ing synthetic questions for improved chunk recall. Chen et al. (2024b) propose a multi-document QA framework with cascading metadata integration and multi-route retrieval for multi-document environ- ments. Anthropic