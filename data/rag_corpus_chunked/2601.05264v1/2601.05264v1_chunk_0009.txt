Dense Passage Retriever (DPR) that has been trained with dual- encoder contrastive objectives and a pretrained sequence-to-sequence gen- erator such as BART [33] or T5 [34]. The architectural blueprint for subse- quent RAG system developments has been established by this foundational pattern [35]. In response to a query q, the retriever calculates inner product similarity to identify a set of top,k documentsD1, ..., Dk from a corpus C. The following is the formal calculation: score(q, d) =fq(q)⊤ fd(d) wheref q andf d are the encoding functions for query and document, re- spectively. Subsequently, the generator receives the retrieved documents and linearly combines them, typically through string concatenation. The generator then based its output on this augmented context: P(y|q, D1,···, Dk) = ∑ i P(y|q, Di)P(D i|q) Thismarginallikelihoodformulation[1]implicitlyintegratesrelevancepriors into the decoding process, thereby establishing a generation pipeline that is probabilistically grounded. 3.2 Architectural Components and Their Interplay Dense Retrieval: Scalability versus Recall DPR facilitates sublinear ANN-based retrieval over billion-scale corpora by employing independently parameterized encoders for queries and documents [36]. However, the semantic compression inherent in dense vector spaces can result in reduced recall for exact-match and out-of-distribution queries, particularly in specialized domains where lexical precision remains critical [37], [38]. 10 Document Ranking: The Role of Marginal Likelihood The marginalization strategy guarantees that generative attention is dis- tributed across multiple passages, thereby enhancing robustness against noisy retrievals [15]. Recent improvements include the use of cross-encoders to rerank modules, which reevaluate the fidelity of evidence, Two-stage reranking patterns such as RE-RAG formalize this design and report con- sistent gains on standard IR benchmarks [17], [18], [52], [53], [54]. The marginalization strategy further stabilizes evidence aggregation across pas- sagesinnoisy-retrievalsettings[39]. However, thisintroducescomputational complexity during inference [40]. Generation: Expressivity under Context Constraints T5 and BART function as high-capacity generators that leverage autoregres- sive decoding and