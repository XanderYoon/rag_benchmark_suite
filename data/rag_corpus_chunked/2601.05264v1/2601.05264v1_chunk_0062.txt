complex data discovery now on GitHub,” Microsoft Research Blog, 2024. [Online]. Available:http s://www.microsoft.com/en-us/research/project/graphrag/ [22] C. Kim et al., “AutoRAG: Automated Pipeline Optimization for RAG,” arXiv preprint arXiv:2403.09192, 2024. [23] S. Yao et al., “ReAct: Reasoning and Acting in Language Models,” arXiv preprint arXiv:2210.03629, 2022. [24] Y. Asai et al., “Self-RAG: Self-Reflective Retrieval-Augmented Gener- ation,” inProc. ICLR, 2024, pp. 567–582. [25] R. Nakano et al., “WebGPT: Browser-Assisted QA with Human Feed- back,” OpenAI Technical Report, 2022. [26] T. Gao et al., “ALCE: Enabling Automatic Evaluation for Long-form Text Generation,” arXiv preprint arXiv:2305.14984, 2023. [27] S. Es et al., “Ragas: Automated Evaluation of Retrieval Augmented Generation,” arXiv preprint arXiv:2309.15217, 2023. [28] LangChain Documentation, “RAG Implementation Patterns,” LangChain Community, 2024. [Online]. Available:https://docs.langcha in.com/docs/use-cases/retrieval/ [29] LlamaIndex Documentation, “RAG Workflow Guide,” LlamaIndex, 2024. [Online]. Available:https://docs.llamaindex.ai/en/stable/ [30] S. Gupta, R. Ranjan, and S. N. Singh, “A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions,” arXiv preprint arXiv:2410.12837, 2024. [Online]. Available:https://arxiv.org/abs/2410.12837 [31] C. Sharma et al., “Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers,” arXiv preprint arXiv:2506.00054, 2025. [Online]. Available:https://arxiv.org/ abs/2506.00054 [32] P. Zhao et al., “Retrieval-Augmented Generation for AI-Generated Con- tent: A Survey,” arXiv preprint arXiv:2402.19473, 2024. [Online]. Available: https://arxiv.org/abs/2402.19473 67 [33] M. Lewis et al., “BART: Denoising Sequence-to-Sequence Pretraining,” inProc. ACL, 2020, pp. 7871–7880. [34] C. Raffel et al., “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,”J. Mach. Learn. Res., vol. 21, pp. 1–67, 2020. [35] W. Fan et al., “A Survey on Retrieval-Augmented LLMs,” inProc. KDD, 2024, pp. 1234–1248. [36] J. Karpukhin et al., “Dense Passage Retrieval for Open-Domain QA,” inProc. EMNLP, 2020, pp. 6769–6781. [37] O. Khattab and M. Zaharia, “ColBERT: Efficient Passage Retrieval via Contextualized Late Interaction,” inProc. SIGIR, 2020, pp. 39–48. [38] L. Yang et al., “Hybrid Sparse-Dense Retrieval for QA,”