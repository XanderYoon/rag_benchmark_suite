contextual appropriateness beyond surface-level text matching are necessary for generation quality assessment in RAG systems [103]. In order to conduct an exhaustive evaluation of generation quality, contempo- rary evaluation frameworks implement both conventional NLP metrics and sophisticated LLM-based judges [104]. Table 7.3: Generation Quality Metrics Comparison 30 Metric Type Metric Name Evaluation Focus Computational Cost Human Correla- tion Reference Require- ment Traditional BLEU Score N-gram precision Low Low- Medium Yes Traditional ROUGE Score N-gram recall Low Medium Yes Model- based BERTScore Semantic similarity Medium High Yes LLM- based Faithfulness Claim ver- ification High Very High No LLM- based Answer Relevancy Query alignment High Very High No LLM- based Answer Correct- ness Factual accuracy High Very High Yes The BLEU score is a metric that quantifies the precision of n-grams between reference and generated texts. Scores range from 0 to 1, with higher values indicating a closer alignment. However, it encounters challenges with se- mantic understanding and word order variations [105]. The ROUGE score is particularly beneficial for evaluating the comprehensiveness of question- answering tasks, as it emphasizes recall-oriented evaluation and concentrates on the amount of reference content that is incorporated in the generated text [106]. BERTScore utilizes contextual embeddings from transformer models to cal- culate semantic similarity through cosine similarity of token representations. This approach offers a more nuanced evaluation that is more closely aligned with human judgment than surface-level metrics [107]. The metric is par- ticularly effective for evaluating conversational interfaces and shorter text generation tasks, as it calculates precision, recall, and F1 measures by align- ing contextually similar utterances between candidate and reference texts [108]. 6.4 Assessment of Safety and Trustworthiness Critical concerns regarding hallucination detection, factual consistency, and appropriate uncertainty management in RAG systems are addressed by 31 trustworthiness evaluation [109]. In enterprise deployments, where inaccu- rate