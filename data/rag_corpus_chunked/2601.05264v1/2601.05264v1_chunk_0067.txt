Available:https://www.ibm.com/products/w atsonx-ai [93] X. Xu et al., “Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey,” arXiv preprint arXiv:2504.14891, 2025. [Online]. Available:https: //arxiv.org/html/2504.14891v1 [94] “Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets,” arXiv, Jan. 2025. [Online]. Available:https: //arxiv.org/html/2504.20119v2 [95] D. Ru et al., “RAGChecker: A Fine-grained Framework for Diagnos- ing Retrieval-Augmented Generation,” inProc. NeurIPS, 2024. [Online]. Available:https://proceedings.neurips.cc/paper_files/paper/202 4/file/27245589131d17368cccdfa990cbf16e-Paper-Datasets_and_Be nchmarks_Track.pdf [96] “Evaluating RAG Applications with RAGAs,” LangSmith Documenta- tion. [Online]. Available:https://docs.smith.langchain.com/old/coo kbook/testing-examples/ragas [97] “Evaluating RAG Systems: A Comprehensive Approach to Assessing Retrieval-AugmentedGeneration,” LinkedIn, May2024. [Online]. Available: https://www.linkedin.com/pulse/evaluating-rag-systems-compreh ensive-approach-assessing-kakkar-esm9c [98] “Evaluate RAG responses with Amazon Bedrock, LlamaIndex and RA- GAS,” AWS Machine Learning Blog, Mar. 2025. [Online]. Available: https://aws.amazon.com/blogs/machine-learning/evaluate-rag -responses-with-amazon-bedrock-llamaindex-and-ragas/ [99] “Evaluating - LlamaIndex,” LlamaIndex Documentation. [Online]. Available:https://docs.llamaindex.ai/en/stable/module_guides/e valuating/ [100] “HotpotQA Dataset,” Hugging Face. [Online]. Available:https: //huggingface.co/datasets/hotpotqa/hotpot_qa 73 [101] “Galileo introduces RAG & Agent Analytics Solution,” AI-Tech Park. [Online]. Available:https://ai-techpark.com/galileo-introduces-r ag-agent-analytics-solution/ [102] “RAG Triad - TruLens,” TruLens Documentation, Jan. 2025. [Online]. Available:https://www.trulens.org/getting_started/core_concept s/rag_triad/ [103] “RAG evaluation metrics: A journey through metrics,” Elastic, Oct. 2024. [Online]. Available:https://www.elastic.co/search-labs/blog /evaluating-rag-metrics [104] “When evaluating a RAG system’s overall performance, how would you combine metrics for retrieval and metrics for generation?,” Milvus, May 2025. [Online]. Available:https://milvus.io/ai-quick-reference/whe n-evaluating-a-rag-systems-overall-performance-how-would-you -combine-metrics-for-retrieval-and-metrics-for-generation-wou ld-you-present-them-separately-or-is-there-a-way-to-aggregate -them [105] “Traditional NLP Metrics - Ragas,” Ragas Documentation. [Online]. Available:https://docs.ragas.io/en/stable/concepts/metrics/ava ilable_metrics/traditional/ [106] “BLEU, ROUGE, and METEOR are traditional metrics used to eval- uate the quality of text,” Milvus, Jun. 2025. [Online]. Available:https: //milvus.io/ai-quick-reference/which-traditional-language-gen eration-metrics-are-applicable-for-evaluating-raggenerated-ans wers-and-what-aspect-of-quality-does-each-bleu-rouge-meteor-c apture [107] “BERTScore in AI: Enhancing Text Evaluation,” Galileo AI, Jun. 2025. [Online]. Available:https://galileo.ai/blog/bert-score-e xplained-guide [108] “Bert Score for Contextual Similarity for RAG Evaluation,” YouTube, Nov. 2023. [Online]. Available:https://www.youtube.com/watch?v=7A Vjk2k8Mbs [109] “OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain,” Hugging Face Papers, Jun. 2025. [Online]. Available:https://huggingface.co/papers/2412.13018 [110] “Evaluate and Optimize RAG Applications - Galileo,” Galileo Docu- 74 mentation,