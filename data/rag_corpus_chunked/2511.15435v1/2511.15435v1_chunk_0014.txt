r1 4:Initialize perturbed imageI p ←I q +δ r 5:forstep←1tosdo 6:I p ←I q +δ r 7:f multistage2 ←E i(Ip) +E t(Tq +C(I p)) 8:clean sim←sim(f multistage2 , fclean query ) 9:ref sim←sim(f multistage2 , fref passage) 10:L R ←max (∥clean sim−β·ref sim∥+γ,0), 11:Optimizeδ r ←δ r −α·sign(∇ δr LR) 12:δ r ←Clip(δ r,−ϵ, ϵ) 13:end for Returnδ r 4. Experiments 4.1. Datasets and evaluation metrics We conduct our experiments on two wide-used datasets: •OK-VQA[26]: This is a large knowledge-based VQA dataset. Each data sample consists of a question, an im- age and 10 gold answers. The images are from the COCO dataset [19] and each question requires external knowl- edge to answer. We use the test set for attack evaluation, which contains 5046 samples. For OK-VQA, we fol- low RA VQA [20], FLMR [21] to use the Google Search corpus as the external knowledge base, which contains 169,306 passages in total. •Infoseek[6]: We follow PoisonedEye [37] to attack 1000 samples from the visual question answering dataset Infos- eek for evaluation. The knowledge database is the same subset of 2M image-text pairs from OVEN-Wiki [12] with PoisonedEye. Evaluation metrics.Our attack method is evaluated from two perspectives: retrieval and generation. For re- trieval, we first useS(q, p)to identify the relation between a queryqand a passagep, which is classified based on whether the passage contains a ground-truth answer to the query. S(q, p) = ( 1,if p contains answer to q, 0,if p does not contain answer to q. Then, we adopt the following two retrieval metrics. Table 1. VQA Performance on OK-VQA. (ASR* is calculated as(s clean −s adv)/sclean for each metrics, EM: Exact Match, “↑” indicates that a higher value is better for this metric, while “↓” indicates that a lower value is better. All VQA metrics are reported with