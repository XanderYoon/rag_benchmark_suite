obvious by changing textual words or can be identified by Knowledge Base (KB) detection al- gorithm. Meanwhile, there are research of attacks on the generator component of MRAG [30, 31, 35], which include manipulations on textual queries and are easy to notice as well. Thus, in this paper, we consider a different setting of attacking MRAG: pure visual attack that solely adds im- perceptible perturbations at the image inputs of users. As shown in Figure 1, our attack method aims to disrupt both retrieval and generation process of MRAG by optimizing visual perturbations, without manipulating any other com- arXiv:2511.15435v1 [cs.CV] 19 Nov 2025 74.4 74.18 72.69 56.92 31.81 clean 74.63 25 40 55 70 85 Random AA XT LA Ours Recall@5 (eps = 8/255) 74.4 74.26 74.22 73.03clean 74.63 25 40 55 70 85 8/255 16/255 32/255 64/255 Random Perturbation Recall@5 Figure 2. Attack performance on a CLIP model fine-tuned for the retrieval task by different attack methods (AA: Any Attack [39]; XT: X-Transfer [14], LA: LMM Attack [7]) and scale. ponents. Compared to poisoning attacks, adversarial visual attack on images can be highly imperceptible to human vi- sion, making them exceptionally stealthy. However, attacking MRAG with pure visual perturba- tion is challenging. The MRAG system is a pipeline that consists of a fine-tuned retriever and a large-scale genera- tor, both of which retain a certain degree of robustness. For fine-tuned retrievers, as shown in Figure 2, advanced vi- sual attack methods [7, 14, 39] can cause limited retrieval performance drop, and random perturbations even with a large scale (64/255) causes subtle performance drop. This may be because the models become adept at multimodal knowledge grounding and logical association, focusing on relational understanding (e.g., the logical relationship be- tween a question and a potential answer). This emphasis on deeper logical