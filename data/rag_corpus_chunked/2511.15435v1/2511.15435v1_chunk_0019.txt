and 57.38%) on non-fine-tuned and fine-tuned retrievers. Note that though the attack perfor- Figure 4. Performance of ablated models with different steps. mance of LMM Attack and our method are compatible on the original CLIP, our method achieves over two times the success rate on the fine-tuned version. Our hierarchical attack works on various black-box generators.We use BLIP-2 and LLaV A as black-box gen- erators. VQA results of OK-VQA using LLaV A is shown in Table 3. With using different black-box generators, our attack with adversarial augmented knowledge and query noise can all achieve damaging attack effect. VQA Score(↓) ASR*(↑) EM(↓) ASR*(↑) Clean 63.30 - 67.34 - AnyAttack 61.31 3.14 65.46 2.79 X-transfer 57.17 9.68 60.98 9.44 LMM Attack 56.72 10.39 60.42 10.23 Ours 54.19 14.39 57.77 14.21 Table 3. VQA results on OK-VQA with LLaV A as generator. Top- 5 retrieved documents are used for generation. 4.4.2. Ablation Studies Analysis of Hierarchical Retrieval Attack.Table 4 and Table 5 show the detailed retrieval results on OK-VQA and Infoseek datasets. Both stages of the attack demonstrate in- dividual effectiveness. Using stage 1 and stage 2 alone both degrades retrieval metrics. Stage 2’s individual attack per- formance is comparably better than stage 1, since its attack goal is more directly targeted at retrieval. While across all datasets and retriever evaluated, the hierarchical two-stage attack consistently achieves optimal performance, demon- strating the attack effect of breaking both modality and se- mantic alignment. Effect of PGD Steps.As shown in Figure 4 (left), we report the Recall@5 metric on OK-VQA dataset with fine- tuned CLIP using 10 to 50 steps of stage 1 and 2. As the PGD algorithm increases optimization steps, the attack re- sults become better, eventually coming to convergence. We finally choose PGD-step50 for each stage of optimization. Effect of Perturbation Budget.We