The generator provides a correct answer ac- cordingly. However, with added imperceptible noise, the retriever returns irrelevant augmented knowledge that are about winter sports. With the disruptions, the final predic- tion of the generator is wrong and unreasonable. 5. Conclusion In this paper, we proposed a novel attack method against MRAG pipeline that focused solely on image inputs. With hierarchical optimization, we target MRAG’s retriever and generator across different levels of abstraction, achieving severe while stealth attack impact. Our research reveals that MRAG technologies, while widely adopted in practice, remain vulnerable to security threats posed by impercepti- ble adversarial visual noise. Future work will focus on un- covering deeper and more diverse potential threats brought by visual attacks on MRAG systems and developing ro- bust defense mechanisms to balance effectiveness and secu- rity. References [1] Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mo- hammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. Ask in any modality: A comprehen- sive survey on multimodal retrieval-augmented generation. arXiv preprint arXiv:2502.08826, 2025. 1, 2 [2] Jay Barach. Cross-domain adversarial attacks and robust de- fense mechanisms for multimodal neural networks. InIn- ternational Conference on Advanced Network Technologies and Intelligent Computing, pages 345–362. Springer, 2024. 2 [3] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models.arXiv preprint arXiv:1712.04248, 2017. 3 [4] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In2017 ieee symposium on security and privacy (sp), pages 39–57. Ieee, 2017. 3 [5] Huanran Chen, Yichi Zhang, Yinpeng Dong, Xiao Yang, Hang Su, and Jun Zhu. Rethinking model ensem- ble in transfer-based adversarial attacks.arXiv preprint arXiv:2303.09105, 2023. 3 [6] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, So- ravit Changpinyo, Alan Ritter, and Ming-Wei Chang. Can pre-trained vision and language models