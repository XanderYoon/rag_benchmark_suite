attack methods [7, 14, 39] can cause limited retrieval performance drop, and random perturbations even with a large scale (64/255) causes subtle performance drop. This may be because the models become adept at multimodal knowledge grounding and logical association, focusing on relational understanding (e.g., the logical relationship be- tween a question and a potential answer). This emphasis on deeper logical correlation and inference ability makes the model less susceptible to simple input perturbations in a single modality, significantly increasing the challenge of our image attack task. For generators, large-scale LMMs such as LLaV A [22] and BLIP-2 [18] are often used, which generally demonstrate superior robustness [36], further pos- ing challenging to MRAG attack. The sequential working pipeline of MRAG also en- hances its robustness towards visual perturbations as the at- tack effect degrades by propagation through the RAG chain. The perturbation’s effect on retriever lies in the recalled knowledge, which experiences chain of transformation and decrease. Consequently, success visual attack on MRAG is hard, while posing a more severe threat than obvious text attacks or KB poisoning attacks, as the resulting bad influ- ence can go undetected and propagate through the entire RAG chain. To effectively subvert the robustness of MRAG with only visual perturbations, we propose a novel hierarchical dis- ruption approach to attack the MRAG’s on two compo- nents and mislead the final result. As shown in Figure 1, there are two inputs for the MRAG generator: the multi- modal user query and the augmented knowledge. Our hier- archical method targets at misaligning and disrupting these two inputs, creating knowledge conflicts for the generator while breaking different levels of model capabilities. For retrieval, we employ a hierarchical two-stage strategy to op- timize the applied noise in modality and semantic levels. We first alter the query features to