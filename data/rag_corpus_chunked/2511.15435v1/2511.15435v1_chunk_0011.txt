enabling relevant knowledge to be retrieved through the ranking of embedding similar- ities during cross-modal retrieval. Thus, the first stage of visual attack on augmented knowledge is to break down the multimodal alignment between these modalities. We achieve this by pushing the query image close to the least similar sample. Given a user query image, we search for a reference image from the database whose em- bedding has the smallest similarity with the query image’s embedding. Then, given the retriever image encoderE i, text encoderE t and image captioning modelC, we obtain the multimodal image representationf multistage1, the clean query caption’s embeddingfclean cap and the reference im- age caption’s embeddingfref cap as follows: fclean cap =E t(C(Iq)), fref cap =E t(C(Ir)), fmultistage1 =E i(Ip) +E t(C(Ip)), (1) whereI q andI r are the input user query image and its refer- ence image,I p is the query image with added perturbation. To learn the best the perturbation, we conduct contrastive learning betweenf multistage1,f clean cap andf ref cap and design the loss functionL CM based on hinge loss [8]: LCM = max (∥clean sim−β·ref sim∥+γ,0), clean sim=sim(f multistage1 , fclean cap), ref sim=sim(f multistage1 , fref cap). (2) As shown in Equation 2, by minimizingL CM , we min- imize the similarity between the query and clean image’s caption, while maximize the similarity with the reference caption. The detailed optimization process can be found in Algorithm 1. In this stage, since we focus on cross-modal alignment, we consider the alignment between multimodal representation of the query image and the corresponding text embeddings. Through iterations, we minimize the sim- ilarity between the multimodal embedding and its text cap- tion embedding, while maximizing the similarity with the reference text caption embedding. Stage 2: Semantic Alignment Attack The second stage of our attack aims