we focus on cross-modal alignment, we consider the alignment between multimodal representation of the query image and the corresponding text embeddings. Through iterations, we minimize the sim- ilarity between the multimodal embedding and its text cap- tion embedding, while maximizing the similarity with the reference text caption embedding. Stage 2: Semantic Alignment Attack The second stage of our attack aims to further break down the semantic relevance between cross-modal embed- dings, which is crucial for retrieving useful knowledge for generation. The form of the loss function is similar to that in the first stage. However, the contrastive learning is now Algorithm 1Modality Alignment Attack (Stage 1) Input:User imageI q, reference imageI r, retriever im- age encoderE i, retriever text encoderE t, image cap- tioning modelC, generation stepss, step lengthα, per- turbation boundϵ, trade-off parameterβ, margin pa- rameterγ. Output:Stage 1’s perturbationδ r1 1:Get clean and reference image caption embedding: fclean cap ←E t(C(Iq)),f ref cap ←E t(C(Ir)) 2:Initialize perturbationδ r1 ←0 3:Initialize perturbed imageI p ←I q +δ r1 4:forstep←1tosdo 5:I p ←I q +δ r1 6:f multistage1 ←E i(Ip) +E t(C(Ip)) 7:clean sim←sim(f multistage1 , fclean cap) 8:ref sim←sim(f multistage1 , fref cap) 9:L CM ←max (∥clean sim−β·ref sim∥+γ,0), 10:Optimizeδ r1 ←δ r1 −α·sign(∇ δr1 LCM ) 11:δ r1 ←Clip(δ r1 ,−ϵ, ϵ) 12:end for Returnδ r1 conducted between the multimodal representation of user query, the query text embedding and reference passage em- bedding. We retrieve the positive text knowledge for the reference image and use its embedding as the positive em- bedding in this stage. The embeddings are obtained by: fclean query =E t(Tq +C(I q)), fref passage =E t(Tp), fmultistage2 =E i(Ip) +E t(Tq +C(I p)), (3) whereT q andT p are the user text query and retrieved refer- ence text knowledge respectively. The contrastive learning of the