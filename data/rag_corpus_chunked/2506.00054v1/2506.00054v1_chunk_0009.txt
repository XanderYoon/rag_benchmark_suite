reasoning agents. These systems emphasize iterative feedback, utility-aware coordination, and dynamic control over retrieval actions, particularly in open-domain, multi-hop, and evolving knowledge contexts. We identify three dominant architectural patterns: iterative or multi-round retrieval, utility-driven joint optimization, and retrieval-aware generation control. Iterative or Multi-Round Retrieval: These systems interleave retrieval and generation across multiple reasoning steps, allowing for evidence refinement and progressive answer construction. IM-RAG (Inner Monologue RAG) [80] simulates an “inner monologue” by alternating between generation and retrieval phases, supporting multi-step reasoning. Generate-Then-Ground (GenGround) [59] follows a similar philosophy, generating a provisional answer first and then retrieving supporting evidence to substantiate or revise it—improving factuality and interpretability in multi-hop settings. G-Retriever [22] retrieves graph-structured subcomponents as generation unfolds, enhancing complex reasoning over textual graphs. Utility-Driven Joint Optimization: Several frameworks seek to align retriever outputs with their downstream utility for generation through joint objectives or reinforcement learning. Stochastic RAG [84] treats retrieval as an expected utility maximization problem, updating both retriever and generator end-to-end using REINFORCE-based gradients. M-RAG [70] applies multi-agent reinforcement learning, coordinating distributed retrievers and generators via shared memory and task-specific roles. MedGraphRAG [72] integrates knowledge graphs into the joint learning loop, facilitating domain-specific reasoning with structured priors. These systems improve factuality and answer consistency, particularly in biomedical and enterprise domains. Dynamic Retrieval Triggering: A growing class of systems dynamically controls when and how to retrieve, condi- tioned on generation uncertainty, task complexity, or intermediate outputs. DRAGIN (Dynamic Retrieval Augmented Generation based on the Information Needs of LLMs) [ 61] triggers retrieval at the token level using entropy-based confidence signals, while FLARE (Forward-Looking Active REtrieval augmented generation () [32] selectively retrieves Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 7 based on low-confidence predictions during sentence generation. SELF-ROUTE [41] dynamically routes tasks between retrieval and generation modules based on model