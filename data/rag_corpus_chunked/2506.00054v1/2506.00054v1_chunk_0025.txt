-/- -/0.016 (EM) -/- -/0.037 (EM) CRAG LLaMA2-7B -/- -/- -/- 3.034/0.471 (Acc) -/- 1.514/0.173 (Acc) 0.045/- (Acc) Self-CRAG LLaMA2-7B -/- -/- -/- 3.204/0.533 (Acc) -/- 2.083/0.439 (Acc) -/- TA-ARE GPT-3.5 -/- -/- -/- -/- -/- -/- -/- TA-ARE GPT-4 -/- -/- -/- -/- -/- -/- -/- TA-ARE LLaMA2-7B -/- -/- -/- -/- -/- -/- -/- 5 Comparative Analysis To assess the empirical effectiveness of design innovations in Retrieval-Augmented Generation (RAG), this section presents a comparative analysis of representative frameworks across three key evaluation settings: short-form question answering, multi-hop reasoning, and robustness under retrieval perturbations. Results are reported as relative improve- ments over both raw and retrieval-augmented baselines, normalized for model and dataset variability. Additionally, we review ablation studies from the literature to disentangle the contributions of specific components such as retrieval triggers, filtering layers, reranking mechanisms, and robustness modules. These insights offer a clearer understand- ing of which enhancements most significantly impact performance, faithfulness, and efficiency across diverse RAG configurations. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 14 5.1 Comparative Analysis of Framework Performance on Short-Form QA This section presents a comparative analysis of Retrieval-Augmented Generation (RAG) frameworks in short-form question answering, emphasizing their relative improvements over raw large language model (LLM) baselines and retrieval-augmented baselines. As shown in Table 2, these comparisons focus on relative gains (e.g., a value of 2.7 indicates a 270% improvement) rather than absolute performance metrics, which normalize for variations in backbone architectures, prompting strategies, and evaluation protocols. This approach enables a meaningful comparison across diverse experimental setups. Among generator-based RAG systems primarily optimized for accuracy, SELF-RAG consistently demonstrates substantial gains across multiple datasets. It achieves over a 270% improvement from the raw LLM baseline on PopQA [42] and over 200% on ARC-Challenge [12], illustrating the effectiveness of deep context integration for enhancing