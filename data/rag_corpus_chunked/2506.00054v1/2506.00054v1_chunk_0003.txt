RAG system consists of three modules: Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 3 Query Encoder: Encodes the input ğ‘¥ into a query representation ğ‘, which is used to retrieve relevant documents. This can be either a neural encoder or a rule-based template. Retriever: Given the query ğ‘, the retriever fetches a ranked list of documents ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘˜ from a corpus C. Retrievers may be sparse (e.g., BM25 [54]), dense (e.g., DPR [36]), hybrid, or generative. Generator: The generator conditions on the input ğ‘¥ and the retrieved documents ğ‘‘ğ‘– to produce the final output ğ‘¦. This is typically a pretrained transformer model (e.g., T5 [51], BART [39], GPT [5]). 2.2 Mathematical Formulation Formally, the generation process in Retrieval-Augmented Generation (RAG) can be expressed as modeling the conditional distribution: ğ‘ƒ (ğ‘¦ | ğ‘¥) = âˆ‘ï¸ ğ‘‘ âˆˆ C ğ‘ƒ (ğ‘¦ | ğ‘¥, ğ‘‘) Â· ğ‘ƒ (ğ‘‘ | ğ‘¥) (1) where: â€¢ ğ‘¥ is the input (e.g., a question or prompt), â€¢ ğ‘‘ is a retrieved document from corpus C, â€¢ ğ‘¦ is the generated response. In practice, the summation is approximated by retrieving the top-ğ‘˜ documents ğ‘‘1, . . . , ğ‘‘ğ‘˜, yielding: ğ‘ƒ (ğ‘¦ | ğ‘¥) â‰ˆ ğ‘˜âˆ‘ï¸ ğ‘–=1 ğ‘ƒ (ğ‘¦ | ğ‘¥, ğ‘‘ğ‘– ) Â· ğ‘ƒ (ğ‘‘ğ‘– | ğ‘¥) (2) This decomposition reflects two key probabilities: â€¢ ğ‘ƒ (ğ‘‘ğ‘– | ğ‘¥): the relevance score of document ğ‘‘ğ‘– given the input ğ‘¥, often derived from a retriever or reranker. â€¢ ğ‘ƒ (ğ‘¦ | ğ‘¥, ğ‘‘ğ‘– ): the probability of generating output ğ‘¦ conditioned on ğ‘¥ and document ğ‘‘ğ‘–, modeled by the language model. Variants of RAG differ in how they estimate and combine these components. Some use a fixed retriever and let the generator handle noisy inputs, while others jointly optimize retrieval and generation