noisy docu- ments that degrade generation quality. Filtering techniques aim to reduce hallucinations and improve answer relevance by selecting only contextually appropriate content. These methods vary in supervision, granularity, and efficiency, and can be categorized into three groups: lexical/statistical filters, information-theoretic optimizers, and self-supervised passage scoring. Lexical filters such as FILCO apply word overlap and statistical relevance scoring. Using STRINC and CXMI metrics, FILCO removes low-relevance passages and reduces hallucinations by up to 64%, improving EM by +8.6. However, its reliance on lexical similarity limits its adaptability across domains and query styles. Information-theoretic methods like IB Filtering [89] use principles from the information bottleneck framework to retain only high-utility input features while discarding noise. Though computation-heavy, IB Filtering improves EM by +3.2 with a 2.5% compression ratio, offering a balance between precision and conciseness. Similarly, Stochastic Filtering models retrieval as an expected utility maximization problem and re-ranks passages based on marginal value, achieving consistent retrieval effectiveness gains with minimal retriever changes. Self-supervised methods like SEER and RAG-Ex use internal feedback signals to filter noisy retrievals. SEER applies label-free training and generates pseudo-relevance judgments, improving F1 by 13.5% and achieving a 9.25Ã— reduction in context length. RAG-Ex perturbs retrieved passages and compares generation outcomes, selecting those that maximize semantic consistency. It aligns with human-assessed faithfulness 76.9% of the time and is model-agnostic, though computationally expensive due to multiple inference passes. Collectively, these methods balance retrieval compression, answer faithfulness, and domain adaptability. While lexical filters are efficient, self-supervised models provide deeper semantic filtering and support long-form reasoning. 4.3 Efficiency Enhancements While Retrieval-Augmented Generation (RAG) significantly enhances factual consistency in large language models (LLMs) by integrating external document retrieval, it introduces new inefficiencies. These include increased memory overhead, latency from retrieval-processing pipelines, and redundancy in passage selection. This section synthesizes key research efforts