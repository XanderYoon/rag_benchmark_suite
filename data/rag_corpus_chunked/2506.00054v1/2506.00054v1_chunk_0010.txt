Generation based on the Information Needs of LLMs) [ 61] triggers retrieval at the token level using entropy-based confidence signals, while FLARE (Forward-Looking Active REtrieval augmented generation () [32] selectively retrieves Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 7 based on low-confidence predictions during sentence generation. SELF-ROUTE [41] dynamically routes tasks between retrieval and generation modules based on model self-assessed difficulty, and AU-RAG [28] leverages agentic decision- making to mediate between diverse retrieval sources and procedural knowledge. TA-ARE (Time-Aware Adaptive REtrieval) [86] introduces a retrieval trigger classifier that adaptively determines when retrieval is necessary and adjusts the granularity of evidence based on query needs. A related approach, CRAG (Corrective RAG) [79], evaluates retrieved evidence quality before generation and dynamically decides whether to proceed with generation, re-trigger retrieval, or decompose the input into simpler sub-queries. This corrective mechanism positions CRAG within the hybrid class, as it tightly coordinates retrieval assessment with adaptive generation pathways under uncertainty. These architectures reflect a broader trend toward treating retrieval as a controllable, contextualized act rather than a fixed preprocessing step. Their strength lies in adaptivity, coordination, and the capacity to handle under-specified or evolving queries. However, they introduce new challenges in training stability, latency, and system transparency— especially when retrieval is performed mid-decoding. These trade-offs, as well as efficiency-oriented enhancements like pipelining and reranking, are further explored in Section 4. 3.4 Robustness and Security-Oriented RAG Systems Robustness- and security-oriented RAG systems are designed to preserve output quality in the face of noisy, irrelevant, or adversarially manipulated retrieval contexts. Unlike models that optimize retrieval or generation under ideal assumptions, these systems explicitly address worst-case scenarios—such as hallucination under misleading evidence, retrieval failures, or corpus poisoning. We identify three major design strategies in this category: noise-adaptive training, hallucination-aware decoding constraints, and adversarial robustness. Noise-Adaptive Training Objectives: