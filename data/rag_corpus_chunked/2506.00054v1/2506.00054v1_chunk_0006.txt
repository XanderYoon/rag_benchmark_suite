retriever versatility—either by unifying reranking and generation within a single backbone or by training general-purpose retrievers across varied downstream tasks. Additionally, systems such as ToolRerank [88] dynamically adapt retrieval strategies based on query semantics, optimizing tool selection in specialized retrieval settings. Relatedly, SEER (Self-Aligned Evidence Extraction for RAG) [87] focuses on post-retrieval adaptation, advancing corpus-based evidence extraction by aligning evidence selection with faithfulness, helpfulness, and conciseness criteria, thereby improving evidence quality for open-domain and temporally sensitive queries. Granularity-Aware Retrieval: This pattern addresses retrieval precision by optimizing the unit of retrieval—from full documents to fine-grained, semantically aligned segments. LongRAG [31] exemplifies this by retrieving compressed long-context chunks, constructed through document grouping, to better exploit long-context language models. Similarly, FILCO (Filter Context) [69] enhances retrieval granularity by filtering irrelevant or low-utility spans from retrieved passages before generation, improving the faithfulness and efficiency of RAG outputs. In parallel, the Sufficient Context analysis framework [34] offers a complementary lens, evaluating whether retrieved contexts contain enough information to support accurate generation, thereby highlighting the importance of granular retrieval quality for system robustness. Each of these patterns anchors its innovation in the retriever, preserving modularity and interpretability. However, they also introduce trade-offs in latency, redundancy, and sensitivity to ambiguous or underspecified queries. Sec- tion 4 elaborates on how downstream enhancements—such as reranking, adaptive filtering, and utility-based context selection—further address these limitations. 3.2 Generator-Based RAG Systems Generator-based RAG systems concentrate architectural innovation on the decoding process, assuming the retrieved content is sufficiently relevant and shifting the burden of factual grounding and integration to the language model. These systems enhance output quality through mechanisms for self-verification, compression, and controlled generation. We identify three recurring design patterns within this category: faithfulness-aware decoding, context compression and utility filtering, and retrieval-conditioned generation control. Faithfulness-Aware Decoding: To reduce hallucinations and improve factual