using the same retrieval backbone—offers negligible gains, illustrating how prompt design alone can meaningfully impact robustness in multi-hop settings. In contrast, Stochastic RAG shows only marginal FactScore gains (≤+0.008) on Fever, suggesting that entropy-driven retrieval without subsequent verification may be insufficient to ensure factual reliability. Generator-based systems present more variable, task-dependent performance. SELF-RAG, evaluated on ASQA [60], achieves sizable improvements in precision (+22–30%) and recall (+16–19%), though its FactScore gains remain modest (+0.03–0.04), implying improved evidence usage without equivalent advances in factual accuracy. DRAGIN similarly improves precision and recall by +9–22% on HotPotQA, leveraging entropy-based token-level triggers suited for multi- hop reasoning. However, lacking reported FactScore, its contribution to factual consistency remains indeterminate. Other generator-oriented systems, including GenRT and Rich Answer Encoding, achieve smaller recall gains (≤+0.1) on datasets such as TriviaQA, KILT-WoW [49], and MSMARCO [3]. These modest improvements suggest better document selection but limited post-retrieval validation, constraining their robustness impact. Retriever-based systems exhibit consistent yet comparatively modest gains. Re2G reports +17.8% precision and +15.9% recall on TriviaQA, reflecting the benefits of retrieval-aware prompt optimization. FILCO, by contrast, improves precision by +3.25% on Fever but fails to enhance recall or FactScore, indicating that filtering irrelevant context improves selectivity, but without downstream verification, its robustness contribution is limited. Not all frameworks report all three metrics across datasets; while relative improvement facilitates normalization, incomplete coverage—particularly of FactScore—may obscure the full extent of a system’s capabilities. In sum, Self-CRAG on Biography delivers the strongest FactScore gain (+0.456), while SELF-RAG on ASQA achieves the best precision (+29.56%) and recall (+18.81%) improvements. Flare-Direct, outperforming Flare-Instruct by over 20% on 2Wiki, highlights the sensitivity of robustness to prompt design. At the lower end, Stochastic RAG on FEVER [65] records the smallest impact ( ≤+0.008 FactScore), reinforcing the necessity of combining retrieval strategies with downstream verification