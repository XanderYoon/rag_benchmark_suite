need for joint evaluation. Frameworks such as ARES and RAGAS have formalized these dimensions, incorporating both automated judgment and reference-free evaluation. 6.2 Automated Evaluation Frameworks ARES [55] introduces an LLM-based judge system that uses few-shot prompted language models to generate synthetic datasets. These judges are trained on three classification tasks corresponding to the core dimensions and use prediction- powered inference (PPI) to align model-based scoring with human judgment. ARES shows significant improvements in accuracy and annotation efficiency, outperforming RAGAS [17] by up to 59.3 percentage points in context relevance. RAGAS employs a modular framework that decomposes generated answers into atomic factual statements, then evaluates each against the retrieved context using LLMs. This structure provides high-resolution feedback, revealing which parts of an answer are hallucinated. These frameworks automate the evaluation of faithfulness, grounding, and contextual relevance—enabling scalable, reference-free analysis of RAG performance. 6.3 Evaluating Retrieval Quality eRAG [56] challenges traditional relevance label techniques by applying the RAG generator to each retrieved document individually. The performance of each document, assessed via downstream task metrics, serves as a relevance label. This method provides a retrieval-aware, document-level granularity and has shown significantly improved correlation with actual RAG performance. INFO-RAG introduces an unsupervised training paradigm that improves the LLM’s ability to refine retrieved information under three scenarios: redundant, noisy, or insufficient context. By viewing the LLM as an “information refiner, ” it enables the model to extract relevant content, reject misinformation, and infer missing details—enhancing retrieval robustness without supervised relevance labels. uRAG proposes a unified retrieval system that serves multiple RAG models across diverse downstream tasks. It introduces a shared reranker trained on feedback signals (e.g., EM, accuracy) from various black-box LLMs, treating each LLM as a user of the search engine. uRAG’s training protocol enables evaluation and optimization of retrieval Manuscript submitted to ACM