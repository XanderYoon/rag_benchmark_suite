architectural optimization in production RAG systems. Robustness and Security. Studies like BadRAG and TrojanRAG emphasize that security-focused ablations reveal novel vulnerabilities. Even lightweight retrieval poisoning or trigger crafting can steer model outputs, while mitigation strategies (e.g., summarization, distance thresholds) offer partial resilience but require further study. Synthesis. Ablation studies consistently reinforce that high-performing RAG frameworks are modular, with com- plementary retrieval, filtering, and generation components. Performance degradation in ablation settings not only validates novel modules but also guides design toward more interpretable, efficient, and secure RAG pipelines. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 18 6 Evaluation and Benchmarking of RAG Systems Retrieval-Augmented Generation (RAG) systems introduce unique challenges for evaluation due to their hybrid architecture combining a retriever and a generator. Accurate evaluation demands assessing multiple interdependent components, including retrieval relevance, faithfulness of generated responses, and overall answer utility. In this section, we synthesize recent advancements in automated evaluation frameworks, retrieval quality assessment techniques, and benchmark construction to provide a comprehensive overview of evaluation practices in RAG systems. 6.1 Evaluation Dimensions The core dimensions [55] used to evaluate RAG systems include: (1) Context Relevance: Measures how pertinent the retrieved documents are to the input query. (2) Answer Faithfulness: Assesses whether the generated output remains grounded in the retrieved evidence. (3) Answer Relevance: Evaluates whether the output adequately addresses the user query. These dimensions are interdependent: poor context relevance often cascades into reduced faithfulness and answer relevance, underscoring the need for joint evaluation. Frameworks such as ARES and RAGAS have formalized these dimensions, incorporating both automated judgment and reference-free evaluation. 6.2 Automated Evaluation Frameworks ARES [55] introduces an LLM-based judge system that uses few-shot prompted language models to generate synthetic datasets. These judges are trained on three classification tasks corresponding to the core dimensions and use prediction- powered inference