208% improvement on ARC-Challenge, suggesting that combining retrieval refinement with generation adaptation can be highly effective when well aligned. However, TA-ARE, despite achieving a significant 28× improvement over raw baselines on RetrievalQA, occasionally underperforms relative to the standard retrieval baseline, indicating that retrieval frequency reduction strategies, while efficient, may introduce trade-offs. Stochastic RAG frameworks, meanwhile, display relatively modest gains (typically under 4%), reflecting that introducing retrieval randomness increases diversity without consistently boosting short-form QA accuracy. Efficiency-focused generator-based systems such as xRAG exhibit mixed results. While xRAG achieves 10–29% improvements over raw LLM baselines on datasets such as NQ and TriviaQA, its gains over retrieval baselines are marginal or occasionally negative. This suggests that while resource-efficient designs are promising for scaling RAG systems, further optimization is needed to maintain competitive factual accuracy in short-form tasks. Finally, robustness-oriented frameworks such as RAAT demonstrate strong performance, with a 116% improvement from the raw baseline and over 27% gain compared to retrieval on RAG-Bench—a robustness-focused variant of NQ, WebQ, and TriviaQA. Although evaluated under challenging retrieval noise conditions, RAAT’s results suggest that robustness-driven retrieval strategies can effectively complement factual QA objectives. Overall, retrieval- and generation-enhanced frameworks deliver substantial relative gains in short-form QA, while hybrid and efficiency-focused approaches offer promising but variable results depending on dataset and retrieval complexity. These findings underscore the critical role of retrieval optimization and generation-adaptive strategies in advancing retrieval-augmented short-form question answering. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 15 5.2 Comparative Analysis of Framework Performance on Multi-Hop QA A comparative evaluation of various Retrieval-Augmented Generation (RAG) frameworks reveals distinct patterns in their ability to enhance multi-hop question answering, assessed through improvements over both raw large language models (LLMs) and standard retrieval-augmented baselines. Similar to the previous section, this analysis focuses on relative gains rather than absolute scores