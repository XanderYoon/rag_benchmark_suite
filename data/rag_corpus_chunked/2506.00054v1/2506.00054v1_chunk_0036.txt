retrieval robustness without supervised relevance labels. uRAG proposes a unified retrieval system that serves multiple RAG models across diverse downstream tasks. It introduces a shared reranker trained on feedback signals (e.g., EM, accuracy) from various black-box LLMs, treating each LLM as a user of the search engine. uRAG’s training protocol enables evaluation and optimization of retrieval Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 19 based on downstream task performance, offering retrieval diagnostics grounded in actual utility rather than surface similarity. 6.4 Benchmarking RAG Capabilities As RAG systems mature, a growing suite of benchmarks has emerged to evaluate them across dimensions like robustness, factuality, adaptivity, and domain sensitivity. These benchmarks not only reflect the evolving needs of real-world RAG deployments but also shape future directions by surfacing recurrent failure modes and task-specific limitations. Robustness to retrieval noiseis a core requirement in operational RAG systems. RGB [7] evaluates four fundamental capacities—noise robustness, negative rejection, information integration, and counterfactual resistance—revealing consistent weaknesses in LLMs when handling distracting or misleading context. Complementing this, RAG-Bench [18] introduces a noise-centric benchmark simulating three retrieval corruption types—relevant-but-incomplete, irrelevant, and counterfactual—and applies adaptive adversarial training to improve model tolerance. These benchmarks enable fine-grained analysis of how retrieval perturbations degrade end-task performance and inform robust retrieval-policy design. Faithfulness and hallucination detection benchmarks have taken center stage in evaluating generation quality. RAGTruth [46] provides nearly 18,000 annotated examples from QA, summarization, and data-to-text generation, offering both response- and span-level hallucination labels across four types: subtle vs. evident, and conflict vs. baseless information. Uniquely, it supports training hallucination detectors and benchmarking span-level detection precision and recall—tasks not addressed by coarse-grained metrics. This makes it foundational for measuring factual integrity in RAG outputs. Reasoning and retrieval chaining are central to multi-hop question answering, where evidence spans multiple documents. MultiHop-RAG [63] targets