tion Higher latency Iterative reasoning Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 13 Table 2. Comparative Performance of Retrieval-Augmented Generation Frameworks Across Multi-Hop and Short-Form QA Benchmarks. This table reports relative performance improvements achieved by each RAG framework over two baselines: (i) the raw backbone language model (B) and (ii) the same model augmented with a standard retrieval module (B+R). Results are shown across multi-hop benchmarks (HotpotQA [81], 2Wiki [23], MuSiQue [67]) and short-form QA datasets (PopQA [42], TriviaQA [35], ARC-Challenge [12], NQ [ 37]), with metrics including F1, Exact Match (EM), and Accuracy (Acc). Frameworks are grouped by architectural category: retriever-based, generator-based, and hybrid. A “–” indicates that the corresponding score was not reported in the original publication. Backbone LLMs referenced in this table include LLaMA 2 [66], LLaMA 3 [21], GPT-3.5/4 [47], Vicuna [11], Mistral [29], Mixtral [30], Gemini [53], and Gemma [64]. Framework Backbone HotpotQA 2Wiki MusiQue PopQA TriviaQA ARC-Challenge NQ B/B+R B/B+R B/B+R B/B+R B/B+R B/B+R B/B+R Retriever-Based RAG RQ-RAG LLaMA2-7B 8.485/2.749 (F1) 1.8/1.396 (F1) 12.9/4.635 (F1) 2.884/0.434 (Acc) -/- 2.133/1.379 (Acc) -/- SimRAG LLaMA3-8B -/- -/- -/- -/- -/- 0.145/- (Acc) -/- SimRAG Gemma2-27B -/- -/- -/- -/- -/- 0.034/- (Acc) -/- SEER LLaMA2-7B-Chat 0.104/0.037 (F1) -/- -/- -/- -/- -/- -/- RankRAG LLaMA3-8B -/0.079 (F1) -/0.323 (F1) -/- -/- -/- -/- -/- RankRAG LLaMA3-70B -/0.242 (F1) -/0.376 (F1) -/- -/- -/- -/- -/- LQR LLaMA3-8B 2.081/0.516 (F1) 0.706/0.141 (F1) 2.922/0.841 (F1) -/- -/- -/- -/- LongRAG GPT-4o 0.517/- (EM) -/- -/- -/- -/- -/- -/- LongRAG Gemini-1.5-Pro 0.696/- (EM) -/- -/- -/- -/- -/- -/- FILCO LLaMA2-7B -/0.057 (EM) -/- -/- -/- -/0.056 (EM) -/- -/0.298 (EM) Re2G BART Large -/- -/- -/- -/- 0.251/- (Acc) -/- 0.144/- Generator-Based RAG xRAG Mistral-7B 0.26/-0.122 (EM) -/- -/- -/- 0.152/-0.002 (EM) -/- 0.293/-0.085 (EM) xRAG