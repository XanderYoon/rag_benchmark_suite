discard useful docs Long-context tasks Sparse Selection R2AG Context-aware retrieval injec- tion Enhances coherence, lowers re- dundancy Retriever fine-tuning needed Knowledge-intensive QA Inference Acceler- ation FiD-Light Compresses passages Faster decoding Slight loss in recall Low-latency applications Caching Speculative Pipelining Overlaps retrieval and genera- tion 20–50% TTFT reduction Risk of hallucination Real-time applications Caching RAGCache Hierarchical cache w/ PGDSF Eliminates recomputation Cache complexity in long-tail High-throughput workloads Retrieval Quality RAE Retriever-as-answer scorerBoosts grounding and preci- sion Requires scoring/retraining Factual QA Robustness Noise Mitigation RAAT Adversarial training +20–30% F1/EM High training cost Offline pretraining Noise Mitigation CRAG Inference-time filtering +12–18% precision gain Ineffective on “soft” noise Real-time support Hallucination Control Structured RAG Curated corpus retrieval30–40% hallucination reductionLow adaptability Static domains Hallucination Control IM-RAG Iterative retrieval refinement +5.3 F1 / +7.2 EM Inference latency Multi-hop QA Security BadRAG Adversarial retrieval poisoningDemonstrates corpus-level threat Needs stronger filtering Security evaluation Security TrojanRAG Embedding-level backdoor Persistent attack vector Requires secure training Security-sensitive pipelines Reranking Adaptive RLT Dynamic list truncation +15% noise reduction Heuristic tuning needed Real-time QA Adaptive ToolRerank Familiarity-aware reranking +12% recall for unseen toolsComplexity for un- seen/frequent tools Tool-aware retrieval Unified Pipeline RankRAG Joint rerank + generate +7.8% MRR@10 Domain-specific tuning End-to-end QA systems Unified Pipeline uRAG Shared reranking engine +8% MRR@10, task generaliza- tion Higher setup cost Multi-task enterprise RAG Fusion-based RAG-Fusion Reciprocal rank fusion +9% accuracy Query explosion risk Complex multi-hop QA Fusion-based R2AG Recursive reranking refine- ment 15% irrelevant retrieval reduc- tion Higher latency Iterative reasoning Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 13 Table 2. Comparative Performance of Retrieval-Augmented Generation Frameworks Across Multi-Hop and Short-Form QA Benchmarks. This table reports relative performance improvements achieved by each RAG framework over two baselines: (i) the raw backbone language model (B) and (ii) the same model augmented with a standard retrieval module