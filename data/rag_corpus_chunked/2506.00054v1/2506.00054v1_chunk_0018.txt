RAG systems improve factual accuracy in language models by retrieving external information. However, they remain vulnerable to retrieval noise, hallucinations, and adversarial attacks. While past research has addressed these challenges separately—such as noise resilience, hallucination control, and retrieval security—a unified perspective is essential. This section groups robustness techniques into three areas: noise mitigation, hallucination reduction, and security defenses. Empirical studies further support this need for a unified view; a recent study identifies seven recurrent failure points in operational RAG systems, spanning retrieval errors, context consolidation failures, hallucinated outputs, and incomplete answers [4]. Noise mitigation strategies target irrelevant, misleading, or adversarial content that can degrade RAG accuracy. However, recent work challenges the assumption that all retrieval noise is detrimental; Cuconasu et al. [14] demonstrate that carefully positioned random documents can paradoxically improve LLM reasoning and answer quality by promoting evidence selection behaviors. Two contrasting approaches address this: RAAT and CRAG. RAAT uses adversarial pretraining to expose models to subtle and counterfactual retrieval noise, improving F1/EM scores by 20–30%. Its high training cost limits it to static, high-stakes domains. CRAG filters low-confidence retrievals at inference time and Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 11 works well in real-time systems, reducing retrieval errors by 12–18%. However, it struggles with “soft noise, ” where superficially relevant content misleads the model. Hallucination reduction techniques such as Structured RAG [2] and IM-RAG aim to improve the faithfulness of generated content. Structured RAG constrains retrieval to verified corpora, lowering hallucination rates by 30–40% with minimal compute cost. Its drawback is poor adaptability, requiring manual updates. IM-RAG uses iterative retrieval refinement, achieving +5.3 F1 / +7.2 EM on HotPotQA. Though more accurate in evolving domains, it is computationally intensive and slower at inference. Security defenses focus on adversarial threats such as data poisoning and backdoor attacks.