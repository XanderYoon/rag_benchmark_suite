While lexical filters are efficient, self-supervised models provide deeper semantic filtering and support long-form reasoning. 4.3 Efficiency Enhancements While Retrieval-Augmented Generation (RAG) significantly enhances factual consistency in large language models (LLMs) by integrating external document retrieval, it introduces new inefficiencies. These include increased memory overhead, latency from retrieval-processing pipelines, and redundancy in passage selection. This section synthesizes key research efforts aimed at improving retrieval efficiency across four areas: sparse retrieval and context selection, inference acceleration, caching and redundancy reduction, and retrieval faithfulness. Sparse context selection and retrieval-aware generation techniques aim to reduce the input length and improve semantic alignment without sacrificing output quality. Sparse RAG addresses this by filtering low-relevance content before self-attention, retaining only high-signal tokens via parallel encoding. While it builds on Fusion-in-Decoder (FiD), it improves efficiency by avoiding dense input concatenation. However, it may discard useful context under suboptimal retrieval, requiring fine-tuning to maintain robustness. R2AG takes a complementary approach by embedding retrieval Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 10 representations directly into the LLM‚Äôs context space, enhancing semantic alignment. Unlike prompt-based methods (e.g., REPLUG [58]), R2AG bypasses explicit concatenation, reducing redundant processing. Both approaches enhance efficiency at different stages but require retriever fine-tuning and increase model complexity. Inference acceleration strategies focus on reducing decoding latency in autoregressive models by minimizing redundant token processing. FiD-Light achieves this through token-level passage compression, which lowers decoding time while preserving key information. Though effective, aggressive filtering can marginally reduce retrieval precision. Speculative Pipelining [71] further reduces latency by overlapping retrieval and generation. It incrementally processes top-ùëò candidates before retrieval completes, lowering time-to-first-token (TTFT) by 20‚Äì30%. However, it risks speculative hallucinations unless controlled by fallback mechanisms and selective decoding checkpoints. This line of work opens the door for future speculative decoding architectures‚Äîdiscussed in Section 8‚Äîthat balance responsiveness and reliability