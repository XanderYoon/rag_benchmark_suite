RAG frameworks. These values were used to compute the normalized improvements presented in Section 5. Taxonomy Framework Backbone Dataset Metric Raw LLM LLM+Retrieval Framework Score Retriever-Based RAG RQ-RAG LLaMA2-7B PopQA Acc 14.7 39.8 57.1 RQ-RAG LLaMA2-7B ARC-Challenge Acc 21.8 28.7 68.3 SimRAG LLaMA3-8B ARC-Challenge Acc – 71.08 81.4 SimRAG LLaMA3-8B SciQ EM – 20.8 57.5 SimRAG Gemma2-27B ARC-Challenge Acc – 85.75 88.65 SimRAG Gemma2-27B SciQ EM – 44.8 58.1 Re2G BART Large NQ Acc 45.22 – 51.73 Re2G BART Large TriviaQA Acc 60.99 – 76.27 FILCO LLaMA2-7B (Top-5) NQ EM – 47.6 61.8 FILCO LLaMA2-7B (Top-5) TriviaQA EM – 67.3 71.1 Generator-Based RAG SELF-RAG LLaMA2-7B PopQA Acc 14.7 38.2 54.9 SELF-RAG LLaMA2-7B TriviaQA Acc 30.5 42.5 66.4 SELF-RAG LLaMA2-7B ARC-Challenge Acc 21.8 48.0 67.4 SELF-RAG LLaMA2-13B PopQA Acc 14.7 45.7 55.8 SELF-RAG LLaMA2-13B TriviaQA Acc 38.5 47.0 69.3 xRAG Mistral-7B NQ EM 30.25 42.71 39.1 xRAG Mistral-7B TriviaQA EM 57.08 65.88 65.77 xRAG Mistral-7B WebQA EM 34.89 37.84 39.4 xRAG Mixtral-8x7B NQ EM 41.99 45.15 47.28 xRAG Mixtral-8x7B TriviaQA EM 71.1 70.34 74.14 xRAG Mixtral-8x7B WebQA EM 40.31 41.26 44.5 FiD-Light FiD+DPR TriviaQA EM 48.6 – 57.6 FiD-Light FiD+DPR NQ EM 41.9 – 53.2 R2AG LLaMA2-7B NQ Acc 0.38 – 0.693 SELF-RAG LLaMA2-7B NQ Acc 0.38 – 0.188 Hybrid RAG Stochastic RAG FiD-Light (T5-Base) NQ EM – 45.6 46.2 Stochastic RAG FiD-Light (T5-Base) TriviaQA EM – 57.6 59.7 Stochastic RAG FiD-Light (T5-XL) NQ EM – 51.1 53.0 Stochastic RAG FiD-Light (T5-XL) TriviaQA EM – 63.7 64.7 CRAG LLaMA2-7B NQ Acc 0.38 – 0.397 CRAG LLaMA2-7B PopQA Acc 14.7 40.3 59.3 CRAG LLaMA2-7B ARC-Challenge Acc 21.8 46.7 54.8 Self-CRAG LLaMA2-7B PopQA Acc 14.7 40.3 61.8 Self-CRAG LLaMA2-7B ARC-Challenge Acc 21.8 46.7 67.2 TA-ARE GPT-3.5 RetrievalQA Acc 1.2 38.2 35.8 TA-ARE GPT-4 RetrievalQA Acc 2.4 46.0 46.4 TA-ARE LLaMA2-7B RetrievalQA Acc 2.0