Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, Torino, Italia, 16263–16273. https://aclanthology.org/2024.lrec-main.1413/ [89] Kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang, Qianglong Chen, Zheng Chu, Jingchang Chen, and Bing Qin. 2024. An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1044–1069. doi:10.18653/v1/2024.acl-long.59 [90] Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, and Jindong Chen. 2025. Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=HE6pJoNnFp A Appendix To support transparency and reproducibility, we include in the Appendix the original benchmark scores reported in the primary publications of each RAG framework. These tables serve as the empirical source for the relative improvement Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 29 analyses presented in Sections 5. All values are cited from original papers, preserving reported metrics such as F1, EM, Accuracy, and FactScore. Where applicable, dataset splits, backbone models, and evaluation metrics are clearly labeled to ensure traceability. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 30 Table 5. Reported Performance Scores for Short-Form QA Frameworks. Accuracy and Exact Match (EM) scores as reported in the original publications of short-form RAG frameworks. These values were used to compute the normalized improvements presented in Section 5. Taxonomy Framework Backbone Dataset Metric Raw LLM LLM+Retrieval Framework Score Retriever-Based RAG RQ-RAG LLaMA2-7B PopQA Acc 14.7 39.8 57.1 RQ-RAG LLaMA2-7B ARC-Challenge Acc 21.8 28.7 68.3 SimRAG LLaMA3-8B ARC-Challenge Acc – 71.08 81.4 SimRAG LLaMA3-8B SciQ EM – 20.8 57.5 SimRAG Gemma2-27B ARC-Challenge Acc –