benefits of tightly integrating retrieval signals into the generation process. In contrast, INFO-RAG exhibits more modest improvements, with relative gains around 16–35% across different backbones and datasets, suggesting that while generator-side augmentations enhance output faithfulness, their standalone effect may be limited without concurrent retrieval refinement. xRAG, while improving from raw baselines by approximately 20–26%, shows negative or marginal gains compared to the retrieval baseline in some settings, indicating that extreme context compression, although efficient, may compromise the model’s ability to utilize retrieved evidence effectively for complex multi-hop reasoning. Hybrid RAG frameworks, such as DRAGIN, FLARE, GenGround, and Stochastic RAG, present a diverse range of outcomes. DRAGIN frameworks achieve moderate improvements, typically ranging between 22–44% over raw LLMs and 14–34% over retrieval baselines, reflecting the incremental gains from dynamically adapting retrieval to evolving information needs. FLAREdirect stands out, achieving a 62% improvement from the raw LLM and a 22% improvement over standard retrieval on 2Wiki [23], suggesting that model-guided active retrieval significantly strengthens multi-hop evidence gathering. GenGround reports relatively smaller improvements (13–36% from the baseline) but is evaluated against already-strong baselines, which partially accounts for the more conservative gains. Stochastic RAG frameworks offer consistent yet modest gains (6%), indicating that introducing randomness into retrieval can modestly diversify and enhance evidence coverage without destabilizing performance. Overall, retrieval-based RAG frameworks demonstrate the most consistent and substantial improvements across multi-hop QA tasks, particularly when retrieval quality, ranking, and query decomposition are optimized. Generator- based adaptations, while beneficial in specific cases, often require complementary retrieval-side enhancements to realize their full potential. Hybrid frameworks offer promising but more variable results, underscoring the challenge of harmonizing retrieval and generation strategies dynamically. These findings highlight retrieval optimization as a critical lever for advancing complex reasoning capabilities in RAG systems. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 16