RAG pipelines, especially retrieval inconsistency and hallucination amplification due to poor corpus coordination. Evaluation infrastructure and reproducibility are addressed by BERGEN [52], a benchmarking library designed to unify assessment across RAG components. It offers modular templates for measuring retrieval precision, generation faithfulness, and their interplay across datasets and model configurations. BERGEN facilitates consistent and extensible RAG benchmarking in both academic and applied settings. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 20 Table 4. Emerging Benchmarks for Evaluating Retrieval-Augmented Generation (RAG) Systems.This table summarizes recent benchmarks developed to assess key aspects of RAG systems, including robustness, multi-hop reasoning, medical-domain adaptation, and federated retrieval. These benchmarks differ in evaluation granularity—ranging from query-level to document-level—and employ varied annotation methods such as manual labeling, programmatic perturbation, and LLM-based scoring. Distinctive features, such as noise stress-testing (RGB), zero-shot medical QA (MIRAGE), and federated source merging (FeB4RAG), support targeted evaluations of both retriever components and full RAG pipelines. Benchmark Evaluation Focus Granularity Annotation Type Unique Features Evaluation Tar- get RGB Robustness (noise, inte- gration, hallucination) Query-context pair None Stress tests for noise, contradiction, and multi- source fusion Full pipeline MultiHop-RAG Multi-hop reasoning and retrieval chaining Document-level Manual + derivedLinked multi-hop queries and bridge-entity chaining Full pipeline RAGTruth Hallucination detec- tion and factuality evaluation Response-level (yes/no), span- level (exact) Human-labeled 18,000+ examples, 4 hallu- cination types, span-level F1 Generator MIRAGE Medical domain QA under real-world con- straints Query-level Dataset-native Zero-shot, multi-choice, question-only retrieval (MEDRAG) Full pipeline FeB4RAG Federated retrieval evaluation Document + re- source LLM-labeled Measures retrieval + merg- ing across 16 BEIR sources Retriever RetrievalQA Adaptive retrieval ne- cessity detection Query-level Derived Queries with and without need for retrieval Retriever RAG-Bench Retrieval robustness to noise Query-level Programmatic Irrelevant, incomplete, and counterfactual retrieval noise Full pipeline BERGEN Retrieval, generation, and joint evaluation Query-context and document- level