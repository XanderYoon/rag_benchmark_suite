preserve output quality in the face of noisy, irrelevant, or adversarially manipulated retrieval contexts. Unlike models that optimize retrieval or generation under ideal assumptions, these systems explicitly address worst-case scenariosâ€”such as hallucination under misleading evidence, retrieval failures, or corpus poisoning. We identify three major design strategies in this category: noise-adaptive training, hallucination-aware decoding constraints, and adversarial robustness. Noise-Adaptive Training Objectives: These systems aim to make RAG outputs resilient to degraded or spurious input evidence by training under perturbed, irrelevant, or misleading contexts. RAAT [18] classifies retrieved passages into relevant, irrelevant, or counterfactual categories and introduces an adversarial training objective to maximize worst- case performance. Bottleneck Noise Filtering [89] applies information bottleneck theory to identify the intersection of useful and noisy information, compressing retrieved context into minimal, high-utility representations. These approaches are particularly effective in retrieval-heavy pipelines where context precision cannot be guaranteed. Hallucination-Aware Decoding Constraints: To mitigate factual inaccuracies in generation, several systems in- troduce decoding-time constraints or architectures designed to enforce grounding. RAGTruth [46] provides benchmarks and evaluation protocols for hallucination detection, guiding system-level design. Structured retrieval-based approaches have also been explored: one method [24] retrieves executable templates (e.g., JSON workflows) to constrain output generation, minimizing reliance on generative interpolation and reducing domain-specific hallucination. RAG-Ex [62] simulates retrieval variability by injecting perturbed documents during training, improving robustness to inconsistent or adversarial context. In high-stakes domains such as healthcare, Confidence-Calibrated RAG [ 48] explores how document ordering and prompt design affect both answer accuracy and model certainty. Adversarial Robustness and Security: Emerging work also highlights new vulnerabilities. BadRAG [ 78] and TrojanRAG [8] demonstrate that adversarially poisoned passages can serve as semantic backdoors, triggering spe- cific behaviors in LLM outputs even when base models remain unmodified. These attacks rely on stealthy corpus manipulations that are hard to detect and pose