Survey 15 5.2 Comparative Analysis of Framework Performance on Multi-Hop QA A comparative evaluation of various Retrieval-Augmented Generation (RAG) frameworks reveals distinct patterns in their ability to enhance multi-hop question answering, assessed through improvements over both raw large language models (LLMs) and standard retrieval-augmented baselines. Similar to the previous section, this analysis focuses on relative gains rather than absolute scores to normalize for architectural and experimental variations. The results, summarized in Table 2, enable a consistent comparison of framework contributions across diverse multi-hop QA settings. Among retrieval-based RAG systems, models such as RQ-RAG, RankRAG, LQR, and LongRAG demonstrate substantial relative gains. Notably, RQ-RAG achieves over an 800% improvement from its raw LLM baseline on HotpotQA [81], and a 275% improvement over standard retrieval, highlighting the effectiveness of sophisticated query decomposition techniques in multi-hop settings. Similarly, LQR achieves a 292% improvement from the raw baseline and an 84% improvement over retrieval in the MuSiQue dataset [67], suggesting that intelligent retrieval-ranking substantially boosts multi-hop reasoning. LongRAG also exhibits strong performance, improving by over 50% from the raw LLM baseline on HotpotQA, further emphasizing the value of extended retrieval for complex question answering. These patterns collectively affirm that optimizing retrieval quality remains a dominant driver of performance gains in multi-hop RAG applications. Generator-based RAG frameworks, including R2AG, INFO-RAG, and xRAG, display more varied relative improve- ments. R2AG shows consistent strong gains, improving by over 300% relative to the baseline on HotpotQA, demonstrating the benefits of tightly integrating retrieval signals into the generation process. In contrast, INFO-RAG exhibits more modest improvements, with relative gains around 16–35% across different backbones and datasets, suggesting that while generator-side augmentations enhance output faithfulness, their standalone effect may be limited without concurrent retrieval refinement. xRAG, while improving from raw baselines by approximately 20–26%, shows negative or marginal gains compared