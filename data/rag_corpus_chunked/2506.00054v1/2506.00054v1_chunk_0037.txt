hallucination labels across four types: subtle vs. evident, and conflict vs. baseless information. Uniquely, it supports training hallucination detectors and benchmarking span-level detection precision and recall—tasks not addressed by coarse-grained metrics. This makes it foundational for measuring factual integrity in RAG outputs. Reasoning and retrieval chaining are central to multi-hop question answering, where evidence spans multiple documents. MultiHop-RAG [63] targets this challenge through linked question-answer pairs, bridge entities, and explicit multi-hop query types, enabling systematic assessment of retrieval chaining, evidence linking, and document-level reasoning—all key bottlenecks in complex RAG workflows. Adaptive retrieval and necessity estimation are benchmarked in RetrievalQA [86], which mixes queries requiring external retrieval with those answerable via the base LLM alone. This design tests whether models can intelligently toggle retrieval based on query uncertainty, supporting the development of resource-efficient, retrieval-aware systems that avoid introducing unnecessary context. Domain-specific evaluation is exemplified by MIRAGE [48], a benchmark tailored to medical RAG. It contains 7,663 questions sourced from five clinical and biomedical QA datasets and incorporates real-world evaluation constraints: zero-shot generalization, multiple-choice formats, retrieval necessity assessment, and question-only retrieval. This multi-faceted setup tests reliability under high-stakes conditions where factual errors can be consequential. Cross-corpus and federated retrieval are explored in FeB4RAG [68], a benchmark constructed from 16 BEIR sub- collections. It evaluates federated retrieval through 790 conversational queries with LLM-graded relevance judgments and quantifies the impact of resource selection and result merging strategies. This benchmark surfaces key risks in multi-source RAG pipelines, especially retrieval inconsistency and hallucination amplification due to poor corpus coordination. Evaluation infrastructure and reproducibility are addressed by BERGEN [52], a benchmarking library designed to unify assessment across RAG components. It offers modular templates for measuring retrieval precision, generation faithfulness, and their interplay across datasets and model configurations. BERGEN facilitates consistent and extensible RAG benchmarking in both academic