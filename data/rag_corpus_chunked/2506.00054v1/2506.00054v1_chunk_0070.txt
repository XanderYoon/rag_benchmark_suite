(T5-XL) TriviaQA EM – 63.7 64.7 CRAG LLaMA2-7B NQ Acc 0.38 – 0.397 CRAG LLaMA2-7B PopQA Acc 14.7 40.3 59.3 CRAG LLaMA2-7B ARC-Challenge Acc 21.8 46.7 54.8 Self-CRAG LLaMA2-7B PopQA Acc 14.7 40.3 61.8 Self-CRAG LLaMA2-7B ARC-Challenge Acc 21.8 46.7 67.2 TA-ARE GPT-3.5 RetrievalQA Acc 1.2 38.2 35.8 TA-ARE GPT-4 RetrievalQA Acc 2.4 46.0 46.4 TA-ARE LLaMA2-7B RetrievalQA Acc 2.0 36.0 30.7 Robustness-Based RAG RAAT LLaMA2-7B RAG-Bench (TQA/NQ/WebQ) EM 38.37 65.4 83.07 Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 31 Table 6. Reported Performance Scores for Multi-Hop QA Frameworks. Raw F1 and EM scores extracted from the original papers of multi-hop RAG systems, across datasets such as HotpotQA, 2Wiki, and MuSiQue. These scores form the basis of the comparative analysis in Section 5. Taxonomy Framework Backbone Dataset Metric Raw LLM LLM + Retrieval Framework Score Retriever-Based RQ-RAG LLaMA2-7B HotpotQA F1 6.6 16.7 62.6 RQ-RAG LLaMA2-7B 2Wiki F1 16 18.7 44.8 RQ-RAG LLaMA2-7B MuSiQue F1 3 7.4 41.7 RankRAG LLaMA3-8B HotpotQA F1 – 43.3 46.7 RankRAG LLaMA3-8B 2Wiki F1 – 27.9 36.9 RankRAG LLaMA3-70B HotpotQA F1 – 44.6 55.4 RankRAG LLaMA3-70B 2Wiki F1 – 31.9 43.9 LQR LLaMA3-8B MuSiQue F1 10.7 22.8 41.97 LQR LLaMA3-8B HotpotQA F1 22.71 46.15 69.96 LQR LLaMA3-8B 2Wiki F1 32.04 47.9 54.65 LongRAG GPT-4o HotpotQA EM 42.4 – 64.3 LongRAG Gemini-1.5-Pro HotpotQA EM 33.9 – 57.5 SEER LLaMA2-7B-Chat HotpotQA F1 0.5471 0.5826 0.604 FILCO LLaMA2-7B HotpotQA EM – 61.5 65 Generator-Based R2AG LLaMA2-7B HotpotQA F1 8.52 – 36.05 R2AG LLaMA2-7B MuSiQue F1 2.41 – 16.87 R2AG LLaMA2-7B 2Wiki F1 6.34 – 34.52 xRAG Mistral-7B HotpotQA EM 27.02 38.79 34.05 xRAG Mixtral-8x7B HotpotQA EM 32.87 43.46 39.66 INFO-RAG LLaMA2-7B HotpotQA EM 39.4 – 46.56 INFO-RAG LLaMA2-13B HotpotQA EM 42.12 – 51.48 INFO-RAG LLaMA2-13B-chat HotpotQA EM 61.23 – 61.91 INFO-RAG LLaMA2-7B MuSiQue EM 25.95 – 30.19 INFO-RAG