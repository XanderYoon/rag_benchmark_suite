to verified corpora, lowering hallucination rates by 30–40% with minimal compute cost. Its drawback is poor adaptability, requiring manual updates. IM-RAG uses iterative retrieval refinement, achieving +5.3 F1 / +7.2 EM on HotPotQA. Though more accurate in evolving domains, it is computationally intensive and slower at inference. Security defenses focus on adversarial threats such as data poisoning and backdoor attacks. Research into BadRAG shows that poisoning just 0.04% of a corpus can lead to a 98.2% attack success rate and 74.6% system failure. Defenses like cryptographic document signing or adversarial filtering are only partially effective. TrojanRAG embeds backdoors in retrieval embeddings, bypassing traditional sanitization. Stronger mitigations—secure training and integrity validation— are needed but require proactive design. Beyond adversarial attacks, privacy vulnerabilities in RAG systems have also been identified; Zeng et al. [85] show that both retrieval databases and pretraining corpora can be exploited through structured prompting, although retrieval can paradoxically help reduce memorization leakage by acting as a grounding mechanism. 4.5 Enhancements and Optimizations in Reranking Reranking plays a vital role in improving the relevance and faithfulness of Retrieval-Augmented Generation (RAG) outputs. While initial retrieval stages often return noisy results, reranking refines document ordering before passage selection and generation, reducing hallucinations and improving response accuracy. Recent work advances reranking across three key areas: adaptive reranking, unified pipelines, and fusion-based reranking. Adaptive reranking methods dynamically adjust the number of documents reranked based on query complexity. RLT [44] uses ranked list truncation to improve MRR/nDCG while reducing retrieval noise by 15%. ToolRerank further adapts reranking depth based on familiarity with seen vs. unseen tools, boosting recall by 12% in hierarchical retrieval tasks. These methods optimize computation by avoiding unnecessary reranking in low-complexity scenarios. Unified reranking pipelines combine retrieval, document ranking, and generation within a single architecture. RankRAG fine-tunes a language model to