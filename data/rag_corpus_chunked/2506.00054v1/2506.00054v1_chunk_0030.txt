adaptations, while beneficial in specific cases, often require complementary retrieval-side enhancements to realize their full potential. Hybrid frameworks offer promising but more variable results, underscoring the challenge of harmonizing retrieval and generation strategies dynamically. These findings highlight retrieval optimization as a critical lever for advancing complex reasoning capabilities in RAG systems. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 16 5.3 Comparative Robustness Analysis: Framework Gains Over Retrieval-Only Baselines To assess robustness in Retrieval-Augmented Generation (RAG) systems, we report incremental improvements each framework achieves over its retrieval-augmented LLM baseline. This isolates the added value of mechanisms such as critique, reranking, and filtering, independent of the baseline retrieval gain. Evaluations span multiple datasets and focus on gains in precision, recall, and FactScore. By standardizing on relative improvements, the analysis enables fair comparisons across models with differing backbone architectures. A summary of these results is provided in Table 3. Among hybrid systems, the most substantial gains in factual consistency are observed. Self-CRAG yields the highest FactScore improvement—+0.456 on the Biography dataset [ 45]—significantly surpassing other frameworks, most of which report ≤0.05 gains. The multi-sentence compositional nature of the Biography task likely benefits from Self- CRAG’s feedback-based reranking and correction loop, which aligns generation with retrieved evidence. Comparable improvements are evident with Self-RAG and CRAG, reporting +0.372 and +0.252 gains on the same dataset, underscoring the importance of evidence-aware generation refinement. On 2Wiki, Flare-Direct improves both precision and recall by +21.6%, while Flare-Instruct—despite using the same retrieval backbone—offers negligible gains, illustrating how prompt design alone can meaningfully impact robustness in multi-hop settings. In contrast, Stochastic RAG shows only marginal FactScore gains (≤+0.008) on Fever, suggesting that entropy-driven retrieval without subsequent verification may be insufficient to ensure factual reliability. Generator-based systems present more variable, task-dependent performance. SELF-RAG, evaluated on ASQA [60], achieves sizable improvements