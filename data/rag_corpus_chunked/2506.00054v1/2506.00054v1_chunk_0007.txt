the retrieved content is sufficiently relevant and shifting the burden of factual grounding and integration to the language model. These systems enhance output quality through mechanisms for self-verification, compression, and controlled generation. We identify three recurring design patterns within this category: faithfulness-aware decoding, context compression and utility filtering, and retrieval-conditioned generation control. Faithfulness-Aware Decoding: To reduce hallucinations and improve factual grounding, several systems embed mechanisms for self-reflection, verification, or correction during generation. SELF-RAG (Self-Reflective RAG) [ 1] introduces a critique–generate loop, allowing the model to assess and revise its outputs before finalization. SelfMem [9] builds on this by incorporating a self-memory module that enables the model to revisit and refine prior generations. INFO-RAG [76] treats the LLM as a denoising module trained with contrastive objectives. Collectively, these systems decouple output faithfulness from retrieval fidelity, enabling recovery even when retrieval is suboptimal. Context Compression and Utility Filtering: To address context window limitations, several systems optimize retrieval inputs into denser or more structured forms. FiD-Light [24], a streamlined variant of the Fusion-in-Decoder (FiD) architecture [27], improves decoding efficiency by compressing encoder outputs across retrieved passages and pruning cross-passage attention without modifying retrieval mechanisms. xRAG [10] projects document embeddings directly into the model’s representation space, minimizing token overhead through modality fusion. Rich Answer Encoding (RAE) [26] enhances retrieval relevance by embedding answer-aligned semantics into retriever outputs rather than relying on token overlap. GenRT [75] further refines retrieval utility by reranking and dynamically truncating Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 6 retrieved lists, retaining only the most contextually valuable candidates for generation. Complementing these designs, an information bottleneck-based filtering approach [89] selectively preserves evidence most informative for generation. Together, these strategies advance decoding efficiency and context quality, particularly for long-form and multi-hop RAG tasks. Retrieval-Guided Generation: A third strategy modulates generation based