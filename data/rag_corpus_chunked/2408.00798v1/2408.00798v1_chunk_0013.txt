tions are provided in Appendix B. 4.1.2 Experiment Setup The questions and choices are presented to the LLM/chatbot along with instructions to select an answer. Responses are collected and graded by a human expert who records the number of correct answers for each quiz. Each quiz is repeated five times, and the average score is calculated for each method and LLM backbone. 5 Table 1: Question answering experiment results. We use quizzes from six different domains of the new-hire training documents for engineers as test questions. All questions are multiple choice questions. Average scores across five trials are shown. The best scores are in bold. Vanilla LLM RAG Golden Retriever (Ours) Llama3 Mistral Shisa Llama3 Mistral Shisa Llama3 Mistral Shisa Quiz 1 - 10 Q 3.2 4.0 4.0 5.0 3.0 3.0 6.0 5.8 4.6 Quiz 2 - 10 Q 7.0 6.0 7.0 10.0 10.0 8.0 10.0 10.0 8.0 Quiz 3 - 9 Q 4.2 5.0 5.0 6.0 7.0 4.0 7.0 8.0 5.0 Quiz 4 - 10 Q 3.6 3.0 1.0 2.0 1.0 1.0 6.0 4.0 4.0 Quiz 5 - 10 Q 1.2 4.0 2.0 1.0 3.0 2.0 5.0 3.0 5.0 Quiz 6 - 9 Q 2.0 1.0 2.0 3.0 3.0 4.0 4.0 3.0 4.0 Total Score 21.2 23.0 21.0 27.0 27.0 22.0 38.0 33.8 30.6 Table 2: Abbreviation identification accuracy. Model No. of Abbrev. in Question 1 2 3 4 5 Llama3 70% 100% 90% 90% 100% Mistral 100% 100% 100% 70% 80% Shisa 50% 80% 60% 80% 100% We compare our method with vanilla LLM (with- out RAG) and the vanilla RAG method. For each method, including ours, we test three state- of-the-art models: Meta-Llama-3-70B-Instruct (AI@Meta, 2024), Mixtral-8x22B-Instruct-v0.1, and Shisa-v1-Llama3-70b.2e5. 4.1.3 Result We list the scores of each method and LLM backbone in Table 1. Compared with Vanilla LLM