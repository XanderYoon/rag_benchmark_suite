that the most relevant and accurate documents are retrieved. 3.6 Query Miss Response In some cases, the system may not find any relevant information for certain jargon terms in the dictio- nary. To handle such scenarios, Golden-Retriever has a fallback mechanism that synthesizes a re- sponse indicating that the database is unable to an- swer the question due to missing information. The system instructs the user to check the spelling of the jargon or contact the knowledge base manager to add new terms. This step ensures that the system maintains high fidelity and avoids generating in- correct or misleading responses. The unidentified jargon fits into a response template, instructing the user to check the spelling and contact the knowl- edge base manager to add the new term. 4 Evaluation We conduct two experiments to evaluate our method’s effectiveness. The first experiment tests our method’s ability to answer domain-specific questions based on documents, and the second ex- periment tests LLM’s ability to correctly identify abbreviations from questions. 4.1 Question-Answering Experiment 4.1.1 Dataset Preparation To evaluate our method’s ability to answer domain- specific questions based on documents, we col- lected multiple-choice questions from training doc- uments for new-hire engineers. The questions cover six different domains, with each domain hav- ing nine to ten questions. These questions are one to two sentences long and contain jargon or abbrevi- ations, with choices ranging from two (True/False) to four (Multiple choice). Examples of these ques- tions are provided in Appendix B. 4.1.2 Experiment Setup The questions and choices are presented to the LLM/chatbot along with instructions to select an answer. Responses are collected and graded by a human expert who records the number of correct answers for each quiz. Each quiz is repeated five times, and the average score is calculated for each method and