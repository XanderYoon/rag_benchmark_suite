like those used in (Kochedykov et al., 2023) to classify user intent, would require a dedicated training dataset. This is impractical for our application due to the extensive effort and resources needed to create such a dataset. Instead, we opt for an "LLM as backend" approach, which, despite incurring higher computational costs, does not require a dedicated training dataset and can be run efficiently on a local server. By identifying the context before 4 document retrieval, we ensure that the meaning of jargons and abbreviations is accurately interpreted, which is essential for retrieving the most relevant documents and providing accurate answers. 3.4 Query Jargons Once the jargon and context have been identified, the next step is to query a jargon dictionary for extended definitions, descriptions, and notes on the identified terms. This step is essential for providing the LLM with accurate interpretations of the jargon, ensuring that the augmented question is clear and unambiguous. This process involves querying a SQL database with the list of jargon terms identified in Section 3.2. The jargon list is inserted into a SQL query template, which is then processed to retrieve the rel- evant information from the jargon dictionary. The retrieved information includes extended names, de- tailed descriptions, and any pertinent notes about the jargon. We choose not to use the LLM to gener- ate SQL queries directly, as described in (Qin et al., 2023) and (Li et al., 2024). Generating SQL queries with LLMs can introduce uncertainties regarding query quality and safety, and can also increase in- ference costs. Instead, by using a code-based ap- proach to synthesize the SQL query, we ensure that the queries are verifiably safe and reliable. The detailed information obtained from this step is crucial for augmenting the userâ€™s original ques- tion. It allows for accurate context and