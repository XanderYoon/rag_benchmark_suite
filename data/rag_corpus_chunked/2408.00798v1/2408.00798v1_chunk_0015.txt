This experiment is conducted on the three aforementioned LLMs. 4.2.3 Result We list the accuracy of each LLM in identifying all abbreviations in questions with varying numbers of abbreviations in Table 2. The experiment shows that state-of-the-art models such as Llama3 and Mistral have high accuracy in identifying unknown abbreviations. We also observe different failure modes across the three LLMs, with detailed fail cases shown in Appendix C.2. 5 Conclusion This paper presents Golden-Retriever, a novel agen- tic RAG system designed to efficiently navigate vast industrial knowledge bases and overcome the challenges of domain-specific jargon and context interpretation. Experiment on a dedicated question- answer dataset shows that Golden-Retriever signif- icantly improves answer accuracy, demonstrating its superior performance compared with traditional RAG method. 6 Acknowledgments Zhiyu An would like to acknowledge Western Dig- ital Corporation for offering generous support dur- ing the summer internship and providing the chal- lenging problems that inspired this research. References AI@Meta. 2024. Llama 3 model card. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. The reversal curse: Llms trained on" a is b" fail to learn" b is a". arXiv preprint arXiv:2309.12288. Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, and Sainbayar Sukhbaatar. 2024. Reverse train- ing to nurse the reversal curse. arXiv preprint arXiv:2403.13799. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adap- tation of large language models. arXiv preprint arXiv:2106.09685. Denis Kochedykov, Fenglin Yin, and Sreevidya Kha- travath. 2023. Conversing with databases: Practical natural language querying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry