Golden-Retriever with three open- source LLMs and compare its performance with baselines on a dedicated, domain-specific question-answer dataset. 2 Related Work Current RAG techniques often fall short of the ideal scenario for handling domain-specific queries in industrial knowledge bases. Vanilla RAG (Lewis et al., 2020), for instance, struggles with accurately interpreting domain- specific jargons. When asked, "What is the PUC ar- chitecture of Samsung or Hynix NAND chip?" , the system incorrectly interprets "PUC" as "Process- Unit-Controller" instead of the correct "Peripheral Under Cell". This misinterpretation highlights the problem of hallucination, where the model gener- ates incorrect or nonsensical information based on ambiguous input. This issue is further illustrated in Figure 1, which shows that both Corrective RAG (Yan et al., 2024) and Self-RAG (Asai et al., 2023) attempt to modify the response after the document retrieval step. However, if the initial retrieval is flawed due to misinterpreted jargons or lack of con- text, these post-processing techniques cannot fully rectify the inaccuracies. Moreover, Corrective-RAG and Self-RAG focus on refining the generated responses after retrieval, which is inherently limited if the retrieved docu- ments themselves are not relevant. As depicted in Figure 1, these methods fail to address the root cause: the ambiguity in the userâ€™s question and the initial retrieval process. A related approach by (Kochedykov et al., 2023) aims to address vague questions by deconstructing them into an AST and synthesizing SQL queries accordingly. While this method improves query fidelity, it is limited to SQL queries and does not generalize to broader question- answering scenarios. Figure 1 illustrates this limi- tation, showing that while the method can disam- biguate and structure queries more effectively, it is not applicable to general retrieval tasks where context and jargon interpretation are crucial. 3 Method Golden-Retriever consists of offline and online parts. The