the dictio- nary, which could lead to misinterpretation in the following process. The LLMâ€™s ability to adapt to new terms provides a more robust solution. This step is represented as a two-way branching node in the workflow, shown in Figure 2. If the result- ing list is empty, the main program proceeds along the "No" path; otherwise, it follows the "Yes" path. The structured response containing the identified terms is saved in a temporary file, which is then ac- cessed by the main program to determine the next steps in the workflow. 3.3 Identify Context After identifying jargon, it is crucial to determine the context in which the question is asked, as the meaning of terms can vary significantly across dif- ferent contexts. For instance, "RAG" could mean "Retrieval Augmented Generation" in the context of LLMs or "Recombination-Activating Gene" in genetics. To accurately interpret the context, we use a similar reflection step as in jargon identifica- tion. This involves designing a prompt template that takes the question as input. The prompt con- tains a list of pre-specified context names and their descriptions. The LLM uses this prompt to identify the context of the question. Few-shot examples with Chain-of-Thought (CoT) prompting are ap- plied to enhance performance, guiding the LLM to respond in a specified data structure. The identified context is then stored and accessed by the main program for further processing. Using simpler methods, such as transformer- based text classifiers like those used in (Kochedykov et al., 2023) to classify user intent, would require a dedicated training dataset. This is impractical for our application due to the extensive effort and resources needed to create such a dataset. Instead, we opt for an "LLM as backend" approach, which, despite incurring higher computational costs, does not require a dedicated training dataset and