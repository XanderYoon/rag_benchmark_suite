is challenging. These domain- specific documents normally have many abbrevia- tions and jargons unique to their technical commu- nity, further complicating the problem. Large Language Models (LLMs) offer excellent performance for general question-answering tasks 1Work conducted while interning at Western Digital Cor- poration. (Petroni et al., 2019; Hu et al., 2021). To make a pre-trained LLM incorporate a company’s domain- specific knowledge, we may fine-tune it over the company’s proprietary documents. However, fine- tuning is computationally expensive, generalize poorly to new knowledge due to the Reversal Curse (Berglund et al., 2023), and limited in capacity, as it may overwrite old knowledge (Roberts et al., 2020; Zhai et al., 2024). Retrieval Augmented Generation (RAG) (Lewis et al., 2020) offers a flexible and scalable approach for utilizing large document collections. RAG con- sists of an embedding model, a document database, and a LLM. During offline preparation, RAG em- beds document chunks into the document database that retains semantic information. When an user asks a question, RAG first retrieves relevant doc- ument chunks according to semantic similarity. Then, the retrieved chunks are incorporated into prompts for the LLM, which then generates an an- swer. The output of RAG is the answer generated by the LLM based on the document chunks. This allows dynamic updates of knowledge base for an LLM without retraining it. Despite its advantages, RAG also faces chal- lenges to be used for domain-specific documents. First, since some jargons and abbreviations only ap- pear in proprietary documents, RAG’s LLM back- bone may hallucinate and misinterpret them. Ex- isting methods like Corrective RAG (Yan et al., 2024) and Self-RAG (Asai et al., 2023) enhance the LLM’s response post-retrieval. But when user’s question contains ambiguous jargons, RAG fails to retrieve the most relevant documents, limiting the effectiveness of post-retrieval enhancements. To disambiguate user’s