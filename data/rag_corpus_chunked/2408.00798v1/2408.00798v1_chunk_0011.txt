LLMs can introduce uncertainties regarding query quality and safety, and can also increase in- ference costs. Instead, by using a code-based ap- proach to synthesize the SQL query, we ensure that the queries are verifiably safe and reliable. The detailed information obtained from this step is crucial for augmenting the user’s original ques- tion. It allows for accurate context and jargon in- terpretation, which is fundamental for the RAG process to retrieve the most relevant documents and generate precise answers. 3.5 Augment Question With the jargon definitions and context identified, the next step is to augment the user’s original ques- tion to include this additional information. This augmentation ensures that the RAG process re- trieves the most relevant documents by providing clear context and resolving any ambiguities in the question. This step involves integrating the original question with the context information and the de- tailed jargon definitions obtained from Sections 3.3 and 3.4. The augmented question explicitly states the context and clarifies any ambiguous terms, fa- cilitating enhanced document retrieval. The process is automated, with the code taking the original question and the results from the con- text and jargon identification steps and combining them into a structured template. The context infor- mation grounds the LLM to the specified scenario, and the jargon definitions add relevant notes to clar- ify terms. The augmented question then replaces the user’s original question and is used as input for the RAG framework, ensuring that the most relevant and accurate documents are retrieved. 3.6 Query Miss Response In some cases, the system may not find any relevant information for certain jargon terms in the dictio- nary. To handle such scenarios, Golden-Retriever has a fallback mechanism that synthesizes a re- sponse indicating that the database is unable to an- swer the question due to missing