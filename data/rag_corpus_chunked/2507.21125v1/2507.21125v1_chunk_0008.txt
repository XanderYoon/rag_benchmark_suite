with relevant contextual documents. resulting in domain understanding and term disambiguation for the LLM. RAG’s knowledge base is constructed by consolidat- ing diverse public technology lists, including Wikipedia’s list of emerging technologies (drawing on insights from Puccetti et al. [16]), the Chinese catalogue of prohibited and restricted technologies (based on CSET [22]), the International Energy Agency’s (IEA) list of clean energy technologies (based on IEA’s ETP clean energy technology [23]) and O*NET’s list of technology skills and tools (based on National Center for O*NET Development [24]). Entries from these sources were structured into documents with standardized fields of name, type, domain, and description. These documents were then segmented into manageable chunks using RecursiveCharac- terTextSplitter of LangChain library [25] and subsequently vectorized using the OllamaEmbeddings model mxbai-embed- large [26], [27]. The resulting standardized numerical em- beddings, along with their corresponding text chunks and metadata, are then stored locally. From this store, a retriever is configured to fetch an initial set of relevant documents based on distance (the top 20 most similar) for any given input ( abstract, keyword, and title). Finally, this retrieved set undergoes a custom, two-pass diversity filtering process. This process first prioritizes documents exhibiting unique metadata combinations (specifically type and domain fields) and subsequently fills the selection with additional documents from the retrieved set if needed, aiming for a target of up to seven diverse documents. This method ensures a varied yet relevant contextual input for the technology extraction LLM. 2) LLM-based Candidate Technology Extraction : For the ini- tial candidate technology extraction, the RATE pipeline uti- lizes the DeepSeek-V3 model, accessed via its API endpoint [28]. Key generation parameters were set to ensure determin- istic and comprehensive output, including a temperature of 0.0 and max tokens set to 4096. This model is then prompted using a precisely