the proposed model with another state-of-the-art model was required. Therefore, a supervised machine learning model has been trained for the task of the technology extraction from text so its results would be compared with RATE’s output and the gold standard list of technologies. E. BERT baseline for technology extraction Bidirectional Encoder Representations from Transformers (BERT) is a pretrained transformer with ability to achieve exceptional results in many tasks including sequence classi- fication [29], [30]. Using this model, we aimed to imply a supervised machine learning model for the technology extrac- tion task which then enabled us to quantify the precision and F1-score of our model which did not benefit from a training phase. The BERT model was fine-tuned on a custom NER dataset comprising approximately 2200 rows of tokenized scien- tific sentences from domains like neuroscience, robotics, and VR/AR, sourced from open-source publications. BIO tags were used in the task of curating this dataset, B-TECH indicating the beginning of a technology, I-TECH for inside of a technology entity, and O for a non-technology token. The dataset were split into training (80%), validation (10%), and test (10%) sets. ”bert-base-cased” variant from Hugging Face Transform- ers library were used as the base model, initialized with 3 output labels corresponding to the BIO tags [31]. The dataset was preprocessed into a custom NERdataset class, handling tokenization, padding/truncation to a maximum length of 128 tokens, and alignment of labels with subword tokens. Training was conducted using the Trainer API with the fol- lowing hyperparameters: 20 epochs, batch size of 16, learning rate of 2e-5, AdamW optimizer with weight decay of 0.01, and early stopping based on F1-score on the validation set. Evaluation metrics included token-level accuracy and entity- level F1-score, accounting for partial matches in multi-word entities. This BERT baseline was trained on a