perform evaluation. 5 Experiments 5.1 Experimental Setup Datasets. We select 5 benchmark datasets from different domains to evaluate GPT-FedRec. They are Beauty, Games, Toys, Auto (He and McAuley, 2016; McAuley et al., 2015) and ML-100K (Harper and Konstan, 2015). The first four datasets are Amazon review datasets consisting of user feed- back on different categories of products. ML-100K is a benchmark movie recommendation dataset. Following (Chen, 2023; Yue et al., 2022), we pre- process the raw data with 5-core, and construct training sequences in chronological order. The de- tailed dataset statistics are in Table 3 (Appendix A). Baselines. We adopt three groups of baseline methods for comparison. (1) Group 1: ID-based FR models, namely FedSAS, FedLRU and CF-FedSR. FedSAS and FedLRU are federated version of SAS- Rec (Kang and McAuley, 2018) and LRURec (Yue et al., 2023b) with FedAvg (McMahan et al., 2017) as the aggregation protocol. CF-FedSR (Luo et al., 2022) uses a client utility-aware aggregation proto- col for better FR. (2) Group 2: Text-based models, i.e., TransFR (Zhang et al., 2024), P5 (Geng et al., 2022) and RecFmr (Li et al., 2023a) and UniSRT (Hou et al., 2022). TransFR trains a BERT-like retriever. UniSR T trains SASRec using BERT- encoded item texts. P5 and RecFmr finetune pre- trained T5 or LongFormer with item texts and use the model as recommender. (3) Group 3: The hy- brid UniSRIT (Hou et al., 2022), an ID-augmented version of UniSRT. Note that P5, RecFmr, UniSRT and UniSRIT are not originally designed for FR. We use FedAvg as the aggregation protocol to ex- tend them into our FR setting. We did not include LLM-based recommenders as baselines, because existing LLM-based recommenders solely focus on the ranking stage, and could not provide a complete solution for FR: they can only rank ground-truth items along