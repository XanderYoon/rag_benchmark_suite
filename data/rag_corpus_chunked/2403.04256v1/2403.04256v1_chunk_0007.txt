Another stream of LLM-based recommendation designs tuning strategies tailored for specific subtasks (e.g., rating prediction) to im- prove recommendation performance (Chen, 2023; Kang et al., 2023; Yue et al., 2023a). However, ex- isting LLM-based recommenders mainly generate recommendation candidates in an autoregressive fashion. As such, the generated content might be irrelevant to the items of interests or even are hallu- cinated, leading to undesired performance in real- world applications. As such, we design a retrieval augmented recommendation framework, where the RAG approach effectively reduces hallucination and improves recommendation performance. 3 Preliminaries Data. We adopt the sequential recommendation setting to illustrate the data format. In sequential recommendation, a data point is the historical data of a user: a sequence of interacted items x (sorted by timestamps) within her history. A sequence x is a list of items [x1, x2, ..., xl] of length l. Each ele- ment in x belongs to the item scope I that contains all items: xi ∈ I . The goal of a recommender is to predict the next user-item interaction xl+1 ∈ I based on the user history x. In our experiments, xl+1 is used as ground truth y (i.e., y = xl+1). Model. An ID-based recommender directly takes item sequences as input, and maps them into new items as recommendations. Given an input se- quence x, an ID-based recommender computes a score vector over the item scope I, and recom- mends items with highest probabilities. In compar- ison, a text-based recommender usually transforms discrete item IDs into descriptions: transforming a sequence of items (x) into a sequence of descrip- tions. After trained on such text sequences, the text-based recommender generates textual IDs (i.e., IDs in text format) or item titles as the final recom- mendation. Federated Recommendation. In FR, a central server stores a global