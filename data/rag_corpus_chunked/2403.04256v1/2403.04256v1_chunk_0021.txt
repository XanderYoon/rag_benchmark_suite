in Table 2. In Ta- ble 2, w/o LRU refers to GPT-FedRec only with E5 as retriever in the first stage, and w/o E5 indicates that there is only LRURec in the first stage. w/o LLM represents GPT-FedRec without the LLM- based re-rank. As expected, we observe that both hybrid retrieval and LLM-based re-ranking con- tributes to a better recommendation performance. For instance, without either module, the recommen- dation performance basically drops. Such results also validate the merit of our design: our hybrid re- trieval mechanism and the LLM-based re-ranking are indeed effective in terms of addressing the data sparsity and data heterogeneity in FR, which leads to better recommendation performance. 6 Conclusion This work presents a novel federated recommen- dation framework that exploits ChatGPT and a novel hybrid retrieval mechanism. GPT-FedRec provides an effective privacy-aware solution to build a recommender system for data-sparse and data-heterogeneous federated recommendation sce- narios. We highlight the significance of this work: despite the active research on federated recommen- dation, existing methods largely suffer from the data sparsity and data heterogeneity issue in FR. In contrast, our work is deliberately designed to overcome such an issue, achieving generalized rec- ommendation performance. Finally, within GPT- FedRec, there is also a hybrid RAG mechanism to prevent LLM hallucination, improving the au- thenticity of recommendation results in real-world applications. 7 Limitations One limitation of this work is that our method in- troduces extra hyperparameters. For different ap- plications, one might need to finetune these hy- perparameters, which brings extra computational cost. Another limitation of this work is that our method does not take the inherent bias of GPT into account. However, it is known that such pretrained LLMs usually have encoded the bias in the pre- training data (e.g., stereotypical data, racism and hate speech). Such bias could