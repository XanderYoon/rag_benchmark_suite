recommendation setting, where the historical data of all users are used to train the model with the last item used for testing. Our cold-start setting is more realistic in the FR setting, and also more appropriate to test the generalization of the trained FR model. Implementation. To implement GPT-FedRec, we train LRURec from scratch, and finetune the pretrained E5 (e5-base-v2) using the training data. When training LRURec, the learning rate is initial- ized as 1e-3, and the number of local/global epochs is 80/5 (for Beauty, Games, Toys) and 60/5 (for Auto and ML-100k). As for E5, the learning rate is initialized as 1e-6, and the number of local/global epochs is 2/2. We use AdamW as the optimizer for both LRURec and E5. When generating the candidate set ˆI in the first stage, we pick the top-20 items from the hybrid score ˆPhybrid. To evaluate the recommendation performance, we select the commonly used normalized discounted cumula- tive gain (NDCG@N) and recall (Recall@N) with N ∈ [5, 10]. The predictions are ranked against all items in the dataset. More implementation details on baseline methods are in Appendix A. 5.2 Evaluation: Overall Recommendation Performance The first set of results are reported in Table 1, where we compare the recommendation perfor- mance of all schemes. In Table 1, for clarity, we highlight the best results in bold and underline the second best results. Note that the ranking results are from the entire item scope (i.e., a complete ranking of all items). From Table 1, it is observed: (1) GPT-FedRec generally achieves better recom- 0 0.1 0.3 0.5 0.7 0.9 1.0 0.01 0.02 0.03 0.04 0.05 0.06Performance R@5 N@5 R@10 N@10 0 0.1 0.3 0.5 0.7 0.9 1.0 0.02 0.04 0.06 (a) Beauty 0 0.1 0.3 0.5 0.7 0.9 1.0 0.000 0.025 0.050 0.075 (b)