(i.e., a complete ranking of all items). From Table 1, it is observed: (1) GPT-FedRec generally achieves better recom- 0 0.1 0.3 0.5 0.7 0.9 1.0 0.01 0.02 0.03 0.04 0.05 0.06Performance R@5 N@5 R@10 N@10 0 0.1 0.3 0.5 0.7 0.9 1.0 0.02 0.04 0.06 (a) Beauty 0 0.1 0.3 0.5 0.7 0.9 1.0 0.000 0.025 0.050 0.075 (b) Games 0 0.1 0.3 0.5 0.7 0.9 1.0 0.00 0.02 0.04 0.06 (c) Toys 0 0.1 0.3 0.5 0.7 0.9 1.0 0.00 0.05 0.10 (d) ML-100K Figure 4: Sensitivity Analysis. We change λ in Equa- tion 6, and evaluate the recommendation performance based on the hybrid retrieval results of stage 1. mendation performance than the baseline meth- ods. For instance, compared to the second best baseline method over all datasets, GPT-FedRec achieves 36.12%, 29.88%, 45.44% and 36.56% average improvements w.r.t. all 4 metrics, respec- tively. (2) GPT-FedRec achieves satisfying perfor- mance on ML-100K, whereas some baseline meth- ods, especially text-based ones, could barely con- verge. This is expected, because some of them (e.g., RecFmr, UniSR) are pretrained using Amazon re- view datasets, whose domain is different from the domain of movie recommendation. Moreover, the user history of ML-100K is much longer than that in Amazon review datasets. Since the text-based baseline methods could not handle long user his- tory, they are prone to suffer from performance degradation. On the contrary, GPT-FedRec lever- ages hybrid retrieval and exploits the pre-trained knowledge in LLMs, thus enjoying a more compre- hensive understanding of user preferences. 5.3 Sensitivity Analysis In this section, we conduct a sensitivity analysis w.r.t. α in Equation 63. This is a key factor affect- ing the hybrid retrieval before LLM. In particular, we change it from 0 to 1 and keep other configura- tions the same. The results are shown