inadequacy of ID-based retriever motivates us to employ an ad- ditional dense, more generalizable text-based re- triever to build GPT-FedRec. To illustrate the ne- cessity of using text-based retriever, consider the example in Figure 1. In this example, client 1, client 2 and the test user have mutually exclusive item scopes. This is a typical data heterogeneous case. In the discrete ID space, a movie with a spe- cific ID can only appear in one party. An ID-based retriever would fail to capture the correlation be- tween some movie IDs cross parties, leading to degraded performance. In a sharp contrast, in the text space, the movies across these parties may still share common, generalized features (e.g., "war, action movies" of the new user and "sci-fi, war movies" on client 2), despite the different movie IDs. As such, text-based retriever could capture such generalized features, overcoming data hetero- geneity that defects ID-based retrievers. To this end, we propose to use E5 (Wang et al., 2022) as the text-based retriever. E5 is a transformer-based language model, pretrained for text-retrieval tasks. Formally, we define the text- based retriever as fT , parameterized by Î¸T . To adapt E5 into our FR application, we finetune it using text descriptions of items and the InfoNCE loss (Oord et al., 2018) on each local client. In par- ticular, for each training samplex = [x1, x2, ..., xl] and its ground-truth item y, we firstly transform Text Description (e.g. movie titles, genres) ID-based Retriever Txt-R. Client 1 Client 2 Client 3 Text-based Retriever Client Training ID-R. ID 1 ID 2 ID 3 CE InfoNCE Global Aggregation Figure 3: The training and aggregation process of GPT-FedRec. On clients, the ID-based retriever and the text-based retriever are trained using local data. them into input texts t and ground-truth text