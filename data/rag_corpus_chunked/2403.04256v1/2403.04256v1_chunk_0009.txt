retrieval mechanism using a small- scale ID-based retriever and a dense text-based retriever to retrieve generalized candidates (Sec- tion 4.1). Then, based on the retrieved results, we establish a retrieval augmented recommendation pipeline using LLMs (Section 4.2). 4.1 Hybrid Retrieval ID-based Retriever. Given existing ID-based recommenders, any of them could be used by GPT- FedRec to retrieve potential candidates. In our implementation, we choose LRURec (Yue et al., 2023b) as the ID-based retriever for its state-of-the- art performance and light-weight design, avoiding extra communication costs in FR settings. Formally, we define the ID-based retriever asfI, parameterized by θI. As shown in Figure 2, for a piece of user data, fI takes its interacted item sequence x as input, i.e., fI(x). fI returns a vec- tor of similarity scores over the item scope I. To train LRURec on each local client, we optimize the cross-entropy loss over the local dataset: Lce = E(x,y)∼Dk[L(fI(x), y)]. (1) During federated training, the weights of fI are sent to the global server for aggregation. However, it is foreseeable that in a data sparse and data het- erogeneous setting, the predicted scores returned by a local fI are skewed towards the items present in its local dataset. That is, a locally trained fI is prone to retrieve the items present its local datasets, rendering its unsatisfactory performance on test data with unseen new items. Text-based Retriever. In the data sparse and data heterogeneous FR setting, the inadequacy of ID-based retriever motivates us to employ an ad- ditional dense, more generalizable text-based re- triever to build GPT-FedRec. To illustrate the ne- cessity of using text-based retriever, consider the example in Figure 1. In this example, client 1, client 2 and the test user have mutually exclusive item scopes. This is a typical data heterogeneous case. In the