user history and candidate item using their titles and categories/genres. Then, an in- struction to the task is appended to the description. In addition to the text prompts, we also feed sys- tem prompts to GPT, so that its mindset is adjusted to the concrete recommendation application (e.g., recommending movies or cosmetics). A simplifed prompt template looks like: ### system: You are a helpful { role}, {role description }. ### user: I’ve browsed the fol- lowing items in the past in order: {history}. There is also candidate pool: {candidates}. {task instruction }. In the above template, role and role description are replaced with a concrete real- world job (e.g., shopping assistant) and its job de- scription (e.g., recommending products for cus- tomers). history is instantiated as item texts containing item titles, possibly with item cat- egories/genres. As for candidates, it is re- placed with the hybrid retrieval results. task instruction shall be replaced with a direct re- rank instruction. We also add additional formatting instructions in the template for post-processing pur- poses. The detailed, complete prompt template is summarized in Appendix B. Post-processing. As mentioned, we do not ac- cess the model weights or output logits of LLMs. The re-rank results are generated in free texts (e.g., item titles) despite the formatting instructions in the prompts. Therefore, we apply fuzzy matching to transform the generated textual recommenda- tions (e.g., item titles) into a ranked list of item IDs: ˆIRAG to perform evaluation. 5 Experiments 5.1 Experimental Setup Datasets. We select 5 benchmark datasets from different domains to evaluate GPT-FedRec. They are Beauty, Games, Toys, Auto (He and McAuley, 2016; McAuley et al., 2015) and ML-100K (Harper and Konstan, 2015). The first four datasets are Amazon review datasets consisting of user feed- back on different categories of products. ML-100K is a