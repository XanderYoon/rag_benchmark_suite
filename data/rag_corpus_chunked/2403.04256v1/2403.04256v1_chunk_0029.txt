values of Recall@5, Recall@10, NDCG@5 and NDCG@10 either. Technically, to determine whether the recommendation results of a test user should be re-ranked or not, we first validate whether the ground-truth item is included its predicted candidate set (i.e., top-20 items). That is, for a test user, if the ground-truth item is within the predicted top-20 items, then the predicted top-20 items will be fed into LLM for re-ranking. Finally, when post-processing the generated texts, we also filter out the re-ranked results without ground-truth items. This procedure makes sense, because the re-ranking process is expected to explicitly focuses on re-ranking the top-20 items with ground-truth items in them. However, if a generated re-ranked item list does not contain the ground-truth item, we ignore this re-ranked list and use the candidate set from stage one as the final recommendation for this test user. For instance, we observe that GPT-3.5 may ignore some technical parameters within some product titles or abbreviate some product titles. This leads to a discrepancy between the true titles of the ground-truth items and the generated ones, even if in human eyes they may represent the same product. Such generated results are treated as noisy generation and are ignored when calculating the evaluation metrics in our evaluation. Baselines. For all baseline methods, we refer to the original papers and the official implementations. Moreover, for non-FR methods, we used the same code of FedAvg used for GPT-FedRec to perform global model aggregation. For all baselines, we train the models by starting with the default training/finetuning configuration. For FedSAS, FedLRU, CF-FedSR and TransFR, the models are trained from scratch. In comparison, we load the released pretrained weights for P5, RecFmr and UniSR, and finetune them in our FR setting. Finally, we observed training divergence and overfitting of some baseline methods, and