incorporate both domain-generalized features from the text-based retriever and the representative ID- based user patterns. As such, GPT-FedRec is equipped with generalization ability by design, and can overcome the data sparsity and data hetero- geneity issue in FR applications, achieving better recommendation performance. 4.2 Hybrid Retrieval Augmented Generation RAG-based Recommendation. After the hybrid retrieval stage, GPT-FedRec further employs an LLM to perform re-ranking among the retrieved candidates within ˆI. This design is to exploit the pretrained knowledge encoded in LLMs, so that the generalization of recommendations is further enhanced. Moreover, the re-ranking process of the LLM is conditioned on the retrieved results, hallu- cination could be effectively avoided: the LLM is explicitly instructed to only re-rank the candidates within ˆI, and they are retrieved from real-world data during the first stage. Finally, for efficiency, we intend to prompt an LLM for recommendation without finetuning it, and treat the LLM-based re- ranking process as retrieval augmented generation (De Cao et al., 2020; Tay et al., 2022). LLM. In this work, we adopt GPT-3.5-Turbo (OpenAI) from OpenAI to build GPT-FedRec. GPT-3.5-Turbo is a closed-source LLM with abil- ities of solving many complex tasks (OpenAI). Since in a data sparse FR setting, it is infeasible to finetune LLM using such limited data, we select GPT-3.5-Turbo for its powerful zero-shot general- ization ability. To construct a text prompt for GPT-3.5-Turbo, inspired by (Hou et al., 2023b), we start it with a description of both user history and candidate item using their titles and categories/genres. Then, an in- struction to the task is appended to the description. In addition to the text prompts, we also feed sys- tem prompts to GPT, so that its mindset is adjusted to the concrete recommendation application (e.g., recommending movies or cosmetics). A simplifed prompt template looks like: ### system: