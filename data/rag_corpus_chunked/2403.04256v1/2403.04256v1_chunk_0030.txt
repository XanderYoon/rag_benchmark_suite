model aggregation. For all baselines, we train the models by starting with the default training/finetuning configuration. For FedSAS, FedLRU, CF-FedSR and TransFR, the models are trained from scratch. In comparison, we load the released pretrained weights for P5, RecFmr and UniSR, and finetune them in our FR setting. Finally, we observed training divergence and overfitting of some baseline methods, and therefore, adjusted both local epochs and global epoch until find the best performance. B Prompt Engineering E5 Templates. We follow the instructions in (Wang et al., 2022), and design our E5 templates as follows: ### query: {history} ### passage: {candidate}, where history shall be replaced with history item titles and genres/categories. candidate shall be replaced with the title and category/genre of a single candidate item. Moreover, in our experiments, we notice that it is more beneficial to only use last several movies for the ML-100K dataset, and does not use genre. For instance, an exemplar complete template on ML-100K is like (note that the history does not need to be the complete history and the description of genres may be removed for better performance): ### query: The Shawshank Redemption, a movie about Thriller ; Ex Machina , a movie about Sci-Fi, Thriller ; Unchained, a movie about Drama, Western . ### passage: Whiplash, a movie about Drama . GPT-3.5-Turbo Templates. Inspired by (Hou et al., 2023b), we use the following templates to prompt GPT-3.5-Turbo: ### system: You are a helpful {role}, {role description }. ### user: Iâ€™ve browsed the following items in the past in order: {history}. There is also candidate pool: {candidates}. {task instruction }. In the above template, role and role description are replaced with a concrete real-world job (e.g., movie reviewer) and its job description (e.g., recommending movies for people). history is instantiated as item texts containing