similarity between user sequences and their ground-truth items in the text space. More importantly, since E5 is pre- trained, it has already encoded prior knowledge of language patterns. Therefore, the text-based re- triever is capable of extracting generalized textual features from item descriptions, despite the data sparsity and data heterogeneity across clients. Finally, after training the ID-based retriever (f k I ) and the text-based retriever ( f k T ) on each local client, we adopt FedAvg (McMahan et al., 2017) to perform global model aggregation, obtaining the global ID-based retriever and text-based retriever: θ∗ (·) = 1P k |Dk| KX k=1 |Dk| · θk (·) s.t. θ k I = arg minLce(Dk, fI), θk T = arg minLinf o(Dk, fT ), k ∈ {1, ..., K}. (5) Hybrid Retrieval. To generate the retrieval re- sults for any user sequence x, we compute a weighted sum of the normalized prediction scores returned by the aggregated ID-based retriever and text-based retriever through the Tikhonov principle (Tikhonov, 1963): ˆPhybrid = λ · σ(fI(x)) + (1 − λ) · σ(fT (t)), (6) where σ is the softmax opernation. Then, the items of top-N scores within ˆPhybrid are retrieved as can- didates. These candidates form a candidate set ˆI. ˆI is the result of the first stage of GPT-FedRec. We highlight that the candidate set ˆI is retrieved based on the hybrid scores ˆPhybrid. Compared to traditional ID-based FR models, the hybrid scores incorporate both domain-generalized features from the text-based retriever and the representative ID- based user patterns. As such, GPT-FedRec is equipped with generalization ability by design, and can overcome the data sparsity and data hetero- geneity issue in FR applications, achieving better recommendation performance. 4.2 Hybrid Retrieval Augmented Generation RAG-based Recommendation. After the hybrid retrieval stage, GPT-FedRec further employs an LLM