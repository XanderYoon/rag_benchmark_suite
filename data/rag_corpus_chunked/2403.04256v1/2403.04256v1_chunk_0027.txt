Games, Toys and Auto datasets are all Amazon review datasets, whose domain is rather different from that of ML-100K. Datasets users Items Interaction Length Density Beauty 22332 12086 198K 8.88 7e-4 Games 15264 7676 147K 9.69 1e-3 Toys 19412 11924 167K 8.63 7e-3 Auto 1281 844 8K 6.70 8e-3 ML-100K 610 3650 89K 146.70 4e-2 Table 3: Overall dataset statistics. Federated Dataset Statistics. In our experiments, we randomly sample a set of users from the original datasets, and randomly distributed them onto different local clients. In terms of users, it is guaranteed that the local datasets are mutually exclusive: one user could one exist in one local dataset or only in the test dataset. The per-client statistics is summarized in Table 4. Note that considering the dataset size and training convergence, we sampled different number of training users for different datasets. For each dataset, after sampling the training users, the remaining users are used for validation and evaluation. It is observed that for Beauty, Games and Toys, the test item scope is significantly larger than each local item scope, leading to a data sparse and data heterogeneous FR setting. Datasets Users/Client Items/Client Test Users Test Items Beauty 1000 4161 17332 12086 Games 1000 3826 10264 7614 Toys 1000 4412 14412 11766 Auto 200 503 281 591 ML-100K 100 3134 109 3060 Table 4: Averaged statistics on local clients and the statistics of test users. A.2 Implementation. GPT-FedRec. To implement GPT-FedRec, we train LRURec from scratch, and finetune the pretrained E5 (e5-base-v2) using the training data. When training LRURec, the learning rate is initialized as 1e-3, and the number of local/global epochs is 80/5 (for Beauty, Games, Toys) and 60/5 (for Auto and ML-100k). As for E5, the learning rate is initialized as 1e-6, and the number of local/global epochs is