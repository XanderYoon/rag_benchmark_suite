0.0390 R@10↑ 0.0357 0.0786 0.0679 0.0143 0.0083 0.0750 0.0679 0.0821 0.0964 N@10↑ 0.0187 0.0398 0.0361 0.0085 0.0035 0.0332 0.0384 0.0488 0.0492 ML-100K R@5↑ 0.0183 0.0459 0.0550 0.0092 0.0002 0.0001 0.0183 0.0183 0.0642 N@5↑ 0.0085 0.0327 0.0402 0.0035 0.0001 0.0001 0.0150 0.0075 0.0362 R@10↑ 0.0367 0.1101 0.1009 0.0092 0.0002 0.0091 0.0183 0.0459 0.1468 N@10↑ 0.0145 0.0538 0.0550 0.0035 0.0001 0.0031 0.0150 0.0166 0.0621 Average R@5↑ 0.0187 0.0371 0.0350 0.0077 0.0050 0.0345 0.0313 0.0298 0.0505 N@5↑ 0.0118 0.0241 0.0239 0.0045 0.0025 0.0182 0.0215 0.0202 0.0313 R@10↑ 0.0302 0.0616 0.0553 0.0105 0.0070 0.0546 0.0445 0.0504 0.0896 N@10↑ 0.0156 0.0320 0.0304 0.0054 0.0034 0.0256 0.0258 0.0270 0.0437 Table 1: Main results on recommendation performance under different FR schemes. The best results are highlighted in bold and the second best results are highlighted with underline. FR Setup. Since the training of some baseline methods could not converge under a large num- ber of clients, we compare all methods under a setting of 5 clients. Moreover, to simulate data sparsity and data heterogeneity, the local datasets are sparsely sampled from the original datasets and are guaranteed to be mutually exclusive in terms of users. The remaining users are used as cold- start test users. The detailed local datasets statistics are summarized in Table 4 (Appendix A). Note that the test users in our FR setting are cold-start users, whose historical data are not used for model training. This is fundamentally different from the common centralized sequential recommendation setting, where the historical data of all users are used to train the model with the last item used for testing. Our cold-start setting is more realistic in the FR setting, and also more appropriate to test the generalization of the trained FR model. Implementation. To implement GPT-FedRec, we train LRURec from scratch, and finetune the pretrained E5 (e5-base-v2)