may suffer from de- graded performance, due to the data sparsity and heterogeneity in the federated setting (Wu et al., 2022; Zhang et al., 2024). In FR, the clients might only contain the data of a few or even a single user (data sparsity). Training an ID-based recommender on such sparse data is prone to overfitting (Zhang 1General Data Protection Regulation:https://gdpr-info.eu/ arXiv:2403.04256v1 [cs.IR] 7 Mar 2024 et al., 2024). Moreover, data sparsity may also trig- ger data heterogeneity. That is, the item scopes of local datasets are only subsets of the entire item scope, and different local clients may have different item scopes (non-i.i.d. data). As shown in Figure 1, a test userâ€™s data contains novel items never seen in the local clients (i.e., a cold-start user). Conse- quently, the FR system could not make meaningful recommendations. Since the locally trained models may never see certain items during training, aggre- gating such sparsely-trained models results in poor generalization (Zhang et al., 2024). On the other hand, large language models (LLMs) exhibit strong generalization abilities in diverse tasks, thanks to the extensive knowledge learned from the massive-scale real-world data. Re- cently, LLMs as recommenders also demonstrate impressive performance in different recommenda- tion tasks (Harte et al., 2023; Li et al., 2023c; Bao et al., 2023; Zhang et al., 2023; Yue et al., 2023a). Therefore, employing LLMs for FR has the poten- tial to address data sparsity and data heterogene- ity. Yet, existing LLM-based recommenders suf- fer from issues like incomplete recommendations, low inference efficiency and potential hallucination (Yue et al., 2023a; Li et al., 2023b), impairing their applicability in real-world scenarios. Besides, in FR, it is neither affordable nor feasible for local clients to finetune LLMs with limited computa- tional resources and data. In addition, the next- word-generating nature of LLMs