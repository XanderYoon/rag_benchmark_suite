we train LRURec from scratch, and finetune the pretrained E5 (e5-base-v2) using the training data. When training LRURec, the learning rate is initialized as 1e-3, and the number of local/global epochs is 80/5 (for Beauty, Games, Toys) and 60/5 (for Auto and ML-100k). As for E5, the learning rate is initialized as 1e-6, and the number of local/global epochs is 2/2. We use AdamW as the optimizer for both LRURec and E5. When generating the candidate set ˆI in the first stage, we pick the top-20 items from the hybrid score ˆP. In the second stage, when performing the LLM-based re-ranking, the ideal procedure is to re-rank the candidate sets of all users. However, due to the limited GPT API query budget, such an ideal evaluation procedure is too expensive and infeasible. Moreover, since we are only interested in the recommendation performance w.r.t. the top-20 items, it is equivalent and sufficient to only perform the LLM re-ranking for a subset of test users, whose candidate set includes the ground-truth item. This procedure is meaningful and fair. To see this, we emphasize that we only feed the predicted top-20 items into LLM for re-ranking. In this setting, for a test user, if the ground-truth item is not within these predicted top-20 items, then the ground-truth item will not participate in the LLM re-ranking. As such, re-ranking will not affect the ranking of the ground-truth item, and therefore, will not change the values of Recall@5, Recall@10, NDCG@5 and NDCG@10 either. Technically, to determine whether the recommendation results of a test user should be re-ranked or not, we first validate whether the ground-truth item is included its predicted candidate set (i.e., top-20 items). That is, for a test user, if the ground-truth item is within the predicted top-20 items, then the predicted top-20