largest open-source and commercial models to reach SOTA results at minimal costs. accuracy collapses. To remedy this, one must either ensure that the finetuning dataset strictly follows the specified schema, or use Context-Free Grammar-based algorithms for structured generation, such as Outlines’ outlines.generate.cfg module, which does not impose a strict key ordering. 2. Token explosion with optional keys . A common issue we have observed is to require the keys to be generated even when the predicted value is null. E.g., when one builds a Pydantic object with Optional fields then naively pipes the object’s json schema to Outlines. This leads to a lot of unnecessary tokens being generated, slowing down inference. Another bad practice when using Outlines is to make all of the keys optional. This is because Outlines uses a different algorithm to generate the FSM for this case. A workaround for this is to add a required dummy key of type null to the schema and remove it in postprocessing. B. Bounding Box Backcalculation Heuristic For the KIE task, we have found that a simple, greedy al- gorithm (Algo 1) suffices for backcalculating bounding boxes. To use the entire page, simply set the y lowerbound and upperbound to 0 and the page height in pixels, respectively. For the LIR task, a good heuristic is to (1) divide the page vertically into chunks, one for each line item; and (2) re-use Algorithm 1 above to back-calculate the bounding boxes for each line item, but only for the words in the chunk assigned to the line item. The challenge is how to divide the page. The naive dynamic programming approach with 2D states (line item index, page y) has complexity Algorithm 1 Bounding Box Backcalculation Heuristic Inputs: y lower- & upperbound, predicted key-value map, and OCR data Outputs: Matching score,