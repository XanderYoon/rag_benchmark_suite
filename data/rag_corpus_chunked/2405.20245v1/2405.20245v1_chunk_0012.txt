GPT-3.5 + RASG and Hermes 2 Pro + RASG achieves 87.79% and 85.02% ICS, respectively. V. D ISCUSSION AND CONCLUSION Our model performance and ablation results demonstrate a few conclusions. Firstly, for KIE, prompt engineering only provides marginal gains compared to augmenting the model with a retrieval mechanism and/or finetuning it on the target dataset. For LIR, however, prompt engineering is as important as retrieval mechanisms and finetuning. Interestingly, properly tuned and augmented LLMs, can beat finetuned multimodal models such as LayoutLMv3 and Roberta + DeTr. Lastly, our bounding box backcalculation heuristic is only slightly worse than the best baseline at table detection despite not being optimized directly for the task. For teams working in the Business Document Informa- tion space, our recommendation is to start with off-the-shelf LLMs that support structured generation, then implement a retrieval mechanism. If the performance is still poor, consider supervised finetuning. For LIR, we recommend starting with structured prompting first, then finetuning. TABLE II ABLATION BENCHMARKS OF RASG COMPONENTS ON KIE & LIR TASKS ON THE DOCILE DATASET Model KIE F1 Score LIR GLIRM-F1 GPT-3.5 34.17% 28 .31% + 1-Shot Retrieval + 22.08% + 20.67% + Supervised Finetuning + 22.31% + 17.73% + Structured Prompting + 4.96% + 19.42% Hermes 2 Pro - Mistral 7B 13.55% 4 .69% + 1-Shot Retrieval + 36.87% + 40.55% + Supervised Finetuning + 17.71% + 13.53% + Structured Prompting + 0.63% + 10.30% REFERENCES [1] M. Skalick ´y, ˇS. ˇSimsa, M. U ˇriˇc´aˇr, and M. ˇSulc, Business document information extraction: Towards practical benchmarks , 2023. arXiv: 2206.11229 [cs.IR]. [2] B. Smock, R. Pesala, and R. Abraham, GriTS: Grid table similarity metric for table structure recognition , 2023. arXiv: 2203 . 12555 [cs.LG]. [3] P. Lewis, E. Perez, A. Piktus, et al. , Retrieval-augmented genera- tion for knowledge-intensive NLP