of RASG: Retrieval Aug- mented Generation, Supervised Finetuning, and Structured Prompting. We did not include Structured Generation in the ablation benchmarks because it is a necessary component for BDIE. Without Structured Generation, we will not be able to guarantee that the output of the model is interpretable by downstream systems. We also ran experiments with two types of models: a commercially-available LLM, GPT-3.5, and an open-source LLM, Hermes 2 Pro - Mistral 7B. For Supervised Finetuning, we used OpenAI’s Finetuning API for GPT-3.5 while we used 8Bit QLoRA on Axolotl to finetune Hermes 2 Pro - Mistral 7B on a single 80GB A100 GPU [11] [12]. For the retrieval mechanism, we measured the ”similarity” between pages using the manhattan distance of their wavelet hashes [13]. For Structured Generation, we used OpenAI’s Tool Use API for GPT-3.5 while we used Outlines for Hermes 2 Pro - Mistral 7B [4]. Finally, we used LATIN- prompt for Structured Prompting [5]. We finetuned the models for only one epoch and only used one-shot retrieval instead of many-shot retrieval primarily due to token window limits. Business documents are often dense, and a context window containing multiple documents is too large for current language models. Overall, we ran a total of 24 = 16 experiments; one for each combination of the components and base model. We then fitted a linear model to determine the contribution of each component to the overall performance of the model. TABLE I MODEL PERFORMANCE BENCHMARKS ON KIE & LIR TASKS ON THE DOCILE DATASET Model KIE F1 Score LIR GLIRM-F1 LayoutLMv3 63.95% 70 .12% LayoutLMv3 + synthetic data 1 65.28% 71 .02% Roberta 65.88% 70 .44% Roberta + synthetic data 1 65.97% 71 .83% Roberta + DeTr Line Items 2 · · · 49.56% Roberta + DeTr Table 3 · ·