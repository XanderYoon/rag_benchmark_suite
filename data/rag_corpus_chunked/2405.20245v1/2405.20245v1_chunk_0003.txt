beat strong multi- modal baselines on BDIE using an open-source 7B LLM, Hermes 2 Pro - Mistral 7B [6]. But only a subset are necessary when using GPT-3.5 [7]. A. Notes for Finetuning for Structured Generation The language model used must output both the right content and the right structure of said content. Finetuning significantly helps with the former, less so with the latter. To ensure that the output is parseable by downstream systems, we need to zero the probabilities of invalid tokens. This is where structured generation works. Based on our experiments, we found that naively combining finetuning and structured generation leads to poor results. There are two main issues: 1. Schema vs. model mismatch : Regex-based al- gorithms for structured generation, such as Outlines’ outlines.generate.json module, implicitly impose a strict key ordering [4] [8]. E.g., suppose that we have a schema where the key "amount" comes before "currency". Then, Outlines will mask the logits for "currency" until "amount" is generated. However, if the model was finetuned to generate "currency" before "amount", the prediction arXiv:2405.20245v1 [cs.CL] 30 May 2024 Fig. 1. Retrieval Augmented Structured Generation (RASG). We model Business Document Information Extraction as a Tool Use problem with downstream APIs as the tools. We then combine Retrieval Augmented Generation, Supervised Finetuning, & Structured Generation, techniques that improve tool use capabilities of ML models, with Structured Prompting to beat strong multimodal models using only LLMs. This allows to use the largest open-source and commercial models to reach SOTA results at minimal costs. accuracy collapses. To remedy this, one must either ensure that the finetuning dataset strictly follows the specified schema, or use Context-Free Grammar-based algorithms for structured generation, such as Outlines’ outlines.generate.cfg module, which does not impose a strict key ordering. 2. Token explosion with optional keys . A common