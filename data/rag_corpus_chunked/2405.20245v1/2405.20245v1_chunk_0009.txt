This approach is similar to the one used in the GriTS metric, but in one dimension instead of two, and does not enforce column-order preservation. D. General Line Items Recognition Metric To define the General Line Items Recognition Metric (GLIRM), we first define the GLIRM-Precision and GLIRM- Recall scores as follows: GLIRM-Precf (Rp, Rt) = (1 /|Rt|) X i gf ( ˜Rp[i], ˜Rt[i]) (2) GLIRM-Recf (Rp, Rt) = (1 /|Rp|) X i gf ( ˜Rp[i], ˜Rt[i]) (3) The F1-score-like GLIRM then is, GLIRM-F1f (Rp, Rt) = 2 P i gf ( ˜Rp[i], ˜Rt[i]) |Rp| + |Rt| (4) In practice or if humans are reviewing the output of the system, recall is often more important than the precision. This is because it takes more time to look for and box-in missing cells than to verify the correctness of the extracted cells. Thus, we can define GLIRM as: GLIRM-F1β f (Rp, Rt) = (1 + β2) P i gf ( ˜Rp[i], ˜Rt[i]) β2|Rp| + |Rt| (5) where β is a hyperparameter that controls the importance of the recall over the precision. If β = 1, then the metric is the same as GLIRM-F1 f . IV. E XPERIMENTS A. Dataset We used the DocILE dataset for benchmarking [10]. This dataset is a large-scale research benchmark for machine learning evaluation of Key Information Extraction (KIE) and Line Item Recognition (LIR) from semi-structured business documents such as invoices. B. Methods We ablated three components of RASG: Retrieval Aug- mented Generation, Supervised Finetuning, and Structured Prompting. We did not include Structured Generation in the ablation benchmarks because it is a necessary component for BDIE. Without Structured Generation, we will not be able to guarantee that the output of the model is interpretable by downstream systems. We also ran experiments with two types of models: a