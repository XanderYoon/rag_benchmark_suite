I MODEL PERFORMANCE BENCHMARKS ON KIE & LIR TASKS ON THE DOCILE DATASET Model KIE F1 Score LIR GLIRM-F1 LayoutLMv3 63.95% 70 .12% LayoutLMv3 + synthetic data 1 65.28% 71 .02% Roberta 65.88% 70 .44% Roberta + synthetic data 1 65.97% 71 .83% Roberta + DeTr Line Items 2 · · · 49.56% Roberta + DeTr Table 3 · · · 72.73% Hermes 2 Pro 16.44% 7.06% Hermes 2 Pro + RASG 73.41% 69 .44% GPT-3.5 26.80% 23 .03% GPT-3.5 + RASG 75.40% 79.81% 1 Additional pretraining on synthetic data provided by the DocILE dataset. 2 Detection Transformer (DeTr) finetuned on line items detection. 3 Detection Transformer (DeTr) finetuned on table detection. C. Results Table I compares the performance of LLMs with RASG on the KIE and LIR tasks, respectively, against strong, multi- modal LayoutLMv3 and Roberta + DETR baselines [14] [15] [16] [17]. Table II shows the individual contribution of each component of RASG by base model. The minimal resources needed to beat the baselines on the KIE task are either GPT-3.5 + 1-Shot Retrieval or Hermes 2 Pro + full RASG if one is required to run inference using open source components. For the LIR task, GPT-3.5 + 1- Shot Retrieval + Structured Prompting is sufficient to beat the baselines. Finally, we measured the median table-level Information Coverage Score (ICS) for the bounding box backcalculation heuristic [18]. The best baseline, Roberta + finetuned DETR, achieves 92.93% ICS while GPT-3.5 + RASG and Hermes 2 Pro + RASG achieves 87.79% and 85.02% ICS, respectively. V. D ISCUSSION AND CONCLUSION Our model performance and ablation results demonstrate a few conclusions. Firstly, for KIE, prompt engineering only provides marginal gains compared to augmenting the model with a retrieval mechanism and/or finetuning it on the target dataset. For LIR, however, prompt engineering