face challenges in handling complex multimodal structures, such as interwoven or densely intertwined text and images. Furthermore, their application within the MRAG framework is still largely unexplored. This gap becomes even more pronounced in the context of Universal Retrieval-Augmented Generation (URAG), as both queries and documents can be of arbitrary mixed modalities. Unlike traditional settings with purely textual queries, URAG introduces dual challenges: understanding heterogeneous inputs and retrieving from equally diverse corpora. An effective retriever needs to be capable of encoding various content types and matching them with complex document structures. This imposes new technical requirements on representation learning, matching precision, and alignment with downstream VLMs, highlighting the need for a truly universal, flexible, and vision-language-aware retrieval paradigm. To address these challenges, we proposeNyx, a unified retriever designed for mixed-modal-to-mixed-modal retrieval in URAG sce- narios. To mitigate data scarcity of realistic URAG training data, we first introduce a four-stage automatic pipeline to buildNyxQA, a new dataset tailored for URAG.NyxQAconsists of three com- ponents: (1) a large-scale mixed-modal multiple-choice question answering (QA) dataset, (2) a corresponding mixed-modal docu- ment corpus, and (3) a pretraining dataset for contrastive learning. Our construction process begins with sampling naturally inter- leaved image-text documents from the web to form the corpus. We then employ a powerful VLM to generate QA pairs conditioned on these documents. To ensure high data quality, we apply a multi- step post-processing procedure, yielding a clean and diverse QA set. Finally, based on these QA pairs, we mine hard negatives from the corpus to form the pretraining triplets used for contrastive learning. Unlike existing multimodal datasets limited to specific modality combinations,NyxQAsupports retrieval and generation involving arbitrarily structured text, images, and their interleaved formats. Building upon this dataset, we adopt a two-stage training frame- work to developNyxfrom a pretrained VLM. In the first