text and image inputs [ 1, 57]. However, like LLMs, VLMs often struggle with queries needing up-to-date or external knowledge. Retrieval- Augmented Generation (RAG) addresses this by retrieving doc- uments from an external corpus to complement internal knowl- edge [10, 14, 31]. Building on this, Multimodal RAG (MRAG) extends the paradigm to settings where both queries and documents may contain text, images, or both [5, 50]. Current MRAG methods fall broadly into two categories:(1) The divide-and-conquer approach, which utilize text queries for text documents and visual queries for images;(2) The cross- modal retrieval, which uses text queries to retrieve visual content. However, both paradigms suffer from notable limitations. They often overlook the spatial and logical relationships between images and text within a document, making it difficult to capture fine- grained interactions crucial for downstream reasoning. However, web documents in the real world are often far more complex and diverse. As illustrated in Figure 1, they may include pure text, individual images, paired image-text content, or arbitrar- ily interleaved sequences of text and images. We refer to this broad spectrum of formats asmixed-modalcontent, where the interplay between modalities plays a critical role in conveying meaning. While recent efforts, such as VLM2Vec [ 24], have introduced unified multimodal embedding models, these approaches mainly focus on embedding pure text, individual images, or neatly aligned arXiv:2510.17354v1 [cs.CL] 20 Oct 2025 WWW ’26, April 13–17, 2026, Dubai, UAE Zhang et al. text-image pairs. Consequently, they face challenges in handling complex multimodal structures, such as interwoven or densely intertwined text and images. Furthermore, their application within the MRAG framework is still largely unexplored. This gap becomes even more pronounced in the context of Universal Retrieval-Augmented Generation (URAG), as both queries and documents can be of arbitrary mixed modalities. Unlike traditional settings with purely textual queries, URAG