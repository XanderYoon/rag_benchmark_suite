Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2025. InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models.CoRRabs/2504.10479 (2025). arXiv:2504.10479 https://doi.org/10.48550/arXiv.2504.10479 WWW ’26, April 13–17, 2026, Dubai, UAE Zhang et al. Appendix A Training Details We train Nyx based on the Qwen2.5-VL-3B model using a single node equipped with 8×NVIDIA A800-SXM4-80GB GPUs. To enable efficient fine-tuning, we apply Low-Rank Adaptation (LoRA) [20] with a rank of 8. Each GPU processes a batch of 20 samples, and we accumulate gradients over 4 steps, resulting in an effective batch size of 640. To prevent memory overflow when processing multi- image inputs, we cap the visual input resolution at 400 ×28×28 pixels. We use DeepSpeed with bf16 mixed-precision training and en- able gradient checkpointing for memory efficiency. The optimizer is AdamW, combined with a linear learning rate scheduler. The base learning rate is set to 1e-5, with a warmup ratio of 0.05, and a maximum gradient norm of 5.0. The contrastive loss function uses a temperature of 0.02 and a negative sampling ratio of 1. For efficient memory optimization, we employ DeepSpeed’s ZeRO optimization at stage 2, which partitions model states across devices to significantly reduce memory consumption. This allows us to train larger models without overflow. ZeRO stage 2 also en- ables communication optimizations such as allgather and reduce- scatter, improving parallelism and computational efficiency. These configurations, combined with gradient