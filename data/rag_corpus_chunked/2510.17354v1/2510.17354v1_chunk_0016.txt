we employ the Python tabulate library to convert them into a more comprehensible text table format. SciQA presents image- text questions, and we construct its corpus using associated lec- tures and QA examples.NyxQA, constructed from authentic web pages, covers a broader spectrum of input and document modalities, thereby facilitating more realistic and comprehensive evaluation of URAG in real-world web environments. During pretraining, we use the training sets of 2WikiMulti- HopQA, HotpotQA, MuSiQue, andNyxQA, treating Bamboogle, MMQA, and SciQA as out-of-domain (OOD) evaluation sets. For feedback-based fine-tuning, feedback is collected from HotpotQA, MuSiQue, MMQA, SciQA, andNyxQA, with Bamboogle serving as the OOD benchmark. For multiple-choice datasets, we reportaccuracy(Acc), while for open-ended QA, we adoptexact match(EM) andF1 score(F1), reflecting both strict correctness and token-level overlap between predictions and references. WWW ’26, April 13–17, 2026, Dubai, UAE Zhang et al. Table 1: The overall results on the six RAG datasets. To ensure consistent evaluation, the top document retrieved by each retriever was combined with the corresponding question, then input into Qwen2.5-VL-7B for answer generation. The exception is SciQA, where the retrieval content consists of one lecture and two example-based retrieval results to suit the dataset’s structure. This setup isolates the effect of the retrievers, facilitating a controlled comparison of retrieval performance. The best results are highlighted in bold, and the second-best results are underlined. Method HotpotQA Bamboogle MuSiQue SciQA MMQA NyxQA Avg. EM F1 EM F1 EM F1 Acc EM F1 Acc Direct Answer InternVL3 (8B) [57] 16.40 22.88 9.60 15.49 3.60 8.16 78.87 20.07 23.99 53.33 25.31 Qwen2.5-VL (7B) [1] 12.40 18.36 6.40 11.50 3.29 7.32 77.98 20.73 24.39 50.17 24.38 Text RAG E5-v2 (109M) [45] 14.40 19.18 7.20 12.80 2.40 6.79 – – – – – Vision-Language RAG CLIP (150M) [39] 14.00 21.12 6.40 11.64 3.20 6.74 73.07 18.03 20.67 61.50