on these QA pairs, we mine hard negatives from the corpus to form the pretraining triplets used for contrastive learning. Unlike existing multimodal datasets limited to specific modality combinations,NyxQAsupports retrieval and generation involving arbitrarily structured text, images, and their interleaved formats. Building upon this dataset, we adopt a two-stage training frame- work to developNyxfrom a pretrained VLM. In the first stage, we pretrain the retriever onNyxQAand several public contrastive learning datasets to establish general-purpose multimodal retrieval capabilities. To balance retrieval effectiveness and efficiency, we incorporate Matryoshka Representation Learning (MRL) [28], re- sulting in a compact yet expressive encoder, termedNyx-pretrained. In the second stage, we perform feedback-driven fine-tuning, align- ing the retriever with the generative preferences of downstream VLMs. This yields the final version of our retriever,Nyx. Extensive experiments demonstrate thatNyxconsistently en- hances retrieval accuracy and downstream reasoning performance in challenging mixed-modal scenarios, showcasing its strong suit- ability for URAG tasks. In conclusion, our contributions are as follows: • We pioneer the exploration of the Universal Retrieval Aug- mented Generation (URAG) problem, addressing scenarios where both queries and documents consist of arbitrarily in- terleaved image-text content. • We introduce a dataset specifically designed for real-world URAG applications, created through a comprehensive four- step web-based multimodal data synthesis pipeline. This dataset offers a rich variety of interleaved content formats, serving as an effective benchmark for practical multimodal retrieval tasks. • We propose a two-stage training paradigm to developNyx, a unified retriever optimized for URAG. The first stage in- volves contrastive pretraining using MRL on both public and synthetic datasets, resulting inNyx-pretrained. Then we utilize feedback from VLMs to refine the retriever through targeted supervision, culminating in our final model,Nyx. 2 Related Work Multimodal Retrieval-Augmented Generation (MRAG).ex- tends the traditional RAG framework to multimodal settings by retrieving text, images, or image-text pairs from