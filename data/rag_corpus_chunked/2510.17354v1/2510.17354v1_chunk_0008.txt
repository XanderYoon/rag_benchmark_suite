mixed-modal dataset designed for the URAG setting. Our dataset comprises three components: (1) a high-quality multiple- choice QA dataset DNyxQA with mixed-modal questions, (2) a corre- sponding mixed-modal document corpus Cmix, and (3) a contrastive pretraining set Dpretrain containing positive and hard negative ex- amples for retriever training. The construction ofNyxQAfollows a four-stage pipeline. First, we sample and segment web documents to create a diverse mixed- modal corpus. Next, we utilize a VLM to generate QA pairs from these document segments. This is followed by a post-processing pipeline to filter errors, refine answers, and format multiple-choice options. Finally, we employ hard negative mining using an existing retriever to produce high-quality contrastive training triplets for pretraining theNyxmodel. Web Document Sampling.To obtain naturally occurring mixed- modal documents, we sample from OBELICS [ 30], a large-scale dataset of web pages featuring interleaved text and images that reflect real-world multimodal distributions. Following standard practices in text-only RAG [25], each document is segmented into smallerchunks {ğ‘‘ğ‘– }ğ‘ ğ‘–=1, where each ğ‘‘ğ‘– contains up to 200 textual tokens (excluding image tokens from the count). This segmen- tation maintains semantic coherence and prevents length imbal- ance caused by densely illustrated documents. The resulting set of chunks forms our mixed-modal corpus Cmix ={ğ‘‘ ğ‘– }ğ‘ ğ‘–=1, comprising 46,741 segments in total. We then perform stratified sampling of 10,000 chunks from Cmix as the basis for QA pair generation, while preserving the original modality distribution. QA Pair Generation.For each sampled chunk ğ‘‘ğ‘–, whether text- only or containing images, we use a VLM to generate up to five context-independent raw QA pairs (ğ‘raw ğ‘– ğ‘— , ğ‘raw ğ‘– ğ‘— ), ensuring that each question can be answered solely based on its associated chunk. For chunks with images, we specifically prompt the VLM to create questions that reference the visual content.