its superiority. We further highlight the following insights: Performance in Text-Only RAG.Despite the powerful 11 bil- lion parameter VLM backbone of mmE5, our 3 billion parameter Nyx-pretrained model still outperforms mmE5 on HotpotQA, Bam- boogle, and MuSiQue, with performance gains of 9% and 6% on Hot- potQA and Bamboogle, respectively. This result shows the strength of targeted training. Moreover,Nyxsubstantially surpasses the text-only retriever E5 that is commonly used in RAG frameworks, further demonstrating its effectiveness in unimodal retrieval. Multimodal RAG Performance.In multimodal tasks,Nyx- pretrained performs competitively on MMQA andNyxQA, though it trails mmE5 slightly on SciQA. This may be attributed toNyx‚Äôs smaller parameter count and its broader training coverage, which includes interleaved and text-only examples. Nevertheless, its ro- bust performance across different input types highlights the ben- efit of mixed-modal training. After incorporating feedback from downstream VLMs,Nyxachieves the best performance across all multimodal benchmarks, with great results on MMQA (F1: 35.97% ‚Üí 44.50%) andNyxQA(Accuracy: 74.83% ‚Üí 81.83%). On SciQA, the gain is modest, possibly due to the limited informativeness of the provided lecture corpus. Nonetheless, fine-tuning with feedback still leads to alignment with the VLM‚Äôs preferences. A McNemar‚Äôs test was conducted onNyxQAto assess the per- formance differences among mmE5,Nyx-pretrained, andNyxas retrievers. The comparison between mmE5 andNyx-pretrained yielded a test statistic of 19.0631 ( ùúí 2, 1 degree of freedom) with a p-value of 0.0000. Furthermore, the comparison betweenNyx- pretrained andNyxresulted in a test statistic of 15.7538 and a p-value of 0.0001. These results provide strong evidence that the retrieval performance differs significantly across the methods. Beyond Gold Documents: Learning from Preference.An in- teresting observation arises fromNyxQA, where each question is originally paired with a generation-originated ‚Äúgolden‚Äù docu- ment. Although semantically relevant, these gold documents do not always lead to correct answers during inference. Our feedback analysis shows that documents preferred