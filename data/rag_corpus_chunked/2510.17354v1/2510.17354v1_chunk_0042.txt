are employed to evaluate retrieval and reasoning in purely linguistic settings, while the multimodal ones are used to as- sess cross-modal understanding and MRAG performance. Together, these datasets provide a comprehensive evaluation framework for analysing the effectiveness and generalization of our proposed method. HotPotQA[ 49] is a popular dataset for multi-hop question answer- ing, comprising questions that require synthesizing information across multiple Wikipedia articles. The dataset includes complex query types, such as comparison and bridge questions. It contains 90,447 training samples, and we follow ARPO [9, 12] use a held-out validation set with 250 examples for evaluation. 2WikiMultihopQA[ 19] is a large-scale dataset aimed at multi-hop reasoning, constructed by combining structured knowledge from Wikidata with unstructured passages from Wikipedia. It features diverse question formulations and annotated reasoning chains to facilitate explainable multi-step QA. The dataset includes 15,000 training samples, and our experiments use the test set consisting of 250 examples. Bamboogle[ 36] consists of manually curated multi-hop questions designed to test compositional reasoning. Some questions demand up to four inference steps, presenting a significant challenge in integrating information across multiple supporting facts. It provides only a test set, which we use for evaluation and which contains 125 examples. MuSiQue[ 44] focuses on sequential multi-hop inference, where each reasoning step depends on the output of the previous one. This dependency-based structure increases the difficulty of the task. The dataset comprises 19,938 training examples, and we use its development set with 250 held-out samples for evaluation. For the four text-only datasets above, we construct two separate Wikipedia-derived corporaâ€”one for training and one for evaluation. To build the training corpus, we aggregate all training questions and retrieve their top-20 relevant Wikipedia passages using the E5 retriever over the full Wikipedia dump. The retrieved passages are then deduplicated to form the final training corpus.