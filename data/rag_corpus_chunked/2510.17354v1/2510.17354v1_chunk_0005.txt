for URAG. The first stage in- volves contrastive pretraining using MRL on both public and synthetic datasets, resulting inNyx-pretrained. Then we utilize feedback from VLMs to refine the retriever through targeted supervision, culminating in our final model,Nyx. 2 Related Work Multimodal Retrieval-Augmented Generation (MRAG).ex- tends the traditional RAG framework to multimodal settings by retrieving text, images, or image-text pairs from an external corpus to support Vision-Language Models (VLMs) in generating textual responses [5]. Current methods use various retrieval strategies: dual-path strategies retrieve text with text queries and images with image queries [13, 40]; cross-modal retrieval techniques [6, 50]; and treating multimodal documents as images for retrieval [42]. In real-world applications, queries and corpora often contain mixed-modalinputsâ€”combinations of text and images. New multi- modal deep search paradigms introduce iterative retrieval [18, 33, 34, 47], where intermediate queries may also be mixed-modal. How- ever, a unified retrieval framework for URAG scenarios remains undeveloped. Multimodal Embedding Retrievers.focus on retrieving rele- vant multimodal documents by encoding both queries and docu- ments into a shared embedding space. In text-only contexts, embedding- based retrievers have shown strong performance across various tasks and languages [ 3, 11, 21, 26, 45]. Extending this to multi- modal scenarios, cross-modal retrievers like CLIP [22, 27, 39, 51] and vision-language models such as BLIP-2 [ 17, 32] encode text and images into a unified space, enabling retrieval tasks such as image-to-text, text-to-image, and image-to-image. Recent advancements [23, 24, 46, 55] have leveraged VLMs as general-purpose encoders for text, images, and image-text pairs. Other studies have focused on using synthetic data [2, 54, 56] and improving contrastive learning objectives [29, 43] to enhance em- bedding quality. Building on this, MME [53] utilized synthetic data to improve performance on interleaved text-image retrieval in the wikiHow task. However, these methods lack support for text-to-text