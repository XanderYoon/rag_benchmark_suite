multimodal embedding model designed to convert any state-of-the-art VLM into a unified embedding space. It employs a contrastive training framework on the Massive Multi- modal Embedding Benchmark (MMEB), which encompasses four meta-tasks—classification, visual question answering, multimodal retrieval, and visual grounding—across 36 datasets. Unlike models such as CLIP or BLIP, which process text and images independently, VLM2Vec integrates both modalities based on task-specific instruc- tions to produce fixed-dimensional vector representations. mmE5[ 2] is a multimodal multilingual embedding model that en- hances performance by leveraging high-quality synthetic datasets. These datasets encompass a wide range of tasks, modality combi- nations, and languages, and are generated using a deep thinking process within a single pass of a VLM. The synthetic data incorpo- rates real-world images with accurate and relevant texts, ensuring fidelity through self-evaluation and refinement. The model’s effec- tiveness underscores the potential of high-quality synthetic data in improving multimodal multilingual embeddings. VisRAG-Ret[ 50] is a VLM-based retriever component of the Vis- RAG framework, designed to enhance retrieval-augmented gener- ation by directly processing document images. Unlike traditional text-based RAG systems that rely on parsed text, VisRAG-Ret uti- lizes a VLM to embed document images, preserving the visual layout and content. It employs a bi-encoder architecture, mapping both the query and document images into a shared embedding space, facilitating efficient retrieval. C Dataset Details In this section, we describe the datasets used in our experiments, covering both text-only and multimodal benchmarks. The text- only datasets are employed to evaluate retrieval and reasoning in purely linguistic settings, while the multimodal ones are used to as- sess cross-modal understanding and MRAG performance. Together, these datasets provide a comprehensive evaluation framework for analysing the effectiveness and generalization of our proposed method. HotPotQA[ 49] is a popular dataset for multi-hop question answer- ing, comprising questions that require synthesizing information