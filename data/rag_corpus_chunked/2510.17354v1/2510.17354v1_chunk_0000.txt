Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation Chenghao Zhang Guanting Dong Renmin University of China Beijing, China chenghao-zhang@outlook.com Xinyu Yang Zhicheng Dou Renmin University of China Beijing, China dou@ruc.edu.cn Abstract Retrieval-Augmented Generation (RAG) has emerged as a pow- erful paradigm for enhancing large language models (LLMs) by retrieving relevant documents from an external corpus. However, existing RAG systems primarily focus on unimodal text documents, and often fall short in real-world scenarios where both queries and documents may contain mixed modalities (such as text and im- ages). In this paper, we address the challenge of Universal Retrieval- Augmented Generation (URAG), which involves retrieving and rea- soning over mixed-modal information to improve vision-language generation. To this end, we proposeNyx, a unified mixed-modal to mixed-modal retriever tailored for URAG scenarios. To miti- gate the scarcity of realistic mixed-modal data, we introduce a four-stage automated pipeline for data generation and filtering, leveraging web documents to constructNyxQA, a dataset compris- ing diverse mixed-modal question-answer pairs that better reflect real-world information needs. Building on this high-quality dataset, we adopt a two-stage training framework forNyx: we first per- form pre-training onNyxQAalong with a variety of open-source retrieval datasets, followed by supervised fine-tuning using feed- back from downstream vision-language models (VLMs) to align retrieval outputs with generative preferences. Experimental re- sults demonstrate thatNyxnot only performs competitively on standard text-only RAG benchmarks, but also excels in the more general and realistic URAG setting, significantly improving gen- eration quality in vision-language tasks. Our code is released at https://github.com/SnowNation101/Nyx CCS Concepts •Information systems → Web search engines;Multimedia and multimodal retrieval;Web mining;Question answering; Retrieval models and ranking. Keywords Multimodal Retrieval-Augmented Generation, Vision-Language Model, Multimodal Embedding, Contrastive Learning Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that