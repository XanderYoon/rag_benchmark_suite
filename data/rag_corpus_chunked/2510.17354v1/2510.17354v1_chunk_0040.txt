sampling ratio of 1. For efficient memory optimization, we employ DeepSpeed’s ZeRO optimization at stage 2, which partitions model states across devices to significantly reduce memory consumption. This allows us to train larger models without overflow. ZeRO stage 2 also en- ables communication optimizations such as allgather and reduce- scatter, improving parallelism and computational efficiency. These configurations, combined with gradient checkpointing and mixed- precision training, work together to maximize both performance and memory efficiency during training. B Baseline Retriever Models In this section, we present the baseline retriever models employed in our main experiments. These models capture several recent advances in text and multimodal retrieval, and provide strong base- lines for assessing the effectiveness of our proposed method. E5[ 45] is a series of cutting-edge text embeddings that perform exceptionally well across a variety of tasks. The model is trained us- ing a contrastive approach, with weak supervision signals derived from a carefully curated large-scale text pair dataset. It demon- strates strong performance both in zero-shot scenarios and after fine-tuning. CLIP[ 39] is a powerful multimodal model developed by OpenAI that learns visual and textual representations jointly. It is trained using a large dataset of image-text pairs in a contrastive manner, enabling it to understand images and texts in a shared embedding space. This approach allows CLIP to perform a variety of tasks, such as zero-shot image classification, image search, and text-to-image retrieval, without task-specific fine-tuning. VLM2Vec[24] is a versatile multimodal embedding model designed to convert any state-of-the-art VLM into a unified embedding space. It employs a contrastive training framework on the Massive Multi- modal Embedding Benchmark (MMEB), which encompasses four meta-tasks—classification, visual question answering, multimodal retrieval, and visual grounding—across 36 datasets. Unlike models such as CLIP or BLIP, which process text and images independently, VLM2Vec integrates both modalities based