same contrastive learning framework described earlier, thus obtaining the final retrieverNyx. 4 Main Experiments Our main experiments consist of two parts, where we first evaluate the generation performance in URAG scenarios and then examine the embedding performance, given that the model is inherently an embedding model. All experiments were conducted on a single node equipped with 8 √óNVIDIA A800-SXM4-80GB GPUs. For efficient training, we applied LoRA [ 20] with a rank of 8. The per-device batch size was set to 20 with 4 gradient accumulation steps, and the temperature parameter ùúè in the InfoNCE loss was fixed at 0.02. To avoid memory overflow when processing multi-image samples, the maximum visual input resolution was limited to400 √ó 28 √ó 28pixels. Additional implementation details can be found in the appendix. 4.1 Experimental Setup Datasets and Metrics.We evaluate RAG pipelines incorporat- ing our retriever across two categories: (1) text-only datasets and (2) multimodal datasets, including MRAG and URAG. For text-only RAG, followingReCall[ 4], we evaluate onHot- potQA[ 49],MuSiQue[ 44], andBamboogle[ 36]. For each ques- tion, we use E5-v2 to retrieve the top 20 documents from Wikipedia, merge and deduplicate them to create a task-specific corpus, and randomly sample up to 250 questions per dataset. In the case of MRAG and URAG, we utilizeMultimodalQA (MMQA) [41],ScienceQA(SciQA) [ 35], andNyxQA. MMQA re- quires retrieval from a corpus containing text, tables, and images. Since the tables in the MMQA corpus are originally stored in JSON format, we employ the Python tabulate library to convert them into a more comprehensible text table format. SciQA presents image- text questions, and we construct its corpus using associated lec- tures and QA examples.NyxQA, constructed from authentic web pages, covers a broader spectrum of input and document modalities, thereby facilitating more realistic and comprehensive evaluation of URAG in real-world web environments.