Answer InternVL3 (8B) [57] 16.40 22.88 9.60 15.49 3.60 8.16 78.87 20.07 23.99 53.33 25.31 Qwen2.5-VL (7B) [1] 12.40 18.36 6.40 11.50 3.29 7.32 77.98 20.73 24.39 50.17 24.38 Text RAG E5-v2 (109M) [45] 14.40 19.18 7.20 12.80 2.40 6.79 – – – – – Vision-Language RAG CLIP (150M) [39] 14.00 21.12 6.40 11.64 3.20 6.74 73.07 18.03 20.67 61.50 23.64 VLM2Vec (4B) [24] 14.40 22.08 10.40 16.95 3.60 10.12 79.56 19.91 23.34 56.50 25.69 VisRAG-Ret (3B) [50] 12.08 19.84 8.80 16.05 3.60 8.29 80.45 18.84 21.55 64.33 25.38 mmE5 (11B) [2] 17.60 24.30 13.60 18.69 5.20 9.70 81.40 34.00 38.50 66.83 30.98 Ours Nyx-pretrained (3B) 22.00 31.38 16.00 22.87 5.60 11.00 81.33 31.75 35.97 74.83 33.27 Nyx(3B)24.40 33.19 16.80 25.93 7.20 12.80 81.75 39.66 44.50 81.83 36.46 Baseline Models.For the text-only retriever, we use E5-v2 [ 45] as the unimodal RAG baseline, since it serves as the backbone model for constructing our text-only retrieval datasets and is also one of the most widely used retrievers in text-based RAG systems [ 25]. For multimodal retrievers, we use well-supervised fine-tuned em- bedding models CLIP [39], VLM2Vec [24] and mmE5 [2], as well as a retriever for visual document retrieval for RAG, VisRAG-Ret [50]. We also report the direct answering results of InternVL3-8B [57] and Qwen2.5-VL-7B as baselines for comparison. 4.2 Results on Generation Performance Our generation performance results are presented in Table 1. Over- all,Nyxconsistently outperforms all baselines, clearly demonstrat- ing its superiority. We further highlight the following insights: Performance in Text-Only RAG.Despite the powerful 11 bil- lion parameter VLM backbone of mmE5, our 3 billion parameter Nyx-pretrained model still outperforms mmE5 on HotpotQA, Bam- boogle, and MuSiQue, with performance gains of 9% and 6% on Hot- potQA and Bamboogle, respectively. This result shows the strength of targeted training. Moreover,Nyxsubstantially surpasses