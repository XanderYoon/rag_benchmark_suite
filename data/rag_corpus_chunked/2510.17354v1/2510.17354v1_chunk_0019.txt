0.0001. These results provide strong evidence that the retrieval performance differs significantly across the methods. Beyond Gold Documents: Learning from Preference.An in- teresting observation arises fromNyxQA, where each question is originally paired with a generation-originated “golden” docu- ment. Although semantically relevant, these gold documents do not always lead to correct answers during inference. Our feedback analysis shows that documents preferred by the VLM may differ from the labelled positives. Incorporating this preference signal during fine-tuning leads to a 7-point accuracy gain onNyxQA. This suggests the importance of further aligning retrieval models with downstream generative utility in URAG systems. 4.3 Embedding Capability Analysis Although aligning mixed-modal retriever with downstream models can enhance the generative quality of the final VLM, the capability of the retriever itself is also of central importance. We evaluate the embedding ability of our models on the MMEB benchmark [ 24], and the results are reported in Table 2. In the table, models from CLIP to MMRet are evaluated in the zero-shot setting, whereas the remaining models are trained with MMEB-labelled data. In particu- lar,mmE5-Qwen2.5-3Bis trained on Qwen-2.5-VL-3B-Instruct, with Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation WWW ’26, April 13–17, 2026, Dubai, UAE Table 2: Performance comparison on the MMEB benchmark, which includes 36 tasks spanning four categories: classifi- cation (Class.), visual question answering (VQA), retrieval (Retr.), and visual grounding (Ground.). Models Per Meta-Task Score Overall Class. VQA Retr. Ground. CLIP [39] 42.8 9.1 53.0 51.8 37.8 BLIP2 [32] 27.0 4.2 33.9 47.0 25.2 OpenCLIP [7] 47.8 10.9 52.3 53.3 39.7 E5-V [23] 21.8 4.9 11.5 19.0 13.3 MagicLens [52] 38.8 8.3 35.4 26.0 27.8 MMRet [56] 47.2 18.4 56.5 62.2 44.0 VLM2Vec [24] 52.8 50.3 57.8 72.3 55.9 mmE5 [2] 67.6 62.8 70.9 89.7 69.8 mmE5-Qwen-3B 56.6 56.0 59.4 71.5 59.0 Nyx-pretrained 55.2 53.7 58.4 70.5 57.5