the relevance of retrieved documents. This can be done using an LLM [Es et al., 2023], an already prepared test set with labeled datasets [Tang and Yang, 2024], or human judgment [Afzal et al., 2024]. Another important aspect is the generator quality, where the generated answer is evaluated for quality attributes like faithfulness [Es et al., 2023]. In this case, LLMs [Es et al., 2023], embedding techniques [Kukreja et al., 2024], token- based approaches [Li et al., 2024], or human evaluators [Pipitone and Alami, 2024] may be employed. Additionally, the overall performance of the system is measured by factors such as retrieval time and processing speed [Kukreja et al., 2024]. These various quality metrics help assess the effectiveness of each component of the RAG system and ensure optimal performance. Most existing research focuses on broad use cases, offering little insight into how RAG systems are applied in real-world corporate settings. Furthermore, there is a noticeable lack of empirical qualitative studies exploring the challenges organizations face when adopting and utilizing these systems. To address this gap, our research focuses on industrial use cases, enhancement strategies, and evaluation approaches. 3 Methodology For conducting our semi-structured interviews, we stuck to the step-wise guidelines by [Adams, 2015], considering in addition the recommendations for software engineering interviews by [Hove and Anda, 2005]. Each interview was scheduled to take approximately one hour and was structured into five main parts: Company and Interviewee Background, Adoption and Implementation of RAGs, Requirements for RAGs, Quality Assessment of RAGs, and Outcomes and Future Outlook, with mostly open questions and few closed questions, as recommended by [Adams, 2015]. Before conducting the semi-structured interviews, the interview guidelines were pilot-tested with one individual to refine the guide. We selected purposive sampling [Campbell et al., 2020] to contact companies from the authorsâ€™ network that