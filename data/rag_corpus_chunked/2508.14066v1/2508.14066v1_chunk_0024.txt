to the system. If the underlying data or any parameter of a component is modified, a comprehensive re- evaluation is necessary. Continuous evaluation should also be part of the deployment phase to ensure the system maintains consistent performance in real- world conditions [B,I,K]. 4.4 RQ4: Industrial RAG-Evaluation In research, various evaluation frameworks have been proposed [Brehme et al., 2025b], ranging from fully automated to entirely manual approaches. In industrial use cases, evaluations were predominantly performed manually by human experts [A,B,C,E,I,J,L,M]. For this manual approach, testers analyzed the retrieved documents and generated answers to assess whether they are correct and relevant to the input queries, based on their human perception. This was sometimes further unified by selecting a group of testers who manually reviewed the RAG outputs using a predefined question test set [I,J]. In some cases, a user feedback UI mechanism was also integrated, like thumbs up/down buttons, allowing users to rate responses [A,B]. These ratings were then reviewed to identify issues and iteratively improve the system. In some cases, this was further extended by incorporating categories, such as tone preferences (e.g., ”I do not like the tone.“) to refine the evaluation process [B,H]. Apart from participants [I,B], who applied specific evaluation metrics, such as tone [B], correctness [I], coherence [I], answer quality [I], robustness [I], and the alignment between the retriever and generator components [B,I], the remaining participants primarily relied on their personal experience for assessment. Only two participants described the use of automated evaluation methods with the help of AI [K,G]. For participant K, the system output was binary (i.e., correct or incorrect), enabling them a straightforward evaluation based on boolean values. The test dataset was constructed from anonymized historical queries, allowing for the evaluation of RAG within an existing workflow where only a system component had been