as those outlined by [Zhao et al., 2024]. Based on these insights, we provide practical recommendations on key priorities and common pitfalls to avoid when adopting RAG systems. RQ3: What challenges do companies face when implementing and using RAG systems, and what lessons have been learned from these experiences? Lastly, we investigate how industry practitioners assess the quality of their RAG systems, focusing on whether and how they have adopted evaluation methods from academic research in their practical applications. Academic research has proposed several automated evaluation methods for assessing entire RAG systems [Brehme et al., 2025b]. In this study, we examine whether and how these methods have been adopted in industry by exploring if practitioners evaluate their systems against the specific requirements they identified and measure the extent to which these requirements are fulfilled: RQ4: How do industry practitioners evaluate the quality of RAG systems? To explore these questions, we conducted 13 semi-structured interviews, following stepwise guidelines [Adams, 2015] and a systematic five-step qualitative procedure [Schmidt, 2004], focusing on industry experts directly involved in the implementation of RAG systems. Our main findings are as follows: • The most common applications of implemented RAG systems are QA tasks, with each RAG typically designed for a specific use case within a limited domain. • The technical readiness level in most companies remains at the prototype stage. • The industry’s main requirement focus is on data protection, security, and quality, whereas most of the time ethical considerations, bias mitigation, costs, and scalability are less prioritized. • One of the main challenges is data preprocessing for preparing the RAG system, which is critical for the overall system quality. • RAG systems are primarily evaluated by humans, rather than through automated methods using LLMs. 2 Related Work This section explores three key aspects: types of