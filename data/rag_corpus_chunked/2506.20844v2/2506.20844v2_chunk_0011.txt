a common issue in the relatively small datasets characteristic of scientific fact-checking [ 97, 116]. Furthermore, poor verification performance deteriorates retrieval accuracy, cre- ating a vicious feedback loop that further degrades overall system effectiveness. A multi-pronged strategy could mitigate these chal- lenges by pooling verification signals from various high-performing verifier models, leveraging large-scale datasets such as FEVER [90] to improve training robustness, and providing a shared retrieval checkpoint enable subsequent studies to fine-tune the model for specific scenarios or datasets while reducing training cost. Recent work [51, 74] has explored unified retrieval models for knowledge- intensive NLP tasks, focusing on retrieval quality and downstream task utility [73, 115], including question answering (QA) and fact- checking. Similarly, we propose a verification-driven IR system for evidence retrieval, which explicitly incorporates evidential infor- matics. This approach follows a two-step training paradigm: general pre-training on large, diverse datasets followed by domain-specific fine-tuning. This approach balances scalability and domain speci- ficity, ensuring IR models are both robust across different contexts and highly effective in targeted fact-checking applications. Addi- tionally, a corresponding benchmark should employ a diverse set of evaluation metrics beyond for the fact-checking task to ensure comprehensive assessment of performance within fact-checking [6]. These metrics could include verification accuracy, reflecting the downstream utility of retrieved evidence; decision latency, measur- ing the computational efficiency of retrieval models; and robustness to real-world conditions such as noisy data and incomplete evidence, to improve system resilience. Integrating verification feedback into evidence retrieval improves relevance assessment beyond binary labels, enhancing retrieval per- formance. Future research should focus on developing a benchmark IR system tailored for fact-checking, incorporating fine-grained relevance labels and verification-driven retrieval models. A scalable pre-training and fine-tuning approach has the potential to improve retrieval robustness and generalizability thereby producing more accurate and efficient fact-checking systems. 3 Imbalanced