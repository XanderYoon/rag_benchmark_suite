4 shows that well- studied claims tend to have higher top-ranked scores and sharper decay patterns, suggesting that relevance distributions may serve as indicators of evidence sufficiency. Table 4: Statistical analysis of average relevance scores for well-studied and less-studied claims. ‘I/F’ ratio is Initial-to- Final ratio. ‘Exp k’ denotes the exponential decay factor k. Metric 1st Doc Mean Sum I/F ratio Exp k Less- 0.941 0.502 25.097 3.514 1.544 Well- 0.995 0.741 37.052 1.963 0.651 Based on these findings, one possible direction is to leverage existing techniques to estimate whether a claim is less-studied or well-studied. A prediction module could utilise statistical features of relevance distribution such as those presented in Table 4. In addi- tion, metadata such as retrieved entity counts in PubMed (Table 3) can serve as auxiliary signals to refine the prediction. Claims pre- dicted as less-studied – e.g., with low total relevance or steep score decay – may be assigned smaller cut-offs to reduce verification cost, while RLT and stopping techniques could be applied to well-studied claims where concentrated high scores suggest richer evidence. While this naive strategy relies on heuristic features, it does not explicitly optimise verification performance. To address this, future approaches could explore learning a cut-off policy using feedback from the verification stage. Specifically, truncation points may be selected based on reward signals, such as whether the claim is cor- rectly verified or the confidence of the verifier. This would bypass the need for relevance-labelled supervision, which is often infeasi- ble in scientific fact-checking due to sparse annotations. Inspired by prior RLT and stopping method work, such a learned policy could optimise both efficiency and factual accuracy by aligning truncation decisions with downstream verification performance. 4 Time and Citation General fact-checking evidence corpus such as Wikipedia and fact- checking websites, often lack sentence-level