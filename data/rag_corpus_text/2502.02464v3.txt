Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation Abdelrahman Abdallah University of Innsbruck Innsbruck, Tyrol, Austria abdelrahman.abdallah@uibk.ac.at Bhawna Piryani University of Innsbruck Innsbruck, Tyrol, Austria bhawna.piryani@uibk.ac.at Jamshid Mozafari University of Innsbruck Innsbruck, Tyrol, Austria jamshid.mozafari@uibk.ac.at Mohammed Ali University of Innsbruck Innsbruck, Tyrol, Austria Mohammed.ali@uibk.ac.at Adam Jatowt University of Innsbruck Innsbruck, Tyrol, Austria adam.jatowt@uibk.ac.at Abstract Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical components of modern applications in information re- trieval, question answering, or knowledge-based text generation. However, existing solutions are often fragmented, lacking a uni- fied framework that easily integrates these essential processes. The absence of a standardized implementation, coupled with the com- plexity of retrieval and re-ranking workflows, makes it challenging for researchers to compare and evaluate different approaches in a consistent environment. While existing toolkits such as Rerankers and RankLLM provide general-purpose reranking pipelines, they often lack the flexibility required for fine-grained experimentation and benchmarking. In response to these challenges, we introduce Rankify, a powerful and modular open-source toolkit designed to unify retrieval, re-ranking, and RAG within a cohesive framework. Rankify supports a wide range of retrieval techniques, including dense and sparse retrievers, while incorporating state-of-the-art re- ranking models to enhance retrieval quality. Additionally, Rankify includes a collection of pre-retrieved datasets to facilitate bench- marking, available at Huggingface1. To encourage adoption and ease of integration, we provide comprehensive documentation2, an open-source implementation on GitHub3, and a PyPI package for easy installation4. As a unified and lightweight framework,Rankify allows researchers and practitioners to advance retrieval and re- ranking methodologies while ensuring consistency, scalability, and ease of use. 1https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light 2http://rankify.readthedocs.io/ 3https://github.com/DataScienceUIBK/rankify 4https://pypi.org/project/rankify/ Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’25, July 13–18, 2025, Padova, IT © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX Figure 1: Rankify logo. CCS Concepts •Information systems → Information retrieval; Evaluation of retrieval results; Presentation of retrieval results; Top-k retrieval in databases; Rank aggregation. Keywords Neural IR, Dense Retrieval, Sparse Retrieval, Re-ranking, Toolkit ACM Reference Format: Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, and Adam Jatowt. 2018. Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation. In Pro- ceedings of Make sure to enter the correct conference title from your rights confirmation emai (SIGIR ’25). ACM, New York, NY, USA, 12 pages. https: //doi.org/XXXXXXX.XXXXXXX 1 Introduction Information retrieval (IR) systems [16, 86] are fundamental to many applications [56, 98], including question-answering [4, 43], search engines [20, 103], and knowledge-based generation [ 57]. These systems often rely on a two-stage pipeline: a retriever that efficiently identifies a set of candidate documents and a re-ranker that refines these results to maximize relevance to the query [66, 79, 109]. This approach has proven highly effective, with retrieval and re-ranking methods achieving state-of-the-art performance across diverse NLP benchmarks. Retrievers form the backbone of information retrieval systems by identifying a subset of documents relevant to a user’s query. The retrieval landscape includes sparse, dense, and hybrid methods. Sparse retrievers, such as BM25 [ 80], rely on exact term match- ing by representing queries and documents as high-dimensional sparse vectors. These methods are highly effective when query terms closely align with document content but they struggle to capture semantic relationships. Dense retrievers, including models arXiv:2502.02464v3 [cs.IR] 19 Feb 2025 SIGIR ’25, July 13–18, 2025, Padova, IT Abdallah et al. Table 1: Comparison of Retriever, Re-ranking, and RAG toolkits. Modular Design indicates if the toolkit uses modular components. Automatic Evaluation refers to the availability of built-in evaluation tools. Corpus shows whether the toolkit provides utilities for corpus processing, such as cleaning and chunking. The Combination column represents the total number of possible configurations, calculated as the product of the number of datasets, retrievers, re-rankers, and RAG methods (e.g., for Rankify: 40 × 7 × 24 × 3 = 20,160). Toolkit # Datasets # Retrievers # Re-rankers # RAG Methods Modular Design Automatic Evaluation Corpus Combination FlashRAG [39] 32 3 2 12 ✓ ✓ ✓ 2,304 FastRAG [2] 0 1 3 7 ✓ ✗ ✗ 21 AutoRAG [46] 4 2 7 1 ✓ ✓ ✗ 56 LocalRQA [111] 0 2 0 0 ✗ ✓ ✗ 2 Rerankers [19] 0 0 15 0 ✓ ✓ ✗ 15 CHERCHE [88] 0 7 2 0 ✗ ✗ ✗ 14 Rankify 40 7 24 3 ✓ ✓ ✓ 20,160 like DPR [109], BPR [105], ColBERT [83], BGE [13], Contriever [36] and ANCE [ 104], overcome this limitation by encoding queries and documents into low-dimensional dense vectors using neural networks, enabling retrieval based on semantic similarity even when lexical overlap is minimal. However, dense retrieval requires significant computational resources for training and inference. Hy- brid retrieval methods [ 15, 106] integrate the strengths of both approaches, combining the precision of sparse retrievers with the semantic understanding of dense models to achieve a more balanced and robust retrieval system. Re-ranking techniques enhance the initial retrieval results by ensuring the most relevant documents appear at the top. These methods are categorized into three main approaches: (1) Pointwise reranking [3, 4, 66, 82] treats reranking as a regression or classifica- tion task, assigning independent relevance scores to each document. (2) Pairwise reranking [35, 73, 87] refines ranking by comparing document pairs and optimizing their relative order based on rele- vance. However, this approach treats all document pairs equally, sometimes improving lower-ranked results at the expense of top- ranked ones. (3) Listwise reranking [71, 90, 110] considers the en- tire document list, prompting LLMs with the query and a subset of candidates for reranking. Due to input length limitations, these methods tend to employ a sliding window strategy, progressively refining rankings from back to front. Retrieval-Augmented Generation (RAG) [27, 51] enhances lan- guage models by integrating retrieval and generation, making them more effective in knowledge-intensive tasks. Rather than relying solely on pre-trained knowledge, RAG dynamically retrieves rele- vant documents from external sources during inference, incorpo- rating them into the generation process. The growing complexity and diversity of retrieval, re-ranking, and RAG methods pose significant challenges for benchmarking, re- producibility, and integration. Existing toolkits, such as Pyserini [52], Rerankers [19] and RankLLM [ 70] often lack flexibility, enforce rigid implementations, and require extensive preprocessing, mak- ing them less suitable for research-driven experimentation. Addi- tionally, retrieval and re-ranking datasets are scattered across differ- ent sources, complicating evaluation and comparison. To address these challenges, we introduce Rankify, an open-source frame- work that unifies retrieval, re-ranking, and RAG into a modular and extensible ecosystem (logo shown in Figure 1). Rankify sup- ports diverse retrieval techniques, integrates the state-of-the-art re-ranking models, and provides curated pre-retrieved datasets to streamline experimentation. Designed for maximizing flexibility, it enables researchers to efficiently build, evaluate, and extend re- trieval pipelines while ensuring consistency in benchmarking. The main contributions of this paper are as follows: • Curated retrieval datasets and precomputed embed- dings: Rankify provides 40 datasets, each with 1,000 pre- retrieved documents per query, across various domains (QA, dialogue, entity linking, etc.). It also includes pre-computed Wikipedia and MS MARCO corpora for multiple retrievers, eliminating preprocessing overhead. • Diverse retriever and re-ranking support: Rankify inte- grates dense (DPR, ANCE, BPR, ColBERT, BGE, Contriever) and sparse (BM25) retrievers, along with 24 state-of-the-art re-ranking models, enabling seamless retrieval and ranking experimentation. • RAG integration and evaluation tools: Rankify bridges retrieval, re-ranking, and RAG by passing retrieved docu- ments to LLMs for evaluation. • Comprehensive evaluation tools: Rankify offers a di- verse range of evaluation metrics and tools for retrieval and question answering. The framework is accompanied by ex- tensive online documentation, making it easy for users to explore its features. Additionally,Rankify is freely available on PyPI and GitHub, ensuring accessibility for researchers and practitioners. 2 Related Work Research in information retrieval (IR), re-ranking, and retrieval- augmented generation (RAG) has progressed significantly over the past decade. Traditional retrieval models like BM25 [ 80] offered robust lexical matching capabilities but struggled with capturing semantic relationships. This limitation led to the development of dense retrieval methods [49], which leverage pre-trained neural encoders to represent queries and documents in a shared semantic space. Notable approaches, such as DPR [ 109], ANCE [104], and multi-vector models like ColBERT [ 45], have demonstrated sub- stantial improvements in retrieval effectiveness. Hybrid retrievers, Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and RAG SIGIR ’25, July 13–18, 2025, Padova, IT Raw DocumentsIndexes Datasets Collection NQ-DPR NQ-BM25 NQ-ColBert ….. ….. ….. WebQ-DPR WebQ-BM25 WebQ-ColBert ….. ….. ….. PopQA-DPR PopQA-BM25 PopQA-ColBert DPR-Wiki DPR-MsMarco BM25-MsMarcoBM25-Wik ….. ….. Select Retriever dataset Combination Send Questions (Select Retriever and Corpus) Dense Retriever Sparse Retriever Pointwise Reranking Listwise Reranking 12345 1 234 5 Top-K documents Prompt Augmentation Generator Model Final Answer Pre-Retrieved Datasets & Indexing ….. Retrieval & Re-Ranking Generator Pairwise Reranking Zero Shot Figure 2: An overview of the Rankify pipeline, demonstrating its dual capability for document retrieval. Users can interact with the system either by providing a query to retrieve relevant documents in real-time or by leveraging pre-retrieved datasets already indexed by the framework. The process starts with Pre-Retrieved Datasets & Corpus Indexing, where documents are indexed using both dense (e.g., DPR) and sparse (e.g., BM25) retrievers across corpus like Wikipedia and MS MARCO. Next, in the Retrieval & Re-Ranking stage, the system retrieves candidate documents using dense, or sparse retrieval methods and re-ranks them with pointwise pairwise, or listwise models powered by large language models (LLMs). Finally, the Generator stage applies prompt augmentation and uses models like Fusion-in-Decoder (FiD) to generate accurate and contextually informed answers. combining sparse and dense signals [26, 58], further enhance per- formance by leveraging both lexical and semantic features. Recent advancements, including knowledge distillation [74] and curricu- lum learning [113], continue to refine retrieval performance across diverse datasets. Re-ranking methods have also evolved alongside retrieval tech- niques to improve the ordering of retrieved documents. Traditional pointwise [114] and pairwise [11] approaches have given way to listwise methods like LambdaRank and ListNet [10, 55]. Deep neural models, including cross-encoders and transformer-based architec- tures, have demonstrated remarkable success in re-ranking tasks by capturing complex interactions between queries and documents. Zero-shot and in-context re-ranking with large language models (LLMs), such as GPT-4 [5] and RankT5 [115], now enable effective ranking adjustments without task-specific training. Retrieval-Augmented Generation (RAG) has emerged as a pow- erful paradigm for enhancing generative models in knowledge- intensive tasks [51]. By retrieving relevant documents and inte- grating them into the generative process, RAG systems improve factual accuracy and reduce hallucinations. Techniques like self- consistency [101] and noise filtering [25] have been proposed to further improve the reliability of RAG outputs. However, the effec- tiveness of these systems heavily depends on the quality of retrieved and re-ranked documents, highlighting the need for robust retrieval frameworks. In response to the growing complexity of IR, re-ranking, and RAG tasks, several frameworks have been introduced. Rerankers [19] provides a lightweight Python interface for common re-ranking models, while RankLLM focuses on listwise re-ranking with LLMs. Other frameworks like FlashRAG [39] and AutoRAG [46] offer mod- ular components for RAG experimentation, though they often lack support for diverse datasets and advanced retriever configurations. Tools such as LangChain [12], LlamaIndex [54], and DSPy [44] fur- ther contribute to the ecosystem by simplifying model integration and workflow design. A comparison of retrieval, re-ranking, and RAG toolkits is presented in Table 1, highlighting the advantages of Rankify in dataset diversity, retriever and re-ranker support, and modularity. 3 Rankify Modern information retrieval systems rely on a combination of retrieval, ranking, and generative techniques to surface relevant content and generate accurate responses. However, existing solu- tions are often fragmented, requiring researchers and practitioners to integrate multiple toolkits to experiment with different retrieval and ranking strategies.Rankify addresses this challenge by offering an end-to-end retrieval and ranking ecosystem, enabling seamless experimentation across a wide range of retrieval, re-ranking, and retrieval-augmented generation (RAG) models. Unlike existing retrieval and ranking frameworks that focus on a single aspect of the pipeline, Rankify is designed to be modular, SIGIR ’25, July 13–18, 2025, Padova, IT Abdallah et al. Data Retrievers Rerankers RAG Corpora Wikipedia MS MARCO Datasets Methods DPR BM25 BGB Other retreivers AnceContriever Pre-defined datasets NQ WebQ Hotpot QA Archivial QA America QA ColBERT Other Pre-retrieved datasets Other Corpus Pointwise Reranking Listwise Reranking RankT5 MonobertFlashrank Sentence Transformer UPRAPIRanker Other Rerankers Other Rerankers RankGPT ListT5 LiT5 LLMLayerwise Zero-shot FiD In-Context Other RAG Musique WoW WNED Fever ASQA Strategy QA Inranker MonoT5 Blender Twolar EchoRank Transformer Ranker Incontext-RankerLLM2VecColBERT Splade FRIST RankGPT- api Vicuna Zephyr Question Answering Temp QA Entity Question Muti-Hop QA Fact QA Mutiple Choice Dialog Generation Long Form QA Entity LinkingSolt Filling Open Domain Summarization Others TriviaQA Figure 3: The architecture of the Rankify, showing the in- terplay between its core modules: Datasets, Retrievers, Re- Rankers, and RAG Evaluation. Each module operates inde- pendently while seamlessly integrating with others, enabling end-to-end retrieval and ranking workflows. extensible, and lightweight, allowing users to plug in different re- trievers, ranking models, and RAG techniques with minimal effort. Built in Python, it leverages machine learning libraries such as TensorFlow [1], PyTorch [67], Spacy [34], and Pyserini [52] . The framework is available on PyPI5, making installation straightfor- ward: $ pip install rankify Rankify supports both precomputed and real-time retrieval workflows, allowing users to benchmark with pre-retrieved datasets or perform live document retrieval on large-scale corpora. The framework includes ready-to-use indices for Wikipedia and MS MARCO, eliminating the need for time-consuming indexing. The architecture of Rankify consists of four core modules: • Datasets: Provides standardized access to datasets like Nat- ural Questions, TriviaQA, and HotpotQA, with support for custom dataset creation. • Retrievers: Integrates diverse retrieval methods such as BM25, DPR, ANCE, BGE, and ColBERT, enabling flexible retrieval strategies. 5https://pypi.org/project/rankify/ Table 2: Summary of datasets used in Rankify. Categories include QA, Multi-Hop QA, Long-Form QA, Multiple-Choice, Entity-Linking, Slot Filling, Fact Verification, Dialog Genera- tion, Summarization, and Other specialized datasets. Task Dataset Name # Train # Val # Test QA NQ [47] 79,168 8,757 3,610 TriviaQA [40] 78,785 8,837 11,313 WebQ [8] 3,778 - 2,032 SQuAD [76] 87,599 10,570 - NarrativeQA [81] 32,747 3,461 10,557 MSMARCO-QA [62] 808,731 101,093 - PopQA [59] - - 14,267 SIQA [84] 33,410 1,954 - Fermi [41] 8,000 1,000 1,000 WikiQA [107] 20,360 2,733 6,165 AmbigQA [47, 61] 10,036 2,002 - CommenseQA [92] 9,741 1,221 - PIQA [9] 16,113 1,838 - BoolQ [17] 9,427 3,270 - Multi-Hop QA 2WikiMultiHopQA [32] 15,000 12,576 - Bamboogle [72] - - 125 Musique [97] 19,938 2,417 - HotpotQA [108] 90,447 7,405 - Temp QA ArchivalQA [99] 384,426 48,304 48760 ChroniclingQA [69] 385,629 21,739 21,735 Long-Form QA ELI5 [24] 272,634 1,507 - ASQA [89] 4,353 948 - Multiple-Choice MMLU [30, 31] 99,842 1,531 14,042 TruthfulQA [53] - 817 - HellaSwag [112] 39,905 10,042 - ARC [18] 3,370 869 3,548 OpenBookQA [60] 4,957 500 500 Entity-linking WNED [68, 94] - 8,995 - AIDA CoNLL-YAGO [33, 68] 18,395 4,784 - Slot filling Zero-shot RE [50, 68] 147,909 3,724 - T-REx [23, 68] 2,284,168 5,000 - Dialog Generation WOW [22, 68] 63,734 3,054 - Fact Verification FEVER [68, 95] 104,966 10,444 - Open-domain SummarizationWikiAsp [29] 300,636 37,046 37,368 • Re-Rankers: Includes 24 models with 41 sub-methods, sup- porting pointwise, pairwise, and listwise re-ranking. • RAG: Facilitates retrieval-augmented generation with mod- els like LLAMA, GPT, and T5, supporting in-context learning, Fusion-in-Decoder (FiD), and Incontext RALM. A key advantage of Rankify is its unified API, which abstracts implementation details and simplifies experimentation across dif- ferent models. Figure 2 and 3 illustrates the interactions between these modules within the pipeline. The framework is scalable, adapt- able, and suited for both research and real-world applications like search engines and question-answering systems. Rankify is open- source, actively maintained6, and accompanied by comprehensive documentation7 to support easy adoption for both beginners and experts. 3.1 Prebuilt Retrieval Corpora and Indexes Rankify provides two large-scale retrieval corpora:Wikipedia [42] and MS MARCO [62], enabling researchers to conduct retrieval experiments on well-established benchmarks. To support efficient retrieval, Rankify includes pre-built index files for each retriever, al- lowing users to directly query the corpora without requiring costly 6https://github.com/abdoelsayed2016/reranking 7http://rankify.readthedocs.io/ Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and RAG SIGIR ’25, July 13–18, 2025, Padova, IT indexing. Each corpus has been indexed for multiple retrieval mod- els, including BM25, DPR, ANCE, BGE, Contriever, and ColBERT. By providing precomputed indexes, Rankify simplifies large-scale retrieval research while maintaining consistency across different retrieval pipelines. Users can view all available datasets using the following command (see Listing 1): 1 from rankify . dataset . dataset import Dataset 2 Dataset . avaiabl e_datase t () Listing 1: This script retrieves the latest information about the available datasets, and displays metadata for each dataset in the terminal. 3.2 Datasets Rankify provides a standardized and efficient way to handle datasets for retrieval, re-ranking, and retrieval-augmented generation (RAG). To simplify retrieval workflows, Rankify integrates pre-retrieved datasets with structured annotations, enabling users to experiment with various retrieval and re-ranking methods with minimal code. The datasets in Rankify are categorized based on task type and size, as shown in Table 2. Each dataset follows a standardized schema with three main components: the Question, which represents the query requiring relevant information; the Answers, which are the expected correct responses to the query; and the Contexts (or Retrieved Documents), which provide a ranked list of candidate documents retrieved by a specific retriever. To support benchmarking and reproducibility,Rankify provides pre-retrieved datasets, each with 1,000 top-ranked documents from Wikipedia8, processed by various retrievers such as BM25, DPR, ANCE, BGE, Contriever, and ColBERT. These datasets cover diverse Re-Ranking and RAG tasks. Rankify allows users to load and explore datasets with minimal effort. Below is an example demonstrating how to download and inspect a dataset using Rankify ’s API: 1 from rankify . dataset . dataset import Dataset 2 # Load the nq - test dataset retrieved with BM25 3 dataset = Dataset ( retriever = " bm25 " , dataset_name = " nq - test " ) 4 documents = dataset . download ( force_download = False ) Listing 2: Loading a dataset in Rankify and inspecting its structure. Beyond preloaded datasets, Rankify enables users to define their own datasets using the Dataset class. Below is an example demonstrating how to create a custom retrieval dataset: 1 from rankify . dataset . dataset import Dataset , Document , Question , Answer , Context 2 question = Question ( " What is the capital of France ? " ) 3 answer = Answer ([ " Paris " ]) 4 context = [ Context ( score =0.9 , has_answer = True , id =1 , title = " France " , text = " The capital is Paris . " ) ] 5 document = Document ( question , answer , context ) 6 custom_dataset = Dataset ( retriever = " custom " , dataset_name = " MyDataset " ) 7 custom_dataset . documents = [ document ] 8 custom_dataset . save_dataset ( " my_dataset . json " ) Listing 3: Creating a custom dataset in Rankify. 8Note: We are currently processing different retrievers based on MS MARCO to gener- ate and store 1,000 top-ranked documents per query for each dataset. Users can load their own datasets using load_dataset and load_dataset_qa functions. The load_dataset function is de- signed for structured datasets that include both queries and re- trieved documents. On the other hand, load_dataset_qa is in- tended for question-answering datasets that contain only queries and answers, allowing users to use them directly for retrieval tasks. The following examples demonstrate how users can load these datasets in Rankify: 1 from rankify . dataset . dataset import Dataset 2 r etr ie va l_ dat as et = Dataset . load_dataset ( " path / to / re tr ie va l_d at as et . json " ) 3 qa_dataset = Dataset . load_dataset_qa ( " path / to / qa_dataset . jsonl " ) Listing 4: Loading a dataset with documents and a QA-only dataset. 3.3 Retriever Models Rankify supports a diverse set of retriever models to facilitate document retrieval for various information retrieval tasks. The current version includes both sparse retrievers, such as BM25 [80], and dense retrievers, such as DPR [109], ANCE [104], and BGE [13], Contriever [36] and ColBERT [ 45]. The retrievers implemented in Rankify are accessible via a unified interface, where users can specify the retrieval method and parameters. Rankify allows users to retrieve documents efficiently using dif- ferent retrieval models. The retrievers can be applied to any dataset supported by Rankify, making it flexible for experimentation and benchmarking. Users can easily initialize a retriever and apply it to a list of query-document pairs, as demonstrated in Listing 5. 1 from rankify . dataset . dataset import Document , Question , Answer 2 from rankify . retrievers . retriever import Retriever 3 4 # Define sample documents 5 documents = [ 6 Document ( question = Question ( " the cast of a good day to die hard ? " ) , 7 answers = Answer ([ " Jai Courtney " , " Bruce Willis " ]) , contexts =[]) , 8 Document ( question = Question ( " Who wrote Hamlet ? " ) , 9 answers = Answer ([ " Shakespeare " ]) , contexts =[]) 10 ] 11 # Initialize a retriever ( example : ColBERT ) 12 retriever = Retriever ( method = " colbert " , model = " colbert - ir / colbertv2 .0 " , n_docs =5 , index_type = " msmarco " ) 13 # Retrieve documents 14 r e t r i e v e d _ d o c u m e n t s = retriever . retrieve ( documents ) 15 # Print retrieved documents 16 for i , doc in enumerate ( r e t r i e v e d _ d o c u m e n t s ) : 17 print ( f " \ nDocument { i +1}: " ) 18 print ( doc ) Listing 5: Using a retriever model in Rankify to retrieve documents. In Listing 5, users can specify the retrieval corpus by setting index_type to msmarco or wiki, select the desired retriever model, and define the number of documents to retrieve. This flexibility allows researchers to experiment with different retrieval settings, optimizing their retrieval strategy based on the task requirements. SIGIR ’25, July 13–18, 2025, Padova, IT Abdallah et al. 3.4 Re-Ranking Models In information retrieval, two-stage pipelines are widely used to max- imize retrieval performance. The first stage involves retrieving a set of candidate documents using a computationally efficient retriever, followed by a second stage where these documents are re-ranked using a stronger, typically, neural network-based model. This re- ranking step enhances retrieval quality by considering deeper se- mantic relationships between the query and documents. While effective, re-ranking models vary significantly in their architec- tures, trade-offs, and implementations, making it challenging for users to select the most suitable method for their specific needs. To address this, Rankify provides a unified interface for re- ranking, enabling users to effortlessly switch between different models with minimal modifications. The framework supports a diverse set of 24 primary re-ranking models with 41 sub-methods, spanning different re-ranking strategies such as pointwise, pair- wise, and listwise approaches. The implemented models include MonoBERT [64], MonoT5 [65], RankT5 [ 115], ListT5 [ 110], Col- BERT [83], RankGPT [90], and various transformer-based re-rankers. Users can apply these models to re-rank retrieved documents using a simple interface, as demonstrated in Listing 6. 1 from rankify . dataset . dataset import Document , Question , Answer 2 from rankify . retrievers . retriever import Retriever 3 from rankify . rerankers . reranker import Reranker 4 # Define sample documents 5 documents = [ 6 Document ( question = Question ( " the cast of a good day to die hard ? " ) , 7 answers = Answer ([ " Jai Courtney " , " Bruce Willis " ]) , contexts =[]) , 8 Document ( question = Question ( " Who wrote Hamlet ? " ) , 9 answers = Answer ([ " Shakespeare " ]) , contexts =[]) 10 ] 11 # Initialize a retriever 12 retriever = Retriever ( method = " colbert " , model = " colbert - ir / colbertv2 .0 " , n_docs =5 , index_type = " msmarco " ) 13 r e t r i e v e d _ d o c u m e n t s = retriever . retrieve ( documents ) 14 # Initialize a re - ranker 15 reranker = Reranker ( method = " monot5 " , model_name = " monot5 - base " ) 16 # Apply re - ranking 17 r e ra n ke d _ do c u me n t s = reranker . rerank ( r e t r i e v e d _ d o c u m e n t s ) 18 # Print re - ranked documents 19 for i , doc in enumerate ( r er a n ke d _d o c um e n ts ) : 20 print ( f " \ nDocument { i +1}: " ) 21 print ( doc ) Listing 6: Applying re-ranking in Rankify. Rankify supports both pre-trained models from Hugging Face and API-based re-ranking models such as RankGPT, Cohere, and Jina Reranker. Users can select from a variety of models, includ- ing cross-encoders, T5-based ranking models, and late-interaction retrieval models like ColBERT. The flexibility to switch between different re-ranking strategies allows researchers and practitioners to optimize ranking performance based on their task requirements. 3.5 Retrieval-Augmented Generation (RAG) Models Retrieval-Augmented Generation (RAG) enhances language models by integrating retrieval mechanisms, allowing them to generate responses based on dynamically retrieved documents rather than relying solely on pre-trained knowledge. This approach is particu- larly effective for knowledge-intensive tasks such as open-domain question answering, fact verification, and knowledge-based text generation. Rankify provides a modular and extensible interface for applying multiple RAG methods, including zero-shot generation, Fusion-in-Decoder (FiD) [38], and in-context learning [77]. In Rankify, the Generator module enables seamless integration of RAG techniques, allowing users to experiment with different gen- erative approaches. Users can specify the desired RAG method and model, applying generation strategies across retrieved documents. Users can apply these methods to generate responses based on retrieved documents. Listing 7 demonstrates how to use Rankify’s RAG module with an in-context learning approach. 1 from rankify . dataset . dataset import Document , Question , Answer , Context 2 from rankify . generator . generator import Generator 3 # Sample question and contexts 4 question = Question ( " What is the capital of France ? " ) 5 answers = Answer ([ " Paris " ]) 6 contexts = [ 7 Context ( id =1 , title = " France " , text = " The capital of France is Paris . " , score =0.9) , 8 Context ( id =2 , title = " Germany " , text = " Berlin is the capital of Germany . " , score =0.5) 9 ] 10 # Create a Document 11 doc = Document ( question = question , answers = answers , contexts = contexts ) 12 # Initialize Generator with In - Context RALM 13 generator = Generator ( method = " in - context - ralm " , model_name = ' meta - llama / Llama -3.1 -8 B ') 14 # Generate answer 15 g ene ra te d_ ans we rs = generator . generate ([ doc , doc ]) 16 print ( g en er at ed_ an sw er s ) Listing 7: Applying Retrieval-Augmented Generation (RAG) in Rankify. Rankify allows users to leverage large-scale language mod- els such as LLaMA [ 96], GPT-4 [ 5], and T5-based models [ 75] for retrieval-augmented generation. By supporting both encoder- decoder architectures (FiD [ 38]) and decoder-only models (e.g., GPT, LLaMA), the framework provides flexibility for optimizing generation quality based on task-specific requirements. 3.6 Evaluation Metrics Rankify provides evaluation metrics for retrieval, re-ranking, and retrieval-augmented generation (RAG). For retrieval and re-ranking, Top-k accuracy measures whether documents containing the cor- rect answer appear within the top-k results. This definition of rel- evance, while task-specific, aligns with the goal of downstream applications like question answering. Listing 8 demonstrates how to compute Top-k accuracy. 1 from rankify . metrics . metrics import Metrics 2 metrics = Metrics ( documents ) 3 # Compute Top - k accuracy before and after re - ranking 4 before_ranking = metrics . c a l c u l a t e _ r e t r i e v a l _ m e t r i c s ( ks =[1 ,5 ,10 ,20 ,50 ,100] , use_reordered = False ) 5 after_ranking = metrics . c a l c u l a t e _ r e t r i e v a l _ m e t r i c s ( ks =[1 ,5 ,10 ,20 ,50 ,100] , use_reordered = True ) 6 print ( " Before Ranking : " , before_ranking ) Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and RAG SIGIR ’25, July 13–18, 2025, Padova, IT Table 3: Retrieval performance of BM25 across several selected datasets. The table reports Top-1, Top-5, Top-10, Top-20, Top-50, and Top-100 retrieval accuracy for each dataset split. Datasets vary in complexity, including open-domain QA (e.g., TriviaQA, NQ), multi-hop reasoning (e.g., 2WikiMultiHopQA, HotpotQA), fact verification (e.g., TruthfulQA), and temporal retrieval (e.g., ArchivalQA, ChroniclingAmericaQA). Dataset Split Top-1 (%) Top-5 (%) Top-10 (%) Top-20 (%) Top-50 (%) Top-100 (%)Dataset Split Top-1 (%) Top-5 (%) Top-10 (%) Top-20 (%) Top-50 (%) Top-100 (%) 2WikiMultiHopQA [32]Train 8.85 19.78 28.15 38.16 51.16 57.70 ArchivalQA [99]Val 18.18 33.29 39.72 45.96 53.68 58.74Val 17.17 31.72 40.14 47.73 55.88 61.17 Test 17.68 32.55 39.26 45.36 52.76 58.08 ChroniclingAmericaQA [69]Val 4.26 11.80 16.42 21.50 28.19 33.15 AmbigQA [47, 61]Train 27.30 50.67 59.63 67.63 75.68 79.87Test 4.18 11.23 15.83 20.87 27.49 32.48 Val 30.22 54.25 65.58 72.98 80.47 84.92 ARC [18] Train 4.45 18.16 31.39 51.39 78.19 91.36 TriviaQA [40]Train 47.87 67.22 72.90 77.15 81.30 83.64Val 4.83 19.45 30.84 52.13 79.06 91.83 Val 48.70 67.57 72.84 77.38 81.19 83.59Test 4.96 19.84 33.57 51.94 79.09 91.80 Test 48.22 67.44 72.79 77.28 81.42 83.87 SQuAD [76] Train 31.84 50.24 57.15 63.33 70.32 74.61 MMLU [30, 31]Train 3.65 14.14 23.98 38.68 62.22 78.04Val 36.94 57.61 64.78 71.31 77.89 81.96 Val 6.53 21.75 33.64 50.62 75.77 88.18Test 36.67 57.37 64.50 71.09 77.74 81.84 Test 6.53 22.01 34.67 51.22 74.35 86.40 QuaRTz [91] Train 7.81 26.82 40.62 63.54 84.38 92.19 NarrativeQA [81]Train 16.80 25.24 28.46 31.50 35.74 38.65Val 7.81 26.82 40.62 63.54 84.38 92.19 Val 15.46 23.92 27.10 30.45 35.08 38.89Test 5.87 23.60 38.90 58.67 80.74 90.43 Test 16.80 25.24 28.46 31.50 35.74 38.65 NQ [47] Train 22.89 44.97 54.32 62.73 71.75 77.03 OpenBookQA [60]Train 3.35 16.24 28.99 47.85 75.01 88.56Val 23.62 45.60 55.41 64.19 72.51 77.72 Val 3.40 15.20 31.60 50.60 79.00 90.00Test 23.46 45.62 56.32 64.88 74.57 79.72 Test 4.00 16.80 32.20 51.20 74.00 88.80 WebQuestions [8]Train 21.41 44.71 55.35 64.21 72.71 78.45 Musique [97]Train 6.23 13.70 18.54 24.39 33.77 40.81Test 19.54 42.86 53.44 63.25 72.34 76.43 Val 7.36 15.47 21.39 25.98 34.59 41.29 T-REx [23, 68] Val 45.48 64.34 70.60 76.04 80.42 82.98 TruthfulQA [53] Val 2.33 7.59 12.12 20.69 38.19 56.55 SIQA [84] Train 0.34 1.52 2.78 4.85 8.92 13.43 HotpotQA [108]Train 34.89 51.71 58.13 64.09 71.11 75.49Val 0.61 2.05 3.58 6.55 11.31 16.58 Val 28.13 45.02 51.92 57.79 65.44 70.67 StrategyQA [28] Train 4.19 15.72 26.29 39.87 52.88 57.86 PopQA [59] Test 25.00 38.77 44.70 49.74 56.82 62.36 WOW [22, 68] Train 0.26 0.38 0.41 0.45 0.49 0.52 ZSRC [50] Train 50.53 67.23 72.06 76.10 80.35 82.99Val 0.20 0.33 0.36 0.39 0.46 0.46 Val 52.26 70.86 76.29 81.44 85.39 87.73 Bamboogle [72] Test 5.60 12.80 17.60 24.80 39.20 44.00 EntityQuestions [85] Test 43.36 60.50 66.08 70.61 75.41 79.06 7 print ( " After Ranking : " , after_ranking ) Listing 8: Computing Top-k accuracy for retrieval and re- ranking in Rankify. For QA and RAG, Rankify supports Exact Match (EM), recall, precision, and containment to evaluate generated answers. Listing 9 shows how these metrics are computed. 1 from rankify . metrics . metrics import Metrics 2 qa_metrics = Metrics ( documents ) 3 qa_results = qa_metrics . c a l c u l a t e _ g e n e r a t i o n _ m e t r i c s ( ge ne ra te d_a ns we rs ) 4 print ( qa_results ) Listing 9: Computing QA and RAG evaluation metrics in Rankify. By providing a unified evaluation module, Rankify ensures con- sistent benchmarking across retrieval, re-ranking, and RAG tasks. 4 Experimental Result and Discussion 4.1 Experiment Setup Rankify enables researchers to benchmark retrieval, re-ranking, and RAG methods, evaluate their own approaches, and explore optimizations within these tasks. To demonstrate its capabilities, we conducted multiple experiments to provide reproducible bench- marks and performance insights. All main experiments were con- ducted on 2x NVIDIA A100 GPUs. Experiments were performed on a diverse set of datasets, cov- ering open-domain and multi-hop QA, fact verification, and tem- poral retrieval tasks. All the experiments in our study use prepro- cessed English Wikipedia dump from December 2018 as released by [109] as evidence passages. Each Wikipedia article is split into non-overlapping 100-word passages, with over 21 million passages in total. We evaluated retrieval performance on datasets such as Natu- ral Questions (NQ), TriviaQA, HotpotQA, 2WikiMultiHopQA, and ArchivalQA, while re-ranking performance was analyzed on MS- MARCO, WebQuestions, and PopQA. For retrieval evaluation, we measured Top-k accuracy at k=1, 5, 10, 20, 50, 100. Exact match (EM), Precision, Recall, Contains, and F1 were used as the primary evaluation metrics for QA tasks. We conducted experiments across all supported retrieval, re- ranking, and RAG methods. Retrieval models BM25, DPR, ANCE, BGE, and ColBERT. Re-ranking methods tested included MonoT5, RankT5, RankGPT, and various transformer-based re-rankers. For RAG, we evaluated zero-shot generation, Fusion-in-Decoder (FiD), in-context learning (RALM). These experiments highlight the adapt- ability of Rankify for benchmarking retrieval, ranking, and knowledge- grounded text generation. Table 4: Comparison of different retrieval models on NQ, WebQ, and TriviaQA. The table reports Top-1 to Top-100 accuracy for each retriever. Retriever Dataset Top-1 Top-5 Top-10 Top-20 Top-50 Top-100 Contriever [36] NQ 38.81 65.65 73.91 79.56 84.88 88.01 WebQ 35.97 63.83 69.78 75.94 81.64 83.76 TriviaQA 49.85 71.39 76.68 80.26 83.85 85.72 DPR [109] NQ 44.57 67.76 74.52 79.50 84.40 86.81 WebQ 44.64 63.98 70.52 75.05 80.51 82.97 TriviaQA 57.47 72.40 76.50 79.76 82.96 85.09 ColBert [83] NQ 42.99 68.78 76.12 82.08 86.15 88.59 WebQ 40.11 64.81 71.56 76.57 81.20 84.50 TriviaQA 57.36 75.90 79.49 82.18 84.74 86.41 Ance [104] NQ 50.80 71.86 78.12 82.52 86.23 88.28 WebQ 46.51 66.24 71.80 76.62 81.79 84.25 TriviaQA 56.46 72.14 76.65 80.04 83.37 85.22 BGE [13, 102] NQ 48.03 72.22 78.50 82.66 86.90 89.45 WebQ 42.67 65.45 72.54 79.04 82.78 85.88 TriviaQA 57.81 75.39 79.83 82.92 85.53 86.94 SIGIR ’25, July 13–18, 2025, Padova, IT Abdallah et al. Table 5: Top-20/Top-100 retrieval accuracy of different re- trievers on three open-domain QA datasets (NQ, TriviaQA, and WebQ). Our Rankify implementation achieves results identical to Pyserini across all retrievers, as we use Pyserini as the indexing backend. This validates the correctness of our implementation. We also match the official GitHub results for ColBERT and Contriever on these datasets. Retrievers NQ TriviaQA WebQ DPR [109] 78.4 / 85.4 79.4 / 85.0 73.2 / 81.4 DPR_Pyserini [52] 79.5 / 86.8 79.7 / 85.1 75.1 / 82.9 DPR_Rankify 79.5 / 86.8 79.7 / 85.1 75.1 / 82.9 BM25 [82] 63.0 / 78.2 76.4 / 83.1 62.3 / 75.5 BM25_Pyserini [52] 64.8 / 79.7 77.3 / 83.8 63.3 / 76.4 BM25_Rankify 64.8 / 79.7 77.3 / 83.8 63.3 / 76.4 Contriever [37] 79.6 / 88.0 80.4 / 85.7 75.9 / 83.7 Contriever_Rankify 79.6 / 88.0 80.3 / 85.7 75.9 / 83.7 ColBert [83] 82.1 / 88.5 82.2 / 86.4 76.6 / 84.5 ColBert_Rankify 82.1 / 88.5 82.2 / 86.4 76.6 / 84.5 ANCE [104] 82.1 / 87.9 80.3 / 85.2 - / - ANCE_Rankify 82.5 / 88.5 80.0 / 85.2 76.6 / 84.25 4.2 Retrieval Results The retrieval performance of BM25 across multiple datasets is shown in Table 3. The results report Top-1, Top-5, Top-10, Top- 20, Top-50, and Top-100 retrieval accuracy for each dataset split. Performance varies significantly based on dataset characteristics, with high Top-k accuracy achieved on open-domain QA datasets such as TriviaQA and Natural Questions. On the other hand, more complex multi-hop datasets like 2WikiMultiHopQA and HotpotQA are characterized by lower retrieval rates, reflecting the challenges in retrieving supporting evidence across multiple documents. Tem- poral datasets like ArchivalQA and ChroniclingAmericaQA exhibit moderate retrieval performance, likely due to the temporal nature of their content. It is important to note that some datasets in Table 3 were either not officially recorded in previous studies or rely on different underlying corpora, making direct comparison difficult. Additionally, some datasets (e.g., ChroniclingAmericaQA) do not report retrieval accuracy as a standard Top-k metric, further compli- cating direct validation. Despite these variations, our results shown in Table 3 provide a comprehensive overview of BM25’s strengths and limitations. Table 4 presents the retrieval performance of various dense re- trievers across three benchmark datasets: NQ, WebQ, and Trivi- aQA. The results report Top-1, Top-5, Top-10, Top-20, Top-50, and Top-100 retrieval accuracy, providing insights into how different retrieval models rank relevant documents. Among the tested retriev- ers, dense retrieval methods such as DPR, MSS-DPR, and ColBERT consistently outperform retrievers like MSS and Contriever. MSS- DPR achieves the highest Top-1 accuracy, reaching 50.16% on NQ, 44.24% on WebQ, and 61.63% on TriviaQA, demonstrating the effec- tiveness of fine-tuned dense retrieval. Standard DPR also performs well, particularly on NQ and TriviaQA, where it reaches 48.67% and 57.47% Top-1 accuracy, respectively. ColBERT, which employs late- interaction ranking, shows competitive performance, particularly in higher recall settings (Top-50 and Top-100). Table 6: Performance of re-ranking methods on BM25- retrieved documents for NQ Test and WebQ Test. Results are reported in terms of Top-1, Top-5, Top-10, Top-20, and Top- 50 accuracy, highlighting the impact of various re-ranking models on retrieval effectiveness. Please note that some re- sults may differ from the original papers (e.g., UPR) as our experiments were conducted with the top 100 retrieved doc- uments, whereas the original studies used 1,000 documents for ranking. Reranking/Model NQ WebQTop-1 Top-10 Top-50Top-1 Top-10 Top-50 BM25 - 23.46 56.32 74.57 19.54 53.44 72.34 UPR [82] T5-small 23.60 59.97 75.15 18.55 55.56 72.68T5-base 26.81 63.32 76.12 20.66 58.56 72.68T5-large 29.75 65.67 76.48 24.21 60.38 73.32T0-3B 35.42 67.56 76.75 32.48 64.17 73.67gpt2 25.95 60.47 75.87 20.47 56.49 72.78gpt2-medium 26.75 63.04 75.95 22.39 59.54 72.44gpt2-large 26.59 62.68 75.95 24.06 59.84 72.78gpt2-xl 27.28 63.24 75.84 23.57 60.48 72.73gpt-neo-2.7B 28.75 64.81 76.56 24.75 59.64 72.63 RankGPT [90]llamav3.1-8b 41.55 66.17 75.42 38.77 62.69 73.12 FlashRank [21] TinyBERT-L-2-v231.49 61.57 74.95 28.54 60.62 73.17MultiBERT-L-12 11.99 43.54 69.63 12.54 45.91 67.91ce-esci-MiniLM-L12-v234.70 64.81 76.17 31.84 62.54 73.47T5-flan 7.95 36.14 66.67 12.05 42.96 67.27 RankT5 [115]base 43.04 68.47 76.28 36.95 64.27 74.45large 45.54 70.02 76.81 38.77 66.48 74.313b 47.17 70.85 76.89 40.40 66.58 74.45 Inranker [48]small 15.90 46.84 69.83 14.46 46.25 69.98base 15.90 48.11 69.66 14.46 46.80 69.683b 15.90 48.06 69.00 14.46 46.11 69.34 LLM2Vec [7]Meta-Llama-31-8B24.32 59.55 75.26 26.72 60.48 73.47 MonoBert [66]large 39.05 67.89 76.56 34.99 64.56 73.96 Twolar [6] twolar-xl 46.84 70.22 76.86 41.68 67.07 74.40 Echorank [78]flan-t5-large 36.73 59.11 62.38 31.74 58.75 61.51flan-t5-xl 41.68 59.05 62.38 36.22 57.18 61.51 IncontextReranker [14]llamav3.1-8b 15.15 57.11 76.48 18.89 52.16 71.70 Lit5 [93] LiT5-Distill-base40.05 65.95 75.73 36.76 63.48 73.12LiT5-Distill-large44.40 67.59 76.01 39.66 64.56 73.67LiT5-Distill-xl 47.81 68.55 76.26 42.37 65.55 73.62LiT5-Distill-base-v242.57 66.73 75.56 39.61 64.22 73.32LiT5-Distill-large-v246.53 67.83 75.87 41.97 65.64 72.98LiT5-Distill-xl-v247.92 69.03 76.17 41.53 65.69 73.27 SentenceTransformerReranker GTR-base [63] 39.41 65.95 76.03 36.56 64.32 73.62GTR-large 40.63 68.25 76.73 38.97 65.30 73.57T5-base [75] 31.19 63.60 76.06 29.77 62.84 73.52T5-large 30.80 63.35 76.37 30.51 61.71 73.37all-MiniLM-L6-v2 [100]33.35 65.37 76.01 30.95 62.10 73.52GTR-xl 41.55 67.78 76.81 38.92 66.04 74.01GTR-xxl 42.93 68.55 77.00 39.41 65.89 74.01T5-xxl 38.89 67.78 76.64 35.82 65.20 74.01Bert-co-condensor30.96 61.91 75.20 32.43 62.20 73.08Roberta-base-v2 32.60 63.24 75.42 31.34 62.64 73.37 4.3 Comparison with Original Implementations To validate the correctness and reliability of theRankify’s retrieval module, we compare its performance against original implementa- tions and Pyserini-based baselines for multiple retrievers. Specif- ically, we evaluate DPR, Contriever, BM25, and ColBERT across three open-domain QA datasets: NQ, TriviaQA, and WebQ. The retrieval performance is measured using Top-20 and Top-100 accu- racy. Table 5 presents the results of our Rankify implementation alongside the original models and Pyserini-based implementations. Our results show thatRankify achieves retrieval performance iden- tical to Pyserini for DPR, BM25 as we utilize Pyserini as the back- end for indexing and retrieval. However, for Contriever BGE, and ColBERT, we implemented their methods independently, and our Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and RAG SIGIR ’25, July 13–18, 2025, Padova, IT BM25 Contriever DPR Retriever 0 10 20 30 40 50 60 70 80EM Score 14.90 15.29 28.08 12.82 13.24 23.21 14.02 13.96 13.99 19.81 19.78 19.78 21.14 20.47 21.94 11.19 11.08 11.11 NQ - EM Comparison Language Model LLama V3 8B LLama V3.1 8B Gemma-2-2b Gemma-2-9b Llama-2-13b-hf Mistral-7B-v0.1 BM25 Contriever DPR Retriever 42.10 36.25 45.88 40.13 35.29 43.62 43.28 33.05 33.05 57.55 50.93 50.93 57.90 42.69 51.07 52.85 42.69 42.69 TriviaQA - EM Comparison Language Model LLama V3 8B LLama V3.1 8B Gemma-2-2b Gemma-2-9b Llama-2-13b-hf Mistral-7B-v0.1 BM25 Contriever DPR Retriever 10.23 10.67 19.83 9.25 9.35 14.32 14.71 14.71 14.71 14.96 14.96 14.96 19.54 19.98 19.83 6.40 6.40 6.40 WebQ - EM Comparison Language Model LLama V3 8B LLama V3.1 8B Gemma-2-2b Gemma-2-9b Llama-2-13b-hf Mistral-7B-v0.1 Figure 4: Exact Match (EM) for BM25, Contriever, and DPR retrievers across three datasets (NQ, TriviaQA, WebQ) using various language models (LLaMA V3/V3.1, Gemma 2B/9B, LLaMA 2 13B, and Mistral 7B). results closely match the original implementations, demonstrating the correctness of our approach. Additionally, we observe improvements in the performance of BM25 and DPR compared to their original baselines. These improve- ments can be attributed to the optimizations present in Pyserini, such as better indexing techniques and parameter tuning. For Col- BERT and Contriever, we replicate the results reported in their respective papers by running their official GitHub implementations on our datasets, yielding identical outcomes. 4.4 Re-Ranking Results The performance of various re-ranking methods applied to BM25- retrieved documents is presented in Table 6. The table reports Top-1, Top-10, and Top-50 accuracy for the NQ-Test and WebQ datasets. The baseline BM25 retrieval scores serve as a reference, demonstrat- ing the extent to which re-ranking improves document ranking quality across different models. Across all methods, re-ranking consistently improves retrieval effectiveness, with notable gains observed in Top-1 accuracy. Transformer-based models such as UPR (T5-based), RankGPT, FlashRank (MiniLM and TinyBERT), and RankT5 achieve substantial improvements over the BM25 base- line. In particular, RankT5-3B outperforms other models, achiev- ing a Top-1 accuracy of 47.17% on NQ-Test and 40.40% on WebQ, highlighting the effectiveness of T5-based models for large-scale retrieval. RankGPT, leveraging LLaMA-3.1-8B, also shows strong performance, particularly in WebQ, where it improves Top-1 accu- racy from 18.80% (BM25) to 38.77%. Among lighter-weight models, FlashRank (MiniLM and Tiny- BERT) and MonoBERT-large provide solid improvements while maintaining efficiency. MiniLM-L-12-v2 achieves 41.02% Top-1 ac- curacy on NQ-Test and 37.05% on WebQ, making it a strong candi- date for deployment scenarios where computational efficiency is critical. The MonoBERT-large model reaches 39.05% on NQ-Test and 34.99% on WebQ, reinforcing the effectiveness of BERT-based cross-encoders for document ranking. Models that integrate list- wise and contrastive ranking techniques, such as Twolar-xl and LiT5, show competitive performance. Twolar-xl achieves 46.84% Top-1 accuracy on NQ-Test and 41.68% on WebQ, while LiT5-Distill- xl reaches 47.81% on NQ-Test and 42.37% on WebQ, demonstrating that fine-tuned T5 models can significantly improve ranking quality. The sentence-transformer rerankers, particularly GTR-xxl, achieve 42.93% and 39.41% Top-1 accuracy on NQ-Test and WebQ, respec- tively, making them effective for diverse retrieval tasks. 4.5 Generator Results In this section, we present the performance of Rankify’s generator module when integrated with Retrieval-Augmented Language Mod- eling (RALM) [77]. We evaluate the effectiveness of various retrieval models, including BM25, Contriever, and DPR, across three open- domain QA datasets: NQ, TriviaQA, and WebQ. The experiments utilize multiple LLMs: LLaMA V3 8B, LLaMA V3.1 8B, Gemma 2B/9B, LLaMA 2 13B, and Mistral 7B to assess the robustness of the retrieval and generation pipeline. We focus on the Exact Match (EM) metric, which measures the proportion of generated answers that exactly match the ground-truth references. Figure 4 provides a consolidated view of the EM scores for all datasets and models. From the figure, we observe the following key patterns: For the NQ dataset, DPR consistently outperforms other retrievers, achieving its peak performance with the LLaMA V3 8B model (28.08%). On the TriviaQA dataset, BM25 achieves the highest EM score (57.55%) when paired with the Gemma 2 9B model, outperforming both DPR and Contriever. For the WebQuestions (WebQ) dataset, DPR again exhibits strong performance, achieving its best EM score (19.83%) with the LLaMA V3 8B model. 5 Conclusion We introduced Rankify, a modular toolkit that unifies retrieval, re-ranking, and retrieval-augmented generation (RAG) within a cohesive framework. By integrating diverse retrievers, state-of-the- art re-rankers, and seamless RAG pipelines, Rankify streamlines experimentation and benchmarking in information retrieval. Our experiments demonstrate its effectiveness in enhancing ranking quality and retrieval-based text generation. Designed for extensibility, Rankify allows for easy integration of new models and evaluation methods. Future work will focus on improving retrieval efficiency, optimizing re-ranking strategies, and advancing RAG capabilities. The framework is open-source, ac- tively maintained, and freely available for the research community. Future work will focus on incorporating alternative retrieval evalu- ation metrics such as MAP, Precision, and NDCG, while expanding dataset coverage to include BEIR for broader benchmarking. The framework is open-source, actively maintained, and freely available for the research community. SIGIR ’25, July 13–18, 2025, Padova, IT Abdallah et al. References [1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Man- junath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: a system for large-scale machine learning. In Proceedings of the 12th USENIX Conference on Operating Systems De- sign and Implementation (Savannah, GA, USA) (OSDI’16). USENIX Association, USA, 265–283. [2] Amar Abane, Anis Bekri, and Abdella Battou. 2024. FastRAG: Retrieval Aug- mented Generation for Semi-structured Data. arXiv preprint arXiv:2411.13773 (2024). [3] Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, and Adam Jatowt. 2025. ASRank: Zero-Shot Re-Ranking with Answer Scent for Document Re- trieval. arXiv:2501.15245 [cs.CL] https://arxiv.org/abs/2501.15245 [4] Abdelrahman Elsayed Mahmoud Abdallah, Jamshid Mozafari, Bhawna Piryani, Mohammed M.Abdelgwad, and Adam Jatowt. 2025. DynRank: Improve Passage Retrieval with Dynamic Zero-Shot Prompting Based on Question Classification. In Proceedings of the 31st International Conference on Computational Linguis- tics, Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Bar- bara Di Eugenio, and Steven Schockaert (Eds.). Association for Computational Linguistics, Abu Dhabi, UAE, 4768–4778. https://aclanthology.org/2025.coling- main.319/ [5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren- cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [6] Davide Baldelli, Junfeng Jiang, Akiko Aizawa, and Paolo Torroni. 2024. TWOLAR: a TWO-step LLM-Augmented distillation method for passage Rerank- ing. In European Conference on Information Retrieval . Springer, 470–485. [7] Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bah- danau, Nicolas Chapados, and Siva Reddy. 2024. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961 (2024). [8] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic Parsing on Freebase from Question-Answer Pairs. InProceedings of the 2013 Con- ference on Empirical Methods in Natural Language Processing , David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard (Eds.). As- sociation for Computational Linguistics, Seattle, Washington, USA, 1533–1544. https://aclanthology.org/D13-1160 [9] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. PIQA: Reasoning about Physical Commonsense in Natural Language. In AAAI Conference on Artificial Intelligence. https://api.semanticscholar.org/CorpusID: 208290939 [10] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81. [11] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the 24th international conference on Machine learning . 129–136. [12] Harrison Chase. 2022. LangChain: Building applications with LLMs through composability. https://www.langchain.com Available at https://www.langchain. com. [13] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2023. BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv:2309.07597 [cs.CL] [14] Shijie Chen, Bernal Jiménez Gutiérrez, and Yu Su. 2024. Attention in Large Lan- guage Models Yields Efficient Zero-Shot Re-Rankers. arXiv:2410.02642 [cs.CL] https://arxiv.org/abs/2410.02642 [15] Tao Chen, Mingyang Zhang, Jing Lu, Michael Bendersky, and Marc Najork. 2022. Out-of-domain semantics to the rescue! zero-shot hybrid retrieval models. In European Conference on Information Retrieval . Springer, 95–110. [16] Gobinda G Chowdhury. 2010. Introduction to modern information retrieval. Facet publishing. [17] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In NAACL. [18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved Question An- swering? Try ARC, the AI2 Reasoning Challenge. CoRR abs/1803.05457 (2018). arXiv:1803.05457 http://arxiv.org/abs/1803.05457 [19] Benjamin Clavié. 2024. rerankers: A Lightweight Python Library to Unify Ranking Methods. arXiv:2408.17344 [cs.IR] https://arxiv.org/abs/2408.17344 [20] W Bruce Croft, Donald Metzler, and Trevor Strohman. 2010. Search engines: Information retrieval in practice . Vol. 520. Addison-Wesley Reading. [21] P. Damodaran. 2024. FlashRank, Lightest and Fastest 2nd Stage Reranker for search pipelines. https://doi.org/10.5281/zenodo.11093524 [22] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of Wikipedia: Knowledge-Powered Conversational Agents. In International Conference on Learning Representations . https://openreview. net/forum?id=r1l73iRqKm [23] Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena Simperl. 2018. T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018. [24] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long Form Question Answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , Anna Ko- rhonen, David Traum, and Lluís Màrquez (Eds.). Association for Computational Linguistics, Florence, Italy, 3558–3567. https://doi.org/10.18653/v1/P19-1346 [25] Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. 2024. Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training. arXiv preprint arXiv:2405.20978 (2024). [26] Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, and Jamie Callan. 2021. Complement Lexical Retrieval Model with Semantic Residual Embeddings. In Advances in Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 12656), Djoerd Hiemstra, Marie-Francine Moens, Josiane Mothe, Raffaele Perego, Martin Potthast, and Fabrizio Sebastiani (Eds.). Springer, 146–160. https://doi.org/10.1007/978-3-030-72113-8_10 [27] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023). [28] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Be- rant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics 9 (2021), 346–361. https://doi.org/10.1162/tacl_a_00370 [29] Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neer- vannan, and Graham Neubig. 2020. WikiAsp: A Dataset for Multi-domain Aspect-based Summarization. Transactions of the Association for Computational Linguistics (TACL) (2020). https://arxiv.org/abs/2011.07832 [30] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021. Aligning AI With Shared Human Values. Proceedings of the International Conference on Learning Representations (ICLR) (2021). [31] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Under- standing. Proceedings of the International Conference on Learning Representations (ICLR) (2021). [32] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reason- ing Steps. In Proceedings of the 28th International Conference on Computational Linguistics. International Committee on Computational Linguistics, Barcelona, Spain (Online), 6609–6625. https://www.aclweb.org/anthology/2020.coling- main.580 [33] Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fürstenau, Man- fred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust Disambiguation of Named Entities in Text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , Regina Barzilay and Mark Johnson (Eds.). Association for Computational Linguistics, Edinburgh, Scotland, UK., 782–792. https://aclanthology.org/D11-1072 [34] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, Adriane Boyd, et al. 2020. spaCy: Industrial-strength natural language processing in python. (2020). [35] Chao-Wei Huang and Yun-Nung Chen. 2024. PairDistill: Pairwise Relevance Distillation for Dense Retrieval. arXiv preprint arXiv:2410.01383 (2024). [36] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo- janowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised Dense Infor- mation Retrieval with Contrastive Learning. https://doi.org/10.48550/ARXIV. 2112.09118 [37] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo- janowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised Dense Infor- mation Retrieval with Contrastive Learning. arXiv:2112.09118 (2021). [38] Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282 (2020). [39] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. 2024. FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research. arXiv preprint arXiv:2405.13576 (2024). [40] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Trivi- aQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Com- prehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Regina Barzilay and Min- Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 1601–1611. https://doi.org/10.18653/v1/P17-1147 Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and RAG SIGIR ’25, July 13–18, 2025, Padova, IT [41] Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, and Peter Clark. 2021. How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI. arXiv preprint arXiv:2110.14207 (2021). [42] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . https://www.aclweb.org/anthology/ 2020.emnlp-main.550 [43] Danupat Khamnuansin, Tawunrat Chalothorn, and Ekapol Chuangsuwanich. 2024. MrRank: Improving Question Answering Retrieval System through Multi- Result Ranking Model. arXiv preprint arXiv:2406.05733 (2024). [44] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714 (2023). [45] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. arXiv:2004.12832 [cs.IR] https://arxiv.org/abs/2004.12832 [46] Dongkyu Kim, Byoungwook Kim, Donggeon Han, and Matouš Eibich. 2024. AutoRAG: Automated Framework for optimization of Retrieval Augmented Generation Pipeline. arXiv:2410.20878 [cs.CL] https://arxiv.org/abs/2410.20878 [47] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken- ton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019), 452–466. https: //doi.org/10.1162/tacl_a_00276 [48] Thiago Laitz, Konstantinos Papakostas, Roberto Lotufo, and Rodrigo Nogueira. 2024. InRanker: Distilled Rankers for Zero-shot Information Retrieval. arXiv:2401.06910 [cs.IR] [49] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent Retrieval for Weakly Supervised Open Domain Question Answering. In Proc. ACL. 6086– 6096. [50] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero- Shot Relation Extraction via Reading Comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017) . Association for Computational Linguistics, Vancouver, Canada, 333–342. https: //doi.org/10.18653/v1/K17-1034 [51] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock- täschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459–9474. [52] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021) . 2356–2362. [53] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measur- ing How Models Mimic Human Falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). As- sociation for Computational Linguistics, Dublin, Ireland, 3214–3252. https: //doi.org/10.18653/v1/2022.acl-long.229 [54] Jerry Liu. 2022. LlamaIndex. https://doi.org/10.5281/zenodo.1234 [55] Yaqi Liu, Xiaoyu Zhang, Xiaobin Zhu, Qingxiao Guan, and Xianfeng Zhao. 2017. Listnet-based object proposals ranking. Neurocomputing 267 (2017), 182–194. [56] Zheng Liu, Yujia Zhou, Yutao Zhu, Jianxun Lian, Chaozhuo Li, Zhicheng Dou, Defu Lian, and Jian-Yun Nie. 2024. Information Retrieval Meets Large Language Models. In Companion Proceedings of the ACM on Web Conference 2024 . 1586– 1589. [57] Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou. 2024. Generative multi-modal knowledge retrieval with large language models. In Proceedings of the AAAI Conference on Artificial Intel- ligence, Vol. 38. 18733–18741. [58] Xueguang Ma, Kai Sun, Ronak Pradeep, Minghan Li, and Jimmy Lin. 2022. Another Look at DPR: Reproduction of Training and Replication of Retrieval. In Advances in Information Retrieval: 44th European Conference on IR Research, ECIR 2022, Stavanger, Norway, April 10–14, 2022, Proceedings, Part I (Stavanger, Norway). Springer-Verlag, Berlin, Heidelberg, 613–626. https://doi.org/10.1007/ 978-3-030-99736-6_41 [59] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. 2022. When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. arXiv preprint (2022). [60] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. In EMNLP. [61] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering Ambiguous Open-domain Questions. In EMNLP. [62] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2017. MS MARCO: A Human-Generated MAchine Reading COmprehension Dataset. https://openreview.net/forum?id=Hk1iOLcle [63] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021. Large dual encoders are generalizable retrievers. arXiv preprint arXiv:2112.07899 (2021). [64] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019). [65] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a pretrained sequence-to-sequence model. arXiv preprint arXiv:2003.06713 (2020). [66] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT. arXiv preprint arXiv:1910.14424 (2019). [67] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre- gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: an imperative style, high-performance deep learning library. Curran Associates Inc., Red Hook, NY, USA. [68] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Mail- lard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Lin- guistics, Online, 2523–2544. https://doi.org/10.18653/v1/2021.naacl-main.200 [69] Bhawna Piryani, Jamshid Mozafari, and Adam Jatowt. 2024. Chroniclingamer- icaqa: A large-scale question answering dataset based on historical american newspaper pages. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2038–2048. [70] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models. arXiv:2309.15088 [cs.IR] https://arxiv.org/abs/2309.15088 [71] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze! arXiv preprint arXiv:2312.02724 (2023). [72] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and Narrowing the Compositionality Gap in Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Com- putational Linguistics, Singapore, 5687–5711. https://doi.org/10.18653/v1/2023. findings-emnlp.378 [73] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, et al. 2023. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563 (2023). [74] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . https://doi.org/10.18653/v1/2021.naacl-main.466 [75] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 1–67. [76] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. InProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , Jian Su, Kevin Duh, and Xavier Carreras (Eds.). Association for Computational Linguistics, Austin, Texas, 2383–2392. https://doi.org/10.18653/v1/D16-1264 [77] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-Context Retrieval-Augmented Lan- guage Models. Transactions of the Association for Computational Linguistics 11 (2023), 1316–1331. https://doi.org/10.1162/tacl_a_00605 [78] Muhammad Shihab Rashid, Jannat Ara Meem, Yue Dong, and Vagelis Hristidis. 2024. EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models. arXiv preprint arXiv:2402.10866 (2024). [79] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021. Rocketqav2: A joint training method for dense passage retrieval and passage re-ranking. arXiv preprint arXiv:2110.07367 (2021). [80] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al . 1995. Okapi at TREC-3. Nist Special Publication Sp 109 SIGIR ’25, July 13–18, 2025, Padova, IT Abdallah et al. (1995), 109. [81] Tomáš Koˇ ciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Her- mann, Gábor Melis, and Edward Grefenstette. 2018. The NarrativeQA Reading Comprehension Challenge. Transactions of the Association for Computational Linguistics TBD (2018), TBD. https://TBD [82] Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen- tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval with Zero-Shot Question Generation. (2022). https://arxiv.org/abs/2204.07496 [83] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. arXiv preprint arXiv:2112.01488 (2021). [84] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense Reasoning about Social Interactions. In Pro- ceedings of the 2019 Conference on Empirical Methods in Natural Language Pro- cessing and the 9th International Joint Conference on Natural Language Process- ing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 4463–4473. https://doi.org/10.18653/v1/D19-1454 [85] Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021. Sim- ple Entity-centric Questions Challenge Dense Retrievers. In Empirical Methods in Natural Language Processing (EMNLP) . [86] Amit Singhal et al. 2001. Modern information retrieval: A brief overview. IEEE Data Eng. Bull. 24, 4 (2001), 35–43. [87] Nilanjan Sinhababu, Andrew Parry, Debasis Ganguly, Debasis Samanta, and Pabitra Mitra. 2024. Few-shot Prompting for Pairwise Ranking: An Effective Non-Parametric Retrieval Model. arXiv preprint arXiv:2409.17745 (2024). [88] Raphaël Sourty, Jose G Moreno, Lynda Tamine, and François-Paul Servant. 2022. Cherche: A new tool to rapidly implement pipelines in information retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 3283–3288. [89] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid Questions Meet Long-Form Answers. In Proceedings of the 2022 Con- ference on Empirical Methods in Natural Language Processing , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Lin- guistics, Abu Dhabi, United Arab Emirates, 8273–8288. https://doi.org/10.18653/ v1/2022.emnlp-main.566 [90] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. ArXiv abs/2304.09542 (2023). [91] Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. 2019. QuaRTz: An Open-Domain Dataset of Qualitative Relationship Questions. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Process- ing and the 9th International Joint Conference on Natural Language Process- ing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 5941–5946. https://doi.org/10.18653/v1/D19-1608 [92] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Min- nesota, 4149–4158. https://doi.org/10.18653/v1/N19-1421 [93] Manveer Singh Tamber, Ronak Pradeep, and Jimmy Lin. 2023. Scaling Down, LiT- ting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder Models. arXiv preprint arXiv: 2312.16098 (2023). [94] Simone Tedeschi, Simone Conia, Francesco Cecconi, and Roberto Navigli. 2021. Named Entity Recognition for Entity Linking: What Works and What’s Next. In Findings of the Association for Computational Linguistics: EMNLP 2021 . Associa- tion for Computational Linguistics, Punta Cana, Dominican Republic, 2584–2596. https://aclanthology.org/2021.findings-emnlp.220 [95] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification. In NAACL-HLT. [96] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [97] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop Questions via Single-hop Question Composition. Transactions of the Association for Computational Linguistics (2022). [98] Jiajia Wang, Jimmy Xiangji Huang, Xinhui Tu, Junmei Wang, Angela Jennifer Huang, Md Tahmid Rahman Laskar, and Amran Bhuiyan. 2024. Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges. Comput. Surveys 56, 7 (2024), 1–33. [99] Jiexin Wang, Adam Jatowt, and Masatoshi Yoshikawa. 2022. Archivalqa: A large- scale benchmark dataset for open-domain question answering over historical news collections. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 3025–3035. [100] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. 2020. Minilmv2: Multi-head self-attention relation distillation for compressing pre- trained transformers. arXiv preprint arXiv:2012.15828 (2020). [101] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022). [102] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597 [cs.CL] [103] Haoyi Xiong, Jiang Bian, Yuchen Li, Xuhong Li, Mengnan Du, Shuaiqiang Wang, Dawei Yin, and Sumi Helal. 2024. When search engine services meet large language models: visions and challenges. IEEE Transactions on Services Computing (2024). [104] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. In Proceedings of the 9th International Conference on Learning Representations (ICLR 2021) . [105] Ikuya Yamada, Akari Asai, and Hannaneh Hajishirzi. 2021. Efficient Passage Retrieval with Hashing for Open-domain Question Answering. In ACL. [106] Liu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jianfeng Gao, W Bruce Croft, Xi- aodong Liu, Yelong Shen, and Jingjing Liu. 2019. A hybrid retrieval-generation neural conversation model. In Proceedings of the 28th ACM international confer- ence on information and knowledge management . 1341–1350. [107] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain Question Answering. In Proceedings of the 2015 Con- ference on Empirical Methods in Natural Language Processing , Lluís Màrquez, Chris Callison-Burch, and Jian Su (Eds.). Association for Computational Lin- guistics, Lisbon, Portugal, 2013–2018. https://doi.org/10.18653/v1/D15-1237 [108] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 2369–2380. https://doi.org/10. 18653/v1/D18-1259 [109] Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained transformers for text ranking: BERT and beyond. In Proceedings of the 14th ACM International Conference on web search and data mining . 1154–1156. [110] Soyoung Yoon, Eunbi Choi, Jiyeon Kim, Hyeongu Yun, Yireun Kim, and Seung won Hwang. 2024. ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval. arXiv:2402.15838 [cs.IR] https://arxiv.org/abs/2402.15838 [111] Xiao Yu, Yunan Lu, and Zhou Yu. 2024. LocalRQA: From Generating Data to Locally Training, Testing, and Deploying Retrieval-Augmented QA Systems. arXiv preprint arXiv:2403.00982 (2024). [112] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a Machine Really Finish Your Sentence?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . [113] Hansi Zeng, Hamed Zamani, and Vishwa Vinay. 2022. Curriculum Learning for Dense Retrieval Distillation. In Proc. SIGIR. 1979–1983. [114] Nengjun Zhu, Jian Cao, Xinjiang Lu, and Qi Gu. 2021. Leveraging pointwise prediction with learning to rank for top-N recommendation. World Wide Web 24 (2021), 375–396. [115] Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. 2023. Rankt5: Fine-tuning t5 for text ranking with ranking losses. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2308–2313. Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009