Reality Distortion Room: A Study of User Locomotion Responses to Spatial Augmented Reality Effects * Y ou-Jin Kim University of California Santa Barbara Andrew D. Wilson Microsoft Research Jennifer Jacobs University of California Santa Barbara Tobias H¨ollerer University of California Santa Barbara Figure 1: The Spatial Augmented Reality (SAR) environment is augmented by four depth cameras and five short throw overhead projectors. a. User looking at a wall as the room’s geometric shape transforms. b. First-person point of view (POV) from the perspective of a user viewing a virtually extended space in the room. c. To improve users’ spatial awareness, numerous floating particles were rendered to their POV in the environment. d. In order to ensure user safety in a relatively dark area, texture pattern overlays are projected onto physical furniture to make it look like an illuminated hologram-like item. ABSTRACT Reality Distortion Room (RDR) is a proof-of-concept augmented re- ality system using projection mapping and unencumbered interaction with the Microsoft RoomAlive system to study a user’s locomotive response to visual effects that seemingly transform the physical room the user is in. This study presents five effects that augment the appearance of a physical room to subtly encourage user motion. Our experiment demonstrates users’ reactions to the different dis- tortion and augmentation effects in a standard living room, with the distortion effects projected as wall grids, furniture holograms, and small particles in the air. The augmented living room can give the impression of becoming elongated, wrapped, shifted, elevated, and enlarged. The study results support the implementation of AR experiences in limited physical spaces by providing an initial under- standing of how users can be subtly encouraged to move throughout a room. Index Terms:Human-centered computing—Empirical studies in HCI; Computing methodologies—Mixed / augmented reality Com- puting methodologies—Virtual reality Computing methodologies— Perception 1 INTRODUCTION A growing number of virtual experiences take the user’s physical environment into account, which leads to an expansion of potential and possibilities in immersive home entertainment. Many gamers consuming entertainment in their homes are increasingly turning to immersive experience technology such as virtual reality (VR) and augmented reality (AR) for an extended reality or presence platform experience [38, 49]. Some VR work specifically addresses the question of supporting navigation in large VR environments while relying on real walking *This is a preprint version of this article. The final version of this paper can be found in the Proceedings of IEEE ISMAR 2023. For citation, please refer to the published version. This work was initially made available on the Microsoft Research website [microsoft.com] on September 2023, and was subsequently uploaded to arXiv for broader accessibility. in smaller physical environments. Redirected walking offers natural locomotion with correct proprioceptive, kinesthetic, and vestibu- lar stimulation, but it requires sizable actual tracking spaces [35]. Interaction-based redirected walking uses techniques such as warp- ing [9, 60] and sensory technologies [55] perceptual illusion [52], or space mapping [13, 56]. These techniques can operate in a smaller space; however, the experience is regularly interrupted to correct the user’s position when the user approaches the limit of the available walking space. VR routinely utilizes techniques such as room marking systems to assist users in navigating safely within the room when using VR. Room setup features introduced in SteamVR and Meta Quest SDK allow users to mark out surfaces of the physical layout of the home to better avoid collisions when the user is immersed in virtual reality [37, 58]. These methods, such as collision bounds and Chaperone, cannot be directly applied to AR applications as the physical layout is present at all times in the platform experience. While redirected walking in Mixed Reality using VR headset and passive haptics [22, 54] has been examined, our research marks a step towards redirected walking in visual AR. Motion parallax and perspective-correct rendering of computer graphics content allow augmented experiences that are different from the physical layout the viewer is in [4, 12]. In this work, we additionally explore the possibility of subtly manipulating a user’s natural locomotion via visual motion effects. Reality Distortion Room (RDR) presents five room distortion treatments in augmented reality that employ the user’s visual percep- tion and spatial understanding to subtly manipulate their position via natural locomotion. Using wireframe overlay effects, we mapped walls and furniture to generate an omnidirectional room-scale dis- play that renders a 3D reconstruction and extension of the user’s physical space. Out of the five distortion treatments, three treatments are designed to impact the user’s directional motion pattern (move- ment along an axis, refer to Fig. 2a for axis directions), whereas the other two treatments are designed to impact the user’s motion to and from the room center (distance-to-center). Our study examines natural locomotion and visual perception. We selected RealityShader to simulate distortion treatments because it seamlessly blended projected and physical environments while the physical layout remained undistorted [61]. AR headsets with see-through waveguide displays, such as HoloLens 2 or Magic Leap arXiv:2510.23840v1 [cs.HC] 27 Oct 2025 Figure 2: a. Top view of the 4.5 m x 5.5 m room with furniture where the study and distortion effects were conducted. Left: Floor layout as scanned by Kinect v2 sensor cameras. Right: Digital Double 3D model of room. b. Left: Using SLAM (Simultaneous Localization and Mapping), a live 3D reconstruction of the room from a side angle. Right: Digital Double 3D model of the room from the same angle. Figure 3: a. First-person point of view (POV) from the perspective of a user viewing a virtually extended space in the room. b. Over- head view of the room environment where the user experiences the distortion effects. c. First-person POV as seen via head-tracking and perspective correction, where the green area designates the space that is extended through projection using the Elongation Distortion. d. Overhead view of the physical space of the room compared to its virtual extension during the Elongation Distortion. 2, are equipped with relatively small field-of-view displays, which limits immersion. Video pass-through MR, such as the Varjo XR-3 or Meta Quest Pro, is becoming more commonplace but still has fidelity problems. Thus, we opted to run this study in spatial aug- mented reality, using spatial projection as seen in Fig.1a. We utilized RealityShader’s projected augmented reality system [18] to capture and react to the user’s locomotion response. The RealityShader sys- tem allows us to assess visual distortion effects while enabling users to walk in a fully surrounded projected space and untethered to a physical device as seen in Fig. 3b. We performed several additional treatments, including augmenting the space with randomly floating particles and overlaying furniture outlines as seen in Fig. 1. Our system encourages users to move in certain ways within the room. In applying the distortion treatments in an immersive AR ex- perience, our study found that the Reality Distortion Room impacts users’ movement and reactions in specific ways, without explicitly telling them to do so. This study provides the first empirical evidence that developers can influence the motion of users by warping and modulating the AR space, which suggests a potential mechanism to be used in the eventual realization of redirected walking in an AR environment. We see great application in our system for helping users make modest positioning adjustments, especially when stan- dard tools for adjustments, such as sound systems, are being used. We make the following contributions: • Design and pilot testing of different distortion geometries for our system, the Reality Distortion Room. • A user study (n=20) demonstrating the effectiveness of system- atically influencing a user’s natural locomotion. • Analysis of study results, demonstrating user locomotion re- sponses to generic room shape changes in a projected aug- mented environment. In particular, a directional effect and a center distancing effect are demonstrated as a reaction to the ge- ometric deformation of the environment (distortion treatment) alone. 2 RELATEDWORK Our platform offers a virtual world that alters and extends existing physical space. Making use of physical layout to enhance the expe- rience in a virtual world was previously explored. Fuchs envisioned the potential ways to utilize the combination of virtual and real worlds, implementing the CA VE system in the Office of the Future project [43]. The spectrum of the AR-VR continuum was previously explored from the human computer interaction aspect [2,3,32,48,50] Each method utilizes space in different ways when users are explor- ing virtual environments. For example, one such method detaches the user entirely from the physical space, making the movement fric- tionless and stationary using a treadmill [8,15,16,30]. Other methods include adjusting sensitivity of input and output of the tracked move- ment between virtual and real to redirect the user [26, 56] or provid- ing a dynamic haptic environment where the physical layout adopts to a virtual one [6, 15, 57]. In addition, previous work investigating “vection”, or “illusions of self-motion” study how to convincingly simulate human locomotion in virtual environments without having to allow for full physical movement of the user [46,47]. RDR, on the other hand, looks at the problem from the perspective of inducing the full physical movement of the user without explicit guidance systems or instructions. Therefore, we designed a system to in- duce movement patterns in user locomotion without informing them about the desired movement pattern, meaning participants were only instructed to move around freely in the environment. Inspired by these earlier studies, our work embraces the situated physical reality and our senses within it in our mixed reality experience. Through our proof of concept and user study, we demonstrate how to subtly manipulate user locomotion in AR space. Figure 4: Example path of a user moving around the room during a one-minute trial, demonstrating full possible utilization of the physical space without fear of bumping into objects in the dark environment. 2.1 Experiencing Large Space Experiencing a larger environment than the one in which one exists can be both physically and conceptually disengaging for users [13]. To overcome these issues of disengagement, creative measures are taken to redirect attention and enable redirected walk- ing [40, 44, 52, 56] through procedurally generated virtual space from 3D reconstructed physical space [48, 51]. Event-based meth- ods, such as dynamic saccadic redirection, have showcased a way to reduce the space required for immersive experiences [55]. Remixed Reality [28] explores the direct manipulation of the environment by offering users different and larger room layout options from their perspective. To further extend virtual space through augmentation and remote user presence, Room2Room [41] presented a prototype implementation for rearranging and extending the virtual layout in consideration of physical space through augmentation. Additionally, Reality Check [13] demonstrated how a virtual game environment can be combined with real-time 3D reconstruction of the room, resulting in a presence platform experience. 2.2 Room-scale Interaction The physical constraints of a room can make it difficult to fully experience the six degrees of freedom in virtual reality. However, extensive research led to the exploration of redirected walking [35, 44, 53], a method that subtly adjusts the user’s direction without them noticing, allowing the user to be immersed in a large virtual environment within limited physical layout they are operating in to provide a seamless experience [25, 60, 62]. Recent works [3, 18, 19] adopted a spatial augmented reality (SAR) system, embracing limited room space and scale and deliver- ing a customized layout. This platform suggests strategies that use a room for an access point, port, and physical interactive space while expanding the interactivity well beyond the room scale. In recent years, we have seen many room-scale VR games that use the layout of the room both as part of the virtual environment layout and use the full floor space as an interactive space [7, 17, 59]. For example, the VR game Custom Home Mapper: Castle Defender (2020) uses the player’s room layout to generate the top floor of the tower balcony. Eye of the Temple (2020) constructs a temple maze from surroundings for the player, and Tea for God (2021) converts the player’s room to a bunker with windows on each wall. Room-scale interactability is appealing as it can be catered to current VR users, which is why research targeting room-scale mixed reality persists [10, 25, 52]. Our system also caters to the standard room- scale layout as seen in Fig. 2. 2.3 3D Reconstruction for Physical Spaces The wide accessibility of depth cameras has produced much re- markable research around 3D reconstruction using these technolo- gies [34, 39] such as adding IMU sensors [11] to examine outdoor 3D navigation [42]. Robust 3D reconstruction in a combination of object detection research [1, 24, 27, 31] provides insight into always- on display AR experiences [14,33]. HoloLens and Magic Leap, both augmented reality (AR) devices, utilize spatial mapping [29, 63], scan and examine the physical layout of the user’s surroundings to find the optimal location to place virtual contents [20, 23]. Projects like FLARE, SnapToReality, VRoamer and Dynamic Theater ex- hibit the potential these intelligent virtual content projections have to enrich immersive mixed reality experiences [5, 10, 21, 36]. While current spatial mapping for VR-AR systems does well with scanning surroundings for planar surfaces, finding usable space to project vir- tual objects, and designing a virtual environment, they are ultimately restricted to the physical layout. The benefits and impacts of a wide field of view beyond our vision are clear [45]. Our projection mapping system utilizes a 3D reconstructed model of the room to create a view from the virtual environment. That view is then projected onto real physical surfaces such as the walls and furniture in the room, simulating the perspective of a co-located virtual world [13] using the real world as a baseline. We conduct our user study in a full-surround spatial augmented reality system, augmenting the entire human field of view and beyond, in order to create the most convincing user experience of the transformation of their surroundings. We designed a set of virtual environments that are aligned or par- tially aligned with the physical room that is deformed, extended, and subtracted from the perspective of the user’s eyes as demonstrated in Remixed Reality [28]. In our system, the user sees the physical world at all times as seen in Fig. 1b. We build the experience around the room layout including the furniture in the room. 3 REALITYDISTORTIONROOM Reality Distortion Room uses the RoomAlive infrastructure [3,18] to deliver a full surrounded augmented reality experience. The room ge- ometry is scanned and loaded into the game environment and Unity workspace where sets of geometric space transformation (distortion treatment) are deployed. Virtual models are placed into the scene in relation to the physical room to project reconstructed geometry. This allows our system to extend the virtual world from the real world, as if the room is transforming, by rendering the physical world within the scene. To simulate the changing geometric environment through the projected walls of the physical room, the synchronization be- tween the user’s head position, digital twin, and the real world is crucial. The live reconstruction of the physical environment is done using four RGB-D cameras (Microsoft Kinect v2) that are placed in each corner of the room. The real-time geometric representation of the world is then directly placed in the virtual game environment that warps and enlarges while tracking the head position to reflect the user’s perspective. 3.1 System Infrastructure The AR projected room of approximately 4.5 × 5.5 meters incorpo- rates four Microsoft Kinect v2 depth cameras in each corner of the ceiling line. We placed the furniture objects as we would in our own living room, making a close representation of the living room where the user would use our system. Five wide field-of-view projectors render a 360◦ Spatial AR system, projecting to surfaces within the room and fully utilizing the objects inside the room including the furniture and moving objects. Virtual objects are presented from the user’s viewpoint as found in RoomAlive system [18]. The distorting 3D geometry model is placed and oriented in the physical room and the scene is constructed based on the viewpoint of the user. As the projection is rendered in a view-dependent manner, the participant is free to walk around the room without a headset. As the user in the room is not tethered to anything they are encouraged to walk around and conduct true normal locomotion. Since Reality Distortion Room features full surround visual coverage of the virtual environment, projected onto the physical environment, what the user will see is the room they are standing in, transformed into a different shape. While the distortion treatment is underway, the user’s stereopsis will not align with the intended visual of a deformed room. We made sure that all our distortion treatments return back to the default phys- ical room layout with the standard projection mapping applied. We believe coming back to a condition where the virtual environment aligns with the physical arrangement is crucial for the user’s affor- dance as this process blends real and virtual. For the virtual room to be precisely aligned with the physical counterpart, the system goes through a calibration process. Static 3D geometry that includes both stationary and moving objects is reconstructed with data collected from scanning a cloud of 3D points. With these baseline dimensions, projected content may be precisely aligned with the physical layout. Details on this calibration process can be found in the RoomAlive paper [18]. 3.2 Distortion Treatment Components Distortion treatment alters the geometric perception of the physical layout. We implement various transformations of the environment. We evaluated various distortion treatments to determine their ef- fectiveness in generating consistent locomotion transitions. We examined 10 treatment designs: elongation, warp, shift, elevation, enlarge, enlarge (even larger), rotation, twist, furniture rotation, and furniture shift. Highlighted grid panels, where each tile is 65cm x 65cm, overlay the entire room assisting the user in understanding the transforming geometry while showing the scale transformation. We also added particles in the space to demonstrate accurate reflections of motion parallax as users moved as well as to assist with seeing added or subtracted space. This easily allows better spatial aware- ness, while inducing more movement without presenting one target. Each particle is 1.92cm in radius and floats at a speed of 1 cm per second in a random direction (Fig. 1c). Approximately 712 particles float around in every 10-meter cubed space. Every trial we tested featured particles in space except for one trial, which was baseline without distortion. While designing three distortion treatments that stimulate the oc- cupant’s movement along an axis identified in Fig. 2), we imagined how we would move and respond to a transforming space around us. We identified where people would be most likely to walk to- ward to secure their safety or view when the room began distorting. The same concept was applied to the two distortion treatments that influenced the user’s distance to the center of the room. When de- signing the floor layout of the distortion treatment we made sure the virtual space did not transform any smaller than the physical walkable space, presenting the full room layout for users to walk around without being able to step out from the virtually rendered space. Each distortion treatment is divided into two phases: apply and return. Apply represents the first segment where the distortion treat- ment begins, and return entails the latter segment where the virtually distorted room reverts to the original physical room layout. The timeline of the transformations used in each trial for all distortion effects can be seen in Fig. 6. The dependent variable is the participant’s reactions, which we measured through user locomotion. User locomotion is when a participant moves around the room while an assigned stimulus is being applied or returned. The Reality Distortion stimulus was measured as either a directional effect (user movement along an axis) or the Central Effect (change of the user’s distance to the room center) and depends on the design of the distortion treatment. The directional effect measures the user’s movement in relation to the axis along the room’s smaller dimension (i.e. left and right in Figure 11) while the apply and return stimuli are in effect and consists of Elongation, Warp, and Shift Distortions. The central effect (distance to center) measures the change in the user’s posi- tioning away from or towards the center of the room compared to before/after the stimulus segment and consists of Elevation and En- large Distortions. The average of the total user movement during each stimulus segment (10 seconds) was used for our evaluation. Baseline refers to the situation in which no distortion treatment is augmented. We established a baseline by running trials with parti- cles but no visual effects or treatments. We separated the baseline data into 10-second segments for the analysis. We randomly selected 15 segments from each user’s trial to include all potential data seg- ments from the trial. Along with the five distortion treatments, we conducted two additional trials: 1) no distortion treatment with no particles and 2) no distortion treatment with particles. As all trials with distortion treatment included particles, base data points were collected in trials with particles although no distortion treatment was applied. 3.3 Pilot Study The experimental portion of this study was designed to measure the effect of distortion treatment on user locomotion responses. First, experiment design (distortion treatment) is a set of geometric distor- tions that affect the user’s movement patterns within the room along the axes, central point, and rotation. All the distortion effects occur with respect to the physical room, regardless of the orientation of the user. We examined all ten distortion designs in our pilot study. In addition, each treatment was tested with two different segment speeds (7 seconds and 10 seconds), for a total of twenty treatments in which ten induced the directional effect while the other ten induced the central effect. We also assessed the impact of deploying particles to encourage increased walking. We found that distortion effects characterized by excessively rapid transformations, intricate geometric alterations, and rotation effects did not conform to discernible patterns. For example, some participants noted difficulty comprehending the trans- formations based on the speed or geometry of their implementation, citing dizziness and confusion. Based on this feedback, We omitted certain distortion treatments, altered the geometry and speed of the remaining transformations, and here are the five distortion designs included in the main study: • Distortion treatment 1 (Elongation) refers to a geometric layout transformation of a room where one wall recedes into the distance and then returns (elongates and shortens). This is called Elongation Distortion and the treatment is designed to have a directional effect. The Elongation Distortion (Fig. 3) elongates one side of the wall outward horizontally for 3.35 meters during the “Apply” segments of the stimulus, while in the “Return” segments the elongated wall shortens back to align with the physical room. • Distortion treatment 2 (Warp) refers to a geometric layout transformation of a room where the entire room warps and unwarps. This is called the Warp Distortion and the treatment is designed to have a directional effect. The Warp Distortion consisted of the continuous warping (bending) of extended versions of two opposite parallel walls of the room. The dis- tortion treatment consists of one warp and unwarp action for each stimulus segment. The bend angle of the parallel walls consists of 160° with the bend executed along a 19.33m length of the wall. • Distortion treatment 3 (Shift) refers to a geometric layout transformation of a room where parallel walls shift and unshift (shift back) horizontally. This is called the Shift Distortion and the treatment is designed to have a directional effect. The Shift Distortion utilizes two parallel walls of the room, shifting 5.14 meters side to side in the horizontal direction. During the “Apply” segments the wall shifts in a set horizontal direction, Figure 5: Distortion treatments 1, 2 and 3 stimulate axis movement, while distortion treatments 4 and 5 stimulate distance to the center. while in the “Return” segments (unshift) the wall shifts back in the opposite horizontal direction. • Distortion treatment 4 (Elevate) refers to a geometric layout transformation of a room that ascends from and descends to the ground level. The user experiences this as either being elevated above or sinking below the ground. This is called the Elevation Distortion and the treatment is designed to have a central effect (changing the user’s distance to the room center). The Elevation Distortion consists of the virtual room going up and down 8.07 meters vertically. During the stimulus segments, the room elevates from and descends to the base ground level. • Distortion treatment 5 (Enlarge) refers to a geometric layout transformation of a room that enlarges and compresses. Users experience this as the walls moving away from or coming closer to them. This is called the Expansion Distortion and, like for Elevate, the treatment is designed to have a central gathering or dispersion effect. The Enlarge Distortion consists of two segments: virtual room ’enlarge’ and ’compress’. The room enlarges to double the width, length and height while expanding its volume from 56.82 m3 to 454.56 m3. 4 EXPERIMENT We recruited 20 participants, ages 23 to 32, of which eight identi- fied as male and twelve identified as female. Four participants had previously tried VR while only two previously experienced AR. On average, the study lasted approximately 30 minutes, including the time needed for the instruction, and consisted of seven trials and a 10-minute interview. We conducted one trial for each distortion treatment and two additional baseline trials: no treatment with parti- cles and without particles. The RDR system described in 3.1 System Infrastructure is used in a room environment that is approximately 4.5m × 5.5m × 2.5m (Fig.2). Based on dominant results from prior work in Spatial AR that highlight creative ways to interact with ex- tended reality, and the ways that participants responded to controlled and comprehensive space, we defined the following hypotheses: • H1: The augmented distortion treatment can induce partici- pants to move more in some directions than others. • H2: The augmented distortion treatment can induce partici- pants to move closer to or away from the center of the room. • H3: The augmented distortion treatment can induce partici- pants to turn or move in a circular direction. 4.1 Procedure Participants were introduced to all five distortion treatments in each trial in addition to two trials for the baseline: static room with particles and without particles in the space. Each trial lasted a full 60 seconds, consisting of two cycles of stimulus segments: two “Apply” segments and two “Return” segments. When participants arrived, a researcher guided them into the room equipped with the projection system. Upon entry, the room projectors remained off though ambient lighting allowed the user to see objects in the room. The ambient lights remained on through the entire trial but were outshone by the lights from the projectors. Participants were given a brief introduction to our system and verbal instructions for the study: they are free to move around and interact naturally within the room during the trial but asked to refrain from sitting down on any surfaces. We encouraged walking and examining the room during the trial and let them know that we would be asking a few questions following the trial. We also informed participants that they were free to leave the room at any time if they felt discomfort (i.e. sickness, fright). A researcher remained outside of the room to monitor participants through the 3D reconstructed live inspector. After each 60-second trial, we turned off the projection and asked the participants to sit in the center seat of the couch. In between trials, we asked a few questions about their experience to ensure the participant felt okay and ready to continue. Before moving to the next trial, we asked if they could describe in a few words what they saw and experienced. After the final trial, participants were asked to reflect on their experience of each treatment and safety concern. Finally, each participant was compensated with a $10 gift card. Figure 6: The room transformation process during active distortion treatment, using the Expansion Distortion as shown above. During the “Apply” segments, the virtual room enlarges for 10 seconds. This is immediately followed by the “Return” segments, where the virtual room is compressed for 10 seconds until the virtually extended space merges back to the original room layout. Figure 7: The two location density maps, shown above, reflect data collected from twenty users during each segment: (a) Enlarge (apply phase) and (b) Compress (return phase). As shown in the timeline, collections are taken from two 4-second intervals: 2 seconds after and 2 seconds before the end of each phase of “Apply” and “Return”. This shows how floor space was utilized immediately before and after the end of each phase. 5 RESULTS The experimental design (Fig. 5) of this study intended to evalu- ate the changes in users’ positions before and after the geometric transformation in the Spatial Augmented Reality room due to the application of distortion treatments. In this section, we report two types of Distortion Effects, with three distortion treatments designed for directional effect, and two distortion treatments designed for central effect. 5.1 Particle Effect and Natural Locomotion Our goal was to induce more natural walking without influencing the participant’s movement in a specific direction, as there were no user tasks assigned. Here we compared the total walking distance in the room with particles, without particles, and without the distortion treatment. Without particles, the mean total walking distance was 16.70 m with a standard deviation of 6.38 m. Furniture outlines and wall grids were present in baseline trials with and without particles. Users walked an average distance of 28.80 meters, with a standard deviation of 6.75m, in studies with particles, revealing that the existence of particles significantly increased walking distance. (p< 0.001). As a result of an ANOV A analysis by classifying walking distance into a “No particle” group and a “With particles” group, there was a significant difference in the mean between the two groups (Fig. 8c). In the ANOV A table, the F value was 34.538, and the Figure 8: Total walking distance (TWD) in effects with and without particles, revealing that the existence of particles increases walking distance ( p< 0.001). The raincloud plot shows user distribution of TWD result and box indicating the median and interquartile range (IQR). p-value was less than 0.001 demonstrating a statistically significant difference between the “No particle” group and the “With particles” group. 5.2 Directional Effect: Axis Movement The following reports the directional effects of the three distortion treatments we designed and tested. We evaluated the 10-second user movement along axis direction within each stimulus segment. As seen from the data point of Fig.9, we saw a clear signal and trend between each stimulus segment and baseline. “Baseline” shows point data generated from 20 participants’ baseline trials (no distor- tion treatment + particles) in two 10-second intervals throughout the trial. In the apply stimulus segment, users tend to move toward the positive axis direction (refer to Fig. 2 for axis directions), while moving in the opposite direction for the return stimulus segment. The experimental group was classified by Baseline Group, Apply Group, and Return Group and the ANOV A analysis showed signifi- cant differences between the three. In the ANOV A table between the three groups, the F value was 198.329, and the p-value < 0.00005, showing a statistically significant difference in the size of apply segment and return segment. Post hoc analysis using Bonferroni adjustment also showed a significant difference in the mean between the three groups as follows. The difference between Baseline Group and Apply Group is t value = -7.747, p-value < 0.001. The differ- ence between Baseline Group and Return Group is t value=-8.924, p-value < 0.01. The difference between Return Group and Apply Group istvalue=-13.948,p-value<0.001. Each stimulus segment was tracked from apply and return, re- spectively. The movement of the axis for each group is analyzed as shown in the Fig. 11. The average value of the return and apply seg- ment is different for each group of the Elongation Distortion, Warp Distortion, and Shift Distortion. Based on mean distance moved, the Shift Distortion resulted in the largest directional user movement, followed by the Warp Distortion, and the Elongation Distortion. 5.3 Central Effect: Distance to Center We analyzed the average movement change of the user throughout the 10-second distortion segments by comparing their position in the room before and after the segments. In other words, we measured the displacement of the user during the 10-second interval. Out of the five distortion treatments, two of them were designed and tested to manipulate the user’s position relative to the distance from the center of the room. During the application stimulus segment, users generally moved away from the center, whereas during the return stimulus segment, users tended to move toward the center of the Figure 9: Users’ movements on the axis of consideration corresponding to the effect applied to the room. The chart shows data points (vertical lines) from each stimulus segment from 20 participants. Each vertical line shows data points denoting the average movement made in the axis direction over a 10-second period. We show stimulus segments among four conditions (Baseline, Elongation Distortion, Warp Distortion and Shift Distortion). Visible trends of users’ axis movement in the apply/return phase of three groups of distortion treatments become apparent by comparing with the baseline condition (see also statistical comparison in Fig. 10). Figure 10: Users’ movements on the axis of consideration corre- sponding to the effect applied to the room. “Baseline” represents the movement of twenty users during randomly chosen 10-second inter- vals without distortion effects. During the ”Apply” segments, users generally moved in the positive axis direction and in the opposite direction during the ”Return” segments. Error bars indicate 95%CI. An ANOVA with Bonferroni-corrected post-hoc pairwise comparisons reveals significant induced user movement during the distortion treat- ments. room. The remaining three distortion treatments had no consistent central effect on the user movement. Experimental groups were divided into ”apply” and ”return” groups based on stimulus segments, and their distance to the center was analyzed using ANOV A. As a result, significant differences between groups are shown. In the ANOV A table, theF value was 109.123 and the p-value < 0.001, showing a statistically significant difference in the size of apply and return. The movement from the distance from the center for each group is analyzed as shown in Fig.13. Based on the mean distance moved from the center, the Enlarge Distortion resulted in the most user movement. This was found by comparing both stimulus segments although both distortion effects performed comparably. Overall, our study aimed to investigate the influence of augmented distortion treatment on users’ natural locomotion while relying on their ability to comprehend the spatial transformation of the environ- ment. We hypothesized that H1: The augmented distortion treatment can induce participants to move more in some directions than others Figure 11: Magnitudes of the apply and return of axis movements compared between three different distortion effects. Based on mean distance moved, the shift effect resulted in the most user movement towards the positive and negative ends of the axis, followed by the warp effect, and the elongation effect. and H2: The augmented distortion treatment can induce participants to move closer to or away from the center of the room. Our user study results indicated that both H1 and H2 hold true. Additionally, the Shift Distortion showed the largest directional effect while the Enlarge Distortion showed the largest central gath- ering/dispersion effect. The third hypothesis (H3) aimed to test whether specific augmented distortion treatments could influence participants’ natural locomotion to turn or move in a circular di- rection. During the pilot study, however, none of the designed treatments including those intended to induce circular movements, such as Rotation or Twist, were successful in producing this motion pattern. Participants also reported experiencing difficulty under- standing and dizziness, which made it challenging to comprehend the nature of these transformations from Rotation or Twist Distortion effects. Therefore, the third hypothesis was not confirmed. Figure 12: A raincloud plot depicting movement away and towards the center of the room when the elevation and expansion effects are applied. The upper whisker boundary of the box-plot is the largest data point that is within the 1.5 IQR above the third quartile. According to apply and return, respectively, the distance to the center was tracked. Figure 13: Magnitudes of the apply and return movements measured how far a user was from the center of the room. Based on the mean distance moved, the expansion effect resulted in slightly more movement away and towards the center, followed by the Elevation Distortion. 6 DISCUSSION As no clear objective was given, participants generally assumed that we wanted feedback on the space we designed, or that we were showcasing the new AR system. Many of them shared ideas on new 3D space ideas and what we should try next. This worked in our favor as we did not want to hint that we are examining their locomotion; our goal was to capture users’ natural locomotion from the distortion treatment. We found compelling differences between the stimulus segments apply, return, and the baseline segment in locomotion response. Among all three distortion treatments for manipulating directional effect, the Shift Distortion had the largest distance manipulated before and after the stimulus segments, while the Warp Distortion treatment showed the biggest positive axis movement when the room is being warped. This is likely due to users trying to get a better vantage point to see the end of the hallway and get a better idea of what is happening in their surroundings. The Elongation Distortion had the weakest effect, but when comparing each stimulus segment to the baseline segment, the treatment still worked with the shortest mean average distance effect to axis. Of the two central effects, the Enlarged Distortion exhibited the most prominent distance-to-center effect before and after the stimulus segments. Only one participant removed themselves from the study, to an- swer a phone call. No participant requested a break during the distortion treatment augmentation or expressed sickness during our study. Participants’ responses to Elevation Distortion showed a gen- erally negative sentiment, stating that “it took some time for me to understand what was happening” (P4) and “it was apparent when the room was going up but when the room was going down I wasn’t convinced” (P17). In contrast, many participants expressed that the Enlarge Distortion was fun and refreshing: “The room gradually ex- panding in all directions was my favorite” (P13) and “I felt like I was floating in space” (P14). Two participants requested to experience the Enlargement Distortion again after the study. In contrast to these many positive experiences, a few participants bumped into objects during the enlarge stimulus segment. Lastly, most participants found particles in the space to be a nice addition, stating it “magical”and “fun to interact with” (P5). 6.1 Limitations Reality Distortion Room demonstrates a visual perception locomo- tion aid designed for the purpose of safety, entertainment, and inter- action. While the study provides insight into how visual perception can be used to manipulate a user’s natural locomotion, its findings are based on a small participant sample and brief trials, underscoring the need for deeper exploration of design considerations. We tested five distortion treatment designs in this study under- standing that many more distortion treatment designs could be ex- plored. Our user study suggests interesting directions for future work and we foresee many ways to expand this concept. The challenge in using distortion treatment is that the transformation is very no- ticeable and, at times, intrusive. Creative solutions must be explored in using distortion treatment to deliver a cohesive user experience. We suggest distortion treatment to be adopted in catered space and situations, where the design of the geometric distortion is modified to the designated physical space. This curated experience, where aspects of the real-world deviate from reality, requires further study. Our study utilized a full-surround augmented reality platform with projections covering the entire human field of view and beyond, achieving this setup at home is unlikely. Additionally, while the system was the best fit for our use, whenever the user came within 60 cm of the wall, the user’s shadow became visible, blocking the projection of the wall. Also, the outline projected to furniture disappeared when the user blocked the projection. Though we had overwhelmingly positive responses from the par- ticipants, we believe the novelty factor may have played a role in how actively the participants explored the room, inducing more walking in the room. In addition, people moved without any in- struction or clear objective in our study, as we wanted to examine if we could manipulate users without using instruction by only using distortion treatment. This raises questions about the performance of distortion treatments when a user is presented with tasks in the space and the effectiveness of the distortion effect when using parts of the projected environment rather than the grid system. We hope to tackle this question in our future work. 7 CONCLUSION Through this project, we examined the Reality Distortion Room, a proof of concept that shows how visual perception of certain room distortion effects can invoke cohesive natural locomotion responses from the user. We tested a variety of spatial orientation visual ef- fects. The user study demonstrated that the distortion treatments we designed were effective in influencing users’ natural locomotion in predictable ways. By relying on users’ reactions to their visual perception of space, we can open new ways to engage with famil- iar environments, or to navigate in an enhanced, altered, or even completely virtual reality. We are especially excited that this study presents possibilities of instilling movement in people solely through visual deformations of the AR spaces they populate. ACKNOWLEDGMENTS The authors thank Emma Lin and Jacqueline Mei for their assistance, generosity, and continuous support in this project. The work was supported in part by NSF award IIS-2211784. REFERENCES [1] L. A. Alexandre. 3d descriptors for object and category recognition: a comparative evaluation. InWorkshop on Color-Depth Camera Fusion in Robotics at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vilamoura, Portugal, vol. 1, p. 7. Citeseer, 2012. [2] H. Benko, E. Ofek, F. Zheng, and A. D. Wilson. Fovear: Combining an optically see-through near-eye display with projector-based spatial augmented reality. InProceedings of the 28th Annual ACM Symposium on User Interface Software & Technology, UIST ’15, p. 129–135. Association for Computing Machinery, New York, NY , USA, 2015. doi: 10.1145/2807442.2807493 [3] H. Benko, A. D. Wilson, and F. Zannier. Dyadic projected spatial augmented reality. InProceedings of the 27th Annual ACM Symposium on User Interface Software and Technology, UIST ’14, p. 645–655. Association for Computing Machinery, New York, NY , USA, 2014. doi: 10.1145/2642918.2647402 [4] G. Bruder, F. Steinicke, P. Wieland, and M. Lappe. Tuning self-motion perception in virtual reality with visual illusions.IEEE Transactions on Visualization and Computer Graphics, 18(7):1068–1078, 2012. doi: 10.1109/TVCG.2011.274 [5] L.-P. Cheng, E. Ofek, C. Holz, and A. D. Wilson. Vroamer: Generating on-the-fly vr experiences while walking inside large, unknown real- world building environments. In2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pp. 359–366. IEEE, 2019. doi: 10.1109/VR.2019.8798074 [6] L.-P. Cheng, T. Roumen, H. Rantzsch, S. K¨ohler, P. Schmidt, R. Kovacs, J. Jasper, J. Kemper, and P. Baudisch. Turkdeck: Physical virtual reality based on people. InProceedings of the 28th Annual ACM Symposium on User Interface Software & Technology, UIST ’15, p. 417–426. Association for Computing Machinery, New York, NY , USA, 2015. doi: 10.1145/2807442.2807463 [7] CuriousVR. Custom home mapper, 2022. [8] P. Darken, Rudolph, R. Cockayne, William, and D. Carmein. The omni-directional treadmill: A locomotion device for virtual worlds. In UIST ’97. Calhoun, 1997. [9] Z.-C. Dong, X.-M. Fu, C. Zhang, K. Wu, and L. Liu. Smooth assembled mappings for large-scale real walking.ACM Trans. Graph., 36(6), nov 2017. doi: 10.1145/3130800.3130893 [10] R. Gal, L. Shapira, E. Ofek, and P. Kohli. Flare: Fast layout for augmented reality applications. InMixed and Augmented Reality (ISMAR), 2014 IEEE International Symposium on. IEEE, September 2014. [11] S. Giancola, J. Schneider, P. Wonka, and B. Ghanem. Integration of absolute orientation measurements in the kinectfusion reconstruction pipeline. pp. 1567–156709, 06 2018. doi: 10.1109/CVPRW.2018. 00198 [12] J. J. Gibson. The theory of affordances. the ecological approach to visual perception. InThe People, Place and, Space Reader, pp. 56–60. Routledge New York and London, 1979. [13] J. Hartmann, C. Holz, E. Ofek, and A. D. Wilson. Realitycheck: Blend- ing virtual environments with situated physical reality. InProceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI ’19, p. 1–12. Association for Computing Machinery, New York, NY , USA, 2019. doi: 10.1145/3290605.3300577 [14] A. Ibrahim, B. Huynh, J. Downey, T. H ¨ollerer, D. Chun, and J. O’donovan. Arbis pictus: A study of vocabulary learning with augmented reality.IEEE Transactions on Visualization and Computer Graphics, 24(11):2867–2874, 09 2018. doi: 10.1109/TVCG.2018. 2868568 [15] H. Iwata, H. Yano, H. Fukushima, and H. Noma. Circulafloor [locomo- tion interface].IEEE Computer Graphics and Applications, 25(1):64– 67, 2005. [16] H. Iwata, H. Yano, and H. Tomioka. Powered shoes. InACM SIG- GRAPH 2006 Emerging Technologies, SIGGRAPH ’06, p. 28–es. As- sociation for Computing Machinery, New York, NY , USA, 2006. doi: 10.1145/1179133.1179162 [17] R. S. Johansen. Eye of the temple: First steps, 2020. [18] B. Jones, R. Sodhi, M. Murdock, R. Mehra, H. Benko, A. Wilson, E. Ofek, B. MacIntyre, N. Raghuvanshi, and L. Shapira. Roomalive: Magical experiences enabled by scalable, adaptive projector-camera units. InProceedings of the 27th Annual ACM Symposium on User Interface Software and Technology, UIST ’14, p. 637–644. Association for Computing Machinery, New York, NY , USA, 2014. doi: 10.1145/ 2642918.2647383 [19] B. R. Jones, H. Benko, E. Ofek, and A. D. Wilson. Illumiroom: Periph- eral projected illusions for interactive experiences. InProceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’13, p. 869–878. Association for Computing Machinery, New York, NY , USA, 2013. doi: 10.1145/2470654.2466112 [20] Y .-J. Kim, R. Kumaran, E. Sayyad, A. Milner, T. Bullock, B. Gies- brecht, and T. H¨ollerer. Investigating search among physical and virtual objects under different lighting conditions.IEEE Transactions on Visu- alization and Computer Graphics, pp. 1–11, 2022. doi: 10.1109/TVCG .2022.3203093 [21] Y .-J. Kim, J. Lu, and T. H¨ollerer. Dynamic theater: Location-based immersive dance theater, investigating user guidance and experience. InProceedings of the 29th ACM Symposium on Virtual Reality Software and Technology, VRST ’23. Association for Computing Machinery, New York, NY , USA, 2023. doi: 10.1145/3611659.3615705 [22] L. Kohli, E. Burns, D. Miller, and H. Fuchs. Combining passive haptics with redirected walking. InProceedings of the 2005 International Conference on Augmented Tele-Existence, ICAT ’05, p. 253–254. As- sociation for Computing Machinery, New York, NY , USA, 2005. doi: 10.1145/1152399.1152451 [23] R. Kumaran, Y .-J. Kim, A. E. Milner, T. Bullock, B. Giesbrecht, and T. H¨ollerer. The impact of navigation aids on search performance and object recall in wide-area augmented reality. InProceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI ’23. Association for Computing Machinery, New York, NY , USA, 2023. doi: 10.1145/3544548.3581413 [24] K. Lai, L. Bo, X. Ren, and D. Fox.RGB-D Object Recognition: Features, Algorithms, and a Large Scale Benchmark, pp. 167–192. Springer London, London, 2013. doi: 10.1007/978-1-4471-4640-7 9 [25] E. Langbehn, P. Lubos, and F. Steinicke. Evaluation of locomotion techniques for room-scale vr: Joystick, teleportation, and redirected walking. InProceedings of the Virtual Reality International Conference - Laval Virtual, VRIC ’18. Association for Computing Machinery, New York, NY , USA, 2018. doi: 10.1145/3234253.3234291 [26] E. Langbehn, P. Lubos, and F. Steinicke. Redirected spaces: Going beyond borders. In2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), pp. 767–768, 03 2018. doi: 10.1109/VR.2018. 8446167 [27] R. Li, K. Olszewski, Y . Xiu, S. Saito, Z. Huang, and H. Li. V olumetric human teleportation. InACM SIGGRAPH 2020 Real-Time Live!, SIG- GRAPH ’20. Association for Computing Machinery, New York, NY , USA, 2020. doi: 10.1145/3407662.3407756 [28] D. Lindlbauer and A. D. Wilson. Remixed reality: Manipulating space and time in augmented reality. InProceedings of the 2018 CHI Conference on Human Factors in Computing Systems, CHI ’18, p. 1–13. Association for Computing Machinery, New York, NY , USA, 2018. doi: 10.1145/3173574.3173703 [29] Magic-Leap-Developer-Portal. What is spatial mapping? Magic Leap. https://resources.magicleap.com/en-us/privacy/spatial-mapping- overview-and-detail-options, 2023. Accessed: August 2023. [30] E. Medina, R. Fruland, and S. Weghorst. Virtusphere: Walking in a human size vr “hamster ball”.Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 52(27):2102–2106, 2008. doi: 10 .1177/154193120805202704 [31] A. Meka, R. Pandey, C. H¨ane, S. Orts-Escolano, P. Barnum, P. David- Son, D. Erickson, Y . Zhang, J. Taylor, S. Bouaziz, C. Legendre, W.- C. Ma, R. Overbeck, T. Beeler, P. Debevec, S. Izadi, C. Theobalt, C. Rhemann, and S. Fanello. Deep relightable textures: V olumetric performance capture with neural rendering.ACM Trans. Graph., 39(6), nov 2020. doi: 10.1145/3414685.3417814 [32] P. Milgram and F. Kishino. A taxonomy of mixed reality visual displays. IEICE TRANSACTIONS on Information and Systems, 77(12):1321– 1329, 1994. [33] A. Nassani, H. Bai, G. Lee, and M. Billinghurst. Tag it! ar annotation using wearable sensors. InSIGGRAPH Asia 2015 Mobile Graphics and Interactive Applications, SA ’15. Association for Computing Ma- chinery, New York, NY , USA, 2015. doi: 10.1145/2818427.2818438 [34] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon. Kinect- fusion: Real-time dense surface mapping and tracking. In2011 10th IEEE International Symposium on Mixed and Augmented Reality, pp. 127–136, 2011. doi: 10.1109/ISMAR.2011.6092378 [35] N. C. Nilsson, T. Peck, G. Bruder, E. Hodgson, S. Serafin, M. Whitton, F. Steinicke, and E. S. Rosenberg. 15 years of research on redirected walking in immersive virtual environments.IEEE Comput. Graph. Appl., 38(2):44–56, mar 2018. doi: 10.1109/MCG.2018.111125628 [36] B. Nuernberger, E. Ofek, H. Benko, and A. D. Wilson. Snaptoreality: Aligning augmented reality to the real world. InProceedings of the 2016 CHI Conference on Human Factors in Computing Systems, CHI ’16, p. 1233–1244. Association for Computing Machinery, New York, NY , USA, 2016. doi: 10.1145/2858036.2858250 [37] Oculus. Oculus SDK, 2022. [38] Oculus Samples. The world beyond, 2022. [39] B. Peasley and S. Birchfield. Replacing projective data association with lucas-kanade for kinectfusion. In2013 IEEE International Conference on Robotics and Automation, pp. 638–645, 2013. doi: 10.1109/ICRA. 2013.6630640 [40] T. C. Peck, H. Fuchs, and M. C. Whitton. Improved redirection with distractors: A large-scale-real-walking locomotion interface and its effect on navigation in virtual environments. InProceedings of the 2010 IEEE Virtual Reality Conference, VR ’10, p. 35–38. IEEE Computer Society, USA, 2010. doi: 10.1109/VR.2010.5444816 [41] T. Pejsa, J. Kantor, H. Benko, E. Ofek, and A. Wilson. Room2room: Enabling life-size telepresence in a projected augmented reality envi- ronment. InProceedings of the 19th ACM Conference on Computer- Supported Cooperative Work & Social Computing, CSCW ’16, p. 1716–1725. Association for Computing Machinery, New York, NY , USA, 2016. doi: 10.1145/2818048.2819965 [42] U. Qayyum and J. Kim. Inertial-kinect fusion for outdoor 3d navigation. 01 2013. [43] R. Raskar, G. Welch, M. Cutts, A. Lake, L. Stesin, and H. Fuchs. The office of the future: A unified approach to image-based modeling and spatially immersive displays. InProceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, SIG- GRAPH ’98, p. 179–188. Association for Computing Machinery, New York, NY , USA, 1998. doi: 10.1145/280814.280861 [44] S. Razzaque, Z. Kohn, and M. C. Whitton. Redirected Walking. In Eurographics 2001 - Short Presentations. Eurographics Association, 2001. doi: 10.2312/egs.20011036 [45] D. Ren, T. Goldschwendt, Y . Chang, and T. H¨ollerer. Evaluating wide- field-of-view augmented reality with mixed reality simulation. In2016 IEEE Virtual Reality (VR), pp. 93–102, 2016. doi: 10.1109/VR.2016. 7504692 [46] B. E. Riecke, D. Feuereissen, J. J. Rieser, and T. P. McNamara. Self- motion illusions (vection) in vr — are they good for anything? In 2012 IEEE Virtual Reality Workshops (VRW), pp. 35–38, 2012. doi: 10 .1109/VR.2012.6180875 [47] B. E. Riecke and J. Schulte-Pelkum. Perceptual and cognitive fac- tors for self-motion simulation in virtual environments: how can self- motion illusions (“vection”) be utilized? InHuman walking in virtual environments, pp. 27–54. Springer, 2013. [48] E. Sayyad, M. Sra, and T. H¨ollerer. Walking and teleportation in wide- area virtual reality experiences. In2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pp. 608–617, 2020. doi: 10 .1109/ISMAR50242.2020.00088 [49] Schell Games. I expect you to die: Home sweet home, 2022. [50] L. Shapira and D. Freedman. Reality skins: Creating immersive and tactile virtual environments. In2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pp. 115–124. IEEE, 2016. doi: 10.1109/ISMAR.2016.23 [51] M. Sra, S. Garrido-Jurado, and P. Maes. Oasis: Procedurally generated social virtual spaces from 3d scanned real spaces.IEEE Transactions on Visualization and Computer Graphics, 24(12):3174–3187, 2018. doi: 10.1109/TVCG.2017.2762691 [52] M. Sra, X. Xu, A. Mottelson, and P. Maes. Vmotion: Designing a seamless walking experience in vr. InProceedings of the 2018 Design- ing Interactive Systems Conference, DIS ’18, p. 59–70. Association for Computing Machinery, New York, NY , USA, 2018. doi: 10.1145/ 3196709.3196792 [53] F. Steinicke, G. Bruder, J. Jerald, H. Frenz, and M. Lappe. Estimation of detection thresholds for redirected walking techniques.IEEE Trans- actions on Visualization and Computer Graphics, 16(1):17–27, 2010. doi: 10.1109/TVCG.2009.62 [54] E. A. Suma, D. M. Krum, and M. Bolas. Redirected walking in mixed reality training applications. InHuman Walking in Virtual Environments, pp. 319–331. Springer, 2013. [55] Q. Sun, A. Patney, L.-Y . Wei, O. Shapira, J. Lu, P. Asente, S. Zhu, M. Mcguire, D. Luebke, and A. Kaufman. Towards virtual reality infinite walking: Dynamic saccadic redirection.ACM Trans. Graph., 37(4), jul 2018. doi: 10.1145/3197517.3201294 [56] Q. Sun, L.-Y . Wei, and A. Kaufman. Mapping virtual and physical reality.ACM Trans. Graph., 35(4), jul 2016. doi: 10.1145/2897824. 2925883 [57] R. Suzuki, H. Hedayati, C. Zheng, J. L. Bohn, D. Szafir, E. Y .-L. Do, M. D. Gross, and D. Leithinger.RoomShift: Room-Scale Dynamic Hap- tics for VR with Furniture-Moving Swarm Robots, p. 1–11. Association for Computing Machinery, New York, NY , USA, 2020. [58] Valve. Steamvr, 2022. [59] V oid Room. Tea for god, 2022. [60] N. L. Williams, A. Bera, and D. Manocha. Arc: Alignment-based redirection controller for redirected walking in complex environ- ments.IEEE Transactions on Visualization and Computer Graphics, 27(5):2535–2544, 2021. doi: 10.1109/TVCG.2021.3067781 [61] A. Wilson. Realityshader: Holograms without headsets. [62] G. Wilson, M. McGill, M. Jamieson, J. R. Williamson, and S. A. Brewster. Object manipulation in virtual reality under increasing levels of translational gain. InProceedings of the 2018 CHI Conference on Human Factors in Computing Systems, CHI ’18, p. 1–13. Association for Computing Machinery, New York, NY , USA, 2018. doi: 10.1145/ 3173574.3173673 [63] M. Zeller, V . Tieto, H. Arya, and H. Ferrone. Spatial mapping with HoloLens. https://docs.microsoft.com/en-us/windows/mixed- reality/design/spatial-mapping, 2023. Accessed: August 2023.