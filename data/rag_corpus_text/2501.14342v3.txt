Chain-of-Retrieval Augmented Generation Liang Wang†∗ Haonan Chen‡ Nan Yang† Xiaolong Huang† Zhicheng Dou‡ Furu Wei† †Microsoft Research ‡Renmin University of China https://aka.ms/GeneralAI Abstract This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method,CoRAG (Chain-of-RetrievalAugmentedGeneration), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model’s test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models. 4k 8k 16k 32k 64k 128k # Avg. T okens 18 20 22 24 26 28 30EM CoRAG on MuSiQue Ours Previous SoTA Pareto frontier (a) Where did the star of Dark Hazard study? What was the name of the star of Dark Hazard? Edward G. Robinson Where did Edward G. Robinson go to college? No relevant information found. What college did Edward G. Robinson attend? City College of New York. City College of New York decomposition retrieval failure reformulation (b) Figure 1: (a) Test-time scaling behavior of CoRAG. Increased token budget leads to consistent performance improvements. (b) An example of CoRAG on the MuSiQue dataset. It learns to decompose the complex query and conduct query reformulation when encountering a retrieval failure. ∗Correspondence to wangliang@microsoft.com 39th Conference on Neural Information Processing Systems (NeurIPS 2025). arXiv:2501.14342v3 [cs.IR] 10 Oct 2025 1 Introduction Retrieval-augmented generation (RAG) [20] is one of the core techniques in enterprise applications, necessitating the integration of large foundation models with proprietary data sources to produce responses that are both grounded and factual. Conventionally, foundation models are trained on large-scale datasets comprising trillions of tokens and remain frozen post-deployment. Nonetheless, these models frequently struggle to memorize long-tail factual knowledge or may hallucinate false claims, resulting in unreliable responses in real-world scenarios. RAG mitigates this challenge by augmenting the generation process with retrieved information, thereby improving the trustworthiness of model-generated content and facilitating the incorporation of up-to-date information. Contemporary RAG systems typically employ a sequential pipeline of retrieval and generation, wherein the retrieved information serves as additional input to the generative model. The effectiveness of RAG systems predominantly relies on the quality of the retrieved information. Retrieval models are engineered for efficiency to ensure scalability to large corpora. For instance, dense retrievers [18, 35] commonly utilize a bi-encoder architecture to compress documents and queries into fixed-size vector representations. This architectural choice permits the use of fast approximate nearest neighbor search algorithms but simultaneously constrains the expressive capacity of retrieval models to handle complex queries. Furthermore, in multi-hop reasoning tasks, it is often unclear what information should be retrieved initially; decisions must be made based on the progressively evolving state of the reasoning process. To break the bottleneck of retrieval quality, we propose a framework that dynamically retrieves relevant information and plans subsequent retrieval steps based on the current state. By adjusting the number of retrieval steps at test time, our model can explore various aspects of the query and experiment with different query rewriting strategies when the retriever does not yield useful information. This paradigm mirrors the human problem solving process, where we iteratively seek information to address complex questions. An example is illustrated in Figure 1. Rather than solely relying on the model’s in-context learning capability [ 42] or distillation from proprietary models [1], we advocate for explicitly training language models to retrieve step by step. To this end, we utilize rejection sampling [43, 5] to augment existing RAG datasets with intermediate retrieval chains. Open-source language models are then fine-tuned on these augmented datasets using standard next-token prediction objectives. To examine the scaling behavior of our model, we propose various test-time decoding strategies, including greedy decoding, best-of-N sampling, and tree search. Diverse decoding strategies and hyperparameter configurations can be employed to control test-time token consumption and the frequency of retriever calls. Our empirical evaluation demonstrates that CoRAG substantially surpasses strong baselines in QA tasks that require multi-hop reasoning, where retrievers frequently struggle to recall all necessary information in a single retrieval step. Across diverse decoding strategies, the Pareto frontier approxi- mately adheres to a log-linear relationship between total token consumption and model performance, although the coefficients differ across datasets. On the KILT benchmark [27], which encompasses a more diverse array of tasks, new state-of-the-art scores are achieves on thehidden test setfor nearly all tasks. Additionally, we uncover that CoRAG exhibits varied scaling behaviors across different task types. For datasets such as NQ [ 19], where state-of-the-art retrievers already achieve high recall, the benefits of test-time scaling are often marginal. This suggests the potential for dynamically allocating test-time compute based on the complexity of the query and the quality of the retriever. Upon further analysis, we find that CoRAG can effectively decompose complex queries and perform flexible query reformulation to improve the quality of the generated responses. It also shows robustness against retrievers of varying quality. We posit that CoRAG represents a promising avenue for future research in the RAG domain, with the potential to mitigate hallucination in model-generated content. Our code, data and trained models are available athttps://github.com/microsoft/LMOps/tree/main/corag. 2 Related Work Retrieval-Augmented Generation (RAG)integrates information retrieval techniques with generative models to enhance the quality and factual accuracy of generated content [ 20, 21]. By equipping LLMs with the ability to browse the web [ 26], RAG systems can access real-time data, thereby 2 providing responses that are both up-to-date and grounded. The relevance and quality of the retrieved information are pivotal for the efficacy of RAG systems. A substantial body of recent research has concentrated on developing better general-purpose text embeddings [ 18, 35]. Nevertheless, text embeddings frequently face limitations in addressing complex queries due to their reliance on fixed-size vector representations for efficiency purposes. To mitigate this constraint, contemporary research has extended the conventional paradigm of a single retrieval step followed by generation, advancing to multi-step iterative retrieval and generation [6]. FLARE [ 13] prompts an LLM to actively determine when and what to retrieve during the generation process. ITER-RETGEN [ 30] proposes to interleave retrieval-augmented generation with generation-augmented retrieval, demonstrating enhancements in multi-hop QA tasks. Similarly, IRCoT [ 33] employs a chain-of-thought methodology, which recursively refines the reasoning thought for subsequent retrieval steps. Self-RAG [1] empowers LLMs to adaptively retrieve, generate, and critique through self-reflection, thus improving factual accuracy and citation precision in open-domain QA and long-form generation tasks. Auto-RAG [41] utilizes heuristic rules and exact answer matching to construct intermediate retrieval steps, yet its performance remains significantly below that of state-of-the-art models. AQA [3] learns to reformulate questions using reinforcement learning but only focuses on single-hop QA tasks. In this study, rather than exclusively on few-shot prompting or distillation from proprietary models, we propose a novel approach to explicitly train LLMs to iteratively retrieve and reason over relevant information. Scaling Test-time ComputeInstead of prompting LLMs to directly generate the final answer, Chain- of-Thought (CoT) [ 36] demonstrates that letting the model to think step by step can drastically improve the performance on mathematical reasoning tasks. Tree-of-Thought (ToT) [ 40] extends the idea of CoT by adopting a tree structure, allowing the model to explore the search space more comprehensively. To further enhance the reasoning capabilities of LLMs, STaR [ 43] proposes to leverage bootstrapping techniques to generate intermediate states for training. OpenAI o1 [ 12] conducts large-scale reinforcement learning and exhibits promising test-time scaling behaviors on advanced reasoning datasets, but the technical details are not publicly available. A drawback of these methods is the increased token consumption, which consequently increases the response latency. In the realm of RAG, test-time compute can be increased by retrieving more documents or performing additional retrieval steps. LongRAG [14] posits that RAG performance can be enhanced by integrating long-context LLMs with more retrieved documents. In contrast, IterDRAG [42] empirically examines the test-time scaling law through few-shot prompting and iterative retrieval for up to 5M tokens. Search-o1 [22] combines the open-source QwQ model [37] with active search from Bing, achieving competitive results on knowledge-intensive tasks. Concurrent works such as Search-R1 [15] train LLMs to use retrieval as a tool via reinforcement learning. Our work extends the study of test-time scaling in RAG to a targeted fine-tuning paradigm under diverse decoding strategies. 3 Methodology The CoRAG framework is illustrated in Figure 2. The “Current State” denotes the input context and instructions provided to the LLM, while the “Next Action” refers to the LLM output responding to the given instruction. In this section, we describe the key components of CoRAG, including retrieval chain generation through rejection sampling, model training with augmented datasets, and strategies for scaling test-time compute. 3.1 Retrieval Chain Generation Most RAG datasets only come with a query Q and the corresponding final answer A, without providing intermediate retrieval steps. We propose an automated method for generating retrieval chains through rejection sampling. Each sampled chain consists of a sequence of sub-queries Q1:L ={Q 1, Q2, . . . , QL} and the corresponding sub-answers A1:L, where L is a predetermined maximum chain length. The sub-query Qi =LLM(Q <i, A<i, Q) is generated by sampling an LLM based on the query Q and the preceding sub-queries and sub-answers. To generate the sub-answer Ai, we first retrieve the top-k most relevant documents D(i) 1:k using a text retriever with Qi as the search 3 output query … … output query … …… … … … output query … … … … … … rolloutsrollouts Greedy Best-of-N Tree Search Inference Training Rejection Sampling LLM Current State Next Action Query sub-query 1 sub-answer 1 sub-query n sub-answer n … … P(Ans | Chain 1) sub-query 1* sub-answer 1* sub-query n* sub-answer n* … … P(Ans | Chain 2) … … … … … … … … … … P(Ans | Chain 3) LLM retriever chosen state discarded state answer Figure 2: Overview of CoRAG. Rejection sampling is utilized to augment QA-only datasets with retrieval chains. Each chain starts with the original query, followed by a sequence of sub-queries and sub-answers. An open-source LLM is then fine-tuned to predict the next action based on the current state. During inference, multiple decoding strategies are available to control the test-time compute. query, and subsequently prompt an LLM to yield the answer Ai =LLM(Q i, D(i) 1:k). This procedure is iterated until the chain reaches the maximum lengthLorA i matches the correct answerA. To assess the quality of a retrieval chain, we calculate the log-likelihood of the correct answer logP(A|Q, Q 1:L, A1:L) conditioned on the chain information. The retrieval chain with the highest log-likelihood score is selected to augment the original QA-only dataset. 3.2 Training Each training instance in the augmented dataset is represented as a tuple (Q, A, Q1:L, A1:L), accom- panied by the corresponding top- k retrieved documents for the query Q and each sub-query. We fine-tune an LLM on the augmented dataset using the standard next-token prediction objective within a unified multi-task learning framework. The model is simultaneously trained on three tasks: next sub-query prediction, sub-answer prediction, and final answer prediction. We employ the same prompt templates as utilized in the retrieval chain generation process, with the exception that we also incorporate the top retrieved documents D1:k for the original queryQas input for the final answer prediction task. Lsub_query =−logP(Q i|Q, Q<i, A<i), i∈[1, L] Lsub_answer =−logP(A i|Qi, D(i) 1:k), i∈[1, L] Lfinal_answer =−logP(A|Q, Q 1:L, A1:L, D1:k) The cross-entropy loss is computed only for the target output tokens. As we reuse the prompt templates for both data generation and model training, a fine-tuned model can be utilized for the next round of rejection sampling in an iterative manner. 3.3 Test-time Scaling Given a trained CoRAG model, we propose several decoding strategies to control the trade-off between model performance and test-time compute. The test-time compute is measured by the 4 total number of token consumptions, excluding the retrieval costs. Unlike previous approaches that consider only prompt tokens [42] or generated tokens [12], we account for both. To simplify further discussion, the prompt tokens are treated equally as the generated tokens, despite prompt tokens typically being less expensive due to prefix caching and computation parallelism of the prefilling stage. Greedy DecodingThis strategy utilizes greedy decoding to generate L sub-queries and their corre- sponding sub-answers sequentially. The final answer is generated using the same prompt template as employed during the training phase. Best-of-N SamplingThis method involves sampling N retrieval chains with a temperature 0.7, subsequently selecting the best chain to generate the final answer. As the ground truth answer is not available at test time, we instead calculate the conditional log-likelihood of“No relevant information found”as a penalty score for each chain. The retrieval chain with the lowest penalty score is chosen. Tree SearchWe implement a breadth-first search (BFS) variant with retrieval chain rollouts. At each step, the current state is expanded by sampling several sub-queries. For each expanded state, we perform multiple rollouts, and then compute the average penalty score of these rollouts. The state with the lowest average penalty score is retained for further expansion. To control the test-time compute, the maximum length of the retrieval chain L can be adjusted across all decoding strategies. For best-of-N sampling, the number of sampled chainsN offers an alternative option to scale the test-time compute. In tree search, the number of rollouts and expansion size are two additional hyperparameters. 4 Experiments 4.1 Setup Data and EvaluationWe evaluate CoRAG utilizing two sets of benchmarks: (1) a collection of multi-hop QA datasets, including 2WikiMultihopQA [ 8], HotpotQA [ 39], Bamboogle [ 28], and MuSiQue [32]; (2) the KILT benchmark [27], which encompasses a broad spectrum of knowledge- intensive tasks. The multi-hop QA datasets serve to evaluate the model’s capacity to perform multi-hop reasoning, whereas the KILT benchmark assesses the framework’s ability to generalize across more diverse tasks. For each training dataset, we prompt the open-sourceLlama-3.1-8B- Instructmodel to perform rejection sampling, unless specified otherwise. We utilize E5-large [ 34] as the text retriever for intermediate retrieval steps. The retrieval corpus is the English Wikipedia provided by KILT, comprising approximately36 million passages [25]. The selected retrieval chains are employed to augment the original QA-only datasets for subsequent model training. Regarding evaluation metrics, we report the exact match (EM) and F1 scores [29] for the multi-hop QA datasets. For the KILT benchmark, we submit the model’s predictions to the official evaluation server and report the downstream metrics on thehidden test set. To adhere to the leaderboard submission policy, we reportpublic validation setresults when conducting ablation studies on the KILT benchmark. Note that while HotpotQA and MuSiQue maintain public leaderboards, these adopt either a simplified reading comprehension setting or an abstract-only retrieval configuration. Conse- quently, the leaderboard results are not directly comparable to our open-domain QA evaluation setting. Model TrainingWe conduct full-parameter fine-tuning on the augmented datasets, initializing from theLlama-3.1-8B-Instructcheckpoint. Two separate models are trained: one for the multi-hop QA datasets and another for the KILT benchmark. The compiled multi-hop QA dataset comprises 125k training instances, whereas the KILT benchmark includes 660k instances after sub-sampling. The model is fine-tuned for 1 epoch with a maximum sequence length of 3k tokens. For the KILT benchmark, we fine-tune an E5-Mistral retriever [ 35] and a RankLLaMA re-ranker [ 24] on the respective training set to boost the ranking quality. Further implementation details are provided in Appendix A. 5 Table 1: Results on multi-hop QA datasets. We report the performance of CoRAG-8B using various decoding strategies and retrieval chain lengths L. The “Few-shot w/o Retrieval” configuration utilizes only QA pairs without retrieval augmentation. Both DRAG and IterDRAG are based on Gemini 1.5 Flash [31], while Search-o1-32B is based on QwQ [37] and the Bing Search API. 2WikiQA HotpotQA Bamboogle MuSiQue EM F1 EM F1 EM F1 EM F1 F ew-shot w/o Retrieval 3-shot Llama-3.1-8B-Inst. 27.6 32.1 20.8 28.8 17.6 21.3 3.4 9.7 3-shot GPT-4o 39.5 47.3 38.2 51.2 49.6 61.5 15.8 27.2 w/ Retrieval 3-shot Llama-3.1-8B-Inst. 30.7 39.9 34.1 46.6 28.0 37.3 7.7 15.4 3-shot GPT-4o 49.0 56.2 45.8 59.4 53.6 63.8 15.7 25.8 Self-RAG-7B 12.2 24.1 16.6 29.4 5.6 16.8 4.6 13.2 ITER-RETGEN 35.5 47.4 45.1 60.4 40.0 50.7 26.1 42.0 DRAG (32k) 45.9 53.7 46.9 60.3 48.8 59.2 15.4 26.0 IterDRAG (32k) 44.3 54.6 38.3 49.8 46.4 56.2 12.5 23.1 Search-o1-32B 58.0 71.4 45.2 57.356.067.8 16.6 28.2 Fine-tuned Llama-8B w/ E5large 55.1 60.7 50.3 63.5 40.8 53.7 17.4 28.1 CoRAG-8B (Ours) ▷ L=1, greedy 56.5 62.3 50.1 63.2 37.6 51.4 18.6 29.3 ▷ L=6, greedy 70.6 75.5 54.4 67.5 48.0 63.5 27.7 38.5 ▷ L=6, best-of-471.7 76.5 55.3 68.5 51.2 63.1 28.1 39.7 ▷ L=6, tree search 71.7 76.4 55.8 69.0 48.8 64.4 29.0 40.3 ▷ L=10, best-of-872.5 77.3 56.3 69.854.468.3 30.9 42.4 4.2 Main Results Multi-hop QAIn Table 1, we present a comparative analysis of CoRAG-8B against several models, including few-shot Llama-3.1-8B-Instruct [5], GPT-4o [10], Self-RAG-7B [1], ITER-RETGEN [30], DRAG, IterDRAG [42], and Search-o1-32B [ 22]. For a fair comparison, we also include a fine- tuned Llama-8B baseline utilizing the E5-large retriever, which is fine-tuned on the same datasets as CoRAG-8B but without retrieval chain augmentation. CoRAG-8B substantially surpasses all baselines, with the exception of the Bamboogle dataset, despite being based on a weaker LLM compared to Search-o1-32B and IterDRAG. Conversely, we recognize that fine-tuning on multi-hop QA datasets creates an advantage for CoRAG-8B, compared to the few-shot setting for DRAG and IterDRAG. The Bamboogle dataset comprises only 125 instances, resulting in considerable variance in perfor- mance across different runs. Certain questions within Bamboogle necessitate access to knowledge more recent than the Wikipedia dump used for retrieval. Systems like Search-o1-32B, which rely on commercial search engines, possess an advantage in this regard. KILT BenchmarkWe present several strong systems on the KILT benchmark in Table 2, including KILT-RAG [27], SEAL [2], Atlas-11B [11], RA-DIT 65B [23], and FiD with RS [9]. For submission to the KILT leaderboard, we choose the best decoding configuration for each task based on the public validation set. The results of different decoding strategies are detailed in Appendix Table 7. Our CoRAG-8B model achieves a new state-of-the-art performance across all tasks, with the exception of FEVER, where it marginally trails behind a larger model with 11B parameters. 4.3 Scaling Test-Time Compute In alignment with OpenAI o1 [12], our model allows for scaling test-time compute to potentially achieve better performance without updating model weights. There are multiple ways to control the test-time compute. In Figure 3, we concentrate on two factors: the retrieval chain length L and the number of sampled chains N for best-of-N sampling. Greedy decoding is a special instance of best-of-Nsampling withN= 1and the temperature set to0. 6 Table 2: The downstream results on thehidden test setof the KILT benchmark. All scores are sourced directly from the official leaderboard, with the exception that “RA-DIT 65B” is from the original paper [23]. ∗: “Previous Best” refers to the highest score for each task on the public KILT leaderboard as of January 10, 2025. System Entity Linking Slot Filling Open QA Fact AIDA WnWi WnCw T-REx zsRE NQ HoPo TQA FEVER KILT-RAG 72.6 48.1 47.6 59.2 44.7 44.4 27.0 71.3 86.3 SEAL - - - 83.6 74.6 53.7 40.5 70.9 89.5 Atlas-11B 90.6 - - 85.1 80.8 61.3 50.6 84.093.5 RA-DIT 65B 80.5 - - 72.8 78.1 43.5 36.6 72.8 86.9 FiD with RS - - - 85.2 83.7 61.2 39.1 84.6 92.2 Previous Best∗ 90.6 87.4 71.2 87.7 85.3 62.3 50.6 84.693.5 CoRAG-8B (Ours)93.9 88.2 76.7 88.0 87.2 63.1 60.6 88.393.1 2 4 6 8 10 Chain Length 56 58 60 62 64 66 68 70 72EM 2WikiMultihopQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 50 51 52 53 54 55 56 HotpotQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 40 42 44 46 48 50 52 54 Bamboogle greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 18 20 22 24 26 28 30 MuSiQue greedy best-of-4 best-of-8 4k 8k 16k 32k 64k 128k # Avg. T okens 57.5 60.0 62.5 65.0 67.5 70.0 72.5EM greedy best-of-4 best-of-8 tree search pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 50 51 52 53 54 55 56 greedy best-of-4 best-of-8 tree search pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 40 42 44 46 48 50 52 54 greedy best-of-4 best-of-8 tree search pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 18 20 22 24 26 28 30 greedy best-of-4 best-of-8 tree search pareto frontier Figure 3: Scaling test-time compute on multi-hop QA datasets. The Pareto frontier is in the form of y=a×log(x+b) +c fitted on the Pareto optimal points. A point is consideredPareto optimalif no other point achieves a higher EM score with less token consumption. The metric “# Avg. Tokens” represents the average number of tokens consumed per test instance, summing up both the prompt and generated tokens. We observe that increasing the retrieval chain lengthL results in substantial performance improve- ments when L is small, but the gains diminish as L increases. This observation aligns with the intuition that longer chains can encapsulate more reasoning steps and allows for trial-and-error explo- ration of various query rewriting strategies. Several examples are provided in Appendix Table 11. Conversely, increasing N for best-of-N sampling yields mixed effects depending on the dataset. For the most challenging dataset, MuSiQue, in terms of EM score, a larger N enhances performance, whereas for the less challenging dataset, 2WikiMultihopQA, a smaller N suffices. We defer the further exploration of tree search to future work, as it is considerably more computationally expensive than greedy decoding and best-of-Nsampling. The Pareto frontier between the EM score and token consumption approximately follows a log-linear trajectory for up to 128k tokens, although the scaling behavior varies across different datasets. This observation assists practitioners in making informed decisions regarding the allocation of test-time compute based on the quality requirements. It is important to note that we make several simplifications 7 in this scaling study, such as treating the prompt tokens equivalently to the generated tokens and ignoring the retrieval costs. A more rigorous analysis could take these factors into account. 5 Analysis Table 3: Ablation study results. “Iterative training” employs a trained CoRAG model for another round of rejection sampling. “Distill from GPT-4o” leverages the GPT-4o model to generate retrieval chains. “Weak-to-strong Generalization” utilizes weaker LLMs for retrieval chain generation while using stronger LLMs (Llama-3.1-8B-Inst.) for training. “Different Retrievers” replaces the text retriever at test time. 2WikiQA HotpotQA Bamboogle MuSiQue EM F1 EM F1 EM F1 EM F1 CoRAG-8B (L=6, greedy) 70.6 75.5 54.4 67.5 48.0 63.5 27.738.5 ▷iterative training 72.2 76.9 53.4 66.5 45.6 60.9 26.6 37.6 ▷distill from GPT-4o75.1 79.5 56.6 70.2 51.2 67.0 28.2 38.5 Weak-to-strong Generalization w/ Llama-3.2-1B-Inst. 59.3 64.2 50.3 63.6 40.8 51.6 22.3 32.7 w/ Llama-3.2-3B-Inst. 69.9 74.0 53.9 67.3 45.6 59.8 25.2 36.0 Different Retrievers E5-base w/o chain-of-retrieval 53.1 58.9 47.9 61.1 38.4 52.7 15.8 26.4 ▷L=6, best-of-470.8 75.4 53.0 66.2 47.2 59.8 26.3 37.6 BM25 w/o chain-of-retrieval 49.1 55.3 46.9 60.3 36.8 48.6 14.3 24.8 ▷L=6, best-of-462.6 67.7 51.6 64.7 37.6 52.5 23.5 33.0 5.1 Iterative Rejection Sampling Our framework facilitates self-improvement through iterative training, akin to the iterative rejection sampling employed in LLM post-training [ 5]. By utilizing the same prompt templates for both data generation and model training, a trained CoRAG model can generate new sets of retrieval chains. However, the results in Table 3 are mixed, showing performance improvements on the 2WikiMultihopQA dataset but slight declines on other datasets. This indicates that instruction-tuned LLMs already possess a strong ability to generate high-quality retrieval chains. 5.2 Robustness and Generalization Different RetrieversWe further investigate the influence of various text retrievers at test time. Instead of using the E5-large dense retriever, we substitute it with two weaker alternatives in a plug-and-play fashion: E5-base and BM25. Across all datasets, we observe consistent performance gains when investing more test-time compute, although stronger retrievers continue to outperform in terms of absolute performance. Improvements to text retriever quality represent an orthogonal dimension that can further amplify CoRAG’s performance gains. Weak-to-strong GeneralizationDue to the need of repeated sampling and autoregressive generation, the retrieval chain generation process costs more GPU hours than the model training. To mitigate this cost, one strategy is to employ weaker LLMs for retrieval chain generation and subsequently fine-tune stronger LLMs on the augmented datasets, similar to the weak-to-strong generalization setting [4]. The results in Table 3 demonstrate that utilizing Llama-3B achieves very close performance compared to the 8B model, whereas Llama-1B exhibits a noticeable performance drop. Manual inspection re- veals that the 1B model frequently struggles to follow the given instructions, resulting in sub-optimal retrieval chains. Employing weaker LLMs also lowers the barrier to adopting more computationally expensive tree search strategies during data generation, which show great potential in mathematical reasoning tasks [7]. In contrast, distilling from a stronger model like GPT-4o yields a further perfor- mance boost, indicating that the quality of the retrieval chains is crucial for the final performance. 8 5.3 Does Chain-of-Retrieval Always Help? 2 4 6 8 10 Chain Length 90 91 92 93 94 95EM FEVER greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 87 88 89 90 91EM TQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 59 60 61 62 63 64 65 66Accuracy NQ greedy best-of-4 best-of-8 Figure 4: Scaling test-time compute across three datasets from the KILT benchmark. We report scores on the public validation set. Multi-hop QA datasets are specifically designed to evaluate complex reasoning capabilities and are expected to benefit from the chain-of-retrieval mechanism. Table 1 presents empirical evidence supporting this assertion. In contrast, for tasks that a single retrieval step is typically sufficient, the advantage tends to be marginal, as demonstrated in Figure 4. Datasets such as NQ [ 19] and TriviaQA [17] are known for their (mostly) single-hop nature. This phenomenon implies that decoding strategies should be adaptive based on the complexity of the query. Additional results on the full KILT benchmark are listed in Appendix Table 7, where similar observations for other task types also hold. 5.4 Learning to Stop at Test Time 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Early Stop Logit Bias 18 20 22 24 26 28EM EM Score L=6 L=0 3.9k 4.9k 5.9k 6.8k 7.8k T oken Consumption # T okens MuSiQue (Greedy Decoding) Figure 5: Learning to stop at test time. Larger logit bias values result in earlier stopping. L= 6 correspond to always performing 6 retrieval steps, while L= 0 indicate no intermediate retrieval steps. Instead of always performing L retrieval steps, we explore a model variant that learns to stop at test time. After each retrieval step, the model is prompted to predict whether the information gathered thus far suffices to answer the query. Note that this prompt itself also incurs token consumption and additional cost. The decoding space is constrained to two tokens:“Yes”and “No”. If the decoded output is “Yes”, no further sub-queries are generated. By adjusting the logit bias of the “Yes” token, we can control the early stopping behavior. During the training phase, an additional loss term is added for the stop prediction task. The target output is “Yes” if the current retrieval chain encompasses the prefix that maximizes the likelihood of the final answer, and “No” oth- erwise. The associated prompt template is in Appendix Section D. In Figure 5, we illustrate how the performance varies along with the token consumption on the MuSiQue dataset. While early stopping can save some amount of token quota, it comes at the cost of performance degradation. The optimal configuration depends on the dataset characteristics and the quality expectations. 9 5.5 Does CoRAG Learn to Retrieve Better? To evaluate whether CoRAG improves retrieval quality beyond just answer accuracy, we measure retrieval recall across multiple datasets. We report Recall@k metrics for k∈ {10,20,100} , compar- ing standard retrieval using E5large against our approach. We follow the evaluation protocol from DPR [18] for calculating recall based on answer matches, as not all datasets provide gold supporting paragraphs. For CoRAG, we utilize reciprocal rank fusion to merge multiple retrieval results from the chain into a single ranked list, from which recall is calculated. Table 4: Retrieval recall comparison between standard retrieval and CoRAG across multi-hop QA datasets. R@10 R@20 R@100 HotpotQA w/ E5large 59.1 65.2 76.8 w/ CoRAG72.1 76.7 84.3 2WikiMultiHopQA w/ E5large 54.9 62.1 74.6 w/ CoRAG81.4 84.8 88.8 Bamboogle w/ E5large 31.2 40.0 57.6 w/ CoRAG59.2 68.0 75.2 MuSiQue w/ E5large 29.0 36.5 52.7 w/ CoRAG47.1 54.6 68.4 The results in Table 4 demonstrate that CoRAG consistently improves recall across all datasets and recall thresholds. The improvements are particularly pronounced on more challenging datasets like MuSiQue and Bamboogle, where single-step retrieval struggles most. This indicates that CoRAG’s iterative query reformulation and decomposition strategy effectively addresses the limitations of traditional dense retrieval, enabling the model to gather more relevant information through multiple retrieval steps. 6 Conclusion In this work, we introduce CoRAG, a framework that teaches LLMs to conduct iterative retrieval and reasoning to answer complex queries. The intermediate retrieval chains are automatically generated via rejection sampling, thereby alleviating the need for manual annotation. At test time, we offer multiple decoding strategies to manage the trade-off between performance and compute. Our experiments demonstrate that CoRAG-8B achieves state-of-the-art performance on both multi- hop QA datasets and the KILT benchmark, surpassing many baselines built with larger LLMs. A comprehensive analysis is conducted to understand its scaling behavior and generalization capability. In the future, we intend to extend CoRAG to more challenging and economically valuable RAG tasks, advancing towards building factual and trustworthy AI systems. 7 Limitations and Broader Impacts This study primarily investigates RAG tasks characterized by short and easy-to-verify answers, such as multi-hop QA and entity linking. However, real-world applications often necessitate addressing more complex tasks that demand generating long-form outputs. A significant challenge in long-form generation lies in the absence of robust evaluation metrics within the current research landscape. Regarding broader impacts, the proposed framework aims to improve the factuality and groundedness of language model outputs. It is anticipated that this work can facilitate more efficient and effective information retrieval for users. Nevertheless, the inherent risk of hallucination persists and warrants careful monitoring in practical deployments. References [1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learn- ing to retrieve, generate, and critique through self-reflection. InThe Twelfth International 10 Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Open- Review.net, 2024. URLhttps://openreview.net/forum?id=hSyW5go0v8. [2] Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H. Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. Autoregressive search engines: Generating substrings as document iden- tifiers. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors,Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/cd88d62a2063fdaf7ce6f9068fb15dcd-Abstract-Conference.html. [3] Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby, and Wei Wang. Ask the right questions: Active question reformulation with reinforcement learning.arXiv preprint arXiv:1705.07830, 2017. [4] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschen- brenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeffrey Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. InForty- first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URLhttps://openreview.net/forum?id=ghNRg2mEgN. [5] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.ArXiv preprint, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407. 21783. [6] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey.ArXiv preprint, abs/2312.10997, 2023. URL https://arxiv. org/abs/2312.10997. [7] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking.ArXiv preprint, abs/2501.04519, 2025. URLhttps://arxiv.org/abs/2501.04519. [8] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong, editors,Proceedings of the 28th International Conference on Computational Linguistics, pages 6609–6625, Barcelona, Spain (Online), 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https://aclanthology.org/2020.coling-main.580. [9] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. Multi-task retrieval- augmented text generation with relevance sampling.ArXiv preprint, abs/2207.03030, 2022. URLhttps://arxiv.org/abs/2207.03030. [10] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card.ArXiv preprint, abs/2410.21276, 2024. URLhttps://arxiv.org/abs/2410.21276. [11] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models.Journal of Machine Learning Research, 24(251): 1–43, 2023. [12] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card.ArXiv preprint, abs/2412.16720, 2024. URLhttps://arxiv.org/abs/2412.16720. [13] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969–7992, Singapore, 2023. Association 11 for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.495. URL https:// aclanthology.org/2023.emnlp-main.495. [14] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented generation with long-context llms.ArXiv preprint, abs/2406.15319, 2024. URL https: //arxiv.org/abs/2406.15319. [15] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Za- mani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning.arXiv preprint arXiv:2503.09516, 2025. [16] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for efficient retrieval-augmented generation research.ArXiv preprint, abs/2405.13576, 2024. URLhttps://arxiv.org/abs/2405.13576. [17] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min- Yen Kan, editors,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology. org/P17-1147. [18] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769–6781, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550. [19] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research.Transactions of the Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026. [20] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors,Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html. [21] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. From matching to generation: A survey on generative information retrieval.ArXiv preprint, abs/2404.14851, 2024. URLhttps://arxiv.org/abs/2404.14851. [22] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models.ArXiv preprint, abs/2501.05366, 2025. URLhttps://arxiv.org/abs/2501.05366. [23] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. RA-DIT: retrieval-augmented dual instruction tuning. InThe Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URLhttps://openreview.net/forum?id=22OTbutug9. [24] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. In Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang, editors,Proceedings of the 47th International ACM SIGIR Conference 12 on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, pages 2421–2425. ACM, 2024. doi: 10.1145/3626772.3657951. URL https://doi.org/10.1145/3626772.3657951. [25] Jean Maillard, Vladimir Karpukhin, Fabio Petroni, Wen-tau Yih, Barlas Oguz, Veselin Stoyanov, and Gargi Ghosh. Multi-task retrieval for knowledge-intensive tasks. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1098–1111, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.89. URL https: //aclanthology.org/2021.acl-long.89. [26] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo- pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback.ArXiv preprint, abs/2112.09332, 2021. URL https://arxiv.org/abs/2112.09332. [27] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523–2544, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.200. URL https:// aclanthology.org/2021.naacl-main.200. [28] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687–5711, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.findings-emnlp.378. URL https://aclanthology.org/2023.findings-emnlp. 378. [29] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras, editors,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URLhttps://aclanthology.org/D16-1264. [30] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248–9274, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.620. URL https: //aclanthology.org/2023.findings-emnlp.620. [31] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.ArXiv preprint, abs/2403.05530, 2024. URL https://arxiv.org/abs/2403.05530. [32] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition.Transactions of the Association for Computational Linguistics, 10:539–554, 2022. doi: 10.1162/tacl_a_00475. URL https: //aclanthology.org/2022.tacl-1.31. [33] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors,Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10014–10037, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.557. URLhttps://aclanthology.org/2023.acl-long.557. 13 [34] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. ArXiv preprint, abs/2212.03533, 2022. URLhttps://arxiv.org/abs/2212.03533. [35] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models.ArXiv preprint, abs/2401.00368, 2024. URLhttps://arxiv.org/abs/2401.00368. [36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors,Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html. [37] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report.ArXiv preprint, abs/2412.15115, 2024. URLhttps://arxiv.org/abs/2412.15115. [38] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report.arXiv preprint arXiv:2505.09388, 2025. [39] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URLhttps://aclanthology.org/D18-1259. [40] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors,Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html. [41] Tian Yu, Shaolei Zhang, and Yang Feng. Auto-rag: Autonomous retrieval-augmented generation for large language models.ArXiv preprint, abs/2411.19443, 2024. URL https://arxiv.org/ abs/2411.19443. [42] Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation.ArXiv preprint, abs/2410.04343, 2024. URL https://arxiv.org/ abs/2410.04343. [43] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors,Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html. A Implementation Details Rejection SamplingFor each training instance, we sample up to 16 retrieval chains, with the maximum length randomly selected from the interval [1,5] . The sampling temperature is set to 0.7 for sub-query generation and 0 for sub-answer generation. Chain generation is terminated if the sub-answer matches the correct answer or if the average conditional log-likelihood of the correct 14 answer exceeds −0.05. For each sub-query, we utilize the E5-large retriever 2 to retrieve the top-5 most relevant documents from the KILT version of the Wikipedia corpus [25]. This corpus comprises 36million passages. Table 5: Hyperparameters for training CoRAG. Multi-hop QA KILT Benchmark InitializationLlama-3.1-8B-Instruct Learning rate5×10 −6 10−5 Batch size256 1024 Epoch1 1 Warmup steps100 100 # Training samples125k660k # Retrieved passages20 20 Max sequence length3072 3072 Table 6: Statistics of the datasets used for multi-hop QA training. 2WikiMultihopQA HotpotQA Bamboogle MuSiQue # Training Samples15,000 90,447-19,938 # Validation Samples12,576 7,405 125 2,417 Multi-Hop QA Training HyperparametersThe training set is the union of the 2WikiMultihopQA, HotpotQA, and MuSiQue datasets, comprising a total of 125k samples as shown in Table 6. The Bamboogle dataset, consisting of only 125 questions, is reserved for evaluation only. Additional hyperparameters are detailed in Table 5. To balance the three loss terms in Section 3.2, we set a sample ratio of 0.2 for both the sub-query and sub-answer generation tasks; this ratio is also applied to the KILT training. KILT Training HyperparametersWe utilize the official training set of the KILT benchmark, omitting the ELI5 and WoW datasets due to the lack of reliable evaluation metrics. To balance the task distribution, we only select 100k samples for large datasets like T-REx and Zero-Shot RE. In accordance with the benchmark’s guidelines, we also add100k samples from the BLINK dataset for entity linking. Rather than using off-the-shelf retrievers, we fine-tune an E5-Mistral retriever following Wang et al., and a RankLLaMA re-ranker following Ma et al.. We adhere to the exact training hyperparameters outlined in the original papers, except that the training data is replaced with the KILT training set. For training the RankLLaMA re-ranker, the backbone is initialized with theLlama-3-8B-Basemodel, as opposed to Llama-2, to enhance performance. Retrieval and re-ranking scores are presented in Table 8. All training jobs are conducted using 8 A100 GPUs. The multi-hop QA task requires less than 6 hours of training, whereas the KILT training takes approximately 30 hours. When submitting to the KILT leaderboard, we select the optimal decoding strategy for each task based on validation set performance. Decoding StrategiesIn the context of best-of- N sampling, the temperature is set to 0.7 for sub-query generation. For sub-answer generation and final answer prediction, the temperature is always set to 0 across all decoding strategies. Regarding tree search, we set the expansion size to4and the number of rollouts to 2. Given that tree search incurs a significantly higher token consumption compared to other decoding strategies, we limit the rollouts to a maximum of 2 steps for each expansion. To avoid the model from generating repetitive sub-queries endlessly, any generated sub-query identical to previous ones is discarded. EvaluationFor multi-hop QA tasks, we evaluate the performance using the exact match (EM) and F1 scores [18]. For Self-RAG-7B, we reproduce the results utilizing the FlashRAG [16] toolkit with the official checkpoint released by the authors. 2https://huggingface.co/intfloat/e5-large-v2 15 For the KILT benchmark, we employ the official evaluation scripts provided by the organizers. For Open QA tasks, the main evaluation metric is the EM score, while other task types are evaluated using accuracy scores. The KILT benchmark also offers a variant of the evaluation protocol that requires the model not only to generate the correct answer but also to provide the correct supporting evidence. However, our method spreads the evidence documents across the retrieval chain, rendering it challenging to conform to such an evaluation protocol. B Additional Results Table 7: Downstream results on the publicvalidation setof the KILT benchmark. System Entity Linking Slot Filling Open QA Fact AIDA WnWi WnCw T-REx zsRE NQ HoPo TQA FEVER CoRAG-8B (Ours) ▷ L=1, greedy 90.4 86.076.8 87.082.1 62.5 56.4 88.4 91.4 ▷ L=6, greedy92.7 87.475.8 86.683.8 63.259.1 88.6 93.8 ▷ L=6, best-of-492.587.475.8 86.3 83.5 62.6 59.6 88.793.9 ▷ L=6, tree search 91.8 86.8 75.5 86.4 83.0 62.459.9 88.9 93.9 Table 8: Retrieval results (R-Precision) on the publicvalidation setof the KILT benchmark. For re-ranking, we use the top-100candidates from the fine-tuned retriever as input. System Entity Linking Slot Filling Open QA Fact AIDA WnWi WnCw T-REx zsRE NQ HoPo TQA FEVER Fine-tuned E5mistral 92.9 86.7 76.0 80.5 95.3 77.7 66.7 78.9 90.9 ▷w/ re-ranking 93.3 88.0 77.1 83.2 97.6 78.2 78.2 81.5 92.3 Different Decoding Strategies on the KILT BenchmarkIn Table 7, we present the results of various decoding strategies applied to thevalidation setof the KILT benchmark. Given that most tasks within the KILT benchmark are much easier for strong dense retrievers compared to multi-hop QA, the disparity in performance across different decoding strategies is less pronounced. This observation underscores the necessity of developing a system capable of adaptively selecting the optimal decoding strategy to effectively balance the trade-off between performance and test-time compute. 4 8 16 Rejection Sampling Size 66 68 70 72 74 76EM 2WikiMultihopQA L=6, greedy 4 8 16 Rejection Sampling Size 50 52 54 56 58 60 HotpotQA L=6, greedy 4 8 16 Rejection Sampling Size 40 42 44 46 48 50 52 Bamboogle L=6, greedy 4 8 16 Rejection Sampling Size 22 24 26 28 30 32 MuSiQue L=6, greedy Figure 6: Scaling rejection sampling compute for training data generation. We vary the number of sampled chains from4to16while maintaining all other hyperparameters fixed. Scaling Compute for Training Data GenerationWithin our proposed framework, rather than investing more compute at test time, we can scale the compute for retrieval chain generation during rejection sampling. By increasing the number of sampled chains, we may identify better chains that contribute to higher-quality training data. However, as illustrated in Figure 6, no definitive trend emerges indicating that increasing the number of sampled chains always leads to better performance. Conversely, the training loss consistently decreases as we scale up rejection sampling, suggesting that the training data becomes less noisy and easier to fit. We hypothesize that the majority of sampled 16 2 4 6 8 10 Chain Length 30 32 34 36 38 40 42 44 46EM 2WikiMultihopQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 34 36 38 40 42 HotpotQA greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 25 30 35 40 45 Bamboogle greedy best-of-4 best-of-8 2 4 6 8 10 Chain Length 8 10 12 14 16 18 MuSiQue greedy best-of-4 best-of-8 4k 8k 16k 32k 64k 128k # Avg. T okens 30.0 32.5 35.0 37.5 40.0 42.5 45.0 47.5EM greedy best-of-4 best-of-8 pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 34 36 38 40 42 greedy best-of-4 best-of-8 pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 25 30 35 40 45 50 greedy best-of-4 best-of-8 pareto frontier 4k 8k 16k 32k 64k 128k # Avg. T okens 8 10 12 14 16 18 20 greedy best-of-4 best-of-8 pareto frontier Figure 7: Scaling test-time compute on multi-hop QA datasets withLlama-3.1-8B-Instruct. No fine-tuning is performed on the model weights. chains are already of high quality and that LM fine-tuning exhibits considerable robustness to noisy training data. Scaling Test-Time Compute without Model Fine-TuningIn Figure 7, we present the scaling results on multi-hop QA datasets using theLlama-3.1-8B-Instructmodel directly without any fine- tuning. The scaling curves are similar to those observed in Figure 3, but the absolute performance is significantly lower, indicating that targeted fine-tuning is essential for improving the scaling upper bound. 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 70.0 70.5 71.0 71.5 72.0 72.5EM 2WikiMultihopQA L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 54.0 54.5 55.0 55.5 56.0 56.5 HotpotQA L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 46 48 50 52 Bamboogle L=6, best-of-4 0.2 0.5 0.7 1.0 1.2 1.5 T emperature 26.5 27.0 27.5 28.0 28.5 29.0 29.5 30.0 MuSiQue L=6, best-of-4 Figure 8: Effects of varying the sampling temperature on multi-hop QA datasets. Effects of Sampling TemperatureIn best-of- N sampling, the sampling temperature controls the diversity and quality trade-off in the generated retrieval chains. A higher temperature results in more diverse chains, albeit with the potential introduction of increased noise. Figure 8 illustrates the lack of a consistent conclusion regarding the impact of sampling temperature on performance. For the MuSiQue and HotpotQA datasets, a lower temperature generally yields superior results, whereas for the 2WikiMultihopQA dataset, a medium temperature leads to the best performance. As a result, we stick to a temperature of0.7for both rejection sampling and test-time decoding for simplicity. Extension to Other Model FamiliesTo demonstrate that our CoRAG framework is agnostic to model families and not limited to Llama-based architectures, we conduct experiments using Qwen3-4B and Qwen3-8B [38] models following the same training procedure. As shown in Table 9, CoRAG consistently outperforms the baseline fine-tuned models across all datasets and model sizes, with improvements of over 10 EM points on average. This validates that the chain-of-retrieval mechanism 17 Table 9: Extension to the Qwen3 model families. 2WikiQA HotpotQA Bamboogle MuSiQue EM F1 EM F1 EM F1 EM F1 Fine-tuned Qwen3-4B w/ E5large 49.3 55.3 45.0 57.9 32.8 43.1 13.4 23.8 CoRAG-Qwen3-4B (L=6, greedy)69.3 74.1 51.6 64.2 49.6 62.5 24.0 34.5 Fine-tuned Qwen3-8B w/ E5large 52.1 57.9 47.1 60.0 33.6 47.6 15.3 26.3 CoRAG-Qwen3-8B (L=6, greedy)70.0 74.8 52.8 66.0 49.6 63.7 25.2 35.9 is broadly applicable across different model architectures and confirms the generalizability of our approach beyond specific model families. Case AnalysisTable 11 presents several model predictions on the validation set of the HotpotQA dataset. We compare the performance of RAG without chain-of-retrieval against CoRAG. CoRAG effectively decompose the complex multi-hop queries into a sequences of simpler sub-queries and dynamically conducts query reformulation when the retrieved information proves unhelpful. In the fourth example, the model initially hallucinates some incorrect information but subsequently self-corrects by verifying the poet’s name and country of origin through additional retrieval steps. C Statistical Significance We compute the 95% confidence intervals for our main results in Table 1 and 2 using the bootstrap resampling method. On all datasets except the Bamboogle dataset, we observe that the performance of CoRAG-8B is significantly better than the baselines. D Prompts Table 10: Task descriptions for each dataset. Dataset Task Description HotpotQA / 2WikiMulti- hopQA answer multi-hop questions NQ answer natural questions from Google search AidaYago 2 / WnWi / WnCw / Blink link the mention surrounded by [START_ENT] and [END_ENT] to the title of the correct Wikipedia page FEVER verify if the claim is supported or refuted T-REx / Zero-Shot RE given head entity and relation separated by [SEP], find the correct tail entity, return the title of its Wikipedia page Trivia QA answer trivia questions MuSiQue / Bamboogle answer multi-hop questions All prompts are listed in this section. Texts highlighted in blue denote placeholders to be filled in. The “task description” field is dataset-specific, with corresponding values detailed in Table 10. 18 Table 11: Examples from the validation set of the HotpotQA dataset. For conciseness, all retrieved documents at each step are omitted. Correct answers are highlighted in blue, while incorrect answers are highlighted in red. Query:What wrestling team is Mathew Thomas Rehwoldt a part of? RAG without Chain-of-Retrieval Final Answer:WWE✗ CoRAG (Ours) Sub-query 1: What is Mathew Thomas Rehwoldt’s profession? Sub-answer 1: No relevant information found. Sub-query 2: What is Mathew Thomas Rehwoldt’s name in the wrestling industry? Sub-answer 2: Aiden English Sub-query 3: What wrestling team is Aiden English a part of? Sub-answer 3: The Vaudevillains Final Answer:The Vaudevillains✓ Query:How many months apart are Johan Mjällby and Neil Lennon in age? RAG without Chain-of-Retrieval Final Answer:two months✗ CoRAG (Ours) Sub-query 1: What is Johan Mjällby’s birthdate? Sub-answer 1: 9 February 1971 Sub-query 2: What is Neil Lennon’s birthdate? Sub-answer 2: 25 June 1971 Sub-query 3: What is the difference in months between 9 February 1971 and 25 June 1971? Sub-answer 3: 4 months Final Answer:4✓ Query:Thomas Parker Sanborn was a model for the protagonist in the only novel by a poet originally from what country? RAG without Chain-of-Retrieval Final Answer:Italian✗ CoRAG (Ours) Sub-query 1: What country is the poet originally from? Sub-answer 1: Hungary, Romania, Switzerland, China, Ukraine, France, Turkey. Sub-query 2: What is the name of the poet who wrote the only novel? Sub-answer 2: Emily Brontë Sub-query 3: What country is the poet who wrote the only novel originally from? Sub-answer 3: United States Sub-query 4: What is the name of the poet who wrote the only novel that Thomas Parker Sanborn was a model for? Sub-answer 4: George Santayana Sub-query 5: What poet wrote the only novel that Thomas Parker Sanborn was a model for? Sub-answer 5: George Santayana Sub-query 6: What country is the poet George Santayana originally from? Sub-answer 6: Spain. Final Answer:Spain✓ Query:Which university is further North, University of New Hampshire or Stony Brook University? RAG without Chain-of-Retrieval Final Answer:University of New Hampshire✗ CoRAG (Ours) Sub-query 1: What is the location of the University of New Hampshire? Sub-answer 1: Durham, New Hampshire. Sub-query 2: What is the location of Stony Brook University? Sub-answer 2: Stony Brook, NY . Sub-query 3: What is the latitude of Durham, New Hampshire? Sub-answer 3: No relevant information found. Sub-query 4: What is the latitude of Stony Brook, NY? Sub-answer 4: No relevant information found. Final Answer:University of New Hampshire✗ 19 Prompt: Sub-query Generation You are using a search engine to answer the main query by iteratively searching the web. Given the following intermediate queries and answers, generate a new simple follow-up question that can help answer the main query. You may rephrase or decompose the main query when previous answers are not helpful. Ask simple follow-up questions only as the search engine may not understand complex questions. ## Previous intermediate queries and answers {intermediate queries and answers} ## Task description {task description} ## Main query to answer {query} Respond with a simple follow-up question that will help answer the main query, do not explain yourself or output anything else. Prompt: Intermediate Answer Generation Given the following documents, generate an appropriate answer for the query. DO NOT hallucinate any information, only use the provided documents to generate the answer. Respond “No relevant information found” if the documents do not contain useful information. ## Documents {retrieved documents} ## Query {sub-query} Respond with a concise answer only, do not explain yourself or output anything else. Prompt: Final Answer Generation Given the following intermediate queries and answers, generate a final answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate. ## Documents {retrieved documents} ## Intermediate queries and answers {intermediate queries and answers} ## Task description {task description} ## Main query {query} Respond with an appropriate answer only, do not explain yourself or output anything else. 20 Prompt: Learning to Stop Given the following intermediate queries and answers, judge whether you have enough information to answer the main query. If you believe you have enough information, respond with “Yes”, otherwise respond with “No”. ## Intermediate queries and answers {intermediate queries and answers} ## Main query {query} Respond with “Yes” or “No” only, do not explain yourself or output anything else. 21 NeurIPS Paper Checklist 1.Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope. Guidelines: • The answer NA means that the abstract and introduction do not include the claims made in the paper. • The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. • The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. • It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2.Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in Section 7. Guidelines: • The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate "Limitations" section in their paper. • The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren’t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3.Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] 22 Justification: No theoretical results are provided in the paper. Guidelines: • The answer NA means that the paper does not include theoretical results. • All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. • All assumptions should be clearly stated or referenced in the statement of any theorems. • The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced. 4.Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Implementation details are provided in Section A. Guidelines: • The answer NA means that the paper does not include experiments. • If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submis- sions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5.Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? 23 Answer: [Yes] Justification: We include implementation details in the paper and will release the code and data after publication. Guidelines: • The answer NA means that paper does not include experiments requiring code. • Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. • While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). • The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. • The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. • At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). • Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6.Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 4.1 and A. Guidelines: • The answer NA means that the paper does not include experiments. • The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. • The full details can be provided either with the code, in appendix, or as supplemental material. 7.Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report confidence intervals in Section C. Guidelines: • The answer NA means that the paper does not include experiments. • The authors should answer "Yes" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. • The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). • The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors). 24 • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8.Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details on compute resources in Section A. Guidelines: • The answer NA means that the paper does not include experiments. • The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into the paper). 9.Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and our work conforms to it. Guidelines: • The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. • If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10.Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please see Section 7. Guidelines: • The answer NA means that there is no societal impact of the work performed. • If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. 25 • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11.Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No safeguards are needed for our work as it does not involve high-risk data or models. Guidelines: • The answer NA means that the paper poses no such risks. • Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12.Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All assets used in this paper are properly credited and the licenses are respected. Guidelines: • The answer NA means that the paper does not use existing assets. • The authors should cite the original paper that produced the code package or dataset. • The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset. • For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. 26 • If this information is not available online, the authors are encouraged to reach out to the asset’s creators. 13.New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The documentation is provided in the supplemental material. Guidelines: • The answer NA means that the paper does not release new assets. • Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14.Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing or human subjects were involved in this research. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Including this information in the supplemental material is fine, but if the main contribu- tion of the paper involves human subjects, then as much detail as possible should be included in the main paper. • According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human subjects were involved in this research. Guidelines: • The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. • Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16.Declaration of LLM usage 27 Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: The core method development in this research does not involve LLMs as any important, original, or non-standard components. Guidelines: • The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. • Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM) for what should or should not be described. 28