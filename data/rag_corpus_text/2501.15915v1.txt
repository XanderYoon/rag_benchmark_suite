Parametric Retrieval Augmented Generation Weihang Su swh22@mails.tsinghua.edu.cn DCST, Tsinghua University Beijing 100084, China Yichen Tangâˆ— DCST, Tsinghua University Beijing 100084, China Qingyao Aiâ€  aiqy@tsinghua.edu.cn DCST, Tsinghua University Beijing 100084, China Junxi Yan DCST, Tsinghua University Beijing 100084, China Changyue Wang DCST, Tsinghua University Beijing 100084, China Hongning Wang DCST, Tsinghua University Beijing 100084, China Ziyi Ye DCST, Tsinghua University Beijing 100084, China Yujia Zhou DCST, Tsinghua University Beijing 100084, China Yiqun Liu DCST, Tsinghua University Beijing 100084, China Abstract Retrieval-augmented generation (RAG) techniques have emerged as a promising solution to enhance the reliability of large language models (LLMs) by addressing issues like hallucinations, outdated knowledge, and domain adaptation. In particular, existing RAG methods append relevant documents retrieved from external corpus or databases to the input of LLMs to guide their generation process, which we refer to as the in-context knowledge injection method. While this approach is simple and often effective, it has inherent limitations. Firstly, increasing the context length and number of relevant documents can lead to higher computational overhead and degraded performance, especially in complex reasoning tasks. More importantly, in-context knowledge injection operates primarily at the input level, but LLMs store their internal knowledge in their pa- rameters. This gap fundamentally limits the capacity of in-context methods. To this end, we introduce Parametric retrieval-augmented generation (Parametric RAG), a new RAG paradigm that integrates external knowledge directly into the parameters of feed-forward networks (FFN) of an LLM through document parameterization. This approach not only saves online computational costs by elimi- nating the need to inject multiple documents into the LLMsâ€™ input context, but also deepens the integration of external knowledge into the parametric knowledge space of the LLM. Experimental results demonstrate that Parametric RAG substantially enhances both the effectiveness and efficiency of knowledge augmentation in LLMs. Also, it can be combined with in-context RAG methods to achieve even better performance1. Keywords Large Language Model, Retrieval Augmented Generation, Knowl- edge Representation, Parametric Information Representation 1 Introduction Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of information retrieval (IR) and âˆ—Contributed equally â€  Corresponding author 1We have open-sourced all the code, data, and models in the following anonymized GitHub link: https://github.com/oneal2000/PRAG natural language processing (NLP) tasks [5, 6, 11, 36, 48, 55]. De- spite these successes, a critical limitation remains: once training is complete, an LLMâ€™s internal knowledge becomes effectively static, making it challenging to incorporate newly emerging information or knowledge not included in its pre-training data. To address this challenge, retrieval-augmented generation (RAG) has emerged as a prominent solution. RAG enables LLMs to dynamically access and utilize information beyond their pre-trained parameters by retriev- ing relevant information from an external corpus, thus improving their adaptability and performance [4, 12, 16, 18, 23, 44â€“46]. Existing studies have explored various aspects of the RAG pipeline, considering factors such as retrieval timing [2, 18, 44, 45], document selection [21, 58], and external knowledge organization [10, 15, 32]. While these innovations improve different stages of the pipeline, all RAG methods, regardless of their variations, share a common characteristic: they inject external knowledge by directly adding passages or documents into the input context of LLMs, which we refer to as the in-context knowledge injection. Although this in-context knowledge injection approach is straight- forward and often effective, recent studies have highlighted several limitations of this paradigm. First, injecting knowledge through input prompts will inevitably increase the context length. Long context not only introduces extra computational overhead and la- tency for LLM inference, but also hurts the performance of LLMs in understanding and utilizing external knowledge, especially in tasks that involve complex reasoning [22, 26]. Second, more importantly, the way LLMs process information in context is fundamentally different from the way they utilize internal knowledge stored in their parameters. Studies have shown that LLMs store most of their knowledge within the parameters of their neural network architecture (e.g., the parameters of their feed-forward network layers) [31, 59]. Adding passages or documents in the input context could only affect the online computation of key-value (KV) pairs in the attention networks of LLMs, but not the modelâ€™s stored param- eters, where its knowledge is encoded [59]. This means that LLMs may never be able to utilize external knowledge as effectively as they use their internal knowledge in in-context RAG methods. A straightforward solution to this problem is to conduct supervised fine-tuning (SFT) with retrieved documents, thereby incorporating relevant knowledge directly into the LLMâ€™s parameters. However, arXiv:2501.15915v1 [cs.CL] 27 Jan 2025 Conference, Under Review, Su, et al. PromptTemplate MergetheParameters MergedDocumentRepresentation {RetrievedDocuments}-----------------------------------------------------------------------AnswerthefollowingQuestionbasedontheprovidedinformation:Question:{Question} LLMWeight:ğœ½ ğœ½!=ğœ½+âˆ†ğœ½ğœ½âˆ†ğœ½=ğ’‡(ğ’Œ,ğœ½) LLMWeight:ğœ½!LLMWeight:ğœ½ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â· Â·Â·Â·Â·Â·Â·Tokenize TraditionalRAG: InjectRetrievedDocumentstotheInputContext OriginalLLM OriginalLLM Question FillinthePromptTe m p l a te ParametricRAG:InjectRetrievedDocumentstotheLLMâ€™sParameter Response Question UserInput Retrieved Documents Corresponding Parametric Representations of Documents Response Figure 1: An illustration of the comparison of in-context RAG and our proposed Parametric RAG paradigms: In-context RAG combines the tokens of relevant documents and the query in the input, using the original LLM ğœƒ to answer the question without modifying its parameters. Our proposed Parametric RAG updates the LLMâ€™s parameters ğœƒ â€² = ğœƒ + Î”ğœƒ based on the retrieved documents, temporarily integrating relevant knowledge into LLMâ€™s parameters to answer the question. such SFT-based methods are considered suboptimal as they require substantial computational resources and GPU memory, making it impractical to inject relevant documents online for every query. In addition, they can negatively affect the original ability of the LLM to follow instructions [7, 54] and lack the flexibility of in-context methods, which allow external knowledge to be added or removed on the fly. The observations above inspire us to raise the following research question: Is it possible to inject external knowledge into LLM pa- rameters effectively, efficiently, and flexibly for retrieval-augmented generation? To this end, we introduce a new RAG paradigm, Parametric Retrieval Augmented Generation (Parametric RAG), which directly injects the external knowledge into the Feed-Forward Net- works (FFN) of an LLM. Specifically, our approach begins with an offline preprocessing phase that parameterizes each document from the external corpus, transforming them into a small set of parameters (usually a few MB per document) that can be directly integrated into the downstream LLM. We refer to this set of pa- rameters as the parametric representation of the document. In the inference phase, we conduct retrieval augmented generation fol- lowing a Retrieve-Update-Generate (RUG) workflow as shown in Figure 1. The Retrieve step retrieves top-n documents from the external corpus based on the input prompt following the same pro- cedure used by the existing RAG pipeline. Then, the Update step uses the parametric representations of the retrieved documents to update the LLM. Finally, in the Generate step, we use the updated LLMs to conduct inference directly based on the original input prompt. Theoretical and empirical analysis show that our Parametric RAG method has superior inference efficiency and outperforms state-of- the-art in-context methods on several RAG benchmarks that involve tasks with complex reasoning. While the preprocessing phase of Parametric RAG introduces an offline computational overhead, this cost is affordable and even neglectable compared to the online cost of large-scale inference requests, leading to long-term savings in terms of power and carbon footprints. Also, similar to in-context RAG, our method can adapt to various numbers of input documents on the fly. Furthermore, our proposed Parametric RAG pipeline is in parallel with existing in-context methods. As shown in our experiments, combining our methods with in-context RAG could produce even better performance on the benchmark datasets. This indicates that parametric knowledge injection could be a fruitful direction for the future development of the RAG system. In summary, this paper makes the following key contributions: â€¢ We propose Parametric RAG, a new RAG paradigm that integrates external knowledge directly into LLMâ€™s parameters. â€¢ We propose an offline method to build parametric document rep- resentation and a Retrieve-Update-Generate pipeline to conduct Parametric RAG on the fly. â€¢ We conduct extensive experiments to compare the state-of-the- art in-context RAG methods with our method to demonstrate the potential of Parametric RAG in terms of effectiveness and efficiency. 2 Related Work Large language models have shown remarkable performance across diverse applications. However, their inherent knowledge often proves insufficient for tackling knowledge-intensive tasks, under- scoring the necessity of integrating external knowledge for robust performance in these contexts. One prominent approach to address this gap is Retrieval-Augmented Generation (RAG), which improves LLMs by integrating relevant external knowledge sources [4, 8, 12, 16, 18, 23, 37, 42, 50, 51]. In the traditional RAG framework, an external retriever [9, 27, 34, 40, 41, 43, 60] or a more complex re- trieval system [ 24, 35] retrieves relevant documents based on a query. These documents are then appended to the LLMâ€™s input con- text, allowing the LLM to leverage knowledge beyond its training data [23]. Parametric Retrieval Augmented Generation Conference, Under Review, Building upon the traditional RAG framework, several exten- sions have been proposed to improve its effectiveness and efficiency. One such extension, Adaptive RAG [17, 52], introduces adaptive retrieval strategies that actively adjust the retrieval pipeline based on the complexity of the query to improve the adaptability of RAG in different tasks. From another angle, to make in-context knowl- edge injection more effective in RAG scenarios, IR-CoT [49] designs prompt templates specifically tailored for RAG that demonstrate how to perform chain-of-thought (CoT) reasoning based on the given passage. Each sentence in the CoT reasoning content is then applied to retrieve more relevant documents. Another research direction, GraphRAG [10, 15, 32], employs pre-constructed knowl- edge graphs to retrieve graph elements that encapsulate relational knowledge relevant to the query. GraphRAG has demonstrated enhanced performance, particularly in tasks that require struc- tured, relational information. In the context of long-form genera- tion, where the LLMâ€™s informational needs may change during the generation process, dynamic RAG techniques have been developed to actively decide when and what to retrieve during the generation process [3, 19, 25, 44, 45, 57]. For example, FLARE [19] triggers the retrieval module when the modelâ€™s token prediction probability falls below a predefined threshold. Similarly, DRAGIN [45] models the real-time information needs of the LLM, generating queries based on the modelâ€™s internal state and preceding context to fetch relevant external knowledge dynamically. To summarize, existing RAG approaches have explored vari- ous aspects of the RAG pipeline, considering factors such as re- trieval timing [3, 19, 25, 44, 45, 57], prompt template for in-context knowledge injection [49, 53], document selection [58], and external knowledge organization[10, 15, 32]. While these innovations im- prove different stages of the pipeline, all RAG methods, regardless of their variations, share a common characteristic at the knowledge injection level: relevant passages or documents are appended di- rectly to the LLMâ€™s input context to inject external knowledge. In contrast, our proposed Parametric RAG paradigm diverges from all the existing RAG frameworks by directly injecting documents into the LLMâ€™s parameters. This shift in knowledge integration addresses the inherent limitations of the in-context knowledge injection methods employed in all existing RAG frameworks. 3 Methodology In this section, we introduce our proposed Parametric RAG frame- work, shown in Figure 1. This section begins by formulating the problem and providing an overview of the Parametric RAG frame- work (Â§3.1). Next, we introduce the Offline Document Parameteri- zation process (Â§3.2), which transforms documents into parametric representations through Document Augmentation and Para- metric Document Encoding . Finally, we introduce the Online Inference procedure (Â§3.3), where the parametric representations are retrieved, merged, and integrated into the LLM to generate responses. 3.1 Problem Formulation and Overview This subsection introduces the problem formulation of the RAG task and provides an overview of our proposed Parametric RAG pipeline. Consider an LLM (denoted as L) with base parameters ğœƒ. Given a user query ğ‘, we aim to generate an accurate response using an external corpus ğ¾. Formally, the corpus ğ¾ is defined as: ğ¾ = {ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘ }, where each ğ‘‘ğ‘– represents a text chunk, such as documents, Wikipedia articles, or passages (for convenience, we refer to each ğ‘‘ğ‘– as â€˜documentâ€™ in the following sections). The system contains a retrieval module ğ‘… that calculates the relevance score of each document {ğ‘†ğ‘‘1, ğ‘†ğ‘‘2, . . . , ğ‘†ğ‘‘ğ‘ } corresponding to the query ğ‘. Traditional RAG paradigms select the top ğ‘˜ documents with the highest relevance scores as relevant external knowledge and append them to the input context of the L. This process is typically guided by a prompt template that instructs L to generate the response based on the provided knowledge. In contrast to the in-context RAG paradigm that injects relevant documents into the LLMâ€™s input context, in Parametric RAG, we propose to insert documents directly into the parameters of L. To achieve this, the Parametric RAG framework is designed with two stages: an offline document parameterization stage and an online inference stage with a Retrieve-Update-Generate workflow. Offline document Parameterization. In this step (illustrated in Figure 2), we offline transform each document inğ¾ into a parametric representation, thereby forming a set of parameters known as the Parametric Corpus ğ¾ğ‘ƒ . Specifically, we define: ğ¾ğ‘ƒ = {ğ‘ğ‘– | ğ‘ğ‘– = ğ‘“ğœ™ (ğ‘‘ğ‘– ), ğ‘– = 1, 2, . . . , ğ‘}, (1) where ğ‘“ğœ™ is a mapping function that converts each document ğ‘‘ğ‘– into its corresponding parametric representation ğ‘ğ‘–. We define parametric representations ğ‘ğ‘– to possess the following properties: (1) The parameters ğ‘ğ‘– can be plugged into the feed-forward net- work weights of the LLM. (2) After inserting the parametric representationğ‘ğ‘– into ğ¿, the LLM can effectively comprehend the knowledge contained within the corresponding document ğ‘‘ğ‘–. (3) Different document parameters ğ‘ğ‘– can be merged through spe- cific algorithms, and after merging, the LLM can grasp the combined knowledge corresponding to the merged documents. Online Inference. During the online inference process, our method first merges the parametric representations corresponding to the retrieved top-k documents and then plugs the merged parameters into the LLM. Subsequently, the updated LLM is used to answer the userâ€™s question. This overall framework allows for more efficient and effective knowledge injection, overcoming the limitations of traditional RAG by leveraging parameterized representations of external knowledge. 3.2 Offline Document Parameterization In this subsection, we describe the detailed process of offline param- eterizing each document in the corpus ğ¾ during the pre-processing phase. Given a document ğ‘‘ğ‘– and an LLM L, our objective is to construct a parametric representation ğ‘ğ‘– of the document. This representation enables L to effectively comprehend and utilize the knowledge contained in ğ‘‘ğ‘– during inference. To achieve this, we propose a two-step approach: Document Augmentation and Parametric Document Encoding . These steps are combined to generate robust and informative parametric representations for each document. Conference, Under Review, Su, et al. LLMWeight:ğœ½ Input Fine-tuningUsingğ‘«ğ’Š Random LoRAWeightParametricRepresentationofğ’…ğ’Š ğ’…ğ’Š ğ’‘ğ’Š ParametricDocumentEncoding Rewrite ğ’’ğŸğ’‚ğŸğ’’ğŸğ’‚ğŸğ’’ğ’ğ’‚ğ’ LLMWeight:ğœ½ â€¦ğ’’ğŸ, ğ’‚ğŸ GenerateQAPairs â€¦ ğ’’ğ’ ,ğ’‚ğ’â€¦ ğ€ğ®ğ ğ¦ğğ§ğ­ğğ ğƒğ¨ğœğ®ğ¦ğğ§ğ­ ğ‘«ğ’Š DocumentAugmentation Figure 2: An illustration of how we parameterize each document ğ‘‘ğ‘– in the corpus during the Offline Document Parameterization stage. 3.2.1 Document Augmentation. Existing studies have shown that effectively incorporating factual knowledge from external docu- ments into LLMs requires more than simply pre-training the model on raw text via standard next-token prediction. For example, Allen- Zhu and Li [1] find that after being trained repeatedly on a given document, LLMs can memorize its content but fail to extract and apply this knowledge effectively in downstream tasks such as ques- tion answering. To address this issue, Allen-Zhu and Li [1] propose two key strategies: (1) incorporating question-answer (QA) pairs derived from the document during training and (2) augmenting the document through multiple rewrites that express the same factual content in different forms. Their findings indicate that these two approaches enable LLMs to internalize knowledge to support accu- rate application in downstream tasks rather than reproducing the original text token-by-token2. Building upon the insights discussed above, we introduce the doc- ument augmentation process consisting of two steps: Document Rewriting and QA Pair Generation, to construct robust and in- formative parametric representations for documents. Specifically, for each document, we prompt3 the LLM L to rewrite the content multiple times using different wording, styles, or organizational structures. Formally, each documentğ‘‘ğ‘– is transformed into multiple rewritten documents {ğ‘‘ğ‘– 1, ğ‘‘ğ‘– 2, . . . , ğ‘‘ğ‘–ğ‘› }, which preserve the orig- inal facts but vary in language expression. Once each document has been rewritten into multiple documents, we prompt L again to generate question-answer (QA) pairs based on the original docu- ment ğ‘‘ğ‘–. For each document ğ‘‘ğ‘–, L produces a set of questions and their corresponding answers: {(ğ‘ğ‘– 1, ğ‘ğ‘– 1), (ğ‘ğ‘– 2, ğ‘ğ‘– 2), . . . , (ğ‘ğ‘–ğ‘š, ğ‘ğ‘–ğ‘š)}, where ğ‘š is a tunable hyperparameter representing the number of QA pairs we aim to generate per document. Integrating multiple rewrites with corresponding QA pairs transforms each documentğ‘‘ğ‘– into a more comprehensive resource ğ·ğ‘– that preserves its original factual content while incorporating diverse linguistic variations. Formally: ğ·ğ‘– = {(ğ‘‘ğ‘–ğ‘˜, ğ‘ğ‘– ğ‘— , ğ‘ğ‘– ğ‘— ) | 1 â‰¤ ğ‘˜ â‰¤ ğ‘›, 1 â‰¤ ğ‘— â‰¤ ğ‘š}, (2) where each (ğ‘‘ğ‘–ğ‘˜, ğ‘ğ‘– ğ‘— , ğ‘ğ‘– ğ‘— ) triple corresponds to a rewritten docu- ment ğ‘‘ğ‘–ğ‘˜ from the original document ğ‘‘ğ‘–, coupled with a question ğ‘ğ‘– ğ‘— and its respective answer ğ‘ğ‘– ğ‘— . 2Our experimental results corroborate these conclusions, as shown in Figure 3. 3Due to space constraints, we have not included the specific prompts in the main text. However, all prompts used in this study are available at the following anonymous link: https://github.com/oneal2000/PRAG/blob/main/all_prompt.md 3.2.2 Parametric Document Encoding. In this subsection, we in- troduce the second step of the offline document parameterization pipeline, where we leverage the augmented dataset ğ·ğ‘– (defined in Eq. 2) to train the parametric representation ğ‘ğ‘– for each docu- ment ğ‘‘ğ‘–. Specifically, we initialize these parametric representations as low-rank matrices corresponding to the feed-forward network (FFN) parameter matrix ğ‘Š of the LLM L, following the LoRA ap- proach [14]. This design allows each document ğ‘‘ğ‘– to be associ- ated with independently trained low-rank parameters, allowing the model to internalize the knowledge specific to ğ‘‘ğ‘– in a parameter- efficient manner.4 Suppose the Transformer layers in L have a hidden dimension â„, and the feed-forward network (FFN) within each layer has an intermediate dimension ğ‘˜. Consequently, each FFN layer of L con- tains a weight matrixğ‘Š âˆˆ Râ„Ã—ğ‘˜. To incorporate document-specific knowledge, we introduce low-rank matrices ğ´ and ğµ such that ğ‘Š â€² = ğ‘Š + Î”ğ‘Š = ğ‘Š + ğ´ ğµâŠ¤, (3) where ğ´ âˆˆ Râ„Ã—ğ‘Ÿ and ğµ âˆˆ Rğ‘˜ Ã—ğ‘Ÿ , with ğ‘Ÿ â‰ª min(â„, ğ‘˜). The original weight matrix ğ‘Š is kept fixed, while ğ´ and ğµ are the only trainable parameters for that layer. We denote these newly introduced param- eters as Î”ğœƒ = {ğ´, ğµ}. By combining the pre-trained weights ğ‘Š and Î”ğœƒ, the model obtains the knowledge from the selected document. Each document in the corpus is associated with its instance of Î”ğœƒ. For each document ğ‘‘ğ‘–, we train its corresponding parametric representation Î”ğœƒ using its corresponding augmented dataset ğ·ğ‘–. Recall from Eq. 2 that ğ·ğ‘– contains triplets ğ‘‘ğ‘–ğ‘˜, ğ‘ğ‘– ğ‘— , ğ‘ğ‘– ğ‘—. For each triplet, we concatenate ğ‘‘ğ‘–ğ‘˜, ğ‘ğ‘– ğ‘— , and ğ‘ğ‘– ğ‘— into a token sequence: ğ‘¥ = [ ğ‘‘ğ‘–ğ‘˜ âŠ• ğ‘ğ‘– ğ‘— âŠ• ğ‘ğ‘– ğ‘— ], (4) where [ Â· âŠ• Â· ] indicates concatenation. Let ğ‘‡ be the total number of tokens in ğ‘¥. We adopt a standard sequential language modeling objective to ensure that the LLM internalizes knowledge from the entire augmented text (i.e., both the documents and QA pairs). Specifically, we optimize: min Î”ğœƒ âˆ‘ï¸ (ğ‘‘ğ‘– ğ‘˜ ,ğ‘ğ‘– ğ‘— ,ğ‘ğ‘– ğ‘— ) âˆˆğ·ğ‘– ğ‘‡âˆ‘ï¸ ğ‘¡ =1 âˆ’ log ğ‘ƒğœƒ +Î”ğœƒ ğ‘¥ğ‘¡ ğ‘¥<ğ‘¡ , (5) 4Other parameter-efficient methods (e.g., Adapters or Prefix-Tuning) could also be used; exploring them is left for future work. In this work, we chose LoRA because it offers practical advantages over other alternatives. For example, LoRA is easier to merge compared to Adapters, and it requires less computational overhead during inference compared to Prefix-Tuning. Parametric Retrieval Augmented Generation Conference, Under Review, where ğœƒ are the frozen pretrained parameters of the LLM, and Î”ğœƒ = {ğ´, ğµ} are the trainable low-rank matrices introduced in Eq. 3. The innermost summation is taken over all tokens ğ‘¥ğ‘¡ in the con- catenated input sequence (document, question, and answer)5. This design inherently encourages the LLM to internalize the factual details in the documents into its parameters during training. Al- though the generated question-answer pairs do not directly cover all the facts within the document, repeated training on the docu- mentsâ€™ tokens allows the model to reinforce its understanding of the textual content. Consequently, once the training is complete, the parametric representation Î”ğœƒ serves as a lightweight document- specific knowledge representation that can be directly added to the original model L at inference time. Notably, this entire process can be conducted offline, as each document (or batch of documents) is processed to produce its re- spective low-rank representation Î”ğœƒ. At inference time, we only need to load the LoRA parameters corresponding to the specific document(s) rather than appending the document directly into the LLMâ€™s context. The computational cost of loading these parametric representations constitutes only a minimal portion, approximately 1% of the computation required to decode a single token. 3.2.3 Discussion on LoRA Initialization. In our proposed training framework, the LoRA parameters for each document ğ‘‘ğ‘– are initial- ized randomly without any warm-up stage. This choice is delib- erate and aligns with our goal of developing a general-purpose parametric knowledge injection method rather than one tailored to specific downstream tasks. If not explicitly mentioned other- wise, we initialize LoRA with random values. However, randomized LoRA initialization is not necessarily the most effective way to train parametric document representations. For example, we could pre-train the random LoRA with a couple of few-shot examples following the same method described with Eq. (5) and save the LoRA weight ğ‘Šğ‘¤ğ‘ğ‘Ÿğ‘š âˆ’ğ‘¢ğ‘ to initialize the training of each docu- mentâ€™s LoRA (i.e., the documentâ€™s parametric representation). Our experiment (Section Â§ 5.2) demonstrates that this warm-up process can significantly improve the performance compared to random initialization on RAG tasks, indicating that a task-aware initializa- tion can further enhance the effectiveness of parametric knowledge injection for specific downstream tasks. Yet, we use random ini- tialization for LoRA if not mentioned explicitly for simplicity and broad applicability across various tasks. 3.3 Online Inference In the previous stage (Â§3.2.2), we generated a set of document- specific low-rank parameters for each document in the corpus ğ¾. This section describes how these parameters are utilized for RAG pipelines. Given a user query ğ‘, our proposed Parametric RAG pipeline proceeds through three steps:Retrieve, Update, and Generate. The following section details each step and illustrates the underlying mathematical operations. 3.3.1 Retrieve. We first use a retrieverğ‘… to calculate a relevance score ğ‘†ğ‘‘ğ‘– for each document ğ‘‘ğ‘– âˆˆ ğ¾ to the query ğ‘. We then select the top-ğ‘˜ documents with the highest relevance scores, denoted 5The loss is computed not only on the answer but also across the entire concatenated sequence, including the documents and the question as {ğ‘‘1, ğ‘‘2, . . . , ğ‘‘ğ‘˜ } âŠ† ğ¾ as the relevant external knowledge. Each retrieved document ğ‘‘ğ‘– has a corresponding parametric representa- tion, i.e., a pair of low-rank matrices ğ´ğ‘–, ğµğ‘– , previously obtained by the procedure described in Â§3.2.2. 3.3.2 Update. After retrieval, we merge the low-rank matrices from the top-ğ‘˜ retrieved documents to form a single plug-in module for the LLM. Following the setting of LoRA [14] convention, we use a scalar scaling factor ğ›¼ to modulate the final update. The merged weight update, Î”ğ‘Šmerge, is computed by summing over all retrieved documents: Î”ğ‘Šmerge = ğ›¼ Â· ğ‘˜âˆ‘ï¸ ğ‘—=1 ğ´ğ‘— ğµâŠ¤ ğ‘— . (6) Intuitively, Î”ğ‘Šmerge combines the knowledge from multiple rele- vant documents into a single low-rank update that can be applied to the LLMâ€™s base parameters. Once we obtainÎ”ğ‘Šmerge, we update the original feed-forward weight ğ‘Š by: ğ‘Š â€² = ğ‘Š + Î”ğ‘Šmerge, thus yielding the final set of parameters for that layer at inference time. Conceptually, ğ‘Š â€² encodes the base modelâ€™s knowledge plus the aggregated knowledge from the top-ğ‘˜ retrieved documents. 3.3.3 Generate. After updating all feed-forward layers in the Trans- former with Î”ğ‘Šmerge, we obtain a temporary model Lâ€² (ğœƒ â€²), where ğœƒ â€² represents the updated model parameters, which are obtained by incorporating the merged low-rank parameters for all retrieved doc- uments. We can then directly use Lâ€² to generate the final response to the query ğ‘ using a standard left-to-right decoding process. 3.4 Discussion on Time/Space Efficiency 3.4.1 Computation Cost. The computation cost of our method can be divided into offline preprocessing cost and online inference cost. The offline cost primarily arises from the Parametric Docu- ment Encoding (Â§3.2.2). Let |ğ‘‘ | be the average number of tokens in a document ğ‘‘, and â„ be the hidden dimension size of the LLM. The computational complexity of a typical decoder-only LLM is O (|ğ‘‘ |2â„ + |ğ‘‘ |â„2), where the attention layers complexity is O (|ğ‘‘ |2â„) plus the FFN layers O (|ğ‘‘ |â„2). Theoretically, our method only in- troduces a constant coefficient change on the number of tokens processed, thus the overall time complexity remainsO (|ğ‘‘ |2â„+|ğ‘‘ |â„2). Based on our implementation settings detailed in Â§ 4.3, the Data Augmentation process takes the original document ğ‘‘ as input and subsequently generates approximately 2|ğ‘‘ | new tokens. This pro- cess requires computational costs equivalent to a forward pass over 3|ğ‘‘ | tokens, including the decoding of 1|ğ‘‘ | tokens and the infer- ence of 2|ğ‘‘ | tokens. Training LoRA parameters on these augmented tokens requires a forward pass over 3|ğ‘‘ | tokens and a backward pass equivalent to processing 6|ğ‘‘ | tokens (typically about twice the forward-pass cost), resulting in an overall computational cost equivalent to processing 9|ğ‘‘ | tokens. Adding the 3|ğ‘‘ | tokens from the Document Augmentation phase, the total computational cost is approximately the cost of decoding 12|ğ‘‘ | tokens in the LLM. The online inference cost mainly depends on the number of in- put and output tokens. For simplicity, we focus on input tokens and ignore the variance in output tokens since they vary signifi- cantly from tasks and LLMs. Let |ğ‘| represent the length of input prompt/question ğ‘, and ğ‘¡ be the number of retrieved documents. Conference, Under Review, Su, et al. Since the time needed to load the LoRA parameters for ğ‘¡ docu- ments is neglectable, the inference time complexity of our method is O (|ğ‘|2â„ + |ğ‘|â„2). In contrast, the time complexity of in-context RAG methods is O (ğ‘¡ |ğ‘‘ | + | ğ‘|) 2â„ + ( ğ‘¡ |ğ‘‘ | + | ğ‘|)â„2, which means that our method can save O ğ‘¡ 2|ğ‘‘ |2â„ + ğ‘¡ |ğ‘‘ ||ğ‘|â„ + ğ‘¡ |ğ‘‘ |â„2 time for online inference. Empirically, suppose that the lengths of ğ‘ and ğ‘‘ are approximately the same and significantly smaller than the hidden dimension of the LLM (e.g., about 4096 for LLaMA-8B), and we retrieve ğ‘¡ = 6 documents for each ğ‘, then our method can roughly save 6|ğ‘‘ | tokens in inference. Compared to its offline cost, this means that Parametric RAG is more cost-friendly than in-context RAG when the number of queries is more than twice that of documents in the life cycle of the service. In summary, while the offline preprocessing step in Parametric RAG introduces additional computational overhead compared to traditional RAG, our analysis demonstrates that, when the system handles a large number of queries, Parametric RAG can provide a more carbon-efficient solution for large-scale RAG systems. 3.4.2 Storage Overhead. In Parametric RAG, storage overhead comes from the Parametric Representation of each document, which consists of low-rank matrices from the FFN layer. Letğ‘Ÿ be the LoRA rank, ğ‘› be the number of Transformer layers, â„ be the hidden size, and ğ‘™ the intermediate size of FFN, then the number of parameters in the Parametric Representation of a document is 2ğ‘›ğ‘Ÿ (â„ + ğ‘™). For example, with the LLaMA3-8B model (32 layers, hidden size 4096, intermediate size 14336), we need to store approximately 2.36M extra parameters (with ğ‘Ÿ = 2 as used in our experiments). Stored at 16-bit precision, this requires around 4.72MB per document. While the storage requirements for Parametric RAG may seem substantial compared to the raw documents, there are multiple methods to reduce its cost in practice. For example, previous studies find that the access of information in real user traffic follows a long- tail distribution [38]. Taking Google as an example, about 96.55% of Web pages receive zero traffic, and only 1.94% get one to ten visits per month [39]. Therefore, creating parametric representations for a tiny set of head documents can serve the majority of user requests, which significantly reduces the storage cost of Parametric RAG. Also, as shown in our experiments, Parametric RAG can be used with in-context RAG together for downstream tasks. Thus, it can serve as a natural boost to existing RAG methods without breaking their system pipelines. 4 Experimental Setup In this section, we detail the experimental framework used to evalu- ate Parametric RAG. We begin with the introduction of our selected benchmark datasets(Â§4.1). Next, we introduce our selected baseline methods (Â§4.2) and implementation. Finally, we provide implemen- tation details regarding our parameterization method, retrieval strategy, and inference settings (Â§4.3). 4.1 Benchmarks and Metrics We evaluate our approach on diverse benchmark datasets, each designed to assess different reasoning capabilities, such as multi- hop reasoning and commonsense inference. Specifically, we select the following datasets: â€¢ 2WikiMultihopQA (2WQA) [13] is a dataset designed to test the modelâ€™s ability to perform multi-hop reasoning by integrating information across multiple Wikipedia passages. â€¢ HotpotQA (HQA) [56] also focuses on evaluating multi-hop reasoning skills, requiring models to combine information from different contexts to address a single query. â€¢ PopQA (PQA) [28] assesses factual question answering, chal- lenging the modelâ€™s ability to recall accurate knowledge and resolve ambiguity in entity representation. â€¢ ComplexWebQuestions (CWQ)[47] involves answering multi- step, web-based questions, further testing the modelâ€™s capacity to retrieve and reason over large-scale web content. For evaluation metrics, we use the F1 score to evaluate performance on question-answering tasks, as it captures the balance between precision and recall by accounting for partially correct answers. Both 2WQA and HQA categorize questions based on reasoning types, with 2WQA divided into four categories and HQA into two. To comprehensively compare the performance of P-RAG and other RAG baselines across different reasoning tasks, our main experi- mental table (Table 1) presents the performance for each sub-task separately, using the first 300 questions from each sub-dataset. Ta- ble 1 also presents the overall performance of each RAG baseline on the two datasets in the â€œTotalâ€ column. Since the original datasets contain uneven distributions of question types, the â€œTotalâ€ column is not a simple average of the sub-dataset performances. 4.2 Baselines We choose the following RAG baselines for comparison: â€¢ Standard RAG. This RAG method directly appends the top re- trieved documents to the LLMâ€™s input prompt. The prompt ex- plicitly instructs the LLM to refer to the provided documents when answering the question and also includes instructions on the output format expected from the model. â€¢ DA-RAG incorporates the augmented documents and QA pairs using the Data Augmentation method introduced in Â§ 3.2.1. This baseline aims to demonstrate that the performance improvement observed in Parametric RAG does not stem from the data aug- mentation phase but from the in-parameter knowledge injection. â€¢ FLARE [19] is a multi-round retrieval augmentation method that triggers retrieval each time it encounters an uncertain to- ken. When the retrieval module is triggered, the last generated sentence without the uncertain tokens is defined as the query. â€¢ DRAGIN [45] is a multi-round retrieval augmentation method. It triggers retrieval when an uncertain token has semantic meaning and also has a strong influence on the following tokens. When the retrieval module is triggered, it formulates the query based on the modelâ€™s internal state and preceding context. â€¢ P-RAG directly injects relevant documents into the LLMâ€™s pa- rameters through document parameterization, enabling efficient RAG without increasing the input context length. â€¢ Combine Both . This baseline combines the in-context RAG method with P-RAG, leveraging both in-context and parametric knowledge injection. This baseline aims to evaluate whether the fusion of these approaches leads to better performance. For P-RAG and all the baselines, we use the same retriever and select the top 3 retrieved documents as relevant. To ensure a fair Parametric Retrieval Augmented Generation Conference, Under Review, Table 1: The overall experiment results of Parametric RAG and other baselines across four tasks. All metrics reported are F1 scores. Bold numbers indicate the best performance of all baselines, and the second-best results are underlined. â€œ*â€ and â€  denote significantly worse performance than the bolded method and our proposed P-RAG with ğ‘ < 0.05 level, respectively. 2WikiMultihopQA HotpotQA PopQA CWQ Compare Bridge Inf. Compose Total Bridge Compare Total LLaMA-1B Standard RAG 0.4298â€ * 0.3032â€ * 0.2263 0.1064 0.2520* 0.2110 0.4083 0.2671 0.1839* 0.3726 DA-RAG 0.3594â€ * 0.2587â€ * 0.2266 0.0869â€ * 0.2531* 0.1716* 0.3713â€ * 0.2221 0.2012* 0.3691 FLARE 0.4013â€ * 0.2589â€ * 0.1960 0.0823â€ * 0.2234* 0.1630* 0.3784â€ * 0.1785* 0.1301â€ * 0.3173* DRAGIN 0.4556 0.3357* 0.1919 0.0901 0.2692* 0.1431* 0.4015 0.1830* 0.1056â€ * 0.3900 P-RAG (Ours) 0.4920 0.3994 0.2185 0.1334 0.2764 0.1602* 0.4493 0.1999* 0.2205* 0.3482* Combine Both 0.5046 0.4595 0.2399 0.1357 0.3237 0.2282 0.4217 0.2689 0.2961 0.4101 Qwen-1.5B Standard RAG 0.3875â€ * 0.3884â€ * 0.1187â€ * 0.0568â€ * 0.2431â€ * 0.1619* 0.3713â€ * 0.2073* 0.0999â€ * 0.2823* DA-RAG 0.3418â€ * 0.4015 0.1269â€ * 0.0514â€ * 0.2156â€ * 0.1182â€ * 0.3041â€ * 0.1683* 0.1197â€ * 0.2718â€ * FLARE 0.1896â€ * 0.1282â€ * 0.0852â€ * 0.0437â€ * 0.1004â€ * 0.0750â€ * 0.1229â€ * 0.0698â€ * 0.0641â€ * 0.1647â€ * DRAGIN 0.2771â€ * 0.1826â€ * 0.1025â€ * 0.0680â€ * 0.1538â€ * 0.0801â€ * 0.1851â€ * 0.0973â€ * 0.0548â€ * 0.1788â€ * P-RAG (Ours) 0.4529 0.4494 0.2072 0.1372 0.3025 0.1720 0.4623 0.2165* 0.1885 0.3280 Combine Both 0.4053 0.4420 0.1705 0.1154 0.2627 0.2383 0.5037 0.2942 0.2261 0.3495 LLaMA-8B Standard RAG 0.5843â€ * 0.4794â€ * 0.1833â€ * 0.0991â€ * 0.3372â€ * 0.1823â€ * 0.3493â€ * 0.2277â€ * 0.1613â€ * 0.3545â€ * DA-RAG 0.4921â€ * 0.3344â€ * 0.1523â€ * 0.0670â€ * 0.2396â€ * 0.1587â€ * 0.2860â€ * 0.1996â€ * 0.2255* 0.3481â€ * FLARE 0.4293â€ * 0.3769â€ * 0.3086 0.1627* 0.3492* 0.2493â€ * 0.4324â€ * 0.2771â€ * 0.2393* 0.3084â€ * DRAGIN 0.5185â€ * 0.4480â€ * 0.2664 0.1833 0.3544* 0.2618* 0.6116* 0.2924* 0.1772â€ * 0.3101â€ * P-RAG (Ours) 0.6353 0.5437 0.2471* 0.1992 0.3932 0.3115* 0.6557 0.3563* 0.2413* 0.4541 Combine Both 0.6432 0.5556 0.3160 0.2339 0.4258 0.4025 0.6918 0.4559 0.3059 0.4728 comparison, we ensured that P-RAG and all the baselines share the same prompt template6 under the same dataset. 4.3 Implementation Details In this subsection, we introduce the specific implementation of our experiments: Base Models We implement Parametric RAG using open-source pre-trained LLMs. To ensure the broad effectiveness of P-RAG across different models, we selected LLMs of varying scales and from different series, including Qwen2.5-1.5B-Instruct [55], LLaMA- 3.2-1B-Instruct [29], and Llama-3-8B-Instruct [30]. All experiments were conducted using PyTorch on NVIDIA A100 GPUs with 40GB of memory. Preprocessing and Parameterization. Consistent with prior works [19, 20, 44, 45], we utilize Wikipedia dumps as our external knowledge corpus, specifically adopting the dataset7 proposed by DPR [20]. For document augmentation, each document is rewritten once, and three QA pairs are generated based on the document 8 (using the downstream LLM, if not mentioned explicitly). In the LoRA fine-tuning process, the learning rate was set to3 Ã— 10âˆ’4, and the training epoch was set to1. The LoRA modules were exclusively integrated into the feed-forward network (FFN) matrices, excluding the query, key, and value (ğ‘„ğ¾ğ‘‰ ) matrices. The scaling factor ğ›¼ was configured to 32, LoRA rank ğ‘Ÿ was set to 2, and no dropout was applied during training to ensure stability and full utilization of 6All the prompt templates used in this paper are available in our GitHub repository: https://github.com/oneal2000/PRAG/blob/main/all_prompt.md 7https://github.com/facebookresearch/DPR/tree/main 8The detailed prompt template for document augmentation is publicly available on our official GitHub repository. the parameter updates. The LoRA weight is randomly initialized following the setting of the original LoRA paper [14]. Retrieval Module. Recent studies on retrieval-augmented gen- eration (RAG) [33] reveal that BM25 performs on par with, or even outperforms, state-of-the-art dense models in some scenarios. Given its strong performance, simplicity, and low computational cost, we adopt BM25 as the retriever for our approach. We use Elasticsearch as the backend for implementing BM25, with detailed configuration settings and instructions available on our official GitHub repository. Generation Configuration. All experiments are conducted us- ing the publicly released Hugging Face implementations of LLaMA and Qwen. We adopt the default hyperparameters and chat tem- plate provided in the official Huggingface repository, with the only modification being the use of greedy decoding to ensure the repro- ducibility of our reported results. 5 Experiments 5.1 Main Experiment In this section, we present the main experimental result and an in-depth analysis of our proposed Parametric RAG compared with other RAG baselines and a combined setting that leverages both parametric and in-context knowledge injection. The experimen- tal results are presented in Table 1, and we provide the following analysis: (1) Overall Analysis. P-RAG outperforms existing RAG frameworks in most of the benchmarks and LLMs evaluated. This trend is especially obvious for Qwen-1.5B and LLaMA-8B. The improvements suggest that the incorporation of knowledge into model parameters can enhance the overall performance of the RAG pipeline, enabling the model to recall and reason over the injected Conference, Under Review, Su, et al. Table 2: Ablation study on the impact of LoRA weight ini- tialization strategies for P-RAG. All metrics reported are F1 scores. â€œP-RAG Rand. â€ and â€œP-RAG Warm. â€ indicate ran- domly initialized LoRA weights and warm-up LoRA initial- ization, respectively. The best results are in bold. 2WQA HQA PQA CWQ LLaMA-1B P-RAG Rand. 0.2764 0.1999 0.2205 0.3482 P-RAG Warm. 0.3546 0.2456 0.2035 0.4263 Qwen-1.5B P-RAG Rand. 0.3025 0.2165 0.1885 0.3280 P-RAG Warm. 0.3542 0.2718 0.2418 0.5018 LLaMA-8B P-RAG Rand. 0.3932 0.3563 0.2413 0.4541 P-RAG Warm. 0.4201 0.4499 0.2952 0.5591 knowledge more effectively. Furthermore, since these gains are observed in models from different series and parameter sizes, the results underscore the robustness and broad applicability of P-RAG. (2) Comparison with DA-RAG. DA-RAG incorporates all the content generated during the Document Augmentation phase into the context, whereas our proposed P-RAG consistently outperforms DA-RAG across all settings. This result demonstrates that the per- formance improvement observed in Parametric RAG does not stem from the document augmentation phase, but from the in-parameter knowledge injection paradigm. (3) Impact of Model Scale on P-RAG. The performance gap between P-RAG and other RAG baselines is noticeably more significant when moving from the LLaMA-1B model to LLaMA-8B. This discrepancy indicates that parametric injection becomes even more beneficial in larger-scale models because larger models can better leverage internalized docu- ment knowledge. (4) Combine In-context RAG and P-RAG. The combined use of parametric and in-context RAG methods (Combine Both) yields the highest overall performance across various datasets and base LLMs. This result highlights that in-parameter knowledge injection is not in conflict with traditional RAG methods based on in-context knowledge injection. Consequently, our proposed doc- ument parameterization approaches can be seamlessly integrated for downstream tasks, allowing Parametric RAG to enhance ex- isting RAG systems without disrupting their pipelines. (5) Other Findings. Both DRAGIN and FLARE underperform significantly when applied to Qwen-2.5-1.5B. Our analysis suggests that Qwen- 2.5-1.5B tends to produce highly confident answers regardless of uncertainty. Since these dynamic RAG frameworks rely on confi- dence to trigger retrieval, they rarely activate on Qwen-2.5-1.5B. This highlights a key limitation of uncertainty-based triggers and underscores the need for more robust mechanisms in dynamic RAG frameworks. 5.2 Impact of LoRA Weight Initialization To investigate the impact of LoRA weight initialization strategies on our proposed Parametric RAG framework, we conducted an ablation study using two initialization strategies: (1) Random Initialization (P-RAG Rand.): The LoRA weights are initialized randomly without any pretraining or warm-up, which represents the default setting for our proposed Parametric RAG approach. All the Parametric RAG experimental results reported in other sections in this paper are based on this setting. (2) Warm-Up Initialization (P-RAG Warm.): LoRA weights are pre-trained using a set of 600 sampled question-answer (QA) pairs. These QA pairs are selected from the training sets of our chosen benchmarks and are distinct from the test questions to ensure no data leakage. The pre-training process involves training the LoRA parameters using the standard next-token prediction method on the concatenated tokens of the QA pairs. The specific implementation follows Section 4.3, and the pre-trained LoRA parameters are saved as initialization for the Document Parameterization phase9. The experimental results in Table 2 clearly indicate that across different model series and scales, as well as diverse datasets, the warm-up initialization strategy (P-RAG Warm.) consistently out- performs random initialization (P-RAG Rand.). This demonstrates the effectiveness of task-aware pretraining in enhancing the Para- metric RAG pipeline. Furthermore, the observed improvements across varying model sizes confirm the scalability and generality of this approach. The superior performance of the warm-up ap- proach in downstream tasks can be attributed to two key factors. First, it effectively aligns the additional LoRA parameters with the base LLM before document parameterizing, ensuring a smoother integration of knowledge. Second, it facilitates the incorporation of task-relevant knowledge, including output formats and generation patterns, which are critical for enhancing the quality of response in certain tasks. This finding suggests that in practical Parametric RAG applications where the downstream task is fixed, warming up the LoRA parameters for the task offers a promising approach to boost effectiveness. It is important to note that our main ex- periments (as well as all other experiments in this paper) were conducted using random initialization without any task-specific optimizations or dataset-specific tuning. This further highlights the strong generalization capability of our proposed Parametric RAG paradigm. These findings also highlight a broader insight: embedding few- shot examples either in the modelâ€™s context or directly into its parameters leads to improved downstream task performance. In- terestingly, our proposed parametric information representation method offers compatibility with few-shot in-context learning, en- abling a combination of parametric and in-context knowledge aug- mentation. 5.3 Impact of Document Augmentation To investigate the individual contributions of the rewriting and question-answer (QA) generation steps in the document augmen- tation process, we conduct a series of ablation experiments by removing (1) both rewriting and QA, (2) QA alone, and (3) rewriting alone. The experimental results are shown in Figure 3, and we have the following observations: (1) When neither rewriting nor QA generation is employed, the performance consistently degrades significantly across all evaluated tasks and models. This reduction suggests that simply training the LLM on the selected document via the next token prediction task without any form of data augmenta- tion leads to insufficient internalization of facts by the model. (2) 9All the training code and data are publicly available at our anonymous GitHub repository: https://github.com/oneal2000/PRAG Parametric Retrieval Augmented Generation Conference, Under Review, LLaMA Qwen 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 2WQA LLaMA Qwen 0.14 0.16 0.18 0.20 0.22 HQA LLaMA Qwen 0.00 0.05 0.10 0.15 0.20 PQA LLaMA Qwen 0.20 0.23 0.25 0.28 0.30 0.33 0.35 CWQ w/o Both w/o QA w/o Rewrite with Both Figure 3: Ablation study on the impact of the document aug- mentation stage. LLaMA indicates LLaMA-3.2-1B, and Qwen indicates Qwen-2.5-1.5B. The metric used is the F1 Score. Removing either QA or rewriting alone yields better results than removing both, indicating that each step offers distinct benefits. However, we notice that removing QA leads to a more significant performance decline than removing rewriting. This observation suggests that QA pair generation is more crucial for pushing the model to recall and apply factual information while rewriting offers valuable diversity in phrasing and structure and benefits the overall performance. (3) Incorporating both rewriting and QA results in the strongest overall performance on most of the evaluated tasks and models. These findings reinforce that rewriting and QA generation play complementary roles. In general, this ablation study indicates that both rewriting and QA generation significantly enhance the performance of document parameterizing. Their integration produces the best performance. Rewriting expands the coverage and diversity of context, while QA explicitly encourages the model to encode the knowledge of the selected document in a necessary way to apply the knowledge for downstream tasks. Therefore, it is advisable to incorporate both components of our document augmentation for effective internal- ization of knowledge. 5.4 Impact of Data-augmentation Model To evaluate the impact of the choice of LLM in the Document Augmentation phase, we conducted an ablation study comparing different configurations of the model used for document rewriting and QA pair generation. In our default setting, we use the same LLM for both the document augmentation process and the down- stream task. However, to explore whether the performance of our method is sensitive to this choice, we tested alternative configu- rations. Specifically, we tested different model sizes by using both smaller and larger LLMs for document augmentation and QA pair generation. The experimental results are shown in Table 3, indi- cating that our framework demonstrates an insensitivity to the Table 3: Ablation study comparing different document aug- mentation models. GenLM indicates the generator LLM and AugLM indicates the LLM for document augmentation. LLaMA indicates LLaMA-3.2-1B, and Qwen indicates Qwen- 2.5-1.5B. The best results are in bold. The metric used in the table is F1 Score. GenLM AugLM Dataset 2WQA HQA PQA CWQ LLaMA-1B LLaMA-1B 0.2764 0.1999 0.2205 0.3482 Qwen-1.5B 0.2753 0.1980 0.2340 0.3495 LLaMA-8B 0.2748 0.1935 0.2207 0.3498 Qwen-1.5B LLaMA-1B 0.2974 0.2005 0.1829 0.3183 Qwen-1.5B 0.3025 0.2165 0.1885 0.3280 LLaMA-8B 0.2948 0.2161 0.2156 0.3211 choice of data augmentation model. Performance remains consis- tent across different setups, regardless of whether a smaller, larger, or the same model as the generator is used for data augmentation. Importantly, using a small model for augmentation yields compa- rable results to employing significantly larger models, indicating that the augmentation step does not require high-capacity models to be effective. Similarly, when the same model is used for both generation and augmentation, the outcomes are indistinguishable from those where separate models are employed. 5.5 Runtime Analysis We present the inference time for the LLaMA3-8B model across various RAG baselines on 2WikiMultihopQA (2WQA) and Com- plexWebQuestions (CWQ) in Table 4. This evaluation simulates the online inference latency for answering a question using different RAG approaches. All experiments were conducted on the same GPU server to ensure consistent evaluation conditions. The experi- mental results indicate that P-RAG reduces the time per question by 29% to 36% compared to Standard RAG. Notably, the Combine Both baseline, which showed the best performance and significant improvements in the main experiment, requires almost the same online computation time as the Standard RAG method. In contrast, multi-round RAG frameworks like DRAGIN and FLARE exhibit significantly higher latency for answering a question compared to single-round methods. For both P-RAG and Combine Both baselines, we present the inference times separately from the time required for merging and loading the LoRA (0.32s). This distinction arises because, in our current implementation, the time spent on merging and loading the LoRA significantly exceeds theoretical expectations. The floating-point operations involved in the LoRA operation step contribute less than 1% to the total computational cost of generat- ing a response [14], but the latency of memory loading and data communications in our current implementation is far from perfect. We believe this latency can potentially be addressed through en- gineering optimizations. It is important to note that our analysis primarily emphasizes the relative time ratios and trends across the different methods, as actual application times and latencies can vary depending on hardware configurations, such as CPU, GPU, memory, and storage. Conference, Under Review, Su, et al. Table 4: The average time required by the LLaMA3-8B model to answer a question on the 2WikiMultihopQA (2WQA) and ComplexWebQuestions (CWQ) datasets. The "+0.32" footnote for P-RAG and Combine Both indicates the total time needed for merging and loading the LoRA adapter. 2WQA CWQ Time(s) Speed Up Time(s) Speed Up P-RAG 2.34+0.32 1.29x 2.07+0.32 1.36x Combine Both 3.08+0.32 0.98x 2.84+0.32 0.99x Standard RAG 3.03 1.00x 2.82 1.00x FLARE 10.14 0.25x 11.31 0.25x DRAGIN 14.60 0.21x 16.21 0.17x 6 Conclusion and Future Directions This work introduces Parametric RAG, a novel framework that addresses the limitations of in-context knowledge augmentation by parameterizing external documents. Parametric RAG infuses these parameterized documents directly into the model, reducing contextual overload and online computational costs while maintain- ing robust performance. Our experiments on multiple benchmarks demonstrate that Parametric RAG outperforms traditional retrieval- augmented generation methods across different LLMs. Ultimately, Parametric RAG offers a more efficient and scalable pathway to integrate external knowledge into LLMs, paving the way for further innovation in parametric-based knowledge augmentation. Despite its significant potential, Parametric RAG presents sev- eral challenges that warrant further investigation. First, the current parameterization process is computationally intensive, and the para- metric representations of each document are substantially larger than plain text. Future work could explore more methods to improve computational and storage efficiency, making the parameterization process more scalable. Second, the parameterized documents are currently tied to specific LLMs, restricting their ability to generalize across different models. Developing universal, model-agnostic rep- resentations could significantly enhance flexibility and reuse across diverse systems. Finally, we believe the potential applications of information parameterization can be extended beyond RAG. For instance, LLM-based agents could benefit from parameterizing the agentâ€™s profiles and configuration, which could alleviate context- length constraints and improve online computational efficiency. By addressing these challenges, future research could unlock more potential for the Parametric RAG paradigm. References [1] Zeyuan Allen-Zhu and Yuanzhi Li. [n. d.]. Physics of Language Models: Part 3.1, Knowledge Storage and Extraction. In Forty-first International Conference on Machine Learning. [2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. [n. d.]. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations . [3] Ingeol Baek, Hwan Chang, Byeongjeong Kim, Jimin Lee, and Hwanhee Lee. 2024. Probing-RAG: Self-Probing to Guide Language Models in Selective Document Retrieval. arXiv preprint arXiv:2410.13339 (2024). [4] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther- ford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bog- dan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from trillions of tokens. In International conference on machine learning . PMLR, 2206â€“2240. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877â€“1901. [6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se- bastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022). [7] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayi- heng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023. How abilities in large language models are affected by supervised fine-tuning data composition. arXiv preprint arXiv:2310.05492 (2023). [8] Qian Dong, Qingyao Ai, Hongning Wang, Yiding Liu, Haitao Li, Weihang Su, Yiqun Liu, Tat-Seng Chua, and Shaoping Ma. 2025. Decoupling Knowledge and Context: An Efficient and Effective Retrieval Augmented Generation Framework via Cross Attention. In Proceedings of the ACM on Web Conference 2025 . [9] Qian Dong, Yiding Liu, Qingyao Ai, Haitao Li, Shuaiqiang Wang, Yiqun Liu, Dawei Yin, and Shaoping Ma. 2023. I3 retriever: incorporating implicit interaction in pre-trained language models for passage retrieval. InProceedings of the 32nd ACM International Conference on Information and Knowledge Management . 441â€“451. [10] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130 (2024). [11] Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, and Yiqun Liu. 2024. Scaling laws for dense retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1339â€“1349. [12] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International conference on machine learning. PMLR, 3929â€“3938. [13] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060 (2020). [14] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations . [15] Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. 2024. GRAG: Graph Retrieval-Augmented Generation. arXiv preprint arXiv:2405.16506 (2024). [16] Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282 (2020). [17] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. 2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity. InProceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 7036â€“7050. https://doi.org/10.18653/v1/2024.naacl-long.389 [18] Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, and Graham Neubig. 2022. Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer. arXiv preprint arXiv:2212.02027 (2022). [19] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983 (2023). [20] Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open- domain question answering. arXiv preprint arXiv:2004.04906 (2020). [21] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Bridging the preference gap between retrievers and llms. arXiv preprint arXiv:2401.06954 (2024). [22] Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task, more tokens: the impact of input length on the reasoning performance of large language models. arXiv preprint arXiv:2402.14848 (2024). [23] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459â€“9474. [24] Haitao Li, Jia Chen, Weihang Su, Qingyao Ai, and Yiqun Liu. 2023. Towards better web search performance: pre-training, fine-tuning and learning to rank. arXiv preprint arXiv:2303.04710 (2023). [25] Huanshuo Liu, Hao Zhang, Zhijiang Guo, Kuicai Dong, Xiangyang Li, Yi Quan Lee, Cong Zhang, and Yong Liu. 2024. CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control. arXiv preprint arXiv:2405.18727 (2024). [26] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models Parametric Retrieval Augmented Generation Conference, Under Review, use long contexts. Transactions of the Association for Computational Linguistics 12 (2024), 157â€“173. [27] Yixiao Ma, Yueyue Wu, Weihang Su, Qingyao Ai, and Yiqun Liu. 2023. CaseEn- coder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding.arXiv preprint arXiv:2305.05393 (2023). [28] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 9802â€“9822. https: //doi.org/10.18653/v1/2023.acl-long.546 [29] Meta. 2024. Llama-3.2-1B-Instruct. https://huggingface.co/meta-llama/Llama- 3.2-1B-Instruct Accessed: 2024-09. [30] Meta. 2024. Meta-Llama-3-8B-Instruct. https://huggingface.co/meta-llama/Meta- Llama-3-8B-Instruct Accessed: 2024-04. [31] Neel Nanda, Senthooran Rajamanoharan, JÃ¡nos KramÃ¡r, and Rohin Shah. 2023. Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level. https://www.lesswrong.com/posts/iGuwZTHWb6DFY3sKB/fact-finding- attempting-to-reverse-engineer-factual-recall Accessed: 2025-01-24. [32] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. 2024. Graph retrieval-augmented generation: A survey. arXiv preprint arXiv:2408.08921 (2024). [33] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083 (2023). [34] Stephen Robertson, Hugo Zaragoza, et al . 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and TrendsÂ® in Information Retrieval 3, 4 (2009), 333â€“389. [35] Alireza Salemi and Hamed Zamani. 2024. Towards a search engine for machines: Unified ranking for multiple retrieval-augmented large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . 741â€“751. [36] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡, Daniel Hesslow, Roman CastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon, Matthias GallÃ©, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022). [37] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652 (2023). [38] Craig Silverstein, Hannes Marais, Monika Henzinger, and Michael Moricz. 1999. Analysis of a very large web search engine query log. SIGIR Forum 33, 1 (Sept. 1999), 6â€“12. https://doi.org/10.1145/331403.331405 [39] Tim Soulo. 2023. 96.55% of Content Gets No Traffic From Google. Hereâ€™s How to Be in the Other 3.45% [New Research for 2023] . https://ahrefs.com/blog/search- traffic-study/ Accessed: 2025-01-24. [40] Weihang Su, Qingyao Ai, Xiangsheng Li, Jia Chen, Yiqun Liu, Xiaolong Wu, and Shengluan Hou. 2023. Wikiformer: Pre-training with Structured Information of Wikipedia for Ad-hoc Retrieval. arXiv preprint arXiv:2312.10661 (2023). [41] Weihang Su, Qingyao Ai, Yueyue Wu, Yixiao Ma, Haitao Li, and Yiqun Liu. 2023. Caseformer: Pre-training for Legal Case Retrieval.arXiv preprint arXiv:2311.00333 (2023). [42] Weihang Su, Yiran Hu, Anzhe Xie, Qingyao Ai, Quezi Bing, Ning Zheng, Yun Liu, Weixing Shen, and Yiqun Liu. 2024. STARD: A Chinese Statute Retrieval Dataset Derived from Real-life Queries by Non-professionals. InFindings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 10658â€“10671. https://doi.org/10.18653/v1/2024.findings-emnlp.625 [43] Weihang Su, Xiangsheng Li, Yiqun Liu, Min Zhang, and Shaoping Ma. 2023. Thuir2 at ntcir-16 session search (ss) task. arXiv preprint arXiv:2307.00250 (2023). [44] Weihang Su, Yichen Tang, Qingyao Ai, Changyue Wang, Zhijing Wu, and Yiqun Liu. 2024. Mitigating entity-level hallucination in large language models. In Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region . 23â€“31. [45] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 12991â€“13013. https://doi.org/10.18653/v1/2024. acl-long.702 [46] Weihang Su, Changyue Wang, Qingyao Ai, Yiran Hu, Zhijing Wu, Yujia Zhou, and Yiqun Liu. 2024. Unsupervised real-time hallucination detection based on the internal states of large language models. arXiv preprint arXiv:2403.06448 (2024). [47] Alon Talmor and Jonathan Berant. 2018. The Web as a Knowledge-Base for Answering Complex Questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 641â€“651. https://doi.org/10.18653/v1/N18-1059 [48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [49] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Interleaving retrieval with chain-of-thought reasoning for knowledge- intensive multi-step questions. arXiv preprint arXiv:2212.10509 (2022). [50] Changyue Wang, Weihang Su, Qingyao Ai, and Yiqun Liu. 2024. Knowledge Editing through Chain-of-Thought. arXiv preprint arXiv:2412.17727 (2024). [51] Changyue Wang, Weihang Su, Hu Yiran, Qingyao Ai, Yueyue Wu, Cheng Luo, Yiqun Liu, Min Zhang, and Shaoping Ma. 2024. LeKUBE: A Legal Knowledge Update BEnchmark. arXiv preprint arXiv:2407.14192 (2024). [52] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-knowledge guided retrieval augmentation for large language models.arXiv preprint arXiv:2310.05002 (2023). [53] Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. 2024. Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation. arXiv preprint arXiv:2403.05313 (2024). [54] Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gho- lamreza Haffari. 2024. Continual learning for large language models: A survey. arXiv preprint arXiv:2402.01364 (2024). [55] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al . 2024. Qwen2. 5 Technical Report. arXiv preprint arXiv:2412.15115 (2024). [56] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for di- verse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600 (2018). [57] Zijun Yao, Weijian Qi, Liangming Pan, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, and Juanzi Li. 2024. Seakr: Self-aware knowledge retrieval for adaptive retrieval augmented generation. arXiv preprint arXiv:2406.19215 (2024). [58] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro. 2024. Rankrag: Unifying context ranking with retrieval-augmented generation in llms. arXiv preprint arXiv:2407.02485 (2024). [59] Zeping Yu and Sophia Ananiadou. 2024. Neuron-Level Knowledge Attribution in Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 3267â€“3280. https://doi.org/10.18653/v1/2024.emnlp-main.191 [60] ChengXiang Zhai. 2008. Statistical language models for information retrieval. Synthesis lectures on human language technologies 1, 1 (2008), 1â€“141.