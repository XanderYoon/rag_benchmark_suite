HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation Hao Liu1†, Zhengren Wang1,2†, Xi Chen3, Zhiyu Li2∗, Feiyu Xiong2, Qinhan Yu1, Wentao Zhang1∗ 1Peking University 2Center for LLM, Institute for Advanced Algorithms Research, Shanghai 3Huazhong University of Science and Technology {liuhao_2002,wzr,yuqinhan}@stu.pku.edu.cn xichenai@hust.edu.cn {lizy, xiongfy}@iaar.ac.cn wentao.zhang@pku.edu.cn Abstract Retrieval-Augmented Generation (RAG) sys- tems often struggle with imperfect retrieval, as traditional retrievers focus on lexical or semantic similarity rather than logical rele- vance. To address this, we proposeHopRAG, a novel RAG framework that augments retrieval with logical reasoning through graph-structured knowledge exploration. During indexing, Ho- pRAG constructs a passage graph, with text chunks as vertices and logical connections es- tablished via LLM-generated pseudo-queries as edges. During retrieval, it employs a retrieve- reason-prune mechanism: starting with lexi- cally or semantically similar passages, the sys- tem explores multi-hop neighbors guided by pseudo-queries and LLM reasoning to iden- tify truly relevant ones. Experiments on mul- tiple multi-hop benchmarks demonstrate that HopRAG’sretrieve-reason-prune mechanism can expand the retrieval scope based on logical connections and improve final answer quality. 1 Introduction “Everyone and everything is six or fewer steps away, by way of introduction, from any other person in the world. ” — Six Degrees of Separation Retrieval-augmented generation (RAG) has be- come the standard approach for large language models (LLMs) to tackle knowledge-intensive tasks (Guu et al., 2020a; Lewis et al., 2020a; Izac- ard et al., 2022; Min et al., 2023; Ram et al., 2023; Liang et al., 2025). Not only can it effectively address the inherent knowledge limitations and hal- lucination issues (Zhang et al., 2023), but it can also enable easy interpretability and provenance tracking (Akyurek et al., 2022). Especially, the † Equal contribution; * Corresponding author. efficacy of RAG hinges on its retrieval module for identifying relevant documents from a vast corpus. Currently, there are two mainstream types of re- trievers: sparse retrievers (Jones, 1973; Robertson and Zaragoza, 2009b) and dense retrievers (Xiao et al., 2024; Wang et al., 2024b; Sturua et al., 2024; Wang et al., 2024c), which focus on lexical simi- larity and semantic similarity respectively, and are often combined for better retrieval performance (Sawarkar et al., 2024). Despite advancements, the ultimate goal of information retrieval extends beyond lexical and semantic similarity, striving in- stead for logical relevance. Due to the lack of logic-aware mechanism, the imperfect retrieval re- mains prominent (Wang et al., 2024a; Shao et al., 2024; Dai et al., 2024; Su et al., 2024a,b). For precision, the retrieval system may return lexically and semantically similar but indirectly relevant pas- sages; regarding recall, it may fail to retrieve all the necessary passages for the user query. Both cases eventually lead to inaccurate or in- complete LLM responses (Chen et al., 2024; Xiang et al., 2024; Zou et al., 2024), especially for multi- hop or multi-document QA tasks requiring multiple relevant passages for the final answer. In contrast, the reasoning capability of generative models is rapidly advancing, with notable examples such as OpenAI-o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025). Therefore, a natural research question arises: "Is it possible to introduce reason- ing capability into the retrieval module for more advanced RAG systems?" From a logical structure perspective, existing RAG systems can be mainly categorized into three types: Non-structured RAG simply adopts sparse or dense retrievers. The retrieval is only based on keyword matching or semantic vector similarity, but fails to capture the logical relations between user queries and passages. Tree-structured RAG (Sarthi et al., 2024; Chen et al., 2023; Fatehkia et al., 2024) focuses on the hierarchical logic of arXiv:2502.12442v2 [cs.IR] 26 May 2025 (a) Precision, recall and F1 score (b) Proportions of passages on relevance Figure 1: (a) Precision, recall and F1 score of BGE dense retrievers on MuSiQue, 2WikiMultiHopQA and HotpotQA with different topk parameters, revealing the severe imperfect retrieval phenomenon. The highest recall reaches saturation at 0.45 in our settings. (b) We categorize retrieved passages into relevant, indirectly relevant and irrelevant according to the logical relevance to the query. The relevant passages are exactly the supporting facts, and indirectly relevant passages can hop to the supporting facts via HopRAG while irrelevant passages cannot. A large proportion of retrieved passages are indirectly relevant. Figure 2: Demonstration of hopping between passages. For the user query, BGE dense retriever can only re- turn one of the three supporting facts within topk bud- get. However, lexically or semantically similar passages complement each other. Hopping between passages, by questions as pathways, improves the retrieval accuracy and completeness. passages within a single document, but ignores re- lations beyond the hierarchical structure or across documents. Further, it introduces redundant infor- mation across different levels. Graph-structured RAG (Soman et al., 2024; Kang et al., 2023; Edge et al., 2024a; Guo et al., 2024) models logical rela- tions in the most ideal form by constructing knowl- edge graphs (KGs) to represent documents, where entities are vertices and their relations are edges. However, the reliance on predefined schemas limits the flexible expressive capability (Li et al., 2024); constructing and updating knowledge graphs are challenging and prone to errors or omissions (Edge et al., 2024a); the triplet format of knowledge ne- cessitates extra textualization or fine-tuning to im- prove LLMs’ understanding (He et al., 2024). Motivation As reported by (Wang et al., 2024a), even with advanced real-world search engines, roughly 70% retrieved passages do not directly contain true answers in their settings. We con- firm the severity of imperfect retrieval in terms of both precision and recall, as illustrated in Figure 1(a). Inspired by the small-world theory (Kleinberg, 2000) or six degrees of separation (Guare, 2016), we propose that, although lexically and semanti- cally similar passages could be indirectly relevant or even distracting, they can serve as helpful start- ing points to reach truly relevant ones. As shown in Figure 1(b), considering a graph composed of passages with logical relations as edges, a large proportion of retrieved passages fall within several hops of the ground truths. Based on these observations, we propose Ho- pRAG, an innovative graph-structured RAG sys- tem. At indexing phase, we construct a graph- structured knowledge index with passages as ver- tices and logic relations as directed edges. Specifi- cally, the passages are connected by pseudo-queries generated by query simulation and edge merging operations. For example, as demonstrated in Fig- ure 2, the pseudo-query "Why does the princess kiss the frog?" connects the raiser passage and the solver passage, as the pivot for logical hops. During retrieval, we employ reasoning-augmented graph traversal, following a three-step paradigm of retrieval, reasoning, and pruning. This pro- cess searches for truly relevant passages within the multi-hop neighborhood of indirectly relevant passages, guided by both the index structure and LLM reasoning. Contributions Our contributions are as follows: • We reveal the severe imperfect retrieval phe- nomenon for multi-hop QA tasks. The results quantify that currently over 60% of retrieved passages are indirectly relevant or irrelevant. To turn "trash" into "treasure", we further em- ploy indirectly relevant passages as stepping stones to reach truly relevant ones. • We propose HopRAG, a novel RAG system with logic-aware retrieval mechanism. As lexi- cally or semantically similar passages comple- ment each other, HopRAG connects the raiser and solver passages with pseudo-queries. Be- yond similarity-based retrieval, it reasons and prunes along the queries during retrieval. It also features flexible logical modeling, cross- document organization, efficient construction and updating. • Extensive experiments confirm the effective- ness of HopRAG. The retrieve-reason-prune mechanism achieves over 36.25% higher an- swer metric and 20.97% higher retrieval F1 score compared to conventional information retrieval approaches. Several ablation studies provide more valuable insights. 2 Related Work Retrieval-Augmented Generation Retrieval- augmented generation significantly improves large language models by incorporating a retrieval mod- ule that fetches relevant information from external knowledge sources (Févry et al., 2020; Guu et al., 2020b; Izacard and Grave, 2021; Zhao et al., 2024; Yu et al., 2025). Retrieval models have evolved from early sparse retrievers, such as TF-IDF (Jones, 1973) and BM25 (Robertson and Zaragoza, 2009b), which rely on word statistics and inverted indices, to dense retrievers (Lewis et al., 2020b) that uti- lize neural representations for semantic matching. Advanced methods, such as Self-RAG (Asai et al., 2023) and FLARE (Jiang et al., 2023) which deter- mine the necessity and timing of retrieval, represent significant developments. However, the knowledge index remains logically unstructured, with each round of search considering only lexical or seman- tic similarity. Tree&Graph-structured RAG Tree and graph are both effective structures for modeling logical relations. RAPTOR (Sarthi et al., 2023) recursively embeds, clusters, and summarizes passages, con- structing a tree with differing levels of summariza- tion from the bottom up. MemWalker (Chen et al., 2023) treats the LLM as an interactive agent walk- ing on the tree of summarization. SiReRAG (Zhang et al., 2024) explicitly considers both similar and related information by constructing both similar- ity tree and relatedness tree. PG-RAG (Liang et al., 2024) prompts LLMs to organize docu- ment knowledge into mindmaps, and unifies them for multiple documents. GNN-RAG (Mavroma- tis and Karypis, 2024) reasons over dense KG subgraphs with learned GNNs to retrieve answer candidates. For query-focused summarization, GraphRAG (Edge et al., 2024b) builds a hierarchi- cal graph index with knowledge graph construction and recursive summarization. Despite advance- ments, tree-structured RAG only focuses on the hierarchical logic within a single document; graph- structured RAG is costly, time-consuming, and re- turns triplets instead of plain text. In contrast, Ho- pRAG offers a more lightweight and downstream task friendly alternative, with flexible logical mod- eling, cross-document organization, efficient con- struction and updating. 3 Method In this section, we introduce our logic-aware RAG system, named HopRAG. An overview of this sys- tem is illustrated in Figure 3. 3.1 Problem Formulation Given a passage corpus P = {p1, p2, ..., pN } and a query q which requires the information from multiple passages in P , the task is to design (1) a graph-structured RAG knowledge base that not only stores all the passages in corpus P but also models the similarity and logic between passages; (2) a corresponding retrieval strategy that can hop from indirectly relevant passages to truly relevant passages for better retrieval. Finally, with the query q and k passages as context C = {pi1, pi2, ..., pik }, the LLM generates the response O ∼ P (O|q, C). 3.2 Graph-Structured Index We construct a graph-structured index G = (V, E) where the vertex set V consists of vertices storing all the passages and the directed edge set E = {⟨vi, ei,j, vj⟩|vi, vj ∈ V} ⊂ V × V is established based on the logical relations between passages for multi-hop reasoning. To establish G, we utilize Query Simulation to identify the logical relations and leverage textual similarity for efficient Edge Merging. Figure 3: The workflow of HopRAG. Left: At indexing time, we first utilize Query Simulation to generate pseudo- queries for each passage and then apply Edge Merging to connect passages with directed logical edges. Right: At retrieval time, we employ a Retrieve-Reason-Prune pipeline. We first retrieve through purely similarity-based retrieval, then run reasoning-augmented graph traversal to explore the neighborhood, and finally prune the search by a novel metric Helpfulness considering both textual similarity and logical importance. Query Simulation To identify the logical rela- tions between passages, we generate a series of pseudo-queries for each passage, and use them to explore the passage’s relations with the others and bridge the inherent gap between user-queries and passages (Wang et al., 2024c). Specifically, we adopt LLM to generate two groups of pseudo- queries for each passage pi: (1) m out-coming questions Q+ i = S 1≤j≤m{q+ i,j} that originate from this passage but cannot be answered by itself; (2) n in-coming questions Q− i = S 1≤j≤n{q− i,j} whose answers are within the passage. As demonstrated in Figure 2, for the toy passage "Rose is the princess in the story The Frog Prince", one in-coming ques- tion might be "What is the name of the princess?" and one out-coming question might be "How is the frog connected to Rose?". The prompts are in Appendix A.5. We extract keywords from Q+ i and Q− i using named entity recognition NER(·) for sparse repre- sentation, and embed these questions into seman- tic vectors using an embedding model EMB(·) for dense representation. This results in sparse rep- resentations K+ i = S 1≤j≤m{k+ i,j} and K− i =S 1≤j≤n{k− i,j}, and dense representations V + i =S 1≤j≤m{v+ i,j} and V − i = S 1≤j≤n{v− i,j}. We further define out-coming triplets as r+ i,j := (q+ i,j, k+ i,j, v+ i,j) and in-coming triplets r− i,j := (q− i,j, k− i,j, v− i,j). Each passage pi is stored inside a vertex vi, featured with its out-coming triplets R+ i = S 1≤j≤m{r+ i,j} and in-coming tripletsR− i =S 1≤j≤n{r− i,j}. Edge Merging Given the out-coming and in- coming triplets, we match paired triplets via hy- brid retrieval and establish directed edges between the corresponding passages. For each out-coming triplet r+ s,i of source vertex vs, the most matching in-coming triplet r− t∗,j∗ is determined as follows: SIM(r+ s,i, r− t,j) = |k+ s,i∩k− t,j | |k+ s,i∪k− t,j | + v+ s,i·v− t,j ||v+ s,i||·||v− t,j || 2 r− t∗,j∗ = arg max r− t,j SIM(r+ s,i, r− t,j) (1) We then build the directed edge ⟨vs, es,t∗, vt∗⟩ with aggregated features, where es,t∗ := (q− t∗,j∗, k− t∗,j∗ ∪ k+ s,i, v− t∗,j∗). 3.3 Reasoning-Augmented Graph Traversal For more accurate and complete responses, Ho- pRAG’s retrieval strategy leverages the reasoning ability of LLM to explore the neighborhood of probably indirectly relevant passages and hop to relevant ones based on the logical relations in the graph structure. As shown in Algorithm 1, by rea- soning over the questions on out edges ei,j of a current vertex vi and then choosing to hop to the most promising vertex vj, we realize reasoning- augmented graph traversal for better retrieval per- formance. Retrieval Phase To start the local search over the graph for query q, we first use NER(·) and EMB(·) to get the keywords kq and vector vq of q, which will be used for hybrid retrieval to match topk sim- ilar edges ⟨vi, ei,j, vj⟩, following Equation 1. With each vertex vj from these edges we initialize a context queue Cqueue for breadth-first local search (V oudouris et al., 2010). Reasoning Phase To fully exploit the logical re- lations over the graph and hop from indirectly rele- vant vertices to relevant ones, we introduce breadth- first local search which utilizes the LLM to choose the most appropriate neighbor for eachvj in Cqueue to append to the tail of the queue. Specifically, for each vj in Cqueue in each round of hop, we lever- age LLM to reason over all the questions from its out edges to choose one ej,k with the question which the LLM regards as the most helpful for an- swering q and append vertex vk to Cqueue. After hopping from each vertex in the current Cqueue we can expand the context with at most topk new ver- tices. From these new vertices we continue the next round of hop. Since different vertices may hop to the same vertex, we believe the vertices with more visits are more important for answering q, and use a counter Ccount to track the number of visits for each vertex and measure its importance. By con- ducting nhop rounds of hop, we realize reasoning- augmented graph traversal and expand the context length to at most (nhop + 1) × topk. Pruning Phase To avoid including too many in- termediate vertices during the traversal, we intro- duce a novel metric Helpfulness H(·) that inte- grates similarity and logic to re-rank and then prune the traversal counter Ccount. We calculate Hi fol- lowing Equation 2 for each vi in Ccount and keep the topk vertices with the highest Hi, where hybrid textual similarity SIM(vi, q) calculates the aver- age lexical and semantic similarity between the passage in vi and query q following Equation 1; and IMP(vi, Ccount) is defined as the normalized number of visits of vi in Ccount during traversal fol- lowing Equation 3. We prune Ccount by retaining topk vertices with the highest H value, resulting in the final context C. Hi = SIM(vi, q) + IMP(vi, Ccount) 2 (2) IMP(vi, Ccount) = Ccount[vi]P vj ∈Ccount Ccount[vj] (3) Algorithm 1: Reasoning-Augmented Graph Traversal Input: q, topk, nhop, G Output: C 1 vq ← EMB(q); 2 kq ← NER(q); 3 Cqueue ← Retrieve(vq, kq, G); 4 Ccount ←Counter(Cqueue); 5 for i ← 1, 2, ..., nhop do 6 for j ← 1, 2, ..., | Cqueue | do 7 vj ← Cqueue.dequeue(); 8 vk ← Reason({⟨vj, ej,k, vk⟩}); 9 if vk not in Ccount then 10 Cqueue.enqueue(vk); 11 Ccount[vk] ← 1 ; 12 else 13 Ccount[vk] + + ; 14 end 15 end 16 end 17 C ←Prune(Ccount, vq, kq, topk); 18 return C 4 Experiments 4.1 Experimental Setups Datasets We collect several multi-hop QA datasets to evaluate the performance of HopRAG. We use HotpotQA dataset (Yang et al., 2018), 2WikiMultiHopQA dataset (Ho et al., 2020) and MuSiQue dataset (Trivedi et al., 2022). Follow- ing the same procedure as (Zhang et al., 2024), we obtain 1000 questions from each validation set of these three datasets. See Appendix A.2 for details. Baselines We compare HopRAG with a variety of baselines: (1) unstructured RAG - sparse re- triever BM25 (Robertson and Zaragoza, 2009a) (2) unstructured RAG - dense retriever BGE (Xiao et al., 2024; Karpukhin et al., 2020) (3) unstruc- tured RAG - dense retriever BGE with query decomposition (Min et al., 2019) (4) unstruc- tured RAG - dense retriever BGE with reranking MuSiQue 2Wiki HotpotQA Average Method EM F1 EM F1 EM F1 EM F1 BM25 5.80 11.00 27.00 31.55 33.40 44.30 22.07 28.95 BGE 11.80 18.60 27.90 30.80 38.40 50.56 26.03 33.32 Query Decomposition 21.50 31.40 43.90 47.06 43.60 58.94 31.10 40.01 Reranking 24.50 34.53 46.70 50.89 47.70 62.95 34.67 43.60 HippoRAG 32.60 43.78 66.40 74.01 59.90 74.29 52.97 64.03 RAPTOR 35.30 47.47 54.90 61.20 58.10 72.48 49.43 60.38 SiReRAG 38.90 52.08 60.40 68.20 62.50 77.36 53.93 65.88 HopRAG 39.10 53.00 61.60 68.93 61.30 78.34 54.00 66.76 Table 1: We test our HopRAG against a series of baselines on multiple datasets using GPT-4o and GPT-3.5-turbo as the inference model with top 20 passages. We report the QA performance metrics EM and F1 score with GPT-3.5-turbo here and GPT-4o in Table 2, where the best score is inbold and the second best is underlined. MuSiQue 2Wiki HotpotQA Average Method EM F1 EM F1 EM F1 EM F1 BM25 13.80 21.50 40.30 44.83 41.20 53.23 31.77 39.85 BGE 20.80 30.10 40.10 44.96 47.60 60.36 36.17 45.14 Query Decomposition 29.00 38.50 55.70 60.57 52.80 68.67 47.46 55.91 Reranking 32.00 40.29 53.70 58.44 55.40 70.03 48.61 56.25 GraphRAG 12.10 20.22 22.50 27.49 31.70 42.74 22.10 30.15 RAPTOR 36.40 49.09 53.80 61.45 58.00 73.08 49.40 61.21 SiReRAG 40.50 53.08 59.60 67.94 61.70 76.48 53.93 65.83 HopRAG 42.20 54.90 61.10 68.26 62.00 76.06 55.10 66.40 Table 2: We report the QA performance metrics EM and F1 score with GPT-4o and top 20 passages here, where the best score is in bold and the second best is underlined. (Nogueira and Cho, 2020) (5) tree-structured RAG - RAPTOR (Sarthi et al., 2024) (6) tree-structured RAG - SiReRAG (Zhang et al., 2024) (7) graph- structured RAG - GraphRAG (Edge et al., 2024a) with the local search function (8) graph-structured RAG - HippoRAG (Gutiérrez et al., 2025). For structured RAG baselines, we follow the same set- ting as previous work (Zhang et al., 2024). Metrics To measure the answer quality of dif- ferent methods, we adopt exact match (EM) and F1 score which focus on the accuracy between a generated answer and the corresponding ground truth. We also use retrieval metrics to compare graph-based methods. Since tree-based methods like SiReRAG (Zhang et al., 2024) and RAPTOR (Sarthi et al., 2024) create new candidates (e.g., summary nodes) in the retrieval pool, it would be unfair to use retrieval metrics to compare them with others. We report both the answer and retrieval met- rics in the ablations and discussion on HopRAG. See Appendix A.3 for more metric details. Settings We use BGE embedding model for se- mantic vectors at 768 dimensions. To avoid the loss of semantic information caused by chunking at a fixed size, we adopt the same chunking methods uti- lized in the original datasets respectively. GPT-4o- mini serves as both the model generating in-coming and out-coming questions when constructing the graph index, and the reasoning model for graph traversal. We use two reader models GPT-4o and GPT-3.5-turbo to generate the response given the context with 20 retrieval candidates and nhop = 4. See Appendix A.4 for more setting details. 4.2 Main Results The main results are presented in Table 1 and 2. We observe that almost in all the settings HopRAG gives the best performance, with exceptions on HotpotQA when compared against SiReRAG and 2WikiMultiHopQA against HippoRAG. Overall, HopRAG achieves approximately 76.78% higher than dense retriever (BGE), 48.62% higher than query decomposition, 36.25% higher than rerank- MuSiQue 2Wiki HotpotQA Average Answer Retrieval Answer Retrieval Answer Retrieval Answer Retrieval topk EM F1 F1 EM F1 F1 EM F1 F1 EM F1 F1 2 32.50 46.31 37.83 47.80 53.91 36.77 52.00 67.78 50.23 44.10 56.00 41.61 4 36.50 49.53 35.02 54.50 59.35 33.22 55.60 71.10 46.45 48.87 59.99 38.23 8 38.50 50.81 26.36 56.10 61.81 23.90 58.20 75.05 34.14 50.93 62.56 28.13 12 37.50 51.47 20.38 57.70 64.33 18.54 59.50 75.54 26.34 51.57 63.78 21.75 16 37.50 51.44 16.47 60.00 67.52 15.02 59.50 76.45 21.75 52.33 65.14 17.75 20 39.10 53.00 13.89 61.60 68.93 12.51 61.30 78.34 18.48 54.00 66.76 14.96 Table 3: We test the robustness w.r.t hyperparametertopk on HopRAG using GPT-3.5-turbo on multiple datasets. We vary topk from 2 to 20 and report both the answer and retrieval metrics, where the best score is in bold and the second best is underlined. MuSiQue 2Wiki HotpotQA Average nhop Retrieval F1 LLM Cost Retrieval F1 LLM Cost Retrieval F1 LLM Cost Retrieval F1 LLM Cost 1 8.78 20.00 8.68 19.86 6.78 19.91 8.08 19.92 2 11.86 30.32 11.42 31.52 15.13 29.39 12.80 30.41 3 12.67 37.28 11.97 37.15 16.76 33.35 13.80 35.93 4 13.89 40.32 12.51 40.12 18.48 35.14 14.96 38.53 Table 4: We test the effect of hyperparameter nhop on HopRAG using GPT-3.5-turbo on multiple datasets with top 20 passages. We vary nhop from 1 to 4 and report both the answer and retrieval metrics in Table 8, and report the retrieval metrics here. For retrieval metrics, we calculate the retrieval F1 score and also the average number of calling LLM during traversal to measure the cost (the lower, the better). The best score is in bold and the second best is underlined. ing (BGE), 9.94% higher than RAPTOR, 3.08% higher than HippoRAG, 1.11% higher than SiR- eRAG. This illustrates the strengths of HopRAG in capturing both textual similarity and logical rela- tions for handling multi-hop QA. Specifically, BM25, BGE and BGE with query decomposition yield unsatisfactory results since they rely solely on similarity, and BGE with rerank- ing cannot capture logical relevance among can- didates. Since GraphRAG considers relevance among entities instead of similarity for graph search, and RAPTOR focuses on the hierarchical logical relations among passages but cannot cap- ture other kinds of relevance, both of them are more suitable for query-focused summarization but not the most competitive method for multi-hop QA tasks, as also reported in (Zhang et al., 2024). In terms of HippoRAG, it prioritizes relevance signals such as vertices with the most edges and does not explicitly model similarity while our de- sign HopRAG directly integrates similarity with logical relations when constructing edges. Al- though HopRAG only outperforms SiReRAG by a small margin in the scenario with top 20 candidate passages, our general graph structure does not in- troduce additional summary and proposition aggre- gate nodes and can facilitate efficient graph traver- sal for faster retrieval compared with SiReRAG. In the discussion, we will show that HopRAG can achieve competitive results with a smaller context length. Besides quantitative scores, we also demon- strate a case study in Appendix A.6 comparing HopRAG and GraphRAG. 4.3 Ablations and Discussion To confirm the robustness of HopRAG and provide more insights, we vary topk, nhop and conduct ablation studies on traversal model. Effects of topk To show our efficiency in faster hop from indirectly relevant passages to truly rel- evant ones, we test the robustness by evaluating both the QA and retrieval performance on GPT-3.5- turbo with smaller topk, as is shown in Table 3. From the results, we find that even with top 12 can- didates, the QA performance of HopRAG is still comparable to that of HippoRAG or RAPTOR with 20 candidates, which highlights the effectiveness of our graph traversal design in efficiently retrieving more information within a limited context length. Meanwhile, we also observe that as topk increases, the retrieval F1 score gradually decreases due to MuSiQue 2Wiki HotpotQA Average Answer Retrieval Answer Retrieval Answer Retrieval Answer Retrieval Method (Traversal Model)EM F1 F1 EM F1 F1 EM F1 F1 EM F1 F1 BM25 5.80 11.00 5.79 27.00 31.55 9.25 33.40 44.30 8.75 22.07 28.95 7.93 BGE 11.80 18.60 8.76 27.90 30.80 7.60 38.40 50.56 11.10 26.03 33.32 9.16 HopRAG (non-LLM)19.00 27.68 8.27 42.20 46.72 8.09 46.90 61.17 11.73 36.04 45.19 9.36 HopRAG (Qwen2.5-1.5B-Instruct)38.0046.73 11.91 58.4064.78 11.82 58.2074.74 18.22 51.5362.08 13.98 HopRAG (GPT-4o-mini)39.10 53.00 13.89 61.60 68.93 12.51 61.30 78.34 18.48 54.00 66.76 14.96 Table 5: We conduct an ablation study on the reasoning model during traversal with GPT-3.5-turbo as the inference model and top 20 passages. We compare 5 scenarios including sparse retriever (BM25), dense retriever (BGE), HopRAG (non-LLM), HopRAG (Qwen2.5-1.5B-Instruct) and HopRAG (GPT-4o-mini) and report both the answer and the retrieval metrics, where the best score is in bold and the second best is underlined. the inclusion of excessive redundant information. Conversely, the answer quality generally improves, attributed to GPT-3.5-turbo’s strong capability in processing and reasoning over extended contexts, with only one exception in the MuSiQue dataset. Effects of nhop To assess the effects of the hy- perparameter nhop on reasoning-augmented graph traversal, we vary nhop from 1 to 4 and evaluate the corresponding retrieval performance and cost, which is measured by the total number of LLM calls during graph traversal. The results shown in Table 4 indicate that as nhop increases, retrieval performance tends to improve, as more vertices are visited during traversal for reasoning and prun- ing. However, the expense and latency from calling LLM also increase with nhop, creating a trade-off between performance and cost. We notice that as nhop increases, the number of new vertices in Cqueue requiring LLM reasoning decays rapidly. Since different vertices may hop to the same impor- tant vertex, the actual queue length in each round of hop is less than topk. Specifically, the aver- age queue length is 2.60 in the fourth round and 1.23 in the fifth round, suggesting that for the three datasets, the local area in the graph structure can be largely explored within four rounds of hop, elimi- nating the need for an additional hop. We set nhop as 4 in Table 1 and 2. We also evaluate the answer performances as nhop varies and show the overall results in Appendix A.8. Ablation on Traversal Model In order to gener- alize HopRAG to scenarios with less computational overhead during retrieval, we supplement results from (1) HopRAG with traversal model Qwen2.5- 1.5B-Instruct (2) HopRAG with non-LLM graph traversal that replaces the reasoning phase in Algo- rithm 1 with similarity matching. Table 5 shows that even without using the reasoning ability of LLM in the graph traversal, HopRAG can achieve 45.84% higher than BM25 and 25.43% higher than dense retriever (BGE), which proves the effective- ness of HopRAG in capturing textual similarity and logical relations for logic-aware retrieval. The introduction of reasoning ability from LLM (GPT- 4o-mini) can achieve about 45.78% higher average score than the non-LLM version, and Qwen2.5- 1.5B-Instruct as traversal model produces compara- ble results with less cost and higher efficiency. We analyze the retrieval efficiency in Appendix A.7. 5 Conclusion In this paper, we introduced HopRAG, a novel RAG system with a logic-aware retrieval mecha- nism. HopRAG connects related passages through pseudo-queries, which allows identifying truly rel- evant passages within multi-hop neighborhoods of indirectly relevant ones, significantly enhancing both the precision and recall of retrieval. Extensive experiments on multi-hop QA bench- marks, i.e. MuSiQue, 2WikiMultiHopQA, and Hot- potQA, demonstrate that HopRAG outperforms conventional RAG systems and state-of-the-art baselines. Specifically, HopRAG achieved over 36.25% higher answer accuracy and 20.97% im- proved retrieval F1 score compared to conventional information retrieval approaches. It highlights the effectiveness of integrating logical reasoning into the retrieval module. Moreover, ablation studies provide insights into the sensitivity of hyperparam- eters and models, revealing trade-offs between re- trieval performance and computational costs. HopRAG paves the way toward reasoning-driven knowledge retrieval. Future work involves scaling HopRAG to broader domains beyond QA tasks; op- timizing indexing and traversal strategies for more complex scenarios with lower computation costs. Acknowledgments This work is supported by the National Key R&D Program of China (2024YFA1014003), National Natural Science Foundation of China (92470121, 62402016), CAAI-Ant Group Research Fund, and High-performance Computing Platform of Peking University. Limitations Despite the benefits of HopRAG, the current eval- uation focuses on multi-hop or multi-document QA tasks. In order to mitigate the risk of per- formance fluctuations when applying HopRAG to other datasets, we should explore its generaliza- tion capabilities across a broader range of domains. Besides, more sophisticated query simulation and edge merging strategies may lead to further im- provements. Finally, though we were inspired by the theories of six degrees of separation and small- world networks, the degree distribution of our pas- sage graph vertices does not exhibit the power-law characteristic. On the one hand, these theories only serve as the motivation and intuitive analogy; on the other hand, exploring more appropriate degree dis- tribution strategies is an interesting research topic. We leave these research problems for future work. Ethics Impact In our work, we acknowledge two key ethical con- siderations. First, we utilized AI assistant to en- hance the writing process of our paper and code. We ensure that the AI assistant was used as a tool to improve clarity and conciseness, while the final content and ideas were developed and reviewed by human authors. Second, we employed multi- ple open source datasets and one open source tool Neo4j Community Edition (Webber, 2012) under GPL v3 license in our experiments. We are trans- parent about their origin and limitations, and we respect data ownership and user privacy. References Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Bin- bin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. 2022. Towards tracing knowledge in language models back to the training data. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2429–2446, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Akari Asai, Zeqiu Wu, Yizhong Wang, et al. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arxiv:2310.11511. Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. arXiv preprint. ArXiv:2310.05029. Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 38, pages 17754–17762. Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhen- hua Dong, and Jun Xu. 2024. Unifying bias and unfairness in information retrieval: A survey of chal- lenges and opportunities with large language models. arXiv preprint arXiv:2404.11457. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024a. From Local to Global: A Graph RAG Approach to Query-Focused Summa- rization. arXiv preprint. ArXiv:2404.16130 [cs]. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024b. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130. Masoomali Fatehkia, Ji Kim Lucas, and Sanjay Chawla. 2024. T-rag: Lessons from the llm trenches. Preprint, arXiv:2402.07483. Thibault Févry, Livio Baldini Soares, et al. 2020. En- tities as experts: Sparse memory access with entity supervision. In EMNLP. John Guare. 2016. Six degrees of separation. In The Contemporary Monologue: Men, pages 89–93. Rout- ledge. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: In- centivizing reasoning capability in llms via reinforce- ment learning. arXiv preprint arXiv:2501.12948. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. LightRAG: Simple and Fast Retrieval-Augmented Generation. arXiv preprint. ArXiv:2410.05779. Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michi- hiro Yasunaga, and Yu Su. 2025. Hipporag: Neu- robiologically inspired long-term memory for large language models. Preprint, arXiv:2405.14831. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020a. Retrieval augmented language model pre-training. In International confer- ence on machine learning, pages 3929–3938. PMLR. Kelvin Guu, Kenton Lee, Zora Tung, et al. 2020b. REALM: retrieval-augmented language model pre- training. ICML. Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V . Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval- Augmented Generation for Textual Graph Under- standing and Question Answering. arXiv preprint. ArXiv:2402.07630. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. Preprint, arXiv:2011.01060. Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In EACL. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lu- cas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with re- trieval augmented language models. arXiv preprint arXiv:2208.03299. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richard- son, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Zhengbao Jiang, Frank F Xu, Luyu Gao, et al. 2023. Active retrieval augmented generation. arXiv:2305.06983. Karen Sparck Jones. 1973. Index term weighting. In- formation storage and retrieval, 9(11):619–633. Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. 2023. Knowledge graph-augmented language models for knowledge-grounded dialogue generation. Preprint, arXiv:2305.18846. Vladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense passage retrieval for open-domain question answering. Preprint, arXiv:2004.04906. Jon Kleinberg. 2000. The small-world phenomenon: An algorithmic perspective. In Proceedings of the thirty-second annual ACM symposium on Theory of computing, pages 163–170. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock- täschel, et al. 2020a. Retrieval-Augmented Genera- tion for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems, 33:9459– 9474. Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, et al. 2020b. Retrieval-augmented generation for knowledge-intensive NLP tasks. In NeurIPS. Zijian Li, Qingyan Guo, Jiawei Shao, Lei Song, Jiang Bian, Jun Zhang, and Rui Wang. 2024. Graph Neural Network Enhanced Retrieval for Question Answering of LLMs. arXiv preprint. ArXiv:2406.06572. Xun Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Hanyu Wang, Feiyu Xiong, Jason Zhaoxin Fan, Bo Tang, Shichao Song, Mengwei Wang, et al. 2025. Saferag: Benchmarking security in retrieval-augmented gen- eration of large language model. arXiv preprint arXiv:2501.18636. Xun Liang, Simin Niu, Sensen Zhang, Shichao Song, Hanyu Wang, Jiawei Yang, Feiyu Xiong, Bo Tang, Chenyang Xi, et al. 2024. Empowering large lan- guage models to set up a knowledge retrieval indexer via self-learning. arXiv preprint arXiv:2405.16933. Costas Mavromatis and George Karypis. 2024. Gnn- rag: Graph neural retrieval for large language model reasoning. arXiv preprint arXiv:2405.20139. Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen- tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2023. Nonparametric masked language modeling. In Findings of the Association for Computational Linguistics: ACL 2023, pages 2097–2118, Toronto, Canada. Association for Computational Linguistics. Sewon Min, Victor Zhong, Luke Zettlemoyer, and Han- naneh Hajishirzi. 2019. Multi-hop reading compre- hension through question decomposition and rescor- ing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6097–6109, Florence, Italy. Association for Compu- tational Linguistics. Rodrigo Nogueira and Kyunghyun Cho. 2020. Passage re-ranking with bert. Preprint, arXiv:1901.04085. Qwen Development Team. 2025. Qwen 2.5 Speed Benchmark. https://qwen.readthedocs.io/en/ stable/benchmark/speed_benchmark.html. Ac- cessed: 2025-5-4. Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented lan- guage models. arXiv preprint arXiv:2302.00083. Stephen Robertson and Hugo Zaragoza. 2009a. The Probabilistic Relevance Framework: BM25 and Be- yond. Foundations and Trends® in Information Re- trieval, 3(4):333–389. Stephen E. Robertson and Hugo Zaragoza. 2009b. The probabilistic relevance framework: BM25 and be- yond. FTIR, 3(4):333–389. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. 2024. RAPTOR: Recursive Abstractive Process- ing for Tree-Organized Retrieval. arXiv preprint. ArXiv:2401.18059 [cs]. Parth Sarthi, Salman Abdullah, Aditi Tuli, et al. 2023. Raptor: Recursive abstractive processing for tree- organized retrieval. In ICLR. Kunal Sawarkar, Abhilasha Mangal, and Shivam Raj Solanki. 2024. Blended rag: Improving rag (retriever-augmented generation) accuracy with se- mantic search and hybrid query-based retrievers. arXiv preprint arXiv:2404.07220. Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. 2024. Scaling retrieval-based lan- guage models with a trillion-token datastore. arXiv preprint arXiv:2407.12854. Karthik Soman, Peter W Rose, John H Morris, Ra- bia E Akbas, Brett Smith, Braian Peetoom, Catalina Villouta-Reyes, Gabriel Cerono, Yongmei Shi, An- gela Rizk-Jackson, Sharat Israni, Charlotte A Nel- son, Sui Huang, and Sergio E Baranzini. 2024. Biomedical knowledge graph-optimized prompt generation for large language models. Preprint, arXiv:2311.17330. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, et al. 2024. jina-embeddings-v3: Mul- tilingual embeddings with task lora. arXiv preprint arXiv:2409.10173. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S Siegel, Michael Tang, et al. 2024a. Bright: A realistic and challenging bench- mark for reasoning-intensive retrieval. arXiv preprint arXiv:2407.12883. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, and Tao Yu. 2024b. Bright: A realistic and challenging bench- mark for reasoning-intensive retrieval. Preprint, arXiv:2407.12883. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multi- hop questions via single-hop question composition. Preprint, arXiv:2108.00573. Christos V oudouris, Edward PK Tsang, and Abdullah Alsheddy. 2010. Guided local search. In Handbook of metaheuristics, pages 321–361. Springer. Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, and Sercan Ö Arık. 2024a. Astute rag: Overcom- ing imperfect retrieval augmentation and knowledge conflicts for large language models. arXiv preprint arXiv:2410.07176. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024b. Multilin- gual e5 text embeddings: A technical report. arXiv preprint arXiv:2402.05672. Zhengren Wang, Qinhan Yu, Shida Wei, Zhiyu Li, Feiyu Xiong, Xiaoxing Wang, Simin Niu, Hao Liang, and Wentao Zhang. 2024c. QAEncoder: Towards Aligned Representation Learning in Question An- swering System. arXiv preprint. ArXiv:2409.20434 [cs]. Jim Webber. 2012. A programmatic introduction to neo4j. In Proceedings of the 3rd Annual Conference on Systems, Programming, and Applications: Soft- ware for Humanity , SPLASH ’12, page 217–218, New York, NY , USA. Association for Computing Machinery. Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. 2024. Certifiably robust rag against retrieval corruption. arXiv preprint arXiv:2405.15556. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen- nighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack: Packaged resources to advance general chinese em- bedding. Preprint, arXiv:2309.07597. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answer- ing. Preprint, arXiv:1809.09600. Qinhan Yu, Zhiyou Xiao, Binghui Li, Zhengren Wang, Chong Chen, and Wentao Zhang. 2025. Mramg- bench: A beyondtext benchmark for multimodal retrieval-augmented multimodal generation. arXiv preprint arXiv:2502.04176. Nan Zhang, Prafulla Kumar Choubey, Alexander Fab- bri, Gabriel Bernadett-Shapiro, Rui Zhang, Prasenjit Mitra, Caiming Xiong, and Chien-Sheng Wu. 2024. Sirerag: Indexing similar and related information for multihop reasoning. Preprint, arXiv:2412.06206. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023. Siren’s Song in the AI Ocean: A Survey on Hallucina- tion in Large Language Models. arXiv preprint . ArXiv:2309.01219 [cs]. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024. Retrieval- augmented generation for ai-generated content: A survey. arXiv preprint arXiv:2402.19473. Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024. Poisonedrag: Knowledge poisoning at- tacks to retrieval-augmented generation of large lan- guage models. arXiv preprint arXiv:2402.07867. A Appendix Contents A.1 Symbols . . . . . . . . . . . . . . 12 A.2 Datasets . . . . . . . . . . . . . . 12 A.3 Metrics . . . . . . . . . . . . . . 12 A.4 Settings . . . . . . . . . . . . . . 12 A.5 Prompts . . . . . . . . . . . . . . 13 A.6 Case Study . . . . . . . . . . . . 13 A.7 Retrieval Efficiency . . . . . . . . 14 A.8 Discussion Results on nhop . . . . 15 A.1 Symbols The symbols and their corresponding meanings are listed in Table 6. A.2 Datasets Table 7 shows the basic statistics of our datasets with their corresponding passage pool. Compared with knowledge graph, our graph-structured index is less dense and more efficient to construct. Since we put each passage text in the vertex, we can use fewer vertices to cover all the passages, which lowers the space complexity of the database. The average number of directed edges for each vertex is only 5.87, which lowers the time complexity for graph traversal. A.3 Metrics In our experiment, we mainly report the answer exact match (EM) and F1 score to compare all the methods. Exact Match (EM) The Exact Match (EM) met- ric measures the percentage of predictions that match any one of the ground truth answers exactly. It is defined as: EM = |{p | p = g}| |P | where p denotes a predicted answer, g denotes the corresponding ground truth answer, and P is the set of all the predictions. F1 Score The F1 score is the harmonic mean of precision and recall, which measures the average overlap between the prediction and ground truth answer. Precision P and recall R are defined as: P = |A ∩ ˆA| | ˆA| , R = |A ∩ ˆA| |A| where |A ∩ ˆA| refers to the number of matching to- kens between the prediction ˆA and the ground truth A, and | ˆA|, |A| denote the number of tokens in the predicted and ground truth answers, respectively. The F1 score is then computed as: F 1 = 2 · P · R P + R In our ablation study, we also report the retrieval F1 score to test the sensitivity of HopRAG, which is calculated as follows. The Precision (P) and Recall (R) for retrieval are computed as: P = |Ret ∩ Rel| |Ret| , R = |Ret ∩ Rel| |Rel| where Ret represents the set of passages retrieved during retrieval, and Rel denotes the set of relevant passages that support the ground truth answer. The Retrieval F1 score is then calculated as the harmonic mean of precision and recall: F 1retrieval = 2 · P · R P + R A.4 Settings To avoid semantic loss by chunking the documents at a fixed size, we chunk each document in a way corresponding to the supporting facts of each dataset. Specifically, we chunk each document in HotpotQA and 2WikiMultiHopQA by sentence since the smallest unit of these two datasets’ sup- porting facts is a sentence. To get embedding rep- resentation for each chunk, we use bge-base model. To extract keywords, we use the part-of-speech tag- ging function of Python package PaddleNLP to extract and filter entities. In our method, we use the Neo4j graph database to store vertices and build edges. When building edges we employ prompt engineering technique to instruct the LLM to gen- erate an appropriate number of questions for each vertex to cover its information, with a minimum requirement of at least 2 in-coming questions and 4 out-coming questions. To prevent the graph struc- ture from becoming overly complex and dense, we retain only O(n · log(n)) edges, where n is the number of vertices. We use GPT-4o-mini for reasoning-augmented graph traversal, GPT-4o and GPT-3.5-turbo for inference with 2048 max tokens and 0.1 temperature in our main experiments. For sparse and dense retrievers, we use the Neo4j database to conduct retrieval on the vertices. With this setting, we align the retrieval engine for un- structured baselines with HopRAG to fairly demon- strate the effectiveness of our graph structure index. symbol meaning symbol meaning P passage corpus Q+ i set of out-coming questions forpi p passage q+ i,j the j-th out-coming question forpi q query Q− i set of in-coming questions forpi C retrieval context q− i,j the j-th in-coming question forpi P(·|·) LLM distribution K+ i set of keywords forQ+ i O response forq k+ i,j keywords forq+ i,j G graph K− i set of keywords forQ− i V set of vertices k− i,j keywords forq− i,j v vertex V + i set of embeddings forQ+ i E set of directed edges v+ i,j embedding forq+ i,j ei,j directed edge fromvi to vj V − i set of embeddings forQ− i kq keywords forq v− i,j embedding forq− i,j vq embedding forq R+ i set of out-coming triplets Ccount counter of vertices during traversalr+ i,j out-coming triplet forq+ i,j Cqueue queue of vertices during traversal R− i set of in-coming triplets H helpfulness metric r− i,j in-coming triplet forq− i,j topk context budget nhop number of hop Table 6: Table of symbols and meanings. dataset number docs supporting facts vertices edges avg text length avg edge number MuSiQue 1000 19990 2800 13086 81348 489.52 6.22 2Wiki 1000 10000 2388 23360 167068 116.07 7.15 HotpotQA 1000 9942 2458 40534 171946 132.44 4.24 Average 1000 13311 2549 25660 140121 246.01 5.87 Table 7: Dataset Statistics. We report the basic statistics of the graph structure on different datasets and demonstrate that our efficient graph structure is traversal-friendly. For query decomposition, we use GPT-4o-mini to break down the query into multiple sub-queries, each of which should be a single-hop query. With m sub-queries we conduct dense retrieval with BGE for each sub-query to get topk/m candidates independently and combine them all for final con- text. For reranking baseline, we use bge-reranker- base to rank2∗topk candidates from dense retriever BGE and keep the topk ones as the final context. The structured baseline methods rely on specific open-source projects according to their papers. A.5 Prompts The prompt used for generating in-coming ques- tions is shown in Figure 4. The prompt used for generating out-coming questions is shown in Fig- ure 5. The prompt for reasoning-augmented graph traversal is shown in Figure 8. A.6 Case Study We demonstrate the graph structure in Figure 7(a), one example edge with two vertices in Figure 6(a). Using the query "Donnie Smith who plays as a left back for New England Revolution belongs to what league featuring 22 teams?" as an example we conduct a qualitative analysis. For this multi-hop question (correct answer: Major League Soccer), the HotpotQA corpus contains three relevant sen- tences: (1) "Donald W. Donnie Smith (born Decem- ber 7, 1990 in Detroit, Michigan) is an American soccer player who plays as a left back for New England Revolution in Major League Soccer."; (2) "Major League Soccer (MLS) is a men’s profes- sional soccer league, sanctioned by U.S. Soccer, that represents the sport’s highest level in both the United States and Canada." and (3) "The league comprises 22 teams in the U.S. and 3 in Canada." With dense retriever (BGE), we can easily re- trieve the first sentence but the last two facts can’t be retrieved in the context even with a topk of 30. However, in our graph-structured index, these three vertices are logically connected, as is shown in Fig- ure 6(b). During the traversal, LLM starts from the Figure 4: Prompt for generating in-coming questions. semantically similar but indirectly relevant vertices and can reach all the supporting facts within only a maximum of 3 hops. Using this query and its passages for demonstra- tion, we provide the visualizations of HopRAG’s graph index against GraphRAG to help readers grasp the differences and innovations. As shown in Figure 7 , the main differences between HopRAG and GraphRAG are listed as follows. • Vertices. GraphRAG uses LLM to summarize the information from text chunks and then cre- ates additional vertices (e.g., event, organiza- tion and person) in the graph, while HopRAG directly stores the original chunks in the ver- tices and thus avoids LLM hallucination dur- ing summarization, information loss during entity extraction and overly dense graph struc- ture from redundant vertices. • Edges. GraphRAG connects vertices with pre-defined relationships like "part of" or "related", while HopRAG flexibly stores in- coming questions on the edges along with their keywords and embeddings, which can not only guide reasoning-augmented graph traversal but also facilitate edge retrieval. • Index. GraphRAG creates and stores em- beddings for the summarizations from LLM, while HopRAG creates sparse and dense in- dexes for both the vertices and the edges, which leads to more precise and efficient in- formation retrieval. In summary, the graph structure of HopRAG not only excavates logical relationships without creat- ing additional vertices, but also paves the way for reasoning-driven knowledge retrieval. A.7 Retrieval Efficiency We supplement more comprehensive analysis of retrieval efficiency, along with optimization strate- gies for further speedup. The main latency in Ho- pRAG’s retrieval comes from the LLM inference time during retrieval-augmented graph traversal. Since HopRAG with locally deployed Qwen2.5- 1.5B-Instruct as the traversal model also showcases competitive performances, we focus on the retrieval efficiency in this scenario. Following the hyper- Figure 5: Prompt for generating out-coming questions. parameters nhop = 4 and topk = 20 from our main experiments, each question requires calling LLM 38.53 times, where each LLM call involves selecting one edge from an average of 5.87 edges, with an input of around 500 tokens and an output of 20 tokens. According to (Qwen Development Team, 2025), the output token speed for Qwen2.5- 1.5B-Instruct is about 40.86 token/s using BF16 and Transformer. Therefore, the additional latency for each question from LLM inference will be 38.53∗20/40.86 = 18 .86 seconds. However, there are many optimization strategies to improve the re- trieval efficiency. Using vLLM and GPTQ-Int4 techniques, the additional latency for each question can be reduced to 38.53 ∗ 20/174.04 = 4 .43 sec- onds. Moreover, parallelism techniques like mul- tithreading can further reduce the total execution time for all the queries. A.8 Discussion Results on nhop In the discussion we notice that as the hyper- parameter nhop varies from 1 to 4, the answer and retrieval performance both increase, along with the retrieval cost of calling LLM during traversal. Since the average queue length in the fifth hop is only as small as 1.23, we believe 4 is the idealnhop. The overall results are shown in Table 8. (a) Demonstration of one edge between two vertices. (b) Demonstration of reasoning- augmented graph traversal. Figure 6: Demonstration of the traversal in the graph structure (a) Demonstration of HopRAG’s graph. (b) Demonstration of GraphRAG’s graph. Figure 7: Visualizations of HopRAG and GraphRAG MuSiQue 2Wiki HotpotQA Average Answer Retrieval Answer Retrieval Answer Retrieval Answer Retrieval nhop EM F1 F1 Cost EM F1 F1 Cost EM F1 F1 Cost EM F1 F1 Cost 1 21.50 30.77 8.78 20.00 48.60 52.44 8.68 19.86 47.90 62.92 6.78 19.91 39.33 48.71 8.08 19.92 2 32.00 43.75 11.86 30.32 54.90 60.37 11.42 31.52 55.90 71.26 15.13 29.39 47.60 58.46 12.80 30.41 3 32.50 44.50 12.67 37.28 52.90 59.16 11.97 37.15 57.40 73.86 16.76 33.35 47.60 59.17 13.80 35.93 4 39.10 53.00 13.89 40.32 61.60 68.93 12.51 40.12 61.30 78.34 18.48 35.14 54.00 66.76 14.96 38.53 Table 8: We test the effect of hyperparameter nhop on HopRAG using GPT-3.5-turbo with top 20 passages. We vary nhop from 1 to 4 and report both the answer and retrieval metrics. For answer metrics, we report the answer EM and F1 score; For retrieval metrics, we report the F1 score and average number of calling LLM during traversal to measure the cost (the lower, the better). The best score is in bold and the second best is underlined. Figure 8: Prompt for reasoning-augmented graph traversal.