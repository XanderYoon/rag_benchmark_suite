FiCo-ITR: bridging Ô¨Åne-grained and coarse-grained image-text retrieval for comparative performance analysi s Mikel Williams-Lekuona and Georgina Cosma Computer Science Department, Loughborough University, Ep inal Way, Loughborough, LE11 3TU, Leicestershire, United Kingdom. *Corresponding author(s). E-mail(s): m.williams@lboro.ac.uk; Contributing authors: g.cosma@lboro.ac.uk; Abstract In the Ô¨Åeld of Image-Text Retrieval (ITR), recent advanceme nts have leveraged large-scale Vision- Language Pretraining (VLP) for Fine-Grained (FG) instance -level retrieval, achieving high accuracy at the cost of increased computational complexity. For Coar se-Grained (CG) category-level retrieval, prominent approaches employ Cross-Modal Hashing (CMH) to p rioritise eÔ¨Éciency, albeit at the cost of retrieval performance. Due to diÔ¨Äerences in methodologi es, FG and CG models are rarely com- pared directly within evaluations in the literature, resul ting in a lack of empirical data quantifying the retrieval performance-eÔ¨Éciency tradeoÔ¨Äs between the two. This paper addresses this gap by introduc- ing the FiCo-ITR library, which standardises evaluation methodologies for both FG and CG models, facilitating direct comparisons. We conduct empirical eva luations of representative models from both subÔ¨Åelds, analysing precision, recall, and computational complexity across varying data scales. Our Ô¨Åndings oÔ¨Äer new insights into the performance-eÔ¨Éciency tr ade-oÔ¨Äs between recent representative FG and CG models, highlighting their respective strengths and limitations. These Ô¨Åndings provide the foundation necessary to make more informed decisions regar ding model selection for speciÔ¨Åc retrieval tasks and highlight avenues for future research into hybrid systems that leverage the strengths of both FG and CG approaches. Keywords: Cross-Modal Retrieval, Fine-Grained, Coarse-Grained, Vi sion-Language Pretraining, Cross-modal hashing 1 Introduction Cross-Modal Retrieval (CMR) involves using one type of data, such as text, to search for another type of data, such as images. Unlike general multi-modal tasks, CMR speciÔ¨Åcally focuses on bridging the gap between diÔ¨Äerent modalities to enable retrievals across modalities. CMR has gained prominence over the past decade due to its success in various applications, including e- commerce [ 1, 2], content-based retrieval [ 3, 4], video surveillance [ 5], and recommendation sys- tems [ 6]. When the retrieval task speciÔ¨Åcally involves images and text, it is referred to as Image-Text Retrieval (ITR). There are two dis- tinct subÔ¨Åelds within ITR: Fine-Grained (FG) and Coarse-Grained (CG) ITR. FG ITR aims to Ô¨Ånd instance-level matches, retrieving the image that directly corresponds to a detailed text query, and vice versa. State-of- the-art FG methods employ large-scale Vision- Language Pretraining (VLP) followed by retrieval 1 arXiv:2407.20114v3 [cs.IR] 16 Jan 2026 Ô¨Åne-tuning, often through the use of contrastive learning techniques [ 7]. CG ITR focuses on category-level retrieval, where the retrieved samples should broadly belong to the semantic category which the query is searching for, rather than aiming for speciÔ¨Åc exact matches. State-of-the-art CG methods implement Cross-Modal Hashing [ 8], which train hash func- tions to map image and text samples onto a common Hamming subspace for eÔ¨Écient bitwise similarity comparisons. Figure 1 illustrates the dif- ferent search criteria between FG and CG search approaches. Despite sharing the same overarching retrieval task, the subÔ¨Åelds of FG and CG ITR have evolved independently with limited integration, leading to the following three major challenges: 1) Because of methodological diÔ¨Äerences between the two subÔ¨Åelds, obtaining informative quanti- tative results which are directly comparable is a non-trivial task. 2) Researchers in one subÔ¨Åeld rarely benchmark against works from the other in a comprehensive manner. Although FG and CG methods have been comparatively surveyed within the literature [ 11], direct empirical comparative evaluations of recent representative models are lacking. 3) Traditional ITR benchmark datasets such as Flickr30K [ 12] (1K sample test set) and MS-COCO [ 13] (5K sample test set) are small compared to real-world applications. Therefore, these benchmark datasets do not oÔ¨Äer a com- prehensive understanding of model performance on large-scale data, potentially skewing perceived retrieval performance-eÔ¨Éciency trade-oÔ¨Äs. The lack of standardised evaluation method- ologies and empirical comparisons between FG and CG methods has hindered the Ô¨Åeld‚Äôs under- standing of their relative strengths and limita- tions in real-world scenarios. This understanding is necessary for informing model selection deci- sions and identifying opportunities for hybrid approaches that could leverage the advantages of both methodologies. To address these critical gaps, this paper makes the following contribu- tions: 1. The implementation and distribution of the FiCo-ITR library and uniÔ¨Åed toolkit that bridges the methodological diÔ¨Äerences between the CG and FG ITR subÔ¨Åelds. This contribu- tion addresses the challenge of establishing fair comparisons by implementing carefully selected tasks, datasets, and metrics that accommo- date the distinct characteristics of each subÔ¨Åeld while maintaining evaluation consistency. The library is made available in the Python Package Index (PyPI) and GitHub, enabling researchers to conduct standardised evaluations across both subÔ¨Åelds, facilitating more informed com- parisons and potential cross-pollination of ideas between the two approaches. 2. A systematic empirical evaluation that quan- tiÔ¨Åes the trade-oÔ¨Äs between CG and FG approaches across multiple aspects of perfor- mance: Recall, precision, encoding time, stor- age costs, query-time attention costs and simi- larity search time. This evaluation provides the Ô¨Årst comprehensive empirical basis for under- standing the relative strengths and limitations of representative approaches from both sub- Ô¨Åelds, addressing the current lack of direct comparative analysis in the literature. 3. Scalability experiments that evaluate model performance using incrementally larger retrieval sets, quantifying the computational costs of large-scale ITR. These experiments reveal critical trade-oÔ¨Äs between retrieval per- formance and computational eÔ¨Éciency that only become apparent at scale, providing practical guidance for real-world deployment decisions. Section 2 provides an overview of FG and CG ITR, highlighting key trends and major works. Section 3 outlines the implementation of the proposed FiCo-ITR library. Section 4 details the experiment methodology for the conducted evaluations. Section 5 employs the library to jointly evaluate representative FG and CG mod- els. Section 6 discusses the key Ô¨Åndings, providing recommendations for the use-cases of FG and CG ITR. Section 7 concludes the paper. 2 Related works 2.1 Fine-grained image-text retrieval FG methods aim to map visual and textual infor- mation to a joint space where relevant samples are aligned at the instance level. The alignment of samples is achieved through learning shared 2 %HHRU )ORZHU" %HHRU )ORZHU" %HHRU )ORZHU" )LQH*UDLQHG6HDUFKDQG(YDOXDWLRQ4XHU\7H[W 5DQN 5DQN 5DQN ,PDJH,'  ,PDJH,'  ,PDJH,'  &RDUVH*UDLQHG6HDUFKDQG(YDOXDWLRQ 5DQN 5DQN 5DQN &DWHJRU\/DEHO %HH)ORZHU &DWHJRU\/DEHO %HH&DU &DWHJRU\/DEHO %XWWHUIO\ ,PDJH 6HDUFK 5HVXOWV 6HDUFK 5HVXOWV ,VLWUHOHYDQW" ,VLWUHOHYDQW" &RDUVH*UDLQHG 0RGHO 5HOHYDQW,PDJH,'  4XHU\ V5HOHYDQFH0HWDGDWD 5HOHYDQW&DWHJRU\/DEHOV %HH)ORZHU >  @ >  @ +DVK&RGHV &RQWLQXRXV (PEHGGLQJV (QFRGLQJ &RPSXWDWLRQDO&RVWV ,' " ,' " ,' " )LQH*UDLQHG5HOHYDQFH &RDUVH*UDLQHG5HOHYDQFH $EHHO\LQJRQDIORZHU )LQH*UDLQHG(QFRGLQJ &RDUVH*UDLQHG(QFRGLQJ )LQH*UDLQHG 0RGHO Fig. 1 : Comparison of Fine-Grained (FG) and Coarse-Grained (CG) Image -Text Retrieval approaches. FG search uses continuous embeddings, aiming to Ô¨Ånd the retrieval sample that speciÔ¨Åcally corresponds to the query sample. Under evaluation conditions, this involves Ô¨Åndin g the retrieval sample with the same ID as the query sample. CG search employs bitwise hash codes t o Ô¨Ånd retrieval samples that are more broadly relevant to the query instead of exact matches. Dur ing evaluation, this involves Ô¨Ånding any retrieval sample with at least one matching category label relat ive to the query‚Äôs relevant category labels. The broader search criteria of CG search allows for more eÔ¨Éc ient computational costs, as seen in the comparison of encoding time and embedding storage costs of tw o representative FG and CG models (IMRAM [ 9] and UCCH [ 10]) feature representations from image-text pairs dur- ing training. Establishing the joint space enables retrieval across modalities, where the relative dis- tance of items in the shared space determines their relevance. While FG methods share the same fun- damental principles as CG ones, they diÔ¨Äer in the level of scrutiny with which the processed sam- ples are analysed. SpeciÔ¨Åcally, FG methods focus on low-level features and object-level relationships to achieve an in-depth understanding of the scene within a given sample. The format of the encoded samples is typically real-valued continuous embed- dings. These embeddings enable calculating the similarity between samples using vector distance measurements such as Cosine distance [ 14]. T raditional Ô¨Åne-grained methods. Early deep learning-based FG methods typically employed Long Short-Term Memory (LSTM) networks for text encoding and convolutional neural networks (CNNs) such as AlexNet and VGGNet for image feature extraction [ 15]. VSE++ [ 14] built on this approach by experi- menting with VGG19 [ 16] and ResNet152 [ 17] for image encoding along with a GRU-based text encoder and implementing hard negative mining and reranking loss functions. SCAN [ 18] achieved a breakthrough in performance through its proposed bottom-up attention module align- ing image regions and words based on Faster R-CNN object detection [ 19]. CAMP [ 20] aggre- gates salient messages between image regions and words via attention to handle negative pairs before directly predicting matching scores. VSRN [ 21] introduced hierarchical reasoning to capture region-concept interactions and was later upgraded to VSRN++ [ 22] using BERT [ 23] text features. KASCE [ 24] expands image concepts using ‚Äúcommon-sense‚Äù relationships from scene graphs and selects the most relevant expansions. CAAN [ 25] employs context-aware attention to selectively attend to fragments based on inter- and intra-modal semantics. IMRAM [ 9] progressively aligns fragments through a recurrent attention unit to capture diÔ¨Äerent semantics at each step, alongside a memory unit to accumulate cues. 3 Fine-grained vision-language pretrain- ing methods. Following the success of the attention mechanism in BERT [ 23], the trans- former architecture has been extensively adapted for vision-language tasks. Moreover, Ô¨Årst pio- neered by the model ViLT [ 26], state-of-the-art performance is achieved by using transformer encoder stacks end-to-end, without the need for additional pre-processing steps such as object detection feature extraction predominant in tra- ditional FG models. This shift most notably involves leveraging pretraining on large-scale data to obtain task-agnostic features for subsequent Ô¨Åne-tuning onto downstream tasks such as ITR. Initial VLP methods are categorised into two main encoder architectures: Dual-encoder and fusion-encoder architectures. Dual-encoders, such as ALIGN [ 27] and BEIT-3 [ 28], employ separate image and text encoders, allowing the indepen- dent computation and oÔ¨Ñine storage of embed- dings for each modality. This approach eliminates the need to compute embeddings at query time. Fusion-encoder models, such as UNITER [ 29] and METER [ 30], enhance interactivity between modalities by employing a uniÔ¨Åed encoder that jointly processes image and text inputs. Although the fusion encoder approach may potentially lead to higher-quality embeddings, the embeddings must be computed at query time due to the requirement to process all possible query/retrieval sample combinations jointly. Building on these two approaches, models such as X2-VLM [ 31] and BLIP-2 [ 32] adopt a hybrid methodology. These hybrid methods Ô¨Årst leverage a dual-encoder step to independently compute embeddings to boost eÔ¨Éciency. Then, to further improve the embedding quality, they employ a fusion-encoder reranking step that allows for modality interaction. SpeciÔ¨Å- cally, the top candidates retrieved from the initial dual-encoder step are reranked using the fusion- encoder during query time. EÔ¨Éciency in FG ITR. To address eÔ¨É- ciency challenges inherent to FG ITR, recent approaches have aimed to optimise inference speeds by proposing lightweight implementa- tions of the dual-encoder architecture. Lightning- DOT [ 33] tackles this by simplifying pre-training tasks, maximising the amount of computations that can be done oÔ¨Ñine, and promoting the use of lightweight encoders. VLDeformer [ 34] pro- poses a two-stage retrieval process, Ô¨Årst using a transformer learning stage followed by more eÔ¨É- cient indexing-based retrieval in the second stage. HiVLP [ 35] uses coarse-grained screening as a Ô¨Årst step, followed by a Ô¨Åne-grained rerank in the second step. However, rather than a hash-based approach, HiVLP generates features for the coarse step using early layers of the transformer stack, whereas fully inferred features from later layers are used for the Ô¨Åne-grained rerank. 2.2 Coarse-grained image-text retrieval CG methods, similarly to FG methods, also aim to learn joint visual and textual representations. However, in contrast to the instance-level align- ment of FG methods, CG methods aim to align relevant samples at a broader semantic category level. By using a broader criterion, CG methods can place more emphasis on computational eÔ¨É- ciency. The most prominent approach within CG ITR is Cross-Modal Hashing (CMH) [ 8], which is primarily characterised by the use of bit hash codes to represent their encoded data. The use of hash codes aims for lower storage costs and faster retrieval speeds, due to the bit hash format being inherently lightweight and the associated bitwise operations being less computationally complex than continuous embedding operations. Recent CMH methods use deep neural networks to learn hash functions that map image and text samples to binary hash codes. During retrieval, the hash codes are compared using Hamming distance, an eÔ¨Écient similarity measure for bit strings which counts the number of diÔ¨Äering bits between two equal-length bit hash codes. Supervised cross-modal hashing. Super- vised CMH methods leverage multi-category labelling to train the hash function for each modality. DCMH [ 36] pioneered this approach by proposing an end-to-end deep learning CMH framework. Leveraging Generative Adversarial Networks (GAN), SSAH [ 37] implements label information as network input to strengthen cat- egory alignment in the hash space. AGAH [ 38] uses label information directly in its loss function, implementing a multi-labeling map. DADH [ 39] adopts a weighted cosine triplet-margin con- straint for ranking-based similarity preservation. 4 DCHUC [ 40] introduces a four-step iterative opti- misation process that allows simultaneous learn- ing of uniÔ¨Åed hash codes for database samples and modality-speciÔ¨Åc hashing functions for unseen queries. DCHMT [ 41] employs a dual transformer tower network and a diÔ¨Äerentiable hashing mod- ule, enabling location-aware image encoding and continuous, gradient descent-optimised modality representation. LDSAH [ 42] integrates label-wise semantic alignment with a dissimilarity-penalising strategy using a combination of Jensen‚ÄìShannon divergence loss and attention-driven sample re- weighting. Unsupervised cross-modal hashing. Unsupervised CMH methods use image-text pair coupling information to learn the modality hash functions, avoiding the reliance on category labelling information. This makes unsupervised CMH methods more analogous to typical FG methods, which also do not rely on labelling. Unsupervised UGACH [ 43] uses GANs to exploit the underlying manifold structure of cross- modal data with a max-margin ranking loss. UDCMH [ 44] constructs self-taught deep hash functions by minimising quantisation errors while preserving nearest and farthest neighbourhood relationships. DJSRH [ 45] uses a joint seman- tics aÔ¨Énity matrix to combine neighbourhood relations from diÔ¨Äerent modalities for improved quantisation and batch-wise training eÔ¨Éciency. DSAH [ 46] employs a semantic-alignment loss function with auto-encoder-based hash code generation. JDSH [ 47] proposes a similarity decision and weighting approach which uses a threshold-based weighting scheme to increase the discrimination of hash codes. DGCPN [ 40] uses a graph-based framework with nodes representing related data and employs a conditional proba- bility model to evaluate the coherence between neighbouring nodes. ADV [ 48] adopts Ô¨Åne-grained learning objectives for a two-step instance-level retrieval task where shorter hash code embeddings perform initial screening for subsequent reranking using longer hash codes. UCCH [ 10] uses con- trastive learning with a momentum optimiser and introduces a cross-modal ranking learning loss to address binary-continuous relaxation challenges and mitigate the impact of false-negative pairs. 3 Proposed library and toolkit We propose FiCo-ITR, a comprehensive library and toolkit that uniÔ¨Åes the evaluation of the FG and CG ITR subÔ¨Åelds, facilitating direct compar- isons for both image-to-text (i ‚Üí t) and text- to-image (t ‚Üí i) retrieval tasks. As illustrated in Figure 2, the library‚Äôs framework comprises Ô¨Åve key components: 1) Data Pre-Processing, 2) Model Encoding, 3) Similarity Metrics, 4) Retrieval Tasks, and 5) Evaluation Metrics, form- ing a complete pipeline for image-text retrieval evaluation. 1) Data pre-processing. To enable both instance-level and category-level retrieval eval- uations, the selected benchmark datasets must have full-sentence captions as well as semantic category labelling. For datasets which do not have semantic category labelling, we employ the Query2Label (Q2L) [ 49] classiÔ¨Åer to label them with synthetic semantic category labels. We adopt pre-processing steps which most closely adhere to each subÔ¨Åeld‚Äôs standard approaches [ 10, 18]: For CG models, FiCo-ITR provides the toolkit to extract 4096D VGG-19 features for images and 300D Doc2Vec features for text. Images are preprocessed by resizing to VGG-19‚Äôs expected 224√ó 224 pixels and applying standard normal- isation (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]). The VGG-19 network, pretrained on ImageNet, has its Ô¨Ånal three layers removed to obtain the feature vectors. Text features are generated using Doc2Vec trained on the combined training and test captions, with minimum word count thresholding to handle vocabulary sparsity. For traditional FG models, we refer to the Bottom- Up Attention [ 50] repository for Faster R-CNN to extract 36 region proposals per image with 1024- dimensional features, whereas the raw text is given as input to each model‚Äôs speciÔ¨Åc encoder. VLP models process raw data using their respective transformer encoder stacks. 2) Model encoding interface. The frame- work provides standardised interfaces for han- dling model outputs while maintaining evaluation consistency across diÔ¨Äerent encoding approaches. While the actual encoding process is model- speciÔ¨Åc and outside the scope of our framework, FiCo-ITR implements pre-encoding dataset man- agement and post-encoding output handling. For 5 Fig. 2 : An extendable framework of the FiCo-ITR library and toolkit. The p ipeline consists of Ô¨Åve main components: 1) Data Pre-Processing, which standardises da taset handling and oÔ¨Äers optional label generation via Query2Label (Q2L) [ 49] for unlabeled datasets; 2) Model Encoding, supporting embeddin gs in the form of binary hash codes and continuous embeddings, as well as direct similarity matrices; 3) Similarity Measures, implementing four distance measures for unifor m similarity calculation; 4) Retrieval Tasks, implementing instance- and category-level retrieval for b oth (i ‚Üí t) and (t ‚Üí i); and 5) Evaluation Metrics, reporting Recall@K for instance-level retrieval and mAP@ K with P/R curves for category-level retrieval dataset management, the toolkit provides func- tionality to split samples into pre-deÔ¨Åned splits along their IDs, ensuring that when diÔ¨Äerent models encode the same test set, they process exactly the same samples in the same order. For output handling, the framework supports three types of encoded outputs: Binary hash codes from CG models (typically 64-bit to 2048-bit lengths), continuous embeddings from FG models (typi- cally 256 to 2048 dimensions), and directly com- puted similarity matrices from models employing query-time attention. This Ô¨Çexible interface design enables fair comparisons while respecting each model‚Äôs native encoding process. 3) Similarity measures. Given a set of eval- uation embeddings, we implement four similarity measures optimised for diÔ¨Äerent embedding types: ‚Ä¢ Cosine similarity for continuous embeddings, computed as the normalised dot product between vectors x and y: x¬∑y ‚à•x‚à•‚à•y‚à• , with values ranging from [-1, 1] ‚Ä¢ Euclidean similarity, transformed to a (0, 1] range through 1 / (1 + d + «´) where d is the Euclidean distance ‚àö ‚àë(x ‚àí y)2 and «´ = 10 ‚àí 8 for numerical stability ‚Ä¢ Hamming similarity for binary hash codes, com- puted as the negative normalised count of dif- fering bits: ‚àí ‚àë(x Ã∏= y)/n where n is the bit length ‚Ä¢ Inner product similarity x ¬∑y, particularly suit- able for embeddings trained with dot-product- based loss functions For traditional binary hash codes (0s and 1s), we implement Hamming distance which calcu- lates the number of diÔ¨Äering bits between two hash codes eÔ¨Éciently on CPUs. The library also supports hash codes represented as -1s and 1s, enabling the use of inner product, cosine simi- larity, and Euclidean distance. This design choice allows users to leverage GPU acceleration for hash code comparisons, potentially boosting perfor- mance in GPU-centric environments. For contin- uous embeddings, all measures except Hamming distance are applicable. 4) Retrieval tasks. Given a similarity matrix, FiCo-ITR implements two retrieval task 6 evaluations: Instance-level retrieval and category- level retrieval. For the instance-level retrieval task, the objective is to search for the retrieval sample which directly corresponds to the query sample. Given an image with multiple captions, for the (i ‚Üí t) task, retrieving any one of the multiple captions of the query image is considered a 100% recall. For the (t ‚Üí i) task, the speciÔ¨Åc image cor- responding to the query caption must be retrieved for the query to be correct. For the category-level retrieval task, the objective is to retrieve samples that share at least one of multiple semantic cate- gory labels with the query sample. This approach results in a broader deÔ¨Ånition of relevance, where a large amount of samples in the retrieval set may be considered relevant to a given query based on shared category labels. 5) Evaluation metrics. With the retrieval results produced by the retrieval tasks, for instance-level retrieval, we use recall at k (R@k) as the primary retrieval performance metric, deÔ¨Åned as follows: R@k = number of relevant items in top k results total relevant items (1) Recall is particularly suitable for instance- level retrieval as it measures whether speciÔ¨Åc, individual items have been found. For category- level retrieval, we use mean average precision at k (mAP@k) as the standard evaluation metric. Precision at k (P @k) is deÔ¨Åned as follows: P @k = number of relevant items in top k results number of retrieved items (2) For a single query, AP@ k averages the preci- sion values at all available ranks within the top k. For a query set, mAP@k computes the mean of the AP@k scores of all the queries within the query set. mAP@k is well-suited for evaluating category- level retrieval where queries typically have a large number of relevant items, as it measures a model‚Äôs ability to include many relevant results towards the top of a large retrieved set. Addition- ally, we implement the 11-point precision-recall curve for the category-level retrieval task, which plots interpolated precision at recall levels R = {0. 0, 0. 1, . . . , 1. 0}. 4 Experiment methodology The following is the experiment methodology for the comparative experiments conducted in Section 5. Datasets. We use MS-COCO and Flickr30K as primary benchmark datasets. MS-COCO con- tains 123 287 images, each with Ô¨Åve human- annotated captions and native multi-label anno- tations across 80 semantic categories, making it particularly suitable for evaluating both instance- level and category-level retrieval approaches. MS- COCO‚Äôs adoption as a standard benchmark across both FG and CG communities [ 7, 8] fur- ther enables fair cross-methodology comparisons. Flickr30K comprises 31K images, also with Ô¨Åve human-annotated captions per image, focusing on human-object interactions. To enhance the com- parability of the results, the toolkit provided through our FiCo-ITR library implements the Karpathy [ 51] split uniformly across all evalu- ated models for consistency. This split allocates 5K images for testing and 5K for validation in MS-COCO, with the remainder used for train- ing. For Flickr30K, 1K images are designated for testing and validation each, with the remaining 29K samples used for training. Due to Flickr30K not containing semantic category labelling, we employ the label generation module of FiCo-ITR enabled by Q2L [ 49] to generate semantic cate- gory labelling for it. These generated labels are available for use within the provided FiCo-ITR repository. Model selection. The model selection pro- cess considered both academic impact and archi- tectural diversity while ensuring practical repro- ducibility. From recent surveys and benchmarks [ 7, 8], we identiÔ¨Åed representative models across the key architectural approaches in ITR (as detailed in Table 1). Our model selection thus aims to span the architectural spectrum: For VLP models, we include the fusion-encoder ViLT [ 26], dual-encoder BEiT-3 [ 28], and hybrid dual+fusion rerank models BLIP-2 [ 32] and X- VLM [ 52]. Traditional FG models are represented through both local+global attention (IMRAM [ 9], SCAN [18]) and global-only attention (VSRN [ 21]) approaches, while CG models cover the main paradigms: supervised (DADH [ 39]), unsupervised (UCCH [ 10]), and quantisation-based (ADV [ 48]) approaches. Some eÔ¨Éciency-focused VLP models 7 of interest (LightningDot [ 33], VLDeformer [ 34], and HiVLP [ 35]) could not be included due to practical constraints such as inaccessible data dependencies or lack of public implementations. To partially address this gap, we implement dual- encoder-only variants of BLIP-2 and X-VLM to promote eÔ¨Éciency (BLIP-2 NF and X-VLM NF). Additionally, we evaluate ADV in two conÔ¨Ågura- tions: as a CG model with 64-bit hash codes and as a FG model with 2048-bit codes. Table 1 provides a detailed overview of the architectural character- istics and computational requirements across our model selection. Evaluation metrics. We use the following conÔ¨Ågurations of the metrics implemented in the FiCo-ITR library for our experiments: Instance- level retrieval performance is assessed using R@1, R@5, and R@10. Additionally, we explore R@50, R@100, and R@200 for the CG models to exam- ine the viability of their use case as Ô¨Årst-screening steps where their top-k results are passed to a FG model for further reranking. For category-level retrieval, we employ mAP@10, mAP@100, and mAP@N (where N is the total number of retrieval candidates), oÔ¨Äering a comprehensive view of per- formance across increasing retrieval depths while having several relevant samples available. To fur- ther illustrate the trade-oÔ¨Ä between precision and recall in category-level retrieval, we include 11- point interpolated precision-recall curves. 5 Comparative experiments 5.1 Instance-level retrieval results This experiment aims to empirically assess the comparative performance of CG and FG models in instance-level retrieval tasks. Through the use of the proposed FiCo-ITR library, the instance- level Recall@k evaluation results for the selected models on the Flickr30K and MS-COCO datasets are reported in Table 2, with additional results at higher top-k levels for the CG models being reported in Table 3. From these results, the fol- lowing observations can be made: Where CG succeeds. The model ADV in its 64-bit CG setting achieves moderate success, with R@10 scores of 75.0% for (i ‚Üí t) and 60.5% for (t ‚Üí i) retrieval. The improvement of ADV over DADH and UCCH can primarily be attributed to adopting instance-level matching as the primary objective function. In its 2048-bit hash code FG setting, ADV achieves retrieval performance com- parable to other continuous embedding-based FG models (Table 2). The extended top-k results in Table 3‚Äîwhere ADV 64bit attains R@100 scores of 96.4% and 90.2% for the (i ‚Üí t) and (t ‚Üí i) tasks‚Äîsuggest that CG models have room to be used as initial retrieval candidate screening steps, provided the eÔ¨Éciency gained by employing such a strategy outweighs the information that is lost in this initial screening step. CG limitations. The CG models DADH and UCCH were unable to properly capture instance- level relationships compared to the FG ones, as evidenced by their achieved R@1 scores (Table 2). SpeciÔ¨Åcally, on the Flickr30K dataset, DADH achieves R@1 scores of 0.1% for (i ‚Üí t) and 0.2% for (t ‚Üí i) retrieval, while UCCH achieves R@1 scores of 12.7% and 8.7%, respectively. On the larger MS-COCO dataset, DADH achieves an R@10 score of 4.7% for (i ‚Üí t) and 3.0% for (t ‚Üí i) retrieval, whereas UCCH achieves R@10 scores of 26.5% and 24.5%, respectively. This limited recall performance is primarily due to the objec- tive function of these coarse models not targeting instance-level retrieval. Model comparison and attention mecha- nisms. The recall performance diÔ¨Äerence between models employing query-time attention (BLIP-2, X-VLM, ViLT, IMRAM, SCAN) and those that do not is notable for its limited magnitude. In the case of pretrained models, the state-of-the- art fusion-reranking model BLIP-2 outperforms the dual-encoder model BEIT-3 by a moderate R@1 diÔ¨Äerence of 6.4 and 6.9 on the MS-COCO (i ‚Üí t) and (t ‚Üí i) tasks, respectively. The eÔ¨Äec- tiveness of dual-encoder architectures is further evidenced by BEIT-3 achieving superior perfor- mance across all evaluation metrics compared to the full fusion-based ViLT, demonstrating that competitive retrieval performance can be attained without extensive inter-modal interaction at query time. Similarly, among non-pretrained models, the cross-attention model IMRAM outperforms the global-representation model VSRN by a modest R@1 diÔ¨Äerence of 4.2 for the MS-COCO (i ‚Üí t) task and 1.6 for (t ‚Üí i) task. Whether these moderate improvements in retrieval performance justify the use of query-time attention is a consid- eration that the experiments in Section 5.3 aim to inform. 8 Model Approach Q-Time Attn Dim Img Feat Txt Feat DType Fine-Grained Vision-Language Pretrained BLIP-2 Dual+Fusion Fusion Rerank 256D 1408x677D 768 √ó 40D f32 BEIT-3 Dual None 768D - - f32 X-VLM Dual+Fusion Fusion Rerank 256D 1024 √ó 145D 768 √ó 40D f32 ViLT Fusion Full Fusion n/a 768x217D 768 √ó 40D f32 Fine-Grained IMRAM Local+Global Cross-Attn 1024D 1024 √ó 36D 1024 √ó 70D f64 VSRN Global None 2048D - - f64 SCAN Local+Global Cross-Attn 1024D 1024 √ó 36D 1024 √ó 70D f64 Coarse-Grained ADV Dual None 64D/2048D - - i8 UCCH Dual Hash None 64D - - i8 DADH Dual Hash None 64D - - i8 T able 1: Architectural details and computational characteristics of the evaluated image-text retrieval models. For models using query time attention, the image and text fe atures used are shown as these are necessary during query time computation for their respective att ention steps Dataset structure impact. The dataset structure can explain the superior (i ‚Üí t) retrieval performance across both datasets and all models compared to the (t ‚Üí i) task. Each image query has Ô¨Åve relevant captions to choose from, whereas each caption query has only one relevant image, making the (i ‚Üí t) task inherently easier. 5.2 Category-level retrieval results This experiment aims to empirically compare CG and FG models in category-level retrieval tasks, where their relative performance is not well- established in the literature. The category-level mAP@k evaluation results for the selected mod- els on the Flickr30K and MS-COCO datasets are reported in Table 4. The 11-point interpolated precision-recall curves of the evaluated models for both datasets are shown in Figure 3. From these results, the following observations can be made: mAP@10 performance comparison. Given the majority of the queries within both the Flickr30K and MS-COCO evaluation sets have hundreds to thousands of retrieval candidates which are considered relevant at the category level, scoring well on the mAP@10 metric is the least challenging aspect of this task. Nevertheless, the no fusion variant of BLIP-2 achieves an (i ‚Üí t) mAP@10 score of 96.2% and a (t ‚Üí i) mAP@10 score of 95.9% on the MS-COCO dataset, whereas the coarse model UCCH achieves scores of 86.2% and 86.6%, respectively (Table 4). The superior performance of FG models on the mAP@10 met- ric can be attributed to instance-level matches being retrieved at the top ranks, which inherently share category labels with the query. mAP@N challenge and model size. In contrast to the relative ease of scoring well on the mAP@10 metric, the mAP@N metric presents a more substantial challenge. Unlike instance-level retrieval, performance on this metric does not appear to align with the size of the model being used. For example, on the MS-COCO dataset, the state-of-the-art FG VLP BLIP-2 NF model achieves the second lowest category-level mAP@N score of 47.9% on the (i ‚Üí t) task, while the coarse model UCCH attains the highest score of 60.7% (Table 4). This suggests that FG low-level seman- tics may introduce noise when high-level category semantics is all that is needed for the task. Precision-recall trade-oÔ¨Äs. The precision- recall curves in Figure 3 illustrate that the CG unsupervised model UCCH and, in particular, the supervised model DADH trained with learning objectives targeting category-level retrieval better maintain their precision as the proportion of rele- vant samples to be retrieved increases. In contrast, FG models, which are not optimised for retrieving large numbers of broadly relevant samples, show a sharper decline in precision with increases in recall requirement. F usion-encoder global similarity limita- tion. The fusion-encoder approach implemented 9 Instance-Level Retrieval Task MS-COCO Flickr30K Image to Text Text to Image Image to Text Text to Image Recall@k @1 @5 @10 @1 @5 @10 @1 @5 @10 @1 @5 @10 Fine-Grained Vision-Language Pretrained BLIP-2 85.4 97.0 98.4 68.2 87.2 92.6 97.6 100.0 100.0 89.7 98.1 98.9 BLIP-2NF 74.3 94.2 97.4 63.5 86.1 91.8 90.8 99.6 99.9 85.7 97.6 99.1 BEIT-3 79.0 94.3 97.2 61.3 84.6 90.7 96.3 99.7 100.0 86.1 97.6 98.8 X-VLM 81.0 95.1 98.0 63.0 85.7 91.5 96.8 99.8 100.0 86.1 97.4 98.7 X-VLMNF 71.4 91.9 96.3 54.4 81.5 88.8 90.2 99.0 99.7 78.4 95.2 97.8 ViLT 61.6 86.3 92.7 43.0 72.7 83.1 83.8 96.8 98.6 65.2 88.7 93.6 Fine-Grained IMRAM 54.5 83.1 90.3 39.5 68.8 79.8 71.0 93.0 96.5 52.1 78.7 86.0 VSRN 50.3 79.5 87.9 37.9 58.6 79.4 70.4 89.2 93.7 53.0 77.9 85.7 SCAN 44.9 76.7 86.7 33.7 62.8 75.2 66.7 89.3 94.0 43.1 73.4 82.2 ADV2048bit - - - - - - 63.7 87.1 92.8 44.4 75.4 83.8 Coarse-Grained ADV64bit - - - - - - 33.4 63.6 75.0 23.5 49.0 60.5 UCCH 6.2 17.9 26.5 4.4 14.9 24.5 12.7 30.7 40.6 8.7 25.8 37.4 DADH 0.4 2.3 4.7 0.3 1.5 3.0 0.1 0.7 2.6 0.2 0.8 1.6 T able 2: Comparison of instance-level retrieval performance across va rious models on MS-COCO and Flickr30K datasets. Results are reported as Recall@k (k=1,5,10) fo r both (i ‚Üí t) and (t ‚Üí i) tasks. Models are grouped into three categories: Vision-Language Pre-t rained Fine-grained (VLP FG) models, non-pretrained Fine-Grained (FG) models, and Coarse-Grained (C G) models. BLIP-2 and X-VLM are additionally evaluated in a no-fusion (NF) conÔ¨Åguration. ADV was only evaluated on Flickr30K due to the required model-speciÔ¨Åc dataset preprocessing Ô¨Åles for MS-CO CO being unavailable 0.0 0.2 0.4 0.6 0.8 1 R ecall 0.4 0.5 0.6 0.7 0.8 0.9 1 P r ecision BLIP -2 (NF) BEIT -3 X - VLM(NF) V iL T IMR AM VSRN SCAN AD V(2048bit) AD V(64bit) UCCH D ADH (a) MS-COCO Image ‚Üí Text 0.0 0.2 0.4 0.6 0.8 1 R ecall 0.4 0.5 0.6 0.7 0.8 0.9 1 P r ecision (b) MS-COCO Text ‚Üí Image 0.0 0.2 0.4 0.6 0.8 1 R ecall 0.9 0.9 0.9 1.0 1.0 1 P r ecision (c) Flickr30K Image ‚Üí Text 0.0 0.2 0.4 0.6 0.8 1 R ecall 0.9 0.9 0.9 1.0 1.0 1 P r ecision (d) Flickr30K Text ‚Üí Image Fig. 3 : Precision-recall curves of the selected models for the (i ‚Üí t) and (t ‚Üí i) tasks on the MS-COCO and Flickr30K datasets by ViLT is less suitable for category-level retrieval due to both its architectural design and training approach. ViLT is trained using a binary matching approach where the model learns to distinguish correct pairings from a limited set of negative samples [ 26]. This setup trains the model to give high scores only to exact correct pairing while treating all other combinations as non-matches. However, for category-level retrieval, we need a model that can identify many relevant items that share semantic similarities with the query, not just Ô¨Ånd the single exact match. As a result, fusion- based models like ViLT, while eÔ¨Äective at speciÔ¨Åc 10 Instance Retrieval Recall@k: Flickr30K Image to Text Text to Image @50 @100 @200 @50 @100 @200 Coarse-Grained ADV64bit 93 96.4 98.2 82.9 90.2 95.7 UCCH 68.4 81.3 88.5 68.7 80.6 89.5 DADH 8.6 16.3 28.0 8.3 15.4 26.3 T able 3 : Extended top-k retrieval results (k=50, 100, 200) for CG models on the Flickr30K dataset instance-level retrieval, perform poorly at ranking more generally similar items at the category level. Dataset diÔ¨Éculty comparison. Across all models and tasks, the average mAP@N was 92.9% for Flickr30K compared to 56% for MS- COCO, indicating that the MS-COCO category- level retrieval benchmark presents a more chal- lenging task. This diÔ¨Äerence in diÔ¨Éculty can be attributed to the datasets‚Äô composition. MS- COCO features more diverse and less overlapping labels, resulting in a more complex retrieval task at the category level. In contrast, Flickr30K has a narrower focus, with a majority of samples con- taining the ‚Äòperson‚Äô label due to its emphasis on human-object interactions. As a result, Flickr30K oÔ¨Äers a more homogeneous set of images and labels, while MS-COCO provides a broader range of scenes and objects, leading to more varied queries and a more challenging test of model performance across diverse visual scenarios. 5.3 Scaling encoding, storage, and attention This experiment aims to evaluate the scalabil- ity of the selected models by measuring encoding time and embedding storage cost across progres- sively larger test sets. We additionally measure query-time attention costs of applicable models to assess the trade-oÔ¨Ä in real-time performance which attention-based models face. We incrementally duplicate the Flickr30K evaluation set, generating four test sets: 1K/5K, 10K/50K, 100K/500K and 1M/5M image/text samples. By conducting these experiments on an NVIDIA A100 80GiB GPU and Intel Xeon Platinum 8480+ CPU, we aim to pro- vide insights into the practical trade-oÔ¨Äs between model complexity, computational resources, and retrieval performance at scale. Encoding time and storage cost. Table 5 shows the encoding time and storage require- ments for increasing data volumes for the selected models. The experiments were constrained by an 80GiB GPU memory threshold; runs exceed- ing this limit due to the size of the Ô¨Ånal or intermediate embeddings did not Ô¨Ånish (DNF). For instance, BEIT-3, despite having the most compact Ô¨Ånal embeddings among VLP models, required 8.12GiB per 1K/5K images/captions to compute intermediate embeddings, causing its 10K/50K encoding run to exceed available GPU memory. These experiments used original model implementations without custom batching to ensure consistency and reÔ¨Çect practical limita- tions users might encounter with similar hardware. This consideration highlights the storage eÔ¨Éciency of cross-modal hashing models, which require only 0.36GiB to store 1M/5M image/text embeddings. For query-time attention models, Table 5 includes storage costs for original image and text features, which must be retained for the attention step; an important consideration when assessing the practicality of query-time attention in memory- constrained applications. Encoding time generally increased by a factor of 10 from CG to FG meth- ods and again to VLP models, with X-VLM and ADV as exceptions. The rapid encoding times of UCCH and DADH demonstrate the computa- tional eÔ¨Éciency of hash function encoders. ViLT‚Äôs end-to-end fusion architecture computes similar- ity scores directly without encoding intermediate embeddings, requiring full model inference for each new query and preventing the possibility of oÔ¨Ñine embedding storage and reuse. Con- sequently, ViLT‚Äôs computational costs are not represented in Table 5, which focuses on one- time encoding costs for oÔ¨Ñine storage; instead, its query-time costs are analyzed in the following segment on attention mechanisms. Attention time. Attention in this context refers to end-to-end fusion for ViLT, fusion- based reranking for BLIP-2 and X-VLM and cross-attention for IMRAM and SCAN; processes which are computed at query time. Due to the end-to-end fusion and cross-attention mechanisms used by ViLT, IMRAM and SCAN attending to all possible image-text pairs, the computa- tional complexity of the mechanism is O(n √ó m), where n is the number of queries and m the number of retrieval candidates, quickly making 11 Category-Level Retrieval Task mAP@k MS-COCO Flickr30K Image to Text Text to Image Image to Text Text to Image mAP @10 @100 @N @10 @100 @N @10 @100 @N @10 @100 @N Fine-Grained Vision-Language Pretrained BLIP-2 93.7 79.4 n/a 94.7 86.7 n/a 99.4 96.8 n/a 99.3 97.3 n/a BLIP-2NF 96.2 87.8 47.9 95.9 88.5 51.1 99.5 98.1 92.5 99.5 98.2 92.5 BEIT-3 95.4 84.8 56.1 95.6 88.3 60.7 99.3 97.5 94.3 99.2 97.5 94.6 X-VLM 95.0 86.1 n/a 94.6 83.4 n/a 99.5 96.7 n/a 99.0 96.9 n/a X-VLMNF 95.8 87 58.4 95.8 88.8 60.3 99.3 97.7 94.5 99.3 97.6 94.5 ViLT 63.5 42.6 32.9 62.8 42.5 33.0 97.2 92.0 88.7 94.6 90.1 88.7 Fine-Grained IMRAM 94.5 86.2 57.9 94.5 86.2 57.9 98.8 96.8 93.6 98.9 96.9 93.6 VSRN 95.1 83.5 45.8 94.7 83.5 46.6 98.8 95.7 91.6 98.8 95.7 91.6 SCAN 94.9 87.1 55.5 95.0 87.5 58.2 98.9 96.7 93.4 98.9 96.8 93.5 ADV2048bit - - - - - - 98.6 96.3 92.2 98.4 95.9 92.1 Coarse-Grained ADV64bit - - - - - - 98.4 95.6 91.7 98.7 95.7 91.8 UCCH 86.2 81.5 60.7 86.6 82.1 60.2 95.1 92.1 89.3 90.7 88.2 88.8 DADH 84.8 78.4 58.7 75.6 71.6 60.5 98.6 97.8 96.3 96.7 96.1 95.7 T able 4: Comparison of category-level retrieval performance across v arious Vision-Language Pre-trained Fine-grained (VLP FG) models, non-pretrained Fine-Grained (FG) m odels, and Coarse-Grained (CG) models on the MS-COCO and Flickr30K datasets. Results are report ed as mean Average Precision (mAP) at k (k=10,100,N) for both (i ‚Üí t) and (t ‚Üí i) tasks. Note that the fusion steps of BLIP-2 and X-VLM discard all samples that are left out of the fusion reranking step (k =128 and k=256 for Flickr30K and MS-COCO respectively). Therefore, mAP@N is not applicable (n/a) f or these fusion-based models due to the number of relevant samples often exceeding the top-k of th eir reranking range. Hence, results for the no-fusion (NF) variants BLIP-2 and X-VLM are also provided retrieval prohibitively expensive computationally. This computational burden has driven the Ô¨Åeld towards dual-encoder architectures, which avoid query-time attention by computing and storing embeddings independently for each modality. For fusion-reranking, the time complexity is O(n √ó k), where k is the reranking shard threshold, which leads to improved costs relative to full atten- tion. In the case of BLIP-2 and X-VLM, we kept k = 128 irrespective of the increase in sample size, which leads to a linear increase in attention computation time. However, if the k value were also scaled to the number of retrieval candidates, the computational complexity would approximate O(n √ó m). Such attention mechanisms, therefore, may be impractical for large-scale applications where query latency is critical. 5.4 Scaling similarity search with F AISS The Ô¨Ånal step in retrieval, similarity search, is typically independent of the encoding model. Assuming no query-time attention or reranking, the model‚Äôs task is complete once samples are encoded. These encoded samples are then typi- cally passed to a separate specialised similarity search implementation. In that case, the only factors aÔ¨Äecting similarity search time are the embedding type (real-valued continuous or bitwise hash code embeddings) and the embedding dimen- sions. To explore the practical implications of using CG binary hash codes compared to FG con- tinuous embeddings, we employ Facebook AI Sim- ilarity Search (F AISS) [ 53], a robust and widely adopted [ 54‚Äì56] similarity search implementa- tion oÔ¨Äering various indexes for both exhaus- tive search and Approximate Nearest Neighbour Search (ANNS). By using F AISS, we transition 12 Encoding Time Size (1M/5M) Img/Txt 1K/5K 10K/50K 100K/500K 1M/5M Embed. Features Fine-Grained Vision-Language Pretrained BLIP-2 93.27s 908.18s DNF DNF 5.72GiB 4240.25GiB BEIT-3 56.76s DNF DNF DNF 17.2GiB n/a X-VLM 12.42s 100.87 s DNF DNF 5.72GiB 1126.82GiB ViLT n/a n/a n/a n/a n/a 2723.37 GiB Fine-Grained IMRAM 50.4s 47.35s 458.10s DNF 45.75GiB 3257.79GiB VSRN 5.03s 19.55s 180.08s 1770.48s 91.5GiB n/a SCAN 6.47s 51.7s 464.464s DNF 45.75GiB 3212.01GiB Coarse-Grained ADV 2048-Bit 5.86s 37.19s 447s 9854.41s 11.71GiB n/a ADV 64-Bit 5.75s 35.29s 334.67s 3571.91s 0.36GiB n/a UCCH 0.40s 4.12s 60.57s 488.28s 0.36GiB n/a DADH 0.53s 1.13s 6.46s 59.41s 0.36GiB n/a T able 5: Encoding time cost for encoding 1K/5K, 10K/50K, 100K/500K and 1M/5M images/text in seconds and storage costs for holding 1M/5M image/text embeddin gs and features. ViLT‚Äôs encoding time and embedding storage are not applicable due to its end-to-end fusion architecture, which computes similarity scores directly without encoding embeddings. For ViLT‚Äôs com putational costs, see Table 6. DNF: Did not Ô¨Ånish due to memory overÔ¨Çow. Features n/a: The mode l does not require original features during query time Attention (Images/Text) 1K/5K 10K/50K 100K/500K Transformer End-to-End Fusion ViLT 3093.76s ¬ø880h DNF Transformer Fusion Re-ranking BLIP-2 2376.89s 24314.27s DNF X-VLM 370.48s 3707.37s DNF Cross-Attention IMRAM 190.67s 12200.28s DNF SCAN 17.51s 1285.34s DNF T able 6 : Time cost for computing end-to-end fusion, fusion re-ranking, cross-attention 1K/5K, 10K/50K and 100K/500K images/text. from model-speciÔ¨Åc comparisons to a generalised evaluation of embedding types within an industry- standard framework. The experiments were con- ducted using the following four indexes: Indexes. IndexFlatIP is a continuous- embedding brute-force index that exhaustively searches the entire dataset via inner product and serves as our baseline for evaluating the performance of the other three indexes against it. IndexHNSW (Hierarchical Navigable Small World) is a graph-based index that organises vectors in a hierarchical structure and is the preferred index under F AISS guidelines for con- tinuous embedding ANNS, given enough memory. IndexBinaryFlat is the exhaustive search index for binary vectors which computes the Hamming distance between all query and retrieval hash codes. IndexBinaryIVF (Inverted File System) is a partitioning-based index that divides the dataset into multiple inverted lists and is the preferred binary embedding ANNS index. Dataset. The experiments were conducted using the SyntheticDataset class provided by F AISS. The choice of embedding sizes within these experiments aimed to exploit the strengths of diÔ¨Äerent granularity levels. For CG embeddings, a bit hash length of 64 was chosen to maxi- mize eÔ¨Éciency based on previous experiments. For FG embeddings, a 2048-dimensional embed- ding, similar to VSRN, was selected to maximize accuracy. From Table 7 and Figure 4, the following observations can be made: Index recall performance. The HNSW index demonstrates high recall performance, maintaining 98% of the recall relative to an exhaustive search in the 1M queries/5M retrieval candidates test case across all reported R@k scores 13 100K Queries/500K Candidates R@1 R@100 R@1000 Index Search FG Flat 2048D Baseline (100.0) 1.93s 57.17s FG HNSW 2048D 99.2 99.2 99.2 32.44s 3.09s CG Flat 64-bit 10.7 60.6 93.6 <0.01s 8.84s CG IVF 64-bit 10.5 68.9 89.7 0.33s 0.70s 500K Queries/2.5M Candidates R@1 R@100 R@1000 Index Search FG Flat 2048D Baseline (100.0) 9.55s 1216.75s FG HNSW 2048D 98.3 98.3 98.3 154.29s 18.17s CG Flat 64-bit 8.2 62.6 89.3 <0.01s 181.83s CG IVF 64-bit 8.3 61.6 86.7 1.41s 11.13s 1M Queries/5M Candidates R@1 R@100 R@1000 Index Search FG Flat 2048D Baseline (100.0) 19.04s 4728.18s FG HNSW 2048D 98.0 98.0 98.0 300.65s 39.80s CG Flat 64-bit 6.9 56.1 84.3 0.02s 692.80s CG IVF 64-bit 6.9 55.3 82.3 2.82s 40.20s T able 7: Recall and search time performance of Fine-Grained (FG) and Coa rse-Grained (CG) F AISS indexes on the F AISS Synthetic Dataset. The FG exhaustive searc h (FG Flat) is the baseline ground truth to measure the scalability of other indexes relative to it. The time for ‚ÄòIndex‚Äô refers to the construction time of the index. The time for ‚ÄòSearch‚Äô refers to the search time fo r the entire test set using the previously built index 1 100 500 1000 @k 0.2 0.4 0.6 0.8 1.0 R ecall Inde xHNSWFlat Inde xBinaryFlat Inde xBinaryIVF (a) Retrieval performance 0.2M/1.0M 0.4M/2.0M 0.6M/3.0M 0.8M/4.0M 1.0M/5.0M Query/R etrieval Samp es 0 1000 2000 3000 4000 Search T ime (seconds) IndexBinaryF at IndexBinaryIVF IndexF atIP IndexHNSWF at (b) Search time Fig. 4 : Comparison of search time and recall performance for both exhaustive and ANNS indexes of continuous and binary embeddings with increasing amounts of data using F AISS (Table 7). In contrast, binarising the embeddings to CG representations signiÔ¨Åcantly impacts per- formance: For the 1M/5M test case, the CG approach retains only 6.9% of R@1 performance compared to the exhaustive FG search. However, this retention improves at higher recall levels, reaching 84.3% for R@1000 (Figure 4). After the initial performance drop due to binarisation, fur- ther indexing these CG embeddings using an IVF index results in minimal additional recall degra- dation. For instance, in the 1M/5M test, the R@1 performance remains at 6.9% for both Ô¨Çat and IVF indexes, while R@1000 shows a moderate decrease from 84.3% to 82.3%. Search scalability and hardware impact. As visualised in Figure 4, when not applying any ANNS indexes, the CG exhaustive search IndexBi- naryFlat is a more scalable search approach than the FG IndexFlatIP. However, once the two embedding types are indexed by their preferred ANNS method - HNSW for FG embeddings and IVF for CG embeddings - their search times become comparable (Table 7). For example, in the 1M/5M test, the search time for the FG HNSW (39.80s) is similar to the CG IVF (40.25s). Despite the potential for computational eÔ¨Éciency oÔ¨Äered by CG methods through lightweight bitwise oper- ations, Figure 4 illustrates a practical overlap in search time between FG and CG indexes across increasing data sizes. This convergence in search time can be attributed to recent hardware opti- misations favouring continuous embeddings. This result suggests that binarised embeddings have room for further optimisation research eÔ¨Äorts, 14 particularly in standardising methodologies that optimise hardware utilisation to handle bitwise embeddings more eÔ¨Éciently. 6 Key Ô¨Åndings and discussion Based on the results obtained from the conducted comparative experiments, the following are the main key Ô¨Åndings and recommendations derived from this study: Performance comparison. FG models con- sistently outperformed CG models in instance- level retrieval tasks. CG models, however, demon- strated competitive performance in category-level retrieval, especially when retrieving large num- bers of relevant samples. These Ô¨Åndings challenge the conventional notion of FG models having uni- versal superiority in retrieval performance over CG models; while FG models excel in speciÔ¨Åc instance-level retrieval, their performance advan- tage diminishes in broader, category-level tasks. This suggests that, when retrieval performance is the main concern, the choice between FG and CG models should be task-dependent rather than always defaulting to FG models. Hybrid coarse-to-Ô¨Åne potential. An intu- itive integration of CG and FG models could involve a two-step approach, where a CG model selects top-k candidates to reduce the computa- tional load of a subsequent Ô¨Åne-grained rerank- ing step. Our evaluation results showed that CG models trained speciÔ¨Åcally with instance-level loss functions could potentially serve as this ini- tial screening step within a coarse-to-Ô¨Åne ITR pipeline. However, traditional CG models trained on category-level loss functions are not suitable for this purpose. Despite their eÔ¨Äectiveness in broadly retrieving samples of the same category as the query, these models do not consistently rank the exact instance-level match high enough in the retrieval rank to serve as an eÔ¨Äective initial screening step. Attention mechanisms. State-of-the-art benchmark recall results are achieved by mod- els such as BLIP-2, which employ fusion-encoder reranking to reÔ¨Åne search results. However, the computational load associated with such query- time attention mechanisms is diÔ¨Écult to justify for practical use in retrieval applications. This inef- Ô¨Åciency stems from fusion encoders attending to every possible image-text pair with a computa- tional complexity of O(n √ó m), where n is the number of queries and m is the number of retrieval candidates. Unlike retrieval tasks where fusion encoders must process all query-candidate pairs, in applications like VQA or image captioning, the fusion encoder only needs to attend to a single image-text pair at a time (e.g. the given question and image), resulting in O(n) complexity. This makes fusion encoders particularly well-suited for these single-pair reasoning tasks[ 57, 58], but prohibitively expensive for large-scale retrieval. Therefore, for retrieval applications where stor- age concerns can be addressed, dual-encoder VLP models which do not have query-time attention represent the most practical architecture type for instance-level retrieval. Search scalability insights. Scalability experiments revealed that the potential eÔ¨Éciency advantages of CG models, particularly in terms of bitwise operations, do not always translate into practical performance gains for query latency. This was evidenced when applying F AISS-based ANNS indexing, where query latency of CG and FG embeddings was equalised, yet FG embeddings achieved considerably higher recall performance. This outcome highlights the signiÔ¨Åcant impact of hardware optimisations and similarity search implementations on search performance. Without custom implementations of hardware-level opti- mised bitwise operations, CG embeddings do not improve query-time latency over FG ones when using a standard similarity search implementation such as F AISS. Further research into standardised optimisations of CG search for modern hardware architectures is recommended to fully leverage their potential eÔ¨Éciency advantages. Storage eÔ¨Éciency . In terms of storage eÔ¨É- ciency, CG models oÔ¨Äer signiÔ¨Åcant advantages. The 64-bit embeddings used for the evaluated CG models were over 15.8 times smaller than the most compact FG embeddings evaluated (256D). For category-level retrieval tasks with large amounts of relevant retrieval samples where FG models do not yield improvements in retrieval perfor- mance, the storage savings of CG models become particularly compelling. 15 6.1 Limitations Our proposed FiCo-ITR library and experiments identify new insights into the tradeoÔ¨Äs of FG and CG ITR models, however, we acknowledge cer- tain limitations. CG models are commonly bench- marked on datasets which contain hashtag collec- tions as text samples (e.g. MIR-Flickr25K [ 59] or NUS-WIDE [ 60]). In contrast, FG models require full sentences to enable instance-level retrieval. There are currently no benchmark ITR datasets that contain both full sentences and hashtag col- lections as their text samples. This limitation presents an opportunity for an extension of our work, where an image captioning model could be used to generate full sentences for MIR-Flickr25K or NUS-WIDE to enable further evaluations. Our scalability experiments quantiÔ¨Åed the computational costs associated with increasingly large retrieval sets. To achieve this, we used data duplication to grow our datasets. However, the potential degradation of retrieval performance as the size of the retrieval set increases could not be measured through this approach. Large-scale ITR benchmarks which would enable this investigation do not exist in the Ô¨Åeld. Currently available large- scale image-text datasets (e.g. LAION-5B [ 61], CC13 [ 62]) are built for vision-language pretrain- ing and have data imbalance and noise, which makes them unsuitable for benchmarking pur- poses. Our future work will target creating large- scale ITR benchmark datasets which maintain data quality and balance. Our study could be extended in sev- eral directions: Our experiments focused on a cross-methodology comparison of FG and CG approaches. However, further within-methodology experiments could inform future work, such as implementing FG model ensembles or identifying the most suitable CG architecture for real-world use cases. Additional experiments could examine the generalisability of our Ô¨Åndings across diÔ¨Äer- ent data distributions, application domains, and model sensitivity to diÔ¨Äerent types of data. Fur- ther eÔ¨Éciency gains could be achieved through hardware optimisations and custom batching solu- tions. 7 Conclusion While the conceptual diÔ¨Äerences in performance between FG and CG image-text retrieval models are well-documented in the literature, empiri- cal data quantifying these diÔ¨Äerences has been sparse. This study introduced the library and toolkit FiCo-ITR, which provides a standardised toolkit for evaluating both FG and CG image-text retrieval models, which addresses this empirical gap. The results within this paper indicate that while FG methods excel in instance-level retrieval, CG approaches demonstrate competitive perfor- mance in category-level tasks where a large num- ber of relevant samples must be retrieved. Despite the potential for computational eÔ¨Éciency oÔ¨Äered by CG models, practical evaluations showed com- parable query latencies between FG continuous embeddings and CG bitwise hash code embed- dings, highlighting the practical impact of recent hardware optimisations. These results highlight that the notion of FG models oÔ¨Äering more robust retrieval performance while CG models are more eÔ¨Écient is not straightforward; instead, these char- acteristics depend on the speciÔ¨Åc retrieval task and implementation. Future work will focus on two key areas: First, the development of a large- scale image-text retrieval benchmark to enable scalability experiments on real data. Second, the exploration of hybrid coarse-to-Ô¨Åne approaches that leverage insights from this study to bal- ance retrieval performance and eÔ¨Éciency. These directions aim to further bridge the gap between FG and CG methodologies, potentially leading to more robust and scalable image-text retrieval systems. Declarations Code availability . The source code for the FiCo-ITR library and toolkit can be found in the project‚Äôs GitHub repository: https://github.com/MikelWL/FiCo-ITR. ConÔ¨Çict of interest. The authors declare no ConÔ¨Çict of interest. Ethical approval. This article contains no data or other information from studies or experimenta- tion involving human or animal subjects. 16 References [1] Ma H, Zhao H, Lin Z, et al (2022) EI-CLIP: Entity-aware interventional contrastive learning for e-commerce cross-modal retrieval. In: IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR), pp 18030‚Äì18040, https://doi.org/10.1109/CVPR52688.2022.01752 [2] Zhang Y, Wang Q, Pan P, et al (2021) Fashion focus: Multi-modal retrieval sys- tem for video commodity localization in e-commerce. In: AAAI Conference on ArtiÔ¨Åcial Intelligence, pp 16127‚Äì16128, https://doi.org/10.1609/aaai.v35i18.18033 [3] Nakatsuka T, Hamasaki M, Goto M (2023) Content-based music-image retrieval using self-and cross-modal feature embed- ding memory. In: Proceedings of the IEEE/CVF Winter Conference on Appli- cations of Computer Vision, pp 2174‚Äì2184, https://doi.org/10.1109/W ACV56688.2023.00221 [4] Gong Y, Cosma G, Finke A (2023) Neural-based cross-modal search and retrieval of artwork. In: 2023 IEEE Symposium Series on Computa- tional Intelligence (SSCI), pp 264‚Äì269, https://doi.org/10.1109/SSCI52147.2023.10371948 [5] Yang Y, Shang X, Li B, et al (2024) Detection-free cross-modal retrieval for person identiÔ¨Åcation using videos and radar spectrograms. IEEE Transactions on Instrumentation and Measurement https://doi.org/10.1109/TIM.2024.3372210 [6] Truong QT, Salah A, Tran TB, et al (2021) Exploring cross-modality uti- lization in recommender systems. IEEE Internet Computing 25:50‚Äì57. https://doi.org/10.1109/MIC.2021.3059027 [7] Gan Z, Li L, Li C, et al (2022) Vision- language pre-training: Basics, recent advances, and future trends. Foun- dations and Trends ¬Æ in Computer Graphics and Vision 14(3‚Äì4):163‚Äì352. https://doi.org/10.1561/9781638281337 [8] Luo X, Wang H, Wu D, et al (2023) A survey on deep hashing methods. ACM Transactions on Knowledge Discovery from Data 17(1):1‚Äì 50. https://doi.org/10.1145/3532624 [9] Chen H, Ding G, Liu X, et al (2020) IMRAM: Iterative matching with recur- rent attention memory for cross-modal image-text retrieval. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 12655‚Äì12663, https://doi.org/10.1109/CVPR42600.2020.01267 [10] Hu P, Zhu H, Lin J, et al (2022) Unsu- pervised contrastive cross-modal hashing. IEEE Transactions on Pattern Analysis and Machine Intelligence pp 3877‚Äì3889. https://doi.org/10.1109/TPAMI.2022.3177356 [11] Zhu L, Wang T, Li F, et al (2023) Cross- modal retrieval: A systematic review of meth- ods and future directions. arXiv preprint https://doi.org/10.48550/arXiv.2308.14263 [12] Plummer BA, Wang L, Cervantes CM, et al (2015) Flickr30k entities: Collect- ing region-to-phrase correspondences for richer image-to-sentence models. In: Pro- ceedings of the IEEE international confer- ence on computer vision, pp 2641‚Äì2649, https://doi.org/10.1109/ICCV.2015.303 [13] Lin TY, Maire M, Belongie S, et al (2014) Microsoft COCO: Common objects in context. In: Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Pro- ceedings, Part V 13, Springer, pp 740‚Äì755, https://doi.org/10.1007/978-3-319-10602-1 48 [14] Faghri F, Fleet DJ, Kiros JR, et al (2017) VSE++: Improving visual-semantic embed- dings with hard negatives. In: Proceedings of the British Machine Vision Conference, p 12, https://doi.org/10.48550/arXiv.1707.05612 [15] Kiros R, Salakhutdinov R, Zemel RS (2015) Unifying visual-semantic embed- dings with multimodal neural language models. Transactions of the Association for Computational Linguistics pp 1‚Äì13. 17 https://doi.org/10.48550/arXiv.1411.2539 [16] Simonyan K, Zisserman A (2015) Very deep convolutional networks for large- scale image recognition. International Conference on Learning Representations https://doi.org/10.48550/arXiv.1409.1556 [17] He K, Zhang X, Ren S, et al (2016) Deep residual learning for image recognition. In: Proceedings of the IEEE conference on com- puter vision and pattern recognition, pp 770‚Äì 778, https://doi.org/10.1109/CVPR.2016.90 [18] Lee KH, Chen X, Hua G, et al (2018) Stacked cross attention for image-text matching. In: Proceedings of the European Conference on Computer Vision (ECCV), pp 201‚Äì216, https://doi.org/10.1007/978-3-030-01225-0 13 [19] Ren S, He K, Girshick R, et al (2015) Faster R-CNN: Towards real-time object detection with region proposal networks. Advances in neural information processing systems 28. https://doi.org/10.1109/TPAMI.2016.2577031 [20] Wang Z, Liu X, Li H, et al (2019) CAMP: Cross-modal adaptive message pass- ing for text-image retrieval. In: Proceed- ings of the IEEE/CVF international con- ference on computer vision, pp 5764‚Äì5773, https://doi.org/10.1109/ICCV.2019.00586 [21] Li K, Zhang Y, Li K, et al (2019) Visual semantic reasoning for image- text matching. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp 4654‚Äì4662, https://doi.org/10.1109/ICCV.2019.00475 [22] Li K, Zhang Y, Li K, et al (2022) Image-text embedding learning via visual and textual semantic reasoning. IEEE Transactions on Pattern Analysis and Machine Intelligence 45(1):641‚Äì656. https://doi.org/10.1109/TPAMI.2022.3148470 [23] Devlin J, Chang MW, Lee K, et al (2019) BERT: Pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp 4171‚Äì4186, https://doi.org/10.18653/v1/N19-1423 [24] Shi B, Ji L, Lu P, et al (2019) Knowl- edge aware semantic concept expansion for image-text matching. In: Proceedings of the Twenty-Eighth International Joint Confer- ence on ArtiÔ¨Åcial Intelligence, pp 5182‚Äì5189, https://doi.org/10.24963/ijcai.2019/720 [25] Zhang Q, Lei Z, Zhang Z, et al (2020) Context-aware attention network for image- text retrieval. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 3536‚Äì3545, https://doi.org/10.1109/CVPR42600.2020.00359 [26] Kim W, Son B, Kim I (2021) Vilt: Vision-and-language transformer with- out convolution or region supervision. In: International conference on machine learning, PMLR, pp 5583‚Äì5594, URL https://proceedings.mlr.press/v139/kim21k.html [27] Jia C, Yang Y, Xia Y, et al (2021) Scal- ing up visual and vision-language repre- sentation learning with noisy text super- vision. In: Proceedings of Machine Learn- ing Research, PMLR, pp 4904‚Äì4916, URL https://proceedings.mlr.press/v139/jia21b [28] Wang W, Bao H, Dong L, et al (2023) Image as a foreign language: BEiT pre- training for vision and vision-language tasks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 19175‚Äì19186, https://doi.org/10.1109/CVPR52729.2023.01838 [29] Chen YC, Li L, Yu L, et al (2020) UNITER: Universal image-text representation learn- ing. In: Computer Vision ‚Äì ECCV 2020, Lecture Notes in Computer Science, vol 12375. Springer, Cham, pp 104‚Äì120, https://doi.org/10.1007/978-3-030-58577-8 7 18 [30] Dou ZY, Xu Y, Gan Z, et al (2022) An empirical study of training end-to- end vision-and-language transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 18166‚Äì18176, https://doi.org/10.1109/CVPR52688.2022.01763 [31] Zeng Y, Zhang X, Li H, et al (2023) X2- VLM: All-in-one pre-trained model for vision-language tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence https://doi.org/10.1109/TPAMI.2023.3339661 [32] Li J, Li D, Savarese S, et al (2023) BLIP-2: Bootstrapping language- image pre-training with frozen image encoders and large language models. In: International conference on machine learning, PMLR, pp 19730‚Äì19742, URL https://proceedings.mlr.press/v202/li23q [33] Sun S, Chen YC, Li L, et al (2021) Light- ningDot: Pre-training visual-semantic embeddings for real-time image-text retrieval. In: Proceedings of the 2021 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp 982‚Äì997, https://doi.org/10.18653/v1/2021.naacl-main.77 [34] Zhang L, Wu H, Chen Q, et al (2022) VLDeformer: Vision‚Äìlanguage decomposed transformer for fast cross-modal retrieval. Knowledge-Based Systems 252:109316. https://doi.org/10.1016/j.knosys.2022.109316 [35] Shao B, Liu J, Pei R, et al (2023) Hivlp: Hierarchical interactive video- language pre-training. In: 2023 IEEE/CVF International Conference on Com- puter Vision (ICCV), pp 13710‚Äì13720, https://doi.org/10.1109/ICCV51070.2023.01265 [36] Jiang QY, Li WJ (2017) Deep cross- modal hashing. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 3232‚Äì3240, https://doi.org/10.1109/CVPR.2017.348 [37] Li C, Deng C, Li N, et al (2018) Self- supervised adversarial hashing networks for cross-modal retrieval. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 4242‚Äì4251, https://doi.org/10.1109/CVPR.2018.00446 [38] Gu W, Gu X, Gu J, et al (2019) Adver- sary guided asymmetric hashing for cross-modal retrieval. In: Proceedings of the 2019 on international conference on multimedia retrieval, pp 159‚Äì167, https://doi.org/10.1145/3323873.3325045 [39] Bai C, Zeng C, Ma Q, et al (2020) Deep adversarial discrete hashing for cross-modal retrieval. In: Proceedings of the 2020 International Conference on Multimedia Retrieval, pp 525‚Äì531, https://doi.org/10.1145/3372278.3390711 [40] Tu RC, Mao XL, Ma B, et al (2020) Deep cross-modal hashing with hashing functions and uniÔ¨Åed hash codes jointly learning. IEEE Transactions on Knowl- edge and Data Engineering 34(2):560‚Äì572. https://doi.org/10.1109/TKDE.2020.2987312 [41] Tu J, Liu X, Lin Z, et al (2022) Dif- ferentiable cross-modal hashing via multimodal transformers. In: Proceed- ings of the 30th ACM International Conference on Multimedia, pp 453‚Äì461, https://doi.org/10.1145/3503161.3548187 [42] Li L, Sun W (2023) Label-wise deep semantic-alignment hashing for cross- modal retrieval. In: Proceedings of the 2023 ACM International Conference on Multimedia Retrieval, pp 416‚Äì424, https://doi.org/10.1145/3591106.359228 [43] Zhang J, Peng Y, Yuan M (2018) Unsu- pervised generative adversarial cross-modal hashing. In: Proceedings of the AAAI conference on artiÔ¨Åcial intelligence, https://doi.org/10.1609/aaai.v32i1.11263 [44] Wu G, Lin Z, Han J, et al (2018) Unsu- pervised deep hashing via binary latent factor models for large-scale cross-modal retrieval. In: International Joint Con- ference on ArtiÔ¨Åcial Intelligence, p 5, https://doi.org/10.24963/ijcai.2018/396 19 [45] Su S, Zhong Z, Zhang C (2019) Deep joint- semantics reconstructing hashing for large- scale unsupervised cross-modal retrieval. In: Proceedings of the IEEE/CVF international conference on computer vision, pp 3027‚Äì3035, https://doi.org/10.1109/ICCV.2019.00312 [46] Yang D, Wu D, Zhang W, et al (2020) Deep semantic-alignment hashing for unsu- pervised cross-modal retrieval. In: Proceed- ings of the 2020 International Confer- ence on multimedia retrieval, pp 44‚Äì52, https://doi.org/10.1145/3372278.3390673 [47] Liu S, Qian S, Guan Y, et al (2020) Joint-modal distribution-based similar- ity hashing for large-scale unsupervised deep cross-modal retrieval. In: Proceed- ings of the 43rd international ACM SIGIR conference on research and development in information retrieval, pp 1379‚Äì1388, https://doi.org/10.1145/3397271.3401086 [48] Li Y, Mu Y, Zhuang N, et al (2021) EÔ¨É- cient Ô¨Åne-grained visual-text search using adversarially-learned hash codes. In: 2021 IEEE International Conference on Multi- media and Expo (ICME), IEEE, pp 1‚Äì6, https://doi.org/10.1109/ICME51207.2021.9428271 [49] Liu S, Zhang L, Yang X, et al (2021) Query2Label: A simple transformer way to multi-label classiÔ¨Åcation. arXiv preprint https://doi.org/10.48550/arXiv.2107.10834 [50] Anderson P, He X, Buehler C, et al (2018) Bottom-up and top-down atten- tion for image captioning and visual question answering. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 6077‚Äì6086, https://doi.org/10.1109/CVPR.2018.00636 [51] Karpathy A, Fei-Fei L (2017) Deep visual-semantic alignments for gen- erating image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence 39(4):664‚Äì676. https://doi.org/10.1109/TPAMI.2016.2598339 [52] Zeng Y, Zhang X, Li H (2022) Multi- grained vision language pre-training: Aligning texts with visual concepts. In: International Conference on Machine Learning, PMLR, pp 25994‚Äì26009, URL https://proceedings.mlr.press/v162/zeng22c [53] Johnson J, Douze M, J¬¥ egou H (2021) Billion- scale similarity search with gpus. IEEE Transactions on Big Data 7(3):535‚Äì547. https://doi.org/10.1109/TBDATA.2019.2921572 [54] Pizzi E, Roy SD, Ravindra SN, et al (2022) A self-supervised descriptor for image copy detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 14532‚Äì14542, https://doi.org/10.1109/CVPR52688.2022.01413 [55] Barrault L, Chung YA, Meglioli MC, et al (2023) SeamlessM4T‚Äî massively multilingual & multimodal machine translation. arXiv preprint https://doi.org/https://doi.org/10.48550/arXiv.2308.11596 [56] Thakur N, Reimers N, R¬® uckl¬¥ e A, et al (2021) BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. In: Proceedings of the 35th Con- ference on Neural Information Processing Systems, Datasets, and Benchmarks Track, https://doi.org/https://doi.org/10.48550/arXiv.2104.08663 [57] Ishmam MF, Shovon MSH, Mridha MF, et al (2024) From image to language: A critical analysis of visual question answering (vqa) approaches, challenges, and oppor- tunities. Information Fusion p 102270. https://doi.org/https://doi.org/10.1016/j.inÔ¨Äus.2024.102270 [58] Stefanini M, Cornia M, Baraldi L, et al (2022) From show to tell: A survey on deep learning-based image captioning. IEEE transactions on pattern analysis and machine intelligence 45(1):539‚Äì559. https://doi.org/https://doi.org/10.1109/TPAMI.2022.31482 [59] Huiskes MJ, Lew MS (2008) The mir Ô¨Çickr retrieval evaluation. In: Proceedings of the 1st ACM international conference on Mul- timedia information retrieval, pp 39‚Äì43, https://doi.org/https://doi.org/10.1145/1460096.1460104 20 [60] Chua TS, Tang J, Hong R, et al (2009) Nus-wide: a real-world web image database from national university of singapore. In: Proceedings of the ACM international con- ference on image and video retrieval, pp 1‚Äì9, https://doi.org/https://doi.org/10.1145/1646396.1646452 [61] Schuhmann C, Beaumont R, Vencu R, et al (2022) Laion-5b: An open large-scale dataset for training next generation image-text models. In: Advances in Neural Informa- tion Processing Systems, pp 25278‚Äì25294, URL https://proceedings.neurips.cc/paper Ô¨Åles/paper/2022/hash/a1859debfb3b59d094f3504d5ebb6c2 5-Abstract-Datasets and Benchmarks.html [62] Changpinyo S, Sharma P, Ding N, et al (2021) Conceptual 12m: Pushing web-scale image-text pre-training to recognize long- tail visual concepts. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 3558‚Äì3568, https://doi.org/https://doi.org/10.1109/CVPR46437.2021.00356 21