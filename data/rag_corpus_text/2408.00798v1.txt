Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base Zhiyu Anσµ1 Xianzhong Dingγ Yen-Chun Fuµ Cheng-Chung Chuµ Yan Liµ Wan Duσ σ : University of California, Merced, CA, USA µ : Western Digital Corporation, CA, USA γ : Lawrence Berkeley National Laboratory, CA, USA {zan7, wdu3}@ucmerced.edu Abstract This paper introduces Golden-Retriever, de- signed to efficiently navigate vast industrial knowledge bases, overcoming challenges in traditional LLM fine-tuning and RAG frame- works with domain-specific jargon and context interpretation. Golden-Retriever incorporates a reflection-based question augmentation step before document retrieval, which involves iden- tifying jargon, clarifying its meaning based on context, and augmenting the question accord- ingly. Specifically, our method extracts and lists all jargon and abbreviations in the input question, determines the context against a pre- defined list, and queries a jargon dictionary for extended definitions and descriptions. This comprehensive augmentation ensures the RAG framework retrieves the most relevant docu- ments by providing clear context and resolving ambiguities, significantly improving retrieval accuracy. Evaluations using three open-source LLMs on a domain-specific question-answer dataset demonstrate Golden-Retriever’s supe- rior performance, providing a robust solution for efficiently integrating and querying indus- trial knowledge bases. 1 Introduction Technological companies maintain massive col- lections of proprietary documents generated over years, such as training materials, design documents, and research outputs. Engineers, especially new hires, are expected to quickly query these docu- ments or assimilate the new knowledge in these documents. However, navigating in the large num- ber of documents is challenging. These domain- specific documents normally have many abbrevia- tions and jargons unique to their technical commu- nity, further complicating the problem. Large Language Models (LLMs) offer excellent performance for general question-answering tasks 1Work conducted while interning at Western Digital Cor- poration. (Petroni et al., 2019; Hu et al., 2021). To make a pre-trained LLM incorporate a company’s domain- specific knowledge, we may fine-tune it over the company’s proprietary documents. However, fine- tuning is computationally expensive, generalize poorly to new knowledge due to the Reversal Curse (Berglund et al., 2023), and limited in capacity, as it may overwrite old knowledge (Roberts et al., 2020; Zhai et al., 2024). Retrieval Augmented Generation (RAG) (Lewis et al., 2020) offers a flexible and scalable approach for utilizing large document collections. RAG con- sists of an embedding model, a document database, and a LLM. During offline preparation, RAG em- beds document chunks into the document database that retains semantic information. When an user asks a question, RAG first retrieves relevant doc- ument chunks according to semantic similarity. Then, the retrieved chunks are incorporated into prompts for the LLM, which then generates an an- swer. The output of RAG is the answer generated by the LLM based on the document chunks. This allows dynamic updates of knowledge base for an LLM without retraining it. Despite its advantages, RAG also faces chal- lenges to be used for domain-specific documents. First, since some jargons and abbreviations only ap- pear in proprietary documents, RAG’s LLM back- bone may hallucinate and misinterpret them. Ex- isting methods like Corrective RAG (Yan et al., 2024) and Self-RAG (Asai et al., 2023) enhance the LLM’s response post-retrieval. But when user’s question contains ambiguous jargons, RAG fails to retrieve the most relevant documents, limiting the effectiveness of post-retrieval enhancements. To disambiguate user’s question before retrieval, another approach (Kochedykov et al., 2023) de- constructs vague questions into an Abstract Syn- tax Tree (AST) and synthesizes SQL queries, im- proving query fidelity. But their work is limited to SQL instead of natural language documents. 1 arXiv:2408.00798v1 [cs.IR] 20 Jul 2024 Document Retrieval Question Response Corrective-RAG / Self-RAG Extracting Structured Query Golden Retriever Correct / Select Reflect / Critique Web Search Database Query Question Response AST SELECT … WHERE … Deconstruct Question Synthesis Query Document Retrieval Question Response Reflect & Identify Context Augmented Question Jargon Dictionary Jargon Query Augment Question See Fig. 2 See Fig. 3 Document Database LLM augmentation Offline Methods Online Methods Existing Methods Augment Data for Model Training LLM augmentation Model training/fine-tuning Model Prompt engineering Related Work Figure 1: An illustration comparing our method with related works. We consider two types of methods: offline and online. On the upper-left, existing offline methods use LLMs to generate datasets for training. The upper-right shows our offline method, using LLMs to enhance the document database for the online phase. Online methods are depicted in the lower part of the figure. From lower-left to lower-right: Corrective RAG and Self-RAG modify the response of RAG after the document retrieval step. If the user’s question is ambiguous or lacks context, RAG cannot retrieve the most relevant documents, limiting the effectiveness of these methods. Another approach deconstructs the question into an AST and synthesizes SQL queries accordingly, improving query fidelity but only applicable to SQL queries. Our method reflects upon the question, identifies its context, and augments the question by querying a jargon dictionary before document retrieval. The augmented question allows RAG to faithfully retrieve the most relevant documents despite ambiguous jargon or lack of explicit context. Second, while identifying the context of the ques- tion is crucial for retrieving relevant documents, the actual questions asked by the user rarely con- tain context information. The mentioned approach (Kochedykov et al., 2023) trains a transformer- based text classifier to classify user’s question to a predefined set of contexts, but this requires col- lecting a dedicated training dataset with diverse questions, which is exceedingly laborious. To tackle these challenges, we propose Golden- Retriever, which enhances the traditional RAG framework with a reflection-based question aug- mentation step before document retrieval. Golden- Retriever identifies jargons and clarifies their mean- ing based on the context. By ensuring that the context is identified and ambiguities are resolved before document retrieval, Golden-Retriever sig- nificantly reduces the risk of misinterpretation and improves the relevance of retrieved documents. Golden-Retriever includes both offline and on- line processes. The offline part involves a data pre- processing step where Optical Character Recogni- tion (OCR) is used to extract text from various doc- ument formats. This text is then summarized and contextualized by LLMs to enhance the document database, ensuring that documents are more likely to be relevant when queried. Unlike existing of- fline methods that use LLMs for model training or fine-tuning to improve cross-language performance or generate counterfactual data (Whitehouse et al., 2023; Sen et al., 2023), our approach focuses on augmenting the RAG document database directly. The online part is an interactive process that oc- curs each time a user asks a question. It starts with identifying jargons and context within the user’s query using LLMs. The identified jargons is then queried against a jargons dictionary to retrieve accu- rate definitions and descriptions. This information is used to augment the original question, providing clear context and resolving any ambiguities. The 2 augmented question is then used as input for the RAG framework, ensuring that the most relevant and accurate documents are retrieved. In summary, our contributions are as follows: • We identify the challenges of using LLMs for knowledge bases in real-world deployments. • We propose Golden-Retriever, an agentic derivative of RAG featuring reflection-based question augmentation before document re- trieval, enabling RAG to retrieve the most rel- evant documents despite ambiguous jargons and lack of context. • We evaluate Golden-Retriever with three open- source LLMs and compare its performance with baselines on a dedicated, domain-specific question-answer dataset. 2 Related Work Current RAG techniques often fall short of the ideal scenario for handling domain-specific queries in industrial knowledge bases. Vanilla RAG (Lewis et al., 2020), for instance, struggles with accurately interpreting domain- specific jargons. When asked, "What is the PUC ar- chitecture of Samsung or Hynix NAND chip?" , the system incorrectly interprets "PUC" as "Process- Unit-Controller" instead of the correct "Peripheral Under Cell". This misinterpretation highlights the problem of hallucination, where the model gener- ates incorrect or nonsensical information based on ambiguous input. This issue is further illustrated in Figure 1, which shows that both Corrective RAG (Yan et al., 2024) and Self-RAG (Asai et al., 2023) attempt to modify the response after the document retrieval step. However, if the initial retrieval is flawed due to misinterpreted jargons or lack of con- text, these post-processing techniques cannot fully rectify the inaccuracies. Moreover, Corrective-RAG and Self-RAG focus on refining the generated responses after retrieval, which is inherently limited if the retrieved docu- ments themselves are not relevant. As depicted in Figure 1, these methods fail to address the root cause: the ambiguity in the user’s question and the initial retrieval process. A related approach by (Kochedykov et al., 2023) aims to address vague questions by deconstructing them into an AST and synthesizing SQL queries accordingly. While this method improves query fidelity, it is limited to SQL queries and does not generalize to broader question- answering scenarios. Figure 1 illustrates this limi- tation, showing that while the method can disam- biguate and structure queries more effectively, it is not applicable to general retrieval tasks where context and jargon interpretation are crucial. 3 Method Golden-Retriever consists of offline and online parts. The offline part is a data pre-processing step that occurs before the deployment of the knowledge base chatbot, described in Section 3.1. The online part is an interactive process that takes place every time a user asks a question, detailed in Sections 3.2 through 3.6. 3.1 LLM-Driven Document Augmentation The offline part of Golden-Retriever focuses on enhancing the document database to improve the relevance of retrieved documents. This process begins by collecting the company’s original docu- ments, such as slides, images with embedded text, and tables, to form the knowledge base. These documents are often varied in format and content, lacking a clear narrative, which can lead to low relevance scores when queried with RAG. To address this, we use OCR to extract text from these documents and split it into smaller, manage- able chunks for processing. For the Meta-Llama-3 model, these chunks are approximately 4,000 to- kens each. Each chunk is then processed using an LLM to generate summaries from the perspec- tive of a domain expert, leveraging the LLM’s se- mantic understanding and in-context learning abili- ties. This augmented data is added to the document database, making it more likely to retrieve relevant documents when queried (Figure 3). 3.2 Identify Jargons The first step in the online process involves identifying jargons and abbreviations within the user’s question. This step is essential because many domain-specific questions include special- ized terms that require clarification to ensure ac- curate interpretation. To identify these terms, we utilize a prompt template designed to instruct the LLM to extract and list all jargons and abbrevi- ations found in the input question. This process ensures that all potentially ambiguous terms are rec- ognized, facilitating their resolution in later steps. The identified jargons and abbreviations are out- putted in a structured format for further processing. 3 Get Question Identify Context Generate Query Query Jargon Retrieve Documents Generate AnswerSynthesize Response Respond Augment Question Yes No Miss Hit “What is the PUC architecture of Samsung or Hynix NAND chip?” From this question, identify any jargon and abbreviations, and list them in the following format: [“jargon1”, “jargon2,” …] . Only output the list, nothing else. Question Instructions Response [“PUC”, “NAND”] List not empty, proceed to context identification {{Question}} What is the context of the above question? Select from the classes below: “Process Question”, “Design Question”, …, “None of the above” “Design Question” SELECT * FROM Design WHERE Jargon IN (‘PUC’, ‘NAND’); Instructions Response Identify Jargon Answer the following Design Question: What is the Peripheral Under Cell (PUC) architecture of Samsung or Hynix NAND flash chip? PUC is an architecture defined as … NAND flash refers to … Augmented Question : existing solution : our design Sec. 3.2 Sec. 3.3 Sec. 3.4 Sec. 3.5 Sec. 3.6 Figure 2: Left: the workflow diagram of the online inference part of Golden-Retriever. Right: example interactions between the system and the LLM at the intermediate steps of the workflow. The system prompts LLM to generate intermediate responses, which are saved, accessed, and used for future steps in the workflow. Read the following technical document chunk, pay attention to the knowledge details and insights contained in the document. ### Document chunk starts here ### {{document_text_chunk}} ### Document chunk ends here ### This chunk is retrieved from the {{domain}} domain. Based on the above document, generate a summary from a domain expert’s perspective. Be as detailed as possible. Respond with the summary only, no title or anything else. Instructions Response Document Chunk This chunk discusses the difference between Periph… Instructions Text Corpus Document Chunk Original Document OCR Chunking Then for each chunk, prompt the LLM as follows: First convert documents to corpuses and split to chunks: Figure 3: Section 3.1. Illustration of document pre- processing and an example prompt implementation of the LLM-Driven Document Augmentation Process. We choose to use the LLM for this task because traditional string-exact-match methods are inade- quate. These methods may fail to detect jargons that are mistyped or not yet included in the dictio- nary, which could lead to misinterpretation in the following process. The LLM’s ability to adapt to new terms provides a more robust solution. This step is represented as a two-way branching node in the workflow, shown in Figure 2. If the result- ing list is empty, the main program proceeds along the "No" path; otherwise, it follows the "Yes" path. The structured response containing the identified terms is saved in a temporary file, which is then ac- cessed by the main program to determine the next steps in the workflow. 3.3 Identify Context After identifying jargon, it is crucial to determine the context in which the question is asked, as the meaning of terms can vary significantly across dif- ferent contexts. For instance, "RAG" could mean "Retrieval Augmented Generation" in the context of LLMs or "Recombination-Activating Gene" in genetics. To accurately interpret the context, we use a similar reflection step as in jargon identifica- tion. This involves designing a prompt template that takes the question as input. The prompt con- tains a list of pre-specified context names and their descriptions. The LLM uses this prompt to identify the context of the question. Few-shot examples with Chain-of-Thought (CoT) prompting are ap- plied to enhance performance, guiding the LLM to respond in a specified data structure. The identified context is then stored and accessed by the main program for further processing. Using simpler methods, such as transformer- based text classifiers like those used in (Kochedykov et al., 2023) to classify user intent, would require a dedicated training dataset. This is impractical for our application due to the extensive effort and resources needed to create such a dataset. Instead, we opt for an "LLM as backend" approach, which, despite incurring higher computational costs, does not require a dedicated training dataset and can be run efficiently on a local server. By identifying the context before 4 document retrieval, we ensure that the meaning of jargons and abbreviations is accurately interpreted, which is essential for retrieving the most relevant documents and providing accurate answers. 3.4 Query Jargons Once the jargon and context have been identified, the next step is to query a jargon dictionary for extended definitions, descriptions, and notes on the identified terms. This step is essential for providing the LLM with accurate interpretations of the jargon, ensuring that the augmented question is clear and unambiguous. This process involves querying a SQL database with the list of jargon terms identified in Section 3.2. The jargon list is inserted into a SQL query template, which is then processed to retrieve the rel- evant information from the jargon dictionary. The retrieved information includes extended names, de- tailed descriptions, and any pertinent notes about the jargon. We choose not to use the LLM to gener- ate SQL queries directly, as described in (Qin et al., 2023) and (Li et al., 2024). Generating SQL queries with LLMs can introduce uncertainties regarding query quality and safety, and can also increase in- ference costs. Instead, by using a code-based ap- proach to synthesize the SQL query, we ensure that the queries are verifiably safe and reliable. The detailed information obtained from this step is crucial for augmenting the user’s original ques- tion. It allows for accurate context and jargon in- terpretation, which is fundamental for the RAG process to retrieve the most relevant documents and generate precise answers. 3.5 Augment Question With the jargon definitions and context identified, the next step is to augment the user’s original ques- tion to include this additional information. This augmentation ensures that the RAG process re- trieves the most relevant documents by providing clear context and resolving any ambiguities in the question. This step involves integrating the original question with the context information and the de- tailed jargon definitions obtained from Sections 3.3 and 3.4. The augmented question explicitly states the context and clarifies any ambiguous terms, fa- cilitating enhanced document retrieval. The process is automated, with the code taking the original question and the results from the con- text and jargon identification steps and combining them into a structured template. The context infor- mation grounds the LLM to the specified scenario, and the jargon definitions add relevant notes to clar- ify terms. The augmented question then replaces the user’s original question and is used as input for the RAG framework, ensuring that the most relevant and accurate documents are retrieved. 3.6 Query Miss Response In some cases, the system may not find any relevant information for certain jargon terms in the dictio- nary. To handle such scenarios, Golden-Retriever has a fallback mechanism that synthesizes a re- sponse indicating that the database is unable to an- swer the question due to missing information. The system instructs the user to check the spelling of the jargon or contact the knowledge base manager to add new terms. This step ensures that the system maintains high fidelity and avoids generating in- correct or misleading responses. The unidentified jargon fits into a response template, instructing the user to check the spelling and contact the knowl- edge base manager to add the new term. 4 Evaluation We conduct two experiments to evaluate our method’s effectiveness. The first experiment tests our method’s ability to answer domain-specific questions based on documents, and the second ex- periment tests LLM’s ability to correctly identify abbreviations from questions. 4.1 Question-Answering Experiment 4.1.1 Dataset Preparation To evaluate our method’s ability to answer domain- specific questions based on documents, we col- lected multiple-choice questions from training doc- uments for new-hire engineers. The questions cover six different domains, with each domain hav- ing nine to ten questions. These questions are one to two sentences long and contain jargon or abbrevi- ations, with choices ranging from two (True/False) to four (Multiple choice). Examples of these ques- tions are provided in Appendix B. 4.1.2 Experiment Setup The questions and choices are presented to the LLM/chatbot along with instructions to select an answer. Responses are collected and graded by a human expert who records the number of correct answers for each quiz. Each quiz is repeated five times, and the average score is calculated for each method and LLM backbone. 5 Table 1: Question answering experiment results. We use quizzes from six different domains of the new-hire training documents for engineers as test questions. All questions are multiple choice questions. Average scores across five trials are shown. The best scores are in bold. Vanilla LLM RAG Golden Retriever (Ours) Llama3 Mistral Shisa Llama3 Mistral Shisa Llama3 Mistral Shisa Quiz 1 - 10 Q 3.2 4.0 4.0 5.0 3.0 3.0 6.0 5.8 4.6 Quiz 2 - 10 Q 7.0 6.0 7.0 10.0 10.0 8.0 10.0 10.0 8.0 Quiz 3 - 9 Q 4.2 5.0 5.0 6.0 7.0 4.0 7.0 8.0 5.0 Quiz 4 - 10 Q 3.6 3.0 1.0 2.0 1.0 1.0 6.0 4.0 4.0 Quiz 5 - 10 Q 1.2 4.0 2.0 1.0 3.0 2.0 5.0 3.0 5.0 Quiz 6 - 9 Q 2.0 1.0 2.0 3.0 3.0 4.0 4.0 3.0 4.0 Total Score 21.2 23.0 21.0 27.0 27.0 22.0 38.0 33.8 30.6 Table 2: Abbreviation identification accuracy. Model No. of Abbrev. in Question 1 2 3 4 5 Llama3 70% 100% 90% 90% 100% Mistral 100% 100% 100% 70% 80% Shisa 50% 80% 60% 80% 100% We compare our method with vanilla LLM (with- out RAG) and the vanilla RAG method. For each method, including ours, we test three state- of-the-art models: Meta-Llama-3-70B-Instruct (AI@Meta, 2024), Mixtral-8x22B-Instruct-v0.1, and Shisa-v1-Llama3-70b.2e5. 4.1.3 Result We list the scores of each method and LLM backbone in Table 1. Compared with Vanilla LLM and RAG, Golden-Retriever improves the total score of Meta-Llama-3-70B by 79.2% and 40.7%, respectively. Across all three LLMs tested, Golden-Retriever improves the scores by an aver- age of 57.3% over Vanilla LLM and 35.0% over RAG. This demonstrates that Golden-Retriever sig- nificantly enhances question-answering accuracy across multiple LLM backbones. 4.2 Abbreviation Identification Experiment 4.2.1 Dataset Preparation To test if LLMs can robustly identify unknown abbreviations (Section 3.2), we generated random abbreviations and inserted them into question tem- plates to create a synthetic dataset. For abbreviation generation, we computed the probability distribu- tion of each letter being the first letter in all words in an English dictionary, then sequentially sampled the letters by that distribution to form abbreviations. We manually prepared question templates. The question templates and generated abbreviations are shown in the random abbreviation generation code in Appendix C.1. 4.2.2 Experiment Setup The synthetic questions are integrated with the prompt template, as shown in the "Identify Jargon" step in Figure 2. We prompt the LLM, record the responses, and check if they contain all abbrevi- ations used in the questions. This experiment is conducted on the three aforementioned LLMs. 4.2.3 Result We list the accuracy of each LLM in identifying all abbreviations in questions with varying numbers of abbreviations in Table 2. The experiment shows that state-of-the-art models such as Llama3 and Mistral have high accuracy in identifying unknown abbreviations. We also observe different failure modes across the three LLMs, with detailed fail cases shown in Appendix C.2. 5 Conclusion This paper presents Golden-Retriever, a novel agen- tic RAG system designed to efficiently navigate vast industrial knowledge bases and overcome the challenges of domain-specific jargon and context interpretation. Experiment on a dedicated question- answer dataset shows that Golden-Retriever signif- icantly improves answer accuracy, demonstrating its superior performance compared with traditional RAG method. 6 Acknowledgments Zhiyu An would like to acknowledge Western Dig- ital Corporation for offering generous support dur- ing the summer internship and providing the chal- lenging problems that inspired this research. References AI@Meta. 2024. Llama 3 model card. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. The reversal curse: Llms trained on" a is b" fail to learn" b is a". arXiv preprint arXiv:2309.12288. Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, and Sainbayar Sukhbaatar. 2024. Reverse train- ing to nurse the reversal curse. arXiv preprint arXiv:2403.13799. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adap- tation of large language models. arXiv preprint arXiv:2106.09685. Denis Kochedykov, Fenglin Yin, and Sreevidya Kha- travath. 2023. Conversing with databases: Practical natural language querying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track , pages 372– 379. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock- täschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neu- ral Information Processing Systems, 33:9459–9474. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. 2024. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36. Fabio Petroni, Tim Rocktäschel, Patrick Lewis, An- ton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowl- edge bases? arXiv preprint arXiv:1909.01066. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the pa- rameters of a language model? arXiv preprint arXiv:2002.08910. Indira Sen, Dennis Assenmacher, Mattia Samory, Is- abelle Augenstein, Wil van der Aalst, and Claudia Wagne. 2023. People make better edits: measuring the efficacy of llm-generated counterfactually aug- mented data for harmful language detection. arXiv preprint arXiv:2311.01270. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah- davi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature, 620(7972):172–180. Chenxi Whitehouse, Monojit Choudhury, and Al- ham Fikri Aji. 2023. Llm-powered data augmen- tation for enhanced cross-lingual performance. arXiv preprint arXiv:2305.14288. Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. arXiv preprint arXiv:2401.15884. Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. 2024. Investigating the catastrophic forgetting in multimodal large language model fine-tuning. In Conference on Parsimony and Learning, pages 202–227. PMLR. 7 A Fine-tuning or Retrieval Augmented Generation? Knowledge injection via fine-tuning has several significant drawbacks. For instance, when fine-tuned on a knowledge statement like "A is B," the fine-tuned LLM can correctly answer "What is A?" but fails to answer "What is B?" with "A" for arbitrary A and B. This phenomenon is famously known as The Reversal Curse (Berglund et al., 2023). Although remedies such as generating reversed training data (Golovneva et al., 2024) have been proposed, they require higher training costs and do not guarantee that the tuned LLM will answer all possible forms of a query. Additionally, incorporating knowledge through fine-tuning necessitates a new fine-tuning job for each new piece of knowledge, which incurs computational costs and hinders efficient integration of new information. The amount of knowledge a model can effectively incorporate depends on the capacity of the fine-tuned model part (Roberts et al., 2020), while excessive fine-tuning may lead to catastrophic forgetting, where the model forgets previously learned knowledge (Zhai et al., 2024). In contrast, RAG does not suffer from these drawbacks. The Reversal Curse, observed in fine-tuning methods, does not occur when knowledge statements are presented in-context, as part of the prompt. In RAG, the LLM learns knowledge statements in-context, significantly improving its reasoning capacity and enabling efficient instruction prompt tuning (Singhal et al., 2023). Furthermore, RAG does not require model retraining and can efficiently incorporate new knowledge corpora. These properties make RAG a superior choice for industrial knowledge bases. B Question-Answering Data Examples Here we show a few non-confidential instances of the evaluation data used in the question-answering experiment, as follows: Who decides ACT timing SPECS? a. Memory Team. b. System Team. c. JEDEC/ONFI d. Customer. Answer: c In any of the 2D NAND dies, CMD/ADDR protocol is of what nature? 1. Legacy 2. DDR1 3. DDR2 4. Depends on that particular project/technology node. Answer: 1 In Dynamic timing analysis, timing parameters margin is checked without doing any simulations. a. True b. False. Answer: b What is the VCCQ level for DDR3 standard? 1. 1.8V 2. 1.7V 3. 1.6V 4. 1.5V 5. 1.2V Answer: 4 8 What is the state of ALEx,CLEx during DATA IN operation? a. 00 b. 11 c. 10 d. Don’t care. Answer: a We need 5ns clock period to achieve 400MBps in DDR. To achieve 50MBps in SDR, what should be the clock period? 1. 10ns 2. 20ns 3. 40ns 4. 80ns Answer: 2 Why do we use dummy transistors? a. protect the actual transistors while fabrication b. Can be used as spare transistors to be used in refinement of the circuit. c. To create uniform environment for pair transistors. d. All of the above Answer: d Which of the following tasks is not the responsibility of ACT team? 1. IO design 2. Data path design 3. Pad order design 4. Package design Answer: 4 Which of the following factors affect Electromigration in the circuit? 1. Number of contacts/vias at the connecting junction of two metal layers 2. Current density in the metal layer 3. Temperature 4. All of above Answer: 4 What parameter Receiver skew affects largely in the design? a) Input V oltage b) Duty Cycle c) Input Slew Rate d) Data reception Answer: b 9 C Abbreviation Identification Experiment C.1 Synthetic Dataset Generation Template Below is the question template and the list of random abbreviations used for generating random abbrevia- tions in the abbreviation identification experiment. 1 q u e s t i o n _ t e m p l a t e s = [ 2 # T e m p l a t e s w i t h one a b b r e v i a t i o n 3 " What does t h e a b b r e v i a t i o n { a b b r 1 } s t a n d f o r ? " , 4 " Can you e x p l a i n t h e meaning o f { a b b r 1 }? " , 5 " What i s t h e f u l l form o f { a b b r 1 }? " , 6 " { a b b r 1 } i s an a b b r e v i a t i o n f o r what ? " , 7 8 # T e m p l a t e s w i t h two a b b r e v i a t i o n s 9 " What do t h e a b b r e v i a t i o n s { a b b r 1 } and { a b b r 2 } mean ? " , 10 " I n t h e c a s e where { a b b r 1 } > 0 . 5 , how much s h o u l d { a b b r 2 } be ? " , 11 " What i s t h e r e l a t i o n s h i p between { a b b r 1 } and { a b b r 2 }? " , 12 13 # T e m p l a t e s w i t h t h r e e a b b r e v i a t i o n s 14 " C o n s i d e r { a b b r 1 } = 1 . 5 and { a b b r 2 } < 0 . 1 , what would { a b b r 3 } be ? " , 15 " { a b b r 1 } and { a b b r 2 } a r e t h e same . Should { a b b r 3 } be h i g h o r low ? " , 16 " What i s t h e s t a t e o f { a b b r 1 } , { a b b r 2 } d u r i n g { a b b r 3 } o p e r a t i o n ? " , 17 18 # T e m p l a t e s w i t h f o u r a b b r e v i a t i o n s 19 "We need 10 ns { a b b r 1 } t o a c h i e v e 40{ a b b r 2 } i n { a b b r 3 } . What s h o u l d be { a b b r 4 }? " , 20 " I n any o f t h e { a b b r 1 } , { a b b r 2 } / { a b b r 3 } / { a b b r 4 } s h o u l d be what n a t u r e ? " , 21 22 # Template w i t h f i v e a b b r e v i a t i o n s 23 " { a b b r 1 }=10 , { a b b r 2 }=5 , { a b b r 3 } <0.1 i n { a b b r 4 } . How s h o u l d I s e t { a b b r 5 }? " 24 ] 25 26 r a n d o m _ a b b r e v i a t i o n _ l i s t = [ ' TS ' , ' IE ' , ' MI ' , ' SF ' , 'MP ' , 'U M ' , ' ES ' , ' PE ' , 'U W ' , ' SU ' , ' FSU ' , 'Q M B ' , 'KPU ' , 'VMT ' , 'ESO ' , ' ARI ' , ' SPA ' , 'MTD ' , 'GTC ' , 'ODV ' , 'SLBG ' , 'M U B O ' , 'ROSN ' , ' VPPL ' , ' PIOF ' , ' CCPP ' , 'MBST ' , 'UTUU ' , ' NIRE ' , ' STUP ' ] C.2 Sample Experiment Results Here we present the sample experiment results from the abbreviation identification experiment. We emphasize on demonstrating the failure modes, which is different across LLMs. C.2.1 Meta Llama 3 1 [ 2 { 3 " Q u e s t i o n " : " What i s t h e f u l l form o f KPU? " , 4 " Response " : " [ \ " KPU \ " ] " # c o r r e c t 5 } , 6 { 7 " Q u e s t i o n " : " What i s t h e f u l l form o f ARI? " , 8 " Response " : " [ \ " ARI \ " ] " # c o r r e c t 9 } , 10 { 11 " Q u e s t i o n " : " Can you e x p l a i n t h e meaning o f GTC? " , 12 " Response " : " [ \ " GTC \ " , \ " Good T i l l C a n c e l l e d \ " ] " # i n c o r r e c t 13 } , 14 { 15 " Q u e s t i o n " : " Can you e x p l a i n t h e meaning o f SPA? " , 16 " Response " : " [ \ " S i n g l e −Page A p p l i c a t i o n \ " , \ " SPA \ " ] " # i n c o r r e c t 17 } , 18 { 19 " Q u e s t i o n " : " I n any o f t h e UTUU, ES / NIRE /M U B O s h o u l d be what n a t u r e ? " , 20 " Response " : " [ \ " UTUU\ " , \ " ES / NIRE /M U B O\ " ] " # i n c o r r e c t 21 } , 22 ] 10 C.2.2 Mistral 1 [ 2 { 3 " Q u e s t i o n " : "We need 10 ns ARI t o a c h i e v e 40MI i n M U B O. What s h o u l d be PIOF ? " , 4 " Response " : " [ \ " ARI \ " , \ " MI \ " , \ "M U B O\ " , \ " PIOF \ " ] " # c o r r e c t 5 } , 6 { 7 " Q u e s t i o n " : "We need 10 ns NIRE t o a c h i e v e 40MP i n STUP . What s h o u l d be IE ? " , 8 " Response " : " [ \ " 1 0 ns NIRE \ " , \ " 4 0MP i n STUP \ " , \ " IE \ " ] " # i n c o r r e c t 9 } , 10 { 11 " Q u e s t i o n " : "We need 10 ns MBST t o a c h i e v e 40ROSN i n SPA . What s h o u l d be U W? " , 12 " Response " : " [ \ " 1 0 ns MBST\ " , \ " 4 0ROSN \ " , \ " SPA \ " , \ "U W\ " ] " # i n c o r r e c t 13 } , 14 { 15 " Q u e s t i o n " : " I n any o f t h e UTUU, ES / NIRE /M U B O s h o u l d be what n a t u r e ? " , 16 " Response " : " [ \ " UTUU\ " , \ " ES / NIRE /M U B O\ " ] " # i n c o r r e c t 17 } , 18 ] C.2.3 Shisa 1 [ 2 { 3 " Q u e s t i o n " : " PIOF i s an a b b r e v i a t i o n f o r what ? " , 4 " Response " : " [ \ " PIOF \ " ] < / s >" # c o r r e c t 5 } , 6 { 7 " Q u e s t i o n " : " What i s t h e f u l l form o f IE ? " , 8 " Response " : " [ \ " IE \ " , \ " I n t e r n e t E x p l o r e r \ " ] \ n \ n P l e a s e l e t me know i f you need a n y t h i n g e l s e . < / s >" # i n c o r r e c t 9 } , 10 { 11 " Q u e s t i o n " : "SPA i s an a b b r e v i a t i o n f o r what ? " , 12 " Response " : " [ \ " SPA \ " ] \ n \ n P l e a s e l e t me know i f I can a s s i s t you f u r t h e r . < / s >" # i n c o r r e c t 13 } , 14 { 15 " Q u e s t i o n " : " Can you e x p l a i n t h e meaning o f SPA? " , 16 " Response " : " [ \ " SPA \ " ] \ n \ nThe t e x t t r a n s l a t e d t o J a p a n e s e i s : \ n [ \ u300cSPA \ u300d ] < / s >" # i n c o r r e c t 17 } , 18 { 19 " Q u e s t i o n " : " Can you e x p l a i n t h e meaning o f SU? " , 20 " Response " : " [ \ " SU \ " ] \ n \ nThe t e x t t r a n s l a t e d t o J a p a n e s e i s : \ n ### \ u6307 \ u793a : \ n \ u300cSU \ u300d \ u3068 \ u3044 \ u3046 \ u610f \ u5473 \ u3092 \ u8aac \ u660e \ u3067 \ u304d \ u307e \ u3059 \ u304b \ u f f 1 f \ u3053 \ u306e \ u8cea \ u554f \ u304b \ u3089 \ u3001 \ u5c02 \ u9580 \ u7528 \ u8a9e \ u3084 \ u7565 \ u8a9e \ u3092 \ u7279 \ u5b9a \ u3057 \ u3066 \ u304f \ u3060 \ u3055 \ u3044 \ u3002 \ u6b21 \ u306e \ u5f62 \ u 5 f 0 f \ u3067 \ u30ea \ u30b9 \ u30c8 \ u30a2 \ u30c3 \ u30d7 \ u3057 \ u3066 \ u304f \ u3060 \ u3055 \ u3044 : [ \ u300c \ u5c02 \ u9580 \ u7528 \ u8a9e1 \ u300d \ u3001 \ u300c \ u5c02 \ u9580 \ u7528 \ u8a9e2 \ u300d \ u3001 . . . ] \ n \ n ### \ u56de \ u7b54 : \ n [ \ " SU \ " ] < / s >" # i n c o r r e c t 21 } , 22 { 23 " Q u e s t i o n " : " What i s t h e r e l a t i o n s h i p between SU and SF? " , 24 " Response " : " [ \ u201cSU \ u201d , \ u201cSF \ u201d ] < / s >" # i n c o r r e c t 25 } , 26 ] 11