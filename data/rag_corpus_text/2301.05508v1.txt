arXiv:2301.05508v1 [cs.IR] 13 Jan 2023 Do the Findings of Document and Passage Retrieval Generalize to the Retrieval of Responses for Dialogues? Gustavo Penha and Claudia Hauﬀ TU Delft {g.penha-1,c.hauff}@tudelft.nl Abstract. A number of learned sparse and dense retrieval approaches have recently been proposed and proven eﬀective in tasks suc h as pas- sage retrieval and document retrieval. In this paper we anal yze with a replicability study if the lessons learned generalize to t he retrieval of responses for dialogues, an important task for the increa singly pop- ular ﬁeld of conversational search. Unlike passage and docu ment re- trieval where documents are usually longer than queries, in response ranking for dialogues the queries (dialogue contexts) are o ften longer than the documents (responses). Additionally, dialogues h ave a particu- lar structure, i.e. multiple utterances by diﬀerent users. With these dif- ferences in mind, we here evaluate how generalizable the fol lowing major ﬁndings from previous works are: (F1) query expansion outperforms a no-expansion baseline; (F2) document expansion outperforms a no- expansion baseline; (F3) zero-shot dense retrieval underperforms sparse baselines; (F4) dense retrieval outperforms sparse baselines; (F5) hard negative sampling is better than random sampling for traini ng dense models. Our experiments 1—based on three diﬀerent information-seeking dialogue datasets—reveal that four out of ﬁve ﬁndings ( F2–F5) gener- alize to our domain. 1 Introduction Conversational search is concerned with creating agents that satisfy an informa- tion need by means of a mixed-initiative conversation through natural language interaction, rather than through the traditional search engine r esults page. A popular approach to conversational search is retrieval-based [3]: given an on- going conversation and a large corpus of historic conversations, r etrieve the response that is best suited from the corpus [45,48,28,47,11]. Due to the ef- fectiveness of heavily pre-trained transformer-based language models such as BERT [4], they have become the predominant approach for convers ation re- sponse re-ranking [28,43,53,8,42]. The most common evaluation procedure for conversation respons e re-ranking consists of re-ranking a limited set of n candidate responses (including the 1 https://github.com/Guzpenha/transformer_rankers/tree/full_rank_retrieval_dialogues. 2 Gustavo Penha and Claudia Hauﬀ ground-truth response(s)), followed by measuring the number o f relevant re- sponses found in the ﬁrst K positions—Recalln@K [52]. Since the entire collec- tion of available responses is typically way bigger 2 than such a set of candidates, this setup is in fact a selection problem, where we have to choose the correct response out of a few options. This evaluation overlooks the ﬁrst- stage retrieval step, which retrieves a set of n responses to be re-ranked. If the ﬁrst-stage model, e.g. BM25, fails to retrieve relevant responses, the entire pipeline f ails. Motivated by a lack of research on the ﬁrst-stage retrieval step , we are inter- ested in answering in our replicability study whether the considerable knowledge obtained on document and passage retrieval tasks generalizes to the dialogue do- main. Unlike document and passage retrieval where the documents are generally longer than the queries, in response retrieval for dialogues the qu eries (dialogue contexts) tend to be longer than the documents (responses). A second important diﬀerence is the structure induced by the dialogue as seen in Table 1. Table 1. Comparison between passage retrieval and response retriev al for dialogues. In §3 we deﬁne the task of First-stage Retrieval for Dialogues . Colors symbolize the information-seeker and information-provider. p+/r+ are the relevant passage/response. Passage Retrieval First-stage Retrieval for Dialogues Input Query q Dialogue context U = {u1, u2, ..., uτ } Example q: what is theraderm used for u1: I want a ﬁrewall that will protect me but more of that to monitor any connection in or out of my mac [...] u2: {url} allows you to map joypad buttons to keyboard keys and [...] u3: Do the diagonals for the analog stick work correctly for you? [...] Output Ranked list of passages Ranked list of responses Example p+: Thera-Derm Lotion is used as a moisturizer to treat or prevent dry, rough, scaly, [...] r+: In the ”Others” tab, try [...] Given the diﬀerences between the domains, we verify empirically acro ss three information-seeking datasets and 1.7M queries, the generalizability of ﬁve ﬁnd- ings ( F1 to F5) from the passage and document retrieval literature related to state-of-the-art sparse and dense retrieval models. We are mo tivated in our se- lection of these ﬁve ﬁndings by their impact in prior works (cf. §2). Our results show that four out of ﬁve previous ﬁndings do indeed generalize to o ur domain: 2 While for most benchmarks [52] we have only 10–100 candi- dates, a working system with the Reddit data from PolyAI https://github.com/PolyAI-LDN/conversational-datasets would need to retrieve from 3.7 billion responses. From Document and Passage Retrieval to Response Retrieval f or Dialogues 3 F1 ✗ 3 Dialogue context (i.e. query) expansion outperforms a no-expans ion base- line [1,18,49,21]. F2 ✓ Response (i.e. document) expansion outperforms a no-expansion baseline [25,21,19] if the expansion model is trained to generate the most recent context (last utterance 4 of the dialogue) instead of older context (all utterances). F3 ✓ Dense retrieval in the zero-shot 5 setting underperforms sparse baselines [34,41] except when it goes through intermediate training on large a mounts of out-of-domain data. F4 ✓ Dense retrieval with access to target data 6 outperforms sparse baselines [7,15,34] if an intermediate training step on out-of-domain data is pe rformed before the ﬁne-tuning on target data. F5 ✓ Harder negative sampling techniques lead to eﬀectiveness gains [46,5 1] if a denoising technique is used to reduce the number of false ne gative samples. Our results indicate that most ﬁndings translate to the domain of re trieval of responses for dialogues. A promising future direction is thus to s tart with successful models from other domains—for which there are more d atasets and previous research—and study how to adapt and improve them for r etrieval-based conversational search. 2 Related Work In this section we ﬁrst discuss current research in retrieval-base d systems for conversational search, followed by reviewing the major ﬁndings of (un)supervised sparse and dense retrieval in the domains of passage and documen t retrieval. 2.1 Ranking and Retrieval of Responses for Dialogues Early neural models for response re-ranking were based on match ing the repre- sentations of the concatenated dialogue context and the repres entation of a re- sponse in a single-turn manner with architectures such as CNN and L STM [23,14]. More complex neural architectures matching each utterance with the response were also explored [54,9,22]. Heavily pre-trained language models such as BERT were ﬁrst shown to be eﬀective by Nogueira and Cho [24] for re-ran king. Such models quickly became a predominant approach for re-ranking in IR [ 21] and were later shown to be eﬀective for re-ranking responses in conve rsations [42,28]. In contrast, the ﬁrst-stage retrieval of responses for a dialog ue received rela- tively little attention [29]. Lan et al. [17] and Tao et al. [38] showed tha t BERT- based dense retrieval models outperform BM25 for ﬁrst-stage r etrieval of re- sponses for dialogues. A limitation of their work is that strong spars e retrieval 3 ✗ indicates that the ﬁnding does not hold in our domain whereas ✓ indicates that it holds in our domain followed by the necessary condition or exception . 4 For example in Table 1 the last utterance is u3. 5 A zero-shot is a model that does not have access to target data , cf. Table 2. 6 Target data is data from the same distribution, i.e. dataset , of the evaluation dataset. 4 Gustavo Penha and Claudia Hauﬀ baselines that have shown to be eﬀective in other retrieval tasks, e.g. BM25 with dialogue context expansion [25] or BM25 with response expansion [49], were not employed for dense retrieval. We do such comparisons here and tes t a total of ﬁve major ﬁndings that have been not been evaluated before by previo us literature on the ﬁrst-stage retrieval of responses for dialogues. 2.2 Dense and Sparse Models for Passage and Document Retriev al Context for F1 Retrieval models can be categorized into two dimensions: su- pervised vs. unsupervised and dense vs. sparse representation s [19]. An unsuper- vised sparse representation model such as BM25 [35] represents eac h document and query with a sparse vector with the dimension of the collection’s v ocabu- lary, having many zero weights due to non-occurring terms. Since t he weights of each term are entirely based on term statistics they are considere d unsupervised methods. Such approaches are prone to the vocabulary mismatch problem [6], as semantic matches are not considered. A way to address such a p roblem is by using query expansion methods. RM3 [1] is a competitive [49] query ex pansion technique that uses pseudo-relevance feedback to add new term s to the queries followed by another ﬁnal retrieval step using the modiﬁed query. Context for F2 A supervised sparse retrieval model can take advantage of the eﬀectiveness of transformer-based language models by chan ging the terms’ weights from collection statistics to something that is learned. Docu ment ex- pansion with a learned model can be considered a learned sparse ret rieval ap- proach [19]. The core idea is to create pseudo documents that hav e expanded terms and use them instead when doing retrieval. Doc2query [25] is a strong su- pervised sparse retrieval baseline that uses a language model to p redict queries that might be issued to ﬁnd a document. The predictions of this mode l are used to create the augmented pseudo documents. Context for F3 and F4 Supervised dense retrieval models 7, such as ANCE [46] and coCodenser [7], represent query and documents in a small ﬁxed -length space, for example of 768 dimensions. Dense retrieval models without acce ss to target data for training—known as the zero-shot scenario—have underperformed sparse methods ( F3). For example, the BEIR benchmark [41] showed that BM25 was superior to dense retrieval from 9–18 (depending on the model) ou t of the 18 datasets in the zero-shot scenario. In contrast, when having ac cess to enough supervision from target data, dense retrieval models have shown to consistently outperform strong sparse baselines [7,15,34] ( F4). 7 A distinction can also be made of cross-encoders and bi-enco ders, where the ﬁrst encode the query and document jointly as opposed to separate ly [40]. Cross-encoders are applied in a re-ranking step due to their ineﬃciency and t hus are not our focus. From Document and Passage Retrieval to Response Retrieval f or Dialogues 5 Context for F5 In order to train neural ranking models, a small set of negative (i.e. non-relevant) candidates are necessary as it is prohibitively ex pensive to use every other document in the collection as negative sample for a q uery. A limitation of randomly selecting negative samples is that they might be t oo easy for the ranking model to discriminate from relevant ones, while for n egative documents that are harder the model might still struggle. For this reason hard negative sampling has been shown to perform better than random s ampling for passage and document retrieval [46,36,51]. 3 First-stage Retrieval for Dialogues In this section we ﬁrst describe the problem of ﬁrst-stage retriev al of responses, followed by the ﬁndings we want to replicate from sparse and dense a pproaches. Problem Deﬁnition The task of ﬁrst-stage retrieval of responses for dialogues, concerns retrieving the best response out of the entire collection given the di- alogue context. Formally, let D = {(Ui, Ri, Yi)}M i=1 be a data set consisting of M triplets: dialogue context, response candidates and response re levance labels. The dialogue context Ui is composed of the previous utterances {u1, u2, ..., uτ } at the turn τ of the dialogue. The candidate responses Ri = {r1, r2, ..., rn} are either ground-truth responses r+ or negative sampled candidates r−, indicated by the relevance labels Yi = {y1, y2, ..., yn}. In previous work, the number of candidates is limited, typically n = 10 [29]. The ﬁndings we replicate here come from passage and document retrieval tasks where there is no limit t o the number of documents or passages that have to be retrieved. Thus, in all o f our ﬁrst-stage retrieval task experiments n is set to the size of the entire collection of responses in the corpus. The number of ground-truth responses is one, the observed re- sponse in the conversational data. The task is then to learn a rank ing function f (.) that is able to generate a ranked list from the entire corpus of res ponses Ri based on their predicted relevance scores f (U, r). F1: Unsupervised Sparse Retrieval We rely on classic retrieval methods, for which the most commonly used baseline is BM25. One of the limitation s of sparse retrieval is the vocabulary mismatch problem. Expansion te chniques are able to overcome this problem by appending new words to the dialogue contexts and responses. For this reason, we here translate a query expan sion technique to the dialogue domain and perform dialogue context expansion with RM3 [1], a competitive unsupervised method that assumes that the top-ra nked responses by the sparse retrieval model are relevant. From these pseudo- relevant responses, words are selected and an expanded dialogue context is created an d subsequently employed by the sparse retrieval method to rank the ﬁnal list of re sponses. The eﬀectiveness of RM3 in the domain of dialogues is the ﬁrst ﬁnd ing that we validate . 6 Gustavo Penha and Claudia Hauﬀ F2: Learned Sparse Retrieval Alternatively, we can expand the responses in the collection with a learned method. To do so we “translate” doc2qu ery [25] into our domain, yielding resp2ctxt. Formally, we ﬁne-tune a generative transformer model G for the task of generating the dialogue context Ui from the ground-truth response r+ i . This model is then used to generate expansions for all responses in the collection, ri = concat(ri, G(ri)). These expansions are appended to the responses and the collection is indexed again—the sparse retrieval method it- self is not modiﬁed, i.e. we continue using BM25. This approach (which w e coin resp2ctxt) leads to two improvements: term re-weighting (adding terms that already exist in the document) and dealing with the vocabulary mismat ch prob- lem (adding new terms). The eﬀectiveness of doc2query in the domain of dialogues is the second ﬁnding that we validate . Unlike passage and document retrieval where the queries are smalle r than the documents, for the retrieval of responses for dialogues the que ries are longer than the documents 8. This is a challenge for the generative model, since generating larger pieces of text is a more diﬃcult problem than smaller ones as the re is more room for errors. Motivated by this, we also explored a modiﬁed version of resp2ctxt that aims to generate only the last utterance of the dia logue context: resp2ctxtlu. This model is trained to generate uτ from r+ i , instead of trying to generate the whole utterance Ui = {u1, u2, ..., uτ }. The underlying premise is that the most important utterance from the dialogue is the last one , and if it is correctly generated by resp2ctxt lu, the sparse retrieval method will be able to ﬁnd the correct response from the collection. F3: Zero-shot Dense Retrieval We rely on methods that learn to represent the dialogue context and the responses separately in a dense embe dding space. Responses are then ranked by their similarity to the dialogue contex t. We rely here on pre-trained language transformer models, such as BERT [4 ] and MP- Net [37], to obtain such representations of the dialogue context an d response. This approach is generally referred to as a bi-encoder model [21] and is an ef- fective family of models 9. A zero-shot model is one that is not trained on the target data. Target data is data from the same distribution, i.e. da taset, of the evaluation dataset. One way of improving the representations of a heavily pre-trained la nguage model for the zero-shot setting is to ﬁne-tune it with intermediate data [33]. Such intermediate data contains triplets of query, relevant document, and negative document and can include multiple datasets. The advantage of addin g this step before employing the representations of the language model is to r educe the 8 For example, while the TREC-DL-2020 passage and document retrieval tasks the queries have between 5–6 terms on average and the passages an d documents have over 50 and 1000 terms respectively, for the information-se eking dialogue datasets used here the dialogue contexts (queries) have between 70 an d 474 terms on average depending on the dataset while the responses (documents) ha ve between 11 and 71. 9 See for example the top models in terms of eﬀectiveness from t he MSMarco bench- mark leaderboards https://microsoft.github.io/msmarco/. From Document and Passage Retrieval to Response Retrieval f or Dialogues 7 gap between the pre-training and the downstream task at hand [3 1,26,30]. In Table 2 we clarify the relationship between pre-training, intermediat e training and ﬁne-tuning. Table 2. The diﬀerent training stages and data, their purposes, exam ples of datasets, and the type of dense model obtained after each stage. Pre-training data Intermediate data T arget data Purpose Learn general represen- tations Learn sentence representations for ranking Learn representations for tar- get distribution Model is Zero-shot Zero-shot Fine-tuned Example Wikipedia MSMarco MANtIS The intermediate training step learns to represent pieces of text ( query and documents) by applying a mean pooling function over the transform er’s ﬁnal layer, which is then used to calculate the dot-product similarity. The loss function employs multiple negative texts from the same batch to learn the rep resentations in a constrastive manner, also known as in-batch negative sampling. Such a procedure learns better text representations than a naive appr oach that uses the [CLS ] token representation of BERT [33,2]. The function f (U, r) is then dot(η(concat(U)), η(r)), where η is the repre- sentation obtained by applying the mean pooling function over the las t layer of the transformer model, and concat(U) = u1 | [U ] | u2 | [T ] | ... | uτ , where | indicates the concatenation operation. The utterances from the context U are concatenated with special separator tokens [ U ] and [ T ] indicating end of utter- ances and turns 10. The eﬀectiveness of a zero-shot bi-encoder model in the domain of dialogues is the third ﬁnding we validate. F4: Fine-tuned Dense Retrieval The standard procedure is to ﬁne-tune dense models with target data that comes from the same dataset t hat the model will be evaluated. Since we do not have labeled negative responses, a ll the remain- ing responses in the dataset can be thought of as non-relevant to the dialogue context. Computing the probability of the correct response over all other re- sponses in the dataset would give us P (r | U ) = P (U ,r) ∑ k P (U ,rk) . This computation is prohibitively expensive, and the standard procedure is to approxim ate it using a few negative samples. The negative sampling task is then as follows: given the dialogue context U ﬁnd challenging responses r− that are non-relevant for U. Negative sampling can be seen as a retrieval task, where one can us e a model to retrieve negatives by applying a retrieval function to the collection of responses using U as the query. With such a dataset at hand, we continue the training—after the int ermediate step—in the same manner as done by the intermediate training step, with the following cross-entropy loss function 11 for a batch with size B: 10 The special tokens [ U ] and [ T ] will not have any meaningful representation in the zero-shot setting, but they can be learned on the ﬁne-tuning step. 11 We refer to this loss as MultipleNegativesRankingLoss. 8 Gustavo Penha and Claudia Hauﬀ J (U, r, θ) = − 1 B ∑B i=1 [ f (Ui, ri) − log ∑B j=1,j!=i ef (Ui,rj) ] , where f (U, r) is the dot-product of the mean pooling of the last layer of the transformer model. The eﬀectiveness of a ﬁne-tuned bi-encoder model in the domain of dialogues is the fourth ﬁnding we validate he re. F5: Hard Negative Sampling A limitation of random samples is that they might be too easy for the ranking model to discriminate from relevan t ones, while for negative documents that are hard the model might still str uggle. For this reason, another popular approach is to use a ranking model to retrieve negative documents using the given query with a classic retrieval te chnique such as BM25. This leads to ﬁnding negative documents that are closer to the query in the sparse representation space, and thus they are harder negatives . Since dense retrieval models have been outperforming sparse retrieva l in a number of cases with available training data, more complex negative sampling tec hniques making use of dense retrieval have also been proposed [46,12]. The eﬀectiveness of hard negative sampling for a bi-encoder model in the domai n of dialogues is the ﬁfth ﬁnding we validate here. 4 Experimental Setup In order to compare the diﬀerent sparse and dense approaches w e consider three large-scale information-seeking conversation datasets 12: MSDialog [32] contains 246K context-response pairs, built from 35.5K information seeking conversa- tions from the Microsoft Answer community, a QA forum for severa l Microsoft products; MANtIS [27] contains 1.3 million context-response pairs built from con- versations of 14 Stack Exchange sites, such as askubuntu and travel; UDCDSTC8 [16] contains 184k context-response pairs of disentangled Ubuntu IR C dialogues. Implementation Details For BM25 and BM25+RM313 we rely on the pyserini implementations [20]. In order to train resp2ctxt expansion method s we rely on the Huggingface transformers library [44], using the t5-base model. We ﬁne- tune the T5 model for 2 epochs, with a learning rate of 2e-5, weight decay of 0.01, and batch size of 5. When augmenting the responses with resp2ctx t we follow docT5query [25] and append three diﬀerent context predictions, using sampling and keeping the top-10 highest probability vocabulary tokens. For the zero-shot dense models, we rely on the SentenceTransformers [33] model releases. The library uses Hugginface’s transformers for the pre-trained models such as BERT [4] and MPNet [37]. For the bi-encoder models, we use the 12 MSDialog is available at https://ciir.cs.umass.edu/downloads/msdialog/; MANtIS is available at https://guzpenha.github.io/MANtIS/; UDCDSTC8 is available at https://github.com/dstc8-track2/NOESIS-II . 13 We perform hyperparameter tuning using grid search on the nu mber of expansion terms, number of expansion documents, and weight. From Document and Passage Retrieval to Response Retrieval f or Dialogues 9 pre-trained all-mpnet-base-v2 weights which were the most eﬀective in our initial experiments, compared with other pre-trained models 14. When ﬁne-tuning the dense retrieval models, we rely on the MultipleNegativesRankingLoss, which ac- cepts a number of hard negatives, and also uses the remaining in-ba tch random negatives to train the model. We use a total of 10 negative samples f or dialogue context. We ﬁne-tune the dense models for a total of 10k steps, and every 100 steps we evaluate the models on a re-ranking task that selects the relevant re- sponse out of 10 responses. We use the re-ranking validation MAP t o select the best model from the whole training to use in evaluation. We use a batc h size of 5, with 10% of the training steps as warmup steps. The learning rate is 2e-5 and the weight decay is 0.01. We use FAISS [13] to perform the similarity search. Evaluation To evaluate the eﬀectiveness of the retrieval systems we use R@K. We thus evaluate the models’ capacity of ﬁnding the correct respo nse out of the whole possible set of responses 15. We perform Students t-tests at the 0.95 conﬁdence level with Bonferroni correction to compare statistic al signiﬁcance of methods. Comparisons are performed across the results for eac h dialogue context. 5 Results In this section, we discuss our empirical results along with the ﬁve ma jor ﬁnd- ings from previous work (Section 1) in turn. Table 3 contains the main results regarding F1 to F4. Table 5 contains the results for F5. F1 ✗ Query expansion via RM3 leads to improvements over not using query expansion [1,18,49,21]. BM25+RM3 (row 1b) does not improve over BM25 (1a) on any of the three conversational datasets analyzed . We performed thorough hyperparameter ﬁne-tuning and no combination of the R M3 hyperpa- rameters outperformed BM25. This indicates that F1 does not hold for the task of response retrieval for dialogues. A manual analysis of the new terms appended to a sample of 60 dialogu e contexts by one of the paper’s authors revealed that only 18% of t hem have at least one relevant term added based on our best judgment. Unlike w eb search where the query is often incomplete, under-speciﬁed, and ambiguo us, in the information-seeking datasets employed here the dialogue context (query) is quite detailed and has more terms than the responses (documents). We hypothesize that because the dialogue contexts are already quite descriptive, the task of expansion is trickier in this domain and thus we observe many dialogues for which the added terms are noisy. 14 The alternative models we considered are those listed in the model overview section at https://www.sbert.net/docs/pretrained_models.html. 15 The standard evaluation metric in conversation response ra nking [50,8,39] is recall at position K with n candidates Rn@K. Since we are focused on the ﬁrst-stage retrieval we set n to be the entire collection of answers 10 Gustavo Penha and Claudia Hauﬀ Table 3. Results for the generalizability of F1–F4. Bold values indi cate the high- est recall for each type of approach. Superscripts indicate statistically signiﬁcant im- provements using Students t-test with Bonferroni correcti on. †=other methods from the same group 1=best from unsupervised sparse retrieval ; 2=best from supervised sparse retrieval; 3=best from zero-shot dense retrieval. For example, in F3 † indicates that row (3d) improves over rows (3a–c), 1 indicates that it improves over row (1a) and 2 indicates it improves over row (2b). MANtIS MSDialog UDC DSTC8 R@1 R@10 R@1 R@10 R@1 R@10 (0) Random 0.000 0.000 0.000 0.001 0.000 0.001 Unsupervised sparse F1 (1a) BM25 0.133† 0.299† 0.064† 0.177† 0.027† 0.070† (1b) BM25 + RM3 0.073 0.206 0.035 0.127 0.011 0.049 Supervised sparse F2 (2a) BM25 + resp2ctxt 0.135 0.309 0.074 0.208 0.028 0.067 (2b) BM25 + resp2ctxt lu 0.147†1 0.325†1 0.0751 0.2021 0.029 0.076 Zero-shot dense (ModelIntermediateData ) F3 (3a) ANCE 600K−M SM arco 0.048 0.111 0.050 0.124 0.010 0.028 (3b) TAS-B400K−M SM arco 0.062 0.143 0.060 0.157 0.019 0.050 (3c) Bi-encoder 215M −mul 0.138 0.297 0.108 0.277 0.023 0.076 (3d) Bi-encoder 1.17B−mul 0.155†1 0.341†12 0.147†12 0.339†12 0.041† 0.097†12 Fine-tuned dense (ModelN egativeSampler ) F4 (4a) Bi-encoder Random(0) 0.130 0.307 0.168 123 0.387123 0.05012 0.128123 F2 ✓ Document expansion via resp2ctxt leads to improvements ove r no expansion [25,21,19]. We ﬁnd that a naive approach to response expansion improves marginally in two of the three datasets with BM25+resp2ct xt (2a) outperforming BM25 (1a). However, the proposed modiﬁcation of predicting only the last utterance of the dialogue (resp2ctxt lu) performs better than predicting the whole utterance, as shown by BM25+resp2ctxt lu’s (2b) higher recall values. In the MANtIS dataset the R@10 goes from 0.309 when using the model trained to predict the dialogue context to 0.325 when using the one trained to p redict only the last utterance of the dialogue context. We thus ﬁnd that F2 generalizes to response retrieval for dialogues, especially when predi cting only the last utterance of the context 16. In order to understand what the response expansion methods ar e doing most—term re-weighting or adding novel terms—we present the pe rcentage of novel terms added by both methods in Table 4. The table shows that resp2ctxtlu does more term re-weighting than adding new words when compared to resp2ctxt 16 As future work, more sophisticated techniques can be used to determine which parts of the dialogue context should be predicted. From Document and Passage Retrieval to Response Retrieval f or Dialogues 11 Table 4. Statistics of the augmentations for resp2ctxt and resp2ctx tlu. New words are the ones that did not exist in the document before. MANtIS MSDialog UDC DSTC8 Context avg length 474.12 426.08 76.95 Response avg length 42.58 71.38 11.06 Aug. avg length - resp2ctxt 494.23 596.99 202.3 Aug. avg length - resp2ctxt lu 138.5 135.29 72.57 % new words - resp2ctxt 71% 69% 71% % new words - resp2ctxt lu 59% 37% 63% (53% and 70% on average are new words respectively and thus 47% v s 30% are changing the weights by adding existing words), generating overall smaller aug- mentations (115.45 vs 431.17 on average respectively). F3 ✓ Sparse retrieval outperforms zero-shot dense retrieval [3 4,41]. Sparse retrieval models are more eﬀective than the majority of ze ro-shot dense models, as shown by the comparison of rows (1a–b), and (2a–b) wit h rows (3a– c). However, a dense retrieval model that has gone through inte rmediate training on large and diverse datasets including dialogues is more eﬀective tha n a strong sparse retrieval model, as we see by comparing row (3d) with row (2 b) in Table 3. For example, while the zero-shot dense retrieval models based only on the MSMarco dataset (3a–b) perform on average 35% worse than the strong s parse baseline (2b) in terms of R@10 for the MSDialog dataset, the zero-shot model trained with 1.17B instances on diverse data (3d) is 68% better than the sparse baseline (2b). When using a bigger amount of intermediate training da ta17, we see that the zero-shot dense retrieval model (3d) is able to outp erform the sparse retrieval baseline by margins of 33% of R@10 on average across dat asets. We thus show that F3 only generalizes to response retrieval f or dialogues if we do not employ a large set of diverse intermedi ate data. As expected, the closer the intermediate training data distribution is to the eval- uation data, the better the dense retrieval model performs. Th e results indicate that a good zero-shot retrieval model needs to go through inter mediate training on a large set of training data coming from multiple datasets to gener alize well to diﬀerent domains and outperform strong sparse retrieval bas elines. F4 ✓ Dense models with access to target training data outperform sparse models [7,15,34]. First, we see that ﬁne-tuning the dense retrieval model, which has gone through intermediate training already, with ra ndom sampling—row (4a) in Table 3—achieves the best overall eﬀectivenes s in two of the three datasets. This result shows that F4 generalizes to the task of 17 For the full description of the intermediate data see https://huggingface.co/sentence-transformers/all-mpnet-base-v2 . 12 Gustavo Penha and Claudia Hauﬀ response retrieval for dialogues when employing intermedi ate train- ing18. Having access to the target data as opposed to only the intermed iate training data means that the representations learned by the mode l are closer to the true distribution of the data. We hypothesize that ﬁne-tuning the bi-encoder for MANtIS (4a) is harmful because the intermediate data contains Stack Exchange respons es. In this way, the set of dialogues of Stack Exchange that MANtIS encompasses might be serving only to overﬁt the intermediate representations. As evidence for this hypothe- sis, we found that (I) the learning curves ﬂatten quickly (as oppos ed to other datasets) and (II) ﬁne-tuning another language model that doe s not have Stack Exchange data ( MSMarco) in their ﬁne-tuning, bi-encoder bert−base (3c), improves the eﬀectiveness with statistical signiﬁcance from 0.092 R@10 to 0.2 05 R@10. F5 ✓ Hard negative sampling is better than random sampling for training dense retrieval models [46,51]. Surprisingly we ﬁnd that naively using more eﬀective models to select negative candidates is detrimen tal to the ef- fectiveness of the dense retrieval model (see Hard negative sam pling in Table 5). We observe this phenomenon when using diﬀerent language models, w hen switch- ing intermediate training on or oﬀ for all datasets, and when using an alternative contrastive loss [10] that does not employ in-batch negative samp ling19. After testing for a number of hypotheses that might explain why ha rder negatives do not improve the eﬀectiveness of the dense retrieval model, we found that false negative samples increase signiﬁcantly when using better negative sampling methods. False negatives are responses that are potent ially valid for the context. Such relevant responses lead to unlearning relevant matches between context and responses as they receive negative labels. See below a n example of a false negative sample retrieved by the bi-encoder model (row 3d o f Table 3): Dialogue context ( U ): hey... how long until dapper comes out? [U] 14 days [...] [ U] i thought it was coming out tonight Correct response ( r+): just kidding couple hours F alse negative sample ( r−): there is a possibility dapper will be delayed [...] mean- while, dapper discussions should occur in ubuntu+1 Denoising techniques try to solve this problem by reducing the numbe r of false negatives. We employ a simple approach that instead of using the top -ranked responses as negative responses, we use the bottom responses of the top-ranked responses as negatives 20. This decreases the chances of obtaining false positives and if k << |D| we will not obtain random samples. Our experiments in Table 5 reveal that this denoising technique, row (3b), increases the eﬀe ctiveness for harder negative samples, beating all models from Table 3 for two of t he three 18 Our experiments show that when we do not employ the intermedi ate training step the ﬁne-tuned dense model does not generalize well, with row (3d ) performance dropping to 0.172, 0.308 and 0.063 R@10 for MANtIS, MSDialog and UDCDSTC8 respectively. 19 The results are not shown here due to space limitations 20 For example, if we retrieve k = 100 responses, instead of using responses from top positions 1–10, we use responses 91–100 from the bottom of th e list. From Document and Passage Retrieval to Response Retrieval f or Dialogues 13 datasets. The results indicate that F5 generalizes to the task of respo nse retrieval for dialogues only when employing a denoising tec hnique. Table 5. Results for the generalizability of F5—with and without a de noising strategy for hard negative sampling. Superscripts indicate statist ically signiﬁcant improvements using Students t-test with Bonferroni correction . †=signiﬁcance against the random sampling baseline, ‡=signiﬁcance against hard negative sampling without denoi sing. MANtIS MSDialog UDC DSTC8 R@10 R@10 R@10 Baseline (1) Bi-encoder Random 0.307 0.387 0.128 Hard negative sampling (2a) Bi-encoder BM 25 0.271 0.316 0.087 (2b) Bi-encoder Bi−encoder 0.146 0.306 0.051 Denoised hard negative sampling (3a) Bi-encoder BM 25 0.257 0.358 ‡ 0.121‡ (3b) Bi-encoder Bi−encoder 0.316†‡ 0.397†‡ 0.107‡ 6 Conclusion In this work, we tested if the knowledge obtained in dense and spars e retrieval from experiments on the tasks of passage and document retrieva l generalizes to the ﬁrst-stage retrieval of responses for dialogues. Our replica bility study reveals that while most ﬁndings do generalize to our domain, a simple translatio n of the models is not always successful. A careful analysis of the domain in question might reveal better ways to adapt techniques. As future work, we believe an important direction is to evaluate learn ed sparse methods that do weighting and expansion for both the quer ies and doc- uments [5]—while resp2ctxt is able to both change the weights of the t erms in the response (by repeating existing terms) and expand terms (by adding novel terms), it is not able to do weighting and expansion for the dialogue co ntexts. Acknowledgements This research has been supported by NWO projects SearchX (639.022.722) and NWO Aspasia (015.013.027). References 1. Abdul-Jaleel, N., Allan, J., Croft, W.B., Diaz, F., Larke y, L., Li, X., Smucker, M.D., Wade, C.: Umass at trec 2004: Novelty and hard. Computer Scie nce Department Faculty Publication Series p. 189 (2004) 14 Gustavo Penha and Claudia Hauﬀ 2. Aghajanyan, A., Gupta, A., Shrivastava, A., Chen, X., Zet tlemoyer, L., Gupta, S.: Muppet: Massive multi-task representations with pre-ﬁnet uning. arXiv preprint arXiv:2101.11038 (2021) 3. Anand, A., Cavedon, L., Joho, H., Sanderson, M., Stein, B. : Conversational search (dagstuhl seminar 19461). In: Dagstuhl Reports. vol. 9. Sch loss Dagstuhl-Leibniz- Zentrum f¨ ur Informatik (2020) 4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre -training of deep bidirec- tional transformers for language understanding. arXiv pre print arXiv:1810.04805 (2018) 5. Formal, T., Lassance, C., Piwowarski, B., Clinchant, S.: Splade v2: Sparse lexical and expansion model for information retrieval. arXiv prepr int arXiv:2109.10086 (2021) 6. Furnas, G.W., Landauer, T.K., Gomez, L.M., Dumais, S.T.: The vocabulary prob- lem in human-system communication. Communications of the A CM 30(11), 964– 971 (1987) 7. Gao, L., Callan, J.: Unsupervised corpus aware language m odel pre-training for dense passage retrieval. arXiv preprint arXiv:2108.05540 (2021) 8. Gu, J.C., Li, T., Liu, Q., Ling, Z.H., Su, Z., Wei, S., Zhu, X .: Speaker-aware bert for multi-turn response selection in retrieval-based chat bots. In: Proceedings of the 29th ACM International Conference on Information & Knowled ge Management. pp. 2041–2044 (2020) 9. Gu, J.C., Ling, Z.H., Liu, Q.: Interactive matching netwo rk for multi-turn response selection in retrieval-based chatbots. In: proceedings of the 28th ACM international conference on information and knowledge management. pp. 23 21–2324 (2019) 10. Hadsell, R., Chopra, S., LeCun, Y.: Dimensionality redu ction by learning an invari- ant mapping. In: 2006 IEEE Computer Society Conference on Co mputer Vision and Pattern Recognition (CVPR’06). vol. 2, pp. 1735–1742. I EEE (2006) 11. Han, J., Hong, T., Kim, B., Ko, Y., Seo, J.: Fine-grained p ost-training for im- proving retrieval-based dialogue systems. In: Proceeding s of the 2021 Conference of the North American Chapter of the Association for Computa tional Linguis- tics: Human Language Technologies. pp. 1549–1558. Associa tion for Computational Linguistics, Online (Jun 2021). https://doi.org/10.1865 3/v1/2021.naacl-main.122, https://aclanthology.org/2021.naacl-main.122 12. Hofst¨ atter, S., Lin, S.C., Yang, J.H., Lin, J., Hanbury , A.: Eﬃciently teaching an eﬀective dense retriever with balanced topic aware samplin g. In: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 113–122 (2021) 13. Johnson, J., Douze, M., J´ egou, H.: Billion-scale simil arity search with GPUs. IEEE Transactions on Big Data 7(3), 535–547 (2019) 14. Kadlec, R., Schmid, M., Kleindienst, J.: Improved deep l earning baselines for ubuntu corpus dialogs. arXiv preprint arXiv:1510.03753 (2 015) 15. Karpukhin, V., O˘ guz, B., Min, S., Lewis, P., Wu, L., Edun ov, S., Chen, D., Yih, W.t.: Dense passage retrieval for open-domain question ans wering. arXiv preprint arXiv:2004.04906 (2020) 16. Kummerfeld, J.K., Gouravajhala, S.R., Peper, J.J., Ath reya, V., Gu- nasekara, C., Ganhotra, J., Patel, S.S., Polymenakos, L.C. , Lasecki, W.: A large-scale corpus for conversation disentanglement . Pro- ceedings of the 57th Annual Meeting of the Association for Co m- putational Linguistics (2019). https://doi.org/10.1865 3/v1/p19-1374, http://dx.doi.org/10.18653/v1/P19-1374 From Document and Passage Retrieval to Response Retrieval f or Dialogues 15 17. Lan, T., Cai, D., Wang, Y., Su, Y., Mao, X.L., Huang, H.: Ex ploring dense retrieval for dialogue response selection. arXiv preprint arXiv:211 0.06612 (2021) 18. Lin, J.: The simplest thing that can possibly work: pseud o-relevance feedback using text classiﬁcation. arXiv preprint arXiv:1904.08861 (201 9) 19. Lin, J.: A proposed conceptual framework for a represent ational approach to in- formation retrieval. arXiv preprint arXiv:2110.01529 (20 21) 20. Lin, J., Ma, X., Lin, S.C., Yang, J.H., Pradeep, R., Nogue ira, R.: Pyserini: A Python toolkit for reproducible information retrieval res earch with sparse and dense representations. In: Proceedings of the 44th Annual I nternational ACM SI- GIR Conference on Research and Development in Information R etrieval (SIGIR 2021). pp. 2356–2362 (2021) 21. Lin, J., Nogueira, R., Yates, A.: Pretrained transforme rs for text ranking: Bert and beyond. Synthesis Lectures on Human Language Technologies 14(4), 1–325 (2021) 22. Lin, Z., Cai, D., Wang, Y., Liu, X., Zheng, H.T., Shi, S.: T he world is not binary: Learning to rank with grayscale data for dialogue response s election. arXiv preprint arXiv:2004.02421 (2020) 23. Lowe, R., Pow, N., Serban, I., Pineau, J.: The ubuntu dial ogue corpus: A large dataset for research in unstructured multi-turn dialogue s ystems. arXiv preprint arXiv:1506.08909 (2015) 24. Nogueira, R., Cho, K.: Passage re-ranking with bert. arX iv preprint arXiv:1901.04085 (2019) 25. Nogueira, R., Lin, J., Epistemic, A.: From doc2query to d octttttquery. Online preprint 6 (2019) 26. Peeters, R., Bizer, C., Glavaˇ s, G.: Intermediate train ing of bert for product match- ing. small 745(722), 2–112 (2020) 27. Penha, G., Balan, A., Hauﬀ, C.: Introducing mantis: a nov el multi-domain infor- mation seeking dialogues dataset. arXiv preprint arXiv:19 12.04639 (2019) 28. Penha, G., Hauﬀ, C.: Curriculum learning strategies for ir: An empirical study on conversation response ranking. arXiv preprint arXiv:1912 .08555 (2019) 29. Penha, G., Hauﬀ, C.: Challenges in the evaluation of conv ersational search systems. In: Converse@ KDD (2020) 30. Poth, C., Pfeiﬀer, J., R¨ uckl´ e, A., Gurevych, I.: What t o pre-train on? eﬃcient intermediate task selection. arXiv preprint arXiv:2104.0 8247 (2021) 31. Pruksachatkun, Y., Phang, J., Liu, H., Htut, P.M., Zhang , X., Pang, R.Y., Vania, C., Kann, K., Bowman, S.R.: Intermediate-task transfer lea rning with pretrained models for natural language understanding: When and why doe s it work? arXiv preprint arXiv:2005.00628 (2020) 32. Qu, C., Yang, L., Croft, W.B., Trippas, J.R., Zhang, Y., Q iu, M.: Analyzing and characterizing user intent in information-seeking conver sations. In: The 41st inter- national acm sigir conference on research & development in i nformation retrieval. pp. 989–992 (2018) 33. Reimers, N., Gurevych, I.: Sentence-bert: Sentence emb eddings using siamese bert- networks. In: Proceedings of the 2019 Conference on Empiric al Methods in Nat- ural Language Processing. Association for Computational L inguistics (11 2019), https://arxiv.org/abs/1908.10084 34. Ren, R., Qu, Y., Liu, J., Zhao, W.X., Wu, Q., Ding, Y., Wu, H ., Wang, H., Wen, J.R.: A thorough examination on zero-shot dense retrie val. arXiv preprint arXiv:2204.12755 (2022) 35. Robertson, S.E., Walker, S.: Some simple eﬀective appro ximations to the 2-poisson model for probabilistic weighted retrieval. In: SIGIR’94. pp. 232–241. Springer (1994) 16 Gustavo Penha and Claudia Hauﬀ 36. Robinson, J., Chuang, C.Y., Sra, S., Jegelka, S.: Contra stive learning with hard negative samples. arXiv preprint arXiv:2010.04592 (2020) 37. Song, K., Tan, X., Qin, T., Lu, J., Liu, T.Y.: Mpnet: Maske d and permuted pre- training for language understanding. Advances in Neural In formation Processing Systems 33, 16857–16867 (2020) 38. Tao, C., Feng, J., Liu, C., Li, J., Geng, X., Jiang, D.: Bui lding an eﬃcient and eﬀective retrieval-based dialogue system via mutual le arning. arXiv preprint arXiv:2110.00159 (2021) 39. Tao, C., Wu, W., Xu, C., Hu, W., Zhao, D., Yan, R.: Multi-re presentation fusion network for multi-turn response selection in retrieval-ba sed chatbots. In: WSDM. pp. 267–275 (2019) 40. Thakur, N., Reimers, N., Daxenberger, J., Gurevych, I.: Augmented sbert: Data augmentation method for improving bi-encoders for pairwis e sentence scoring tasks. arXiv preprint arXiv:2010.08240 (2020) 41. Thakur, N., Reimers, N., R¨ uckl´ e, A., Srivastava, A., Gurevych, I.: Beir: A heteroge- nous benchmark for zero-shot evaluation of information ret rieval models. arXiv preprint arXiv:2104.08663 (2021) 42. Whang, T., Lee, D., Lee, C., Yang, K., Oh, D., Lim, H.: An eﬀ ective do- main adaptive post-training method for bert in response sel ection. arXiv preprint arXiv:1908.04812 (2019) 43. Whang, T., Lee, D., Oh, D., Lee, C., Han, K., Lee, D.h., Lee , S.: Do response selection models really know what’s next? utterance manipu lation strategies for multi-turn response selection. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence. vol. 35, pp. 14041–14049 (2021) 44. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C. , Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al.: Huggingface’s tr ansformers: State-of- the-art natural language processing. arXiv preprint arXiv :1910.03771 (2019) 45. Wu, Y., Wu, W., Xing, C., Zhou, M., Li, Z.: Sequential matc hing network: A new architecture for multi-turn response selection in retriev al-based chatbots. In: ACL. pp. 496–505 (2017) 46. Xiong, L., Xiong, C., Li, Y., Tang, K.F., Liu, J., Bennett , P., Ahmed, J., Over- wijk, A.: Approximate nearest neighbor negative contrasti ve learning for dense text retrieval. arXiv preprint arXiv:2007.00808 (2020) 47. Yang, L., Qiu, M., Qu, C., Chen, C., Guo, J., Zhang, Y., Cro ft, W.B., Chen, H.: Iart: Intent-aware response ranking with transformers in information-seeking conversation systems. arXiv preprint arXiv:2002.00571 (2 020) 48. Yang, L., Qiu, M., Qu, C., Guo, J., Zhang, Y., Croft, W.B., Huang, J., Chen, H.: Response ranking with deep matching networks and extern al knowledge in information-seeking conversation systems. In: SIGIR. pp. 245–254 (2018) 49. Yang, W., Lu, K., Yang, P., Lin, J.: Critically examining the” neural hype” weak baselines and the additivity of eﬀectiveness gains from neu ral ranking models. In: Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval. pp. 1129–1132 (2019 ) 50. Yuan, C., Zhou, W., Li, M., Lv, S., Zhu, F., Han, J., Hu, S.: Multi-hop selector network for multi-turn response selection in retrieval-ba sed chatbots. In: EMNLP. pp. 111–120 (2019) 51. Zhan, J., Mao, J., Liu, Y., Guo, J., Zhang, M., Ma, S.: Opti mizing dense retrieval model training with hard negatives. In: Proceedings of the 4 4th International ACM SIGIR Conference on Research and Development in Informatio n Retrieval. pp. 1503–1512 (2021) From Document and Passage Retrieval to Response Retrieval f or Dialogues 17 52. Zhang, Z., Zhao, H.: Advances in multi-turn dialogue com prehension: A survey. arXiv preprint arXiv:2110.04984 (2021) 53. Zhang, Z., Zhao, H.: Structural pre-training for dialog ue comprehension. arXiv preprint arXiv:2105.10956 (2021) 54. Zhou, X., Li, L., Dong, D., Liu, Y., Chen, Y., Zhao, W.X., Y u, D., Wu, H.: Multi- turn response selection for chatbots with deep attention ma tching network. In: Proceedings of the 56th Annual Meeting of the Association fo r Computational Linguistics (Volume 1: Long Papers). pp. 1118–1127 (2018)