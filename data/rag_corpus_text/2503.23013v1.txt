Preprint. Under review. DAT: Dynamic Alpha T uning for Hybrid Retrieval in Retrieval-Augmented Generation Hsin-Ling Hsu National Chengchi University Taipei, Taiwan 112306092@nccu.edu.tw Jengnan Tzeng∗ National Chengchi University Taipei, Taiwan jengnan@math.nccu.edu.tw Abstract Hybrid retrieval techniques in Retrieval-Augmented Generation (RAG) systems enhance information retrieval by combining dense and sparse (e.g., BM25-based) retrieval methods. However, existing approaches struggle with adaptability, as fixed weighting schemes fail to adjust to different queries. To address this, we propose DAT (Dynamic Alpha Tuning), a novel hybrid retrieval framework that dynamically balances dense retrieval and BM25 for each query. DAT leverages a large language model (LLM) to evaluate the effectiveness of the top-1 results from both retrieval methods, assigning an effectiveness score to each. It then calibrates the optimal weighting factor through effectiveness score normalization, ensuring a more adaptive and query-aware weighting between the two approaches. Empirical results show that DAT consistently significantly outperforms fixed-weighting hybrid retrieval methods across various evaluation metrics. Even on smaller models, DAT delivers strong performance, highlighting its efficiency and adaptability. 1 Introduction Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) systems have emerged as a powerful paradigm for enhancing the factuality and reliability of large language model (LLM) outputs by grounding responses in external knowledge sources. At the core of effective RAG systems lies the retrieval component, which is responsible for identifying and surfacing the most relevant documents from a knowledge base in response to user queries. The quality of retrieval directly impacts the overall performance of RAG systems, making it a critical area for optimization. Hybrid retrieval (Ma et al., 2020; Sawarkar et al., 2024; Berntson, 2023) approaches combining sparse (e.g., BM25) and dense methods have demonstrated superior performance compared to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures semantic relationships that may not involve direct lexical overlap. While the complementary strengths of these methods are well established, effectively balancing their contributions remains challenging. Current approaches (Bruch et al., 2023) typically employ a fixed weighting parameter (α) determined through offline tuning on validation datasets. This static weighting scheme, however, fails to account for the diverse nature of user queries, where the optimal balance between keyword matching and semantic similarity varies significantly based on query characteristics and knowledge base structure. Recent efforts to address this limitation include approaches that assign different α values based on query types (e.g., fact-seeking, concept-seeking, etc.) (Theja, 2024). However, these methods still rely on predetermined categories with fixed weights and often overlook the complex interplay between individual queries and the knowledge base. If this assumption holds—that many queries benefit more from extreme values (i.e., pure BM25 or pure dense ∗Corresponding author 1 arXiv:2503.23013v1 [cs.IR] 29 Mar 2025 Preprint. Under review. retrieval)—then using a compromise value such as α = 0.5, while seemingly optimal on average, may in fact lead to suboptimal performance for most individual queries. This would pose a significant challenge to hybrid retrieval optimization. These limitations and opportunities motivate our research questions: • How can we effectively combine sparse and dense retrieval methods to maximize retrieval performance? • How can a retrieval system adapt to the specific relationship between each query and the knowledge base to determine optimal retrieval parameters? • To what extent can dynamic weight adjustment improve individual query perfor- mance compared to fixed weighting approaches? • Can we design a hybrid retrieval system that balances effectiveness and efficiency by leveraging minimal LLM reasoning? Figure 1: Comparison between the traditional fixed alpha approach (top) and our proposed DAT approach (bottom). While fixed alpha methods use predetermined weights regardless of query characteristics, DAT dynamically adjusts the retrieval weighting coefficient based on query-specific features, optimizing retrieval performance. To address these challenges, we introduce DAT, a novel framework that adaptively adjusts the retrieval weighting coefficient based on query-specific characteristics. The key insight of our approach is that the optimal hybrid weighting α for each query should reflect the relative effectiveness of sparse and dense retrieval methods in that specific context. As shown in Figure 1, DAT dynamically computes an optimal α value for each query while minimizing computational overhead. Our approach evaluates the relative effectiveness of each retrieval method by comparing the top-1 retrieved result from both BM25 and dense retrieval. This design is based on the understanding that sparse retrieval effectiveness depends primarily on term overlap 2 Preprint. Under review. between queries and documents, while dense retrieval relies on effective semantic embed- ding alignment. By assessing only the top result from each method with an LLM-based effectiveness scoring mechanism, we can efficiently determine which retrieval method performs better for a given query without the computational burden of evaluating multiple documents. This lightweight evaluation provides a strong signal for estimating relative retrieval strength for dynamic α calculation. Our contributions are four-fold: • We propose a query-adaptive framework that dynamically calibrates the weighting between sparse and dense retrieval methods, eliminating reliance on predetermined, static α values. • We introduce an LLM-based effectiveness scoring mechanism that evaluates re- trieval effectiveness based on the specific relationship between each query and the knowledge base, rather than on general query characteristics. • We demonstrate that evaluating only the top result from each retrieval method provides sufficient signal for effective weighting decisions, offering a significantly more cost-efficient approach to hybrid retrieval optimization. • We demonstrate through extensive experiments that DAT significantly outperforms fixed-weight hybrid approaches, particularly for queries where retrieval methods exhibit varying effectiveness. Our empirical results show that DAT consistently improves retrieval performance across various evaluation metrics, with particularly substantial gains on challenging queries where standard hybrid approaches struggle. Moreover, we demonstrate that our approach reduces the variance in performance across individual queries, providing more consistent user experiences. 2 Related Work Hybrid retrieval systems combine the complementary strengths of multiple retrieval meth- ods. BM25 (Robertson et al., 1994), the predominant sparse retrieval algorithm, calculates relevance based on term frequency and inverse document frequency, efficiently handling exact keyword matching. Meanwhile, dense retrieval approaches (Karpukhin et al., 2020) leverage vector embeddings to capture semantic relationships beyond lexical overlap. The effectiveness of combining sparse and dense methods has been comprehensively demon- strated in hybrid RAG systems, where improved retrieval quality significantly enhances generation accuracy (Ma et al., 2020; Sawarkar et al., 2024; Berntson, 2023). The challenge of determining optimal weighting coefficients has traditionally been ad- dressed through offline tuning (Bruch et al., 2023), where experiments on validation sets establish a fixed weight that is then applied universally to all future queries, regardless of their characteristics. Some approaches (Jeong et al., 2024) attempt refinement by classify- ing queries into predefined types. LlamaIndex (Theja, 2024) proposed assigning different weights based on predefined query categories, though this approach still relies on static weights per category. Our work differs by leveraging LLMs’ reasoning capabilities to dynamically assess retrieval quality and calibrate weighting parameters per query, without relying on predefined cate- gories or fixed weights from offline tuning. This approach aligns with recent trends in using LLMs as judges (Gu et al., 2024) but applies this concept specifically to adaptive hybrid retrieval optimization. Unlike previous approaches that optimize parameters across entire query sets, DAT addresses the limitations of fixed-weight systems by dynamically adapting to each query’s unique characteristics and its relationship to the knowledge base. 3 Preprint. Under review. 3 Hybrid Retrieval Hybrid retrieval combines sparse and dense retrieval to leverage both keyword matching and semantic understanding. Sparse methods like BM25 score documents based on lexical overlap: BM25(q, d) = n ∑ i=1 IDF(qi) · f (qi, d) · (k1 + 1) f (qi, d) + k1 · (1 − b + b · |d| avgdl ) (1) Dense retrieval encodes queries and documents into vectors using embedding functions Eq and Ed, with similarity computed via cosine similarity: Simdense(q, d) = cos(Eq(q), Ed(d)) = Eq(q) · Ed(d) ||Eq(q)|| · || Ed(d)|| (2) To combine these methods, hybrid systems typically use techniques such as the rela- tiveScoreFusion algorithm (Kulawiak & Hwang, 2023) which balances the influence of different retrieval approaches. First, we normalize the similarity scores from both retrieval methods using min-max scaling: ˜Sdense(q, d) = Simdense(q, d) − mind′∈Ddense(q)(Simdense(q, d′)) maxd′∈Ddense(q)(Simdense(q, d′)) − mind′∈Ddense(q)(Simdense(q, d′)) (3) ˜SBM25(q, d) = BM25(q, d) − mind′∈DBM25(q)(BM25(q, d′)) maxd′∈DBM25(q)(BM25(q, d′)) − mind′∈DBM25(q)(BM25(q, d′)) (4) where ˜Sdense(q, d) and ˜SBM25(q, d) are normalized scores in the range [0, 1]. Hybrid systems typically combine these normalized scores using a fixed weighting parameter α: R(q, d) = α · ˜Sdense(q, d) + (1 − α) · ˜SBM25(q, d) (5) This fixed α is determined via offline tuning on validation data and then applied uniformly to all queries. However, such a static strategy fails to account for the varying nature of user queries, limiting retrieval effectiveness across diverse scenarios. 4 DAT To overcome the limitations of static weighting in hybrid retrieval, we introduce DAT, a query-adaptive framework that dynamically adjusts the retrieval weighting coefficient based on the effectiveness of each method for a given query. The central intuition is that different queries inherently favor different retrieval strategies—some require precise keyword overlap (favoring BM25), while others rely on semantic alignment (favoring dense retrieval). DAT harnesses the reasoning capabilities of large language models (LLMs) to estimate the optimal weighting coefficient α(q) for each query at runtime. Unlike traditional approaches that depend on predefined heuristics or offline-tuned parameters, our method adapts on-the-fly to each query’s unique interaction with the knowledge base. Given a query q, we retrieve the top-1 result from both sparse and dense retrieval methods: dv,1 ∈ Ddense(q) and db,1 ∈ DBM25(q). These top-ranked documents are treated as represen- tative indicators of each method’s retrieval effectiveness for the specific query. This focused sampling strategy minimizes computational cost while providing sufficient signal to inform adaptive weighting decisions. 4.1 LLM-Based Retrieval Effectiveness Scoring A key component of DAT is the use of LLMs as evaluators of retrieval quality. We posit that LLMs, with their deep semantic understanding, can assess the relevance of a retrieved document to the original query and thereby estimate each retrieval method’s relative effectiveness. 4 Preprint. Under review. To formalize this, we define a scoring function S(q, d) = fLLM(q, d), which returns an effec- tiveness score in the discrete range {0, 1, 2, 3, 4, 5}—with higher values indicating greater effectiveness. The scoring rubric is carefully designed to reflect retrieval effectiveness: • 5 points: Direct hit—the retrieved document directly answers the question. • 3–4 points: Good wrong result—the document is conceptually close to the correct answer, indicating high likelihood that correct answers are nearby. • 1–2 points: Bad wrong result—the document is loosely related but misleading, with low likelihood that correct answers are nearby. • 0 points: Completely off-track—the result is totally unrelated to the query. Our prompting strategy (see Appendix A) guides the LLM to prioritize factual alignment and informational completeness over superficial similarity or stylistic matching. The LLM independently evaluates each of the top-1 documents and assigns scores: Sv(q) = S(q, dv,1) for dense retrieval and Sb(q) = S(q, db,1) for BM25. This decoupled assessment ensures that the relative retrieval effectiveness is directly captured and can inform downstream weighting. 4.2 Dynamic Alpha Calculation Using the LLM-assigned scores, we compute the dynamic weighting coefficientα(q) through a case-aware formulation that ensures robust behavior across various retrieval outcomes: α(q) =    0.5, if Sv(q) = 0 and Sb(q) = 0, 1.0, if Sv(q) = 5 and Sb(q) ̸= 5, 0.0, if Sb(q) = 5 and Sv(q) ̸= 5, Sv(q) Sv(q)+Sb(q) otherwise. (6) This rule-based approach ensures: • Equal weighting (0.5) when both retrieval methods fail to return relevant content. • Exclusive preference (1.0 or 0.0) when one method yields a perfect result and the other does not. • Proportional weighting when both methods return partially relevant results. For stability and implementation consistency, the final α(q) value is rounded to one decimal place before being applied in the hybrid scoring function. 4.3 Final Score Fusion With the dynamically determined α(q), we compute the final hybrid ranking score by applying the weighted combination to the normalized scores from both retrieval methods: R(q, d) = α(q) · ˜Sdense(q, d) + (1 − α(q)) · ˜SBM25(q, d) (7) Documents are then ranked based on R(q, d), and the top-K results form the final retrieval output Dfinal(q) = {d1, d2, ...,dK} that is passed to the generation component of the RAG system. Through this dynamic approach, DAT effectively overcomes the limitations of fixed- weight hybrid retrieval methods by intelligently adapting to each query’s characteristics. This query-specific adaptation leads to more relevant and accurate retrieval results across diverse query types, enhancing the overall performance of RAG systems. 5 Experiments 5.1 Experimental Setup Datasets and Preprocessing To evaluate the effectiveness and generalizability of our pro- posed method, we conducted experiments on two benchmark datasets: SQuAD (Rajpurkar 5 Preprint. Under review. et al., 2016), a widely used dataset for evaluating retrieval-based question answering in English, and DRCD (Shao et al., 2019), a large-scale traditional Chinese machine reading comprehension dataset. While SQuAD provides a standard benchmark, we include DRCD to examine whether the proposed method can also perform well in a different language setting. As Chinese is one of the most widely spoken languages, DRCD serves as a valuable testbed for assessing the broader applicability of our approach. For each dataset, we constructed an evaluation corpus by randomly sampling articles from the original document collection. For each selected article, we included all its paragraphs P = {p1, p2, . . .} and the corresponding questions Q = {q1, q2, . . .} such that each question qi ∈ Q is answerable by a span in paragraph pi ∈ P . The sampling process continued until the number of questions approached 3000, stopping before the next sampled article would exceed this threshold. This yields a paragraph corpus Peval ⊂ P and a query set Qeval ⊂ Q, with aligned pairs (qi, pi) forming the ground truth for retrieval. To better focus our evaluation, we identified a subset of queries Qhybrid ⊂ Qeval where hybrid retrieval strategies can actually make a difference. Our analysis revealed that for many queries in Qeval \ Qhybrid, retrieval performance remained identical regardless of the α value used—suggesting these queries were too simple to benefit from hybrid approaches and could be optimally retrieved using either BM25 or dense retrieval alone. In contrast, the Qhybrid subset specifically contains queries where BM25 and dense retrieval produce different rankings, and where the choice of α directly impacts whether the correct document appears at the top position. This hybrid-sensitive subset serves as a focused testbed for evaluating the effectiveness of dynamic weighting strategies in scenarios where hybrid retrieval is truly beneficial. Detailed dataset statistics are summarized in Table 1. Dataset Articles Paragraphs Questions Hybrid-Sensitive SQuAD 13 585 2976 1111 DRCD 318 908 3000 1523 Table 1: Dataset statistics for SQuAD and DRCD evaluation corpora. The retrieval task involves identifying the most relevant paragraph ˆpi ∈ P eval for each query qi ∈ Q eval. A retrieval is considered successful if ˆpi matches the ground truth paragraph pi that contains the answer. Metrics and Evaluation Protocol To evaluate retrieval performance, we use Precision@1 and Mean Reciprocal Rank at 20 (MRR@20). Precision@1 measures the fraction of queries where the correct answer appears as the top-ranked retrieved document. MRR@20 assesses ranking quality by computing the reciprocal rank of the first correct document (up to position 20) and averaging across queries. These metrics effectively quantify retrieval accuracy and ranking effectiveness. We conduct our evaluation in two phases: a Complete Dataset Evaluation on the entire query set Qeval and a more focused Hybrid-Sensitive Analysis on the subset Qhybrid where hybrid retrieval methods can make a meaningful difference. Baseline Methods We compare our proposed DAT framework against several baseline retrieval methods. The first baseline is BM25 Only (α = 0), a sparse retrieval approach using only BM25 scores. For English (SQuAD) datasets, we use standard word tokenization, while for Chinese (DRCD) datasets, we adopt the tokenizer from ckiplab/albert-base-chinese1. The second baseline is Dense Only (α = 1), which ranks paragraphs based on cosine similar- ity between query and paragraph embeddings obtained from the text-embedding-3-large model (OpenAI, 2024c). The third baseline is Fixed Hybrid (α = α∗), a hybrid method that linearly combines BM25 and dense scores with a fixed weighting parameter α∗. For both datasets, we conducted exhaustive grid search over α values from 0 to 1 with a step size of 1https://huggingface.co/ckiplab/albert-base-chinese 6 Preprint. Under review. 0.1, and found that α∗ = 0.6 maximized retrieval accuracy on both validation sets, making it the optimal fixed weighting value for our experiments. Model Implementation For our proposed DAT method, we experiment with three dif- ferent base models to demonstrate the robustness of our approach across various model sizes and architectures: GPT-4o, OpenAI’s model (OpenAI, 2024a) estimated to have ap- proximately 200B parameters (Abacha et al., 2025); GPT-4o-mini, OpenAI’s model (OpenAI, 2024b) estimated to have approximately 8B parameters (Abacha et al., 2025); and DeepSeek- R1-Distill-Qwen-14B, DeepSeek’s 14B parameter open source model (DeepSeek-AI, 2025). 5.2 Results 5.2.1 Complete Dataset Evaluation We first evaluate all methods on the complete datasets Qeval for each dataset. Table 2 shows the accuracy of α selection for both SQuAD and DRCD, measuring how often each method selects the optimal weighting value for a given query. We define the optimal weighting value as the α that produces the highest retrieval ranking for the ground truth paragraph pi given query qi. The results in Table 2 show that DAT variants achieve higher alpha selection accuracy than fixed weighting approaches on both datasets, with GPT-4o reaching 0.9234 accuracy on SQuAD and GPT-4o-mini achieving 0.9013 on DRCD compared to 0.8975 and 0.8623 for the Fixed Hybrid approach, respectively. The results in Table 3 demonstrate that our DAT approach consistently outperforms both single-method retrieval and fixed-weight hybrid retrieval across both datasets. For SQuAD, even the DAT variant using the small model (DeepSeek-R1-Distill-Qwen-14B) achieves significant improvements over the best fixed hybrid approach, with a ˜2% increase in Precision@1. Similarly for DRCD, all DAT variants provide notable improvements, with GPT-4o achieving a ˜3.3% increase in Precision@1. Interestingly, we observe that while dense retrieval performs relatively well on the SQuAD dataset, BM25 shows stronger performance on the DRCD dataset, suggesting different retrieval dynamics across datasets. Despite these differences, DAT effectively adapts to both scenarios by dynamically selecting appropriate alpha values. Method SQuAD DRCD BM25 Only (α = 0.0) 0.7981 0.8110 Dense Only (α = 1.0) 0.7789 0.6156 Fixed Hybrid (α = 0.6) 0.8975 0.8623 DAT (DeepSeek-R1-Distill-Qwen-14B) 0.9163 0.8897 DAT (GPT-4o-mini) 0.9194 0.9013 DAT (GPT-4o) 0.9234 0.9010 Table 2: Alpha Selection Accuracy on Complete Datasets Qeval Method SQuAD DRCD Precision@1 MRR@20 Precision@1 MRR@20 BM25 Only ( α = 0.0) 0.7594 0.8223 0.7630 0.8134 Dense Only ( α = 1.0) 0.7396 0.8119 0.5743 0.6708 Fixed Hybrid ( α = 0.6) 0.8461 0.8997 0.8113 0.8619 DAT (DeepSeek-R1-Distill-Qwen-14B) 0.8663 0.9079 0.8347 0.8711 DAT (GPT-4o-mini) 0.8676 0.9093 0.8417 0.8796 DAT (GPT-4o) 0.8740 0.9130 0.8440 0.8807 Table 3: Retrieval Performance on Complete Datasets Qeval 7 Preprint. Under review. 5.2.2 Hybrid-Sensitive Analysis While the complete dataset evaluation demonstrates the overall effectiveness of DAT, we now focus specifically on the hybrid-sensitive subsets Qhybrid (1111 queries for SQuAD and 1523 for DRCD) to examine performance specifically on queries where different retrieval methods produce varying rankings, making the weighting between methods critical for optimal results. Method SQuAD DRCD BM25 Only (α = 0.0) 0.4590 0.6277 Dense Only (α = 1.0) 0.4077 0.2429 Fixed Hybrid (α = 0.6) 0.7255 0.7288 DAT (DeepSeek-R1-Distill-Qwen-14B) 0.7759 0.7827 DAT (GPT-4o-mini) 0.7840 0.8056 DAT (GPT-4o) 0.7948 0.8050 Table 4: Alpha Selection Accuracy on Hybrid-Sensitive Subsets Qhybrid Method SQuAD DRCD Precision@1 MRR@20 Precision@1 MRR@20 BM25 Only ( α = 0.0) 0.3906 0.5420 0.5555 0.6446 Dense Only ( α = 1.0) 0.3375 0.5143 0.1838 0.3636 Fixed Hybrid ( α = 0.6) 0.6229 0.7493 0.6507 0.7401 DAT (DeepSeek-R1-Distill-Qwen-14B) 0.6769 0.7712 0.6967 0.7582 DAT (GPT-4o-mini) 0.6805 0.7750 0.7104 0.7749 DAT (GPT-4o) 0.6976 0.7849 0.7150 0.7771 Table 5: Retrieval Performance on Hybrid-Sensitive Subsets Qhybrid The results in Table 4 demonstrate that DAT variants achieve significantly higher alpha selection accuracy compared to fixed weighting approaches on both hybrid-sensitive subsets, with GPT-4o reaching 0.7948 accuracy for SQuAD and GPT-4o-mini achieving 0.8056 for DRCD versus approximately 0.73 for the Fixed Hybrid approach on both datasets. This improved ability to dynamically select optimal weightings directly translates to enhanced retrieval performance shown in Table 5. The results on both hybrid-sensitive subsets reveal several important insights: 1. The performance gap between single-method retrieval and hybrid methods is sub- stantially wider on these subsets, highlighting the importance of effective weighting in challenging cases. 2. DAT consistently outperforms the fixed hybrid approach, with the best variant (GPT-4o) achieving a ˜7.5% improvement in Precision@1 for SQuAD and a ˜6.4% improvement for DRCD over fixed hybrid weighting. 3. Even the smaller model variants demonstrate significant improvements, with DeepSeek-R1-Distill-Qwen-14B achieving a ˜5.4% improvement in Precision@1 for SQuAD and a ˜4.6% improvement for DRCD over fixed hybrid weighting. These findings confirm that our dynamic alpha tuning approach is particularly valuable for complex queries where different retrieval methods exhibit varying effectiveness, precisely the scenarios where intelligent weighting is most needed. 8 Preprint. Under review. 6 Scoring Mechanism Analysis To provide deeper insight into DAT’s effectiveness, we analyze how the LLM-based scoring mechanism evaluates retrieval quality and dynamically adjusts α values through represen- tative case studies from both datasets. 6.1 SQuAD Query Example For the SQuAD query “What gun did the Royal Navy start using?”, the correct answer mentions Britain’s 3.7-inch HAA gun. The Top-1 result from dense retriever returned: “By the early 20th century balloon, or airship, guns, for land and naval use were attracting attention. Various types of ammunition were proposed...” The Top-1 result from BM25 retriever returned: “AAA battalions were also used to help suppress ground targets. Their larger 90 mm M3 gun would prove... Also available to the Americans at the start of the war was the 120 mm M1 gun...” The LLM’s reasoning process determined: • The dense result provides topical relevance to naval guns but lacks specificity about Royal Navy adoption. Score: 3/5. • The BM25 result focuses on American artillery unrelated to the Royal Navy. Score: 2/5. This yielded α = 3/(3 + 2) = 0.6, appropriately favoring the semantically-related dense result that was more likely to lead to relevant information. 6.2 DRCD Query Example For the DRCD query “ 水分子中的質子在高溫中與鋯進行無氧性氧化反應後什麼物質會產 生?” (What substance is produced when protons in water molecules react anaerobically with zirconium at high temperatures?), the Top-1 result from dense retriever returned: “氫 在氧化後會失去它的電子，形成氫陽離子。氫陽離子不含電子，其原子核通常只含一個質 子...” (When hydrogen is oxidized, it loses its electron and becomes a hydrogen ion...). The Top-1 result from BM25 retriever returned: “ 在無氧條件下，鐵和合成鋼會被水分子中的 質子緩慢氧化，而水則會還原成分子氫...” (Under anaerobic conditions, iron and steel are oxidized by protons in water molecules, producing molecular hydrogen...). The LLM evaluated: • The dense result explains hydrogen ions but lacks discussion of their reaction with zirconium. Score: 3/5. • The BM25 result discusses a reaction mechanism with iron that parallels what occurs with zirconium, correctly identifying hydrogen gas as the product. Score: 4/5. This produced α = 3/(3 + 4) = 0.43 (rounded to 0.4), appropriately weighting toward the more relevant result. These examples demonstrate how DAT’s scoring mechanism effectively balances retrieval methods based on result quality rather than relying on predefined query categories. By evaluating the specific relationship between each query and the retrieved contents, the system adapts to diverse information needs. 7 Conclusion We introduced DAT, a framework that dynamically adjusts the weighting between sparse and dense retrieval for each query by leveraging LLMs to evaluate document effectiveness. Unlike static weighting schemes, DAT adaptively selects the optimalα per query, achieving a balance between performance and efficiency—even with smaller models. Experiments show 9 Preprint. Under review. that DAT consistently outperforms fixed-weight hybrids, especially on hybrid-sensitive queries, and remains robust across different LLM sizes. These results underscore the limitations of static approaches and highlight the value of query-adaptive strategies. References Asma Ben Abacha, Wen wai Yim, Yujuan Fu, Zhaoyi Sun, Meliha Yetisgen, Fei Xia, and Thomas Lin. Medec: A benchmark for medical error detection and correction in clinical notes. arXiv preprint arXiv:2412.19260, 2025. Alec Berntson. Azure ai search: Outperforming vector search with hybrid retrieval and reranking. https://techcommunity.microsoft.com/blog/azure-ai-services-blog/az ure-ai-search-outperforming-vector-search-with-hybrid-retrieval-and-reranki ng/3929167, 2023. Sebastian Bruch, Siyu Gai, and Amir Ingber. An analysis of fusion functions for hybrid retrieval. ACM T ransactions on Information Systems, 42(1):1–35, August 2023. ISSN 1558- 2868. doi: 10.1145/3596512. URL http://dx.doi.org/10.1145/3596512. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Yuanzhuo Wang, and Jian Guo. A survey on llm-as-a-judge. arXiv preprint arXiv: 2411.15594 , 2024. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. Adaptive- RAG: Learning to adapt retrieval-augmented large language models through question complexity. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language T echnologies (Volume 1: Long Papers), pp. 7036–7050, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long. 389. URL https://aclanthology.org/2024.naacl-long.389/. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answer- ing. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.),Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 6769–6781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/20 20.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550/. Dirk Kulawiak and Joon-Pil Hwang. Unlocking the power of hybrid search - a deep dive into weaviate’s fusion algorithms. https://weaviate.io/blog/hybrid-search-fusion-a lgorithms, 2023. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9459–9474. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/file/6b493230205f7 80e1bc26945df7481e5-Paper.pdf. Ji Ma, Ivan Korotkov, Keith B. Hall, and Ryan T. McDonald. Hybrid first-stage retrieval models for biomedical literature. In Conference and Labs of the Evaluation Forum , 2020. OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024a. OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. https://openai.com/index/g pt-4o-mini-advancing-cost-efficient-intelligence/ , 2024b. OpenAI. text-embedding-3-large. https://platform.openai.com/docs/models/text-emb edding-3-large, 2024c. 10 Preprint. Under review. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264. Stephen Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. Okapi at trec-3. pp. 0–, 01 1994. Kunal Sawarkar, Abhilasha Mangal, and Shivam Raj Solanki. Blended rag: Improving rag (retriever-augmented generation) accuracy with semantic search and hybrid query-based retrievers. In 2024 IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR), volume 24, pp. 155–161. IEEE, August 2024. doi: 10.1109/mipr6220 2.2024.00031. URL http://dx.doi.org/10.1109/MIPR62202.2024.00031. Chih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. Drcd: a chinese machine reading comprehension dataset. arXiv preprint arXiv:1806.00920, 2019. Ravi Theja. Llamaindex: Enhancing retrieval performance with alpha tuning in hybrid search in rag. https://www.llamaindex.ai/blog/llamaindex-enhancing-retrieval-per formance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00 , 2024. A Prompt Template You are an evaluator assessing the retrieval effectiveness of dense retrieval ( Cosine Distance ) and BM25 retrieval for finding the correct answer . ## Task : Given a question and two top1 search results ( one from dense retrieval , one from BM25 retrieval ) , score each retrieval method from **0 to 5** based on whether the correct answer is likely to appear in top2 , top3 , etc . ### ** Scoring Criteria :** 1. ** Direct hit --> 5 points ** - If the retrieved document directly answers the question , assign **5 points **. 2. ** Good wrong result ( High likelihood correct answer is nearby ) --> 3 -4 points ** - If the top1 result is ** conceptually close ** to the correct answer ( e . g . , mentions relevant entities , related events , partial answer ) , it indicates the search method is in the right direction . - Give **4** if it ' s very close , **3** if somewhat close . 3. ** Bad wrong result ( Low likelihood correct answer is nearby ) --> 1 -2 points ** - If the top1 result is ** loosely related but misleading ** ( e . g . , shares keywords but changes context ) , correct answers might not be in top2 , top3 . - Give **2** if there ' s a small chance correct answers are nearby , **1** if unlikely . 4. ** Completely off - track --> 0 points ** - If the result is ** totally unrelated ** , it means the retrieval method is failing . --- ### ** Given Data :** - ** Question :** "{ question }" 11 Preprint. Under review. - ** dense retrieval Top1 Result :** "{ vector_r eference }" - ** BM25 retrieval Top1 Result :** "{ bm25_reference }" --- ### ** Output Format :** Return two integers separated by a space : - ** First number :** dense retrieval score . - ** Second number :** BM25 retrieval score . - Example output : 3 4 ( Vector : 3 , BM25 : 4) ** Do not output any other text .** B Limitations While our DAT framework demonstrates significant improvements over fixed-weight hybrid retrieval methods, several limitations warrant discussion. DAT’s dependence on LLM-based effectiveness scoring introduces computational overhead that may increase latency and cost in production environments, despite our efforts to mitigate this by only evaluating top-1 results. However, as large language models continue to evolve and computational hardware advances, we anticipate these efficiency constraints will gradually diminish, expanding the practical deployment scenarios for our approach. 12