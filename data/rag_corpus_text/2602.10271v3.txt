MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Yongyue Zhang Independent Researcher Singapore yongyue002@gmail.com Yaxiong Wu Independent Researcher Singapore wuyashon@gmail.com Abstract Understanding multimodal long-context documents that comprise multimodal chunks such as paragraphs, figures, and tables is chal- lenging due to (1) cross-modal heterogeneity to localize relevant information across modalities, (2) cross-page reasoning to aggre- gate dispersed evidence across pages. To address these challenges, we are motivated to adopt a query-centric formulation that projects cross-modal and cross-page information into a unified query repre- sentation space, with queries acting as abstract semantic surrogates for heterogeneous multimodal content. In this paper, we propose a Multimodal Long-Context Document Retrieval Augmented Gener- ation (MLDocRAG) framework that leverages a Multimodal Chunk- Query Graph (MCQG) to organize multimodal document content around semantically rich, answerable queries. MCQG is constructed via a multimodal document expansion process that generates fine- grained queries from heterogeneous document chunks and links them to their corresponding content across modalities and pages. This graph-based structure enables selective, query-centric retrieval and structured evidence aggregation, thereby enhancing ground- ing and coherence in multimodal long-context question answering. Experiments on datasets MMLongBench-Doc and LongDocURL demonstrate that MLDocRAG consistently improves retrieval qual- ity and answer accuracy, demonstrating its effectiveness for multi- modal long-context understanding. ACM Reference Format: Yongyue Zhang and Yaxiong Wu. 2018. MLDocRAG: Multimodal Long- Context Document Retrieval Augmented Generation. InProceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym â€™XX).ACM, New York, NY, USA, 15 pages. https://doi. org/XXXXXXX.XXXXXXX 1 Introduction Multimodal long-context documents, such as research papers, re- ports, and books, often span tens to hundreds of pages and contain diverse multimodal components/chunks including text, images, and tables [7, 21, 28, 33, 38]. Understanding such lengthy multimodal documents presents two central challenges [ 7]: (1)cross-modal heterogeneity, which requires identifying and localizing relevant Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym â€™XX, Woodstock, NY Â©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX information across heterogeneous modalities; and (2)cross-page reasoning, which demands integrating evidence scattered across multiple pages to support coherent inference. Addressing these challenges necessitates the ability ofmultimodal long-context asso- ciationâ€”accurately identifying, connecting, and integrating seman- tically relevant information across modalities and segments. Large Vision-Language Models (LVLMs) have shown strong cross-modal understanding capabilities of effectively aligning and interpreting multimodal information within localized short con- texts [9, 19, 46]. Representative examples include GPT-4o [20], Gem- ini [35], Qwen2.5-VL [3], and InternVL [6], which demonstrate im- pressive performance on short-range multimodal reasoning bench- marks [23, 43]. However, they often struggle to maintain consistent semantic modeling within limited context windows when applied to long-document scenarios [24]. In particular, their performance deteriorates when evidence is sparsely distributed across pages and modalities by overlooking the relevant information, leading to the so-called â€œneedle in a haystackâ€ problem [41, 42]. Retrieval-Augmented Generation (RAG) has emerged as a promis- ing paradigm for overcoming the limited context windows of large models, such as Large Language Models (LLMs) and Large Vision- Language Models (LVLMs), by incorporating external retrieval [1, 26, 50]. In multimodal document scenarios, existing RAG approaches typically follow five strategies (as shown in Figure 1): (a)ğ‘…ğ´ğºğ‘¡ğ‘¥ğ‘¡ ğ·ğ‘’ğ‘›ğ‘ ğ‘’ : converting multimodal content into plain text via Optical Char- acter Recognition (OCR) [ 32] and applying textual chunk-based RAG with LLMs [48] and dense retrievers (e.g., BGE-m3 [5]); (b) ğ‘…ğ´ğºğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ ğ·ğ‘’ğ‘›ğ‘ ğ‘’ : generating image descriptions (e.g., Image2Text (i2t) [18]) and fusing them with OCR text using document content extrac- tion toolkits (e.g., MinerU [40]) for textual chunk-based RAG; (c) ğ‘€ğ‘…ğ´ğº ğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” ğ¶ğ¿ğ¼ ğ‘ƒ : encoding text and images into a shared embedding space via multimodal encoders (e.g., CLIP [ 29], SigLIP [ 37, 47]) for retrieval followed by LVLM-based generation; (d) ğ‘‰ ğ‘…ğ´ğºğ‘ğ‘ğ‘”ğ‘’ ğ¶ğ‘œğ‘™ğ‘ƒğ‘ğ‘™ğ‘– : rendering document pages as images and retrieving relevant page- level content using vision-based methods (e.g., ColPali [14]) prior to LVLM decoding; and (e)ğºğ‘Ÿğ‘ğ‘â„ğ‘…ğ´ğº ğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ ğ·ğ‘’ğ‘›ğ‘ ğ‘’ : constructing knowledge graphs (KGs) from document content and applying Graph-based RAG [11, 13]. However, these approaches often struggle to capture fine-grained cross-modal and cross-page associations, leading to incomplete grounding and suboptimal retrieval. Document expansion methods such as Doc2Query [27] provide a principled way to map document content into a unified query representation space, enhancing retrieval by generating synthetic queries that capture a documentâ€™s latent information needs. Build- ing upon this idea, QCG-RAG [44] constructs a query-centric graph that explicitly links generated queries to their corresponding textual document chunks, enabling query-aware indexing and multi-hop retrieval over long textual contexts, thereby improving evidence arXiv:2602.10271v3 [cs.IR] 13 Feb 2026 Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu Figure 1: Illustration of RAG for multimodal long-context documents, comparing (a)â€“(e) baselines with (f) our MLDocRAG. aggregation and grounding in long-document question answering. However, these approaches remain largely unexplored in the set- ting of multimodal long-context documents, where information is distributed across heterogeneous modalities and pages. Building on this idea, we extend query-centric formulation to the multimodal setting and propose MLDocRAG (Multimodal Long- Context Document Retrieval-Augmented Generation), a framework for multimodal long-context document understanding. MLDocRAG leverages a unified retrieval structureâ€”the Multimodal Chunk- Query Graph (MCQG)â€”constructed via MDoc2Query, which ex- tends Doc2Query [27] to multimodal scenarios and generates se- mantically rich, answerable queries from heterogeneous chunks spanning text, images, and tables. The resulting MCQG links each query to its corresponding multimodal content, enabling selective retrieval and structured evidence aggregation across modalities and pages. This design effectively addresses cross-modal and cross- page associations, improving retrieval accuracy and grounding in multimodal long-document question answering (QA). Figure 1(f) illustrates the overall pipeline of our proposed MLDocRAG frame- work. Our main contributions are as follows: â€¢We proposeMLDocRAG (Multimodal Long- Context Docu- ment Retrieval-Augmented Generation), a unified framework for multimodal long-document QA that integrates multimodal doc- ument expansion with query-centric, graph-based retrieval for fine- grained and interpretable evidence selection. â€¢We introduce theMCQG (Multimodal Chunk-Query Graph), which links generated queries to corresponding multimodal chunks and connects semantically related information across pages. â€¢To construct MCQG, we leverageMDoc2Query, a multimodal document expansion process that generates answerable queries from multimodal chunks. â€¢Extensive experiments on MMLongBench-Doc [ 25] and Long- DocURL [8] show that MLDocRAG consistently improves QA accu- racy, advancing multimodal long-context document understanding. 2 Related Work Long-Context Document Understanding.Understanding multi- modal long-context documents, such as research papers or techni- cal reports, requires resolving both cross-modal heterogeneity and long-range cross-page reasoningâ€”capabilities still limited in cur- rent models [8, 25]. Despite the strong local alignment abilities of Large Vision-Language Models (LVLMs) like GPT-4o [20], Qwen2.5- VL [3], and Gemini [35], their fixed context windows limit their ef- fectiveness in capturing globally relevant evidence dispersed across pages and modalities. This limitation leads to degraded performance in complex reasoning tasks, where key information is sparsely located, as highlighted in benchmarks such as MMLongBench- Doc [25] and LongDocURL [8]. Retrieval-Augmented Generation (RAG) offers partial relief by introducing external memory, yet struggles with retrieving semantically aligned multimodal content at scale. Retrieval-Augmented Generation (RAG) partly mitigates this by introducing external memory, but struggles to retrieve and integrate semantically aligned multimodal information at scale. These challenges call for new retrieval and representation strate- gies tailored to multimodal long-document understanding. Multimodal RAG.Recent efforts in multimodal Retrieval Aug- mented Generation (RAG) have explored various strategies to adapt long-document understanding to the multimodal setting. Common approaches include OCR-based text extraction, image captioning fused with text chunks, shared embedding retrieval via multimodal encoders (e.g., CLIP [29], SigLIP [37, 47]), and vision-based page retrieval using rendered document images (e.g., ColPali [14]). Some integrate structured knowledge via document-derived graphs to enhance reasoning [1, 26, 49, 50]. However, existing pipelines oper- ate at coarse granularity, overlooking fine-grained cross-modal and cross-page associations, leading to incomplete grounding and re- trieval mismatches. This motivates developing semantically aligned and structurally informed retrieval for multimodal contexts. Document Expansion.Document expansion has been widely adopted in RAG settings to improve retrieval coverage by gen- erating synthetic contents that anticipate potential information needs [12, 31, 34, 39]. Methods such as Doc2Query [ 27] and its enhanced variant Doc2Query-- [15] generate diverse, semantically meaningful queries from document content, effectively enriching the retrieval index. Building on this idea, QCG-RAG [ 44] intro- duces a query-centric graph structure that connects queries to their source textual chunks, enabling multi-hop retrieval and structured evidence aggregation in long-document scenarios. Yet, these meth- ods remain unimodal, lacking explicit modeling of cross-modal relationships. Extending query-centric expansion to multimodal documents remains an open problem for capturing fine-grained, heterogeneous associations. MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY 3 Methodology 3.1 Preliminaries We consider the task of multimodal long-context document question answering, where the goal is to answer a user natural language question ğ‘ğ‘¢ based on a multimodal long-context document ğ· that spans multiple pages and contains heterogeneous content. Each document ğ· originates from a PDF file and is processed into a sequence of pages ğ·={ğ‘ƒ 1, ğ‘ƒ2, . . . , ğ‘ƒğ‘‹ } via OCR. Each page ğ‘ƒğ‘¥ consists of a set of modality-specific chunks ğ¶ğ‘– ={ğ‘ 1, ğ‘2, . . . , ğ‘ğ‘ }, where each chunk ğ‘ is associated with a modality (text ( ğ‘¡ğ‘¥ğ‘¡ ) or image (ğ‘–ğ‘šğ‘”)) and a content type (e.g., paragraph, figure, table, or equation). Text chunks are contiguous paragraphs or extracted titles; image chunks consist of a visual region paired with an associated caption; table chunks include a caption, a rendered table image, and an OCR- converted Markdown-style textual representation [40]. The QA task follows a retrieval-augmented generation (RAG) paradigm: given a query ğ‘, a retriever selects the top-ğ¾ relevant multimodal chunks {ğ‘â€² 1, . . . , ğ‘â€² ğ¾ } from ğ·, and a LVLM conditions on these chunks to generate the final answer Ë†ğ‘. The primary challenge lies in retrieving semantically aligned and cross-modally grounded chunks from the long, heterogeneous document to support accurate and coherent generation. 3.2 Framework Overview To tackle the challenges of cross-modal heterogeneity and long- range reasoning in multimodal long-document QA, we propose a Multimodal Long-Context Document Retrieval Augmented Genera- tion (MLDocRAG)framework based on the construction and usage of aMultimodal Chunk-Query Graph (MCQG)that organizes mul- timodal document content around semantically rich, answerable queries. To construct MCQG, we move beyond conventional chunk- based retrieval paradigms that treat multimodal content as flat and independent units, and instead hypothesize that organizing mul- timodal long-context document understanding around generated, answerable queries enables more fine-grained, semantically aligned, and interpretable retrieval. Inspired by prior work on document expansion via query generation (e.g., Doc2Query [27]), we extend this idea to the multimodal setting by proposing MDoc2Queryâ€”a multimodal document expansion framework that generates seman- tically rich queries from heterogeneous document chunks. These generated queries act asretrieval anchorsthat bridge the gap be- tween user information needs and multimodal document content. Figure 2 illustrates the overall architecture of our proposed ML- DocRAG framework which consists of two main stages:MCQG ConstructionandMCQG Usage. MCQG Construction.In this stage, a multimodal long document parsed from a PDF is decomposed into modality-specific chunks, where text-modality chunks correspond to paragraph text (includ- ing equations), and image-modality chunks correspond to figures or tables, each associated with its caption and any OCR-derived structured textual content. We then apply the MDoc2Query pro- cess to generate a set of answerable queries for each chunk using a Large Vision-Language Model (LVLM). These queries are explicitly linked to their source chunks and further connected to semanti- cally similar queries across the document, forming theMultimodal Chunk-Query Graph (MCQG). This graph captures both intra-modal and inter-modal associations, and provides a structured retrieval index that aligns semantically meaningful questions with relevant multimodal evidence. MCQG Usage.At inference time, a user query is embedded and matched against nodes in the MCQG using KNN-based retrieval in the query embedding space. By retrieving semantically similar generated queries and aggregating their linked source chunks, the system collects a compact yet relevant set of multimodal content. The retrieved chunks are further ranked based on their semantic similarity to the user query and provided as context to a Large Lan- guage or Vision-Language Model (LLM/LVLM) for final answer gen- eration. This query-centric retrieval strategy enables interpretable, structured evidence aggregation over multimodal long contexts. 3.3 MCQG Construction The MCQG Construction stage aims to transform a multimodal long-context document into a query-centric retrieval structure that supports fine-grained, semantically aligned, and cross-modal ev- idence retrieval. Specifically, we convert the long document into a collection of multimodal chunks, generate answerable queries from each chunk, and construct a graph that encodes chunk-query and inter-query associations. This process comprises four steps: (1) Document Parsing, (2) MDoc2Query, (3) Graph Assembly, and (4) Vector & Graph Storage. (1) Document Parsing.We adopt an existing multimodal PDF parsing tool, such as MinerU [40], to extract structured information from the document. Given a PDF document ğ·={ğ‘ƒ 1, ğ‘ƒ2, . . . , ğ‘ƒğ‘‹ } of ğ‘‹ pages, we extract a layout-preserving sequence of heterogeneous document components, including: (i) paragraphs (text only); (ii) fig- ures (images with captions); (iii) tables (table images accompanied by textual captions and OCR-converted Markdown text); and (iv) equations (parsed into Markdown text). The extracted content is stored in a JSON format ordered by visual layout position. Based on this structured output, we define a set of modality-specific chunks: C={ğ‘ 1, ğ‘2, . . . , ğ‘ğ‘ }, ğ‘ ğ‘– âˆˆ {text,image}.(1) Paragraph text is segmented into overlapping spans using a sliding window with a maximum token length and a fixed stride. Equa- tions are treated as part of the regular text content. In contrast, each figure or table is treated as an image-modality chunk, while preserving its associated caption and any structured OCR-derived textual content. Visual noise filtering can be further applied to remove uninformative images (e.g., blank pages, string-only im- ages, or logos; see Appendix B) via visual chunk classification using zero-shot CLIP inference. (2) MDoc2Query.To bridge document content with potential information needs, we extend the Doc2Query paradigm to mul- timodal settings. For each chunk ğ‘ğ‘– âˆˆ C , we employ a Large Vision-Language Model (LVLM) to generate a set of answerable queryâ€“answer pairs: Qğ‘– = n (ğ‘ (1) ğ‘– , ğ‘(1) ğ‘– ), . . . ,(ğ‘ (ğ‘€ğ‘– ) ğ‘– , ğ‘(ğ‘€ğ‘– ) ğ‘– ) o , ğ‘€ ğ‘– â‰¤ğ‘€ max.(2) Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu Figure 2: Overview of our proposed MLDocRAG framework, consisting of (a)MCQG Constructionfor building a multimodal chuck-query graph and (b)MCQG Usagefor retrieving relevant multimodal chunks in long-document QA. Here, ğ‘ (ğ‘—) ğ‘– denotes a generated query and ğ‘ (ğ‘—) ğ‘– its corresponding answer, both grounded in chunk ğ‘ğ‘–. The number of pairs per chunk is adaptively determined by the richness of its content, up to a global cap ğ‘€max per chunk. Each queryâ€“answer pair (ğ‘ (ğ‘—) ğ‘– , ğ‘(ğ‘—) ğ‘– ) is embedded into a dense vector representation using a pretrained text encoderğœ™(Â·)(e.g., BGE-m3 [5]): v(ğ‘—) ğ‘– =ğœ™  [ğ‘ (ğ‘—) ğ‘– ;ğ‘ (ğ‘—) ğ‘– ]  âˆˆR ğ‘‘,(3) where [ğ‘; ğ‘] denotes concatenation of the query and answer as the retrieval unit. We simplify notation in the remainder of the paper by referring toğ‘ (ğ‘—) ğ‘– as a shorthand for the full queryâ€“answer pair. (3) Graph Assembly.We build the Multimodal Chunk-Query Graph (MCQG) as a heterogeneous graph G=(V,E) with nodes: V=C âˆª Q,whereQ= ğ‘Ã˜ ğ‘–=1 Qğ‘– .(4) Nodes in V includeChunk Nodes C andQuery Nodes Q. Edges in E include: (1)Chunkâ€“Query (C-Q) Edges: Each query ğ‘ (ğ‘—) ğ‘– is connected to its originating chunk ğ‘ğ‘– via a directed anchor edge. (2)Queryâ€“Query (Q-Q) Edges: We compute semantic similarity between queries using inner product of embeddings and connect each query to its top-ğ‘˜nearest neighbors (KNN): sim(ğ‘, ğ‘â€²)=âŸ¨ğœ™( [ğ‘;ğ‘]), ğœ™( [ğ‘ â€²;ğ‘ â€²])âŸ© +ğœ–.(5) Here, ğ‘ and ğ‘â€² denote two generated queries. âŸ¨Â·,Â·âŸ© denotes the inner product. All query embeddings are â„“2-normalized prior to similarity computation. The constant offset ğœ– (e.g., ğœ–= 1.0) is added to ensure non-negative similarity scores for stable KNN construction. This dual-edge structure allows the graph to capture both local chunk associations and global semantic neighborhoods across queries, enabling multi-hop traversal and cross-modal reasoning during retrieval. (4) Storage.To support scalable and efficient retrieval, we decou- ple vector retrieval from graph traversal by storing: (1) Queryâ€“Answer vectors {v(ğ‘—) ğ‘– } in a densevector database(e.g., FAISS [ 10], Elastic- Search [22]), supporting fast approximate nearest neighbor (ANN) search [2]. (2) The full MCQG graph structure in agraph data- base(e.g., Neo4j [17]), preserving chunkâ€“query and queryâ€“query relationships. Importantly, we donotperform vectorization of multimodal chunks (e.g., figure or table content) directly. Instead, all retrieval operates in the queryâ€“answer space, thereby avoiding the need for complex multimodal embedding alignment and reducing storage and computation overhead. The generated queries serve as inter- pretable, semantically rich anchors that effectively summarize and index the multimodal document content. 3.4 MCQG Usage The MCQG Usage stage aims to retrieve semantically relevant mul- timodal content in response to a user query by leveraging the struc- ture of the Multimodal Chunk-Query Graph (MCQG). Rather than retrieving from raw document chunks, our MLDocRAG approach retrieves and aggregates content via generated queries that serve as semantically aligned retrieval anchors. This process involves four main steps: (1) Query Node Retrieval, (2) Chunk Node Ranking, (3) Context Collection, and (4) Answer Generation. (1) Query Node Retrieval.Given a user query ğ‘ğ‘¢, we first compute its vector embedding ğœ™(ğ‘ ğ‘¢ ) using the same query encoder used dur- ing MCQG construction. We perform approximate nearest neighbor (ANN) search over the queryâ€“answer vectors in the vector database to retrieve the top-ğ‘›semantically similar generated queries: Qret =Top ğ‘› {ğ‘âˆˆ Q | sim(ğ‘ğ‘¢, ğ‘) â‰¥ğ›¼ } .(6) MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY In practice, the number of retrieved queries is jointly constrained by the maximum node budget ğ‘› and a similarity threshold ğ›¼âˆˆ [ 0, 2] (e.g., ğ›¼= 1.0), such that only queries with sim(ğ‘ğ‘¢, ğ‘) â‰¥ğ›¼ are retained. To capture broader contextual evidence, we expand each retrieved query node ğ‘âˆˆ Q ret in the MCQG via multi-hop neighbor expansion. For each queryğ‘âˆˆ Q ret, we traverseâ„ hops in the graph (through Queryâ€“Query Edges) and collect its neighboring queries: Qexp =Q ret âˆª Ã˜ ğ‘âˆˆ Qret Nbrâ„ (ğ‘),(7) where Nbrâ„ (ğ‘) denotes the set ofâ„-hop neighbors ofğ‘ in the MCQG, and Qexp represents the expanded query set that augments the ini- tially retrieved queries with semantically related queries discovered through multi-hop graph traversal. (2) Chunk Node Ranking.Each query ğ‘âˆˆ Q exp is linked to a source multimodal chunk ğ‘âˆˆ C . We collect all chunks associated with the expanded query set: Ccand =  ğ‘ğ‘– | âˆƒğ‘âˆˆ Q exp s.t.(ğ‘, ğ‘ ğ‘– ) âˆˆ E .(8) To prioritize the most relevant evidence, we assign a relevance score to each candidate chunk ğ‘ğ‘– âˆˆ Ccand based on the maximum semantic similarity between the user queryğ‘ğ‘¢ and any query linked toğ‘ ğ‘–: score(ğ‘ğ‘– )=max ğ‘âˆˆ Qexp (ğ‘,ğ‘ğ‘– ) âˆˆ E sim ğ‘ğ‘¢, ğ‘.(9) Here, {ğ‘| (ğ‘, ğ‘ ğ‘– ) âˆˆ E} denotes the subset of expanded queries in Qexp that are connected to chunkğ‘ ğ‘– in the MCQG. (3) Context Collection.Based on the relevance scores, we select the top-ğ¾(e.g.,ğ¾=5) ranked multimodal chunks: Crel =Top ğ¾ {ğ‘ğ‘– âˆˆ Ccand} ,(10) where the ranking is determined by score(ğ‘ğ‘– ). The selected chunks are concatenated to form the multimodal retrieval context for the LVLM. These chunks may include text blocks, image regions with captions, and tables augmented with structured and OCR-derived textual content. (4) Answer Generation.Finally, the selected multimodal context Crel is provided to a Large Visionâ€“Language Model (LVLM), to- gether with the original user queryğ‘ğ‘¢, to generate the final answer: Ë†ğ‘=LVLM (ğ‘ğ‘¢,C rel) .(11) This generation step benefits from the query-centric retrieval strat- egy and graph-based evidence aggregation, which together improve grounding, coverage, and factual consistency when answering com- plex questions over multimodal long-context documents. 3.5 MDoc2Query Optimization The effectiveness of MLDocRAG largely depends on the quality of the Multimodal Chunkâ€“Query Graph (MCQG), which is con- structed via MDoc2Query. In particular, the quality and granularity of the generatedanswerable queriesproduced by MDoc2Query are critical, as they directly determine the semantic coverage, retriev- ability, and grounding fidelity of the overall pipeline. To this end, we explore optimization strategies for MDoc2Query from both non-parametricandparametricperspectives. Non-Parametric Optimization.By default, MDoc2Query in ML- DocRAG employs a LVLM to generate a set of answerable queries from parsed document chunks, such as cropped images or parsed ta- ble segments. However, document parsing tools (e.g., MinerU [40]) often strip away essential contextual informationâ€”including figure captions, table headers, and hierarchical section titlesâ€”resulting in isolated chunks that can be semantically ambiguous and con- sequently degrade the quality of the generated queries. To miti- gate this issue, we adopt aPage-Context-A ware Generationstrategy. Specifically, for a given chunk ğ‘ğ‘– located on page ğ‘¥ with the cor- responding page rendering image ğ‘ƒ page ğ‘¥ , we construct the LVLM input as a tuple (ğ‘ğ‘–, ğ‘ƒpage ğ‘¥ ). Incorporating page-level visual context enables the model to resolve ambiguities arising from incomplete local information, such as coreference resolution (e.g., linking a chunk labeled â€œTable 3â€ to its corresponding description on the same page). Parametric Optimization.In addition to non-parametric strate- gies, we further explore parametric optimization by fine-tuning a pretrained Large Visionâ€“Language Model (LVLM) on a curated set of high-quality multimodalchunk-to-queryexemplars. Each train- ing instance consists of a multimodal chunk ğ‘ (including text, an image with its caption, or a table with structured content) paired with a set of human-curated or automatically synthesized answer- able queryâ€“answer (ğ‘, ğ‘) pairs. The LVLM is trained using stan- dard teacher forcing, where the model conditions on the input chunk to generate the corresponding answer and subsequently au- toregressively decodes the associated queries. Through parametric adaptation, the model learns to produce more semantically pre- cise, context-aware, and structurally grounded queries, thereby improving the expressiveness and reliability of MDoc2Query for downstream MCQG construction. 4 Experimental Setup In this section, we evaluate the effectiveness of the proposed ML- DocRAG framework for multimodal long-context document ques- tion answering (QA), and compare it with existing approaches (as illustrated in Figure 1). Specifically, our experimental study is de- signed to address the following research questions: â€¢RQ1:How does MLDocRAG perform on multimodal long-context document QA compared with baseline methods? â€¢RQ2:What are the effects on MLDocRAG of different MCQG node variants, including query node choices (query vs. answer), chunk ranking strategies (max vs. mean), and visual noise filtering? â€¢RQ3:How do key hyperparameters of MCQG usage affect the per- formance of MLDocRAG, such as expansion hopsâ„, KNN neighbors ğ‘˜, and max nodesğ‘›? â€¢RQ4:What is the impact of MDoc2Query optimization from both non-parametric and parametric perspectives? 4.1 Datasets & Metrics Datasets.We evaluate our MLDocRAG on two multimodal long- context document QA benchmarks: (1)MMLongBench-Doc[ 25]: A curated benchmark for multimodal long-document understand- ing, consisting of documents in PDF format with diverse content including text, images, tables, and charts. Questions are paired with answerable evidence spread across pages and modalities. (2) Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu LongDocURL[ 8]: A newly collected dataset containing web-based scientific and technical documents. Each instance includes a docu- ment (in PDF or HTML format), a natural language question, and annotated answerable segments across multimodal content. Metrics.We adopt exact match Accuracy as the primary evalua- tion metric to measure the factual correctness of generated answers. To ensure consistent and scalable judgment across modalities and formats, we follow recent work and employ anLLM-as-a-Judge protocol [16], using a strong LLM (e.g., Qwen2.5-72B [36, 45]) to verify whether the predicted answer matches the gold reference answer. The evaluation prompt is shown in Appendix A. 4.2 Baselines We compare MLDocRAG with representative baselines from five categories: â€¢Text Only (txt).Use only text chunks with or without basic retrieval (BM25 [30] / BGE-m3 [5]): LCğ‘¡ğ‘¥ğ‘¡ with textual long-context reasoning, RAGğ‘¡ğ‘¥ğ‘¡ BM25 with sparse retrieval, RAGğ‘¡ğ‘¥ğ‘¡ BGE-m3 with dense retrieval. â€¢Image2Text (txt+i2t).Augment text with LVLM-generated image captions, treated as plain text: LCğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ , RAGğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ BM25 , RAGğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ BGE-m3. â€¢Multimodal (txt+img).Encode both text and image chunks using multimodal embedding models for dense retrieval: LCğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” for multimodal long-context reasoning, MRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” CLIP with CLIP [29], MRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” SigLIP with SigLIP [37, 47], MRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” ColPali with ColPali [14]. â€¢Page-level (page).Render full document pages as images and perform vision-only retrieval followed by VQA: LCğ‘ğ‘ğ‘”ğ‘’ for visual long-context reasoning, VRAGğ‘ğ‘ğ‘”ğ‘’ CLIP, VRAGğ‘ğ‘ğ‘”ğ‘’ SigLIP, VRAGğ‘ğ‘ğ‘”ğ‘’ ColPali. â€¢Graph.Construct knowledge graphs from extracted multimodal entities with the augmented text (i.e.,txt+i2t) for graph-based re- trieval: GraphRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ ğµğºğ¸âˆ’ğ‘š3 extended with efficient and lightweight MiniRAG [13]. In addition, we evaluate several variants of the proposed ML- DocRAG framework under different MCQG construction and usage settings to analyze the effects of key components and hyperparam- eters:(1) Node Variants.(i)MLDocRAG w/ Queryto use queries only as nodes; (ii)MLDocRAG w/ Answerto use answers only as nodes; (iii)MLDocRAG w/ Meanto apply mean semantic similarity for chunk node ranking instead of the max operation; (iv)ML- DocRAG w/o Filterto disable visual noise filtering during document parsing.(2) Hyperparameters.(i)Hops â„âˆˆ { 0, 1, 2, 3} for multi- hop neighbor expansion; (ii)KNN ğ‘˜âˆˆ { 1, 2, 3, 4, 5} for query-query edges; (iii)Max Nodes ğ‘›âˆˆ { 5, 10, 15, 20} for query node retrieval. We additionally compare different query node retrieval backends (BGE-m3 vs. BM25) and similarity thresholds ğ›¼âˆˆ { 1.0, 1.2}. Note that MLDocRAG performs chunk-only query generation by default, whereas MLDocRAGğ‘ƒ and MLDocRAGğ‘ƒ ğ‘ƒ additionally incorporate page context during query generation and final answer generation, respectively. 4.3 Setup Details Document Parsing Setting.Multimodal long-context documents are parsed using MinerU [40], which extracts layout-ordered ele- ments from PDF into structured JSON files. Chunks are constructed as follows: (1) Text: Paragraphs are segmented using a maximum length of 1200 tokens with an overlap of 100 tokens. Equations parsed as Markdown are treated as regular text. (2) Image: Figures and tables, together with their captions and OCR-derived content, are treated as individual image-modality chunks. In addition, doc- ument pages are rendered as images for page-level methods and MDoc2Query optimization. We further apply visual noise filter- ing to image chunks via zero-shot classification using CLIP1, with details in Appendix B. Model Configuration.We employ the BGE-m3 encoder to gener- ate dense embeddings for queries, which are stored and indexed in ElasticSearch [22] as the core vector database to support efficient similarity search. For query generation and final answer generation, we use Qwen2.5-VL-32B2 [4] by default, whileQwen2.5-VL-7B3 [4] is adopted for MDoc2Query optimization. For evaluation, we adopt an LLM-as-a-Judge setup using Qwen2.5-72B4 [36, 45]. All LLM- s/LVLMs are deployed via the SGLang [51] framework on NVIDIA H20 GPUs to enable high-throughput inference. Default Hyperparameters.For experiments on both MMLongBench- Doc and LongDocURL, we adopt a unified set of default hyperpa- rameters. Specifically, we set the KNN neighborhood size to ğ‘˜= 3 for queryâ€“query edge construction, retrieve up to ğ‘›= 10query nodes with a similarity threshold ğ›¼= 1.2, and perform â„= 2-hop query expansion. For answer generation, the top- ğ¾= 5ranked multimodal chunks are selected as the retrieval context. 5 Experimental Results In this section, we analyse the experimental results with respect to the four research questions stated in Section 4 to gauge the effectiveness of our proposed MLDocRAG. 5.1 MLDocRAG vs. Baselines (RQ1) Table 1 reports the performance of MLDocRAG and representative baselines on MMLongBench-Doc and LongDocURL in terms of accu- racy (%). Overall, MLDocRAG achieves the best overall performance on both datasets, with an accuracy of 47.9% on MMLongBench- Doc and 50.8% on LongDocURL, consistently outperforming all baseline methods. In particular, compared to text-only and image- to-text baselines, MLDocRAG benefits from explicitly modeling multimodal evidence without collapsing visual information into flat textual descriptions, thereby preserving fine-grained visual semantics that are critical for layout-, chart-, and figure-centric questions. In contrast to multimodal dense retrieval methods that independently retrieve text and image chunks, MLDocRAG orga- nizes multimodal content around generated, answerable queries and performs query-centric multi-hop expansion, enabling effective aggregation of semantically related evidence scattered across pages. This advantage is particularly evident in multi-page settings, where simple chunk-level retrieval or page-level visual reasoning fails to capture long-range dependencies. Moreover, in contrast to the text-only graph-based baseline, MLDocRAG explicitly models cross- modal associations through the Multimodal Chunkâ€“Query Graph, 1https://huggingface.co/openai/clip-vit-base-patch32 2https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct 3https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct 4https://huggingface.co/Qwen/Qwen2.5-72B-Instruct MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY MMLongBench-Doc LongDocURL Model Modality Evidence Source Evidence Page Overall Evidence Source Evidence Page Overall Method Retriever GEN TXT IMG TXT LAY CHA TAB FIG SIN MUL UNA ACC TXT LAY FIG TAB SP MP CE ACC LCğ‘¡ğ‘¥ğ‘¡ - LLM âœ“ âœ— 44.3 31.9 20.8 12.8 22.0 32.8 21.9 69.3 36.9 59.1 38.8 26.0 41.5 43.3 45.7 29.2 40.2 RAGğ‘¡ğ‘¥ğ‘¡ ğµğ‘€25 BM25 LLM âœ“ âœ— 44.9 26.1 20.2 14.2 17.1 31.6 18.9 76.3 36.8 60.2 37.4 24.9 37.3 43.1 45.9 26.8 39.5 RAGğ‘¡ğ‘¥ğ‘¡ ğµğºğ¸âˆ’ğ‘š3 BGE-m3 LLM âœ“ âœ— 40.0 31.1 20.2 10.6 18.4 29.2 19.4 77.2 36.0 59.8 34.7 19.3 31.5 41.4 44.6 21.0 36.8 LCğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ - LLM âœ“ âœ— 48.5 32.8 40.5 41.7 28.6 44.1 32.5 54.8 42.5 51.1 31.0 29.1 33.6 42.2 49.0 15.7 37.3 RAGğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ ğµğ‘€25 BM25 LLM âœ“ âœ— 43.6 31.9 38.2 38.1 27.0 45.3 26.9 66.7 43.7 60.6 36.5 45.7 40.3 61.9 54.1 22.4 48.1 RAGğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ ğµğºğ¸âˆ’ğ‘š3 BGE-m3 LLM âœ“ âœ— 42.3 26.1 37.6 45.41 31.3 49.0 27.2 71.5 46.5 59.7 32.8 47.0 42.3 61.9 57.417.1 47.8 LCğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” - LVLM âœ“ âœ“ 53.834.5 45.545.9 35.9 52.4 37.845.6 46.1 57.7 34.7 30.9 39.8 46.0 48.9 23.4 40.7 MRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” ğ¶ğ¿ğ¼ ğ‘ƒ CLIP LVLM âœ“ âœ“ 39.0 27.7 27.0 13.8 21.4 34.0 19.2 70.2 36.7 51.7 32.4 22.6 30.7 35.6 44.0 21.0 34.5 MRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” ğ‘†ğ‘–ğ‘”ğ¿ğ¼ ğ‘ƒ SigLIP LVLM âœ“ âœ“ 23.9 15.1 16.9 15.1 18.1 21.9 14.782.0 32.2 23.2 22.7 5.9 14.1 8.0 21.2 17.9 15.5 MRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–ğ‘šğ‘” ğ¶ğ‘œğ‘™ğ‘ƒğ‘ğ‘™ğ‘– ColPali LVLM âœ“ âœ“ 34.1 29.4 28.1 25.7 25.3 35.8 20.3 71.1 38.1 51.0 31.8 32.9 34.0 45.2 44.8 23.4 38.9 LCğ‘ğ‘ğ‘”ğ‘’ - LVLM âœ— âœ“ 41.0 26.9 31.5 30.3 27.6 39.7 23.6 53.1 37.2 40.2 29.7 23.7 30.3 34.0 39.1 17.9 31.3 VRAGğ‘ğ‘ğ‘”ğ‘’ ğ¶ğ¿ğ¼ ğ‘ƒ CLIP LVLM âœ— âœ“ 28.9 27.7 24.7 22.5 28.3 34.8 16.7 75.4 37.3 33.8 26.2 20.1 26.1 24.1 34.2 18.9 26.2 VRAGğ‘ğ‘ğ‘”ğ‘’ ğ‘†ğ‘–ğ‘”ğ¿ğ¼ ğ‘ƒ SigLIP LVLM âœ— âœ“ 16.1 13.5 13.5 9.2 9.5 15.4 9.2 77.2 26.3 21.2 25.3 17.0 19.1 17.4 17.7 25.8 19.9 VRAGğ‘ğ‘ğ‘”ğ‘’ ğ¶ğ‘œğ‘™ğ‘ƒğ‘ğ‘™ğ‘– ColPali LVLM âœ— âœ“ 40.037.832.0 30.7 32.2 44.1 23.9 63.2 41.4 56.839.445.853.1 52.1 50.336.4 47.1 GraphRAGğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ ğµğºğ¸âˆ’ğ‘š3 BGE-m3 LLM âœ“ âœ— 48.1 25.5 36.451.530.2 51.3 27.1 48.5 42.6 58.2 33.9 32.8 36.1 59.2 54.4 15.7 45.3 MLDocRAG BGE-m3 LVLM âœ“ âœ“ 47.237.842.7 41.3 31.9 52.626.4 71.5 47.9 65.1 39.4 48.341.1 66.956.3 23.4 50.8 Table 1: Performance comparison on MMLongBench-Doc and LongDocURL measured by Accuracy (%). For MMLongBench-Doc, five formats are considered: text (TXT), layout (LAY), chart (CHA), table (TAB), and image (FIG), with scopes including single- page (SIN), multi-page (MUL), and unanswerable (UNA). For LongDocURL, four formats are evaluated: text (TXT), layout (LAY), table (TAB), and figures (FIG), with evidence spans categorized as single-page (SP), multi-page (MP), or cross-element (CE), where CE denotes the integration of multiple modalities. Best results are shown in bold, and second-best results are underlined. (a) MMLongBench-Doc (b) LongDocURL Figure 3: Ablation on node variants across both datasets. allowing structured evidence propagation and ranking. As a re- sult, MLDocRAG delivers consistent gains across different evidence sources and document scopes, demonstrating superior grounding and robustness for multimodal long-context document question answering. 5.2 Impact of Node Variants (RQ2) Figure 3 analyzes the impact of different MCQG node variants on the performance of MLDocRAG.(1) Query Node Choices (Query vs. Answer).Generated queries play a dominant role in performance. WhileMLDocRAG w/ Queryremains relatively stable,MLDocRAG w/ Answerexhibits substantial performance degradation (dropping below 39% on MMLongBench-Doc), indicating that answers alone are insufficient as graph node representations. The default ML- DocRAG, which combines queries with answers, achieves the best results by leveraging answers as complementary contextual sig- nals.(2) Chunk Node Ranking Strategies (Max vs. Mean).The Max strategy consistently outperforms Mean aggregation. Using averaged similarity scores (MLDocRAG w/ Mean) degrades perfor- mance compared to the default Max-based ranking, suggesting that a chunkâ€™s relevance is better captured by its most relevant query (a) Hopsâ„ (b) KNNğ‘˜ (c) Max Nodesğ‘› Figure 4: Ablation on hyperparameters (MMLongBench-Doc). (a) Hopsâ„ (b) KNNğ‘˜ (c) Max Nodesğ‘› Figure 5: Ablation on hyperparameters (LongDocURL). rather than by an average over all associated queries.(3) Visual Noise Filtering.Visual noise filtering is crucial for effective re- trieval. Removing this component (MLDocRAG w/o Filter) leads to noticeable performance drops, confirming that filtering out ir- relevant image chunks (e.g., blank pages or logos) is necessary to prevent noise from interfering with retrieval. 5.3 Impact of Hyperparameters (RQ3) Figures 4 and 5 illustrate the impact of key hyperparameters on the performance of MLDocRAG.(1) Expansion Hops ( â„).Multi-hop expansion (â„= 1, 2) consistently outperforms zero-hop retrieval (â„= 0), highlighting the benefit of query node expansion through Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu (a) MMLongBench-Doc (b) LongDocURL Figure 6: Ablation on MDoc2Query optimization. graph traversal. However, performance degrades when â„= 3, in- dicating that excessive expansion introduces semantic noise that outweighs the gains from additional context.(2) KNN Neighbors (ğ‘˜).Accuracy generally improves as the number of nearest neigh- bors increases, peaking aroundğ‘˜= 3. This suggests that a moderate graph density effectively bridges semantic gaps between queries, while denser connections beyond this point yield diminishing re- turns or introduce noise.(3) Max Nodes ( ğ‘›).Performance exhibits a clear optimum at ğ‘›= 10. Retrieving too few nodes (ğ‘›= 5) limits evidence coverage, whereas retrieving too many nodes ( ğ‘›> 10) introduces irrelevant information, leading to notable performance drops. In addition, a stricter similarity threshold (ğ›¼= 1.2) consis- tently outperforms a looser threshold (ğ›¼= 1.0), emphasizing the importance of prioritizing high-quality entry nodes over quantity. 5.4 Impact of MDoc2Query Optimization (RQ4) Figure 6 illustrates the impact of non-parametric and parametric optimization strategies for MDoc2Query on the performance of MLDocRAG.(1) Non-Parametric Optimization.Providing each chunk with its corresponding page image during query generation consistently improves performance. Specifically,MLDocRAG ğ‘ƒ out- performs the chunk-only MLDocRAG across both datasets, demon- strating that incorporating page-level context enhances query qual- ity by mitigating semantic fragmentation caused by isolated chunks. Representative queryâ€“answer examples generated by MDoc2Query with and without page context are provided in Appendix C. Further- more,MLDocRAG ğ‘ƒ ğ‘ƒ integrates page-level information at both the query generation and answer generation stages, achieving the best overall performance. This result confirms that supplying reference page context throughout both retrieval indexing and QA stages maximizes evidence utilization.(2) Parametric Optimization. MLDocRAG remains effective when deployed with smaller LVLMs. Although the 32B model achieves the strongest overall performance, the 7B model with supervised fine-tuning (MLDocRAG w/ 7B SFT) attains 50.1% accuracy on LongDocURL, closely matching the 32B baseline (50.8%). These results demonstrate that MLDocRAG can be efficiently adapted to resource-constrained models through para- metric optimization. 5.5 Use Case Figure 7 illustrates a representative use case from MMLongBench- Doc that requires cross-modal reasoning across distant scientific diagrams (Pages 3 and 14). In contrast to baseline methods such as ğ‘…ğ´ğºğ‘¡ğ‘¥ğ‘¡+ğ‘–2ğ‘¡ BGE-m3 and ğ‘‰ ğ‘…ğ´ğºğ‘ğ‘ğ‘”ğ‘’ ColPali, which fail to bridge this long-context Question & Answerâœ“ Question: Which step in Figure 1 maps to the content of Figure 10? Evidence Pages:[3, 14] Ground Truth:Deletion/duplication/rearrangement of the genetic material and Genetic diseases. RAGtxt+i2t BGE-m3 /times Response: The question asks which step inFigure 1(Pathogenesis of genetic diseases) maps to the content ofFigure 10. However, the provided context does not include any information about Figure 10. Since the content of Figure 10 is not described in the given text, it is impossible to determine which step in Figure 1 corresponds to it. Final Answer: I donâ€™t know. VRAGpage ColPali /times Response: The question asks which step inFigure 1maps to the content ofFigure 10. However, the provided images and text do not include either Figure 1 or Figure 10. Without these figures, it is impossible to determine the mapping between them. Final Answer: I donâ€™t know. MLDocRAGâœ“ Response: To determine which step in Figure 1 maps to the content of Figure 10, letâ€™s analyze both figures step by step. Figure 1: Pathogenesis of Genetic Diseases. Figure 1 outlines a sequence of events leading to genetic diseases: (1)Mutation (2)Deletion / Duplication / Rearrangement of Genetic Material (3)Deficient / Defective Transcription of mRNA (4)Deficient / Defective Post-Transcriptional Modifications of mRNA (5)Deficient / Defective Translation of mRNA (6)Deficient / Defective Production of Proteins / Regulatory Factors (7)Deficient / Defective Post-Translational Modifications of Proteins (8)Disturbed Function(s) of Metabolic Networks (9)Patho-Physiological Alterations of Cellular Functions (10)Genetic Disease Figure 10: Pathological Consequences of Genomic Structural and Functional Mutations. Figure 10 illustrates the consequences of genomic imprinting defects and chromosomal abnormalities, specifically: â€¢Structural and numerical chromosomal mutationsleading to altered genetic material. â€¢ The resulting phenotypes (e.g., normal zygote, gynogenetic constitution, and androgenetic constitution) are shown. Mapping the Content. Figure 10 focuses on the consequences ofstructural and numerical chromosomal mutations, which directly correspond to the second step in Figure 1:Deletion / Duplication / Rearrangement of Genetic Material. The phenotypes shown in Figure 10 (e.g., normal zygote, gynogenetic constitution, and androgenetic constitution) are the downstream effects of these chromosomal mutations, aligning with the pathogenesis outlined in Figure 1. Final Answer: Deletion / Duplication / Rearrangement of Genetic Material. Figure 7: Use Case on MMLongBench-Doc. gap, MLDocRAG successfully retrieves the dispersed evidence and performs the required multi-hop reasoning. Specifically, MLDocRAG accurately aligns the â€œchromosomal mutationsâ€ visually depicted in â€œFigure 10â€ on Page 14 with the corresponding â€œDeletion / Duplica- tion / Rearrangementâ€ stage in â€œFigure 1â€ on Page 3, demonstrating its superior capability in fine-grained information extraction and cross-page visual alignment. More use cases are provided in Ap- pendix D. 6 Conclusion We presented MLDocRAG, a framework for multimodal long-document QA built on the Multimodal Chunk-Query Graph (MCQG), which enables unified, query-centric retrieval. By extending document expansion to the multimodal setting via MDoc2Query, MCQG orga- nizes heterogeneous chunks and their generated queries into a struc- tured graph that captures cross-modal and cross-page associations. This query-centric representation supports selective, multi-hop re- trieval and semantically grounded evidence aggregation, achieving consistent gains on MMLongBench-Doc and LongDocURL. Our results demonstrate the effectiveness of query-based multimodal retrieval and the scalability of graph-structured organization for multimodal long-context understanding. MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY 7 Limitations While MLDocRAG achieves promising results, several limitations remain. First, it currently supports only text and image, limiting generalization to richer modalities such as video or audio. Second, its effectiveness depends on the quality of generated queriesâ€”noisy or incomplete queries may reduce retrieval accuracy. Finally, con- structing large multimodal graphs can be computationally expen- sive, posing challenges for scaling to massive document collections. References [1] Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Moham- madali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soley- mani Baghshah, and Ehsaneddin Asgari. 2025. Ask in any modality: A compre- hensive survey on multimodal retrieval-augmented generation.arXiv preprint arXiv:2502.08826(2025). [2] Martin AumÃ¼ller, Erik Bernhardsson, and Alexander Faithfull. 2020. ANN- Benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. Information Systems87 (2020), 101374. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al . 2025. Qwen2. 5-vl technical report.arXiv preprint arXiv:2502.13923(2025). [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923(2025). [5] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation.arXiv preprint arXiv:2402.03216 4, 5 (2024). [6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al . 2024. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 24185â€“24198. [7] Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal. 2024. M3docrag: Multi-modal retrieval is what you need for multi-page multi-document understanding.arXiv preprint arXiv:2411.04952(2024). [8] Chao Deng, Jiale Yuan, Pi Bu, Peijie Wang, Zhong-Zhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song, Bo Zheng, et al . 2025. Longdocurl: a comprehensive multimodal long document benchmark integrating understanding, reasoning, and locating. InProceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1135â€“1159. [9] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. 2024. Internlm- xcomposer2-4khd: A pioneering large vision-language model handling resolu- tions from 336 pixels to 4k hd.Advances in Neural Information Processing Systems 37 (2024), 42566â€“42592. [10] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel MazarÃ©, Maria Lomeli, Lucas Hosseini, and HervÃ© JÃ©gou. 2025. The faiss library.IEEE Transactions on Big Data(2025). [11] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. 2024. From local to global: A graph rag approach to query-focused summarization.arXiv preprint arXiv:2404.16130(2024). [12] Miles Efron, Peter Organisciak, and Katrina Fenlon. 2012. Improving retrieval of short texts through document expansion. InProceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval. 911â€“ 920. [13] Tianyu Fan, Jingyuan Wang, Xubin Ren, and Chao Huang. 2025. MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation.arXiv preprint arXiv:2501.06713(2025). [14] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, CÃ©line Hudelot, and Pierre Colombo. 2024. Colpali: Efficient document retrieval with vision language models.arXiv preprint arXiv:2407.01449(2024). [15] Mitko Gospodinov, Sean MacAvaney, and Craig Macdonald. 2023. Doc2Queryâ€“: when less is more. InEuropean Conference on Information Retrieval. Springer, 414â€“422. [16] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al . 2024. A survey on llm-as-a-judge.The Innovation(2024). [17] JosÃ© Guia, ValÃ©ria GonÃ§alves Soares, and Jorge Bernardino. 2017. Graph databases: Neo4j analysis.. InICEIS (1). 351â€“356. [18] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. 2019. A comprehensive survey of deep learning for image captioning.ACM Computing Surveys (CsUR)51, 6 (2019), 1â€“36. [19] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. 2024. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. InFindings of the Association for Computational Linguistics: EMNLP 2024. 3096â€“3120. [20] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card.arXiv preprint arXiv:2410.21276(2024). [21] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. Financebench: A new benchmark for financial question answering.arXiv preprint arXiv:2311.11944(2023). [22] Rafal Kuc and Marek Rogozinski. 2013.Elasticsearch server. Packt Publishing Ltd. [23] Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, et al. 2024. A survey on benchmarks of multimodal large language models.arXiv preprint arXiv:2408.08632(2024). [24] Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, et al. 2025. A comprehen- sive survey on long context language modeling.arXiv preprint arXiv:2503.17407 (2025). [25] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. 2024. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations.Advances in Neural Information Processing Systems37 (2024), 95963â€“96010. [26] Lang Mei, Siyu Mo, Zhihan Yang, and Chong Chen. 2025. A survey of multimodal retrieval-augmented generation.arXiv preprint arXiv:2504.08748(2025). [27] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document expansion by query prediction.arXiv preprint arXiv:1904.08375(2019). [28] Jiwon Park, Seohyun Pyeon, Jinwoo Kim, Rina Carines Cabal, Yihao Ding, and Soyeon Caren Han. 2025. DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections.arXiv preprint arXiv:2508.15851(2025). [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. InInternational conference on machine learning. PmLR, 8748â€“8763. [30] Stephen Robertson, Hugo Zaragoza, et al . 2009. The probabilistic relevance framework: BM25 and beyond.Foundations and trendsÂ®in information retrieval 3, 4 (2009), 333â€“389. [31] Amit Singhal and Fernando Pereira. 1999. Document expansion for speech retrieval. InProceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 34â€“41. [32] Ray Smith. 2007. An overview of the Tesseract OCR engine. InNinth international conference on document analysis and recognition (ICDAR 2007), Vol. 2. IEEE, 629â€“ 633. [33] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. 2023. Slidevqa: A dataset for document visual question an- swering on multiple images. InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 13636â€“13645. [34] Tao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. 2006. Language model information retrieval with document expansion. InProceedings of the Human Language Technology Conference of the NAACL, Main Conference. 407â€“ 414. [35] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: a family of highly capable multimodal models.arXiv preprint arXiv:2312.11805(2023). [36] Qwen Team. 2024. Qwen2.5: A Party of Foundation Models. https://qwenlm. github.io/blog/qwen2.5/ [37] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. 2025. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features.arXiv preprint arXiv:2502.14786(2025). [38] Jordy Van Landeghem, RubÃ¨n Tito, Åukasz Borchmann, MichaÅ‚ Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, MickaÃ«l Coustaty, Bertrand Anckaert, Ernest Valveny, et al. 2023. Document understanding dataset and evaluation (dude). InProceedings of the IEEE/CVF International Conference on Computer Vision. 19528â€“19540. [39] Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Single document summa- rization with document expansion. InAAAI. 931â€“936. [40] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. 2024. Mineru: An open-source solution for precise document content extraction.arXiv preprint arXiv:2409.18839 (2024). [41] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang. 2025. Multimodal needle in a haystack: Benchmarking long-context capability of multimodal large language models. InProceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu (Volume 1: Long Papers). 3221â€“3241. [42] Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, et al. 2024. Needle in a multimodal haystack.Advances in Neural Information Processing Systems37 (2024), 20540â€“20565. [43] Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. 2025. Multimodal chain-of-thought reasoning: A comprehensive survey.arXiv preprint arXiv:2503.12605(2025). [44] Yaxiong Wu, Jianyuan Bo, Yongyue Zhang, Sheng Liang, and Yong Liu. 2025. Query-Centric Graph Retrieval Augmented Generation.arXiv preprint arXiv:2509.21237(2025). [45] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Cheng- peng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2 Technical Report.arXiv preprint arXiv:2407.10671(2024). [46] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2024. A survey on multimodal large language models.National Science Review11, 12 (2024), nwae403. [47] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sig- moid loss for language image pre-training. InProceedings of the IEEE/CVF inter- national conference on computer vision. 11975â€“11986. [48] Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying Li, Ka-Ho Chow, Conghui He, and Wentao Zhang. 2025. Ocr hinders rag: Evaluating the cascading impact of ocr on retrieval-augmented generation. InProceedings of the IEEE/CVF International Conference on Computer Vision. 17443â€“17453. [49] Rui Zhang, Chen Liu, Yixin Su, Ruixuan Li, Xuanjing Huang, Xuelong Li, and Philip S Yu. 2025. A Comprehensive Survey on Multimodal RAG: All Combina- tions of Modalities as Input and Output.Authorea Preprints(2025). [50] Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. 2023. Retrieving multimodal information for augmented generation: A survey.arXiv preprint arXiv:2303.10868(2023). [51] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. 2024. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems37 (2024), 62557â€“62583. MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY A LLM Prompt The following are some LLM prompts for query generation and answer evaluation, including MDoc2Query Prompt, Page-Context- Aware MDoc2Query Prompt, Response Generation Prompt, Page- Context-Aware Response Generation and Evaluation Prompt. A.1 MDoc2Query Prompt MDoc2Query Prompt â€”Roleâ€” You are aDoc2Queryassistant. â€”Goalâ€” Given a text chunk and image, generate no more than 20 distinct user queries that can be directly answered by that provided text chunk and image. For each query, also provide an exact answer and a relevance score. â€”Generation Rulesâ€” (1) Answerability:Every query must be answerable usingonlyinfor- mation in the text chunk and image. (2) Comprehensive coverage:Collectively, all generated queries should cover all key ideas in the text chunk and image from differ- ent viewpoints or levels of detail. (3) Adaptive quantity:Adjust the number of generated queries (0-20) according to the semantic richness and information value of both text and image. (4) Diversity requirements:Ensure diversity along the following dimensions: â€¢ Question-style variety:Mix interrogative forms (who/what/why/how/where/when/did), imperative prompts ("List... ", "Summarize... "), comparative questions, conditional or speculative forms, etc. â€¢ Content-perspective variety:Include queries on facts, definitions, methods, reasons, outcomes, examples, comparisons, limita- tions, and so on. â€¢ Information granularity:Combine macro (overall purpose, high- level summary) and micro (specific figures, terms, steps) queries. â€¢ User-intent variety:Simulate intents such as confirmation, eval- uation, usability, diagnosis, and decision-making (e.g., "Is this approach more efficient than...?"). â€¢ Linguistic expression variety:Vary wording, syntax (active â†” passive), and synonyms; avoid repeating near-identical phrases. â€¢ No redundancy:Each query must be meaningfully distinct; elim- inate trivial rephrases that offer no new angle. â€¢ Chunk-grounded specificity:Queries must be grounded in spe- cific factual points from the text chunk and image. Avoid vague or generic formulations such as "What did X say?" or "Tell me more about Y" that lack anchoring in actual content. (5) Required fields:Each output item must be based on the given text chunk and image, including the following fields: â€¢query: A question or search phrase a user might ask. â€¢answer : A concise answer taken verbatim (or nearly verbatim) from the text chunk and image. â€”Exampleâ€” 1. Input Chunk and Images [ {"type": "images", "image": "/path/image1"}, {"type": "images", "image": "/path/image2"}, {"type": "text", "text": "Alice met with Bob at the Central Cafe on Tuesday to discuss their upcoming collaborative research project..."} ] 2. Generated Queries â€¢Where did Alice and Bob meet? â€¢When did the meeting take place? â€¢What was the main topic discussed during the meeting? â€¢Who suggested incorporating advanced AI methodologies? â€¢... (more queries) â€”Output Formatâ€” Return only a JSON array of objects. Each object must include: â€¢"index": a zero-based integer â€¢"query": the generated question â€¢"answer": the exact answer Adjust the number of queries according to the information richness of the text and image (0â€“20 queries). [ {"index": 0, "query": "Where did Alice and Bob meet?", "answer": " Central Cafe"}, {"index": 1, "query": "When did the meeting take place?", "answer": "Tuesday"} ] A.2 Page-Context-Aware MDoc2Query Prompt Page-Context-Aware MDoc2Query Prompt â€”Roleâ€” You are an expert **Document Understanding AI** designed to build a high-precision Retrieval Graph. â€”Inputsâ€” (1) Target Chunk:A specific segment of content (Text, Cropped Image, or Table Data) extracted from a document. (2) Source Page Image:The original full-page screenshot where this chunk is located. â€”Workflow (Strict Execution Order)â€” Step 1: Visual Localization & Context Recovery (Mental Scratchpad) Locate: First, look at the Source Page Image and identify exactly where the Target Chunk is located. Scan Surroundings: Look immediately above, below, and to the sides of the chunk location in the full page. Identify Missing Context: Find any informationnotinside the chunk but essential for understanding it. This includes: â€¢Headers: Section Titles, Page Headers, Chapter Names. â€¢ Captions: Figure Titles (e.g., "Figure 3: Revenue Trend"), Table Headers, or Axis Labels that were cut off. â€¢ Pre-text: Preceding sentences that define what "this table" or "the data below" refers to. Step 2: Adaptive Query Generation Based on the chunkâ€™s content and the recovered context from Step 1, gener- ate a set of 5 to 20 Query-Answer pairs. Quantity Rule:If the chunk is dense (e.g., a complex table or dense text), generate closer to 20. If it is sparse, generate closer to 5. Coverage Rule:You must generate queries for all 4 Levels defined below. The distribution ratio is up to you based on the content type. â€”Query Level Definitionsâ€” Level 1: Integrated Entity Relationships (Dense & Comprehensive) Goal: Instead of asking about single entities one by one, generate complex queries that link multiple entities found in the chunk. Constraint: Do not generate simple questions like "What is X?". Instead, ask: "How does [Entity A] interact with [Entity B] regarding [Topic]?" or "What are the key specifications and performance metrics of [Product X]?" Why: To create strong semantic connections between entities in the graph without flooding it with simple queries. Level 2: Detailed Content Description (The "Core Message") Goal: Paraphrase and summarize the detailed information provided inside the chunk. Instruction: Imagine a user asks "What specific details does this para- graph/chart provide?". Cover the key arguments, data trends, or descriptive points. Why: To ensure the chunk is retrievable via natural language descriptions of its content. Level 3: Macro Hierarchy (Navigation) Goal: Anchor this chunk to the document structure. Instruction: Extract the Section Title, Chapter Name, or Page Header from the Source Page Image. Generate queries that link this specific chunk con- tent to that high-level topic. Example: "What information does the section â€™[Section Title]â€™ provide re- garding [Chunk Topic]?" Level 4: Context Restoration (Immediate Surroundings) Goal: Fix "context loss" caused by chunking. Instruction: Explicitly incorporate the missing headers, captions, or pre-text Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu you found in Step 1 into the query. Critical: If the chunk is a table/chart without a title, use the title found in the Page Image to formulate the query. Example: "According to [Figure Title found in Page Image], what trend is shown in the data regarding [Chunk Content]?" or "As detailed in [Table Caption], what are the specific values for [Row Name]?" â€”Output Format (JSON Only)â€” [ { "index": 0, "level": "level_1_entity_integrated", "query": "Comprehensive query connecting Entity A and B...", "answer": "Detailed answer..." }, { "index": 1, "level": "level_2_detailed_content", "query": "...", "answer": "..." }, { "index": 2, "level": "level_3_macro_hierarchy", "query": "...", "answer": "..." }, { "index": 3, "level": "level_4_context_restoration", "query": "Query incorporating the external Figure Title/Table Header...", "answer": "..." } // ... generate 5 to 20 pairs total ] A.3 Response Generation Prompt Response Generation Prompt You are a knowledgeable assistant that answers questions based on the given text and image data. â€”Guidelinesâ€” (1) Carefully reason through the provided information before answer- ing, but only use evidenceexplicitly supportedby the text or image. (2) If the answer cannot be determined from the provided data, clearly say you donâ€™t know. (3) Avoid unnecessary explanationsâ€”respond concisely and directly. (4) Present the final output in the format:Final Answer: [your an- swer] A.4 Page-Context-Aware Response Generation Prompt Page-Context-Aware Response Generation Prompt You are an expert Multimodal QA Assistant. You will be provided with a user question and a set ofRetrieved Contexts. Each Context consists of: (1) Text/Data Chunk:A specific segment of text, a table row, or a data point retrieved from a document. (2) Reference Page Image:The full document page where this chunk is located. â€”Goalâ€” Answer the userâ€™s question accurately by synthesizing information from the provided Text Chunks and verifying it against the Reference Page Images. â€”Reasoning Guidelinesâ€” (1)Visual Verification (Grounding): â€¢ Use theReference Page Imageto verify the context of the Text Chunk. â€¢ Example:If a chunk is a row of numbers, look at the Page Image to identify theColumn HeadersandRow Labelsto ensure you interpret the numbers correctly. â€¢ Example:If a chunk describes a chart trend, look at the Page Image to confirm theAxis Labels,Units, andLegends. (2)Contextual Synthesis: â€¢ The Text Chunk might be stripped of its section title. Use the Page Image to see whichSection Header(e.g., "2023 Q4 Results" vs "2022 Q4 Results") the chunk belongs to. â€¢ Combine information from multiple chunks if necessary to form a complete answer. (3)Strict Evidence-Based: â€¢ AnswerONLYusing the provided information. Do not use outside knowledge. â€¢ If the text chunk and the image contradict each other, trust the Visual Evidence (Image)for raw data (numbers, charts) and theText Chunkfor semantic explanations. â€¢ If the answer is not present in the provided contexts, explicitly state: "Based on the provided documents, I cannot answer this question. " (4)Conciseness: â€¢ Get straight to the point. Do not start with "Based on the con- text... " or "The image shows... ". Just state the answer. â€”Output Formatâ€” Analysis: (Briefly map the chunk text to the visual location in the page image to confirm headers/legends) Final Answer:[Your direct answer] A.5 Evaluation Prompt Evaluation Prompt You are a strict and precise evaluation assistant. You will be given a ques- tion, a reference answer, and a candidate answer generated via retrieval- augmented generation (RAG). â€”Goalâ€” Evaluate the candidate answer against the reference answer based on factual accuracy and completeness. Slight differences in phrasing are acceptable as long as the meaning is the same. If the candidate answer includes an analysis followed by "Final Answer:", only evaluate the content after "Final Answer:". If "Final Answer:" is missing, treat the entire candidate as the final answer. â€”Normalization for Unanswerable/Noneâ€” Treat the following expressions asequivalent to "Not answerable / No answer / None"when they appear as the candidateâ€™s final answer: â€¢ Explicit statements:"I donâ€™t know", "Not answerable", "Not enough information", "Insufficient information", "Not mentioned", "Un- known", "Cannot be determined", "No information provided", "N/A", "Not applicable", "None". â€¢ Negative-existence statements:Assertions of absence for list/type questions, e.g., "No stages require a cooler. ", "No such category exists. ", "There are none. ", "No [items] are present. ", "None of the stages... ". Note:This normalization appliesonly when the reference answer itself is unanswerable/none/empty. Normalizationtakes precedence over all other ruleswhen determining equivalence. â€”Important Rulesâ€” â€¢ If the candidate answer fails to provide an answer (e.g., says "I donâ€™t know")when the reference is answerable, it must receive ascore of 0. â€¢ If the reference answer is unanswerable/none/empty, the candidate answer must produce an unanswerable-equivalent expression (as normalized above) to receivescore 1. If the candidate gives any substantive or fabricated specific content, assignscore 0. â€¢ If the candidate answer overgeneralizes, omits key elements, or adds unrelated information not supported by the reference, the score must be0, even if part of it is correct. MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY â€¢ Only when the candidate answer coversall essential factual elementsof the reference answer without introducing unrelated content, should the score be1. â€”Scoring Criteriaâ€” Score = 1 (Correct): â€¢ The candidate answer accurately matches the factual content and level of detail of the reference answer. Minor wording differences are acceptable if the meaning is equivalent. â€¢ If the reference answer is "Unable to answer" / "Not answerable" / "None" / empty, and the candidate provides an unanswerable- equivalent expression (after normalization). Score = 0 (Incorrect): â€¢ The reference answer is answerable, but the candidate says any unanswerable-equivalent expression (including negative- existence). â€¢ The reference answer is answerable, but the candidate gives an incorrect, incomplete, or irrelevant answer. â€¢ The reference answer is unanswerable/none/empty, but the candi- date provides any substantive or fabricated answer. B Details of Visual Noise Filtering We employ clip-vit-base-patch32 to classify visual chunks via zero-shot inference. Based on their semantic relevance to the RAG task, visual elements are partitioned into two sets: those retained for indexing and those filtered as decorative noise. Table 2 lists the specific labels for each action. Retain table, chart, graph, diagram, map, infographic, equation, flow chart, scatter plot, bar chart, form Filter logo, banner, advertisement, poster, cover, illustration, background, icon, photo, texture Table 2: Visual chunk classification and filtering actions. Rationale:Elements labeled for filtering are pruned to eliminate redundant visual noise (e.g., repeating logos) and enhance the pre- cision of the embedding space during retrieval. C MDoc2Query Example There are some query-answer pair examples generated by MDoc2Query and Page-Context-Aware MDoc2Query. The multimodal chunk and corresponding page image as follows: Multimodal Chunk with Page Image { "type": "image", "text": "", "image": [ "./images/9bc7afb9a74e615b67303104c415d.jpg" ], "visual_context": "./visual_context/a325667e4e534f96.png" } Chunk Image: Corresponding Page Image: Doc2Query Example [ { "index": 0, "query": "Which app store had more apps in 2015?", "answer": "Google Play Store", "q_id": "reportq32015-151009093138-lva1-app6891_95-6-0" }, { "index": 1, "query": "How many apps were in the Apple App Store in 2015?", "answer": "1,5 million", "q_id": "reportq32015-151009093138-lva1-app6891_95-6-1" }, { "index": 2, "query": "What was the number of apps in the Google Play Store in 2015?", "answer": "1,6 million", "q_id": "reportq32015-151009093138-lva1-app6891_95-6-2" }, { "index": 3, "query": "In which year did the number of apps in both stores start to increase significantly?", "answer": "2013", "q_id": "reportq32015-151009093138-lva1-app6891_95-6-3" }, { "index": 4, "query": "How many apps were in the Apple App Store in 2012?", "answer": "0,5 million", "q_id": "reportq32015-151009093138-lva1-app6891_95-6-4" }, { "index": 5, "query": "What was the number of apps in the Google Play Store in 2012?", "answer": "0,35 million", "q_id": "reportq32015-151009093138-lva1-app6891_95-6-5" }, ... ] Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Yongyue Zhang and Yaxiong Wu Page-Context-Aware Example [ { "index": 0, "level": "level_1_entity_integrated", "query": "How do the growth trends of apps in the Apple App Store and Google Play Store compare from 2012 to 2015, and what does this indicate about their market dynamics?", "answer": "From 2012 to 2015, both the Apple App Store and Google Play Store experienced significant growth in the number of apps. The Google Play Store showed a more rapid increase, starting at 0.35 million in 2012 and reaching 1.6 million in 2015. The Apple App Store started at 0.5 million in 2012 and reached 1.5 million in 2015. This indicates that while both platforms grew, Google Play Store expanded at a faster rate, leading to a larger number of apps by 2015.", "q_id": "reportq32015-151009093138-lva1-app6891_95-6-0" }, { "index": 1, "level": "level_2_detailed_content", "query": "What specific details does the chart provide about the number of apps in the Apple App Store and Google Play Store from 2012 to 2015?", "answer": "The chart shows the number of apps in the Apple App Store and Google Play Store from 2012 to 2015, measured in millions. In 2012, the Apple App Store had 0.5 million apps, while the Google Play Store had 0.35 million. By 2013, the numbers increased to 0.8 million for the Apple App Store and 0.37 million for the Google Play Store. In 2014, the Apple App Store reached 1.25 million apps, and the Google Play Store reached 1.3 million. By 2015, the Apple App Store had 1.5 million apps, and the Google Play Store had 1.6 million apps.", "q_id": "reportq32015-151009093138-lva1-app6891_95-6-1" }, { "index": 2, "level": "level_3_macro_hierarchy", "query": "What information does the section'Global Mobile Apps by Number'provide regarding the growth of apps in the Apple App Store and Google Play Store?", "answer": "The section'Global Mobile Apps by Number'provides information on the growth of apps in the Apple App Store and Google Play Store from 2012 to 2015. It highlights that the number of apps in the Google Play Store grew by more than 50% last year, reaching over 1.6 million, compared to 1.5 million for the Apple App Store. This section emphasizes the rapid expansion of mobile apps globally and the competitive landscape between the two major app stores.", "q_id": "reportq32015-151009093138-lva1-app6891_95-6-2" }, { "index": 3, "level": "level_4_context_restoration", "query": "According to the'Global Mobile Apps by Number' section, what trend is shown in the data regarding the growth of apps in the Apple App Store and Google Play Store?", "answer": "The'Global Mobile Apps by Number'section shows a consistent upward trend in the number of apps in both the Apple App Store and Google Play Store from 2012 to 2015. The Google Play Store experienced a more significant growth rate, surpassing the Apple App Store by 2015. This trend indicates the rapid expansion of the mobile app market and the increasing competition between the two major app stores.", "q_id": "reportq32015-151009093138-lva1-app6891_95-6-3" }, ... ] D MLDocRAG QA Example There is a use case from the MMLongBench-Doc dataset to eval- uate the multi-page and cross-modal reasoning capabilities on base- line method and our MLDocRAG. QA Example 1 on MMLongBench-Doc Souce evidence images: Question:Among the adults conducted the survey on April 25 - May 1 2018, how many adults rated Trumpâ€™s government ethical standards as poor? Evidence Pages:[3, 4] Ground Truth:541 MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY RAGtxt+i2t BGE-m3 /times Response: To answer the question, letâ€™s break down the information provided: (1)Overall Ratings of Trumpâ€™s Government Ethical Standards: â€¢ The survey conducted from April 25 to May 1, 2018, asked respondents to rate the ethical standards of top Trump administration officials. â€¢ According to the text, 58% of adults rated the ethical standards of top Trump administration officials as â€œnot goodâ€ or â€œpoor. â€ (2)Specific Breakdown: â€¢ Poor:The text does not provide a specific percentage for those who rated the ethical standards as â€œpoorâ€ alone. â€¢However, it does state that 36% of adults rated the ethical standards as â€œpoor. â€ (3)Conclusion: â€¢ The question asks for the number of adults who rated Trumpâ€™s government ethical stan- dards as â€œpoor. â€ â€¢Based on the information provided, 36% of adults rated the ethical standards as â€œpoor. â€ Final Answer: 36% VRAGpage ColPali /times Response: To answer the question, letâ€™s analyze the relevant information from the provided images: (1) The survey was conducted from April 25 to May 1, 2018, as stated in the text. (2) The relevant chart is the one titled â€œMost rate top Trump administration officialsâ€™ ethics as â€˜not goodâ€™ or â€˜poorâ€â€™ (Image 2). (3) This chart shows the percentage of adults who rated the ethical standards of top Trump administration officials as â€œPoor, â€ â€œNot good, â€ â€œGood, â€ or â€œExcellent. â€ (4) The â€œPoorâ€ category is represented by the dark blue bar, which shows 58% of the total respon- dents. Final Answer: 58% of adults rated Trumpâ€™s government ethical standards as poor. MLDocRAGâœ“ Response: To answer the question, letâ€™s break down the information provided: (1) The survey was conducted from April 25 to May 1, 2018. (2) The question asks about the number of adults who rated Trumpâ€™s government ethical standards as â€œpoor. â€ (3) From the text, we know: â€¢ About four-in-ten Americans rate the ethical standards of Trump administration officials as excellent (9%) or good (30%). â€¢58% say they are not good (21%) or poor (36%). (4) Specifically, the text states: â€œ58% say they are not good (21%) or poor (36%). â€ This means 36% of the adults rated Trumpâ€™s government ethical standards as â€œpoor. â€ (5) The survey sample size is 1,503 adults. (6) To find the number of adults who rated Trumpâ€™s government ethical standards as â€œpoor, â€ we calculate: â€¢36% of 1,503 = 0.36Ã—1,503 = 541.08 Since we canâ€™t have a fraction of a person, we round to the nearest whole number. Final Answer: 541 Section 5.5 presents a QA example on MMLongBench-Doc with detailed result comparisons; in this section, we further supplement the example with two evidence page images. QA Example 2 on MMLongBench-Doc Souce evidence images: Question:Which step in Figure 1 maps to the content of Figure 10? Evidence Pages:[3, 14] Ground Truth:Deletion/duplication/rearrangement of the genetic material and Genetic diseases.