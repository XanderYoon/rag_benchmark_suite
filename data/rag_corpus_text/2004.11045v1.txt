arXiv:2004.11045v1 [cs.IR] 23 Apr 2020 Distilling Knowledge for Fast Retrieval-based Chat-bots Amir Vakili Tahami a_vakili@ut.ac.ir University of Tehran Kamyar Ghajar k.ghajar@ut.ac.ir University of Tehran Azadeh Shakery shakery@ut.ac.ir University of Tehran ABSTRACT Response retrieval is a subset of neural ranking in which a mo del selects a suitable response from a set of candidates given a c on- versation history. Retrieval-based chat-bots are typically employed in information seeking conversational systems such as cust omer support agents. In order to make pairwise comparisons betwe en a conversation history and a candidate response, two approa ches are common: cross-encoders performing full self-attentio n over the pair and bi-encoders encoding the pair separately. The f ormer gives better prediction quality but is too slow for practica l use. In this paper, we propose a new cross-encoder architecture and trans- fer knowledge from this model to a bi-encoder model using dis til- lation. This eﬀectively boosts bi-encoder performance at n o cost during inference time. We perform a detailed analysis of thi s ap- proach on three response retrieval datasets. CCS CONCEPTS • Information systems → Retrieval models and ranking ; • Com- puting methodologies → Natural language processing . KEYWORDS Retrieval-based chat-bot, Response ranking, Neural information re- trieval ACM Reference Format: Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery. 2020. Dist illing Knowledge for Fast Retrieval-based Chat-bots. In . ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION Response retrieval is a subset of neural ranking in which a mo del selects a suitable response from a set of candidates given a c onver- sation history. Retrieval-based chat-bots are typically employed in information seeking conversational systems such as customer sup- port agents. They have been used in real-world products such as Microsoft XiaoIce [15] and Alibaba GroupâĂŹs AliMe Assist [ 9]. To ﬁnd the best response to a particular conversation’s chat his- tory traditional text retrieval methods such as term frequency have proven to be insuﬃcient [10], therefore the majority of mode rn re- search focuses on neural ranking approaches [4, 6, 10]. These meth- ods rely on training artiﬁcial neural networks on large data sets for Permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided that copies ar e not made or distributed for proﬁt or commercial advantage and that copies bear this n otice and the full cita- tion on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.or g. SIGIR ’20, July 25-30, 2020, Xi’an, China © 2020 Association for Computing Machinery. https://doi.org/10.1145/nnnnnnn.nnnnnnn the task of selecting a suitable response among a set of candi dates according to a conversation history. By pre-training large scale language models on vast corporaand subsequently ﬁne-tuning these models on downstream tasks, re- searchers have achieved state-of-the-art results in a widevariety of natural language tasks [3]. This process has also been succe ssfully applied to the task of response retrieval [4, 6, 13]. Current state-of- the-art response retrieval focuses on using these pre-trained trans- former language models such as BERT [3]. When using a deep pre-trained transformer for the task of comparing two text i nputs, two approaches are common: either encoding representations sep- arately (bi-encoding) or encoding the concatenation of the two (cross-encoding). The BERT bi-encoder encodes two separat e rep- resentations using pre-trained deep multi-layer transfor mers and compares them using a dot product operation. The BERT cross- encoder concatenates the conversation history and candida te re- sponse and encodes them into a single representation, which is fed into a fully connected network that gives a matching score. The lat- ter method achieves better prediction quality but is far too slow for practical use [6]. While bi-encoding does give worse results, previous work ha s shown that one can signiﬁcantly reduce its inference time by pre- encoding candidate responses oﬄine so that during inferenc e, only the conversation history needs to be encoded. This, in turn, means that at inference time, bi-encoders can potentially perfor m pair- wise comparisons between a conversation history and millio ns of candidate responses. Such a feat is impossible to do with cro ss- encoders as they must recalculate encodings for each conver sa- tion history and candidate response pair. Naturally, this m akes bi- encoders a desirable solution in conversational systems where real- time response selection is required [6]. Because of this imp roving the performance of bi-encoders is a popular avenue of resear ch when it comes to response retrieval. In this paper, we demonstrate one possible improvement to bi - encoders, which will boost their prediction quality withou t aﬀect- ing their prediction speed. We propose transferring knowledge from the better performing BERT cross-encoder to the much fasterBERT bi-encoder. This method will raise BERT bi-encoder prediction qual- ity without increasing inference time. We employ knowledgedistil- lation, which is an approach where a model teaches another model to mimic it as a student [5]. Essentially, the student model learns to reproduce the outputs of the more complex teacher model. Unl ike gold labels, the output of a neural network is not constraine d to a binary variable and as such it can provide a much richer sign al when training the student model. Knowledge distillation ha s been successfully applied in natural language understanding, m achine translation, and language modeling tasks [7, 16, 20]. We also introduce a new cross-encoder architecture we call t he enhanced BERT cross-encoder. This architecture is speciﬁc ally de- signed for the task of response retrieval and gives better re sults SIGIR ’20, July 25-30, 2020, Xi’an, China Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery Table 1: Statistics for the datasets. UDC DSTC7 MANtIS № of candidates 10 100 11 Trn Vld Tst Trn Vld Tst Trn Vld Tst № of samples 500k 50k 50k 100k 5k 1k 82k 18k 18k than the regular BERT cross-encoder. It also has the advanta ge of being faster to train. This model serves as our teacher, and w e use the BERT bi-encoder [6] as our student model. We evaluate our approach on three response retrieval data-sets. Our experi ments show that our knowledge distillation approach enhances the pre- diction quality of BERT the bi-encoder. This increase comes to a no-cost during inference time. 2 METHOD First, we explain the task in further detail. Next, we descri be the teacher and student models used for the knowledge distillat ion ap- proach. Then we describe the knowledge distillation proced ure. 2.1 Task Deﬁnition The task of response retrieval can be formalized as follows: Sup- pose we have a dataset D = { ci , ri , /y.alti } N i =1 where ci = { t1, · · · , tm } represents the conversation and ri = { t1, · · · , tn } represents a can- didate response and /y.alti ∈ { 0, 1} is a label. /y.alti = 1 means that ri is a suitable choice for ci . ti are tokens extracted from text . The goal of a model should be to learn a function /afii10069.ital( c, r ) that predicts the matching degree between any new conversation history c and a candidate response r . Once a given model ranks a set of candi- dates, its prediction quality is then measured using recall @1 (1 if the model’s ﬁrst choice is correct otherwise 0) and mean reci procal rank (MRR). 2.2 Model Architecture For the student network, we use the previously proposed BERT bi-encoder [6]. The conversation history and response cand idate tokens are encoded separately using BERT. To aggregate the ﬁ - nal layer encodings into a single vector, the ﬁrst token’s en coding, which corresponds to an individual [CLS] token, is selected . BERT requires all inputs to be prepended with this special token.The two aggregated vectors are compared using a dot-product operat ion. Similarly, our teacher model uses a BERT transformer to en- code the conversation history and candidate response. Howe ver, for comparing the last layer encodings we use a combination o f scaled dot-product attention [18] and the SubMult function [19] for calculating the matching score. Below we give a brief exp lana- tion of these components before describing how they are used . In an attention mechanism, each entry of a key vectork ∈ Rnk × d is weighted by an importance score deﬁned by its similarity to each entry of query q ∈ Rnq × d . For each entry of q the entries of k are then linearly combined with the weights to form a new rep- resentation. Scaled dot-product attention is a particular version of attention deﬁned as: Att( q, k) = so f tmax ( q · kT √ d ) · k (1) The SubMult function [19] is a function designed for comparing two vectors a ∈ Rd and b ∈ Rd which has been used to great eﬀect in various text matching tasks including response retrieva l [17]. It is deﬁned as follows: SubMult ( a,b) = a ⊕ b ⊕ ( a − b) ⊕ ( a ⊙ b) (2) where ⊕ and ⊙ are concatenation and hadamard product oper- ators respectively. Utilizing these components we build our enhanced cross-encoder architecture. First, like the bi-encoder, we encode the conversation history c ∈ Rm× d and candidate response r ∈ Rn× d as follows: c ′ = T ( c) , r ′ = T ( r ) where T is the BERT transformer and c ′ ∈ Rm× d , r ′ ∈ Rn× d are the encoded tokens. To compare the encoded conversation history c ′ and encoded candidate response r ′, ﬁrst we perform a cross attention operation using the previously described components: ˆc = W1 · SubMult ( c ′, Att( c ′, r ′)) ˆr = W1 · SubMult ( r ′, Att( r ′, c ′)) (3) where W1 ∈ R4d × d is a a learned parameter. We aggregate ˆc ∈ Rm× d and ˆr ∈ Rn× d by concatenating the ﬁrst token (corre- sponding to [CLS]), the max pool and average pool over the tokens: ¯c = ˆc1 ⊕ max 1≤ i ≤ m ˆci ⊕ mean 1≤ i ≤ m ˆci ¯r = ˆr1 ⊕ max 1≤ i ≤ n ˆri ⊕ mean 1≤ i ≤ n ˆri (4) We compare the aggregated ¯c, ¯r ∈ Rd vectors using a ﬁnal Sub- Mult function and a two layer fully connected network: /afii10069.ital( c, r ) = W2( ReLU ( W3 · SubMult ( ¯c, ¯r ))) where W2 ∈ R12d × d ,W3 ∈ Rd × 1 are learned parameters. Our en- hanced BERT architecture essentially encodes the conversation his- tory and candidate response tokens separately using BERT, t hen applies as single layer of cross-attention on those encodin gs. We believe our enhanced cross-encoder architecture will pe r- form better than regular cross-encoders for two reasons. Firstly, we do not concatenate conversation history and candidate resp onses. This means we can use the encoded candidate response tokens o f other samples in a training batch as negative samples [11]. S caled dot-product attention is simple enough that recalculating it for other candidates in the batch does not add signiﬁcant overhe ad, especially when compared to rerunning BERT for every possib le conversation history and candidate response pair. Thus we can pro- cess more negative samples than would be feasible in a regula r cross-encoder. Previous research has already shown that in creas- ing the number of negative samples is eﬀective for response r e- trieval [6]. Secondly, the addition of the SubMult function means we can achieve much more reﬁned text matching between the con - versation history and candidate response. Distilling Knowledge for Fast Retrieval-based Chat-bots SIGIR ’20, July 25-30, 2020, Xi’an, China 2.3 Distillation Objective Distillation achieves knowledge transfer at the output lev el. The student learns from both dataset gold labels and teacher pre dicted probabilities, which are also a useful source of information [1]. For example, in sentiment classiﬁcation, certain sentences mi ght have very strong or weak polarities and binary labels are not enou gh to convey this information. Similar to previous work [16], we add a distillation objecti ve to our loss function which penalizes the mean squared error l oss between the student and teacher model outputs: Ldistill = || z( T ) − z( S ) || 2 where z( T ) , z( S ) are the teacher and student model outputs. At training time the distillation objective is used in conjunc tion with regular cross entropy loss as follows: L = α · L C E + ( 1 − α ) · L distill where α is a hyper-parameter. This procedure is model agnostic and can transfer information between entirely diﬀerent arc hitec- tures. 3 EXPERIMENTS In this section we give a brief overview of experiments setti ngs. 3.1 Datasets We consider three information-seeking conversation datasets widely used in the training of neural ranking models for response retrieval. The Ubuntu Dialogue Corpus (UDC) [10] and DSTC7 sentence se- lection track dataset [2] are collected form a chatroom dedi cated to the support of the Ubuntu operating system. We also includ e a version of UDC where the training set has been reduced to 20% so as to study the eﬀects of limited training data. MANtIS [13 ] was built from conversations of 14 diﬀerent sites of the Stack Exchange Network. The statistics for these datasets are provided in T able 1. Data augmentation, where each conversation is split into mu lti- ple samples, is a popular method in dialog research for boost ing the performance of response retrieval models. In this paper , we refrain from using this approach as our focus is not beating s tate- of-the-art results but empirically demonstrating the eﬀec tiveness of knowledge distillation even in limited-resource settin gs. 3.2 Baselines We divide our experiments into three parts. 1. Comparing the reg- ular BERT cross-encoder and our enhanced BERT cross-encode r. Here we aim to demonstrate the superiority of our proposed cr oss- encoder architecture 2. Comparing the BERT bi-encoder with and without distillation. Here we wish to demonstrate the eﬀectiveness of the knowledge distillation approach. 3. Finally, we also train a BiLSTM bi-encoder with and without distillation in order to con- ﬁrm the distillation process works with shallow student mod els. The BiLSTM bi-encoder uses the same tokens as BERT models, but their embeddings are not pre-trained and initialized rando mly. We use the same aggregation strategy (eq. 4) to aggregate the Bi LSTM hidden states. Our code will be released as open-source. 3.3 Implementation Details Our models are implemented in the PyTorch framework [12]. Fo r our BERT component, we used Distilbert [14] since it provide s re- sults somewhat close to the original implementation despit e hav- ing only 6 layers of transformers instead of 12. We tune α from a set of { 0.25, 0.5, 0.75} . We train models using Adam optimizer [8]. We use a learning rate of 5 × 10− 5 for BERT models and 10 − 3 for the BiLSTM bi-encoder. For consistency, we set the batch siz e to 8 for all models. For each dataset, we set the maximum number of tokens in the conversation history and candidate responses so that no more than 20% of inputs are truncated. Unfortunately, due to limited computing resources, we are u n- able to beat state-of-the-art results reported by [6]. Our models are trained on a single GPU; thus, we had to make compromises on the number of input tokens, number of negative samples, and mode l depth. 4 RESULTS AND DISCUSSION In this section, we go over the results of our experiments. We ana- lyze both prediction quality and eﬃciency. 4.1 Prediction Quality The ﬁrst two rows of table 2 demonstrate the eﬀectiveness our the enhanced BERT cross-encoder relative to the regular BERT cr oss- encoder. These results indicate that employing a task-spec iﬁc sin- gle layer cross-attention mechanism on top of separately en coded inputs is highly eﬀective for the task of response retrieval . Of par- ticular note is the increased gap between the performance of the two methods when using smaller training sets (UDC 20%, MANtIS, DSTC7). This shows that the regular bert-cross model strugg les when ﬁne-tuned with smaller response-retrieval sets and data aug- mentation or a some other method must be used to achieve accep t- able results. In contrast, our enhanced BERT cross-encoder ’s R@1 only dropped by 3.3 points when its training set was reduced t o a ﬁfth. To further demonstrate the eﬀectiveness of our modiﬁcation s to the BERT cross-encoder architecture, we perform an ablat ion study on the reduced UDC dataset. We replace the SubMult func- tion with a concatenation operation. We also try removing cr oss- attention (3). In both cases, their removal signiﬁcantly de grades model quality. Across the datasets, bi-encoders show signiﬁcant gains whe n trained with knowledge distillation. The increase in perfo rmance is relatively substantial. Such gains usually require an in crease in model complexity, however with knowledge distillation, we are ef- fectively gaining a free boost in performance as there is no e xtra cost at inference time. The best results were obtained with a n α of 0.5. This indicates that in response retrieval, unlike ot her tasks such as sentiment classiﬁcation and natural language infer ence [16], the gold labels cannot be replaced entirely with teach er out- puts. 4.2 Prediction Eﬃciency We demonstrate the trade-oﬀ in speed and performance betwee n the BERT bi-encoder and our enhanced BERT cross-encoder. We measure the time it takes to process test samples in the DSTC7 SIGIR ’20, July 25-30, 2020, Xi’an, China Amir Vakili Tahami, Kamyar Ghajar, and Azadeh Shakery Table 2: Prediction quality metrics across all datasets. Metrics for models trained with knowledge distillation, which are signif- icant relative to models trained without it, are marked in bo ld. We use paired two-tailed t-tests with a p-value<0.05 to p erform signiﬁcance tests. For easier reading metrics have been mul tiplied by 100. No data augmentation has been used and traini ng samples are used as is. +KD indicates a model trained with kno wledge distillation. UDC20% UDC MANtIS DSTC7 R@1 MRR R@1 MRR R@1 MRR R@1 MRR BERT cross 66.1 76.8 76.5 84.8 59.8 72.0 36.9 47.9 BERT cross enhanced 76.2 84.5 79.5 86.9 66.7 77.3 53.3 63.3 - SubMult 73.4 82.6 — — — — — — - Attention 67.2 78.6 — — — — — — BiLSTM bi-encoder 59.2 72.4 69.4 80.2 35.6 55.1 34.3 46.1 BiLSTM bi-encoder + KD 63.0 75.2 70.4 80.8 45.5 61.4 39.4 50.1 BERT bi-encoder 64.9 76.9 72.9 82.7 47.9 58.4 39.9 51.8 BERT bi-encoder + KD 66.1 77.6 75.8 84.6 53.4 67.3 53.8 54.7 Table 3: Average milliseconds to process a single test sample. № of candidates 10 100 BERT bi-encoder 5.6 6.2 BERT cross-encoder enhanced 81.1 981.2 dataset and show the average time for each example in table 3. Time taken by the cross-encoder to process a set of candidate re- sponses grows exponentially large as the set increases in si ze. In the case of BERT bi-encoders, since candidate vectors can be com- puted oﬄine, increasing candidates has a negligible impact on in- ference time. 5 CONCLUSION AND FUTURE WORK In this paper, we introduced an enhanced BERT cross-encoder ar- chitecture modiﬁed for the task of response retrieval. Alon gside that, we utilized knowledge distillation to compress the co mplex BERT cross-encoder network as a teacher model into the stude nt BERT bi-encoder model. This increases the BERT bi-encoders pre- diction quality without aﬀecting its inference speed. We ev aluate our approach on three domain-popular datasets. The proposed meth- ods were shown to achieve statistically signiﬁcant gains. One possible avenue for research is the exploration of other knowledge transfer methods. Substituting the relatively simple BERT bi-encoder architecture with a more complex architecture [4] or de- veloping further improvements to the BERT cross-encoder are also viable alternatives. REFERENCES [1] Jimmy Ba and Rich Caruana. 2014. Do deep nets really need t o be deep?. In Advances in neural information processing systems . [2] Lazaros Polymenakos Chulaka Gunasekara, Jonathan K. Ku mmerfeld and Wal- ter S. Lasecki. 2019. DSTC7 Task 1: Noetic End-to-End Respon se Selection. In 7th Edition of the Dialog System Technology Challenges at AA AI 2019. [3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Langu age Understanding. In Proceedings of the 2019 Conference of the North American Chap ter of the Asso- ciation for Computational Linguistics: Human Language Tec hnologies. [4] Matthew Henderson, Iñigo Casanueva, Nikola Mrkšić, Pei -Hao Su, Ivan Vulić, et al. 2019. ConveRT: Eﬃcient and Accurate Conversational R epresentations from Transformers. arXiv preprint arXiv:1911.03688 (2019). [5] Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. 2015. Disti lling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015). [6] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Ja son Weston. 2020. Poly-encoders: architectures and pre-training strategie s for fast and accurate multi-sentence scoring. In 8th International Conference on Learning Representa- tions, ICLR 2020 . [7] Yoon Kim and Alexander M Rush. 2016. Sequence-Level Know ledge Distillation. In Proceedings of the 2016 Conference on Empirical Methods in Na tural Language Processing. [8] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for S tochastic Opti- mization. In 3rd International Conference on Learning Representations , ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedin gs. [9] Feng-Lin Li, Minghui Qiu, Haiqing Chen, Xiongwei Wang, X ing Gao, Jun Huang, Juwei Ren, Zhongzhou Zhao, Weipeng Zhao, Lei Wang, Guwei Jin , and Wei Chu. 2017. AliMe Assist : An Intelligent Assistant for Creating an Innovative E-commerce Experience. In Proceedings of the 2017 ACM on Conference on Infor- mation and Knowledge Management, CIKM 2017 . [10] Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau . 2015. The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructur ed Multi-Turn Di- alogue Systems. In Proceedings of the 16th Annual Meeting of the Special Interes t Group on Discourse and Dialogue . [11] Pierre-Emmanuel Mazaré, Samuel Humeau, Martin Raison , and Antoine Bordes. 2018. Training Millions of Personalized Dialogue Agents. I n Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro cessing, Brussels, Belgium, October 31 - November 4, 2018 . [12] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chan an, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, a nd Adam Lerer. 2017. Automatic Diﬀerentiation in PyTorch. In NIPS Autodiﬀ Workshop. [13] Gustavo Penha and Claudia Hauﬀ. 2020. Curriculum Learn ing Strategies for IR: An Empirical Study on Conversation Response Ranking. In European Conference on Information Retrieval. Springer. [14] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thom as Wolf. 2019. Dis- tilBERT, a distilled version of BERT: smaller, faster, chea per and lighter. arXiv preprint arXiv:1910.01108 (2019). [15] Heung-Yeung Shum, Xiao-dong He, and Di Li. 2018. From El iza to XiaoIce: chal- lenges and opportunities with social chatbots. Frontiers of Information Technol- ogy & Electronic Engineering (2018). [16] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechto mova, and Jimmy Lin. 2019. Distilling task-speciﬁc knowledge from BERT into simple neural networks. arXiv preprint arXiv:1903.12136 (2019). [17] Chongyang Tao, Wei Wu, Can Xu, Wenpeng Hu, Dongyan Zhao, and Rui Yan. 2019. Multi-Representation Fusion Network for Multi-Turn Response Selection in Retrieval-Based Chatbots. InProceedings of the Twelfth ACM International Con- ference on Web Search and Data Mining . [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. At tention is all you need. In Advances in neural information processing systems . [19] Shuohang Wang and Jing Jiang. 2017. A Compare-Aggregat e Model for Match- ing Text Sequences. In 5th International Conference on Learning Representations , Distilling Knowledge for Fast Retrieval-based Chat-bots SIGIR ’20, July 25-30, 2020, Xi’an, China ICLR 2017, Toulon, France, April 24-26, 2017, Conference Tr ack Proceedings. [20] Seunghak Yu, Nilesh Kulkarni, Haejun Lee, and Jihie Ki m. 2018. On-device neu- ral language model based word prediction. In Proceedings of the 27th Interna- tional Conference on Computational Linguistics: System De monstrations.