An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation Matan Orbach, Ohad Eytan, Benjamin Sznajder, Ariel Gera, Odellia Boni, Yoav Kantor, Gal Bloch, Omri Levy, Hadas Abraham, Nitzan Barzilay, Eyal Shnarch, Michael E. Factor, Shila Ofek-Koifman, Paula Ta-Shma, Assaf Toledo IBM Research matano@il.ibm.com Abstract Optimizing Retrieval-Augmented Generation (RAG) config- urations for specific tasks is a complex and resource-intensive challenge. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To fill this gap, we present a comprehensive study involv- ing five HPO algorithms over five datasets from diverse do- mains, including a newly curated real-world product docu- mentation dataset. Our study explores the largest RAG HPO search space to date that includes full grid-search evalua- tions, and uses three evaluation metrics as optimization tar- gets. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing model selection first is preferable to the common practice of follow- ing the RAG pipeline order during optimization. 1 Introduction In theRetrieval-Augmented Generation (RAG)paradigm, a generative LLM answers user questions using a retrieval system which provides relevant context from a corpus of documents (Lewis et al. 2020; Huang and Huang 2024; Gao et al. 2024; Wang et al. 2024b). By relying on a dedicated re- trieval component, RAG solutions focus LLMs on grounded data, reducing the likelihood of dependence on irrelevant preexisting knowledge. The popularity of RAG is largely thanks to its modular design, allowing full control over which data sources to pull data from and how to process that data. While advantageous, this modularity also means that practitioners are faced with a wide array of decisions when designing their RAG pipelines. One such choice is which generative LLM to use; other choices pertain to parameters of the retrieval system, such as how many items to retrieve per input question, how to rank them, and so forth. Furthermore, evaluating even a single RAG configuration is costly in terms of time and funds: the embedding step as part of corpus indexing is compute-intensive; generat- ing answers using LLMs is also a demanding task, espe- cially for large benchmarks; and evaluation with LLM-as- Copyright © 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: We study hyper parameter optimization over a RAG pipeline with5parameters. The explored search space includes162RAG configurations formed from combina- tions of the depicted hyper parameters. a-Judge (LLMaaJ) adds another costly round of inference. As a result, exhaustively exploring the exponential search space of RAG parameters is prohibitively expensive. At the same time, suboptimal parameter choices may significantly harm result quality. A promising approach to this challenge ishyper- parameter optimization (HPO)for RAG (Fu et al. 2024; Kim et al. 2024), which aims to identify high-performing configurations by systematically evaluating a subset of the search space. Existing methods range from established HPO algorithms to simple random sampling. Importantly, despite growing interest, the effectiveness of HPO in realistic RAG scenarios has not been rigorously tested. Our work addresses this gap through a comprehensive study of HPO for RAG in a setup that mirrors real-world usage, where a dataset is provided up front for experimen- tation, and unseen queries arrive after deployment. The best RAG configuration is therefore selected by development set performance and evaluated on held out test data. To the best of our knowledge, this is the first study that considers RAG HPO in such a setup. The scope of our experiments includes multiple datasets, evaluation metrics and algorithms. To represent diverse use cases, we evaluate across several domains: scientific (Eibich, Nagpal, and Fred-Ojala 2024), biomedical (Krithara et al. 2023), Wikipedia (Rosenthal et al. 2024; Smith, Heilman, and Hwa 2008) and a newly curated enterprise product docu- mentation dataset that we open-source as part of this work.1 Additionally, recognizing that different applications priori- tize distinct performance metrics – such as answer correct- 1http://huggingface.co/datasets/ibm-research/watsonxDocsQA arXiv:2505.03452v3 [cs.CL] 31 Dec 2025 ness or faithfulness – our study evaluates multiple RAG op- timization objectives, implemented through two alternative approaches: lexical overlap metrics and LLMaaJ. Our evaluation compares five HPO algorithms: Tree- Structured Parzen Estimators (TPE) (Watanabe 2023), three greedy variants, and random search. Also included are base- line grid search results for both the development and test sets, over the full search space. 2 These are crucial for es- tablishing upper bounds on HPO performance for RAG, and understanding its generalization capability. While the inclusion of grid search enables a comparison to the best possible result, it poses computational constraints on the size of the explored search space. Nonetheless, our search space is the largest considered for RAG HPO to date while still including a comparison to full grid search. The search space comprises of162RAG configurations derived from five core RAG parameters (see Figure 1): chunk size and overlap, which control how documents are split, the embedding model used to encode chunks in a vector database, the number of retrieved chunks included as context when answering a question, and the generative model that produces the answer. Exploring an expanded search space introduced by more complex RAG pipelines, such as agentic workflows, poses significant computational challenges for exhaustive grid search. Consequently, this as- pect is deferred to future work. Our evaluation addresses multiple aspects of HPO for RAG, including the convergence properties of the algo- rithms, the impact of the objective metric on the best con- figuration, and an analysis of HPO overall cost. We also ex- plore reducing that cost through development set sampling. In summary, our main contributions are as follows: (i) comprehensive benchmarking of RAG HPO in a realistic generalization setup, over the largest search space with full grid-search evaluations to-date, showing that RAG HPO can be done efficiently, either greedily or with simple random search, and that it significantly boosts RAG performance for all datasets; (ii) a detailed analysis of the results, explor- ing the connections between the optimized parameters, the dataset and the optimization objective. For greedy HPO ap- proaches, we show that the order in which the parameters are optimized is of great importance; (iii) new open-source re- sources: the full grid search results of our experiments, and an enterprise product documentation RAG dataset. 2 Related Work Within the open-source community, several tools offer out- of-the-box HPO algorithms for RAG. AutoRAG (Kim et al. 2024) adopts a greedy approach, optimizing one RAG pa- rameter at a time following the sequential pipeline order. 3 RAGBuilder employs TPE for HPO. 4 Additionaly, RAG- centric libraries such as LlamaIndex 5 support HPO by inte- grating general-purpose optimization frameworks like Ray- 2The grid results are at https://github.com/IBM/rag-hpo-bench. 3https://github.com/Marker-Inc-Korea/AutoRAG 4https://github.com/KruxAI/ragbuilder 5https://www.llamaindex.ai/ Tune (Liaw et al. 2018), optuna (Akiba et al. 2019) and hy- peropt (Bergstra, Yamins, and Cox 2013). Other works investigate the impact of RAG hyper- parameters without automated optimization. For example, Lyu et al. (2024) and Wang et al. (2024c) focus on man- ual tuning of RAG hyper-parameters, while Zhu et al. (2024) evaluates multiple configurations via grid search. Their stud- ies highlight the critical role of hyper-parameter tuning in RAG and motivate the need for automated RAG HPO. Another interesting line of work (Fu et al. 2024) is moti- vated by a setting of online feedback from users. It describes an online HPO algorithm that iteratively updates the reward for the various RAG parameters based on small batches of queries. In contrast, our work addresses offline HPO, where optimization is performed on full benchmark datasets prior to best configuration deployment. More recently, Barker et al. (2025) introduced multi- objective HPO for RAG. The studied methods select a set of RAG configuration deemed Pareto-optimal by multiple met- rics. Choosing the best configuration from this set remains an open challenge. Our study instead focuses on single- objective HPO returning a single configuration as output. Despite these efforts, still missing is a systematic evalua- tion of HPO algorithms across diverse datasets under realis- tic conditions where optimized configurations are tested on held-out sets. This gap is the primary focus of our work. Building upon existing approaches, our evaluation priori- tizes HPO algorithms already considered in the context of RAG, over introducing alternatives such as BOHB (Falkner, Klein, and Hutter 2018) or SMAC (Lindauer et al. 2022). Specifically, the greedy algorithms we explore resemble those used by Kim et al. (2024), and the TPE algorithm we assess is the same one used by RAGBuilder. 3 Experimental Setup Search space In our explored RAG pipeline (see Figure 1), process- ing starts withchunkingthe input documents into smaller chunks, based on two parameters: thesizeof each chunk in tokens, and theoverlapbetween consecutive chunks. This is followed by representing each chunk with a dense vec- tor created by anembeddingmodel – our third parame- ter. The vectors are stored in a vector-database, 6 alongside their original text. Upon receiving a query, the topkrelevant chunks are retrieved (retrieval);kbeing our fourth param- eter. Lastly, a prompt containing the query and the retrieved chunks is passed to agenerative model(our fifth parame- ter) to create an answer (generation). Greedy decoding was used throughout all experiments. The prompts were fixed to RAG prompts tailored to each model.7 The specific values considered for each of the five hyper- parameters are described in Table 1. In total, the search space has3∗2∗3∗3∗3 = 162possibleRAG configurations. Dividing into stages, we get18(3∗2∗3) different config- urations for data indexing (chunking and embedding) and9 6Milvus (Wang et al. 2021) with default settings; index type is HNSW (Malkov and Yashunin 2018). 7See Appendix G. Hyper Parameter RAG step Values Chunk Size (# Tokens) Chunking 256, 384, 512 Chunk Overlap (% Tokens) Chunking 0%, 25% Embedding Model Embeddingmultilingual-e5-large(Wang et al. 2024a) bge-large-en-v1.5(Xiao et al. 2023) granite-embedding-125M-english(IBM 2024) Top-K (# Chunks to retrieve) Retrieval 3, 5, 10 Generative Model GenerationLlama-3.1-8B-Instruct(AI 2024) Mistral-Nemo-Instruct-2407(AI and NVIDIA 2024) Granite-3.1-8B-instruct(Granite Team 2024) Table 1: The hyper parameters explored in our search space, and their values. Dataset Domain #Doc #Dev #Test AIArxiv Papers2673 41 30 BioASQ Biomedical40181 1000 150 MiniWiki Wikipedia3200 663 150 ClapNQ Wikipedia178890 1000 150 WatsonxQA Business5534 45 30 Table 2: Properties of the RAG Datasets in our experiments: the number of documents within the corpus (#Doc), and counts of QA pairs in the#Devand#Testsets. (3∗3) different configurations for answering (retrieval and generation). The values chosen for the chunk size, overlap, and top-k reflect common practices (see Appendix B for de- tails). Popular open source models were selected as options for the embedding and generative models. We opted to focus on LLMs that are similar in size, since otherwise an obvious strategy is to simply discard the smaller less capable models from the search. Our experiments involve performing a full grid search – i.e., evaluating all possible configurations – in order to es- tablish upper bound baselines for the optimization strategies. Hence, due to computational constraints we avoid using an even larger search space, and choose a set of moderately sized LLMs as our generators. Still, to the best of our knowl- edge, our search space is the largest considered to date that compares to full grid-search. Datasets Each RAG dataset is comprised of a corpus of documents and a benchmark of QA pairs, with most also annotating the document(s) with the correct answer. Below are the RAG datasets we used: AIArxivThis dataset was derived from the ARAGOG benchmark (Eibich, Nagpal, and Fred-Ojala 2024) of techni- cal QA pairs over a corpus of machine learning papers from ArXiv.8 As gold documents are not annotated in ARAGOG dataset, we added such labels where they could be found, obtaining71answerable QA pairs out of107in the original benchmark. 8https://huggingface.co/datasets/jamescalam/ai-arxiv2 BioASQ(Krithara et al. 2023) A subset of the BioASQ Challenge train set.9 Its corpus contains40200passages ex- tracted from clinical case reports. The corresponding bench- mark of4200QA pairs includes multiple gold documents per question. MiniWikiA benchmark of918QA pairs over Wikipedia derived from Smith, Heilman, and Hwa (2008). 10 The con- tents are mostly factoid questions with short answers. This dataset has no gold document labels. ClapNQ(Rosenthal et al. 2024) A subset of the Natu- ral Questions (NQ) dataset (Kwiatkowski et al. 2019) on Wikipedia pages, of questions that have long answers. The original benchmark contains both answerable and unanswer- able questions. For our analysis we consider only the former. ClapNQ dataset consists of178890passage texts generated from4293pages. These passages constitute the input to the pipeline. WatsonxQA(ProductDocs) A new open-source dataset and benchmark based on enterprise product documentation, consisting of5534passage texts created from1144HTML product documentation pages.11 These passages serve as the RAG pipeline input. The benchmark includes75QA pairs and gold document labels, of which25were generated by two subject matter experts, and the rest were synthetically produced and then manually filtered. All QA pairs were ad- ditionally reviewed by two of the authors, ensuring high data quality. Further details are in Appendix A. Overall, these datasets exhibit variability in many aspects. They represent diverse domains – research papers, biomed- ical documents, wikipedia pages and enterprise data (see question examples in Table 3). They also vary in ques- tion and answer lengths; for example, MiniWiki has rela- tively short answers, while ClapNQ was purposely built with long gold answers. Corpus sizes also vary, representing real- world retrieval scenarios over small or large sets of docu- ments. Every benchmark was split into development and test sets. To keep computations tractable, the number of questions in the large benchmarks (BioASQ and ClapNQ) was limited 9https://huggingface.co/datasets/rag-datasets/rag-mini-bioasq 10https://huggingface.co/datasets/rag-datasets/rag-mini- wikipedia 11http://huggingface.co/datasets/ibm-research/watsonxDocsQA Dataset Example Question AIArxiv What significant improvements does BERT bring to the SQuAD v1.1,v2.0 and v13.5 tasks compared to prior models? BioASQ What is the implication of histone lysine methylation in medulloblastoma? MiniWiki Was Abraham Lincoln the sixteenth Pres- ident of the United States? ClapNQ Who is given credit for inventing the print- ing press? WatsonxQA What tuning parameters are available for IBM foundation models? Table 3: One question example from each dataset. to1000for development and150for test. Table 2 lists the corpora benchmark sizes and domains. Metrics The following metrics were used in our experiments:Re- trieval qualitywas measured usingcontext correctness with Mean Reciprocal Rank (V oorhees and Tice 2000). 12 Answer faithfulness(Lexical-FF) measures whether a gen- erated answer remained faithful to the retrieved contexts, with lexical token precision.Answer correctnesscompares generated and gold answers, assessingoverall pipeline quality, and is measured in two ways: First, a fast, lex- ical, implementation based on token recall (Lexical-AC), which provides a good balance between speed and quality (Adlakha et al. 2024). Second, a standard LLM-as-a-Judge answer correctness (LLMaaJ-AC) implementation from the RAGAS library (Es et al. 2023), with GPT4o-mini (OpenAI 2024) as its backbone. This implementation performs3calls to the LLM on every invocation, making it much slower and more expensive than the lexical variant. Given a benchmark, all metrics were computed per- question. Averaging the per-question scores yields the over- all metric score. We note that all benchmarked HPO approaches are metric-agnostic - they are not tailored to any specific metric, nor a specific performance axis (such as answer correctness). Similarly, HPO can be applied to multiple RAG metrics at once, forming a single optimization objective. HPO Algorithms An HPO algorithm takes as input a RAG search space, a dataset (corpus and benchmark), and one evaluation met- ric designated as the optimization objective. Its goal is to find the RAG configuration that achieves the highest perfor- mance on the dataset with respect to the objective. The HPO algorithms in our experiments operate itera- tively. At each iteration, the algorithm reviews the scores 12Note that as labeling is at the document level, any chunk from the gold document is considered as a correct prediction, even if it does not include the answer to the question. of all previously explored configurations and selects an un- explored configuration to evaluate next. To simulate a con- strained exploration budget, the algorithm terminates after a fixed number of iterations and returns the best performing configuration. An efficient HPO algorithm identifies a top- performing configuration with minimal iterations. We examine two categories of HPO algorithms: (i) stan- dard HPO algorithms that are not specifically tailored to RAG; (ii) RAG-aware greedy algorithms that leverage some knowledge of the components within the optimized RAG pipeline. All algorithms optimize answer correctness unless explicitly noted otherwise. Standard algorithmsOur first standard HPO algorithm isTPE, using an implementation from hyperopt (Bergstra, Yamins, and Cox 2013), with five random initialization iter- ations, and otherwise the default settings. The second algo- rithm (Random) disregards results from prior iterations and uniformly selects an unexplored RAG configuration. RAG-Aware greedy algorithmsThe second category of algorithms assumes an ordered list of search space param- eters ranked by their presumed impact on RAG perfor- mance. These algorithms take a greedy approach: they it- erate through the parameters list, optimizing one parame- ter at a time, assuming that optimizing high impact param- eters first accelerates convergence to a strong configuration. When optimizing a parameterp, the algorithm uses fixed values for all preceding parameters and evaluates all possi- ble values pfp, with random values assigned to all following parameters. The value ofpyielding the best objective score is picked, and the algorithm continues to the next parameter. The greedy algorithms differ solely in their parame- ter ordering.Model-first ordering(Greedy-M) optimizes the generative and embedding models first, assuming they are more important: Generative Model, Embedding Model, Chunk Size, Chunk Overlap, Top-K.Retrieval- first(Greedy-R) is a prevalent option following the RAG pipeline structure, starting with retrieval optimization (still with model first), then generation: Embedding Model, Chunk Size, Chunk Overlap, Generative Model and Top- K.Retrieval-first with context correctness(Greedy-R-CC) uses the same order, yet optimizes the retrieval-related pa- rameters with a context correctness metric evaluated solely on the retrieval results, saving the costs of LLM inference until all retrieval parameters are chosen. Remaining param- eters are then optimized with answer correctness. Setup Our experimental design reflects a realistic use case: an HPO algorithm is executed over a benchmark (a development set) and the best RAG configuration is chosen for deployment; the deployed configuration is expected to generalize to un- seen questions – simulated here via a test set. Specifically, each HPO algorithm ran on each development set for10it- erations. After every iteration, the best configuration on the development set was evaluated on the test set, enabling a per-iteration generalization analysis. Since all algorithms in- volve a random element, each run was repeated with10dif- ferent random seeds. LLMaaJ-AC Lexical-AC Dataset Worst Best SE Worst Best SE AIArxiv 0.36 0.62 0.03 0.40 0.66 0.04 BioASQ 0.43 0.56 0.01 0.49 0.63 0.01 MiniWiki 0.32 0.51 0.02 0.61 0.85 0.01 ClapNQ 0.46 0.57 0.01 0.34 0.61 0.01 WatsonxQA 0.52 0.76 0.03 0.74 0.87 0.04 Table 4:WorstandBestconfiguration scores per dataset on the development set, reported for both LLMaaJ-AC and Lexical-AC metrics. Also shown is the maximum standard error (SE) observed across all configurations. [0.0 0.1) [0.1 0.2) [0.2 0.3) [0.3 0.4) [0.4 0.5) [0.5 0.6) [0.6 0.7) [0.7 0.8) [0.8 0.9) [0.9 1.0] LLMaaJ-AC 0 5 10 15 20 25% of RAG configurations Dataset ClapNQ BioASQ AIArxiv MiniWiki WatsonxQA Figure 2: The distribution of configurations across bins for the normalized LLMaaJ-AC metric on the development sets. Most datasets have a few top-performing configurations. 4 Results Grid Search We conducted a comprehensive grid search over all162con- figurations (including18different indexes), across the de- velopment and test sets from all datasets. The worst and best performing configuration scores for each dataset, on the de- velopment set, are presented in Table 4.13 There is a substan- tial gap between the two extremes. The exhaustive grid search enables a deeper analysis of the configuration landscape, including the proportion of high and low performing configurations. To quantify this, we computed a min-max normalized score per dataset and met- ric, binned the scores uniformly, and assigned each config- uration to a bin by its normalized metric score. Figure 2 shows the distribution of configurations across bins for the LLMaaJ-AC metric. Notably, for most datasets, there are a few top-performing configurations. One example is the BioASQ dataset with fewer then5%of configurations in the top two bins. In contrast, the MiniWiki dataset exhibits a dense cluster of good configurations, suggesting that a good configuration will be easy to find. These trends exemplify that the difficulty of the HPO setup is dataset and metric de- pendent.14 13For results with Lexical-FF see Appendix D. 14For results with Lexical-AC and Lexical-FF see Appendix D. The grid results serve to establish two important perfor- mance baselines. The first is the performance of the best con- figuration selected directly from test set evaluation (dashed black lines in Figure 3). The second is the best configuration chosen based on development set evaluation, and evaluated on the test set (red lines). The gap between these baselines reflects the inherent challenge of generalization. Since HPO algorithms operate solely on the development set, their real- istic target is the second baseline. The grid search results also reveal the impact of the dif- ferent RAG pipeline parameters on the measured RAG per- formance. Overall, we see that almost all of the parameter choices have some effect on performance, however in our experiments the choice of generative model had the largest impact on the eventual answer correctness. For a detailed statistical analysis of the impact of the different RAG pa- rameters, refer to Appendix C. HPO Results Figure 3 presents the per-iteration performance of the HPO algorithms on the test sets, when optimizing for the lexical and LLMaaJ-based answer correctness metrics.15 Across all datasets and metrics, the results consistently show that ex- ploring around10configurations suffices to match the per- formance of a full grid search over all162configurations. This is a strong result, demonstrating the robustness of HPO over diverse domains and evaluation metrics. Also evident is that the difficulty of the optimization prob- lem varies between datasets. For instance, in ClapNQ con- vergence is rather slow. For MiniWiki, which is rich in good performing configurations (see Figure 2), finding a top con- figuration is easier, with an effective HPO algorithm such as Greedy-M, as few as three iterations can yield a top RAG configuration (by selecting the optimal generative model early). In contrast, even in this easy scenario, a method like Greedy-R-CC converges slower then the alternatives. This underscores the importance of the HPO algorithm choice. Among greedy methods, the order of parameter optimiza- tion is critical. The results show that algorithms starting with optimizing retrieval-related parameters first (Greedy-R and Greedy-R-CC) require more iterations to find good configu- rations. Interestingly, the naive option of random sampling also finds a good RAG configuration after a small number of it- erations. That is likely due to the large impact of the gener- ative model choice on performance, as reflected by the fast convergence achieved by the Greedy-M approach. The complex TPE algorithm was found to be roughly equivalent in quality to random choice. In larger or continu- ous spaces, TPE may offer greater advantages. Impact of Metric Choice The results of Greedy-M in Figure 3 show performance is boosted significantly when the generative model parameter is optimized first, suggesting its importance. We therefore further investigate the best performing model in each setup. 15Answer faithfulness follow similar trends, see Appendix D. 1 2 3 4 5 6 7 8 9 10 # Iterations 0.5 0.6RAGAS AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.500 0.525 0.550 0.575RAGAS AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.500 0.525 0.550 0.575 0.600RAGAS AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.45 0.50 0.55RAGAS AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.65 0.70 0.75RAGAS AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.5 0.6RAGAS AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.500 0.525 0.550 0.575RAGAS AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.500 0.525 0.550 0.575 0.600RAGAS AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.45 0.50 0.55RAGAS AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.65 0.70 0.75RAGAS AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.5 0.6RAGAS AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.500 0.525 0.550 0.575RAGAS AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.500 0.525 0.550 0.575 0.600RAGAS AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.45 0.50 0.55RAGAS AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.65 0.70 0.75RAGAS AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.5 0.6RAGAS AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.500 0.525 0.550 0.575RAGAS AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.500 0.525 0.550 0.575 0.600RAGAS AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.45 0.50 0.55RAGAS AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.65 0.70 0.75RAGAS AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.5 0.6RAGAS AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.500 0.525 0.550 0.575RAGAS AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.500 0.525 0.550 0.575 0.600RAGAS AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.45 0.50 0.55RAGAS AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.65 0.70 0.75RAGAS AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC (a) LLMaaJ-AC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.550 0.575 0.600 0.625Lexical AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.62 0.64 0.66 0.68Lexical AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.55 0.60Lexical AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.80 0.85 0.90Lexical AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.750 0.775 0.800 0.825 0.850Lexical AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.550 0.575 0.600 0.625Lexical AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.62 0.64 0.66 0.68Lexical AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.55 0.60Lexical AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.80 0.85 0.90Lexical AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.750 0.775 0.800 0.825 0.850Lexical AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.550 0.575 0.600 0.625Lexical AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.62 0.64 0.66 0.68Lexical AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.55 0.60Lexical AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.80 0.85 0.90Lexical AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.750 0.775 0.800 0.825 0.850Lexical AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.550 0.575 0.600 0.625Lexical AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.62 0.64 0.66 0.68Lexical AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.55 0.60Lexical AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.80 0.85 0.90Lexical AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.750 0.775 0.800 0.825 0.850Lexical AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.550 0.575 0.600 0.625Lexical AC AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.62 0.64 0.66 0.68Lexical AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.55 0.60Lexical AC ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.80 0.85 0.90Lexical AC MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.750 0.775 0.800 0.825 0.850Lexical AC ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC (b) Lexical-AC Figure 3: Per-iteration performance of all HPO algorithms on the test sets of five datasets, optimizing answer correctness computed with an LLMaaJ metric (a) and a lexical metric (b). The dashed black lines show the best achievable performance. The red lines are the performance of the best configuration chosen with development set evaluation, on the test set. Figure 4 reports the maximal answer correctness score per dataset for each generative model, computed as the highest of the162/3 = 54configurations in which the model ap- pears. When optimizing by LLMaaJ-AC, the best RAG con- figuration consistently involves Llama, whereas for Lexical- AC, Granite or Mistral are better. This difference stems from the nature of the metrics, as Lexical-AC is recall-oriented while LLMaaJ-AC balances precision and recall. These find- ings emphasize the critical role of optimization objective se- lection. Optimal configurations can differ substantially de- pending on the chosen metric, and thus this choice should carefully align with the intended application. Cost Considerations The cost of each HPO algorithm was tracked by counting the number of tokens embedded during indexing, and the num- ber of tokens used in generation (input and output). For each algorithm, we computed its total cost so far at a specific it- eration, by accumulating these token counts over the config- urations evaluated up to that iteration (including). The cost of indices used by multiple configurations was counted just once. Per-iteration plots of these counts are in Appendix E. Overall, generation costs were similar across algorithms and datasets. The embedding costs, dependent on the num- ber of different indices created by the algorithm, behaved similarly across datasets with variations between algorithms. (a) LLMaaJ-AC (b) Lexical-AC Figure 4: The effect of chosen optimization metric on the generative model within the best RAG configuration. Shown is the maximal answer correctness score per dataset and model (the highest of the54configurations in which the model appears). 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.52 0.54 0.56 0.58RAGAS AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.52 0.54 0.56 0.58 0.60RAGAS AC ClapNQ Grid/Full Grid/Sample Random/Full Random/Sample TPE/Full TPE/Sample Greedy-M/Full Greedy-M/Sample (a) LLMaaJ-AC 1 2 3 4 5 6 7 8 9 10 # Iterations 0.60 0.62 0.64 0.66 0.68Lexical AC BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.52 0.54 0.56 0.58 0.60 0.62Lexical AC ClapNQ Grid/Full Grid/Sample Random/Full Random/Sample TPE/Full TPE/Sample Greedy-M/Full Greedy-M/Sample (b) Lexical-AC Figure 5: Per-iteration performance on the test sets of the two largest datasets, for HPO algorithms optimized using thefull development data (solid lines) or itssample(dotted). The dashed black lines show the best achievable test performance. The solid (dashed) red lines are the performance of the best configuration chosen by (sampled) development set evaluation. The Greedy-R and Greedy-R-CC algorithms create a new index at each iteration until all retrieval parameters are op- timized, making them initially expensive. Other algorithms like TPE and Random lack a mechanism to favor index reuse. Greedy-M begins by optimizing the generative model using a single index, making it cost-efficient when budget constraints or iteration limits are tight. Efficient HPO The results of Figure 3 were obtained with each RAG con- figuration evaluated on the whole development set. While simple, this option is costly for large datasets. Prior work in other domains suggests that random sampling of evaluation benchmarks can reduce costs without sacrificing evaluation quality (Perlitz et al. 2024; Polo et al. 2024), and we there- fore explore this direction. To our knowledge, this is the first study of that direction in the context of HPO for RAG. To adapt sampling to RAG, we sample both the bench- mark and the underlying corpus. Specifically, focusing on the larger datasets of BioASQ and ClapNQ, 10% of the de- velopment QA pairs were sampled along with their corre- sponding gold documents (those containing the answers to the sampled questions). To preserve realistic retrieval condi- tions we add “noise” – documents not containing an answer to any of the sampled questions – at a ratio of9such doc- uments per gold document. This yields100sampled bench- mark questions per dataset, with the sampled corpora com- prising of1K (i.e.1000) documents for ClapNQ (out of 178K), and10K for BioASQ (out of40K). 16 Following sampling, we repeat the experiments using the best HPO methods: Random, TPE, and Greedy-M. Figure 5 compares performance when optimizing for LLMaaJ-AC and Lexical-AC, using the full (solid lines) or sampled (dot- ted) development sets. For BioASQ sampling has a negli- gible impact. For ClapNQ, results differ per algorithm and metric, with a suboptimal configuration identified by TPE and Random. For Greedy-M and the LLMaaJ-AC metric, the performance drop is moderate. With this approach, cost reductions are substantial. In- ference costs for a given configuration are10x cheaper, as 10% of the questions are used. Indexing costs drop by4x for BioASQ and178x for ClapNQ. These savings make sam- pling highly attractive for HPO over large datasets. In summary, development set sampling offers a promis- ing path towards efficient RAG HPO. Combined with the Greedy-M approach, the trade-off between cost and perfor- mance remains favorable, making it a practical choice for real-world applications. 5 Conclusion We presented a comprehensive study of HPO for RAG in a generalization setup that reflects real-world usage. Our eval- uation spans five HPO algorithms, three evaluations metrics, and multiple datasets from diverse domains. One is a newly curated enterprise product documentation dataset, released as part of this work, for use by the community. 16BioASQ has multiple gold documents per question, which yields more sampled documents. Our findings are that HPO systematically boosts RAG performance significantly. Compared to an arbitrarily cho- sen RAG configuration, running10HPO iterations yield gains of up to20% (see Figure 3, comparing the perfor- mance at the first and last iterations). While our experiments focus on core RAG components, the potential impact on more complex systems parametrized by larger search spaces is likely even greater. For example, exploring HPO in the context of multi-modal or agentic RAG pipelines seems a promising direction for future work. We showed that RAG HPO can be performed efficiently. Even without prior knowledge of the RAG pipeline param- eters, exploring a small subset of the configuration space is often sufficient. Simple strategies such as random sampling perform surprisingly well, while a greedy approach that pri- oritizes model selection outperforms the common practice of sequential optimization by pipeline order. Our results highlight the importance of the optimization objective choice, as different objective choices lead to differ- ent optimal RAG configurations. We further show that devel- opment set sampling can reduce the costs of HPO for RAG by orders of magnitude. With the use of the Greedy-M algo- rithm, the saved compute, at the mild cost of performance, may be attractive for many users. For practitioners interested in boosting the performance of their RAG pipelines, we strongly suggest the use of HPO, and offer the following recommendations. Carefully choose an optimization metric that reflects the goals of the appli- cation. With that, evaluate multiple configurations with ran- domly picked parameter values, this initial quick exploration is likely to give valuable gains. Next, improve efficiency by using a greedy HPO algorithm that optimizes model choices first, that will provide faster convergence. For large datasets, combine that algorithm with development set sampling to efficiently find a top-performing RAG configuration. Finally, we open-source our complete grid search results over the development and test sets for all datasets. 17 To our knowledge, we are the first to release such a resource for RAG. Building on these results, further research can explore new HPO techniques without incurring the substantial cost of running many RAG configuration across datasets. Our re- lease also includes the generation outputs for each configu- ration, enabling easy computation of additional metrics and their analysis in the context of HPO. We hope this release will serve as a valuable contribution to research on HPO for RAG. References Adlakha, V .; BehnamGhader, P.; Lu, X. H.; Meade, N.; and Reddy, S. 2024. Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering. Transactions of the Association for Computational Linguis- tics, 12: 681–699. AI, M. 2024. Introducing Llama 3.1: Our most capable mod- els to date. AI, M.; and NVIDIA. 2024. Mistral NeMo: A state-of-the- art 12B model with 128k context length. 17https://github.com/IBM/rag-hpo-bench Akiba, T.; Sano, S.; Koyama, T.; Matsumoto, Y .; and Ohta, M. 2019. Optuna: A Next-generation Hyperparameter Opti- mization Framework.arXiv preprint arXiv:1907.10902. Barker, M.; Bell, A.; Thomas, E.; Carr, J.; Andrews, T.; and Bhatt, U. 2025. Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems. arXiv preprint arXiv:2502.18635. Bergstra, J.; Yamins, D.; and Cox, D. D. 2013. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures. InPro- ceedings of the 30th International Conference on Machine Learning (ICML 2013), I–115–I–123. Eibich, M.; Nagpal, S.; and Fred-Ojala, A. 2024. AR- AGOG: Advanced RAG Output Grading.arXiv preprint arXiv:2404.01037. Es, S.; James, J.; Espinosa-Anke, L.; and Schockaert, S. 2023. RAGAS: Automated Evaluation of Retrieval Aug- mented Generation. arXiv:2309.15217. Falkner, S.; Klein, A.; and Hutter, F. 2018. BOHB: Ro- bust and efficient hyperparameter optimization at scale. In International conference on machine learning, 1437–1446. PMLR. Fu, J.; Qin, X.; Yang, F.; Wang, L.; Zhang, J.; Lin, Q.; Chen, Y .; Zhang, D.; Rajmohan, S.; and Zhang, Q. 2024. AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation. In Al-Onaizan, Y .; Bansal, M.; and Chen, Y .-N., eds.,Findings of the Associ- ation for Computational Linguistics: EMNLP 2024, 3875– 3891. Miami, Florida, USA: Association for Computational Linguistics. Gao, Y .; Xiong, Y .; Gao, X.; Jia, K.; Pan, J.; Bi, Y .; Dai, Y .; Sun, J.; Wang, M.; and Wang, H. 2024. Retrieval- Augmented Generation for Large Language Models: A Sur- vey. arXiv:2312.10997. Granite Team, I. 2024. Granite 3.0 Language Models. Ac- cessed: 2025-02-14. Huang, Y .; and Huang, J. 2024. A Survey on Retrieval- Augmented Text Generation for Large Language Models. arXiv:2404.10981. IBM. 2024. Granite-Embedding-125M-English Model Card. Kim, D.; Kim, B.; Han, D.; and Eibich, M. 2024. Au- toRAG: Automated Framework for optimization of Re- trieval Augmented Generation Pipeline.arXiv preprint arXiv:2410.20878. Krithara, A.; Nentidis, A.; Bougiatiotis, K.; and Paliouras, G. 2023. BioASQ-QA: A manually curated corpus for Biomedical Question Answering.Scientific Data, 10(1): 170. Kwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.; Parikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin, J.; Lee, K.; et al. 2019. Natural questions: a benchmark for question answering research.Transactions of the Associa- tion for Computational Linguistics, 7: 453–466. Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V .; Goyal, N.; K ¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt ¨aschel, T.; Riedel, S.; and Kiela, D. 2020. Retrieval-augmented gen- eration for knowledge-intensive NLP tasks. InProceedings of the 34th International Conference on Neural Information Processing Systems, NIPS ’20. Red Hook, NY , USA: Curran Associates Inc. ISBN 9781713829546. Liaw, R.; Liang, E.; Nishihara, R.; Moritz, P.; Gonzalez, J. E.; and Stoica, I. 2018. Tune: A Research Platform for Distributed Model Selection and Training.arXiv preprint arXiv:1807.05118. Lindauer, M.; Eggensperger, K.; Feurer, M.; Biedenkapp, A.; Deng, D.; Benjamins, C.; Ruhkopf, T.; Sass, R.; and Hutter, F. 2022. SMAC3: A Versatile Bayesian Optimiza- tion Package for Hyperparameter Optimization.Journal of Machine Learning Research, 23(54): 1–9. Lyu, Y .; Li, Z.; Niu, S.; Xiong, F.; Tang, B.; Wang, W.; Wu, H.; Liu, H.; Xu, T.; and Chen, E. 2024. CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Mod- els.arXiv:2401.17043 [cs.CL]. Accessed: 2025-02-14. Malkov, Y . A.; and Yashunin, D. A. 2018. Efficient and ro- bust approximate nearest neighbor search using Hierarchical Navigable Small World graphs. arXiv:1603.09320. OpenAI. 2024. GPT-4o mini: Advancing Cost-Efficient In- telligence. Perlitz, Y .; Bandel, E.; Gera, A.; Arviv, O.; Ein-Dor, L.; Shnarch, E.; Slonim, N.; Shmueli-Scheuer, M.; and Choshen, L. 2024. Efficient Benchmarking (of Language Models). In Duh, K.; Gomez, H.; and Bethard, S., eds., Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2519–2536. Mexico City, Mexico: Association for Compu- tational Linguistics. Polo, F. M.; Weber, L.; Choshen, L.; Sun, Y .; Xu, G.; and Yurochkin, M. 2024. tinyBenchmarks: evaluating LLMs with fewer examples. InProceedings of the 41st In- ternational Conference on Machine Learning, ICML’24. JMLR.org. Rosenthal, S.; Sil, A.; Florian, R.; and Roukos, S. 2024. CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems. arXiv:2404.02103. Smith, N. A.; Heilman, M.; and Hwa, R. 2008. Question generation as a competitive undergraduate course project. In Proceedings of the NSF Workshop on the Question Genera- tion Shared Task and Evaluation Challenge, volume 9. V oorhees, E. M.; and Tice, D. M. 2000. The TREC-8 Question Answering Track. In Gavrilidou, M.; Carayan- nis, G.; Markantonatou, S.; Piperidis, S.; and Stainhauer, G., eds.,Proceedings of the Second International Confer- ence on Language Resources and Evaluation (LREC‘00). Athens, Greece: European Language Resources Association (ELRA). Wang, J.; Yi, X.; Guo, R.; Jin, H.; Xu, P.; Li, S.; Wang, X.; Guo, X.; Li, C.; Xu, X.; Yu, K.; Yuan, Y .; Zou, Y .; Long, J.; Cai, Y .; Li, Z.; Zhang, Z.; Mo, Y .; Gu, J.; Jiang, R.; Wei, Y .; and Xie, C. 2021. Milvus: A Purpose-Built Vector Data Management System. InProceedings of the 2021 Interna- tional Conference on Management of Data, SIGMOD ’21, 2614–2627. New York, NY , USA: Association for Comput- ing Machinery. ISBN 9781450383431. Wang, L.; Yang, N.; Huang, X.; Yang, L.; and Wei, R. M. F. 2024a. Multilingual E5 Text Embeddings: A Technical Re- port. arXiv:2402.05672. Wang, X.; Wang, Z.; Gao, X.; Zhang, F.; Wu, Y .; Xu, Z.; Shi, T.; Wang, Z.; Li, S.; Qian, Q.; Yin, R.; Lv, C.; Zheng, X.; and Huang, X. 2024b. Searching for Best Practices in Retrieval-Augmented Generation. arXiv:2407.01219. Wang, X.; Wang, Z.; Gao, X.; Zhang, F.; Wu, Y .; Xu, Z.; Shi, T.; Wang, Z.; Li, S.; Qian, Q.; et al. 2024c. Searching for best practices in retrieval-augmented generation. InProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 17716–17736. Watanabe, S. 2023. Tree-Structured Parzen Estimator: Un- derstanding Its Algorithm Components and Their Roles for Better Empirical Performance. arXiv:2304.11127. Xiao, S.; Liu, Z.; Zhang, P.; and Muennighoff, N. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597. Yehudai, A.; Carmeli, B.; Mass, Y .; Arviv, O.; Mills, N.; Toledo, A.; Shnarch, E.; and Choshen, L. 2024. Ge- nie: Achieving Human Parity in Content-Grounded Datasets Generation. arXiv:2401.14367. Zhu, K.; Luo, Y .; Xu, D.; Wang, R.; Yu, S.; Wang, S.; Yan, Y .; Liu, Z.; Han, X.; Liu, Z.; and Sun, M. 2024. RAGEval: Scenario Specific RAG Evaluation Dataset Gen- eration Framework. arXiv:2408.01262. A WatsonxQA additional details As stated in the body of the paper, the WatsonxQA bench- mark includes75QA pairs and gold document labels, of which50were generated synthetically. The benchmark con- tains five fields: a question, a gold answer and, for comput- ing context relevance metrics, the gold passage id and its content. Some example benchmark entries are given in Fig- ure 12. Synthetic generation was operated using falcon-180b model, and then manually filtered and reviewed for quality - the methodology we used is detailed in Yehudai et al. (2024). The prompt used for the synthetic generation is detailed in Figure 13. B Search Space Selection The search space values for the parameters Chunk Size, Chunk Overlap and Top-K were chosen based on previous works: Chunk sizeWang et al. (2024c) used the values {128,256,512,1024,2048}. They reported that the values 256and512performed well (see Table 3 in Wang et al. (2024c)) in terms of faithfulness and relevancy. Lyu et al. (2024) used{64,128,256,512}. We chose three values {256,384,512}. Top-KFu et al. (2024) experimented with{1,3,5,7,9}, and Lyu et al. (2024) with{2,4,6,8,10}. We chose to use {3,5,10}. Chunk OverlapLyu et al. (2024) used {0%,10%,30%,50%,70%}. Others have not consid- ered this parameter. We used{0%,25%}. C Impact of specific parameters To test the impact of the different RAG pipeline parameter choices, we conduct statistical analyses over the grid-search results for each dataset. Specifically, we fit a linear mixed-effects model on the dataset results, where the per-example answer correctness is the dependent variable, the choice of generative model, em- bedding model, chunk size, chunk overlap and retrieved K are the fixed effects we test for, and the individual examples are modeled as a random effect. In addition to the main ef- fects, we include in our model possibleinteractioneffects: between the embedding model and the generative model, be- tween the embedding model and the chunk size, between the chunk size and chunk overlap, and between the generative model and the retrieved K. To assess the significance of each of the tested main ef- fects and interactions, we performed likelihood ratio tests, comparing the full mixed-effects model to a reduced model that excludes a specific main effect or interaction. We report the resulting test statisticsχ 2 and significance valuespfor each dataset in Tables 6-15. As can be seen in the tables, most of the effects and interactions are statistically signifi- cant (p < .05), indicating that these choices do indeed affect the pipeline result. Consistently, the choice of the generative model has a particularly large effect, explaining much of the variance of the statistical model. In addition, for each dataset and metric we report the marginal means for each chosen pipeline parameter, and their delta from the overall mean metric result. As can be seen in Tables 16-20, the largest differences in the metric scores relate to the choice of generative model. In addition, the relative success of the different generative models de- pends on the chosen metric, as also shown in Figure 4. We conduct all analyses using thestatsmodelspython li- brary (v0.14.6). Parameters and marginal means were es- timated using Restricted Maximum Likelihood (REML). For the likelihood-ratio-testing of effect significance, mod- els were compared using Maximum Likelihood (ML). D Additional results See §4 for a discussion of the main results. • Table 5 shows the worst and best per-dataset Lexical-FF metric scores on the development set. • Figure 6 depicts the percentage of good and bad con- figurations for the five datasets and the Lexical-AC and Lexical-FF metrics. • Figure 7 details HPO results for the Lexical-FF metric. [0.0 0.1) [0.1 0.2) [0.2 0.3) [0.3 0.4) [0.4 0.5) [0.5 0.6) [0.6 0.7) [0.7 0.8) [0.8 0.9) [0.9 1.0] Lexical-AC 0 5 10 15 20 25 30 35% of RAG configurations Dataset ClapNQ BioASQ AIArxiv MiniWiki WatsonxQA [0.0 0.1) [0.1 0.2) [0.2 0.3) [0.3 0.4) [0.4 0.5) [0.5 0.6) [0.6 0.7) [0.7 0.8) [0.8 0.9) [0.9 1.0] Lexical-FF 0 5 10 15 20% of RAG configurations Dataset ClapNQ BioASQ AIArxiv MiniWiki WatsonxQA Figure 6: The percentage of RAG configurations assigned to each bin of normalized metric scores (with the Lexical-AC or Lexical-FF metrics), on the dev sets. E Embedding and Generation Costs Figure 8 details accumulated numbers of (a) embedded to- kens and (b) number of tokens used in generation part for HPO algorithms overall tested configurations. See §4 for a discussion of costs. F Hardware and Costs All used embedding and generation models are open source models. An internal in-house infrastructure containing V100 and A100 GPUs was used to run embedding computations and generative inference. Specifically, embeddings were computed using one V100 GPU, and inference was done on one A100 GPU (i.e. no multi-GPU inference was required). The evaluation of the LLMaaJ-AC metric was done with GPT4o-mini (OpenAI 2024) as its backbone LLM. That model was used through Microsoft Azure. The overall cost was∼500$. G Generation Prompt Details The RAG prompts used by each model are shown in Figure 9 for Granite, Figure 10 for Llama and Figure 11 for Mistral. In each prompt the{question}placeholder indicates where the user question was placed, and{retrieved documents} the location of the retrieved chunks. For Granite, each re- trieved chunk was prefixed with ‘[Document]’ and suffixed 1 2 3 4 5 6 7 8 9 10 # Iterations 0.4 0.5 0.6Lexical-FF AIArxiv 1 2 3 4 5 6 7 8 9 10 # Iterations 0.50 0.55 0.60Lexical-FF BioASQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.45 0.50 0.55Lexical-FF ClapNQ 1 2 3 4 5 6 7 8 9 10 # Iterations 0.475 0.500 0.525 0.550 0.575Lexical-FF MiniWiki 1 2 3 4 5 6 7 8 9 10 # Iterations 0.45 0.50 0.55 0.60Lexical-FF ProductDocs Grid Random TPE Greedy-M Greedy-R Greedy-R-CC Figure 7: Per-iteration performance of all HPO algorithms on the test sets of five datasets, optimizing answer faithful- ness. The dashed black lines denote the best achievable per- formance on each test set. See §4 for a discussion of the main results. by ‘[End]’. Similarly, for Llama each retrieved chunk was prefixed with ‘[document]:’. (a) Total number of tokens from chunks sent to the embedding models. (b) Total number of tokens for prompts sent to the generation models. Figure 8: Cost estimation for each algorithm after each iteration. Lexical-FF Dataset Worst Best SE AIArxiv 0.28 0.64 0.03 BioASQ 0.38 0.60 0.01 MiniWiki 0.39 0.56 0.01 ClapNQ 0.28 0.56 0.01 WatsonxQA 0.37 0.65 0.03 Table 5:WorstandBestconfiguration scores per dataset on the development set for the Lexical-FF metric. Also shown is the maximum standard error (SE) observed across all con- figurations. H Use Of AI Assistants AI Assistants were only used in writing for minor edits and rephrases. They were also used to aid in obtaining the correct LateX syntax for the various figures. Table 6: Results of a likelihood ratio test for the grid-search results of the AIArxiv dataset (LLMaaJ-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 401 105.5×10 −80 Embedding model 42 109.1×10 −6 Chunk size 48 81.2×10 −7 Chunk overlap 1.6 3 0.65K 29 65.5×10 −5 Generative model * Embedding model 2.2 4 0.7Embedding model * Chunk size 31 43.2×10 −6 Chunk size * Chunk overlap 0.28 2 0.87Generative model * K 16 43.5×10 −3 Table 7: Results of a likelihood ratio test for the grid-search results of the BioASQ dataset (LLMaaJ-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 5446 10≈0Embedding model 251 104.0×10 −48 Chunk size 55 85.4×10 −9 Chunk overlap 11 3 0.01K 218 62.4×10 −44 Generative model * Embedding model 33 41.3×10 −6 Embedding model * Chunk size 25 44.6×10 −5 Chunk size * Chunk overlap 7.4 2 0.025Generative model * K 150 42.3×10 −31 Table 8: Results of a likelihood ratio test for the grid-search results of the ClapNQ dataset (LLMaaJ-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 3103 10≈0Embedding model 1883 10≈0Chunk size 46 81.9×10 −7 Chunk overlap 23 33.3×10 −5 K 82 61.4×10 −15 Generative model * Embedding model 304 41.7×10 −64 Embedding model * Chunk size 9.5 4 0.049Chunk size * Chunk overlap 12 22.5×10 −3 Generative model * K 76 41.0×10 −15 Table 9: Results of a likelihood ratio test for the grid-search results of the MiniWiki dataset (LLMaaJ-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 12072 10≈0Embedding model 140 103.6×10 −25 Chunk size 1.6 8 0.99Chunk overlap 0.72 3 0.87K 631 65.4×10 −133 Generative model * Embedding model 41 43.4×10 −8 Embedding model * Chunk size 0.8 4 0.94Chunk size * Chunk overlap 0.72 2 0.7Generative model * K 626 44.2×10 −134 Table 10: Results of a likelihood ratio test for the grid-search results of the WatsonxQA dataset (LLMaaJ-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 757 104.5×10 −156 Embedding model 25 105.3×10 −3 Chunk size 40 83.5×10 −6 Chunk overlap 0.49 3 0.92K 43 61.3×10 −7 Generative model * Embedding model 13 4 0.012Embedding model * Chunk size 11 4 0.026Chunk size * Chunk overlap 0.41 2 0.81Generative model * K 7.1 4 0.13 <|system|> You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. <|user|> You are a AI language model designed to function as a specialized Retrieval Augmented Generation (RAG) assistant. When generating responses, prioritize correctness, i.e., ensure that your response is grounded in context and user query. Always make sure that your response is relevant to the question. Answer Length: detailed [Document] {retrieved documents} [End] {question} <|assistant|> Figure 9: The prompt used for Granite. <|begin of text|><|start header id|>system<|end header id|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <|eot id|><|start header id|>user<|end header id|> [document]:{retrieved documents} [conversation]:{question}. Answer with no more than 150 words. If you cannot base your answer on the given document, please state that you do not have an answer.¡—eot id—¿ <|start header id|>assistant<|end header id|> Figure 10: The prompt used for Llama. <s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don’t know the answer to a question, please don’t share false information. <</SYS>> Generate the next agent response by answering the question. You are provided several documents with titles. If the answer comes from different documents please mention all possibilities and use the titles of documents to separate between topics or domains. If you cannot base your answer on the given documents, please state that you do not have an answer. {retrieved documents} {question}[/INST] Figure 11: The prompt used for Mistral. Question: What are the natural language processing tasks supported in the Product library? Gold answer: The Product library supports the following natural language processing tasks: language detection, syntax analy- sis, noun phrase extraction, keyword extraction and ranking, entity extraction, sentiment classification, and tone classification. Gold context-id: EN-001 Gold passage: The following natural language processing tasks are supported as blocks or workflows in the Product library: * [Language detection] * [Noun phrase extraction] * [Keyword extraction and ranking] * [Entity extraction] * [Sentiment classification] * [Tone classification] Question: What happens to unsaved prompt text within Product, and how long does it persist on the webpage before being deleted? Gold answer: The prompt text remains unsaved unless the user decides to save their progress. While unsaved, the prompt text persists on the webpage until a page refresh occurs, upon which the text is automatically deleted. Gold context-id: EN-101 Gold passage: Privacy of text in Product during a session Text that you submit by clicking Generate from the prompt editor in Product is reformatted as tokens, and then submitted to the foundation model you choose. The submitted message is encrypted in transit. Your prompt text is not saved unless you choose to save your work. Unsaved prompt text is kept in the web page until the page is refreshed, at which time the prompt text is deleted. Figure 12: Examples of benchmark entries in WatsonxQA. Given the next[document], create a[question]and[answer]pair that are grounded in the main point of the document, don’t add any additional information that is not in the document. The[question]is by an information-seeking User and the [answer]is provided by a helping AI Agent. [document]: [A WatsonxQA document example] # Response: [question]: What is a token limit? [answer]: Every model has an upper limit to the number of tokens in the input prompt plus the number of tokens in the generated output from the model (sometimes called context window length, context window, context length, or maximum sequence length.) ... [document]: Figure 13: The prompt used for synthetic data generation. The prompt consists of instruction followed byK= 3examples of document and generated QA. Table 11: Results of a likelihood ratio test for the grid-search results of the AIArxiv dataset (Lexical-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 1918 10≈0Embedding model 143 109.5×10 −26 Chunk size 68 81.4×10 −11 Chunk overlap 1.6 3 0.66K 90 63.7×10 −17 Generative model * Embedding model 6.6 4 0.16Embedding model * Chunk size 53 47.1×10 −11 Chunk size * Chunk overlap 0.27 2 0.87Generative model * K 26 42.8×10 −5 Table 12: Results of a likelihood ratio test for the grid-search results of the BioASQ dataset (Lexical-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 10487 10≈0Embedding model 610 101.3×10 −124 Chunk size 126 81.6×10 −23 Chunk overlap 3.3 3 0.35K 632 62.6×10 −133 Generative model * Embedding model 24 48.2×10 −5 Embedding model * Chunk size 58 47.9×10 −12 Chunk size * Chunk overlap 3.2 2 0.2Generative model * K 313 42.0×10 −66 Table 13: Results of a likelihood ratio test for the grid-search results of the ClapNQ dataset (Lexical-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 24649 10≈0Embedding model 8080 10≈0Chunk size 186 85.4×10 −36 Chunk overlap 149 34.3×10 −32 K 582 61.5×10 −122 Generative model * Embedding model 64 44.5×10 −13 Embedding model * Chunk size 29 48.3×10 −6 Chunk size * Chunk overlap 27 21.1×10 −6 Generative model * K 72 48.2×10 −15 Table 14: Results of a likelihood ratio test for the grid-search results of the MiniWiki dataset (Lexical-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 12805 10≈0Embedding model 73 101.4×10 −11 Chunk size 1.3 8 1Chunk overlap 0.013 3 1K 74 67.3×10 −14 Generative model * Embedding model 27 42.5×10 −5 Embedding model * Chunk size 1.3 4 0.87Chunk size * Chunk overlap 0.012 2 0.99Generative model * K 25 46.0×10 −5 Table 15: Results of a likelihood ratio test for the grid-search results of the WatsonxQA dataset (Lexical-AC metric) Main effect/Interactionχ 2 degrees of freedom p-value Generative model 276 102.3×10 −53 Embedding model 12 10 0.28Chunk size 70 84.8×10 −12 Chunk overlap 2.5 3 0.48K 60 64.1×10 −11 Generative model * Embedding model 2.6 4 0.62Embedding model * Chunk size 7.3 4 0.12Chunk size * Chunk overlap 2 2 0.36Generative model * K 8.7 4 0.068 LLMaaJ-AC LLMaaJ-AC (∆) Lexical-AC Lexical-AC (∆) Generative model ibm-granite/granite-3.1-8b-instruct 0.43 -0.037 0.59 0.029 meta-llama/llama-3-1-8b-instruct 0.53 0.056 0.48 -0.083 mistral-nemo-instruct 0.45 -0.019 0.62 0.054 Embedding model BAAI/bge-large-en-v1.5 0.48 0.0062 0.58 0.014 ibm/slate-125m-english-rtrvr 0.47 0.0017 0.57 0.0012 intfloat/multilingual-e5-large 0.46 -0.0079 0.55 -0.015 Chunk size 256 0.47 0.0016 0.56 -0.0024 384 0.48 0.0092 0.57 0.0067 512 0.46 -0.011 0.56 -0.0043 Chunk overlap 0.0 0.47 0.0024 0.57 0.0015 0.25 0.47 -0.0024 0.56 -0.0015 K 3 0.46 -0.0099 0.55 -0.013 5 0.47 0.0016 0.57 0.0007 10 0.48 0.0083 0.58 0.012 Table 16: Marginal means for the grid search results (AIArxiv). Columns denoted by∆show the relative difference from the overall dataset mean. LLMaaJ-AC LLMaaJ-AC (∆) Lexical-AC Lexical-AC (∆) Generative model ibm-granite/granite-3.1-8b-instruct 0.45 -0.042 0.61 0.035 meta-llama/llama-3-1-8b-instruct 0.53 0.036 0.52 -0.053 mistral-nemo-instruct 0.5 0.0058 0.59 0.017 Embedding model BAAI/bge-large-en-v1.5 0.5 0.0081 0.58 0.011 ibm/slate-125m-english-rtrvr 0.49 -0.0015 0.57 -0.0015 intfloat/multilingual-e5-large 0.49 -0.0066 0.56 -0.0096 Chunk size 256 0.5 0.0015 0.57 -0.0022 384 0.49 -0.0029 0.57 -0.0021 512 0.5 0.0014 0.58 0.0042 Chunk overlap 0.0 0.49 -0.00087 0.57 0.000067 0.25 0.5 0.00088 0.57 -0.000071 K 3 0.49 -0.0041 0.56 -0.0092 5 0.5 -0.00066 0.58 0.0031 10 0.5 0.0048 0.58 0.0061 Table 17: Marginal means for the grid search results (BioASQ). Columns denoted by∆show the relative difference from the overall dataset mean. LLMaaJ-AC LLMaaJ-AC (∆) Lexical-AC Lexical-AC (∆) Generative model ibm-granite/granite-3.1-8b-instruct 0.48 -0.029 0.57 0.059 meta-llama/llama-3-1-8b-instruct 0.54 0.027 0.43 -0.078 mistral-nemo-instruct 0.51 0.0023 0.53 0.019 Embedding model BAAI/bge-large-en-v1.5 0.52 0.0098 0.53 0.025 ibm/slate-125m-english-rtrvr 0.53 0.015 0.53 0.02 intfloat/multilingual-e5-large 0.49 -0.024 0.46 -0.045 Chunk size 256 0.51 -0.0029 0.5 -0.0056 384 0.51 0.00066 0.51 0.0022 512 0.51 0.0023 0.51 0.0035 Chunk overlap 0.0 0.51 -0.0015 0.5 -0.0039 0.25 0.51 0.0015 0.51 0.0039 K 3 0.51 0.0014 0.5 -0.01 5 0.51 -0.00088 0.51 0.0015 10 0.51 -0.00054 0.52 0.0089 Table 18: Marginal means for the grid search results (ClapNQ). Columns denoted by∆show the relative difference from the overall dataset mean. LLMaaJ-AC LLMaaJ-AC (∆) Lexical-AC Lexical-AC (∆) Generative model ibm-granite/granite-3.1-8b-instruct 0.36 -0.081 0.84 0.084 meta-llama/llama-3-1-8b-instruct 0.49 0.05 0.62 -0.13 mistral-nemo-instruct 0.47 0.031 0.8 0.048 Embedding model BAAI/bge-large-en-v1.5 0.44 0.00058 0.75 -0.0012 ibm/slate-125m-english-rtrvr 0.43 -0.0066 0.75 -0.0061 intfloat/multilingual-e5-large 0.44 0.0061 0.76 0.0071 Chunk size 256 0.44 -0.000017 0.75 -0.00016 384 0.44 -0.000067 0.75 0.000091 512 0.44 0.00015 0.75 0.000012 Chunk overlap 0.0 0.44 0.000024 0.75 -0.000054 0.25 0.44 0.000018 0.75 0.000014 K 3 0.44 0.00085 0.74 -0.008 5 0.44 -0.0016 0.75 0.003 10 0.44 0.00086 0.76 0.005 Table 19: Marginal means for the grid search results (MiniWiki). Columns denoted by∆show the relative difference from the overall dataset mean. LLMaaJ-AC LLMaaJ-AC (∆) Lexical-AC Lexical-AC (∆) Generative model ibm-granite/granite-3.1-8b-instruct 0.59 -0.041 0.83 0.034 meta-llama/llama-3-1-8b-instruct 0.7 0.074 0.78 -0.018 mistral-nemo-instruct 0.6 -0.032 0.78 -0.016 Embedding model BAAI/bge-large-en-v1.5 0.63 0.00097 0.8 -0.00023 ibm/slate-125m-english-rtrvr 0.63 -0.0028 0.8 -0.0025 intfloat/multilingual-e5-large 0.63 0.0019 0.8 0.0028 Chunk size 256 0.62 -0.0089 0.79 -0.01 384 0.63 -0.005 0.79 -0.006 512 0.64 0.014 0.82 0.016 Chunk overlap 0.0 0.63 -0.00052 0.8 -0.00098 0.25 0.63 0.00053 0.8 0.00098 K 3 0.65 0.016 0.79 -0.014 5 0.62 -0.0055 0.8 0.003 10 0.62 -0.01 0.81 0.011 Table 20: Marginal means for the grid search results (WatsonxQA). Columns denoted by∆show the relative difference from the overall dataset mean.