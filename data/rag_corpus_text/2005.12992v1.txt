arXiv:2005.12992v1 [cs.IR] 21 May 2020 Evaluating Information Retrieval Systems for Kids Ashlee Milton ashleemilton@u.boisestate.edu PIReT – People and Information Research Team Boise State University, Boise, Idaho, USA Maria Soledad Pera solepera@boisestate.edu PIReT – People and Information Research Team Boise State University, Boise, Idaho, USA ABSTRACT Evaluation of information retrieval systems (IRS) is a prom inent topic among information retrieval researchers–mainly dir ected at a general population. Children require unique IRS and by ext en- sion diﬀerent ways to evaluate these systems, but as a large p op- ulation that use IRS have largely been ignored on the evaluat ion front. In this position paper, we explore many perspectives that must be considered when evaluating IRS; we specially discuss prob- lems faced by researchers who work with children IRS, includ ing lack of evaluation frameworks, limitations of data, and lack of user judgment understanding. CCS CONCEPTS • Social and professional topics → Children; • Information systems → Information retrieval ; Evaluation of retrieval results ; • Human-centered computing → User studies. KEYWORDS Information Retrieval, Evaluation, Children 1 THE ISSUE Evaluation in information retrieval (IR) remains a core research in- terest. Unlike most disciplines, evaluation of informatio n retrieval systems (IRS), including popular ones like recommender systems and search engines, is not limited to eﬀectiveness, as IR str ives to give users what makes them happy not necessarily what they asked for [3]. IR evaluation is an ongoing topic of interest a mong researchers and practitioners, with the focus being on metr ics and their applicability for diﬀerent tasks (e.g., multilingua l IR), diﬀer- ent domain (e.g., medical and legal), and applicability concepts like bias and fairness [4, 14, 16, 21, 22]. The target users of thes e explo- rations on evaluation have been a traditional user [3]. Thus , the frameworks and benchmarks used to evaluate IRS only account for a general user, who are not the only stakeholders. It is our stance that the lack of evaluation frameworks for ch il- dren’s IRS is a problem that needs to be addressed by not only researchers working with children but the IR community. Eva lu- ating IRS for children presents new challenges that have yet to be fully addressed. There have been a few evaluation frameworks presented in recent years that attempt to create structure t o en- able assessment for kids’ IRS [1, 15]. Unfortunately, they a re not always general enough to be applied to the varying IRS that ar e available for children. Even when these frameworks can be us ed, there is an overlying issue of data. Data from children is hard to get in several respects including collecting, accessing, a nd storing due to laws protecting children [6, 19, 20]. Even though some data "Copyright Âľ 2020 for this paper by its authors. Use permitt ed under Creative Com- mons License Attribution 4.0 International (CC BY 4.0). " may be accessible, how children judge IRS diﬀers from the general population [11–13, 24]. We aim at showcasing (i) issues that our- selves and other researchers and practitioners face with da ta, user judgments, and frameworks, as well as (ii) the fact that eval uation that involves protected users is an issue with bigger implic ations that requires attention and must be informed by diﬀerent are as of the IR community and beyond. 2 THE TROUBLE WITH FRAMEWORKS Existing frameworks have set out to make the best of the data t hat is available to evaluate children’s IRS. The one presented i n [15] brings context to evaluating and designing children’s sear ch sys- tems with their four pillars: search strategy, user group, e nviron- ment, and task. While originally designed for search system s, it is general enough to be used for context in other IR tasks [7, 8 ]. However, the proposed framework does not explicitly addres s the limitations that arise due to lack of data. In [1], the authors present a framework to address the issue of relevance judgement with chil- dren regarding search. This framework is task-speciﬁc and w ould need manipulation to work with other IR tasks and areas. A use - ful framework that lends itself to IRS evaluation and that do es not require data from children is the Cranﬁeld paradigm [5]. Cra nﬁeld uses known suitable resources as ground truth. However, it h as been dropped and deemed outdated when evaluating IRS for gen - eral populations in favor of more state-of-the-art alterna tives. The latter tend to require large amounts of data that is not always avail- able with children making them not practical. Works like [23], have defended the Cranﬁeld paradigm as a viable framework, but th e IR community is divided on this issue. In the end, existing frameworks are a helpful foundation but each has constraints that rende r them insuﬃcient for the IR community at large. 3 THE WOES OF DATA Due to privacy laws like the Children’s Online Privacy Prote ction Act, Family Educational Rights and Privacy Act, and General Data Protection Regulation [6, 19, 20], children’s data is highly protected. While these safeguards are of the upmost importance to keep c hil- dren safe online, it makes collecting or ﬁnding children’s d ata diﬃ- cult. Thus, unless researchers have a means of gathering theneeded data within the bounds of the child privacy laws themselves, it is impossible to get such data. Even then, there are additional rules as to what can be collected (e.g., demographics), and how it m ust be stored. These stringent rules (i) make it extremely burde nsome to share data and (ii) can lead to insuﬃcient data for evaluat ing IRS for children. 4 THE AMBIGUITY OF JUDGMENT Existing ground truth may be misleading–it is not always wha t it seems. Studies exploring children’s behavior as they inter act with IRS and evaluation strategies for kid-friendly IRS [11, 18] revealed that young users do not act in the same manners as adults do when interacting with or evaluating IRS [2, 11]. For example, kid s tend to click on the ﬁrst search result on a search result page rega rdless of its relevance [9, 10]. Naturally, the thought is to use the child’s clicked link as the relevant result but since they tend to fav or the ﬁrst result, regardless of relevance, does it work as ground truth? Even if you ask children what their judgments are, instead oftrying to infer them, you can still have problems. Consider when ask ing kids to rate on a standard 1 to 5 scale, studies have shown theytend to only rate 4’s or 5’s regardless of what they think [11]. Beh aviors like these make it hard to deﬁne what the ground truth of collected data is and how applicable it is for conducting oﬄine evaluat ions of IRS. 5 THE UNKNOWN The current state of evaluating IRS for children is in its inf ancy and it is indeed a complex undertaking driven by multiple per - spectives [17]. There is not general framework that can be us ed consistently and is accepted by the community as a whole; the re is no reliable and/or standard way to obtain data for evaluat ion; and ground truth requires a unique perspective of relevance and that is just not the case when it comes to IRS for children. The se is- sues showcase not only the importance of developing framewo rks without the need for massive amounts of data but also why in- volving the larger community to create it is key. The reason f or engaging with the IR community (and beyond) is two-fold. Fir st, if the community is involved, they will become aware of the issu es attached to the development and evaluation of IRS for childr en. Second, the researchers and practitioners can bring in thei r expe- riences on evaluation, especially from other areas working with protected populations. Working together we can learn from e ach other and hopefully come up with ways to facilitate the devel op- ment of evaluation in diﬀerent areas of study and bring the is sues of evaluation of IRS for kids into the spotlight. REFERENCES [1] Dania Bilal and Meredith Boehm. 2017. Towards new method ologies for assess- ing relevance of information retrieval from web search engines on childrenâĂŹs queries. Qualitative and Quantitative Methods in Libraries 2, 1 (2017), 93–100. [2] Dania Bilal and Joe Kirby. 2002. Diﬀerences and similari ties in information seek- ing: children and adults as Web users. Information processing & management 38, 5 (2002), 649–670. [3] Jamie Callan. 2020. Better Representation of Search Tas ks. Available at: https://www.youtube.com/watch?v=eHJTkFUxJgg&feature=youtu.be&t=18047. (2020). (accessed May 6). [4] Rocío Cañamares, Pablo Castells, and Alistair Moﬀat. 20 20. Oﬄine evaluation options for recommender systems. Information Retrieval Journal (2020), 1–24. [5] Cyril Cleverdon. 1967. The Cranﬁeld tests on index langu age devices. In Aslib proceedings. MCB UP Ltd. [6] Federal Trade Commission and others. 1998. ChildrenâĂŹ s online privacy pro- tection act of 1998. (1998). [7] Brody Downs, Oghenemaro Anuyah, Aprajita Shukla, Jerry Fails, Maria Soledad Pera, Katherine Landau Wright, and Casey Kennington. 2020a. KidSpell: A Child- Oriented, Rule-Based, Phonetic Spellchecker. In Proceedings of the The 11th In- ternational Conference on Language, Resources, and Evalua tion (LREC âĂŹ20) . [8] Brody Downs, Aprajita Shukla, Mikey Krentz, Maria Soled ad Pera, Casey Ken- nington, Jerry Fails, and Katherine Landau Wright. 2020b. Guiding the Selection of Child Spellchecker Suggestions using Audio and Visual Cu es. In Proceedings of the The 19th International Conference on Interaction Des ign and Children (IDC âĂŹ20) . Association for Computing Machinery, New York, NY, USA. [9] Sergio Duarte Torres, Djoerd Hiemstra, and Pavel Serdyu kov. 2010. Query Log Analysis in the Context of Information Retrieval for Childr en. In Special Interest Group on Information Retrieval . ACM, 847–848. [10] Jacek Gwizdka, Preben Hansen, Claudia Hauﬀ, Jiyin He, a nd Noriko Kando. 2016. Search as learning (SAL) workshop 2016. In 39th International SIGIR conference on Research and Development in Information Retrieval . ACM, 1249–1250. [11] Lynne Hall, Colette Hume, and Sarah Tazzyman. 2016. Fiv e Degrees of Happi- ness: Eﬀective Smiley Face Likert Scales for Evaluating wit h Children. In Pro- ceedings of the The 15th International Conference on Intera ction Design and Chil- dren (IDC âĂŹ16) . Association for Computing Machinery, New York, NY, USA, 311âĂŞ321. DOI:http://dx.doi.org/10.1145/2930674.2930719 [12] Theo Huibers and Thijs Westerveld. 2019. Relevance and utility in an educa- tional search environment. In 3rd International and Interdisciplinary Perspectives on Children & Recommender and Information Retrieval Systems, KidRec 2019: what does good look like? [13] Hanna Jochmann-Mannak, Leo Lentz, Theo Huibers, and Te d Sanders. 2016. How Interface Design and Search Strategy Inﬂuence Children ’s Search Perfor- mance and Evaluation. In Web Design and Development: Concepts, Methodologies, Tools, and Applications. IGI Global, 1332–1379. [14] Liadh Kelly, Lorraine Goeuriot, Hanna Suominen, Maria na Neves, Evangelos Kanoulas, Rene Spijker, Leif Azzopardi, Dan Li, João Palott i, Guido Zuccon, and others. 2019. CLEF ehealth 2019 evaluation lab. In European Conference on Infor- mation Retrieval. Springer, 267–274. [15] Monica Landoni, Davide Matteri, Emiliana Murgia, Theo Huibers, and Maria Soledad Pera. 2019. Sonny, Cerca! evaluating the impa ct of using a vocal assistant to search at school. In International Conference of the Cross-Language Evaluation Forum for European Languages . Springer, 101–113. [16] Rishabh Mehrotra, James McInerney, Hugues Bouchard, M ounia Lalmas, and Fernando Diaz. 2018. Towards a Fair Marketplace: Counterfa ctual Evalu- ation of the Trade-oﬀ between Relevance, Fairness & Satisfa ction in Rec- ommendation Systems. In Proceedings of the 27th ACM International Con- ference on Information and Knowledge Management (CIKM âĂŹ1 8). Associ- ation for Computing Machinery, New York, NY, USA, 2243âĂŞ22 51. DOI: http://dx.doi.org/10.1145/3269206.3272027 [17] Emiliana Murgia, Monica Landoni, Theo Huibers, Jerry A lan Fails, and Maria Soledad Pera. 2019. The Seven Layers of Complexity of R ecom- mender Systems for Children in Educational Contexts. In Workshop on Rec- ommendation in Complex Scenarios, co-located with ACM RecS ys. 5–9. DOI: http://dx.doi.org/Vol-2449/paper1.pdf [18] Maria Soledad Pera, Emiliana Murgia, Monica Landoni, a nd Theo Huibers. 2019. With a Little Help from My Friends: Use of Recommendations at School. (2019). [19] Family Educational Rights. 1974. Privacy Act or 1974. ( 1974). [20] Family Educational Rights. 2017. Privacy Act of 2017. ( 2017). [21] Pablo Sánchez, Rus M. Mesas, and Alejandro Bellogín. 20 18. New Approaches for Evaluation: Correctness and Freshness: Extended Abstr act. In Proceedings of the 5th Spanish Conference on Information Retrieval (CER I âĂŹ18) . Associa- tion for Computing Machinery, New York, NY, USA, Article 14, 2 pages. DOI: http://dx.doi.org/10.1145/3230599.3230614 [22] Daniel Valcarce, Alejandro Bellogín, Javier Parapar, and Pablo Castells. 2018. On the Robustness and Discriminative Power of Information Ret rieval Metrics for Top-N Recommendation. In Proceedings of the 12th ACM Conference on Recom- mender Systems (RecSys âĂŹ18) . Association for Computing Machinery, New York, NY, USA, 260âĂŞ268. DOI:http://dx.doi.org/10.1145/3240323.3240347 [23] Ellen M Voorhees. 2019. The evolution of cranﬁeld. In Information Retrieval Evaluation in a Changing World . Springer, 45–69. [24] SD Wentzel. 2019. Evaluating Information Retrieval Systems for Children in a n Educational Context. B.S. thesis. University of Twente.