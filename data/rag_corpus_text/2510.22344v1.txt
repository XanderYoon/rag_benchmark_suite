FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation Mohammad Aghajani Asl Sharif University of Technology m.aghajani@physics.sharif.edu Majid Asgari-Bidhendi Iran University of Science and Technology Noor Avaran Jelvehaye Maanaei Najm Co., Ltd. majid.asgari@gmail.com Behrooz Minaei-Bidgoli Iran University of Science and Technology b_minaei@iust.ac.ir Abstract While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge stale- ness in Large Language Models (LLMs), exist- ing frameworks often falter on complex, multi- hop queries that require synthesizing informa- tion from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, still lack a robust mechanism to sys- tematically identify and fill evidence gaps, of- ten propagating noise or failing to gather a com- prehensive context. In this paper, we introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At the core of FAIR-RAG is an Iterative Refine- ment Cycle governed by a novel module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mecha- nism: it deconstructs the initial query into a checklist of required findings and systemati- cally audits the aggregated evidence to iden- tify confirmed facts and, critically, explicit in- formational gaps. These identified gaps pro- vide a precise, actionable signal to an Adap- tive Query Refinement agent, which then gen- erates new, targeted sub-queries to retrieve the missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation step. We conduct exten- sive experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMul- tiHopQA, and MusiQue.Under a unified and controlled experimental setup,FAIR-RAG significantly outperforms strong representative baselines. On HotpotQA, it achieves an F1- score of 0.453—an absolute improvement of 8.3 points over the strongest iterative baseline— establishing a new state-of-the-art for this class of methods on these benchmarks.Our work demonstrates that a structured, evidence- driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks. 1 Introduction Large Language Models (LLMs) have demon- strated remarkable capabilities across a wide range of natural language processing tasks, including question-answering (QA) (Brown et al., 2020; Chowdhery et al., 2022). However, their knowl- edge is inherently static, confined to the data they were trained on, which leads to factual inaccura- cies and an inability to reason about events beyond their training cut-off date. Furthermore, LLMs are prone to “hallucination,” generating plausible yet factually incorrect information, which severely limits their reliability in knowledge-intensive ap- plications (Ji et al., 2023). To mitigate these is- sues, Retrieval-Augmented Generation (RAG) has emerged as a prominent paradigm. By ground- ing the generation process on information retrieved from external knowledge bases, RAG systems aim to produce more accurate, timely, and verifiable responses (Lewis et al., 2020). Despite its advantages, the standard “retrieve- then-read” RAG pipeline often falls short when faced with real-world user queries, which are fre- quently complex and cannot be answered through a single-shot retrieval step. For instance, a query such as“Which movie, directed by the same per- son who directed Inception, won an Oscar for arXiv:2510.22344v1 [cs.CL] 25 Oct 2025 Best Cinematography?”requires multi-hop rea- soning (Yang et al., 2018): first identifying the director ofInception(Christopher Nolan) and then searching for his movies that have won the spec- ified award. Standard RAG frameworks struggle with such multi-hop queries, as well as comparative or analytical questions that require synthesizing in- formation from multiple sources. Moreover, they are not robust to suboptimal user query formula- tions and often fail to enforce that the generated answer remains strictly faithful to the retrieved evi- dence, thus perpetuating the risk of hallucination. To address these limitations, several advanced RAG methodologies have been proposed.Itera- tive approaches, such as ITER-RETGEN (Shao et al., 2023), refine the retrieved information over multiple cycles by using the previously generated output as context for the next retrieval step.Adap- tive approaches, like Adaptive-RAG (Jeong et al., 2024), aim for efficiency by dynamically selecting the retrieval strategy (e.g., no retrieval, single-step, or multi-step) based on an initial assessment of the query’s complexity. Other frameworks like SELF- RAG (Asai et al., 2023) introduce self-reflection mechanisms, training the LLM to generate special tokens that control the retrieval and critique pro- cess on the fly. Nonetheless, a gap remains for a framework that synergistically combines iterative evidence refinement with adaptive query genera- tion and an explicit, modular faithfulness check. Existing iterative methods can propagate noise by using entire generations as queries, while adaptive methods may not sufficiently refine the evidence needed for complex queries after the initial routing. In this paper, we introduceFAIR-RAG, a novel framework forFaithful,Adaptive,Iterative Refinement inRetrieval-AugmentedGeneration. FAIR-RAG is designed to robustly handle complex queries by orchestrating a dynamic, multi-stage workflow. The framework begins with anAdaptive Routingmodule that analyzes query complexity to determine an optimal execution path, either by di- rectly answering simple queries or by allocating the appropriate computational resources for complex ones. For non-trivial queries, FAIR-RAG initiates a cyclical process designed to progressively build and validate a context. At the core of this cycle is anIterative Refinementloop where LLM agents intelligently decompose information needs, retrieve evidence, and filter out noise. Crucially, each cy- cle culminates in aStructured Evidence Assess- ment (SEA)module, which acts as an analytical gating mechanism. This module emulates a cog- nitive workflow by first deconstructing the user’s query into a checklist of required findings. It then systematically synthesizes the retrieved evidence against this checklist, verifying what is confirmed and, most importantly, explicitly identifying any “Remaining Gaps.”These identified gaps provide a precise, actionable signal for theAdaptive Query Refinementmodule, which then generates new, tar- geted queries specifically designed to retrieve the missing information. This evidence-centric loop continues until sufficiency is achieved, ensuring that the finalFaithful Answer Generationstep is strictly grounded in a comprehensive and ver- ified knowledge context, substantially enhancing trustworthiness and reducing hallucination. Our main contributions are as follows: • We introduce a novel agentic RAG architec- ture centered on anIterative Refinement loop. This evidence-centric cycle is governed by an analytical gating mechanism we term Structured Evidence Assessment (SEA). By systematically deconstructing the query and identifying specific information gaps, the SEA module intelligently guides subsequent itera- tions. This process progressively builds and validates a comprehensive context, enabling the system to robustly handle complex, multi- faceted, and multi-hop queries where single- pass retrieval would fail. • We design a sophisticated, two-stage query strategy. It begins withsemantic decomposi- tionto ensure all facets of the initial query are addressed, and more importantly, incorporates anAdaptive Query Refinementmechanism that analyzes evidence gaps to intelligently generate new queries, effectively reasoning about what information is still missing. • We implement an integrated approach to re- source optimization throughdynamic re- source allocation. Our framework employs an initialAdaptive Routingmechanism to bypass the RAG pipeline for simple queries and dynamically assigns LLMs of varying sizes to internal tasks based on their com- plexity, achieving a superior balance between response quality, latency, and computational cost. • We propose a robust, two-pronged approach to guarantee faithfulness. This includes: (1) a pre-generationStructured Evidence Assess- ment (SEA), which performs a final analyt- ical pass to verify that all required findings from the initial query deconstruction are fully supported by the aggregated evidence, and (2) aconstrained generation promptthat en- forces citation and prevents the model from introducing external knowledge. This combi- nation ensures the final answer is both verifi- able and trustworthy. We conducted extensive experiments on a suite of four challenging open-domain QA bench- marks to evaluate the FAIR-RAG framework, en- compassing complex multi-hop reasoning tasks (HotpotQA(Yang et al., 2018),2WikiMulti- HopQA,Musique) and a large-scale single-hop factual dataset (TriviaQA). Our best-performing configuration,FAIR-RAG 3 (Adaptive LLMs), was benchmarked against a wide range of strong baselines, including sequential (Standard RAG), conditional (Adaptive-RAG), and other state-of- the-art iterative methods (Iter-Retgen, Self-RAG). The results clearly demonstrate the superiority of our approach. On theHotpotQAbenchmark, our model sets a new state-of-the-art with an F1-score of0.453, surpassing the strongest iterative baseline, Iter-Retgen (0.370), by a significant margin of8.3 points. This pattern of superior performance is consistent across other complex benchmarks: on 2WikiMultiHopQA, FAIR-RAG achieves an F1 of0.320, outperforming the next best method,Self- RAG (0.251), by6.9 points, and onMusique, it scores an F1 of0.264, which is7.4 pointshigher thanIter-Retgen (0.190). Notably, FAIR-RAG’s architecture also excels on simpler factual queries, achieving a state-of-the- art F1 score of0.731onTriviaQA, showcasing its versatility. Our analysis further validates FAIR- RAG’s core principles: performance consistently improves as the number of refinement iterations increases from one to three (e.g., F1 on HotpotQA improves from 0.398 for FAIR-RAG 1 to 0.447 for FAIR-RAG 3), confirming the value of the iterative evidence-gathering loop. Furthermore, the adap- tive LLM allocation strategy provides an additional performance boost across all metrics. Finally, our framework consistently achieves the highest scores on the semantic metricACC LLM (e.g.,0.847on TriviaQA), confirming that its improvements reflect a deeper contextual understanding, not just lexical overlap. This robust performance validates the ef- fectiveness of our iterative, evidence-driven frame- work in enhancing both the accuracy and faithful- ness of LLM-based QA systems. 2 Related Work The paradigm of Retrieval-Augmented Genera- tion (RAG) has rapidly evolved from a straight- forward retrieve-then-read pipeline to more sophis- ticated, dynamic frameworks. Our work, FAIR- RAG, builds upon and extends several key research threads in this domain. 2.1 Standard Retrieval-Augmented Generation The foundational concept of RAG is to enhance the capabilities of LLMs by grounding them in external, non-parametric knowledge bases (Lewis et al., 2020). Early and influential RAG models typically employ a two-stage process: a retriever first fetches a set of relevant documents from a cor- pus based on the input query, and then a reader (or generator), which is often an LLM, synthe- sizes the final response conditioned on both the original query and the retrieved documents. This approach has proven effective in mitigating fac- tual inconsistencies (hallucinations) and providing answers based on up-to-date information, thereby overcoming the static knowledge limitations of LLMs (Huang et al., 2025; Guu et al., 2020). How- ever, this single-shot retrieval mechanism is primar- ily designed for single-hop queries where the an- swer can be found within a small set of initially re- trieved documents. Consequently, its performance degrades significantly on complex benchmarks like HotpotQA (Yang et al., 2018), which require multi- step reasoning or evidence aggregation from dis- parate sources. 2.2 Iterative and Multi-Step RAG To address the shortcomings of standard RAG, a significant body of research has focused on iterative and multi-step approaches. These methods trans- form the single-shot process into a dynamic, multi- turn interaction. One prominent strategy involves decomposing a complex question into simpler sub- queries. While frameworks likeSelf-Ask(Press et al., 2023) andSuRe(Kim et al., 2024) pioneer this decompositional approach, their strategies dif- fer fundamentally from ours.SuRegenerates a complete reasoning skeletonupfront—a static plan that cannot adapt to the evidence retrieved in inter- mediate steps.Self-Askgenerates sub-queries se- quentially, but each new query is largely informed by theintermediate answerto the previous one.In contrast, FAIR-RAG’s decomposition is neither static nor solely dependent on prior answers; it is a dynamic and context-aware process.After each iteration, our framework uses the Structured Evidence Assessment (SEA) module to perform a holistic analysis of theentirety of the retrieved evidence corpus. New sub-queries are then gener- ated specifically to target theexplicitly identified informational gaps. This gap-driven approach al- lows FAIR-RAG to adapt its evidence-gathering strategy in response to what has been found (and what is still missing), leading to a more robust and focused multi-step reasoning process than methods reliant on static plans or sequential, answer-driven prompting. Another popular approach interleaves reasoning steps with retrieval actions. This paradigm is ex- emplified by methods like ReAct (Yao et al., 2022) and IRCoT (Trivedi et al., 2023), which guide an LLM to generate explicit reasoning traces (e.g., Chain-of-Thought (Wei et al., 2022)), where each step can trigger a retrieval action to gather neces- sary information. The key distinction lies in the scope and triggerfor retrieval. In methods like ReAct and IRCoT, retrieval is typically alocal, step-wise actiontriggered by the immediate need of the next thought in a chain. In contrast, FAIR- RAG’s retrieval is driven by aholistic assessment of the entire evidence pool against the overarch- ing query requirements, allowing it to identify and address complex, non-sequential information gaps that step-wise reasoning might overlook. More directly related to our work, frameworks like ITER-RETGEN (Shao et al., 2023) propose a synergistic loop where the entire generated output from one iteration serves as the context to retrieve more relevant knowledge for the next. While pow- erful, using unstructured generation as a query can be suboptimal, as it may contain noise that misdi- rects the retriever. FAIR-RAG distinguishes itself by employing a more controlled mechanism: in- stead of using the entire previous output, it gener- ates new, targeted sub-queries based on an explicit analysis ofinformational gaps, leading to a more focused and efficient evidence-gathering process. Other iterative frameworks have introduced forward-looking or corrective mechanisms to enhance retrieval precision. For instance, FLARE (Jiang et al., 2023) employs a proactive strategy where the LLM anticipates future informa- tion needs during generation and triggers retrievals accordingly, effectively interleaving prediction and lookup steps. Similarly, Corrective RAG (Yan et al., 2024) incorporates a post-retrieval correc- tion phase, using an evaluator to assess and refine retrieved documents based on their relevance and factual consistency. While these methods improve upon standard iterative loops by adding predictive or corrective elements, they often rely on heuristic triggers or post-hoc adjustments, which can still overlook systematic evidence gaps in highly com- plex queries. In contrast, FAIR-RAG’s Structured Evidence Assessment (SEA) module provides a more principled, checklist-driven analysis that ex- plicitly identifies and targets informational defi- ciencies, enabling a targeted and iterative refine- ment without dependence on generation-time pre- dictions. 2.3 Adaptive and Faithfulness-Aware RAG A third stream of research focuses on making RAG systems more adaptive and reliable. Adaptivity is often geared towards computational efficiency. For instance,Adaptive-RAG(Jeong et al., 2024) intro- duces a classifier to pre-assess query complexity and route it to an appropriate strategy: no retrieval for simple questions, single-step retrieval for mod- erate ones, or a multi-step approach for complex queries. This routing is performed once at the be- ginning. In contrast, FAIR-RAG’s adaptivity is dynamic and occurswithinthe iterative process, as it continually adapts its query generation strategy based on the evolving set of retrieved evidence. Enhancing the faithfulness of the generated out- put is another critical concern. Standard RAG mod- els do not explicitly guarantee that the generator will adhere to the retrieved context. (Es et al., 2025) To address this, SELF-RAG (Asai et al., 2023) fine- tunes an LLM to generate special “reflection to- kens”, enabling it to critique its own output for relevance and factual support in an inline fashion during generation. While effective, this approach has two limitations: its reliance on fine-tuning re- stricts applicability to off-the-shelf models, and its evaluation is inherentlytactical, assessing evi- dence on a step-by-step basis as the answer is being composed. FAIR-RAG addresses the faithfulness challenge differently, through an explicit, modular, and more strategicStructured Evidence Assessment (SEA) module. Instead of an inline critique, SEA acts as a distinct analytical gating mechanismbefore final answer generation. It first deconstructs the user’s query into a checklist of required findings. It then performs a holistic audit of theentireevi- dence corpus against this question-centric check- list to identify confirmed facts and explicit “intel- ligence gaps.” This distinction is crucial: whereas SELF-RAG prompts the model to ask, “Is this next piece of evidence useful for my current generation step?”, SEA forces the model to first ask, “Is the entiretyof my evidence sufficient to address all facets of the user’s original query?” The identi- fied gaps from this strategic assessment provide a precise, actionable signal for the subsequent query refinement step, transforming the check from a pas- sive validation into an active steering mechanism. Crucially, unlike fine-tuning-based methods, our modular SEA requires no model training, allowing for greater flexibility and easier integration with various off-the-shelf language models. In summary, while existing works have made substantial advancements, FAIR-RAG provides a novel contribution by synergistically integrating three core principles into a cohesive framework: (1) a structured, gap-aware iterative refinement loop, (2) context-aware, adaptive sub-query generation, and (3) an explicit, modular faithfulness assessment that requires no model fine-tuning. 3 Methodology The FAIR-RAG framework is designed as a multi- stage, iterative process that dynamically adapts its strategy to the complexity of a user’s query. Our architecture transforms the standard, static RAG pipeline into an intelligent, evidence-driven work- flow that progressively builds context to answer complex questions. The entire process, illustrated in Figure 1, can be divided into four main phases: (1) Initial Query Analysis and Adaptive Routing, (2) The Iterative Retrieval and Refinement Cycle, (3) Faithful Answer Generation, and (4) Dynamic Resource Allocation. The entire inference proce- dure is detailed step-by-step in Algorithm 1. The process is initiated by theAdaptive Routingagent (Arouter), which classifies the query and selects the most appropriate generator LLM ( Gselected) from the predefined set of available models (G). 3.1 Overall Architecture The overall architecture of FAIR-RAG, illustrated in Figure 1, employs a dynamic, multi-step pro- cess to handle user queries. An initial routing step assesses query complexity. Simple queries are an- swered directly, while complex ones trigger an it- erative refinement loop. This core loop consists of adaptive query generation, hybrid retrieval, filter- ing, and a Structured Evidence Assessment (SEA). The SEA module determines if the collected evi- dence is sufficient. If not, the loop repeats with refined queries targeting information gaps. Once sufficiency is met, a final, evidence-grounded an- swer is generated. The entire inference procedure is detailed step-by-step in Algorithm 1. All LLM agents in our pipeline are guided by meticulously engineered prompts. The full details and examples for each prompt are provided in Appendix B. 3.2 Initial Query Analysis and Adaptive Routing The first stage of our framework is a lightweight yet crucial analysis of the input query. An LLM agent classifies the query into one of four categories to determine the subsequent workflow and resource allocation: • OBVIOUS:For queries whose answers are likely stable and contained within the LLM’s parametric knowledge (e.g., “What is the cap- ital of France?”). These queries are routed directly to a large LLM for generation, by- passing the entire RAG pipeline for maximum efficiency. • SMALL:Simple factual queries that require retrieval but minimal reasoning. A smaller, more efficient LLM is designated for the final generation step. • LARGE:Queries that require retrieval and synthesis of information from multiple sources. A larger, more capable LLM is se- lected for generation. • REASONING:Complex queries that demand multi-hop reasoning, comparison, or deep analysis. A state-of-the-art LLM with strong reasoning capabilities is allocated for the final answer. This initial routing mechanism serves a dual purpose: it acts as an efficiency-enhancing short- Algorithm 1FAIR-RAG Inference (Prompts for agents are detailed in Appendix B.) Require: Set of Generator LLMs G={G small, Glarge, Greasoning}, Hybrid Retriever Hretriever, Document Corpus C= {d1, . . . , dN }, Set of LLM AgentsA={A router, Adecompose, Afilter, Asuff, Arefine} 1:Input:User queryx 2:Output:Final, evidence-grounded answery final 3: query_type, G selected ←A router.classify(x)▷Route 4:ifquery_type==OBVIOUSthen 5:y final ←G selected.generate(x)▷Generate directly from parametric knowledge 6:returny final 7:end if 8:E agg ← ∅ // Aggregated evidence 9:Q previous ← {x} 10:fori←1to3do // Max 3 iterations 11:ifi== 1then 12:Q sub ←A decompose.generate(x)▷Decompose initial query 13:else 14:Q sub ←A refine.generate(x, Qprevious,analysis_summary)▷Refine query to fill gaps 15:end if 16:Q previous ←Q previous ∪Q sub 17:D candidate ←H retriever.retrieve_and_rerank(Qsub,C)▷Retrieve 18:E filtered ←A filter.filter(Dcandidate, x)▷Filter irrelevant evidence 19:E agg ←E agg ∪E filtered 20: analysis_summary,is_sufficient←A suff.check(Eagg, x)▷Assess sufficiency with SEA 21:ifis_sufficient==Yesthen 22:break// Exit loop if evidence is sufficient 23:end if 24:end for 25:y final ←G selected.generate(x, Eagg,faith_constraints)▷Generate answer 26:returny final cut for simple queries and implements ourAdap- tive Model Selectionstrategy by pre-allocating the most cost-effective generator model for the final step. While this module is a key feature for prac- tical deployment, it was systematically controlled during our benchmark evaluations to ensure a fair comparison, as detailed in Section 4.4. 3.3 The Iterative Retrieval and Refinement Cycle This cycle is the core of FAIR-RAG’s ability to handle complex information needs. It is designed to run for a maximum of three iterations to ensure a balance between comprehensiveness and latency. Each iteration consists of the following steps: 3.3.1 Adaptive Query Generation The initial user query, if deemed complex, is first subjected to semantic decomposition. An LLM agent breaks down the multifaceted query into a set of up to four distinct, keyword-rich, and seman- tically independent sub-queries. For example, the query“What were Alan Turing’s main contribu- tions to computer science and his role in World War II?”is decomposed into targeted sub-queries like“Alan Turing’s contributions to theoretical computer science”and“Alan Turing’s role in breaking the Enigma code.”This ensures that the retrieval process covers all conceptual facets of the original question. 3.3.2 Hybrid Retrieval and Reranking For each sub-query, we employ a hybrid retrieval strategy to maximize recall. We perform both dense vector search (capturing semantic simi- larity) (Karpukhin et al., 2020) and traditional keyword-based sparse search (capturing exact matches) (Robertson and Zaragoza, 2009). The results from both methods are aggregated, and the documents are re-ranked using theRecipro- cal Rank Fusion (RRF)algorithm (Cormack et al., 2009). RRF effectively combines the rankings from both retrieval methods without requiring hyperpa- rameter tuning, producing a single, robustly ranked list of the top-5 most relevant documents as candi- date evidence. 3.3.3 Evidence Filtering The candidate evidence is then passed to a filtering module. An LLM agent evaluates each document’s utility with respect to theoriginaluser query. Doc- uments that are irrelevant, off-topic, or only tangen- tially related are discarded (Liu et al., 2023a). This step is critical for increasing the signal-to-noise ratio of the context provided to the final genera- tor, preventing the model from being distracted by Figure 1: Schematic overview of the FAIR-RAG architecture. The process starts with initial query analysis and adaptive language model selection (e.g., small, large, or reasoning LLM). For complex queries, it proceeds to query decomposition, followed by an iterative refinement cycle involving hybrid retrieval and reranking, evidence filtering, and Structured Evidence Assessment (SEA). The loop iterates until evidence sufficiency is confirmed, culminating in faithful answer generation grounded in the aggregated evidence. noisy or unhelpful information. 3.3.4 Structured Evidence Assessment (SEA): The Strategic Intelligence Analyst The strategic core of our iterative refinement loop is the Structured Evidence Assessment (SEA) mod- ule. Its primary function is not merely to verify evidence, but to perform a granulargap analysis that generates an explicit, actionable signal for the next iteration. For this critical task, we deliber- ately chose achecklist-based methodologyover alternatives like abstractive summarization or di- rect question-answering. Abstractive summariza- tion, by design, often conceals the very gaps we need to identify by creating a fluent narrative, while direct QA yields binary outcomes that fail to pin- point the precise location of missing information in a multi-fact query. Our approach operationalizes this checklist via an LLM agent prompted to act as aStrategic In- telligence Analyst. Through carefully engineered prompts—defining its role and providing few-shot examples (see Appendix B)—the agent firstde- constructsthe user’s query into a checklist of dis- crete, required informational components or “find- ings.” It then systematicallyauditsthe collected evidence against this checklist, confirming which findings are supported and identifying which re- main as explicit“intelligence gaps.”The evidence is deemed sufficient only if all required findings are confirmed. The ‘unchecked’ items on this list constitute adi- rect, interpretable, and actionable signalfor the subsequent Query Refinement module. This struc- tured, question-centric process provides a more controllable, reliable, and transparent mecha- nismfor gap analysis than less constrained meth- ods, ensuring a rigorous evaluation that prevents the system from being misled by large volumes of tangentially related information. 3.3.5 Iterative Query Refinement If the Structured Evidence Assessment (SEA) re- sults that the evidence is insufficient to correctly an- swer the question, the system activates the query re- finement module, which is designed to be a highly targeted intervention. An LLM agent uses the “Re- maining Gaps” and “Confirmed Findings” from the analyst’s summary (generated in the previous step) as its primary input. Its goal is to generate new, laser-focused queries that are engineered to findonlythe missing pieces of information. By leveraging the confirmed findings, the agent makes the new queries more precise and avoids repeating previous searches. For instance, if a query is“In what city was the lead scientist who broke the Enigma code buried?”and the analyst’s summary confirms“Alan Turing was the lead scientist”but identifies“Alan Turing’s burial place”as a gap, the refinement module will generate new, highly targeted queries like“Alan Turing burial place” or“city where Alan Turing is buried.”These new queries, now contextualized and specific, re-enter the hybrid retrieval cycle (Step 3.3.2). 3.4 Faithful Answer Generation Once the Structured Evidence Assessment (SEA) returns “Yes,” the curated and validated evidence set is passed to the generator LLM selected in the initial routing stage (Step 3.2). The generation process is governed by a meticulously engineered prompt that enforces astrict, evidence-only gen- eration protocol. This constrained prompt is de- signed to maximize faithfulness and minimize the risk of hallucination by instructing the LLM to: • Base its answerexclusivelyon the provided evidence. • Avoid introducing any external information, parametric knowledge, or opinions. • Cite every claim by embedding reference to- kens (e.g., [1], [2]) that link to the source doc- uments. • If the evidence is ultimately insufficient, state this directly without attempting to speculate. This highly constrained generation process is fundamental to mitigating hallucination and en- sures that the final output is not only accurate but also transparent and fully traceable to its sources. 3.5 Dynamic Resource Allocation and Prompt Engineering A key aspect of FAIR-RAG’s design is the efficient use of computational resources. We dynamically allocate LLMs of different sizes (e.g., SMALL, LARGE) for the various internal tasks based on their complexity. For instance, simpler tasks like theInitial Query Analysis and Adaptive Routing are handled by a smaller model, while the highly nuanced task ofquery refinementis assigned to a more capable model to ensure maximum precision. This dynamic allocation optimizes the trade-off between performance, cost, and latency. Furthermore, all interactions with LLM agents are managed through astructured prompt engi- neering methodologydesigned to ensure reliabil- ity and predictability [see Appendix B for details]. Our prompts are consistently engineered to include several key components that guide the model’s be- havior: • Role and Context Definition:Each prompt begins by establishing a clear role for the agent and the context of its task (e.g.,“You are a Strategic Intelligence Analyst. . . ”). • Task Specification:The primary goal or in- tent of the task is explicitly stated (e.g.,“Your mission is to determine if the provided evi- dence is sufficient. . . ”). • Guided Reasoning (Scaffolding):We pro- vide clear, step-by-step instructions, analytical guidelines, or few-shot examples to structure the model’s reasoning process and ensure con- sistency. • Behavioral Constraints:Explicit rules are laid out that the model must follow, govern- ing its process and output (e.g.,“You MUST follow this thinking process and output format exactly”). • Output Formatting:The exact format for the response is strictly defined to ensure a machine-parseable and predictable output (e.g.,“A single word: ‘Yes’ or ‘No”’). This robust, component-based prompting strat- egy ensures that the LLM agents perform their des- ignated roles reliably, contributing to the overall stability and performance of the framework (Liu et al., 2021). 4 Experimental Setup To rigorously evaluate the performance of FAIR- RAG, we conduct a series of experiments on chal- lenging question-answering benchmarks. Our ex- perimental protocol is built upon the standardized and open-sourceFlashRAGtoolkit, ensuring a fair and reproducible comparison against existing meth- ods. 4.1 Datasets We selected a carefully curated suite of four bench- mark datasets to assess the various capabilities of our framework, with a particular focus on complex, multi-hop reasoning where standard RAG systems often underperform. • Multi-hop QA:We useHotpotQA,2Wiki- MultihopQA, andMusiQue. These datasets are specifically designed to require reason- ing and synthesizing information across multi- ple documents to arrive at an answer, making them ideal for evaluating our iterative refine- ment and adaptive query generation modules. • Open-Domain QA:We also useTriviaQA, a popular dataset for open-domain question an- swering, to ensure our model maintains strong performance on simpler, fact-based queries. For all experiments, we follow the standard prac- tice of using the official test split where available; otherwise, we report results on the development split. Consistent with recent studies on computa- tionally intensive RAG models, and to manage the substantial API and computational costs associated with iterative inference frameworks like ours, all evaluations are conducted on a randomly selected subset of 1000 samples from each dataset.This sample size was deliberately chosen to strike a critical balance between statistical robustness and experimental feasibility.It is large enough to ensure stable and meaningful performance com- parisons while enabling the comprehensive suite of ablation studies and baseline comparisons pre- sented in this work, which would be financially and logistically intractable on the full datasets. 4.2 Baselines We compare FAIR-RAG against a comprehensive set of representative RAG baselines implemented within the FlashRAG framework. These baselines cover different architectural paradigms: Dataset Task Type Source Samples HotpotQA multi-hop QA wiki 1000 2WikiMultiHopQA multi-hop QA wiki 1000 MusiQue multi-hop QA wiki 1000 TriviaQA Open-Domain wiki/web 1000 Table 1: Summary of Datasets • Standard RAG(Lewis et al., 2020): A con- ventional retrieve-then-read pipeline serving as a fundamental baseline. • Iterative Methods:We includeIter-Retgen (ITRG)(Shao et al., 2023), which uses the previous generation’s output to retrieve new documents, andIRCoT(Trivedi et al., 2023), which integrates retrieval within a Chain-of- Thought process. • Reasoning-based Methods:We compare againstReAct(Yao et al., 2022), a popular agent-based framework that interleaves rea- soning and action steps to solve problems. • Faithfulness-focused Methods: Self- RAG(Asai et al., 2023) is included as a strong baseline that incorporates explicit reflection and self-critique steps to improve faithfulness. •Branching & Conditional Methods:We in- cludeSuRe(Kim et al., 2024), which gen- erates and ranks multiple candidate answers, andAdaptive-RAG(Jeong et al., 2024), which uses a classifier to conditionally route queries through different execution paths. 4.3 Evaluation Metrics To provide a comprehensive assessment of our sys- tem’s performance, we employ a suite of metrics that capture both lexical accuracy and semantic correctness. 4.3.1 Lexical-Based Metrics Following standard practice for question-answering tasks, we first report two traditional, token-based metrics: • Exact Match (EM):This strict metric mea- sures the percentage of predictions that match one of the ground-truth answers exactly, char- acter for character. • F1 Score:A more lenient metric that com- putes the harmonic mean of precision and re- call at the token level. It accounts for partial overlaps and is less sensitive to minor phras- ing differences than EM. 4.3.2 Automated Evaluation using LLM-as-Judge Lexical metrics like EM and F1 are often insuf- ficient for evaluating generative models, as they unfairly penalize semantically correct answers that are phrased differently or contain additional, rele- vant context not present in the ground truth. To overcome this limitation, we employ LLM-as- Judge methodologies for two distinct, nuanced eval- uation purposes, strategically selecting the judge model based on task complexity. • End-to-End Semantic Correctness (ACCLLM):For the large-scale evaluation of our main results (Table 2), we introduce LLM-as-Judge Accuracy (ACCLLM). This metric requires a scalable and consistent binary judgment on whether a final prediction is semantically equivalent to any ground-truth answer. For this task, we use the highly capable“Meta-Llama-3-8B-Instruct” model as the judge. (AI@Meta, 2024) The specific prompt, designed for efficient “Yes” or “No” classification, is detailed in Appendix C.1 (Chiang et al., 2023; Zheng et al., 2023). • Component-Level Quality Score:Given the nuanced, generative nature of interme- diate outputs in our ablation study (e.g., Query Decomposition), a simple binary met- ric is insufficient. Therefore, we employ an LLM-as-Judge methodology (Zheng et al., 2023), building on established frameworks like G-Eval (Liu et al., 2023b) to ensure a scalable and consistent assessment. For this role, we selected Llama-4-Maverick-17B- 128E-Instruct-FP8. (AI@Meta, 2025) This highly capable model was specifically cho- sen not merely for its general performance, but for its demonstrated aptitude in complex reasoning and nuanced instruction following. These capabilities are critical for accurately assessing the quality of our internal compo- nents, where evaluation criteria are intricate and context-dependent. The reliability of this model as a proxy for human judgment in our specific tasks is not an unsubstantiated claim; as we will demonstrate in our validation study (Section 5.2.1), our LLM-as-Judge’s ratings show a strong correlation with those of hu- man experts, confirming its suitability for this evaluation. To ensure high-quality, structured feedback and mitigate potential biases, we de- signed custom prompts for each component. These prompts provide the judge with clear task definitions, explicit scoring criteria on a 1-to-5 scale, and illustrative examples. This rigorous, prompt-driven approach, detailed in Appendix C.2, ensures consistent and in- terpretable scores for a credible and multi- faceted analysis of our pipeline’s internal me- chanics. 4.3.3 Reliability of LLM-as-Judge Evaluations To establish the credibility of our dual LLM-as- Judge framework, we conducted two separate hu- man verification studies, one for each evaluation type. • Verification of Binary Semantic Correct- ness (ACCLLM):The reliability of the Llama- 3-8B-Instruct judge, used for the ACC LLM metric, was rigorously validated. A random subset of300question-answer pairs was sam- pled, stratified across all datasets. A human expert,blinded to the LLM’s original deci- sion to prevent bias, annotated these pairs for semantic correctness. The human judgments showed astrong degree of concordancewith the LLM-as-Judge’s binary “Yes/No” outputs, aligning in90%of the cases. This level of agreement is well within the range of typical human inter-annotator agreement for such a nuanced task. Therefore, we conclude that the LLM-as-Judge serves as a reliable and scalable proxy for human evaluation in our experiments. • Verification of Component-Level Quality Scores:For the more nuanced 1-to-5 scale scoring performed by the powerful Llama-4- Maverick model in our ablation study, a simi- lar validation was conducted. A separate ran- dom subset of 100 generated outputs from the component-level tasks (e.g., query refine- ment) was evaluated by a human expert, again blinded to the LLM’s score. The evaluation demonstrated a95%agreement between hu- man and LLM judgments across these differ- ent tasks. This confirms the judge’s capability for providing consistent, human-aligned qual- ity assessments. This dual validation confirms that our LLM-as- Judge methodologies, for both binary correctness and fine-grained quality scoring, serve as robust and reliable proxies for human evaluation, enabling a credible and scalable assessment of our frame- work’s performance. 4.4 Implementation Details To ensure a controlled and fair comparison, all ex- periments adhere to the global settings defined by the FlashRAG framework, with specific adapta- tions for our models. Methodological Alignment for Fair Compari- son:A cornerstone of our evaluation is the prin- ciple of fair comparison, ensuring that reported performance gains are attributable to our core ar- chitectural innovations (i.e., the iterative refinement cycle) rather than peripheral components. Our full proposed architecture includes features designed for optimal real-world efficiency, such as a hybrid retriever (Section 3.3.2) and an “OBVIOUS” query shortcut (Section 3.2). However, as these features are not standard in the baseline methods we com- pare against, they could introduce an unfair advan- tage. Therefore, to create a level playing field,these two capabilities were systematically disabled during all benchmark experiments for all meth- ods, including our FAIR-RAG variants.Specifi- cally: • All models used a standardizeddense-only retriever. • The “OBVIOUS” query routing was deacti- vated, forcingevery query to pass through the full RAG pipeline. This rigorous alignment ensures that the ob- served performance differences are a direct result of the models’ reasoning and evidence-handling capabilities. Component Configuration: • Retriever:For all methods, we usee5-base- v2(Wang et al., 2024) as the sole dense re- triever, configured to fetch the top 5 doc- uments per query from the standard DPR Wikipedia (Dec. 2018) (Karpukhin et al., 2020) corpus. The index is built using Faiss (Flat type) (Johnson et al., 2019) to ensure accuracy. • Generator Models:To benchmark against a powerful and widely accessible model, we standardize the generator for all baseline meth- ods to be “Llama-3-8B-Instruct”, accessed via API. A key exception is Self-RAG, for which we use its officially released, fine-tuned selfrag-llama-7b model to respect its original design. • FAIR-RAG Configuration:Our experimen- tal setup for FAIR-RAG is designed to ensure a rigorous and fair comparison. We evaluate two primary configurations of our framework: – Uniform Model Configuration:To iso- late the architectural benefits of our iter- ative approach, the FAIR-RAG 1-4 vari- ants exclusively use Llama-3-8B-Instruct for all internal tasks and for the final an- swer generation. This ensures a direct and fair comparison against all baseline methods, which are also benchmarked us- ing the same Llama-3-8B-Instruct model. In this configuration, the adaptive LLM selection (SMALL, LARGE, REASON- ING roles) is intentionally disabled, with all roles defaulting to the single model. – Adaptive LLM Configuration:To demonstrate the full potential of our framework, we also report results for FAIR-RAG (Adaptive LLMs). This con- figuration employs a dynamic, multi- agent allocation strategy to optimize the trade-off between performance and cost: * For less complex internal tasks, such as query decomposition and Struc- tured Evidence Assessment (SEA), we utilize Llama-3-8B-Instruct. * For more cognitively demanding tasks, including evidence filter- ing, query refinement, and faith- ful answer generation, we leverage the more powerful Llama-3.1-70B- Instruct. (AI@Meta, 2024) * For tasks requiring deep reasoning, the system routes to a specialized DeepSeek-R1 model. (DeepSeek-AI, 2025) Unless otherwise specified, all other hyperpa- rameters adhere to the default settings of the un- derlying framework to maintain consistency across experiments. 5 Results This section presents a comprehensive evaluation of FAIR-RAG. We first report the main end-to-end results against a suite of strong baseline methods (Section 5.1). We then provide a deeper analysis of the framework’s internal mechanics, including a component-wise ablation study (Section 5.2.1) and an examination of the impact of iterative refinement on answer quality and cost (Section 5.2.2). 5.1 Main Results Table 2 presents the main performance comparison of FAIR-RAG and baseline methods across four di- verse question-answering benchmarks. Our frame- work demonstrates state-of-the-art performance, particularly on datasets that require complex, multi- step reasoning. We report four key metrics:Exact Match (EM)andF1-Scorefor lexical accuracy, a script-basedAccuracy (ACC)which measures the presence of the ground-truth answer string in the generation, and our primary semantic metric, LLM-as-Judge Accuracy (ACCLLM). While EM, F1, and ACC are token-based,ACC LLM evaluates semantic equivalence, offering a more robust as- sessment of generative answers. To provide a fine-grained analysis of our frame- work’s iterative capabilities, we evaluate several configurations of FAIR-RAG in Table 2. The FAIR- RAG 1 to 4 variants correspond to the system’s performance with the maximum number of itera- tions capped at 1 to 4, respectively. To ensure a fair comparison against the baselines and to isolate the impact of the iterative refinement cycle itself, these variants utilize a single, consistent genera- tor model (Llama-3-8B-Instruct) across all stages. Conversely, the (Adaptive LLMs) variants repre- sent our full, optimized framework, employing the dynamic allocation of different LLM agents (e.g., small, large, reasoner) based on task complexity, as detailed in our methodology. Our best-performing model,FAIR-RAG 3, demonstrates leading performance within the paradigm of iterative and adaptive RAG. FAIR- RAG significantly outperforms strong representa- tive baselines from its architectural class across all multi-hop benchmarks.It is critical to note that all comparisons were conducted under a unified and controlled experimental setup, ensuring a fair and reproducible evaluation.Within this rig- orous framework, our results not only demonstrate a consistent advantage in our direct head-to-head comparisons but alsoset a new state-of-the-art performance benchmark for this class of itera- tive methods on these datasets.While we refer- ence previously published results from comparable architectures (Shao et al., 2023; Asai et al., 2023; Jeong et al., 2024), our primary claim of superiority is grounded in re-evaluating these methods under our standardized conditions to eliminate confound- ing variables. The most substantial gains are observed on the multi-hop reasoning benchmarks. OnHotpotQA, our model achieves an F1 score of0.453, an abso- lute improvement of+8.3 pointsover the strongest baseline, Iter-Retgen (0.370). Similarly, on2Wiki- MultiHopQAandMusique, FAIR-RAG achieves F1 scores of0.320and0.264, respectively, outper- forming the next-best method (Self-RAG) by a sig- nificant margin. These results strongly validate the efficacy of our core architectural contributions: the Iterative Refinement Cycleand theStructured Evidence Assessment (SEA). These mechanisms empower FAIR-RAG to systematically deconstruct complex information needs, gather comprehensive evidence, and verify its sufficiency where single- pass or less structured iterative methods fall short. Notably, FAIR-RAG also excels on the single- hop factual benchmark,TriviaQA, achieving an F1 score of0.731. This demonstrates that the frame- work’s sophisticated reasoning machinery does not impose a penalty on simpler retrieval tasks and can effectively streamline its process, largely due to the initialAdaptive Routingmodule. Comparing different variants of our model, we see a consistent performance increase from FAIR- RAG 1 to 4, indicating that the incremental en- hancements contribute positively. The introduction ofAdaptive LLMsfurther boosts performance across the board, confirming the benefits of dynam- ically allocating computational resources based on task complexity. To complement the tabular data, Figure 2 pro- vides a visual comparison, plotting the F1 scores (bars) against the ACC LLM metric (lines) for all methods across the four benchmarks. The figure visually corroborates the superior performance of our FAIR-RAG framework, where its variants (in- Type Method HotpotQA 2WikiMultiHopQA Musique TriviaQA EM F1 ACC ACC LLM EM F1 ACC ACC LLM EM F1 ACC ACC LLM EM F1 ACC ACC LLM Sequential Standard RAG .238 .342 .328 .598 .086 .180 .282 .369 .074 .149 .137 .380 .570 .667 .675 .795 Branching Sure .232 .346 .293 .646 .134 .198 .167 .408 .101 .169 .114 .397 .566 .675 .641 .799 Conditional Adaptive-RAG .144 .239 .368 .628 .041 .133 .340 .429 .038 .095 .160 .366 .480 .583 .670 .789 Reasoning ReAct .000 .028 .096 .503 .000 .062 .295 .467 .000 .016 .018 .376 .000 .046 .129 .667 Iterative Iter-Retgen .265 .370 .353 .621 .104 .209 .310 .401 .115 .190 .178 .391 .581 .676 .689 .804 Self-RAG .174 .299 .321 .626 .123 .251 .333 .466 .073 .162 .132 .392 .385 .540 .639 .774 IRCoT .006 .087 .399 .631 .001 .085.433 .488.001 .056 .210 .416 .016 .169 .674 .800 Iterative FAIR-RAG 1 .300 .398 .337 .628 .186 .288 .286 .414 .133 .216 .169 .418 .622 .706 .682 .821 FAIR-RAG 2 .335 .447 .397 .689 .216 .325 .341 .458 .168 .253 .214 .465 .645 .732 .712 .837 FAIR-RAG 3 .332 .447 .404 .697 .183 .305 .338 .450.178 .267 .221 .474 .631 .721 .701 .839 FAIR-RAG 4.344 .456 .401 .697 .209 .333 .357 .486 .175 .266 .228 .475 .644 .728 .710 .837 FAIR-RAG 2 (Adpt.) .331 .436 .384 .673 .183 .296 .313 .444 .158 .241 .207 .447 .640 .722 .704 .828 FAIR-RAG 3 (Adpt.) .338 .453 .399 .694 .206 .320 .350 .452.178 .264 .222 .472 .645 .731 .710 .847 Table 2: Main end-to-end performance comparison of FAIR-RAG against representative baselines across four diverse question-answering benchmarks. The evaluation covers three complex multi-hop datasets (HotpotQA, 2WikiMultiHopQA, Musique) and one open-domain factual dataset (TriviaQA). We report four metrics: Exact Match (EM), F1-Score, script-based Accuracy (ACC), and LLM-as-Judge Accuracy (ACCLLM). Our FAIR-RAG variants consistently achieve state-of-the-art performance, with the most significant improvements on the multi-hop tasks. The best-performing method for each metric is highlighted inbold. Underlined values denote results surpassing the Self-RAG and Iter-Retgen baselines. All scores are based on 1000 test/dev samples. dicated by the patterned bars) consistently achieve the highest F1 scores, particularly on the com- plex reasoning datasets of HotpotQA, 2WikiMulti- HopQA, and Musique. A key trend highlighted by the plots is thestrong positive correlation between the F1 score and the ACCLLM. This suggests that the architectural improvements within FAIR-RAG, which enhance the accuracy of the Large Language Model’s inter- nal processing and decision-making, are directly responsible for the enhanced final answer accu- racy. This relationship is particularly evident in the HotpotQA and 2WikiMultiHopQA results, where a noticeable uplift in the ACC LLM line for FAIR- RAG variants coincides with a significant increase in their corresponding F1 scores compared to the baselines. Thus, the visualizations not only con- firm FAIR-RAG’s state-of-the-art performance but also offer insight into the synergistic relationship between its internal reasoning accuracy and its final output quality. 5.2 Further Analysis Beyond the main end-to-end results, we conduct a series of deeper analyses to deconstruct the sources of FAIR-RAG’s performance. We perform a de- tailed component-wise evaluation to understand the contribution of each module, analyze the spe- cific impact of the iterative refinement process, and present a qualitative case study to illustrate the framework in action. For these fine-grained assess- ments, we employ a sophisticatedLLM-as-Judge methodology, using a powerful LLM to score the output of each internal module against a set of pre- defined criteria (Zheng et al., 2023). This task is guided by a structured prompt (see Appendix C for details). 5.2.1 Component-wise Performance Analysis (Ablation Study) To quantify the contribution of each key module within the FAIR-RAG pipeline, we conducted an ablation study using an LLM-as-Judge method- ology on 1000 samples from each dataset. The results, summarized in Table 3, demonstrate the high functionality of each component, justifying its inclusion in the final architecture. For com- ponents likeQuery DecompositionandQuery Refinement, the judge rated the output quality on a 1-to-5 Likert scale. The rating was based on a holis- tic assessment of criteria includingrelevance(how well it addresses the core need),specificity(how focused the query is), andcoverage(whether it cap- tures all necessary facets of the information gap). The average score across 1000 samples is reported. ForEvidence Filtering, we report F1-Score based on the LLM’s ability to correctly classify docu- ments as relevant or irrelevant. ForSEA, we report Accuracy based on its correctness in judging evi- dence sufficiency. The analysis reveals several key insights: • Query Decomposition & Refinement are Highly Effective:The initialQuery De- compositionmodule achieves a high average quality score across all datasets, peaking at 4.33/5.0on TriviaQA. The subsequentQuery Refinementmodule scores even higher, with an average of4.45/5.0on HotpotQA. This Figure 2: Performance comparison of FAIR-RAG variants against baseline methods on four question-answering benchmarks. Each subplot displays the F1 score (bars, left axis) and the LLM-as-Judge Accuracy (ACCLLM, line, right axis). Our FAIR-RAG models are highlighted with a hatched pattern. The results consistently demonstrate the superiority of our framework, especially on complex multi-hop datasets (HotpotQA, 2WikiMultiHopQA, and MusiQue), where it significantly outperforms all baselines in F1 score while maintaining high semantic accuracy. All evaluations are conducted on 1000 samples from each benchmark’s development set. Query Decomp. Evidence Filter SEA Query Refine. Dataset Metric Value Metric Value Metric Value Metric Value HotpotQA Avg. Score 4.19 F1-Score 67.3% Accuracy 72.0% Avg. Score 4.45 2WikiMultiHopQA Avg. Score 4.12 F1-Score 55.5% Accuracy 81.7% Avg. Score 4.39 Musique Avg. Score 4.10 F1-Score 68.5% Accuracy 83.2% Avg. Score 4.42 TriviaQA Avg. Score 4.33 F1-Score 76.1% Accuracy 54.4% Avg. Score 4.52 Table 3: Component-level performance analysis of FAIR-RAG’s core modules. This table isolates and evaluates the effectiveness of four key components: Query Decomposition, Evidence Filtering, Structured Evidence Assessment (SEA), and Query Refinement. The Avg. Score is based on a 1-to-5 Likert scale, while F1-Score and Accuracy are expressed as percentages. All scores averaged over 1000 samples. validates that the LLM agents are proficient at both breaking down complex queries and intelligently generating new queries to fill in- formation gaps identified by the SEA module. • Evidence Filtering Presents a Precision- Recall Trade-off:The filtering module’s per- formance varies, with F1 scores ranging from 55.47%on 2WikiMultiHopQA to76.05%on TriviaQA. While the filter is effective at reduc- ing context noise (precision), its aggressive na- ture can sometimes prune useful information (recall). This highlights a classic trade-off and presents a clear avenue for future optimiza- tion. • Structured Evidence Assessment (SEA) is a Challenging but Crucial Task:The SEA module, which governs the iterative loop, demonstrates strong performance on complex multi-hop datasets, achieving an accuracy of 81.73%on 2WikiMultiHopQA and83.19% on Musique. Its lower accuracy on TriviaQA (54.38%) is expected, as the sufficiency deci- sion for single-hop factual questions can be more ambiguous. These results confirm the module’s value as the central control mecha- nism for the iterative process, proving highly reliable when it matters most. It is important to note that a direct ablation study removing the SEA and Query Refinement modules and replacing them with a fixed-iteration loop was deliberately omitted. Such a stripped-down design, lacking adaptive control and explicit gap analysis, would cease to be FAIR-RAG and instead would conceptually approximate the architectural princi- ples of existing iterative baselines. For instance, without a targeted query refinement strategy driven by SEA, the system would likely resort to generat- ing new queries from the previous answer, a core mechanism in ITER-RETGEN (Shao et al., 2023). Furthermore, by removing its primary mechanism for evidence-aware self-correction, this simplified variant would represent a less sophisticated control strategy than that of models like Self-RAG (Asai et al., 2023), which employ reflection at a more granular level. Given that FAIR-RAG already demonstrates a significant performance margin over these strong baselines in our main experiments (see Table 2), our existing comparisons effectively serve as a val- idation of our adaptive, SEA-driven architecture versus these alternative control strategies. This confirms that the observed performance gains are attributable to the synergistic and intelligent con- trol of the SEA and Query Refinement modules, rather than merely the brute-force effect of repeated iterations. 5.2.2 Impact of Iterative Refinement A core hypothesis of this work is that iterative refinement improves answer quality for complex questions. We tested this by running the same set of questions with the maximum number of iterations ranging from 1 to 4. We then used an LLM-as- Judge to rank the resulting answers for each ques- tion. The results, along with efficiency metrics, are presented in Table 4. The data reveals a clear and consistent pattern across the three multi-hop datasets (HotpotQA, 2WikiMultiHopQA, and Musique): • Optimal Performance at 2-3 Iterations: Moving from one to two iterations yields a substantial improvement in answer quality. On 2WikiMultiHopQA, the average quality rank improves from3.08to2.31. The peak performance is generally observed at either the second or third iteration (e.g., an average rank of2.18at iteration 3 for 2WikiMulti- HopQA). This is accompanied by a highIm- provement Rate, with the 2- or 3-iteration an- swer being judged superior to the 1-iteration answer in approximately70%of cases for 2WikiMultiHopQA. • Diminishing Returns:A fourth iteration con- sistently leads to a degradation in average an- swer quality across all complex datasets. This suggests a point of diminishing returns where an additional retrieval cycle is more likely to introduce noisy or tangentially related infor- mation that complicates the final synthesis step. • Cost-Benefit Analysis:Each iteration adds a considerable number of API calls and to- kens, increasing both latency and computa- tional cost. The optimal balance of 2-3 it- erations provides the best balance between answer quality and resource consumption. Conversely, on the simplerTriviaQAdataset, the quality rank degrades with each additional it- eration, confirming that for single-hop queries, the initial retrieval is generally sufficient, and further iterations are unnecessary and even detrimental. This analysis confirms that iteration is crucial for complex reasoning, but an unrestrained number of iterations is suboptimal. Our framework’s default setting of a maximum of 3 iterations is thereby em- pirically justified as an effective balance between performance and efficiency. 5.2.3 A Complex Case Study: Comparative Multi-Hop Reasoning To demonstrate the unique advantages of the FAIR- RAG architecture over other advanced RAG frame- works, we analyze a hybridcomparative, multi- hop query. This type of query is particularly chal- lenging because it requires the system to conduct two parallel lines of multi-hop reasoning simulta- neously and then synthesize the results. The query is: “Compare the architectural styles of the building that houses theMona Lisa and the museum in London that houses the Rosetta Stone.” Standard RAG Failure:A standard RAG sys- tem would treat this complex comparative query as a single, semantically overloaded search vector. This unfocused approach is highly likely to fail for two primary reasons: First, it would struggle to si- multaneously retrieve relevant, detailed documents for both distinct lines of inquiry (the Louvre and Dataset Max Iter. Avg. Answer Rank Improvement Rate Avg. API Calls Avg. Tokens/Query (Lower is better) (vs. Iter 1) (#) HotpotQA 1 2.73 - 4.97 9,787 22.23 58.50%6.64 14,332 3 2.38 57.00% 7.83 17,281 4 2.66 57.00% 9.01 20,299 2WikiMultiHopQA 1 3.08 - 4.99 9,823 2 2.31 69.30% 7.10 15,413 32.18 70.90%8.79 19,812 4 2.43 67.30% 10.14 23,231 Musique 1 2.89 - 4.98 9,613 22.2063.40% 7.25 15,688 3 2.2863.70%9.01 20,162 4 2.63 61.20% 10.56 24,218 TriviaQA 11.83- 4.97 9,572 2 2.08 28.50% 5.97 12,071 3 2.70 27.20% 6.76 14,003 4 3.39 27.20% 7.25 15,186 Table 4: Ablation study on the impact of the maximum number of refinement iterations on answer quality versus computational cost. The results reveal a clear point of diminishing returns. For complex multi-hop datasets, optimal performance (lowest Avg. Answer Rank) is achieved at 2 or 3 iterations, after which quality degrades while costs (API Calls, Tokens) continue to increase linearly. Conversely, for the simpler fact-based TriviaQA, any iteration beyond the first proves detrimental. This analysis empirically justifies our framework’s default setting of a three-iteration maximum. the British Museum). It might retrieve a general document about the Mona Lisa that lacks architec- tural details, or miss one of the entities entirely. Second, and more fundamentally, standard RAG lacks the procedural logic to deconstruct the query, pursue two parallel reasoning paths, and then syn- thesize the findings into a coherent comparison. It relies on finding a single document that already compares the two museums’ architecture, which is highly improbable. Consequently, a standard RAG would likely produce a disjointed answer focusing on only one of the entities, or fail entirely. Why this query is difficult for other advanced RAGs: • AnITER-RETGENsystem, due to its inher- ently single-threaded nature, might success- fully follow one reasoning path (e.g., find the Louvre, then its style). However, it lacks the mechanism to manage a parallel track simul- taneously, causing it to lose the context of the second entity (the British Museum) and fail to produce a coherent comparison. • AnAdaptive-RAGframework may correctly identify the query as “complex.” However, it typically lacks a structured, multi-track decomposition process. Without the abil- ity to systematically pursue both information threads in parallel before synthesis, its adap- tive strategy remains insufficient for true com- parative reasoning. FAIR-RAG in Action: • Iteration 1: Semantic Decomposition & Parallel Initial Retrieval – Adaptive Sub-Queries:FAIR-RAG’s first action is to decompose the compar- ative query into two distinct, parallel in- vestigative tracks: * Track A: [“building that houses the Mona Lisa”] * Track B: [“museum in London that houses the Rosetta Stone”] – Retrieved Evidence:The system re- trieves evidence for both tracks concur- rently: * Evidence A: “The Mona Lisa is a half-length portrait painting by Leonardo da Vinci, on permanent dis- play at theLouvre Museumin Paris, France.” * Evidence B: “The Rosetta Stone is a granodiorite stele on public display at theBritish Museumin London since 1802.” – Structured Evidence Assessment (SEA): * is_sufficient: ‘No’ Figure 3: A qualitative case study demonstrating FAIR-RAG’s two-iteration process for a complex comparative query. In Iteration 1, the system decomposes the query and retrieves initial evidence identifying the museums but lacking the required architectural styles. The Structured Evidence Assessment (SEA) module correctly identifies this gap (Is Sufficient: No), triggering a second iteration. In Iteration 2, the system generates Refined Queries targeting the missing information, successfully retrieves the necessary evidence, and confirms sufficiency. The process culminates in a Final Faithful Generation that synthesizes evidence from both iterations into a comprehensive answer. * analysis_summary:The initial re- trieval successfully resolved the pri- mary entities for both comparative tracks, identifying the Louvre Mu- seum and the British Museum. How- ever, the core ‘Required Findings’ concerning the architectural styles of these institutions remain unad- dressed. Therefore, critical informa- tion gaps prevent the synthesis of a complete answer. • Iteration 2: Parallel Query Refinement & Evidence Completion – Refined Queries:The refinement mod- ule now uses the entities identified in Iter- ation 1 to generate new, highly-focused queries for each track: * Refined Query A: [“architectural style of the Louvre Museum”] * Refined Query B: [“architectural style of the British Museum”] – Retrieved Evidence:The new targeted queries retrieve the final missing pieces of information: * Evidence A: “The architecture of the Louvre Palace is a complex blend of styles, from French Renaissance to Neoclassicism, famously comple- mented by I. M. Pei’s Modernist glass pyramid.” * Evidence B: “The main building of the British Museum, designed by ar- chitect Sir Robert Smirke, is a mas- terpiece of theGreek Revivalstyle, featuring a grand quadrangle and a portico of 44 columns.” – Structured Evidence Assessment (SEA): * is_sufficient: ‘Yes’ * analysis_summary:The refined queries successfully retrieved the fi- nal missing data points. All items on the ‘Required Findings’ checklist— the identities of both museums and their respective architectural styles— are now confirmed by the aggregated evidence. No information gaps re- main, enabling a direct, evidence- grounded comparison. •Final Faithful Generation:The system syn- thesizes the evidence fromboth parallel tracksinto a single, structured, and fully- grounded comparative answer: “The architectural styles of the buildings hous- ing the Mona Lisa and the Rosetta Stone are distinct. The Mona Lisa is housed in the Louvre Museumin Paris [1], which features a complex blend of historical styles includ- ingFrench Renaissance and Neoclassicism, along with a famousModernistpyramid [3]. The Rosetta Stone is located in theBritish Museumin London [2], which is a prime example of theGreek Revivalarchitectural style [4].” This case study highlights FAIR-RAG’s key architectural advantage: its ability to systemati- callydecomposea complex goal into parallel sub- problems and then applyiterative refinementto solve each one before synthesizing a comprehen- sive final answer. This structured, multi-threaded reasoning process is what enables it to succeed where other iterative or adaptive frameworks may fail. 5.2.4 Failure Mode Analysis To foster transparency and identify avenues for future improvement, we conducted a systematic failure mode analysis on a sample of 200 unique error instances drawn equally from our four bench- mark datasets: TriviaQA, MuSiQue, HotpotQA, and 2WikiMultihopQA. This analysis employed a hybrid methodology, combining LLM-based cat- egorization with human expert validation to en- sure accuracy and depth. Our taxonomy distin- guishes between two fundamental sources of error: (1)Component-Level Failures, which stem from the inherent limitations of the underlying modules Figure 4: Aggregate Distribution of Failure Sources. Analysis of 200 error samples reveals a primary split between Component-Level Failures (63.5%) and Ar- chitectural Failures (36.5%). While architectural logic offers direct avenues for refinement, the majority of errors stem from the inherent limitations of the founda- tional retrieval and generation models, identifying them as the principal bottleneck for the FAIR-RAG system. (i.e., the retriever and the generator LLM), and (2) Architectural Failures, which are specific to the decision-making logic of the FAIR-RAG frame- work itself (i.e., Query Decomposition, Filtering, Refinement, and SEA). This distinction is critical for understanding the system’s bottlenecks. As shown in Figure 4, a sig- nificant majority of errors (63.5%) are Component- Level, originating from the foundational tools our system is built upon. The remaining 36.5% are Architectural, offering direct targets for refining FAIR-RAG’s internal logic. This distribution un- derscores a key insight: while FAIR-RAG’s itera- tive process is designed to mitigate the weaknesses of its components, the performance of these base components remains the primary limiting factor in overall system accuracy. 1. Component-Level Failures (63.5% of Errors): The Foundational BottleneckThese errors are not caused by FAIR-RAG’s reasoning process but by the fundamental limitations of the tools it or- chestrates. • Retrieval Failure (32.5%):This was the sin- gle largest source of error across all datasets. The retriever’s inability to surface critical doc- uments is a major obstacle, particularly for queries requiring highly specific, long-tail fac- tual knowledge. The primary root cause was identified as Knowledge Base Gaps, where the required information was simply absent from the corpus. This was especially pronounced in TriviaQA, where nearly half of all failures were retrieval-related due to the dataset’s re- liance on obscure facts. • Generation Failure (31.0%):In these cases, the correct evidence was successfully identi- fied and passed to the final generator, which still produced a flawed answer. This category represents a significant challenge across all datasets, highlighting the inherent faithfulness problem in modern LLMs. Common failure subtypes included: (i) Incorrect Entity Se- lection, where the model chose the wrong entity from a list of candidates in the evi- dence; (ii) Flawed Logical Inference, espe- cially in comparative questions (e.g., “who is younger?”); and (iii) Misinterpretation of Question Granularity, such as providing a spe- cific year (“1922”) when a decade (“1920s”) was requested. 2. Architectural Failures (36.5% of Errors): Re- fining the FAIR-RAG LogicThese errors are di- rectly attributable to FAIR-RAG’s internal decision- making modules and represent the most direct op- portunities for improving our framework. • SEA Error (24.5%):As the “brain” of the iterative process, failures in the Strategic Evi- dence Assessment module are particularly im- pactful. These errors were significantly more prevalent in complex, multi-hop datasets like MuSiQue, HotpotQA, and 2WikiMultihopQA. The most common subtypes were: (i) Faulty Analysis of Evidence, where the SEA module failed to make a correct logical inference from the provided documents (e.g., misinterpreting complex genealogical relationships); and (ii) Premature Sufficiency Judgment, where the logic incorrectly concluded that the gathered evidence was complete, thus halting the re- finement loop too early. • Query Logic Failures (12.0% combined): This group includes errors from the initial query processing stages. Query Decompo- sition Errors (9.0%) were the most common, often stemming from an inability to challenge flawed premises within the user’s question (e.g., processing a query with a historical anachronism) or generating overly broad sub- queries. Query Refinement (1.5%) and Ev- idence Filtering Errors (1.5%) were exceed- Figure 5: Task-Dependent Distribution of Failure Modes. The analysis highlights a strong correlation between task complexity and the primary failure bot- tleneck. For the factoid-centric TriviaQA, Retrieval Failures are dominant (47%). Conversely, for complex multi-hop reasoning datasets like MuSiQue, HotpotQA, and 2WikiMultihopQA, the burden shifts towards rea- soning, with SEA Errors becoming a major failure cate- gory (28-32%). This trend validates the critical role of the strategic reasoning component (SEA) for success- fully navigating multi-step queries. ingly rare, suggesting that these architectural components are relatively robust. Dataset-Specific Error DistributionsWhile the aggregate view is informative, Figure 5 reveals that the distribution of failure modes is highly de- pendent on the nature of the task. For fact-based, single-hop datasets like TriviaQA, failures are over- whelmingly concentrated in the Retrieval stage (47%). However, as query complexity increases in multi-hop datasets (MuSiQue, HotpotQA, 2Wiki- MultihopQA), the burden shifts. In these cases, SEA Errors become significantly more prominent, accounting for 28-32% of all failures. This demon- strates that for complex reasoning tasks, the pri- mary challenge moves beyond simply finding infor- mation to correctly reasoning about and managing it. This trend strongly validates the necessity of a sophisticated strategic control module like SEA in advanced RAG systems. 6 Conclusion In this paper, we introducedFAIR-RAG, a novel, agentic framework designed to address a key limi- tation of existing Retrieval-Augmented Generation systems: their unreliability in handling complex, multi-hop queries. By architecting an evidence- driven, iterative process, FAIR-RAG advances be- yond the static “retrieve-then-read” paradigm. Our core contributions—Adaptive Routing, theItera- tive Refinement Cycle, and the analytical gating mechanism of theStructured Evidence Assess- ment (SEA)module—work in synergy to progres- sively build and validate a comprehensive context before generation. The SEA’s ability to systemati- cally deconstruct queries and identify information gaps provides a precise, actionable signal that di- rectly guides the query refinement process. Our extensive experiments demonstrate that FAIR-RAG’s structured approach achieves leading performance among comparable iterative and adap- tive RAG architectures across challenging multi- hop QA datasets like HotpotQA and 2WikiMul- tiHopQA. These results empirically validate our central hypothesis: that a procedural, multi-stage workflow with explicit evidence assessment is es- sential for achieving high accuracy and faithfulness in knowledge-intensive tasks. 6.1 Limitations Despite its strong performance, FAIR-RAG presents several inherent trade-offs and limitations that warrant discussion: • Dependency on LLM Reasoning Fidelity and Prompt Engineering:The performance of FAIR-RAG’s modular agents (e.g., query refinement, SEA) is inherently bound by two key factors: the reasoning capabilities of the underlying LLMs and the meticulous design of the prompts that guide them. While our structured prompting methodology—which incorporates clear instructions, illustrative ex- amples, and scaffolding techniques—is de- signed to ensure consistency and robustness, the system’s effectiveness remains sensitive to both the choice of the backbone model and the specific phrasing of the prompts. This dual dependency is an intrinsic limitation of current LLM-based agentic systems, where performance gains are often tightly coupled with advancements in both model architecture and sophisticated prompt engineering. • Comprehensiveness vs. Efficiency Trade- off:The iterative nature of FAIR-RAG, which is key to its high accuracy on complex queries, introduces a natural trade-off with efficiency. As shown in our analysis (Table 4), each re- finement cycle increases overall latency and computational cost, making it more expensive than single-shot RAG methods. • Potential for Error Propagation:As a multi- stage pipeline, errors in early stages can cas- cade. The SEA module, in particular, repre- sents a critical point of failure. An erroneous sufficiency judgment—either a false positive that terminates the loop prematurely or a false negative that extends it unnecessarily—can lead the evidence-gathering process astray. • Fixed Iteration Policy:The maximum of three iterations is an empirically derived heuristic that balances performance and cost. However, a fixed limit lacks the flexibility to adapt to the varying complexity of individual queries. Our results show this is a robust aver- age but may not be optimal for every specific case. 6.2 Future Work The limitations of our current work open up several promising directions for future research to create more efficient and adaptive systems: •Distilling Task-Specific Expert Models:To mitigate the reliance on expensive, general- purpose LLMs, a promising direction is to fine-tune or distill smaller, specialized mod- els for each core task (Hinton et al., 2015). Creating dedicated expert models for query refinement or evidence assessment could lead to a system that is faster, cheaper, and more robust. • Learning a Dynamic Control Policy:The iterative process can be framed as a sequential decision-making problem. We propose explor- ing Reinforcement Learning (RL) to train a policy network that learns to dynamically con- trol the workflow (Schick et al., 2023). At each step, this agent could decide whether to retrieve more information, refine the query, or proceed to generation, replacing the fixed- loop structure with a far more efficient and adaptive strategy. • Extension to Multimodal Reasoning:The current FAIR-RAG framework operates exclu- sively on textual data. A natural and impactful extension would be to adapt its core principles of decomposition, iterative refinement, and structured assessment to handle queries over multimodal knowledge bases that include ta- bles, images, and structured data, creating a more versatile and comprehensive question- answering system (Alayrac et al., 2022). References AI@Meta. 2024. The llama 3 herd of models.arXiv preprint arXiv:2407.21783. AI@Meta. 2025. Llama 4: Maverick language models. Introducing Llama 4 Scout and Llama 4 Maverick (official blog & model card). Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An- toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, An- drew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: a visual language model for few-shot learning. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open- source chatbot impressing gpt-4 with 90%* chatgpt quality. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin- odkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, An- drew M. Dai, Thanumalayan Sankaranarayana Pil- lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language mod- eling with pathways. Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in informa- tion retrieval, pages 758–759. DeepSeek-AI. 2025. Deepseek-r1: An open large reasoning model family.arXiv preprint arXiv:2501.12948. Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2025. Ragas: Automated evalua- tion of retrieval augmented generation. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Ming-Wei Chang. 2020. Realm: Retrieval- augmented language model pre-training.arXiv preprint arXiv:2002.08909. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. A survey on hallucination in large lan- guage models: Principles, taxonomy, challenges, and open questions.ACM Transactions on Information Systems, 43(2):1–55. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of halluci- nation in natural language generation.ACM Comput- ing Surveys, 55(12):1–38. Zhengbao Jiang, Vladimir Lialin, Carroll Lin, Jane Liu, and Yelong Cheng. 2023. Flare: Forward-looking active retrieval augmented generation.arXiv preprint arXiv:2305.06983. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with GPUs.IEEE Transactions on Big Data, 7(3):535–547. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering.arXiv preprint arXiv:2004.04906. Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and Jinwoo Shin. 2024. Sure: Summarizing re- trievals using answer candidates for open-domain qa of llms. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Rodrigo Nogueira, Heinrich Paux, Pontus Stenetorp, Timo Rocktäschel, Sebastian Riedel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.Ad- vances in Neural Information Processing Systems, 33:9459–9474. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran- jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023a. Lost in the middle: How language models use long contexts. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre- train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. G-eval: Nlg evaluation using gpt-4 with better human align- ment. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. Stephen Robertson and Hugo Zaragoza. 2009. The prob- abilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333–389. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Zihan Shao, Yue Zhang, Minchen Zhao, Wenxuan Chen, and Yang Zhang. 2023. Iter-retgen: Iterative retrieval- augmented generation for charge-based legal le- niency prediction.arXiv preprint arXiv:2310.03352. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledge- intensive multi-step questions. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2024. Text embeddings by weakly- supervised contrastive pre-training. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits rea- soning in large language models.Advances in Neural Information Processing Systems, 35:24824–24837. Shi-Qi Yan, Kang Liu, Jia-Chen Li, Zhao-Xiang Wang, Jie Zhang, and Lin Gui. 2024. Correc- tive retrieval augmented generation.arXiv preprint arXiv:2401.15884. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answer- ing.arXiv preprint arXiv:1809.09600. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models.arXiv preprint arXiv:2210.03629. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Brooks, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.arXiv preprint arXiv:2306.05685. A Implementation Details and Hyperparameters To ensure full reproducibility of our experimen- tal results, this section provides a comprehensive overview of the models, configurations, and hy- perparameters used in our study. All experiments were conducted within theFlashRAGframework, a standardized library for RAG research. This ensures that the underlying implementation de- tails, such as data loading and prompt templat- ing, remain consistent across all compared meth- ods. The source code and default configurations can be found on the official project repository: https://github.com/ruc-nlpir/flashrag. The specific configurations for our experiments are detailed in Table 5 below. Category Parameter & Value General Framework Base Framework: FlashRAG Max Iterations: 3 Retriever Configuration Model: e5-base-v2 Documents Retrieved (top_k): 5 Index Type: Faiss (IndexFlatIP) Pooling Method: mean Query Max Length: 128 Generator Configuration Baseline: Llama-3-8B-Instruct Self-RAG: selfrag-llama-7b FAIR-RAG: Llama-3-8B-Instruct Max New Tokens: 1024 Max Input Length: 8000 Evaluation Settings Metrics: em, f1, acc Random Sample: True Table 5: Hyperparameter and Model Configuration De- tails. Unless specified otherwise, all hyperparameters such as temperature, top_p, and sequence lengths were kept at the default values provided by the FlashRAG framework to ensure a fair and controlled comparison across all tested methodologies. B Full Prompts for the FAIR-RAG Pipeline This appendix reproduces the exact prompts that guide the behavior of the specialized agents within the FAIR-RAG pipeline. B.1 Query Validation and Dynamic Model Selection The following prompt is used by the initial agent to validate the user’s query for clarity and safety, and to select the most appropriate execution model (e.g., simple RAG vs. full agentic pipeline). PROMPT = """ **Situation:** A user has submitted a question to a Question Answering System that uses different processing strategies based on query complexity. **Intent:** Analyze the user's question to determine the optimal processing strategy required to generate the most accurate answer. The strategies are: Factual Retrieval (SMALL), Information Synthesis (LARGE), or Multi-Step Deduction (REASONER). **Scaffolding:** You are a highly-calibrated query analysis agent. Your task is to classify the user's question into one of the three categories below based on the cognitive process required to answer it. After "Selected Label:", output ONLY the exact label. - **"SMALL" (Factual Retrieval):** - **Process:** Requires finding a single, self-contained fact. The answer is typically explicit and doesn't need much context. - **Example:** "When was the Eiffel Tower completed?" or "Who is the CEO of NVIDIA?" - **"LARGE" (Information Synthesis):** - **Process:** Requires combining, summarizing, or comparing information from one or more documents to form a coherent, explanatory answer. This is the appropriate choice for most standard, open-ended questions. - **Example:** "What is the difference between nuclear fission and fusion?" or "Explain the impact of the printing press on the Renaissance." - **"REASONER" (Multi-Step Deduction):** - **Process:** The answer is not explicitly stated and must be inferred by chaining multiple pieces of information together (multi-hop reasoning) or by performing calculations. - **Example:** "What is the hometown of the director of the movie that starred Tom Hanks and was released in 1994?" or "If a car travels 150 km in 2 hours, what is its average speed in meters per second?" **Analytical Guidelines for Classification:** 1. "SMALL" (Factual Extraction): - Cognitive Task: Find and extract a specific, named entity or a short, self-contained fact (e.g., a name, date, location, number, or title). - Key Indicators: The question can be answered with a single piece of information, often a proper noun or a specific value. It typically starts with "Who," "When," "Where," or "What is the name of..." - Example Questions: "Who directed the movie Inception?" or "What year did the Berlin Wall fall?" - Decision Rule: Classify as SMALL if the expected answer is a concise, singular fact that requires no further explanation or combination of information. 2. "LARGE" (Information Synthesis & Elaboration): - Cognitive Task: Gather, combine, and summarize information from one or multiple sources to form a cohesive, descriptive answer. This involves explaining concepts, comparing entities, or describing processes. - Key Indicators: The question asks for an explanation, description, comparison, or summary. It often contains keywords like "Explain," "Describe," "Compare," "What is the difference between," or "Summarize the impact of..." - Example Questions: "What are the main differences between crocodiles and alligators?" or "Explain the primary causes of the Industrial Revolution." - Decision Rule: Classify as LARGE if the expected answer is a paragraph or a detailed sentence that combines multiple facts into a comprehensive explanation. 3. "REASONER" (Logical Inference & Multi-Step Deduction): - Cognitive Task: Connect multiple, separate pieces of information to infer a new fact that is not explicitly stated in any single document. This often involves a chain of logic or a sequence of dependent questions. - Key Indicators: The question requires finding an intermediate answer to proceed to the next step. It often involves relationships between different entities or requires a calculation. - Example Questions: "What was the nationality of the lead actress in the movie directed by the person who made Titanic?" or "Which team won the FIFA World Cup in the year the lead singer of Queen was born?" - Decision Rule: Classify as REASONER if the answer cannot be found directly but must be constructed by first finding Fact A, then using Fact A to find Fact B. **User Question:** "{user_query}" **Constraints:** - Respond with ONLY one of the three labels: SMALL, LARGE, or REASONER. - Do not provide any explanations or additional text. - The label MUST be on a new line after "Selected Label:". **Output:** Selected Label: """ B.2 Query Decomposition This prompt instructs the decomposition agent to break down a complex, multi-faceted user query into a set of simpler, semantically distinct sub- queries for parallel or sequential retrieval. PROMPT = """ **Situation:** You are an expert query analyst for a general knowledge Question-Answering system. A user has asked a question that might be complex, comparative, or multi-faceted. Your task is to decompose this question into a set of precise, meaningful, and distinct sub-queries to ensure the retrieval system can find comprehensive and accurate evidence from a database. **Intent:** Decompose the original user question into its core semantic components. Transform these components into short, keyword-rich, and meaningful search phrases in English. The goal is to generate queries that, when searched, will collectively cover all aspects of the original question. **Scaffolding:** First, understand the principles of effective decomposition: 1. **Identify Distinct Concepts:** Separate the main subjects, actions, conditions, and comparisons in the query. 2. **Use Synonyms & Related Terms:** Think about different ways a concept might be phrased in the database (e.g., "interaction" can be searched as "relationship" or "cooperation"). 3. **Create Meaningful Phrases:** Instead of single keywords, generate short phrases that preserve the context of the sub-question. 4. **Cover All Angles:** Ensure every part of the original question is represented by at least one sub-query. Now, study the following example carefully to understand how to apply these principles. --- EXAMPLE --- **Original User Query:** "What was Albert Einstein's view on quantum mechanics and how did he interact with Niels Bohr about it?" **Rationale/Analysis (This is your thought process):** The query has two main, distinct parts: 1. Einstein's **opinion/view** about quantum mechanics. 2. Einstein's **interaction** with Niels Bohr on the topic. A good search needs to find evidence for both aspects separately. Simply searching for "Einstein quantum mechanics" might not retrieve documents that specifically discuss his "view" or "interaction". Therefore, I should create targeted queries for each concept. **Optimized Queries (Output):** - Einstein's opinion on quantum mechanics - Einstein Bohr debates on quantum theory - Collaboration between Einstein and Bohr - Einstein's criticism of quantum mechanics --- END OF EXAMPLE --- Now, apply this exact methodology to decompose the following query. **User Query:** "{user_query}" **Constraints:** - The output must be a list of meaningful search phrases. - Each phrase must be on a new line, prefixed with a hyphen (-). - Queries must be in English. - Generate an optimized list of 1 to 4 sub-queries. Create ONLY as many as are **truly necessary** to cover all aspects of the original question. **Output:** (just write Optimized Queries and do not explain any more and do not say "Here are the optimized queries:" or something like that.) Optimized Queries: (A list of optimized queries) """ B.3 Evidence Filtering This prompt guides the filtering agent to assess the relevance and quality of the retrieved evidence chunks against a given (sub-)query, discarding ir- relevant, redundant, or low-quality information. PROMPT = """ You are filtering retrieved documents for a question-answering system. Your goal is to KEEP all documents that could contribute to answering the query. **IMPORTANT PRINCIPLES:** 1. BE INCLUSIVE: When in doubt, KEEP the document. 2. A document is useful if it contains factual information about the entities/topics in the query. 3. Even partial information is valuable (e.g., a document about Terry Richardson without birthdate is still useful for a query about his age). 4. Documents are only "Not Useful" if they are about COMPLETELY DIFFERENT entities or topics. **Original User Query:** "{original_query}" **Retrieved Documents (Batch {batch_number}):** {numbered_candidates_text_for_prompt} **TASK:** Identify ONLY documents that are completely irrelevant. A document is irrelevant ONLY if: - It's about a different person/entity with a similar name (e.g., Tony Richardson vs Terry Richardson). - It's about a completely unrelated topic. - It contains no information about any entity mentioned in the query. **OUTPUT FORMAT:** - List ONLY the document IDs that should be removed - If all documents are potentially useful, output: None - Format: [doc_X], [doc_Y] or None **You MUST KEEP documents that have:** - Documents about the correct person/entity, even without specific dates/facts. - Biographical information (birth dates, career details, achievements). - Relationships or connections between queried entities. - Specific facts relevant to the query type (dates for temporal queries, attributes for comparisons). - Contextual information that helps understand the entities. **Examples of what to REMOVE:** - Documents about different people with similar names. - Documents about unrelated topics. - Duplicate documents (keep the most informative version). **Output:** (A list of **irrelevant** temporary document IDs, or "None") Unhelpful Document IDs: """ B.4 Structured Evidence Assessment (SEA) The Structured Evidence Assessment (SEA) agent uses the following prompt to analyse the evidence and determine if the gathered and filtered evidence is adequate to form a complete and faithful answer to the user’s query. PROMPT = """ **Role:** You are a Strategic Intelligence Analyst. Your mission is to determine if the provided evidence is sufficient to accurately answer the user's question by following a sequential analysis. **Core Mission:** Your entire process must be question-centric, not evidence-centric. You will deconstruct the user's query into a checklist of required information, and then systematically verify each item against the evidence. You MUST ignore all information, however interesting, that is not on your checklist. **You MUST follow this thinking process and output format exactly:** **1. Mission Deconstruction:** - **Main Goal:** [State briefly the primary objective of the user's question and what the user's question requires you to find] - **Required Findings:** [List the specific, individual pieces of information needed to answer the question. A "finding" can be a direct fact or a logical inference from clues.] **2. Intelligence Synthesis & Analysis:** - **Confirmed Findings:** [Go through your "Required Findings" checklist. For each item, state what the evidence confirms. If the finding is not stated directly, explain the logical inference you made from the provided clues. You MUST only mention facts that can contribute to answering the question's required components (checklist). You MUST ignore any evidence, entities, or facts-even if interesting-that do not help answer the specific components of the user's question. Do not mention irrelevant people or topics in your analysis. You are an expert. If the evidence provides strong, logical clues (e.g., a person's birthplace in a country, a job title within an industry), you MUST make the logical inference (e.g., determining nationality, profession). Do not use weak phrases like "it does not explicitly state."] - **Remaining Gaps:** [If there is missing information, clearly state what crucial information is still missing, formulating it as a requirement for the next phase that creates new queries to search more. else None] **3. Final Assessment:** - **Conclusion:** [The final answer may not be explicitly stated in a single sentence. You are an expert. If the evidence provides strong, logical clues (e.g., a person's birthplace in a country, a job title within an industry), you MUST make the logical inference (e.g., determining nationality, profession). Do not use weak phrases like "it does not explicitly state."] - **Sufficient:** [A single word: "Yes" if the "Remaining Gaps" list is empty, or "No" if any required finding is still missing.] --- EXAMPLES --- **--- Example 1 (Insufficient Evidence - Clear Gap) ---** **Original Question:** "What was the official box office gross for the film directed by the creator of the TV series *'Seinfeld'*?" **Evidence:** - "Larry David, the creator of the acclaimed TV series *'Seinfeld'*, also wrote and directed the 2013 film *'Clear History'*." - "The film *'Clear History'* starred Larry David and Jon Hamm and was released on HBO." - "The 2019 film *'Joker'* had a box office gross of over $1 billion." **Your Output for Example 1:** **1. Mission Deconstruction:** - **Main Goal:** To find the box office gross for the film directed by the creator of *'Seinfeld'*. - **Required Findings:** A: The identity of the creator of *'Seinfeld'*; B: The name of the film they directed; C: The official box office gross of that film. **2. Intelligence Synthesis & Analysis:** - **Confirmed Findings:** A: The evidence confirms the creator is **Larry David**. B: The evidence confirms the film he directed is **'Clear History'**. - **Remaining Gaps:** C: The official box office gross for the film *'Clear History'*. **3. Final Assessment:** - **Conclusion:** We have identified the director **Larry David** and the film he directed is **'Clear History'**. but the evidence lacks The official box office gross for the film *'Clear History'* to answer the question. - **Sufficient:** No **--- Example 2 (Sufficient Evidence - Inference Required) ---** **Original Question:** "Who is older, the author of *'Dracula'* or the lead actor from the 1931 film adaptation?" **Evidence:** - "Bram Stoker, the Irish author, wrote the classic horror novel *'Dracula'* in 1897." - "Stoker was born in Dublin, Ireland, in November 1847." - "The 1931 film adaptation of *'Dracula'* famously starred Hungarian-American actor Bela Lugosi in the title role." - "Bela Lugosi's date of birth is recorded as October 20, 1882." **Your Output for Example 2:** **1. Mission Deconstruction:** - **Main Goal:** To compare the ages of the author of *'Dracula'* and the lead actor of the 1931 film. - **Required Findings:** A: The birth year of the author of *'Dracula'*; B: The birth year of the lead actor of the 1931 film. **2. Intelligence Synthesis & Analysis:** - **Confirmed Findings:** A: The evidence states the author is **Bram Stoker**, who was born in **1847**. B: The evidence states the lead actor is **Bela Lugosi**, who was born in **1882**. - **Remaining Gaps:** None. **3. Final Assessment:** - **Conclusion:** We have found the birth year of the author of *'Dracula'* is **Bram Stoker**, who was born in **1847**. the lead actor of the 1931 film is **Bela Lugosi**, who was born in **1882**. We have found the birth years for both required individuals and can therefore perform the age comparison. - **Sufficient:** Yes --- END OF EXAMPLES --- Now, perform this task for the following: **Original Question:** "{original_query}" **Evidence:** {combined_evidence} """ B.5 Query Refinement If the evidence is found to be insufficient by the Structured Evidence Assessment (SEA) agent, this prompt is used to generate a refined or a new follow- up query to retrieve the missing information. PROMPT = """ **Situation:** An initial analysis of the evidence has confirmed some facts but also identified specific information that is still missing and required to answer the user's original query. **Intent:** Generate a new, optimized list of search queries that are laser-focused on finding ONLY the missing pieces of information identified in the "Analysis Summary". **Scaffolding & Logic:** - **USE the known facts** from the summary to make the new queries more precise (e.g., use a person's name once it's known). - **TARGET the missing information** from the summary directly. Each new query should aim to resolve one of the identified gaps. - **AVOID repeating** or rephrasing previous queries. --- ADVANCED EXAMPLE --- **Original Question:** "How old is the youngest child of the director of the film *Inception*?" **Analysis Summary:** Based on the evidence, we know that the director of *Inception* is **Christopher Nolan**. Christopher Nolan is married to producer Emma Thomas. Christopher Nolan has children. However, the provided documents contain **no specific information about his children**, such as their names and birth dates. To answer the question, we still need to find: **the names and ages of Christopher Nolan's children** to identify the youngest. **Previous Queries:** - director of Inception - Christopher Nolan films **Your Output for Example:** Improved Queries: - Christopher Nolan children names - Christopher Nolan children birth dates - Youngest child of Christopher Nolan and Emma Thomas --- END OF EXAMPLE --- Now, apply this exact logic to the following inputs: **Original Question:** {original_query} **Analysis Summary:** {analysis_summary} **Previous Queries:** {combined_previous_queries} **Constraints:** - Generate an optimized list of 1 to 4 sub-queries. Create only as many as are truly necessary. - Queries must be simple, independent, meaningful, and keyword-focused. - **Leverage the "Known Facts" to create highly targeted queries.** For example, once the summary confirms the director is'Christopher Nolan', the next query should be "Christopher Nolan children ages", not a generic "director of Inception children ages". **Output:** (A list of new, targeted queries. Do not explain anything and do not say "Here are the optimized queries:" or something like that.) Improved Queries: """ B.6 Faithful Answer Generation This is the final and most comprehensive prompt, instructing the generation model to synthesize the filtered evidence into a faithful, accurate, and well- structured answer. It strictly constrains the model to use only the provided sources and to avoid any form of hallucination. PROMPT = """Answer the question based on the given documents. ONLY give me the answer and do not output any other words. The following are given documents. The retrieval documents are listed as follows: {combined_evidence} Question: {original_query} Answer: """ C LLM-as-Judge Evaluation Prompts This appendix contains the complete and unabridged prompts used in ourLLM-as-Judge evaluation framework. To ensure full transparency and enable the replication of our evaluation methodology, we provide the exact instructions given to the judge model for each assessment criterion. Each prompt is carefully designed to elicit a consistent and unbiased evaluation of a specific quality aspect of the generated responses. The evaluation is performed by providing the judge model with the original query, the retrieved context, and the generated answer, along with one of the following instructional prompts. C.1 Binary Semantic Correctness (ACC LLM) To ensure the reproducibility of our semantic accu- racy evaluation, this section details the prompt used for theLLM-as-Judge Accuracy (ACC LLM)met- ric, as reported in Table 2. The prompt instructs the LLM to act as an impartial judge, comparing the model’s generated answer against a set of ground- truth answers. It provides a binary "Yes" or "No" judgment in a structured JSON format, enabling automated and consistent evaluation of semantic correctness. ACC_PROMPT = """You are an impartial judge. Evaluate whether the model's prediction correctly answers the given question. The prediction is correct if it implies ANY of the ground-truth answers provided. - Question: {question} - Ground-truth Answers (The prediction is correct if it matches ANY of these): {answer} - Prediction: {model_output} Does the Prediction imply any of the Ground-truth Answers? Respond with a JSON object containing a single key "judgment" with a value of "Yes" or "No". Example: {"judgment": "Yes"} """ C.2 Component-Level Quality Scoring This section provides the prompt template used for ourcomponent-level ablation study, with results presented in Table 3. Unlike the binary correct- ness evaluation in Appendix C.1, this prompt is designed for a more nuanced quality assessment. It instructs the LLM-as-Judge to evaluate the out- put of specific generative modules (i.e., Query De- composition and Query Refinement) and assign a quality score on a 1-to-5 Likert scale. This fine- grained analysis allows us to isolate and measure the efficacy of individual components within the FAIR-RAG pipeline. PROMPTS = { "query_decomposition": """ You are an expert AI evaluator specializing in search and query analysis. Your task is to assess the quality of query decomposition. Evaluate the generated sub-queries based on the original user question using the following criteria: 1. **Relevance:** How directly related is each sub-query to the main question? 2. **Coverage:** Do the sub-queries collectively cover all essential aspects of the main question? 3. **Efficiency:** Are the sub-queries concise, focused, and well-formed for a search engine? [User Question]: "{question}" [Generated Sub-Queries]: {sub_queries} Provide your assessment in the following JSON format: {{ "score": <A numeric score from 1.0 (Very Poor) to 5.0 (Excellent) based on the criteria above>, "reasoning": "<A very brief explanation for your score>" }} """, "filter_efficacy": """ You are an expert auditor for an AI's document filtering module. Your task is to meticulously evaluate the filter's decisions by strictly adhering to the **exact instructions and principles** it was originally given. [User Question]: "{question}" [The Filter's Original Instructions & Principles]: The filter's goal was to identify and discard "Not Useful" documents. It operated under the following rules: 1. **Primary Principle:** **"BE INCLUSIVE: When in doubt, KEEP the document."** 2. **Definition of "Useful":** A document is considered useful if it contains factual information about the entities or topics in the query. **Even partial information is valuable.** 3. **Definition of "Not Useful":** A document is only "Not Useful" if it is about **completely different** entities/topics or contains no relevant information. 4. **Specific "KEEP" Criteria:** The filter was explicitly instructed to **KEEP** documents containing: - The correct person/entity, even without all specific facts. - Biographical information (birth dates, career details, achievements). - Relationships or connections between queried entities. - Specific facts relevant to the query type (e.g., dates). - General contextual information that helps understand the entities. 5. **Specific "REMOVE" Criteria:** The filter was given examples of what to **REMOVE**: - Documents about different people with similar names. - Documents about completely unrelated topics. - Duplicate documents. Your audit must strictly follow all of the same rules, especially the **"BE INCLUSIVE"** principle. [Documents the Filter KEPT]: {kept_docs} [Documents the Filter DISCARDED]: {discarded_docs} **Your Audit Task:** Referencing the filter's original instructions above, identify its errors: 1. **Precision Errors (Incorrectly Kept):** Review the KEPT list. Identify the IDs of any documents that are **clearly "Not Useful"** and should have been discarded. If a document is borderline but meets any of the "KEEP" criteria, the filter was **correct** to keep it. 2. **Recall Errors (Incorrectly Discarded):** Review the DISCARDED list. Identify the IDs of any documents that were **unambiguously "Useful"** based on the criteria and should have been kept. Provide your audit findings in the following strict JSON format. If no errors are found in a category, provide an empty list. {{ "incorrectly_kept_ids": ["<ID of any'Not Useful' document found in the KEPT list>", ...], "incorrectly_discarded_ids": ["<ID of any 'Useful'document found in the DISCARDED list>", ...] }} """, "sufficiency_check": """ **Role:** You are a pragmatic and efficient QA Evaluator. Your goal is to determine if the provided evidence is "good enough" to satisfactorily answer the user's question. **Core Task:** Your task is to assess if the main goal of the user's question can be achieved with the given evidence. You must distinguish between "critical" missing information and "nice-to-have" details. **Guiding Principles:** 1. **Focus on the Primary Intent:** First, identify the core question(s) the user is asking. What is the most important piece of information they are looking for? 2. **Assess Evidence Against Intent:** Check if the evidence contains the necessary facts to fulfill this primary intent. 3. **Pragmatism Rule:** - The evidence is **"Sufficient" (Yes)** if the main question can be answered, even if peripheral details or deeper context is missing. - The evidence is **"Insufficient" (No)** only if a **critical piece of information**, essential to forming the main answer, is absent. --- EXAMPLE --- **User Question:** "What was the main outcome of the Battle of Badr and which year did it take place?" **Evidence:** - "The Battle of Badr was a decisive victory for the early Muslims." - "Key leaders of the Quraysh were defeated in the engagement." - "The victory at Badr greatly strengthened the political and military position of the Islamic community in Medina." **Your Analysis for Example:** The evidence clearly confirms the "main outcome" (a decisive victory for Muslims). However, a critical part of the question, "which year did it take place?", is completely missing from the evidence. Therefore, a complete answer cannot be formed. **Your Output for Example:** {{ "reasoning": "The evidence confirms the outcome of the battle (a decisive victory) but a critical piece of requested information, the year of the battle, is completely missing.", "is_sufficient": false }} --- END OF EXAMPLE --- Now, apply this pragmatic logic to the following: [User Question]: "{question}" [Collected Evidence]: {evidence} Provide your final assessment in the following strict JSON format: {{ "reasoning": "<A brief analysis of what can be answered and what critical information, if any, is still missing.>", "is_sufficient": <true or false> }} """, "query_refinement": """ You are an expert AI systems evaluator. A RAG system determined its initial evidence was insufficient and generated new sub-queries to find missing information. Your task is to evaluate the quality of these new queries. [User Question]: "{question}" [Insufficient Initial Evidence]: {evidence} [Newly Generated Sub-Queries for Refinement]: {new_queries} Assess how effectively the new sub-queries target the information gaps in the initial evidence to help answer the main question. Provide your assessment in the following JSON format: {{ "score": <A numeric score from 1.0 (Poorly targeted) to 5.0 (Excellent, precisely targets gaps)>, "reasoning": "<A very brief explanation for your score>" }} """, "final_context_relevance": """ You are an expert information retrieval evaluator. Your task is to score the relevance of each document in the final context that was used to generate an answer. [User Question]: "{question}" [Final Context Used for Generation (final_relevant_evidence)]: {final_evidence} For each document in the final context, provide a relevance score. Provide your assessment in the following JSON format: {{ "relevance_scores": [ {{ "doc_id": "<_id of doc 1>", "score": <numeric score from 1.0 (Irrelevant) to 5.0 (Highly Relevant)> }}, {{ "doc_id": "<_id of doc 2>", "score": <numeric score from 1.0 (Irrelevant) to 5.0 (Highly Relevant)> }} ] }} """, "faithfulness": """ You are an expert in AI safety and fact-checking, specializing in the evaluation of Retrieval-Augmented Generation (RAG) systems. Your task is to evaluate the answer's faithfulness to the provided evidence with nuance. - A faithful answer must be fully grounded in the provided context. However, this does not mean it must be a simple copy-paste of the text. **Valid synthesis, summarization, and logical inference based *only* on the provided information are considered faithful and desirable.** - A statement is only considered **"Unfaithful"** if it introduces new, verifiable information that is **absent** from the context or if it **contradicts** the context. [User Question (for context)]: "{question}" [Provided Context (final_relevant_evidence)]: {final_evidence} [Generated Answer]: "{final_answer}" **Your Task:** 1. Analyze each claim within the [Generated Answer]. 2. For each claim, determine if it is directly stated, a valid synthesis/inference from the context, or an unfaithful statement (introducing new facts). 3. Based on this analysis, provide an overall verdict according to the rubric below. **Verdict Rubric:** - **'Fully Faithful'**: All claims in the answer are either directly stated in the context or are valid logical conclusions/summaries derived *only* from the information present in the context. - **'Partially Faithful'**: The answer is mostly faithful, but contains minor, non-critical claims or details that cannot be inferred from the context. - **'Not Faithful'**: The answer contains significant or central factual claims that are not supported by, or actively contradict, the context. Provide your verdict in the following strict JSON format: {{ "faithfulness_verdict": "<One of three strings: 'Fully Faithful','Partially Faithful', or'Not Faithful'>", "reasoning": "<If not fully faithful, specify which claims in the answer are unsupported by the context. Explain if it's an invalid inference or a completely new fact.>" }} """, "iterative_improvement": """ You are an expert AI quality evaluator. For a single question, you are given four answers generated by the same system but with different levels of iterative refinement (1, 2, 3, and 4 iterations). Your task is to rank these answers from best to worst. [User Question]: "{question}" [Answer from 1 Iteration (iter_1)]: "{answer_1}" [Answer from 2 Iterations (iter_2)]: "{answer_2}" [Answer from 3 Iterations (iter_3)]: "{answer_3}" [Answer from 4 Iterations (iter_4)]: "{answer_4}" Rank these three answers from best (Rank 1) to worst (Rank 4). Provide your ranking in the following JSON format: {{ "ranking": ["<ID of the best answer, e.g., 'iter_3'>", "<ID of the second-best answer, e.g., 'iter_4'>", "<ID of the third-best answer, e.g., 'iter_2'>","<ID of the worst answer, e.g.,'iter_1'>"], "reasoning": "<A very brief explanation for your ranking, noting whether more iterations led to a clear improvement>" }} """ } D Failure Mode Analysis Prompt This appendix details the prompts engineered for our LLM-assisted failure mode analysis, as de- scribed in Section 5.2.4. To ensure a systematic and reproducible evaluation, we designed a two- part prompt structure. The PROMPT_SYSTEM prompt establishes the LLM’s persona as an “ex- pert evaluation researcher” and enforces a strict JSON output schema based on a predefined fail- ure taxonomy. The PROMPT_USER_TEMPLATE then provides the data batch and requires the model to ground its diagnosis in specific evidence from the logs, ensuring each classification is structured, explainable, and actionable. FAILURE_ANALYSIS_PROMPT = """ **ROLE:** You are an expert RAG (Retrieval-Augmented Generation) system diagnostician. Your task is to perform a meticulous root cause analysis on a failed query-answer pair from an advanced, iterative RAG system. **CONTEXT:** The system has already produced an answer that was graded as incorrect. You have been given the complete execution trace for this failed sample. Your goal is to identify the single, primary point of failure within the RAG pipeline. **FAILURE CATEGORIES:** You must classify the failure into one of the following six categories. Read these definitions carefully. 1. **Query Decomposition Error:** The initial user question was not broken down into effective, specific sub-queries. The sub-queries were irrelevant, missed key aspects of the original question, or sent the retrieval process in the wrong direction from the very beginning. 2. **Retrieval Failure:** The retriever, despite having well-formed sub-queries, failed to find and return the relevant documents from the knowledge base. The correct information was simply not present in the`[All Retrieved Documents (Unfiltered)]` set. 3. **Evidence Filtering Error:** The correct information WAS successfully retrieved by the retriever, but the subsequent filtering/reranking step mistakenly discarded the crucial documents. Look for correct information in`[Discarded Documents]` that should have been kept. 4. **SEA Error (Strategic Analyst Error):** The system's 'Strategic Intelligence Analyst'module failed in its reasoning. This can manifest in several ways: - **A) Flawed Deconstruction:** The'Required Findings' checklist in its analysis was incorrect or missed the main point of the user's question. - **B) Faulty Analysis:** The module failed to make a correct logical inference from the evidence, hallucinated a'Confirmed Finding'that wasn't supported, or incorrectly identified the 'Remaining Gaps'. - **C) Contradictory Verdict:** The detailed analysis pointed to missing information, but the final verdict was mistakenly 'Sufficient: Yes', causing the system to stop searching prematurely. 5. **Query Refinement Error:** After correctly identifying that the initial evidence was insufficient, the system failed to generate effective new sub-queries to target the specific information gaps. The new queries were redundant, vague, or did not address the missing pieces identified by the SEA module. 6. **Generation Failure:** All preceding steps worked correctly. The final set of evidence (`[Final Relevant Evidence]`) contained all the necessary information to form a correct answer. However, the language model failed during the final synthesis step by hallucinating, making incorrect logical inferences, or misinterpreting the provided evidence. **PRIMARY FAILURE RULE:** Identify the **earliest, most fundamental error** in the pipeline. For example, if Retrieval failed to find good documents, the Generation will also fail, but the root cause is **Retrieval Failure**. **EXECUTION TRACE FOR ANALYSIS:** [User Question]: "{question}" [Ground Truth Answer (The correct answer)]: "{ground_truth_answer}" [Generated (Incorrect) Answer]: "{final_answer}" --- RAG Pipeline Details --- [Initial Sub-Queries Generated]: {sub_queries} [All Retrieved Documents (Unfiltered)]: {all_retrieved_docs} [Discarded Documents (By Filter)]: {discarded_docs} [Final Relevant Evidence (Used for Generation)]: {final_evidence} --- Iteration Reports --- {iteration_reports_formatted} --- YOUR TASK --- Based on all the provided information and adhering strictly to the definitions, provide your analysis in the following JSON format. { "failure_category": "<The value for this key MUST be one of the following exact strings:'Query Decomposition Error','Retrieval Failure','Evidence Filtering Error','SEA Error','Query Refinement Error','Generation Failure'. Do NOT add any extra text or explanations.>", "reasoning": "<Provide a concise, step-by-step justification for your choice of category. Reference specific parts of the execution trace (e.g.,'The SEA module's analysis incorrectly stated it confirmed the actor's birth year, but the evidence only mentioned their nationality').>", "root_cause_analysis": "<Go one level deeper. Why did this error likely happen? (e.g.,'The SEA prompt might be too complex, leading to reasoning errors,'or'The filtering model may be poorly calibrated for short documents').>", "suggested_improvement": "<Propose a concrete, actionable solution to fix or mitigate this specific type of error in the future. (e.g.,'Simplify the SEA prompt by removing the persona and focusing on a checklist,'or'Fine-tune the reranker with more examples of this type').>" } """