GeoWINE: Geolocation based Wiki, Image, News and Event Retrieval Golsa Tahmasebzadeh TIB ‚Äì Leibniz Information Centre for Science and Technology Germany golsa.tahmasebzadeh@tib.eu Endri Kacupaj University of Bonn Germany kacupaj@cs.uni-bonn.de Eric M√ºller-Budack TIB ‚Äì Leibniz Information Centre for Science and Technology Germany eric.mueller@tib.eu Sherzod Hakimov TIB ‚Äì Leibniz Information Centre for Science and Technology Germany sherzod.hakimov@tib.eu Jens Lehmann University of Bonn Fraunhofer IAIS Dresden Germany jens.lehmann@cs.uni-bonn.de Ralph Ewerth TIB Hannover & L3S Research Center, Leibniz University Hannover Germany ralph.ewerth@tib.eu ABSTRACT In the context of social media, geolocation inference on news or events has become a very important task. In this paper, we present the GeoWINE (Geolocation-based Wiki-Image-News-Event retrieval) demonstrator, an effective modular system for multimodal retrieval which expects only a single image as input. The GeoWINE system consists of five modules in order to retrieve related infor- mation from various sources. The first module is a state-of-the-art model for geolocation estimation of images. The second module performs a geospatial-based query for entity retrieval using the Wikidata knowledge graph. The third module exploits four different image embedding representations, which are used to retrieve most similar entities compared to the input image. The embeddings are derived from the tasks of geolocation estimation, place recognition, ImageNet-based image classification, and their combination. The last two modules perform news and event retrieval from EventReg- istry and the Open Event Knowledge Graph (OEKG). GeoWINE provides an intuitive interface for end-users and is insightful for experts for reconfiguration to individual setups. The GeoWINE achieves promising results in entity label prediction for images on Google Landmarks dataset. The demonstrator is publicly available at http://cleopatra.ijs.si/geowine/. KEYWORDS Geolocation Estimation, Computer Vision, Knowledge Graph. 1 INTRODUCTION Motivation. With the rapid growth of technology, billions of images are appearing on the Internet every day. While surfing the Web one may encounter an image (e.g., depicting a place) and would like to know where it belongs to and what are similar places. Nonetheless, it would be hard to find keywords to query a search engine. With this in mind, reverse-image search is a unique way to browse the Web. Some of existing reverse-image search implemen- tations are Google Images, eBay, Bing, Pinterest, and Alibaba, most of which are mainly based on extracting visual features from the query image or meta information to perform the retrieval task [1]. In this regard, content-based location estimation of images would enhance the retrieval task. There is an increasing research inter- est in exploiting geographical content of images for various tasks like image classification [10], information retrieval [2], and image verification [3]. On the other hand, information retrieval, text and Web mining, as well as knowledge graphs are increasingly inter- twined with the purpose of yielding opportunities for enhanced exploration facilities. In this paper, we propose GeoWINE (Geolocation Wiki-Image- News-Events) to close the gaps between geolocation estimation, information representation in knowledge graphs, and information retrieval. We utilize geolocation estimation as a core task, but also go beyond by including contextual information in order to enrich the information retrieval task. In this regard, the proposed system is an extension of geolocation estimation to highlight its potential by linking the input image to other knowledge sources such as Wikidata [12] and Open Event Knowledge Graph (OEKG) [5]. To the best of our knowledge, this is the first attempt based on image-based geolocation estimation for event and news retrieval in the context of more downstream tasks such as image verification, recommending similar places, or fact-checking. Approach and Contribution. The proposed system is a ge- olocation based multimodal retrieval system that comprises five modules. Given an image as input, GeoWINE applies a state-of-the- art geolocation estimation model as a starting point to retrieve data from Wikidata [ 12], EventRegistry1, and the OEKG [ 5]. The ge- olocation estimation model is responsible for the prediction of the input image coordinates. The second module performs a geospatial query on Wikidata to retrieve all entities of specific types located no farther than a specified radius from the predicted coordinates. Here, the entity types and the radius are given as input to our system. The third module leverages three different image embed- ding representations that are derived from the tasks of geolocation estimation, place recognition, and an ImageNet model for image classification. These embeddings are used to find and rank the most similar entities compared to the input image. Finally, the last two modules retrieve similar news and events from EventRegistry and OEKG. 1http://eventregistry.org/ arXiv:2104.14994v2 [cs.IR] 4 May 2021 Tahmasebzadeh et al. cleopatra.ijs.si/geowine/ Input (Image, types, radius) Map Entity News Events Results Ranked by similarity Frontend Backend UI (React JS) Python Flask App Forward Response Image, Types, Radius Geolocation Estimation Entity Retrieval Embeddings & Similarities News Retrieval Events Retrieval Event RegistryOEKG Wikidata Wikipedia GeoWINE Forward Request Entities, News, Events, Meta info. Step 1 Step 2 Step 3 Step 4Step 5 Figure 1: Overview of the GeoWINE architecture. We evaluate GeoWINE on Google Landmarks dataset [13], where it achieves promising performance in predicting entity labels of query images. GeoWINE enables users to retrieve entities, news, and events related to an image, through a clean and intuitive User Inter- face (UI), with interactive response times. To the best of our knowl- edge, this is the first public and open-source demo for geolocation- based multimodal retrieval through various sources. To facilitate reproducibility and reuse, all material is publicly available2. Use cases. The proposed system provides various search sce- narios each of which could be addressed as a use case. One of the use cases is to assist journalists or social scientists in fact-checking or fake news detection [9]. Given a query image as an input, the system could be used to contextualize the image by retrieving rel- evant documents. First, the geolocation is predicted and then the predicted coordinates are used to query Wikidata [12] to retrieve entities within the selected radius and entity types by the user. In this way, the user can select an entity on the map to get back the relevant news and events in order to check whether the input image is relevant to the retrieved documents or their corresponding images. Another useful application is the verification of images based on the retrieved entities. For instance, given an image of a scenery, the system recommends a ranked list of visually similar places within a selected radius in order to verify the label of the input image. From another perspective the system could also be useful in meta information verification of news. For instance to see if the location claimed of a news matches the location of its image. 2 METHOD In this section, we describe the five modules of the GeoWINE system. The basic functionality is shown in Figure 1. 2.1 Geolocation Estimation Module The goal of geolocation estimation is to predict latitude and longi- tude coordinates where photo was captured, using only the visual 2https://github.com/cleopatra-itn/GeoWINE content without any other metadata. We apply the model3 [8] based on ResNet101 [7] pretrained on a subset of the Yahoo Flickr Creative Commons 100 Million dataset (YFCC100M) [11] with around five million geo-tagged images. In this model, geolocation estimation is treated as a classification problem where the earth is subdivided into geographical cells. This approach exploits the hierarchical representation of multiple partitionings and additionally takes the photo‚Äôs scene category into account, i.e., whether it depicts indoor, natural, or urban setting. 2.2 Geospatial-based Entity Retrieval Module The primary knowledge source for our system is the Wikidata knowledge graph [12] which is a free and open knowledge base that can be read and edited by both humans and machines. Since Wikidata is a rich source for data with semantic relations between entities and gives continuous access using public SPARQL (Simple Protocol and RDF Query Language) endpoint, it meets our require- ments for entity retrieval. 2.2.1 Determining Common Entity Types. In order to determine entity types for the retrieval task, we use an index set from the Google Landmarks dataset [13]. Each image in the index set is linked with the corresponding Wikimedia Commons category, which is also linked with a Wikidata entity. We query the types of Wikidata entities using the instance of (P31) of each image and count their occurrences. The most frequent 35 entity types are selected and clustered in 12 groups where each group contains similar entity types. For instance, Religious building contains entity types such as church building, cathedral, Buddhist temple, shrine, etc. The full list of entity groups is given in Table 1. 2.2.2 Querying Entities from Wikidata. The geolocation estimation module (Section 2.1) outputs the geocoordinates for an input image. In this step, we query Wikidata to retrieve entities of the specified 3https://github.com/TIBHannover/GeoEstimation/tree/pytorch GeoWINE: Geolocation based Wiki, Image, News and Event Retrieval type and located in the selected radius (in kilometers) of the pre- dicted geocoordinates. The user specifies the radius and selects the entity types of interest from the available 12 groups. 2.2.3 Retrieval of Metadata and Images. Eventually, correspond- ing text descriptions and images are downloaded for the retrieved entities. We extract Wikimedia Commons links of the images and the Wikipedia link for each queried entity using MediaWiki and Wikidata query APIs (Application Programming Interface)4. 2.3 Visual Search Embeddings Module To generate the visual representations, state-of-the-art approaches in computer vision are applied. TheGeolocation embedding is taken from the same model used for geolocation estimation [ 8] (Sec- tion 2.1). The Place embedding is another representation based on a ResNet model [7] to recognize 365 distinct places like beach or street. Another embedding applied here is Object embedding based on a ResNet model pre-trained on the ImageNet dataset for image classification [6]. Each of the embeddings is encoded in a vector of size 2048. Eventually, the Combined is resulted from concatenation of all three to get a more general visual representation. These repre- sentations are employed to rank the retrieved entities (Section 2.2.2) based on similarity to the input query image. The similarity be- tween the entity image and the query image is computed using the cosine similarity of their corresponding image embeddings. 2.4 News and Event Retrieval Modules Given the query image, the user can select the corresponding news articles or events for the retrieved entities within the defined radius. We use EventRegistry to retrieve news for each entity, which is a repository of news in different domains, locations and languages. To get relevant news for each selected entity, its Wikidata label is used as a keyword to query the relevant news. Regarding the events, we use OEKG [5], which is a multilingual, event-centric, temporal knowledge graph composed of different datasets linked to Event-centric Knowledge Graph (EventKG) [4]. These datasets come from multiple application domains such as question answering, entity recommendation, and named entity recognition. We use the OEKG endpoint 5 to query the relevant events from the knowledge graph for the selected Wikidata entity. 3 SYSTEM OVERVIEW An overview of the proposed system is shown in Figure 1. The demonstrator consists of a frontend and a Python Flask server as the backend. 3.1 Frontend The frontend was built using React JS (JavaScript), an open-source JavaScript library for user interface components. The page com- prises four main panels: 1) the panel to provide the query image (either by selecting an existing one or uploading new images) along- side the entity type and radius, 2) the map that displays the entity results, 3) the panel that provides entity details, news and events, and 4) the ordered entities based on the similarity to the input image. 4https://www.mediawiki.org/wiki/API:Main_page 5http://oekg.l3s.uni-hannover.de/sparql Figure 2: Snapshots of the demonstration on a sample image. Once the user presses the search button, the provided image, the selected entity type, and the defined radius are sent to the backend. 3.2 Backend In the backend, the request is sent to a Python Flask application in JSON (JavaScript Object Notation) format. Then, the application forwards the request to an initialized instance of GeoWINE, which computes the results using the pipeline components specified in Section 2. Finally, the Flask application sends the computed results back to the frontend in JSON format. 4 DEMO W ALKTHROUGH In this section, we provide an example query in Figure 2 to guide the reader and user through the demo6. Users can upload an image or select from the provided list, spec- ify a radius in kilometers, and select entity types (explained in Section 2.2.1) from the drop-down list and then get the results as il- lustrated in the top left box. Given these inputs, the geocoordinates are predicted and pinpointed on the map (middle box). In addition to the predicted location (the green marker), the corresponding Wikidata entities are represented on the map within the selected radius (blue markers). For the pre-selected images the ground truth geocoordinates are available so that the user can compare the per- formance of the system. In this example, the location is predicted correctly, and the label is Notre-Dame Paris (top right box). In addition to the aforementioned outputs, a list of images is presented at the bottom. These images correspond to the retrieved entities on the map and they are ranked based on similarity to the input image. There are four different options here: similar ge- olocations/entities, similar places, overall similarity by ImageNet embeddings, and combined search. These four different ranked lists correspond to the four different visual representations: Geolocation, Place, Object and Combined respectively (Section 2.3). As shown in 6http://cleopatra.ijs.si/geowine/ Tahmasebzadeh et al. Table 1: The accuracy scores at ùëò ranked results (in [%]) and the number of instances for different input entity groups: Bridge (Br), Historic Site (Hi), Square (Sq), Castle (Ca), Monument (Mo), Museum (Mu), Building (Bu), Religious Building (Rb), Tower (To), Tourist Attraction (Ta), Waterfall (Wa), Skyscraper (Sk). in [%] Br Hi Sq Ca Mo Mu Bu Rb To Ta Wa Sk overall Top 10 90 79 73 86 72 67 86 50 77 81 90 65 84 Top 5 74 70 64 77 68 60 86 50 77 81 85 55 76 Top 1 53 52 27 50 56 40 57 13 46 68 75 35 52 # instances per entity group 19 33 33 22 25 30 14 8 13 31 20 40 275 Figure 2 bottom box, the top-1 similar geolocation is Notre-Dame Paris. Eventually, the user can select one of the retrieved entities on the map and see the relevant news and events (top right box). 5 EV ALUATION In this section, we present an evaluation of the presented demon- strator on a subset of the Google Landmarks dataset. As mentioned in Section 2.2.1, we identified the most frequent entity types and formed 12 distinct groups by clustering the types of similar imagery. Then, we selected one type from each group and downloaded the images, which yielded 13280 images in total. The goal is to see if the system is able to retrieve the corresponding Wikidata [12] entity when the Wikimedia Commons image is given as the query. Initially, we have run the geolocation estimation model on the query images and discarded query images whose estimated geocoordinate errors are above city level (25 kilometres) based on Great Circle Distance (GCD) in comparison with the ground truth geocoordinates. From the remaining 1285 images, we extract geolocation embeddings and rank the retrieved entities based on the similarity to the input image. In order to make the evaluation fair, we discarded all the queries where the image was the same as the Wikidata image by comparing the hash codes of the query and the retrieved images. For the remaining 275 query images, the top-ùëò accuracy is given in Table 1 for different input entity groups. As the Table shows, 84%, 76%, and 52% of the test queries are ranked among the top 10, top 5, and top 1, respectively. Among the top-10 results, Waterfall and Bridge, among the top-5 Building and among top-1 Waterfall entity groups have the highest accuracy. In all the top-k results, Religious building achieves the lowest accuracy. Overall, the experimental results show the feasibility and usability of the whole system for different types of landmarks. 6 CONCLUSIONS In this paper, we have introduced the GeoWINE demonstrator, a geolocation-based multimodal retrieval system aimed at supporting different kinds of applications such as fact-checking, fake news de- tection, or image verification. The system consists of five modules: geolocation estimation, geospatial-based entity retrieval, visual search embeddings, news and event retrieval. The user can provide an image, radius, and types of results (e.g. tourist attraction) as input and receive the image geocoordinates on the map, surround- ing entities from Wikidata, and visually similar images from the retrieved entities. Furthermore, the user can find some relevant news and events based on a selected entity on the map. In order to evaluate the performance of the system, we have used a small subset of the Google Landmark dataset for labeling images. The experiments have shown promising results affirming the potential of the overall system for the applications mentioned above. ACKNOWLEDGMENTS This work was supported by the European Union H2020 founded project CLEOPATRA (ITN, GA. 812997). REFERENCES [1] Hanaa Al-Lohibi, Tahani Alkhamisi, Maha Assagran, Amal Aljohani, and Asia Othaman Aljahdali. 2020. Awjedni: A Reverse-Image-Search Application. ADCAIJ (2020). [2] Jason Armitage, Endri Kacupaj, Golsa Tahmasebzadeh, Swati, Maria Maleshkova, Ralph Ewerth, and Jens Lehmann. 2020. MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities. In CIKM 2020. ACM. [3] Jiaxin Cheng, Yue Wu, Wael AbdAlmageed, and Premkumar Natarajan. 2019. QATM: Quality-Aware Template Matching for Deep Learning. InIEEE Confer- ence on Computer Vision and Pattern Recognition, CVPR 2019 . Computer Vision Foundation / IEEE. [4] Simon Gottschalk and Elena Demidova. 2019. EventKG - the hub of event knowledge on the web - and biographical timeline generation. Semantic Web (2019). [5] Simon Gottschalk, Endri Kacupaj, Sara Abdollahi, Diego Alves, Gabriel Amaral, Elisavet Koutsiana, Tin Kuculo, Daniela Major, Caio Mello, Gullal Singh Cheema, Abdul Sittar, Swati, Golsa Tahmasebzadeh, and Gaurish Thakkar. 2021. OEKG: The Open Event Knowledge Graph. In CLEOPATRA Workshop @ TheWebConf. [6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In CVPR 2016. IEEE Computer Society. [7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity Map- pings in Deep Residual Networks. In ECCV 2016 (Lecture Notes in Computer Science). Springer. [8] Eric M√ºller-Budack, Kader Pustu-Iren, and Ralph Ewerth. 2018. Geolocation Estimation of Photos Using a Hierarchical Model and Scene Classification. In ECCV 2018 (Lecture Notes in Computer Science, Vol. 11216) . Springer, 575‚Äì592. [9] Eric M√ºller-Budack, Jonas Theiner, Sebastian Diering, Maximilian Idahl, and Ralph Ewerth. 2020. Multimodal Analytics for Real-world News using Measures of Cross-modal Entity Consistency. CoRR (2020). [10] Kevin D. Tang, Manohar Paluri, Fei-Fei Li, Robert Fergus, and Lubomir D. Bourdev. 2015. Improving Image Classification with Location Context. In ICCV 2015. IEEE Computer Society. [11] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. 2016. YFCC100M: the new data in multimedia research. Commun. ACM (2016). [12] Denny Vrandecic and Markus Kr√∂tzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. ACM (2014). [13] Tobias Weyand, Andr√© Araujo, Bingyi Cao, and Jack Sim. 2020. Google Land- marks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval. In CVPR 2020. IEEE.