Reliable Decision-Making via Calibration-Oriented Retrieval-Augmented Generation Chaeyun Jang KAIST jcy9911@kaist.ac.kr Deukhwan Cho KAIST macro_boomin@kaist.ac.kr Seanie Lee KAIST lsnfamily02@kaist.ac.kr Hyungi Lee‚Ä† Kookmin University lhk2708@kookmin.ac.kr Juho Lee‚Ä† KAIST juholee@kaist.ac.kr Abstract Recently, Large Language Models (LLMs) have been increasingly used to support various decision-making tasks, assisting humans in making informed decisions. However, when LLMs confidently provide incorrect information, it can lead hu- mans to make suboptimal decisions. To prevent LLMs from generating incorrect information on topics they are unsure of and to improve the accuracy of gener- ated content, prior works have proposed Retrieval Augmented Generation (RAG), where external documents are referenced to generate responses. However, previ- ous RAG methods focus only on retrieving documents most relevant to the input query, without specifically aiming to ensure that the human user‚Äôs decisions are well-calibrated. To address this limitation, we propose a novel retrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG), which ensures that decisions informed by RAG are well-calibrated. Then we empirically validate that CalibRAG improves calibration performance as well as accuracy, compared to other baselines across various datasets. 1 Introduction Large language models [LLMs; 1, 2, 3, 4] have demonstrated remarkable performance on numerous downstream natural language processing (NLP) tasks, leading to their widespread integration into various decision-making processes [5, 6, 7]. However, even with significant increases in model size and the expansion of training datasets, it remains infeasible for LLMs to encode all possible knowledge within their parameters. As a result, the outputs produced by LLMs may not consistently be reliable for important human decision-making processes, potentially overlooking key or hidden details. Additionally, LLMs frequently provide inaccurate or misleading information with a high degree of confidence, a phenomenon referred to ashallucination[8, 9], which can lead humans to make flawed decisions. In addition, Zhou et al. [7] has empirically demonstrated that human users often over-rely on LLM outputs during decision-making processes, and this over-reliance tends to increase in proportion to the model‚Äôs confidence. Here, the model‚Äôs confidence refers to the expression of how certain the model is when asked how confident it is in its answer. Specifically, ‚Ä†Co-corresponding authors 39th Conference on Neural Information Processing Systems (NeurIPS 2025). arXiv:2411.08891v4 [cs.IR] 15 Oct 2025 they have found that for answers with high confidence, users show strong over-reliance regardless of whether the answer is correct or not. These findings highlight that utilizing LLMs without proper calibration of their responses and addressing the frequent occurrence of hallucinations can lead to incorrect decisions in high-stakes tasks such as medical diagnosis and legal reasoning, potentially resulting in severe consequences [10, 11, 12]. Retrieval Augmented Generation (RAG) [13, 14, 15] has emerged as a promising method to address hallucinations, which is one of the two key issues when using LLMs in decision-making [16, 17]. Instead of generating answers directly, RAG retrieves relevant documents from external databases and uses them as an additional context for response generation. This approach supplements the in- formation that LLMs lack, resulting in more accurate and reliable responses. However, the database cannot encompass all information, and the knowledge from world is continuously being updated. In such cases, the retriever may retrieve irrelevant documents, which can distract the LLM and lead to the generation of incorrect answers to the question [18]. Moreover, as described in Sec. 2.2, due to the LLM‚Äôs overconfidence in the retrieved document, they still tend to assign high confidence to its responses even when they are incorrect. Research onuncertainty calibrationhas aimed to address the issue of overconfident outputs in deep neural networks [19, 20, 21]. In image classification, techniques like temperature scaling have proven effective in improving calibration by adjusting logits [22, 23, 24]. However, calibrating LLMs is more challenging due to their sequential token generation and exponentially growing out- put space [21]. Thus, traditional methods like temperature scaling are less effective for long-form generation. To address this, Band et al. [6] proposed a calibration method targeting probabilities associated with user decisions in LLM-generated guidance but noted limitations in calibrating prob- abilities in RAG contexts. To address this issue, we propose the Calibrated Retrieval-Augmented Generation (CalibRAG) framework. CalibRAG allows an LLM using RAG to not only select relevant information to support user decision-making but also provide confidence levels associated with that information by utilizing a forecasting function, ensuring well-calibrated decisions based on the retrieved documents. Here, the forecasting function is the surrogate model that predicts the probability of whether the user‚Äôs decision based on the guidance provided by RAG will be correct. We empirically validate that our CalibRAG significantly improves calibration performance as well as accuracy, compared to other relevant baselines across several datasets. Our contributions can be summarized as follows: ‚Ä¢ We propose the CalibRAG framework, which enables well-calibrated decision-making based on the guidance provided by RAG. ‚Ä¢ We construct a new dataset by creating labels that indicate how much decisions made using re- trieved documents correctly answer the questions, essential for training the forecasting function. ‚Ä¢ We outperform existing uncertainty calibration baselines across various tasks involving RAG con- text in decision-making scenarios. 2 Preliminaries 2.1 Decision Calibration of Long Form Generation As discussed in Sec. 1, since human decision-makers tend to over-rely on the outputs of LLMs during the decision-making process, it is crucial to ensure that the confidence in LLMs‚Äô outputs is well-calibrated. To address this problem, Band et al. [6] proposesdecision calibration, which aims to align the confidence of the model‚Äôs predicted output with the accuracy of the user‚Äôs decision based on the model output. This allows the user to make a reliable decision based on the model‚Äôs confidence. Thus, to achieve this goal, we need to ensure that the model not only generates factual information but also its confidence in the generated responses accurately reflects the likelihood of correctness. To formalize the problem, we introduce the following notations. Letx‚àà Xrepresent the question or task for which a user needs to make a decision (e.g., ‚ÄúShould I take melatonin to help with jet lag after a long flight?‚Äù), and lety‚àà Ydenote the corresponding true answer (e.g., ‚ÄúYes, if taken at a local bedtime.‚Äù). Here,XandYare the set of all possible questions and answers, respectively. Given the questionx, the user provides an open-ended queryq(x)(e.g., ‚ÄúWrite a paragraph about the effects of melatonin on jet lag.‚Äù) to an LLM as a prompt to gather information for the decision making aboutx. The LLM, denoted asM, generates a long-form response to the query, i.e.,z‚àº 2 1 3 5 7 9 Top K Doc 0.5 0.6 0.7 0.8 0.9 1.0Cumulative Accuracy Contriever CalibRAG (a) 0.0 0.2 0.4 0.6 0.8 1.0 Confidence Bin 0.0 0.2 0.4 0.6 0.8 1.0Accuracy in Bin ECE: 0.1085 ACC: 33.76 Base Model (b) 0.0 0.2 0.4 0.6 0.8 1.0 Confidence Bin 0.0 0.2 0.4 0.6 0.8 1.0Accuracy in Bin ECE: 0.2500 ACC: 37.27 Base+RAG Model (c) Figure 1:(a) Cumulative accuracy with the top-K documents on our synthetic validation set (see Sec. 3.3). contriever-msmarcogains 11% compared to top-1 when the top-9 documents are used, showing that the top-1 hit is often not optimal. CalibRAG reaches a higher top-1 accuracy and gains little from additional documents.(b, c) Reliability diagrams on NaturalQA.ForLlama-3.1-8Btrained under the Number baseline (see Sec. 4), adding the retrieved document (c) raises accuracy relative to the no-document baseline (b) but also increases ECE, indicating greater over-confidence. Bar height is the mean accuracy in each confidence bin; darker shading marks bins with more predictions. M(z|q(x)), which serves as the guidance for the decision-making process. For the sake of notational simplicity, unless specified otherwise, we will useqin place ofq(x). Given the questionxand the generated responsez, the user leverages a forecasting functionf:X √ó Z ‚Üí‚àÜ |Y| to assess all possible answersy‚àà Y, where‚àÜ |Y| denotes a simplex over the setYandZis the space of all possible responses fromM. The goal is to use the forecasting functionfto ensure that, given the long-form generated LLM responsez, the user makes calibrated decisions on the question-answer pairs(x, y). Based on this, Band et al. [6] introduces formal definitions for three types of calibrations with varying conditions. For instance, the LLM isconfidence calibrated[25] with respect to the forecasting functionfiffis calibrated on the joint distributionp(x, y, z), that is, for anyŒ≤‚àà[0,1] Pr  y= arg max j‚àà|Y| f(x, z) j |max j‚àà|Y| f(x, z) j =Œ≤  =Œ≤, wheref(x, z) j denotes thej th element of vectorf(x, z). However, the method proposed by Band et al. [6] to tackle this calibration problem has three major limitations. 1) It requires supervised fine-tuning for three different LLMs, including the LLM re- sponsible for generating a responsezand the forecasting functionfparameterized with two LLMs. 2) it further needs proximal policy optimization [PPO; 26] for fine-tuning the LLM for response generation, which is known to suffer from training instability [27]. 3) It cannot be directly applied to calibrate the probabilities associated with the user decisions based on the guidance by RAG. 2.2 Retrieval Augmented Generation (RAG) RAG [13] employs Dense Passage Retrieval [DPR; 28] to retrieve relevant documents for question answering. DPR encodes questions and documents independently, enabling precomputation and in- dexing of document embeddings. At inference time, only the question is embedded and matched against the indexed documents via similarity search. Retrieved documents are then provided as ad- ditional context to an LLM, often improving accuracy of answer. Despite its effectiveness, RAG remains vulnerable to retrieval errors. Since retrievers are typically trained in an unsupervised man- ner [29, 30], their similarity scores do not necessarily reflect the utility of documents for downstream decision-making. As shown in Fig. 1a, the top-ranked document retrieved by Contriever [29] often leads to incorrect predictions, whereas lower-ranked documents may yield better outcomes. Fur- thermore, incorporating irrelevant documents can mislead the LLM, resulting in overconfident but incorrect answers, as illustrated in Fig. 1c. As an alternative to improve retrieval quality, reranking methods [31, 32] have been proposed to reorder candidates based on relevance signals. However, these approaches are typically optimized for ranking metrics (e.g., MRR, NDCG) rather than the correctness of downstream decisions, and thus do not produce calibrated confidence estimates. We provide a detailed discussion of why reranking methods fail to support decision calibration in Sec. A. Existing RAG methods, including those incorporating reranking, lack mechanisms to assess the con- fidence of retrieved documents. Addressing this limitation requires not only identifying documents 3 ùíô: Should I take melatonin to help with jet lag after a long flight?ùíí: Write a paragraph about the effects ofmelatoninon jet lag. Since I have to decide quickly, I'll go with temperature ùë°=1.0 ‚ù∂...melatoninis a natural hormone that regulates the sleep-wake cycle...‚ù∑‚Ä¶melatoninreduced symptoms of Desynchronosiswhen taken at the correct time relative to the new time zone‚Ä¶ CalibRAGselect‚ù∑document(higherdecisionconfidence)z:Melatoninsignificantlyreducessymptomsofjetlagwhenadministeredaccordingtothenewlocaltime.Myconfidencescore(ùíÑ)is84%giventhatyourt=1.0. Rerankerselect‚ù∂document(higherrelevancescore)ùê≥:Melatoninisanaturallyoccurringhormonethatplaysakeyroleinregulatingsleeppatterns. ùë¶: I'm not sure what to do. Is this suggesting I should take melatonin now? ùë¶: I should consider taking melatonin, but only at the correct time based on the new time zone. Retrieverùëû+ {‚ù∂, ‚ù∑} ùëû+{‚ù∂, ‚ù∑} +ùë° Ours(CalibRAG)Reranker w. uncertaintycalibration baseline Rerankerwith CalibRAGwith User User: Is it true? ùíÑ:Yes(confidence:9/10). Figure 2:Comparison between CalibRAG and other reranking methods during inference.In contrast to conventional methods that rely on relevance scores to rerank retrieved documents, CalibRAG leverages a confidence score derived from the user‚Äôs risk tolerancetto guide the reranking process. that better support accurate downstream decisions, but also calibrating the likelihood that such deci- sions are correct given the retrieved context. 3 CalibRAG: RAG for Decision Calibration Overview.When LLMs support user decision-making using RAG, the ability to predict the relia- bility of those decisions in advance can significantly improve the safety and trustworthiness of the overall experience. Given a taskxon which a user must make a decision, we assume an associated open-ended queryq. The retriever then selects a relevant documentdfrom an external database. Conditioned onqandd, the LLMMproduces a long-form guidancezto support the decision. In addition to this guidance, we must also provide a confidence scorec. This score can be obtained in two ways: (i) by askingMto verbalize its own confidence, or (ii) by predicting it using a sep- arately trained forecasting functionf. In our framework, because verbalized confidences fromM are often poorly calibrated, we adopt the latter. The forecasting function outputsc=f(t, q, d), a probability that the user‚Äôs decision will be correct, conditioned on a temperature parametertof the decision-making user,q, andd. While prior work definesf(x, z)over LLM outputs [6], our for- mulationf(t, q, d)predicts correctness beforezis generated. This enables reranking and simplifies supervision. The user ultimately consultszandcto make a decision, and our goal is to ensure that the predicted confidencecis well aligned with the empirical accuracy of decision-making scenarios. Fig. 2rightillustrates the overall usage process of CalibRAG. 3.1 Problem setup To train and evaluate the forecasting functionf, we introduce an LLM-based surrogate user model Uthat can simulate human decision-making without requiring costly human annotations [6]. We construct prompts such thatUmakes decisions by referencing both the guidancezand the con- fidence scorec, deciding whether to accept the guidance based on the confidence (see Sec. G for prompt details). Since the output of the surrogate user modelUis long-form text rather than a class label, we utilize theGPT-4o-minimodel (denoted byG) to automatically evaluate whether the user‚Äôs response matches the correct answery. To construct supervision signals forf, we sample responses fromUacross various decoding tem- peraturest‚àà Tto simulate a range of user behaviors. This temperature reflects behavioral traits of the userU, such as risk tolerance or decision urgency. For instance, if the user needs to make a decision quickly and prefers reliability, they can choose a lowert. On the other hand, if the user has more time and is open to exploring diverse alternatives, they may opt for a highert. For each combination(t, q, d), we collectRresponses and use the proportion of correct ones as a soft label 4 for trainingf. As a result, the label is represented as a probability between[0,1], used as a soft label for binary modeling or extended to a multi-class histogram to support finer-grained calibration. Then, our final goal is to satisfy the following binary calibration condition: Pr [y=U(x, z, f(t, q, d))|f(t, q, d) =Œ≤] =Œ≤,‚àÄŒ≤‚àà[0,1],(1) which means that the predicted confidenceŒ≤should match the actual accuracy of user decisions. As discussed in Sec. 1 and illustrated in Fig. 1c, asking the RAG model to provide its own confidence often leads to excessively high confidence scores, which can cause users to overtrust inaccurate information. In this work, to overcome this limitation, we train a forecasting functionf(t, q, d) instead to satisfy the calibration condition in Eq. 1, using supervision signals derived from self- consistency sampling guided byU. This approach enables better modeling of real user behavior and allows for more effective confidence calibration, ultimately resulting in a safer and more trustworthy decision-making system. 3.2 Modeling and Training To model the forecasting functionf, it is essential to have the capacity to sufficiently analyze the impact of the generated guidancezon the actual decision. For this reason, we use a pre-trained LLMM, responsible for generatingz, as the base feature extractor modelf feat. For details of the feature-extraction process, please refer to Sec. B.4. To incorporate the temperature parametert, we apply a Fourier positional encoding [33] that maps the scalart‚ààRto a2N-dimensional vector as follows: PE(t) =  sin(œâ1t),cos(œâ 1t), . . . ,sin(œâN t),cos(œâ N t)  , œâ n = 2n 2œÄ tmax ‚àít min .(2) whereNis the number of frequencies in the encoding. The resultingPE(t)‚ààR 2N is projected with a learnable matrixW p ‚ààR h√ó2N and then added element-wise to the base featuref feat(q, d). Additionally, to model the probability thatU(x, z, f(t, q, d))leads to a correct decision, we place a linear classification head on top of the extractorf feat derived from the frozen LLMM. For parameter-efficient fine-tuning and to avoid abrupt representation shifts, we keepMfrozen and train only the LoRA adapters [LoRA; 34] and the lightweight head. The resulting forecasting function is defined as: f(t, q, d) :=œÉ W ‚ä§ head (ffeat(concat[q, d];W LoRA) +W p PE(t)) +b head  ,(3) whereœÉ(x) = 1/(1 + exp(‚àíx))is the sigmoid function. Here,W head,b head,W p, and the LoRA parametersW LoRA are all learnable. This formulation allowsfto condition its prediction on both the semantic compatibility of theq-dpair and the user-specific behavior encoded byt, providing an uncertainty-aware estimate of decision correctness. Then we trainfby minimizing the following log-likelihood loss: L=‚àí 1 |S| X (t,q,d,b)‚ààS [blogf(t, q, d) + (1‚àíb) log(1‚àíf(t, q, d))](4) whereSis the training dataset andb‚àà[0,1]is the soft label derived from self-consistency sam- pling. This objective encourages the predicted confidencef(t, q, d)to match the empirical correct- ness probability, enabling the model to generalize across varying decision behaviors encoded byt, while leveraging the LLM‚Äôs latent representations for calibrated prediction. Although the primary formulation uses soft binary labels, we also investigate a multi-class variant where the correctness distribution is discretized into histogram bins and trainfusing a categorical objective. Full experi- mental results and comparisons are provided in Sec. 4. Remark.Among various scoring rules [35, 36, 6] used to measure the forecast quality of functions predicting the true probabilityp, thestrictly proper scoring rulehas the advantage that itsunique maximizeris the true probabilityp. Consequently, training a forecast function using a strictly proper scoring rule as the training objective ensures that the forecasts are learned to be as close as possible to the true probabilitypas the number of training examples increases. Note that the loss in Equation 4 to train our forecast functionfis an example of a strictly proper scoring rule, the logarithmic score. This makes our loss function crucial for trainingfto produce well-calibrated predictions. 5 3.3 Synthetic Supervision Data Generation To conduct the supervised learning discussed in Sec. 3.2, it is essential to construct an appropriate synthetic training datasetSconsisting of the triples(t, q, d, b). We first extract the(x, y)decision- making task pairs from the following three Question Answering datasets: 1) TriviaQA [37], 2) SQuAD2.0 [38], and 3) WikiQA [39] datasets. Then, for everyxin the training dataset, we generate an open-ended queryqbased on eachx, using theGPT-4o-minimodel. At this point, it is important to note that instead of retrieving only the single top documentdwith the highest similarity score from the retriever model for each queryq, we retrieve the top 20 documents. There are two reasons for this. First, as illustrated in Fig. 1a, a large number of low-ranked documents actually help the surrogate user make a correct decision. If we only include the top-1 documents, many of which would be labeled as incorrect, the synthetic dataset would be highly biased to negative samples. Second, using only onedper(x, y)pair for labeling and training could result in the model overfitting to the label without learning the relationship betweenqanddadequately. By pairing the sameqwith variousd‚Äôs, the model can learn from positive and negative samples, improving its ability to generalize. After retrieving multiple documents, we provide each(q, d)pair to the RAG modelM, which generates the guidancezbased ond. Then, the user modelUreceives the taskxand guidancez, and samples responses with the decoding temperaturet, which reflects behavioral variation during sampling. To estimate the reliability of the generated decision, we sampleR= 10fromUat the same temperature and evaluate each using the correctness functionG, which compares the decision with the ground truth answery. The soft labelb‚àà[0,1]is then computed as the proportion of correct responses among theRsamples. Thus, for each(x, y)pair, we generate multiple training quadruples(t, q, d, b), each corresponding to a differenttsetting and document retrieved. Refer to Sec. E for examples of our synthetic data. 3.4 Inference After finishing the training of the forecasting functionf, we perform inference for a new decision taskx ‚àó through the following four stage process: Stage 1: Initial retrieval of documents. Given an open-ended queryq ‚àó, derived from the orig- inal questionx ‚àó, we begin the document retrieval process using the retrieval model. Similarly to the training data generation process, we retrieve the topKrelevant documents from the external database, denoted byD ‚àó :={d ‚àó i }K i=1. The goal of this stage is to construct a diverse set of candidate documents that may contain valuable information for producing the correct answery. Stage 2: Scoring and selection of documents.Once theKcandidate documents are retrieved, we estimate the decision confidence of each document with our trainedt-conditioned forecasting functionf. At inference time, the user may choose $t$ to reflect their decision preference, with lower values for cautious, consistent decisions and higher values for exploratory reasoning. Regardless of the original retrieval similarity score, each document is then reranked by its predicted confidence. Concretely, we sort the documents in descending order of the probabilities{f(t, q ‚àó, d‚àó i )}K i=1, which represent the chance that the user will reach a correct decision if guidancezis generated from each document. The top-ranked document is selected for the next stage. Here, if the predicted probability for the highest-ranked documentd ‚àó falls below a predefined thresholdœµ(defaulted to 0.5, with further details provided in ¬ß D.1), we consider the guidancezinsufficient for a reliable decision at temperaturet. In this case, we move on to Stage 3, where we retrieve a new set ofKcandidate documents to search for documents that provide higher confidence. If this condition is not met, we move forward to Stage 4. Stage 3: Reformulating the query (Optional).If the predicted probability for the highest-ranked documentd ‚àó is lower than a pre-defined thresholdœµin Stage 2, to retrieve a new set ofKcandidate documents, we reformulate our open-ended queryq ‚àó intoq ‚àó‚àó by emphasizing more important con- tent from the questionx. This reformulation focuses on extracting key aspects of the original task, ensuring that the next retrieval attempt targets more relevant and helpful documents. After reformu- lating the query, we repeat Stage 1 and Stage 2 once again. Examples of query reformulation are shown in ¬ß C. Stage 4: Final decision.After retrieving the documentd ‚àó, we generate the guidancez ‚àó using the RAG modelM. Then, the user modelUmakes a decisionU(x ‚àó, z‚àó, f(q ‚àó, d‚àó, t)), with the 6 CT CT-Probe Linguistic Number CalibRAG CalibRAG-multi 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (a) BM25 w/ NQ 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (b) BM25 w/ WebQA 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (c) Contriever w/ NQ 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (d) Contriever w/ WebQA Figure 3:Evaluation results of the baselines and CalibRAG using two retriever models: BM25 and Con- triever on NQ and WebQA.We report four metrics‚Äî1-AUROC, 1-ACC, ECE, and BS‚Äîwherelower values indicate better performance. decoding temperaturetselected in Stage 2. This decision is compared with the correct answery ‚àó byGto determine its accuracy. 4 Experiments Implementation detail.For all experiments, following Sec. 3.3, we collect a total of 20,870 sam- ples for training and 4,125 for validation. All evaluations are conducted in azero-shotsetting on held-out tasks that are disjoint from both the training and validation sets. Unless otherwise speci- fied, we useLlama-3.1-8B[3] as both the RAG modelMand decision modelU. However, we also conduct ablation studies diverse model structures, and the results are presented in Sec. D. To evaluate long-form generated answers, we employGPT-4o-minias the evaluation modelG. Baselines.We compare CalibRAG with the following relevant baselines. ‚Ä¢Uncertainty calibration baselines:(1)Calibration Tuning[21] methods employ a model that outputs probabilities for answering ‚ÄúYes‚Äù or ‚ÄúNo‚Äù to the question, ‚ÄúIs the proposed answer true?‚Äù These probabilities allow us to measure the uncertainty calibration of the response. As baselines, we consider two variants:CT-probe, which adds a classifier head to estimate the probability of correctness, andCT-LoRA, which utilizes the normalized token probability between the tokens ‚ÄúYes‚Äù and ‚ÄúNo.‚Äù (2)Verbalized Confidence Fine-tuning[40, 41, 6] utilizes verbalized tokens to represent the model‚Äôs confidence. In this case, we also consider two baseline variants:Number- LoRA, which expresses confidence as an integer between 0 and 10, andLinguistic-LoRA, which uses linguistic terms (e.g., ‚ÄúDoubtful‚Äù or ‚ÄúLikely‚Äù) to indicate confidence. For all uncertainty cal- ibration baselines, guidance and confidence are generated based on the top-1 document retrieved by the retriever. ‚Ä¢Reranking and Robust RAG baselines:Although CalibRAG is primarily designed to enable well-calibrated decision-making in RAG-guided scenarios, it can also be interpreted as a rerank- ing approach for retrieved documents in downstream tasks as a consequence ofStage 2during inference. Accordingly, we compare CalibRAG against two reranking baselines and one robust RAG baseline: (1)Cross-encoderwith MiniLM-L6-v2, which reranks documents based on the similarity score of the jointly embedded query and documents with cross attention. (2)LLM- rerank[42], which prompts the LLM,GPT-3.5-turbo, to rerank documents by leveraging the relationship between the queryqand the documentd. (3)SelfRAG[43], a robust RAG base- line that dynamically determines the necessity of retrieval and self-evaluates the relevance of a retrieved documentdto the queryq, as well as the usefulness of the generated guidancezforq, using special tokens such as ‚ÄúRetrieve‚Äù, ‚ÄúIsREL‚Äù, ‚ÄúIsSup‚Äù, and ‚ÄúISUSE‚Äù. CalibRAG.Unless otherwise specified, CalibRAG reranks the top-20 documents at inference. The forecasting function is evaluated by marginalizing over a set of six decoding temperatures t‚àà {1.0,1.1, . . . ,1.5}. This is because, in the absence of explicit information about a user‚Äôs prefer- ence, the forecasting function cannot accurately model the user‚Äôs behavior under a single decoding temperature. To reflect this variation, we approximate marginalization by averaging over these six values, as exact integration over all possibletis infeasible. Building on this, CalibRAG-multi ex- tends CalibRAG to a multi-class setting by modeling the correctness histogram across bins (0-10). Evaluation metrics.We evaluate all the models in terms of accuracy, AUROC, and various cali- bration metrics such as Expected Calibration Error [ECE; 44], Brier Score [BS; 45]. For clarity and 7 CT CT-Probe Linguistic Number CalibRAG 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (a) MedCPT w/ BioASQ-Y/N 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (b) MedCPT w/ MMLU-Med 1-ROC 1-ACC ECE BS 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 (c) MedCPT w/ PubMedQA Figure 4:Evaluation results of the baselines and CalibRAG utilizing MedCPT on BioASQ-Y/N, MMLU- Med, and PubMedQA.We report four metrics‚Äî1-AUROC, 1-ACC, ECE, and Brier Score‚Äîwherelower values indicate better performance. Table 1: Comparison of RAG variants across datasets. (a) Comparison of reranking RAG methods. Baseline reranking scores are treated as confidence. Data Method AUROC(‚Üë)ACC(‚Üë)ECE(‚Üì)BS(‚Üì) HotpotQACross-encoder 60.74 34.98 0.477 0.477LLM-rerank 60.57 38.52 0.248 0.297 CalibRAG72.47 42.37 0.106 0.206 (b) Comparison of robust RAG methods using Llama-2-7B asM. Data Method AUROC(‚Üë)ACC(‚Üë)ECE(‚Üì)BS(‚Üì) NQ SelfRAG 48.4 36.2 0.522 0.545CalibRAG63.5 37.4 0.258 0.287 WebQASelfRAG 51.9 39.7 0.478 0.503CalibRAG68.8 40.5 0.217 0.262 consistency, we adopt the 1-AUROC and 1-ACC notation in plots so that all metrics can be inter- preted under the convention that lower values indicate better performance. Details regarding these metrics can be found in Sec. B.3.1. 4.1 Main Results Comparison with uncertainty calibartion baselines.Fig. 3 and Fig. 4 present a comparison of uncertainty-based baselines in both general and specific domain tasks. For the general domain, we evaluated CalibRAG on the Natural QA (NQ) [46] and WebQA [47] datasets. To demonstrate that our method performs well across different retrievers, we conducted experiments using both Contriever [29] and BM25 [48] retrievers. The results in Fig. 3 clearly show that CalibRAG and CalibRAG-multi outperform other baselines in all metrics. In particu- lar, it significantly reduces calibration metrics across all datasets and retriever settings, indicating that CalibRAG effectively calibrates the decision-making process compared to other baselines. Ad- ditionally, while CalibRAG is not explicitly designed to improve accuracy, it naturally identifies documents that are more likely to lead to correct decisions. As a result, it also enhances accuracy compared to other baselines. We also evaluate CalibRAG without reranking, which still shows im- proved calibration metrics compared to both the baselines and the setting where baseline confidence scores are used for reranking. Detailed results can be found in Sec. D. For domain specific evaluation, we assess CalibRAG on the BioASQ-Y/N, MMLU-Med, and Pub- MedQA datasets from the MIRAGE benchmark [49], which focus on the medical domain. Since domain-specific retrievers are necessary in this field, we utilize MedCPT [50], a retriever trained on user click logs on PubMed corpus. Note that the rest of CalibRAG model, including the LLMM and the forecasting function, which have been trained on TriviaQA, SQuAD2.0, and WikiQA, re- mains fixed. Thus, this setup evaluates robustness of CalibRAG when applied to an unseen retriever and out-of-domain datasets. The results in Fig. 3 clearly demonstrate that CalibRAG outperform other baselines across all metrics, even in specialized domain scenarios. Similar to the general do- main, CalibRAG significantly improves calibration metrics across all datasets, showing its ability to effectively calibrate the decision-making process even with an unseen retriever and dataset. Comparison with reranking and robust RAG baselines.CalibRAG retrieves the topKdoc- uments during inference and selects the one most likely to lead to a correct decision, using it to generate guidance and confidence for a well-calibrated decision-making process. To further evalu- ate its effectiveness, we compare CalibRAG against reranking and robust RAG baselines in Table 1a and Table 1b. 8 1.0 1.1 1.2 1.3 1.4 1.5 User model decoding temperature (t) 0.05 0.10 0.15 0.20 0.25 0.30ECE f(t,q,d) f(q,d) (a) Ablation oftconditioning 5 10 20 40 Number of Retrievals (K) 36 38 40 42 44ACC (%) CalibRAG Number + Reranking (b) Number of retrievals BM25 NQ BM25 WebQA Contriever NQ Contriever WebQA 40 42 44 46 48 50ACC (%) Calibrag Calibrag (c) Effect of query reformulation Figure 5:(a)Calibration with and without temperature conditioning on the NQ dataset using Contriever.(b) Effect of the number of retrieved documents on reranking performance on the WebQA dataset using BM25.(c) Impact of query reformulation during inference. First, we conduct a comparison with reranking baselines using the HotpotQA [51] dataset. To as- sess the calibration performance of existing reranking baselines, we use their reranking scores as confidence values. Surprisingly, Table 1a shows that CalibRAG not only improves calibration per- formance compared to the baselines but also enhances accuracy. This result suggests that, while our inference procedure is designed to identify documents that lead to correct decisions, it also ef- fectively retrieves documents that are highly relevant to the queryx. The confidence assignment method for reranking models and the comparison with the BEIR reranking benchmark can be found in Sec. D.2. Next, we compare CalibRAG against a robust RAG baseline using the NQ and WebQA datasets. To evaluate the calibration performance of SelfRAG, we use the ‚ÄúUtility‚Äù score tokens as the con- fidence score. Similar to the reranking baseline comparison, Table 1b demonstrates that CalibRAG consistently outperforms other baselines across all datasets and metrics. 4.2 Ablation Studies Does temperature conditioning improve calibration?To evaluate the impact of temperature- aware modeling, we compare our full modelf(t, q, d)with a temperature-agnostic variantf(q, d) trained without conditioning on user decoding behavior. As shown in Fig. 5a, temperature-aware forecasting significantly improves ECE across user model temperatures. In particular,f(q, d)tends to overestimate confidence under higher-temperature sampling, leading to increased ECE. In con- trast, our proposedf(t, q, d)correctly adapts to user-specific (decoding) behaviors and maintains relatively low ECE across the full temperature range. This confirms that incorporating user variabil- ity via temperature conditioning enables more reliable decision calibration. How does the number of retrieved passages (K) impact reranking?We useK= 20docu- ments for reranking in all experiments, balancing the computational cost and the performance of the decision-making task. To validate our choice, we plot accuracy as a function of the number of doc- uments for reranking in Fig. 5b. The results show that performance improves up to 20 documents, but the gains diminish beyond 40 documents, supporting our choice of 20 documents. Effect of Query ReformulationDuring inference, if none of theKretrieved documents inStage 2are predicted to contribute to a correct decision, CalibRAG can optionally proceed toStage 3, where it reformulates the queryqand retrieves a new set ofKdocuments. To evaluate whether this optional step improves performance, we conducted experiments on the NQ and WebQA datasets using both BM25 and Contriever retrievers. As shown in Fig. 5c, incorporatingStage 3, denoted as CalibRAG‚Ä†, consistently improves performance across all cases. However, this improvement comes at the cost of additional computation, as reformulating the query and retrieving new documents require extra processing. 5 Conclusion In this paper, we introduced CalibRAG, a simple yet effective framework designed to ensure that the RAG-guided decision-making process is well-calibrated. Our experiments demonstrated that CalibRAG significantly enhances QA performance within the RAG setting across various datasets and retriever models. Moreover, ablation studies showed that CalibRAG effectively aligns model 9 confidence with factual correctness, resulting in improved decision-making accuracy and calibra- tion. Overall, CalibRAG stood out as a robust solution for enhancing the reliability of RAG-based LLM guidance in decision-driven scenarios. However, creating synthetic datasets and training the forecasting function for decision calibration may introduce some overhead. Nonetheless, accurately calibrating language model confidence is crucial, making this approach both valid and worthwhile. Acknowledgement This work was supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190075, Artificial Intelligence Graduate School Program(KAIST)). This work was supported by Institute of Infor- mation & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No.RS-2024-00509279, Global AI Frontier Lab). This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (NRF-2022R1A5A708390812). This work was supported by Institute of Information & commu- nications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No.2022-0-00184, Development and Study of AI Technologies to Inexpensively Conform to Evolv- ing Policy on Ethics). This work was supported by the Institute of Information & Communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No.RS- 2025-02219317, AI Star Fellowship(Kookmin University)). This work was supported by Artifi- cial intelligence industrial convergence cluster development project funded by the Ministry of Sci- ence and ICT(MSIT, Korea) & Gwangju Metropolitan City. This research was supported by the MSIT(Ministry of Science, ICT), Korea, under the Global Research Support Program in the Digital Field program(IITP-2024-RS-2024-00417958) supervised by the IITP(Institute for Information & Communications Technology Planning & Evaluation). 10 References [1] Albert Q Jiang et al. ‚ÄúMistral 7B‚Äù. In:arXiv preprint arXiv:2310.06825(2023) (cit. on p. 1). [2] Hugo Touvron et al. ‚ÄúLlama 2: Open foundation and fine-tuned chat models‚Äù. In:arXiv preprint arXiv:2307.09288(2023) (cit. on p. 1). [3] Abhimanyu Dubey et al. ‚ÄúThe llama 3 herd of models‚Äù. In:arXiv preprint arXiv:2407.21783 (2024) (cit. on pp. 1, 7). [4] Josh Achiam et al. ‚ÄúGPT-4 technical report‚Äù. In:arXiv preprint arXiv:2303.08774(2023) (cit. on pp. 1, 22). [5] Rishi Bommasani et al. ‚ÄúOn the opportunities and risks of foundation models‚Äù. In:arXiv preprint arXiv:2108.07258(2021) (cit. on p. 1). [6] Neil Band et al. ‚ÄúLinguistic Calibration of Long-Form Generations‚Äù. In:Forty-first Interna- tional Conference on Machine Learning. 2024 (cit. on pp. 1‚Äì5, 7, 34). [7] Kaitlyn Zhou et al. ‚ÄúRelying on the Unreliable: The Impact of Language Models‚Äô Reluctance to Express Uncertainty‚Äù. In:arXiv preprint arXiv:2401.06730(2024) (cit. on p. 1). [8] Terry Yue Zhuo et al. ‚ÄúExploring ai ethics of chatgpt: A diagnostic analysis‚Äù. In:arXiv preprint arXiv:2301.1286710.4 (2023) (cit. on p. 1). [9] Theodore Papamarkou et al. ‚ÄúPosition paper: Bayesian deep learning in the age of large-scale ai‚Äù. In:arXiv preprint arXiv:2402.00809(2024) (cit. on pp. 1, 22). [10] Margaret Li et al. ‚ÄúDon‚Äôt say that! making inconsistent dialogue unlikely with unlikelihood training‚Äù. In:arXiv preprint arXiv:1911.03860(2019) (cit. on p. 2). [11] Wei Li et al. ‚ÄúFaithfulness in natural language generation: A systematic survey of analysis, evaluation and optimization methods‚Äù. In:arXiv preprint arXiv:2203.05227(2022) (cit. on p. 2). [12] Tessa Han et al. ‚ÄúTowards safe large language models for medicine‚Äù. In:ICML 2024 Work- shop on Models of Human Feedback for AI Alignment. 2024 (cit. on p. 2). [13] Patrick Lewis et al. ‚ÄúRetrieval-augmented generation for knowledge-intensive nlp tasks‚Äù. In: Advances in Neural Information Processing Systems33 (2020), pp. 9459‚Äì9474 (cit. on pp. 2, 3). [14] Huayang Li et al. ‚ÄúA survey on retrieval-augmented text generation‚Äù. In:arXiv preprint arXiv:2202.01110(2022) (cit. on p. 2). [15] Calvin Wang et al. ‚ÄúPotential for GPT technology to optimize future clinical decision-making using retrieval-augmented generation‚Äù. In:Annals of Biomedical Engineering52.5 (2024), pp. 1115‚Äì1118 (cit. on p. 2). [16] Kurt Shuster et al. ‚ÄúRetrieval augmentation reduces hallucination in conversation‚Äù. In:arXiv preprint arXiv:2104.07567(2021) (cit. on p. 2). [17] Jiarui Li, Ye Yuan, and Zehua Zhang. ‚ÄúEnhancing llm factual accuracy with rag to counter hal- lucinations: A case study on domain-specific queries in private knowledge-bases‚Äù. In:arXiv preprint arXiv:2403.10446(2024) (cit. on p. 2). [18] Freda Shi et al. ‚ÄúLarge language models can be easily distracted by irrelevant context‚Äù. In: International Conference on Machine Learning (ICML)(2023) (cit. on p. 2). 11 [19] V olodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. ‚ÄúAccurate uncertainties for deep learning using calibrated regression‚Äù. In:International conference on machine learning. PMLR. 2018, pp. 2796‚Äì2804 (cit. on p. 2). [20] Max-Heinrich Laves et al. ‚ÄúWell-calibrated regression uncertainty in medical imaging with deep learning‚Äù. In:Medical imaging with deep learning. PMLR. 2020, pp. 393‚Äì412 (cit. on p. 2). [21] Sanyam Kapoor et al. ‚ÄúCalibration-Tuning: Teaching Large Language Models to Know What They Don‚Äôt Know‚Äù. In:Proceedings of the 1st Workshop on Uncertainty-Aware NLP (Uncer- taiNLP 2024). 2024, pp. 1‚Äì14 (cit. on pp. 2, 7, 36). [22] Meelis Kull et al. ‚ÄúBeyond temperature scaling: Obtaining well-calibrated multi-class prob- abilities with dirichlet calibration‚Äù. In:Advances in Neural Information Processing Systems 32 (NeurIPS 2019). 2019 (cit. on p. 2). [23] Juozas Vaicenavicius et al. ‚ÄúEvaluating model calibration in classification‚Äù. In:The 22nd international conference on artificial intelligence and statistics. PMLR. 2019, pp. 3459‚Äì3467 (cit. on p. 2). [24] Matthias Minderer et al. ‚ÄúRevisiting the calibration of modern neural networks‚Äù. In:Advances in Neural Information Processing Systems34 (2021), pp. 15682‚Äì15694 (cit. on p. 2). [25] Chuan Guo et al. ‚ÄúOn calibration of modern neural networks‚Äù. In:International conference on machine learning. PMLR. 2017, pp. 1321‚Äì1330 (cit. on pp. 3, 22). [26] John Schulman et al. ‚ÄúProximal policy optimization algorithms‚Äù. In:arXiv preprint arXiv:1707.06347(2017) (cit. on p. 3). [27] Banghua Zhu et al. ‚ÄúFine-tuning language models with advantage-induced policy alignment‚Äù. In:arXiv preprint arXiv:2306.02231(2023) (cit. on p. 3). [28] Vladimir Karpukhin et al. ‚ÄúDense passage retrieval for open-domain question answering‚Äù. In: arXiv preprint arXiv:2004.04906(2020) (cit. on p. 3). [29] Gautier Izacard et al. ‚ÄúUnsupervised dense information retrieval with contrastive learning‚Äù. In:arXiv preprint arXiv:2112.09118(2021) (cit. on pp. 3, 8). [30] Zhuoran Jin et al. ‚ÄúInstructor: Instructing unsupervised conversational dense retrieval with large language models‚Äù. In:Findings of the Association for Computational Linguistics: EMNLP 2023. 2023, pp. 6649‚Äì6675 (cit. on p. 3). [31] Gautier Izacard and Edouard Grave. ‚ÄúDistilling knowledge from reader to retriever for ques- tion answering‚Äù. In:arXiv preprint arXiv:2012.04584(2020) (cit. on p. 3). [32] Xi Victoria Lin et al. ‚ÄúRa-dit: Retrieval-augmented dual instruction tuning‚Äù. In:The Twelfth International Conference on Learning Representations. 2023 (cit. on p. 3). [33] Ashish Vaswani et al. ‚ÄúAttention is all you need‚Äù. In:Advances in neural information pro- cessing systems30 (2017) (cit. on p. 5). [34] Edward J Hu et al. ‚ÄúLoRA: Low-Rank Adaptation of Large Language Models‚Äù. In:Proceed- ings of the International Conference on Learning Representations (ICLR). 2021 (cit. on p. 5). [35] Tilmann Gneiting and Adrian E Raftery. ‚ÄúStrictly proper scoring rules, prediction, and esti- mation‚Äù. In:Journal of the American statistical Association102 (2007) (cit. on p. 5). [36] Leonard J Savage. ‚ÄúElicitation of personal probabilities and expectations‚Äù. In:Journal of the American Statistical Association66.336 (1971), pp. 783‚Äì801 (cit. on p. 5). 12 [37] Mandar Joshi et al. ‚ÄúTriviaQA: A large scale distantly supervised challenge dataset for reading comprehension‚Äù. In:arXiv preprint arXiv:1705.03551(2017) (cit. on pp. 6, 23). [38] Pranav Rajpurkar, Robin Jia, and Percy Liang. ‚ÄúKnow what you don‚Äôt know: Unanswerable questions for SQuAD‚Äù. In:arXiv preprint arXiv:1806.03822(2018) (cit. on pp. 6, 23). [39] Yi Yang, Wen-tau Yih, and Christopher Meek. ‚ÄúWikiQA: A challenge dataset for open- domain question answering‚Äù. In:Proceedings of the 2015 conference on empirical methods in natural language processing. 2015, pp. 2013‚Äì2018 (cit. on pp. 6, 23). [40] Katherine Tian et al. ‚ÄúJust Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback‚Äù. In:Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023, pp. 5433‚Äì 5442 (cit. on pp. 7, 22, 26). [41] Miao Xiong et al. ‚ÄúCan LLMs Express Their Uncertainty? An Empirical Evaluation of Con- fidence Elicitation in LLMs‚Äù. In:The Twelfth International Conference on Learning Repre- sentations. 2024 (cit. on p. 7). [42] Weiwei Sun et al. ‚ÄúIs ChatGPT good at search? investigating large language models as re- ranking agents‚Äù. In:arXiv preprint arXiv:2304.09542(2023) (cit. on pp. 7, 22). [43] Akari Asai et al. ‚ÄúSelf-RAG: Learning to Retrieve, Generate, and Critique through Self- Reflection‚Äù. In:The Twelfth International Conference on Learning Representations. 2023 (cit. on pp. 7, 22). [44] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. ‚ÄúObtaining well calibrated probabilities using bayesian binning‚Äù. In:Association for the Advancement of Artificial Intel- ligence (AAAI)(2015) (cit. on pp. 7, 23). [45] Glenn W Brier. ‚ÄúVerification of forecasts expressed in terms of probability‚Äù. In:Monthly weather review78.1 (1950), pp. 1‚Äì3 (cit. on pp. 7, 24). [46] Tom Kwiatkowski et al. ‚ÄúNatural Questions: A Benchmark for Question Answering Re- search‚Äù. In:Transactions of the Association for Computational Linguistics7 (2019). Ed. by Lillian Lee et al., pp. 452‚Äì466.DOI:10 . 1162 / tacl _ a _ 00276.URL:https : //aclanthology.org/Q19-1026(cit. on pp. 8, 23). [47] Yingshan Chang et al. ‚ÄúWebQA: Multihop and multimodal QA‚Äù. In:Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022, pp. 16495‚Äì16504 (cit. on pp. 8, 23). [48] Stephen E Robertson and Steve Walker. ‚ÄúSome simple effective approximations to the 2- poisson model for probabilistic weighted retrieval‚Äù. In:SIGIR‚Äô94: Proceedings of the Seven- teenth Annual International ACM-SIGIR Conference on Research and Development in Infor- mation Retrieval, organised by Dublin City University. Springer. 1994, pp. 232‚Äì241 (cit. on p. 8). [49] Guangzhi Xiong et al. ‚ÄúBenchmarking Retrieval-Augmented Generation for Medicine‚Äù. In: Findings of the Association for Computational Linguistics: ACL 2024. Ed. by Lun-Wei Ku, Andre Martins, and Vivek Srikumar. Bangkok, Thailand: Association for Computational Lin- guistics, Aug. 2024, pp. 6233‚Äì6251.DOI:10.18653/v1/2024.findings-acl.372. URL:https://aclanthology.org/2024.findings- acl.372/(cit. on pp. 8, 23). [50] Qiao Jin et al. ‚ÄúMedCPT: Contrastive Pre-trained Transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval‚Äù. In:Bioinformatics(2023) (cit. on p. 8). 13 [51] Zhilin Yang et al. ‚ÄúHotpotQA: A Dataset for Diverse, Explainable Multi-hop Question An- swering‚Äù. In:Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018, pp. 2369‚Äì2380 (cit. on pp. 9, 23). [52] Miao Xiong et al. ‚ÄúCan llms express their uncertainty? an empirical evaluation of confidence elicitation in llms‚Äù. In:arXiv preprint arXiv:2306.13063(2023) (cit. on pp. 22, 26). [53] Yaoyiran Li et al. ‚ÄúImproving bilingual lexicon induction with cross-encoder reranking‚Äù. In: arXiv preprint arXiv:2210.16953(2022) (cit. on p. 22). [54] Adam Paszke et al. ‚ÄúPytorch: An imperative style, high-performance deep learning library‚Äù. In:Advances in Neural Information Processing Systems (NeurIPS). 2019 (cit. on p. 22). [55] Thomas Wolf et al. ‚ÄúHuggingFace‚Äôs Transformers: State-of-the-art natural language process- ing‚Äù. In:arXiv preprint arXiv:1910.03771(2019) (cit. on p. 22). [56] Pranav Rajpurkar et al. ‚ÄúSQuAD: 100,000+ Questions for Machine Comprehension of Text‚Äù. In:Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process- ing. 2016, pp. 2383‚Äì2392 (cit. on p. 23). [57] Anastasia Krithara et al. ‚ÄúBioASQ-QA: A manually curated corpus for Biomedical Question Answering‚Äù. In:Scientific Data10.1 (2023), p. 170 (cit. on p. 23). [58] Marah Abdin et al. ‚ÄúPhi-4 technical report‚Äù. In:arXiv preprint arXiv:2412.08905(2024) (cit. on pp. 26, 27). [59] Daya Guo et al. ‚ÄúDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforce- ment Learning‚Äù. In:arXiv preprint arXiv:2501.12948(2025) (cit. on pp. 26, 27). 14 NeurIPS Paper Checklist 1.Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper‚Äôs contributions and scope? Answer: [Yes] Justification: The main claim, CalibRAG ensures that decisions informed by RAG are well calibrated, is clearly stated in both the abstract and introduction. It is also consistently supported by the method‚Äôs design and empirical results. Guidelines: ‚Ä¢ The answer NA means that the abstract and introduction do not include the claims made in the paper. ‚Ä¢ The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. ‚Ä¢ The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. ‚Ä¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2.Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discusses limitations in Sec. 5, noting that synthetic dataset con- struction and training of the forecasting function introduce some overhead. Guidelines: ‚Ä¢ The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper. ‚Ä¢ The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The au- thors should reflect on how these assumptions might be violated in practice and what the implications would be. ‚Ä¢ The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. ‚Ä¢ The authors should reflect on the factors that influence the performance of the ap- proach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. ‚Ä¢ The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. ‚Ä¢ If applicable, the authors should discuss possible limitations of their approach to ad- dress problems of privacy and fairness. ‚Ä¢ While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren‚Äôt acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an impor- tant role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3.Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? 15 Answer: [NA] Justification: The paper does not present formal theoretical results, such as theorems or proofs. While it includes mathematical formulations(1, 2, 3, 4) to define the calibration objective and forecasting function, these are used to describe the method rather than to prove theoretical guarantees. Guidelines: ‚Ä¢ The answer NA means that the paper does not include theoretical results. ‚Ä¢ All the theorems, formulas, and proofs in the paper should be numbered and cross- referenced. ‚Ä¢ All assumptions should be clearly stated or referenced in the statement of any theo- rems. ‚Ä¢ The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. ‚Ä¢ Inversely, any informal proof provided in the core of the paper should be comple- mented by formal proofs provided in appendix or supplemental material. ‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced. 4.Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclu- sions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper provides detailed descriptions of the training (Sec. 3.2) and in- ference pipelines (Sec. 3.4), includes mathematical formulations for key components (e.g. forecasting functionf(t, q, d)), and outlines dataset construction procedures (Sec. 3.3). It also describes experimental settings, and uses widely available models and datasets (Sec. 4, Sec. B). Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ‚Ä¢ If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. ‚Ä¢ Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ‚Ä¢ While NeurIPS does not require releasing code, the conference does require all sub- missions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to re- produce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 16 (d) We recognize that reproducibility may be tricky in some cases, in which case au- thors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5.Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code and data are included in the supplementary materials, with full instruc- tions to reproduce the main results. Guidelines: ‚Ä¢ The answer NA means that paper does not include experiments requiring code. ‚Ä¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. ‚Ä¢ While we encourage the release of code and data, we understand that this might not be possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ‚Ä¢ The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. ‚Ä¢ The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. ‚Ä¢ The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ‚Ä¢ At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ‚Ä¢ Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6.Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper provides all necessary training and test details, including data splits, optimizer, hyperparameters, and model settings, primarily in Sec. B. Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ‚Ä¢ The full details can be provided either with the code, in appendix, or as supplemental material. 7.Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropri- ate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports error bars (standard deviation across random seeds) in most main tables and figures, and explains how they were computed in Sec. D.8. Guidelines: 17 ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ The authors should answer "Yes" if the results are accompanied by error bars, confi- dence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ‚Ä¢ The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). ‚Ä¢ The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) ‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors). ‚Ä¢ It should be clear whether the error bar is the standard deviation or the standard error of the mean. ‚Ä¢ It is OK to report 1-sigma error bars, but one should state it. The authors should prefer- ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. ‚Ä¢ For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). ‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8.Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper specifies that experiments were conducted using NVIDIA RTX 3090 and A6000 GPUs (Sec. B), providing sufficient information on the compute resources. Guidelines: ‚Ä¢ The answer NA means that the paper does not include experiments. ‚Ä¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ‚Ä¢ The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ‚Ä¢ The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn‚Äôt make it into the paper). 9.Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conform with the NeurIPS Code of Ethics, with no identified violations or ethical concerns. Guidelines: ‚Ä¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. ‚Ä¢ If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ‚Ä¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid- eration due to laws or regulations in their jurisdiction). 10.Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] 18 Justification: The paper discusses potential harms of over-reliance on LLMs and motivates CalibRAG as a method to mitigate such risks, especially in high-stakes decision-making tasks such as medical diagnosis and legal reasoning. Guidelines: ‚Ä¢ The answer NA means that there is no societal impact of the work performed. ‚Ä¢ If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ‚Ä¢ Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact spe- cific groups), privacy considerations, and security considerations. ‚Ä¢ The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ‚Ä¢ The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ‚Ä¢ If there are negative societal impacts, the authors could also discuss possible mitiga- tion strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11.Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not release any data or models with high risk for misuse, and thus no specific safeguards are required. Guidelines: ‚Ä¢ The answer NA means that the paper poses no such risks. ‚Ä¢ Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by re- quiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ‚Ä¢ Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. ‚Ä¢ We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. 12.Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper properly cited all external datasets and models in bibliography and properly respected their terms of use as described in Sec. B. Guidelines: ‚Ä¢ The answer NA means that the paper does not use existing assets. 19 ‚Ä¢ The authors should cite the original paper that produced the code package or dataset. ‚Ä¢ The authors should state which version of the asset is used and, if possible, include a URL. ‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset. ‚Ä¢ For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. ‚Ä¢ If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets,paperswithcode.com/ datasetshas curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. ‚Ä¢ For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ‚Ä¢ If this information is not available online, the authors are encouraged to reach out to the asset‚Äôs creators. 13.New assets Question: Are new assets introduced in the paper well documented and is the documenta- tion provided alongside the assets? Answer: [Yes] Justification: The paper introduces both a new synthetic supervision dataset and a trained CalibRAG model designed to forecast decision correctness with temperature-aware con- ditioning. Their construction and usage are well documented in Sec. 3.2, Sec. 3.3, and Sec. B. Guidelines: ‚Ä¢ The answer NA means that the paper does not release new assets. ‚Ä¢ Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. ‚Ä¢ The paper should discuss whether and how consent was obtained from people whose asset is used. ‚Ä¢ At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14.Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the pa- per include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ‚Ä¢ Including this information in the supplemental material is fine, but if the main contri- bution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ‚Ä¢ According to the NeurIPS Code of Ethics, workers involved in data collection, cura- tion, or other labor should be paid at least the minimum wage in the country of the data collector. 15.Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? 20 Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects and thus does not require IRB approval. Guidelines: ‚Ä¢ The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ‚Ä¢ Depending on the country in which research is conducted, IRB approval (or equiva- lent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. ‚Ä¢ We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. ‚Ä¢ For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16.Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] Justification: The paper centrally uses LLMs for generating guidancez(M), simulat- ing user decisions (U), and evaluating the user response (G), as described in Sec. 3.1 and Sec. 3.4. Guidelines: ‚Ä¢ The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. ‚Ä¢ Please refer to our LLM policy (https://neurips.cc/Conferences/ 2025/LLM) for what should or should not be described. 21 A Related Works A.1 Uncertainty Calibration in Language Models Traditional calibration methods rely on token-level log probabilities [25], but modern LLMs gen- erate text autoregressively by multiplying conditional probabilities [4]. Estimating semantic-level probabilities would require marginalization over all possible sequences, which is computationally intractable. As a result, token-level probabilities often fail to provide reliable confidence estimates for long-form text generation. Prompt-based approaches aim to address this problem by eliciting verbalized confidence scores [40, 52]. For example, a model can be prompted with:‚ÄúExpress your confidence as a number between 0 and 100. ‚ÄùIf it responds with‚Äú90‚Äù, this value is interpreted as its confidence level. However, LLMs often exhibit overconfidence in zero-shot settings, resulting in poorly calibrated outputs [9]. Although RAG can mitigate this issue, when the retrieved context is unreliable, LLM may still demonstrate overconfidence, leading to misleading conclusions. Addressing this challenge remains essential for improving LLM reliability in complex decision-making tasks. A.2 Methods for Enhancing RAG Robustness Recent advancements in reranking for RAG have largely focused on enhancing the relevance of retrieved documents with respect to the input query. For example, LLM-based rerankers leverage semantic representations to reorder documents based on their relevance [42], while cross-encoder- based rerankers jointly encode query-document pairs to model their interaction more precisely [53]. These approaches are highly effective in improving retrieval relevance and downstream QA perfor- mance. However, they are fundamentally designed to rank documents by relevance, not to assess how the retrieved information influences the correctness of the final user decision based on the LLM- generated answer. Thus, the resulting scores, although often normalized between 0 and 1, are not calibrated probabilities of correctness and cannot be directly used for decision calibration. Similarly, Self-RAG [43] introduces the notion of utility scores for retrieved documents to identify potentially helpful content. While this provides a signal for filtering noisy documents, the utility score reflects plausibility rather than empirical correctness. As such, these scores are neither opti- mized for nor aligned with standard calibration metrics such as ECE, NLL, or Brier Score. In contrast, our approach directly addresses this gap by training a forecasting function to output calibrated confidence scores that reflect the actual correctness of decisions made by a surrogate user model. We explicitly supervise the forecasting function using binary labels that indicate whether the model‚Äôs final prediction is correct, and optimize this function using strictly proper scoring rules. This ensures that the predicted confidence scores match the empirical likelihood of correctness, thus enabling true decision calibration rather than merely relevance estimation. This fundamental difference in supervisionsignal(relevance vs. correctness) andobjective(ranking vs. calibration) delineates the core novelty of our work from prior reranking-based approaches. By aligning the model‚Äôs confidence estimates with empirical decision accuracy, our method offers a principled and interpretable framework for improving trustworthiness in RAG systems. B Experimental details Our implementation builds on key libraries such as PyTorch 2.1.2 [54], Hugging Face Trans- formers 4.45.1 [55], and PEFT 0.7.1, 3 providing a robust foundation for experimentation. We employ theLlama-3.1-8B-Instructmodel, an open-source multilingual LLM available on Hugging Face. 4 Our experiments are conducted on NVIDIA RTX 3090 and RTX A6000 GPUs. Additionally, we utilize the officialfacebookresearch-contrieverrepository 5 and theelastic-research-bm25repository 6 for our retrieval model. We also useMedMCT 3https://github.com/huggingface/peft 4https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct 5https://github.com/facebookresearch/contriever 6https://www.elastic.co/ 22 based on the MedRAG framework. 7 For training calibration tuning baselines, we reference the calibration-tuningrepository. 8 B.1 Datatsets Train DatasetsSQuAD [56, 38] is a reading comprehension dataset sourced from Wikipedia, containing questions answered by text spans from the articles. WikiQA [39] is a question-sentence pair dataset from Wikipedia, designed for open-domain question answering and includes unanswer- able questions for research on answer triggering. TriviaQA [37] is a reading comprehension dataset with questions authored by trivia enthusiasts, paired with evidence documents from Wikipedia and other web sources. We randomly sampled 10,000 data points each from TriviaQA and SQuAD2.0, and collected all 873 training samples from WikiQA. In addition, we incorporated non-overlapping samples from SQuAD1.0, resulting in a combined training dataset of 61,886 examples after dedu- plication. For the validation set, we gathered 2,000 samples each from TriviaQA and SQuAD, along with 126 samples from WikiQA, and added non-overlapping samples from SQuAD1.0, yielding a total of 12,643 validation data points. All null values were removed prior to finalization. We downloaded all these datasets in Hugging Face datasets 9. For the construction of the labeled datasetSused to train the forecasting function of CalibRAG, we sample a temperaturet‚àºUniform[1.0,2.0]for each queryqand retrieved documentd. For each triplet(t, q, d), we perform user decoding 10 times and assign a soft labelbindicating the ratio of generated answers that contain the ground truth. The final datasetSthus consists of tuples in the form(t, q, d, b).The dataset will be made publicly available upon the publication of this work. Evaluation DatasetsFor zero-shot evaluation, we employ several datasets covering diverse do- mains and question types. HotpotQA [51] is a multi-hop question-answering dataset requir- ing reasoning across multiple supporting documents from Wikipedia to find answers, emphasiz- ing a more complex retrieval and reasoning process. WebQA [47] is an open-domain question- answering dataset consisting of natural, conversational questions paired with web documents, target- ing real-world, context-rich scenarios. Natural Questions (NQ) [46] is another large-scale question- answering dataset, designed to answer questions based on Wikipedia articles, containing both long- form and short-form answers. These datasets are used without additional training, providing a ro- bust evaluation of the generalization capabilities of CalibRAG across different domains and question types. We also evaluate domain-specific datasets, including BioASQ [57], a biomedical QA dataset con- taining factoid, list, and yes/no questions derived from PubMed articles, as well as Medical Infor- mation Retrieval-Augmented Generation Evaluation (MIRAGE) [49] and a textbook corpus. B.2 Hyperparameters Table 2 outlines the hyperparameters used for training the base model and LoRA, including key parameters such as learning rate, batch size, and LoRA-specific settings like rank and alpha. B.3 Evaluation metrics To evaluate long-form text, we utilizedgpt-4o-minito compare the ground-truth answers with the predicted answers in all cases. Based on this comparison, we labeled each instance as correct or incorrect accordingly. B.3.1 Calibration metrics ‚Ä¢Expected Calibration Error[ECE; 44]: ECE= MX m=1 |Bm| n |acc(Bm)‚àíconf(B m)| 7https://github.com/Teddy-XiongGZ/MedRAG 8https://github.com/activatedgeek/calibration-tuning 9https://github.com/huggingface/datasets 23 Table 2: Hyperparameters for LLM Training Base Model Hyperparameters LoRA Hyperparameters Hyperparameter Value Hyperparameter Value Learning Rate{10 ‚àí4,10 ‚àí5}LoRA Rank 8 Batch Size{1,4}LoRA Alpha 16 Max Steps 10,000 LoRA Dropout 0.1 Optimizer AdamW Dropout Rate 0.0 Gradient Accumulation Steps [1, 4] Weight Decay 0.01 Gradient Clipping 1.0 Warmup Steps 500 Scheduler Linear whereB m is the set of predictions in binm, acc(B m)is the accuracy, and conf(B m)is the average confidence of the predictions in that bin. ECE measures how well the model‚Äôs predicted probabilities are calibrated. ‚Ä¢Brier Score[BS; 45]: BS= 1 N NX i=1 (fi ‚àíy i)2 wheref i is the predicted probability andyi is the true label. BS combines both the accuracy and confidence of the predictions, penalizing overconfident and underconfident predictions. ‚Ä¢Negative Log Likelihood (NLL): NLL=‚àí 1 N NX i=1 logp(y i |x i) wherep(y i |x i)is the probability assigned to the correct classy i given inputx i. NLL evaluates the model‚Äôs probabilistic predictions and lower values indicate better calibration. B.4 CalibRAG Details Feature extraction details.To extract features for the forecasting function, we use the hidden state of the last token from the second-to-last layer of the LLMM, as it empirically yielded bet- ter calibration performance than other layers. This hidden state serves as the input representation ffeat(q, d)for the classifier. Positional encoding details.We use a Fourier positional encoding withN= 6frequency com- ponents to encode the temperature parametert. This encoding covers the ranget‚àà[1.0,2.0], and during training data construction, we sampletuniformly from this range to simulate diverse user behaviors. C Examples of query reformulations In CalibRAG, the initial query is generated to simulate how a human decision-maker might pose a simple query based on the input. For example, a decision-maker faced with a problem such as "Is a tomato a fruit or a vegetable?" might craft a straightforward query like "Classification of tomatoes" to query a language model. Using this setup, we employed an LLM generator to create simple yet relevant queries and retrieved documents based on these queries. If the retrieved documents were insufficiently informative, the query was reformulated in Stage 3. This reformulation emphasized key terms to refine the query and improve the quality of retrieved documents. The specific prompt used for this process is detailed in ¬ß G. 24 Table 3: Examples of Query Reformulation Case Original Query Reformulated Query 1 Write a paragraph about the effect of TRH on my- ocardial contractility. Write a paragraph about the effect of Thyrotropin- Releasing Hormone (TRH) on myocardial con- tractility. 2 Write a paragraph about the clinical trials for off- label drugs in neonates as cited in the literature. Write a paragraph about clinical trials for off-label drug use in neonates as reported in the medical literature. 3 Write a paragraph about the current representa- tives from Colorado. Write a paragraph about the current representa- tives from the state of ‚ÄúColorado‚Äù in the United States. 4 Write a paragraph about the current minister of lo- cal government in Zimbabwe and their role within the government. Write a paragraph about the current Minister of Local Government and Public Works in Zim- babwe and their role within the government. Table 4: Effect of Threshold Selection on Performance. Experiments on the BioASQ dataset show how in- creasingœµaffects accuracy and calibration metrics. œµ AUROC ACC ECE BS 0.0 71.21¬±0.8335.03¬±0.140.2500¬±0.010.2900¬±0.01 0.4 76.15¬±1.5035.05¬±0.250.2608¬±0.000.2830¬±0.00 0.5 76.50¬±4.9835.98¬±0.380.2667¬±0.000.2779¬±0.01 0.6 77.20¬±4.1036.50¬±0.450.2707¬±0.000.2800¬±0.01 To help readers understand the transformation from the initial query to its reformulated version, Table 3 provides examples that illustrate how queries evolve during the refinement process, offering practical insights into the mechanism. D Additional experiments D.1 Analysis ofœµ In our experiments,œµwas set as a balanced choice to manage the trade-off between accuracy and cal- ibration error. As shown in Table 4, increasingœµresults in retrieving a larger number of new queries, incorporating more relevant information, and thereby improving accuracy. However, this increase can potentially lead to higher calibration errors. Specifically, while better retrieval enhanced predic- tion accuracy, the confidence scores for these predictions only increased marginally. This mismatch between improved accuracy and relatively low confidence resulted in underconfident predictions, which contributed to a slight increase in calibration error. To assess the impact of differentœµvalues on model performance, we conducted experiments on the BioASQ dataset. Based on these observations, we selectedœµ= 0.5as a reasonable compromise to balance accuracy improvements with calibration reliability. D.2 Evaluation on BEIR Benchmark To provide a more comprehensive evaluation, we conducted experiments using two datasets from the BEIR benchmark: SciFact and TREC-COVID. These evaluations aim to validate the effective- ness of CalibRAG beyond its primary focus on well-calibrated decision-making, which predicts the probability of a correct decision when a user relies on the generated guidance to solve a given problem. While CalibRAG is not specifically designed as a reranking method to optimize retrieval performance, it inherently supports both calibration and retrieval. 25 Table 5: Evaluation results on TREC-COVID and SciFact datasets, a subset of the BEIR benchmark. The evaluation metric is Normalized Discounted Cumulative Gain (NDCG@K). Model Dataset NDCG@5 NDCG@10 Cross-Encoder TREC-COVID 0.7655 0.7576 SciFact 0.6668 0.6914 CalibRAG TREC-COVID 0.7863 0.7660 SciFact 0.6872 0.7114 Table 6: Results of Verbalized Confidence Fine-Tune Evaluation on the MMLU Dataset using Llama-3.1-8B-Instruct. Evaluation metrics are ACC and ECE. Case ACC ECE Continuous-Number 43.63 0.3190 Discrete-Number 44.96 0.1605 Linguistic 45.03 0.1585 For the experiments, we followed the standard retrieval pipeline, retrieving documents using BM25 and reranking the top-100 results. We compared CalibRAG with the Cross-Encoder baseline, and the results, presented in Table 5, demonstrate that CalibRAG consistently outperforms the Cross- Encoder. These findings validate that CalibRAG not only enables well-calibrated decision-making but also enhances retrieval performance, reinforcing its utility in relevant scenarios. D.3 Analysis of Verbalized Confidence Representations CalibRAG does not rely on linguistic or numerical confidence in its primary approach. Instead, it provides confidence scores based on the probability predictions generated by the forecasting func- tion. Verbalized confidence, however, was used as a baseline in comparative models. Verbalized confidence is typically expressed as a continuous number within the range [0, 100] Tian et al. [40] and Xiong et al. [52], but LLMs often struggle to interpret these numerical values precisely. To address this limitation, alternative representations were explored in the baselines: (1) linguistic expressions (e.g., ‚Äúlikely‚Äù), and (2) discrete numerical values ranging from 0 to 10. These ap- proaches were termed Linguistic and Number, respectively, with detailed prompt designs provided in Appendix E. To further analyze verbalized confidence, we conducted experiments on the MMLU dataset using the Llama-3-8B model. We evaluated the effectiveness of three confidence representations: continuous number, discrete number, and linguistic. As shown in Table 6, both discrete number and linguistic representations outperformed the continuous number baseline. Linguistic confidence, in particular, addressed the limitations of the model‚Äôs understanding of numerical relationships and improved calibration. D.4 Ablation on user modelU We additionally conduct an ablation evaluation on various user modelsU, considering that human users may make different decisions depending on their knowledge background in real-world sce- narios. We evaluated the performance of CalibRAG and baseline methods on the NQ and WebQA datasets using two retriever models, BM25 and Contriever. For this, we compare the performance ofPhi-4[58] andDeepSeek-Distill[59], which represent state-of-the-art user models. As shown in Fig. 6 and Fig. 7, our results demonstrate that CalibRAG consistently achieves better accuracy and calibration error across different user models compared to other baselines. 26 CT CT-Probe Linguistic Number CalibRAG 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Phi4: BM25 with NQ (a) 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Phi4: BM25 with WebQA (b) 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Phi4: Contriever with NQ (c) 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Phi4: Contriever with WebQA (d) Figure 6: Evaluation results of the baselines and CalibRAG utilizing two retriever models: BM25 (a, b) and Contriever (c, d) on NQ (a, c) and WebQA (b, d). Here, we utilizePhi-4[58] as our user modelU. We report four metrics‚Äî1-AUROC, 1-ACC, ECE, and Brier Score‚Äîwhere lower values indicate better performance. CT CT-Probe Linguistic Number CalibRAG 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Deepseek: BM25 with NQ (a) 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Deepseek: BM25 with WebQA (b) 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Deepseek: Contriever with NQ (c) 1-ROC ( ) 1-ACC ( ) ECE ( ) BS ( ) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Deepseek: Contriever with WebQA (d) Figure 7: Evaluation results of the baselines and CalibRAG utilizing two retriever models: BM25 (a, b) and Contriever (c, d) on NQ (a, c) and WebQA (b, d). Here, we utilizeDeepSeek-Distill[59] as our user modelU. We report four metrics‚Äî1-AUROC, 1-ACC, ECE, and Brier Score‚Äîwhere lower values indicate better performance. Table 7: Comparison of fine-tuned RAG reranking methods using our synthetic training data on HotpotQA. Methods AUROC(‚Üë)ACC(‚Üë)ECE(‚Üì)BS(‚Üì) Cross-encoder 60.74 34.98 0.477 0.477 Cross-encoder (Fine-tuned) 61.55 32.540.0082.555 CalibRAG 72.47 42.370.1060.206 Table 8: Evaluation metrics of CalibRAG without reranking on WebQA using BM25 Methods AUROC(‚Üë)ACC(‚Üë)ECE(‚Üì)BS(‚Üì) Number 69.38 ¬±2.8436.04 ¬±0.500.1931 ¬±0.01310.2293 ¬±0.0102 CalibRAG w/o Rerank 75.73¬±0.0041.99¬±0.030.0780¬±0.03120.1981¬±0.0025 D.5 Ablation on Fine-Tuning for Reranking Baselines To ensure a fair comparison between CalibRAG and the reranking baseline, we also evaluated a fine-tuned reranker model which fine-tuned using our synthetic datasets. However, as discussed in ¬ß 3.2, training was challenging due to the difficulty in feature extraction without using an embedding model to generate the guidance variablez. And this difficulty let fine-tuned model underfit to the training dataset. As shown in Table 7, the reranker model underperforms compared to the zero-shot setting. Therefore, in theComparison with reranking and robust RAG baselinesexperiments in Sec. 4.1, we evaluated the CalibRAG model alongside zero-shot reranker models. D.6 Ablation on CalibRAG without reranking To isolate the effect of reranking in our confidence calibration framework, we evaluated CalibRAG without using any reranking, where the model directly uses retrieved contexts without reordering them based on predicted confidence. As shown in Table 8, even without reranking, CalibRAG sub- stantially outperforms the Number baseline in both accuracy and calibration metrics. These results 27 Table 9: Evaluation metrics of Number + Rerank and CalibRAG on WebQA Retriever Methods AUROC(‚Üë)ACC(‚Üë)ECE(‚Üì)BS(‚Üì) BM25 Number + Rerank 75.06 ¬±0.0042.42 ¬±0.010.2075 ¬±0.01670.2397 ¬±0.0109 CalibRAG 77.29¬±0.4243.77¬±0.540.0567¬±0.03320.1983¬±0.0045 Contriever Number + Rerank 76.84¬±0.0043.08 ¬±0.000.2088 ¬±0.01270.2390 ¬±0.0083 CalibRAG 76.24 ¬±0.3744.19¬±2.600.0997¬±0.01220.2095¬±0.0062 Table 10: Comparison of zero-shot evaluation of calibration baselines onNQandWebQAdatasets using BM25 (lexical) retrieval. Results are averaged over three random seeds. Methods NQ WebQA AUROC ACC ECE BS AUROC ACC ECE BS CT-LoRA73.51¬±1.6537.70¬±0.280.2479¬±0.0240.2709¬±0.013374.36¬±1.1738.09¬±0.280.2487¬±0.03030.2681¬±0.0200 CT-probe60.92¬±0.9437.59¬±3.030.3490¬±0.02360.3536¬±0.022358.52¬±2.5137.75¬±4.390.3491¬±0.03290.3539¬±0.0332 Linguistic-LoRA57.12¬±4.3539.42¬±0.940.4529¬±0.02230.4362¬±0.028456.44¬±1.9340.58¬±1.180.4536¬±0.00710.4385¬±0.0091 Number-LoRA67.48¬±1.4234.38¬±0.710.1922¬±0.01650.2294¬±0.007669.38¬±2.8436.04¬±0.500.1931¬±0.01310.2293¬±0.0102 CalibRAG77.29¬±0.1242.66 ¬±0.970.0600¬±0.00390.1983¬±0.001777.29¬±0.4243.77¬±0.540.0567¬±0.03320.1983¬±0.0045 CalibRAG-multi76.73¬±0.2246.16¬±0.050.1397¬±0.00220.2138¬±0.001676.40¬±0.2845.84¬±0.250.1372¬±0.00070.2175¬±0.0008 Table 11: Comparison of zero-shot evaluation of calibration baselines onNQandWebQAdatasets using Contriever (dense) retrieval. Results are averaged over three random seeds. Methods NQ WebQA AUROC ACC ECE BS AUROC ACC ECE BS CT-LoRA69.89¬±4.9439.93¬±1.260.2800¬±0.05850.3008¬±0.043569.81¬±6.8237.83¬±1.250.2646¬±0.05100.2860¬±0.0394 CT-probe63.84¬±6.1437.92¬±2.800.3225¬±0.06340.3343¬±0.049862.65¬±8.1036.43¬±4.030.3072¬±0.06700.3180¬±0.0565 Linguistic-LoRA57.05¬±3.9141.50¬±0.370.4368¬±0.02670.4252¬±0.029056.30¬±2.7039.76¬±0.770.4657¬±0.01240.4477¬±0.0162 Number-LoRA71.16¬±0.6135.99¬±0.540.1827¬±0.01240.2214¬±0.001673.47¬±1.0135.61¬±0.120.1754¬±0.01240.2141¬±0.0040 CalibRAG73.89¬±1.5046.55 ¬±2.450.0312¬±0.00730.2074¬±0.006276.24¬±0.3744.19¬±2.600.0970¬±0.01220.2095¬±0.0062 CalibRAG-multi72.73¬±0.0849.42¬±0.070.1656¬±0.00190.2375¬±0.001372.95¬±0.0846.78¬±0.020.1901¬±0.00120.2488¬±0.0009 indicate that the learned calibration itself, without requiring reranking, still provides significant ben- efit, demonstrating the robustness of CalibRAG‚Äôs alignment mechanism. D.7 Using Uncertainty baseline confidence scores for reranking In this section, we investigate the effectiveness of using uncertainty baseline confidence scores for reranking in the RAG pipeline. As described in the main paper, these confidence scores are derived from verbalized scalar predictions generated by the LLM, typically representing values from 0 to 100. While such scalar confidence values can be used to rerank retrieved documents, this approach in- curs significant computational overhead. Specifically, the Number baseline requires generating full guidancezfor every(q, d)pair before estimating confidence, as the model conditions on both the query and document to generate scalar outputs. In contrast, CalibRAG directly estimates confidence from the(q, d)pair using a lightweight forecasting functionf(q, d), thus avoiding this expensive intermediate generation. Despite this additional cost, we performed an ablation to compare the reranking performance of Number-based confidence scores versus CalibRAG. As shown in Table 9, CalibRAG consistently outperforms the baseline across both BM25 and Contriever retrievers on the WebQA dataset. These results demonstrate that CalibRAG not only provides better-calibrated decisions but does so more efficiently without requiring guidance generation for every document candidate. This high- lights the dual advantage of CalibRAG in both performance and computational cost. 28 Table 12: Comparison of zero-shot evaluation of calibration baselines onBioASQ-Y/N,MMLU-Med, and PubMedQAdatasets. Results are averaged over three random seeds. Methods Dataset AUROC ACC ECE BS CT-LoRABioASQ-Y/N 65.20¬±2.3254.31¬±0.730.5167¬±0.0120.5099¬±0.0146 MMLU-Med 66.94¬±0.6847.20¬±1.520.4293¬±0.00880.4262¬±0.0084 PubMedQA 56.67¬±3.1643.80¬±0.910.4307¬±0.00990.4300¬±0.0094 CT-probeBioASQ-Y/N 59.73¬±4.2757.98¬±1.450.5664¬±0.0090.5630¬±0.0094 MMLU-Med 55.39¬±3.2449.49¬±5.000.4771¬±0.03840.4758¬±0.0375 PubMedQA 54.56¬±0.6146.60¬±1.880.4506¬±0.0120.4510¬±0.0121 Linguistic-LoRABioASQ-Y/N 48.24¬±2.2657.82¬±0.500.3193¬±0.00270.3464¬±0.0030 MMLU-Med 51.30¬±0.9355.43¬±0.940.3262¬±0.00780.3544¬±0.0049 PubMedQA 49.13¬±0.7947.13¬±2.250.4047¬±0.03360.4225¬±0.021 Number-LoRABioASQ-Y/N 52.43¬±2.1953.72¬±1.690.4664¬±0.03550.4659¬±0.0332 MMLU-Med 53.47¬±2.5441.44¬±1.010.3394¬±0.01680.3541¬±0.0135 PubMedQA 50.34¬±0.2543.60¬±0.590.3866¬±0.00290.3954¬±0.0032 CalibRAGBioASQ-Y/N66.66¬±1.3470.82¬±3.340.2414¬±0.04270.2606¬±0.0386 MMLU-Med68.93¬±1.3257.20¬±0.210.0625¬±0.06530.2226¬±0.0112 PubMedQA66.57¬±2.0062.20¬±3.530.2250¬±0.03530.2691¬±0.0072 Figure 8: Qualitative comparison of original retrieval model from CalibRAG. D.8 Full numerical results for main experiments Table 10, Table 11 and Table 12 present the complete numerical results from the primary experi- ments. For theBasemodel, we utilized a pretrained model, sampling sentences across three different seeds. For the other methods, training was conducted across three random seeds to ensure robust evaluation. We highlight the best-performing value inboldand the second-best in underline . D.9 Qualitative Results While quantitative metrics alone may not fully capture all the benefits of CalibRAG, we present examples highlighting its ability to identify relevant documents and assign calibrated confidence scores. Given the query ‚ÄúWrite a paragraph about the kind of bug that uses the American Sweetgum as a host plant.", the base retriever focuses only on the keyword ‚ÄúAmerican Sweetgum,", retrieving 29 loosely relevant content and marking its confidence as ‚ÄòConfident‚Äô (10/11) as illustrated in Fig. 8. This led to the incorrect conclusion that the sweetgum is the host plant of Parcoblatta divisa, the southern wood cockroach. In contrast, CalibRAG captures the full context, retrieving documents specifically about the gypsy moth, which uses the sweetgum as a host plant, and correctly assigns a confidence level of 81.41. This demonstrates the capability of CalibRAG to find a relevant docu- ment and assign a confidence level correlated with the accuracy of the downstream surrogate user. Additional examples can be found in ¬ß E. E Data Examples Fig. 9 shows the top 20 examples of queries and their corresponding labels. The full set of data examples will be released upon publication of the paper. Fig. 9 shows that the ranking of the retrieved documents is not correlated with the accuracy of the user decision. As seen in this example, the top-ranked document is not helpful for the user model in decision-making, whereas the second- ranked document provides information that can lead the user model to make a correct decision. This illustrates the importance of CalibRAG‚Äôs forecasting functionfin effectively modeling the probability that a decision made using documentdis correct, emphasizing the need for reranking documents based on this modeling. 30 Figure 9: Top-20 retrieved document examples. 31 F Qualitative Examples Here, we present additional qualitative examples for comparison with other baselines. In Fig. 10, Fig. 11, Fig. 12, and Fig. 13, the examples demonstrate that while the baselines retrieve documents that provide incorrect answers to the queries, they still assign high confidence to the retrieved docu- ments. In contrast, CalibRAG effectively reranks and retrieves documents that are highly relevant to the decision problemx, allowing us to confirm that the guidance generated from these retrieved doc- uments is well-predicted to be helpful for decision-making. Additionally, we can confirm that when the document with the highest rank does not aid in decision-making forx, CalibRAG successfully assigns a lower confidence level, helping to prevent the user from over-relying on the guidance. Figure 10:CalibRAG vs Linguistic-LoRA.In the case of CalibRAG, a document about the person in question is retrieved with a confidence level of 83.93%. In contrast, the document retrieved by the base retrieval model is related to Donald Trump but does not match the specific person in the query. Nevertheless, the Linguistic- LoRA model trust the document confidently. Figure 11:CalibRAG vs Number-LoRA.In the case of CalibRAG, an accurate document about the location following North Africa was retrieved, allowing the user model to make a correct decision. In contrast, the base retrieval model brought a different document. Nevertheless, Number-LoRA model set this context with a confidence level of 6 out of 10, leading the user to ultimately make an incorrect decision. 32 Figure 12:CalibRAG vs Number-LoRA.The base retrieval model focused solely on the word ‚Äôimpeached‚Äô and retrieved a related document, missing the context of ‚Äôfirst.‚Äô Despite this, the Number-LoRA model set a confidence level of 9 out of 10, causing the user to make an incorrect decision. In contrast, CalibRAG retrieved an accurate document that, while not explicitly containing ‚Äôfirst impeached,‚Äô included the phrase ‚Äôfirst being.‚Äô It set a confidence level of 92.32%, allowing the user to arrive at the correct answer. Figure 13:CalibRAG vs CT-LoRA.In the case of CalibRAG, the top-20 confidence score is 20.95 for incorrect information, causing the user to hesitate in making a decision. However, with the CT-LoRA model, incorrect information is assigned a confidence score of 96.83, leading the user to make an incorrect decision. 33 G Prompt Examples In this section, we present prompt examples used during training and inference. Figure 14a shows the prompt that encourages the user modelUto act like a human decision-maker, leading it to over- rely on the guidance provided by the LLM. Figure 14b displays the prompt that generates the open- ended queryqfrom the decision taskx. Figure 14c presents the prompt that induces the generation of guidancezfromMbased on the retrieved documentd. Figure 15a is used when grading the user modelU‚Äôs decision against the true answer usingG. Figure 16a, Figure 16b, and Figure 16c are prompts used to instructMto generate confidence in terms of linguistic or numerical calibration. Lastly, Figure 15b is the prompt used duringStage 3of the inference process. Decision prompt The task is to answer questions based on a context generated by a language model in response to a question about relevant information, along with the model‚Äôs confidence level in the provided answer. Context: {context} Question: {question} Model Confidence: {confidence} Answer: (a) Prompt designed to guide the user modelUin making decisions based on the LLM-generated guidancez and confidencec. Prompt that generates open-ended queryqfrom the decision taskx You are an automated assistant tasked with rephrasing specific questions into open-ended queries to encourage detailed exploration and discussion of the key topics mentioned. Your goal is to prompt someone to write a paragraph exploring the topic without directly revealing the answer. Examples for Guidance: Example 1: Question 1: Which sea creature is the world‚Äôs largest invertebrate? Question 2: Write a paragraph about the world‚Äôs largest invertebrate. ... Now, please rephrase the following question: Question 1: {question} Question 2: (b) This prompt was first suggested by Band et al. [6], and we have modified part of the proposed prompt for our use here. We use this prompt as an input when generating the queryqbased on the decision taskx. Guidancezgeneration prompt Directly state the answer without phrases like ‚Äôthe correct answer is. Given the retrieved context, answer the question as accurately as possible. Question: {question} Retrieved Context: {title} - {context} Answer: (c) This prompt guides the LLMMto provide direct, concise guidancezbased on a given retrieved document d. Figure 14: Prompt used for (a) user model making decisions, (b) generatingqfromx, and (c) generatingz. 34 Evaluation prompt The problem is: {question} The correct answer for this problem is: {ground-truth} A student submitted the answer: {prediction} The student‚Äôs answer must be correct and specific but not overcomplete (for example, if they provide two different answers, they did not get the question right). However, small differences in formatting should not be penalized (for example, ‚ÄôNew York City‚Äô is equivalent to ‚ÄôNYC‚Äô). Did the student provide an equivalent answer to the ground truth? Please answer yes or no without any explanation: (a) Prompt used to evaluate the long-form generated answer. Query regeneration prompt. You are a language model assistant who specializes in improving queries for document search systems. Your task is to highlight and clarify the important parts of a given query to make it more precise and help retrieve relevant documents. Please take the original search query below and rewrite it by emphasizing the important words. Do not add any new information not included in the original query. Original Retrieval Query: {query} Please generate the new retrieval query without any explanation: (b) This prompt assists in rewriting search queries to enhance precision and relevance for document retrieval, emphasizing the crucial elements without adding extraneous information. Figure 15: Prompt used for (a) evaluation and (b) query regeneration. 35 Calibration tuning prompt Is the proposed answer correct? Choices: (i): no (ii): yes Answer: (a) This prompt was first suggested by Kapoor et al. [21]. It poses a straightforward question to verify the correctness of a proposed answer with binary choices for evaluation. We used this prompt when training our baselines. Linguistic calibration prompt Provide the certainty level of answer using the given 11 certainty levels. Give ONLY your certainty level, no other words or explanation. Certainty Levels: Unlikely, Doubtful, Uncertain, Ambiguous, Probable, Likely, Possible, Specified, Confirmed, Certain, Inevitable. For example: Certainty: <ONLY the certainty level that Answer is correct, without any extra commentary whatsoever; just the certainty level!> Certainty: (b) This prompt requires the model to evaluate the certainty of an answer using a predefined set of linguistic levels of certainty. We used this prompt for our baselines that utilize linguistic calibration. Number calibration prompt Provide the certainty level of answer using the given 11 certainty levels. Give ONLY your certainty level, no other words or explanation. Certainty Levels: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. For example: Certainty: <ONLY the certainty level that Answer is correct, without any extra commentary whatsoever; just the number!> Certainty: (c) This prompt is similar to the linguistic calibration prompt but uses numerical certainty levels (from 0 to 10) to rate the confidence in the answer provided. We used this prompt for our baselines that utilize number calibration. Figure 16: Prompt used for baseline experiments. 36