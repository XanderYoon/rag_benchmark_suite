arXiv:2501.09292v3 [cs.CL] 18 Mar 2025 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 TO RETRIEVE OR NOT TO RETRIEVE ? U NCERTAINTY DETECTION FOR DYNAMIC RETRIEVAL AUGMENTED GENERATION Kaustubh D. Dhole Department of Computer Science Emory University Atlanta, GA 30307, USA kdhole@emory.edu ABSTRACT Retrieval-Augmented Generation equips large language mod els with the capabil- ity to retrieve external knowledge, thereby mitigating hallucinations by incorporat- ing information beyond the model’s intrinsic abilities. Ho wever, most prior works have focused on invoking retrieval deterministically, whi ch makes it unsuitable for tasks such as long-form question answering. Instead, dy namically performing retrieval by invoking it only when the underlying LLM lacks t he required knowl- edge can be more efﬁcient. In this context, we delve deeper in to the question, “To Retrieve or Not to Retrieve?” by exploring multiple uncerta inty detection meth- ods. We evaluate these methods for the task of long-form ques tion answering, employing dynamic retrieval, and present our comparisons. Our ﬁndings suggest that uncertainty detection metrics, such as Degree Matrix Jaccard and Eccentricity, can reduce the number of retrieval calls by almost half, with only a slight reduction in question-answering accuracy. 1 I NTRODUCTION Recently, Large Language Models (LLMs) like ChatGPT OpenAI (2023), Gemini Team et al. (2023), and others are showing impressive strides in tasks a cross numerous bench- marks Srivastava et al. (2023). This success has been largel y owed to their exposure to massive training data and successive ﬁne-tuning of instruction dat asets. To increase the helpfulness and decrease the harmfulness of the models, they are being furth er ﬁne-tuned over preference collec- tions Bai et al. (2022); Ouyang et al. (2022); Rafailov et al. (2024). Further, Retrieval Augmented Generation (RAG) Lewis et al. (2020); Dhole (2024a); Dhole et al. (2024), in the effort to mitigate hallucinations, enriches these models with domain-speciﬁc informa- tion and tackles scenarios where the intrinsic knowledge of the base model falls short. By integrating externally retrieved content during the generation phase, RAG enhances the model’s ability to pro- duce less hallucinatory and domain-conditioned responses . This approach has been particularly valuable in complex applications such as long-form generat ion like multi-hop question answering, which often requires multiple retrievals to address a query comprehensively. However, to optimize the efﬁciency of RAG, retrieval should only be invoked when necessary — also referred to as conditional retrieval. Previous condit ional RAG setups have explored multiple paradigms like low token probabilities Jiang et al. (2023), external classiﬁers Wang et al. (2023), or low entity popularity Mallen et al. (2023) as indicators of t he LLMs’ knowledge gaps. However, most of these methods fall short in either approximating kno wledge gaps of the LLMs or lacking the ability to invoke retrieval dynamically. On the other hand, with the potential of LLMs to hallucinate, there has been an increasing interest in uncertainty detection methods to gauge LLMs’ conﬁdence in their outputs Fadeeva et al. (2023). Unlike traditional methods that rely on rigid heuristics or external classiﬁers, uncertainty detection leverages the inherent variability in LLM-generated respo nses to estimate conﬁdence dynamically. 1 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 For instance, semantic sets-based UD approaches Lin et al. ( 2023) group responses based on mean- ing, and use the number of clusters to directly reﬂect the lev el of uncertainty — with greater variabil- ity signaling higher uncertainty. Similarly, spectral met hods using eigenvalue Laplacians quantify response diversity by identifying strong or weak clusterin g patterns in pairwise similarity graphs. These approaches align with the probabilistic nature of LLMs as well as adaptively gauge uncertainty based on output coherence, making them more robust against a dversarial or ambiguous inputs. In this work, we evaluate if such uncertainty detection meth ods can indeed enhance the reliability of conditionally invoking retrieval, by measuring its impa ct on a downstream task of multi-hop question answering. In that regard, we resort to a conditional RAG system and empl oy numerous uncertainty detection metrics to test the need for invoking retrieval. Our RAG syst em performs forward-looking active retrieval in the style of Jiang et al. (2023). Speciﬁcally, we contribute the following: • We design a method that performs retrieval augmented gener ation with dynamic retrieval through uncertainty detection • We perform an exhaustive analysis of various conditions fr om the “uncertainty quantiﬁca- tion” literature to gauge the best strategy to dynamically r etrieve during generation • Based on the results, we present insights for future resear ch Our insights are useful to gauge whether uncertainty detect ion methods can help improve the efﬁ- ciency of RAG. 2 R ELATED WORK Here, we summarise some of the related work on uncertainty qu antiﬁcation and some active RAG efforts. There has been a lot of recent work on uncertainty quantiﬁcat ion of white box and black box NLG models. Lin et al. (2023) showed that along with their genera tions, GPT-3 can output a verbal- ized form of the uncertainty, viz. “high conﬁdence” or “85% c onﬁdence”. Kadavath et al. (2022) show that models can be made to sample answers and then made to self-evaluate the probability of P(True). Kuhn et al. (2023) recently proposed to compute the semantic entropy by considering the equivalence relationships amongst generated responses. Wang et al. (2024) proposed Self-DC that tackled compositio nal questions via iterative divide-and- conquer based on LLM certainty. Y ao et al. (2024) propose uti lising the model’s internal states to estimate uncertainty and deciding whether to retrieve or no t. We now describe the tasks and datasets used in our analysis along with the UD approaches employed. 3 T ASKS AND DATASETS We conduct experiments on the 2WikiMultihopQA dataset Ho et al. (2020), a multi-hop open do- main question answering (QA) dataset that tests the reasoni ng and inference skills of question- answering models. Questions in this dataset generally requ ire two steps of reasoning to deduce the ﬁnal answer, and the information for each step of reasoni ng can be obtained through referencing external information viz., Wikipedia passages. 4 A PPROACH We now describe our uncertainty-aware, retrieval-augment ed generation in the following two sub- sections. 2 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 4.1 U NCERTAINTY EVALUATION OF FUTURE SENTENCE Given a query q, a retriever R, a text generator G, and a black box uncertainty estimation function U, and partially generated sequence t<i until time step i, – we ﬁrst generate a temporary sentence tn in the style of FLARE Jiang et al. (2023). We use a prompt template P, which could take the form of a zero-shot or a few-shot instru ction. This instruction takes as input the query, zero or more retri eved documents d1 . . . dk, and the answer tokens generated until now. Here, we use ti to represent the ith temporary sentence and y<i to represent all the initialised and generated sentences {0 . . .(i − 1)}. ti is ﬁrst obtained without performing retrieval: ti = G(P{q, . . . , yi−1}) (1) During generation, we evaluate the uncertainty of this temp orary sentence tn to gauge if the gen- erator needs more information. If the uncertainty U(tn) exceeds a threshold θU, the model is not certain and may lack the necessary knowledge to provide an ac curate answer. The next sentence yi is then computed by appending retrieved information to the m odel context: yi = {G(P{d1, . . . , dk, q, . . . , yi−1}) if U(ti) > θ U G(P{q, . . . , yi−1}) otherwise (2) where d1 . . . dk are obtained from a retrieval system R. d1 . . . dk := R(q) (3) 4.2 S EQUENCE LEVEL UNCERTAINTY EVALUATION MEASURES We resort to 5 recently introduced sequence-level uncertai nty evaluation measures. Each of them work in a black box manner without requiring information reg arding the model parameters. The high-level strategy of all the methods is the same. Given an input x, ﬁrst generate n responses through some generator G and then compute pairwise similarity scores of each of the n responses with each other. Using these similarity values, compute an uncertainty estimate U (x) or a conﬁdence score. • Semantic Sets : In the black-box approach of Kuhn et al. (2023), the authors propose to compute semantic sets i.e. groups of responses that are clos e together in meaning. These semantic sets of equivalence subsets are computed using a Na tural Language Inference (NLI) classiﬁer. Here, the number of semantic sets can be reg arded as an uncertainty esti- mate as when the responses differ in meaning, the number of gr oups increases. • Eigen V alue Laplacian: deﬁnes the uncertainty estimate by capturing the essence o f spec- tral clustering. First, an adjacency matrix is created from the pairwise similarities of re- sponses. Then the matrix is partitioned into clusters, wher e each cluster corresponds to a distinct “meaning” or category within the responses. The ei genvalues close to one indicate strong cluster formations, thus contributing less to the un certainty estimate, while those fur- ther from one suggest weaker clustering or more diffuse dist ributions of responses, hence increasing the uncertainty estimate. The degree matrix of the adjacency graph is also used to compu te the uncertainty esti- mate Lin et al. (2023). A node that is well-connected to other nodes, might be less uncer- tain. We use two similarity metrics for computing the degree matrix. • Degree Matrix (Jaccard Index) : The Jaccard similarity is a light-weight metric where sentences or passages are treated as sets of words, and simil arity between responses is computed by taking the fraction of the intersection of the tw o sets and the union of the two sets. • Degree Matrix (NLI) : Here, the similarity between responses is computed throug h classi- fying entailment relations amongst them. A classiﬁer predi cts whether a pair of responses contradict, entail, or are neutral to each other. 3 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 Uncertainty Estimator Trigger Retrieval When Retrieval Query #examples #search #steps f1 Always Retrieve U ≥ 0 Temporary Sentence 25 4.60 3.60 0.552 Always Retrieve Sub-Query 25 5.00 4.00 0.538 FLARE-Instruct “...[Search” 25 4.80 3.80 0.531 Degree Matrix Jaccard U > 0.4 Sub-Query 24 1.46 3.67 0.593 Eccentricity U > 2 Sub-Query 22 2.23 4.05 0.605 Semantic Sets U > 2 Sub-Query 23 2.52 4.09 0.411 Degree Matrix NLI U > 0.5 Sub-Query 24 2.25 4.00 0.535 Table 1: Performance Metrics over a smaller seed set Uncertainty Estimator Trigger Retrieval When #search #steps ret ratio correct incorrect f1 Always Retrieve Always 4.63 3.63 1.32 0.493 0.493 0.578 4.61 3.61 1.33 0.52 0.467 0.594 4.61 3.61 1.33 0.493 0.493 0.571 0.581 Degree Matrix Jaccard U > 0.4 1.80 3.61 0.57 0.453 0.533 0.538 1.92 3.60 0.61 0.44 0.547 0.525 1.85 3.61 0.57 0.419 0.568 0.508 0.524 Eccentricity U > 2 2.17 3.60 0.64 0.44 0.547 0.525 2.25 3.63 0.67 0.467 0.533 0.565 2.23 3.63 0.64 0.507 0.493 0.594 0.561 Table 2: Performance Metrics for Different Uncertainty Est imators for 75 examples. 4.3 S UBQUERY GENERATION FOR RETRIEVAL We resort to retrieving relevant knowledge to account for th e information that the model is lacking to answer the question. FLARE Jiang et al. (2023) generates a retrieval query for the missing en- tity in the temporary sentence by using the sentence with the low probability token removed or by prompting an external question generator to generate a ques tion for the missing entity as the answer. We generalize this by instead prompting the model to generat e a subquery to ﬁgure out the missing information needed to answer the user query in an open-ended manner. We deﬁne a subquery generator SQ which takes in as input few-shot exemplars of subqueries, th e current user query q, and the current partial answer sentences uttered in chain- of-thought Wei et al. (2022) fashion. It seeks to generate subqueries to get a spec iﬁc piece of information not generated in the partial answer sentences but is needed to answer q. Once this subquery is generated, we use this subquery to retrieve additional passages from the exte rnal retriever R. These passages are then appended to the user input, and the generation continues. For instance, for the question, “Which ﬁlm has the director w ho died ﬁrst, Promised Heaven or Fire Over England?”, and the partially generated answer, “The ﬁl m Promised Heaven was directed by Eldar Ryazanov. Fire Over England was directed by William K. Howard. Eldar Ryazanov died on November 30, 2015.”, we expect the model to generate a subque ry, “When did William K. Howard die?”. 5 S ETUP The generator used in all experiments was GPT-3 (davinci-00 2) Brown et al. (2020), and the re- triever employed was BM25 through PyTerrier Macdonald et al . (2021); Dhole (2024b). The base code used for conducting the experiments and computing the m etrics presented in the tables was obtained from the active RAG setup by Jiang et al. (2023). For uncertainty detection, we resort to the Fadeeva et al. (2023)’s LM-Polygraph library. Since running GPT-3 (davinci-002) along with many of the unc ertainty detection metrics could be expensive to run (due to making multiple calls), we ﬁrst perf orm a run for a small seed set of 25 4 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 queries across all metrics and then choose the 3 best metrics for a rerun across a larger set of 75 examples. We perform each run three times. 6 R ESULTS We now present the results in Tables 1 and 2 for the smaller and the larger sets respectively. The baseline method where retrieval was always invoked yiel ded an F1 score of 0.552 when using temporary sentences as retrieval queries and 0.538 when subqueries were generated for retrieval but required most number of retrieval operations. Triggering retrieval, when uncertainty computed through Eccentricity i.e. U > 2, led to the high- est F1 score of 0.605, with a lesser number of search operations. This approach ba lanced retrieval efﬁciency and task performance better than other methods. I t required half the number of search operations than an Always Retrieve approach. Semantic Sets’ innovative clustering approach per- formed poorly, with an F1 score of 0.411. Using entailment-based similarity to compute uncertaint y via the Degree Matrix NLI measure achieved an F1 score of 0.535, comparable to the baseline. The lightweight Degree Matrix (Jaccard) necessitated the least number of retrieval operations to perform better than an Always Retrieve baseline. Table 2 presents additional performance metrics over a larg er set of 75 examples. Notably, the Eccentricity method consistently demonstrated the best balance between retrieval efﬁciency and performance, achieving an average F1 score of 0.561 across different experimental runs, while re- ducing unnecessary retrievals compared to the baseline. Degree Matrix (Jaccard) performed slightly worse in F1 score ( 0.524) but depended on retrieval the least indicating its potential for applications where m inimizing retrieval costs is crucial. In contrast, the Always Retrieve approach performed better than both conditional retrieval ap- proaches but necessitated almost twice the number of retrie val calls. 7 C ONCLUSION Our experiments demonstrate that dynamic retrieval, guide d by uncertainty detection, improves the efﬁciency of retrieval-augmented generation systems, making it useful where retrieval can be expen- sive to compute. Among the methods tested, Eccentricity-based uncertainty detection emerged as the best-performing approach, offering the highest F1 sc ore with a moderate number of retrieval steps and searches. This method effectively balances retri eval efﬁciency with task performance. The Degree Matrix (Jaccard) method also showed promising results, particularly in redu cing re- trieval costs while maintaining reasonable performance. C onversely, methods such as Semantic Sets and FLARE-Instruct underperformed, highlighting the need for more reliable un certainty es- timators. Although some black-box uncertainty detection methods req uire multiple runs of generation, which can be costly, always retrieving may be preferable in RAG app lications where lightweight retrieval methods like BM25 sufﬁce. This is also evident from the resul ts on the larger set. Besides, we feel that uncertainty detection might become mo re mainstream as the propensity for hallucination in LLMs increases, and as end applications de mand more conﬁdence and interpretabil- ity Dhole et al. (2024) in their outputs making uncertainty d etection a necessity. Our work focuses on exploiting uncertainty detection for RAG, especially wh ere retrieval can be expensive like the us- age of heavy and composite retrieval systems employing nume rous components like reformulation, dense retrieval Santhanam et al. (2021), reranking, etc. 8 E THICAL CONSIDERATIONS When evaluating large language models (LLMs), it is essenti al to adopt a sociotechnical perspec- tive Dhole (2023), acknowledging that their outputs are inﬂ uenced by both social contexts and technical design choices. Proper safeguards should be in pl ace to mitigate biases and prevent the 5 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 generation of harmful or toxic content. Furthermore, the un certainty detection approaches we em- ployed rely on estimations derived from various neural netw ork computations, which are inherently shaped by the data on which the models are trained. Consequen tly, it is critical to thoroughly test uncertainty detection methods to ensure they meet the requi rements of the intended applications. Despite these precautions, there remains a possibility tha t some approaches may misrepresent the level of certainty, as no method is ﬂawless. Therefore, ongo ing evaluation and reﬁnement of un- certainty detection mechanisms are necessary to minimize i naccuracies and potential misinterpreta- tions. ACKNOWLEDGEMENTS The author would like to thank Eugene Agichtein for insightf ul discussions and the anonymous reviewers for their useful feedback. The author would also l ike to thank Microsoft for providing OpenAI credits through the Microsoft Accelerating Foundat ion Models Research A ward. REFERENCES Y untao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna C hen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Tr aining a helpful and harmless assistant with reinforcement learning from human feedback . arXiv preprint arXiv:2204.05862 , 2022. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Ja red Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Aman da Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner , Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-s hot learners, 2020. Kaustubh Dhole. Large language models as sociotechnical sy stems. In Proceedings of the Big Picture W orkshop, pp. 66–79, 2023. Kaustubh Dhole. Kaucus-knowledgeable user simulators for training large language models. In Proceedings of the 1st W orkshop on Simulating Conversation al Intelligence in Chat (SCI-CHAT 2024), pp. 53–65, 2024a. Kaustubh D Dhole. Pyterrier-genrank: The pyterrier plugin for reranking with large language mod- els. arXiv preprint arXiv:2412.05339 , 2024b. Kaustubh D. Dhole, Kai Shu, and Eugene Agichtein. Conqret: B enchmarking ﬁne-grained evalua- tion of retrieval augmented argumentation with llm judges, 2024. Ekaterina Fadeeva, Roman V ashurin, Akim Tsvigun, Artem V az hentsev, Sergey Petrakov, Kirill Fedyanin, Daniil V asilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, and Artem Shelmanov. LM-polygraph: Uncertainty e stimation for language models. In Y ansong Feng and Els Lefever (eds.),Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 446–461, Singapore, December 2023. Association for Computational Linguistics. doi: 10. 18653/v1/2023.emnlp-demo.41. URL https://aclanthology.org/2023.emnlp-demo.41. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aiz awa. Constructing a multi-hop QA dataset for comprehensive evaluation of reaso ning steps. In Proceedings of the 28th International Conference on Computational Lingui stics, pp. 6609–6625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. URL https://www.aclweb.org/anthology/2020.coling-main.5 80. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Ja ne Dwivedi-Y u, Yiming Y ang, Jamie Callan, and Graham Neubig. Active retrieval aug mented generation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empir- ical Methods in Natural Language Processing , pp. 7969–7992, Singapore, December 2023. 6 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 Association for Computational Linguistics. doi: 10.18653 /v1/2023.emnlp-main.495. URL https://aclanthology.org/2023.emnlp-main.495. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatﬁeld-Dodds, Nova DasSarma, Eli T ran-Johnson, et al. Language mod- els (mostly) know what they know. arXiv preprint arXiv:2207.05221 , 2022. Lorenz Kuhn, Y arin Gal, and Sebastian Farquhar. Semantic un certainty: Linguis- tic invariances for uncertainty estimation in natural lang uage generation. In The Eleventh International Conference on Learning Representa tions, 2023. URL https://openreview.net/forum?id=VD-AYtP0dve. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petro ni, Vladimir Karpukhin, Naman Goyal, Heinrich K¨ uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨ aschel, et al. Retrieval-augmented genera- tion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33: 9459–9474, 2020. Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating wit h conﬁdence: Uncertainty quantiﬁ- cation for black-box large language models. Transactions on Machine Learning Research, 2023. Craig Macdonald, Nicola Tonellotto, Sean MacAvaney, and Iadh Ounis. Pyterrier: Declarative exper- imentation in python from bm25 to dense retrieval. In Proceedings of the 30th acm international conference on information & knowledge management , pp. 4526–4533, 2021. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigati ng effectiveness of paramet- ric and non-parametric memories. In Anna Rogers, Jordan Boy d-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association fo r Computa- tional Linguistics (V olume 1: Long Papers) , pp. 9802–9822, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653 /v1/2023.acl-long.546. URL https://aclanthology.org/2023.acl-long.546. OpenAI. Gpt-4 technical report, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wa inwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Tr aining language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730– 27744, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Y our language model is secretly a reward model. Advances in Neural Information Processing Systems , 36, 2024. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christop her Potts, and Matei Zaharia. Colbertv2: Effective and efﬁcient retrieval via lightweig ht late interaction. arXiv preprint arXiv:2112.01488, 2021. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu A wal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri` a Garr iga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabil ities of language models. Transactions on Machine Learning Research , 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Y onghui Wu, Je an-Baptiste Alayrac, Jiahui Y u, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et a l. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023. Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxi ang Wang, Huimin Wang, Guan- hua Chen, and Kam-fai Wong. Self-dc: When to reason and when t o act? self divide-and-conquer for compositional unknown questions. arXiv preprint arXiv:2402.13514 , 2024. Yile Wang, Peng Li, Maosong Sun, and Y ang Liu. Self-knowledg e guided retrieval augmentation for large language models. In Houda Bouamor, Juan Pino, and K alika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023 , pp. 10303–10315, Singapore, Decem- ber 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.ﬁndings-emnlp.691. URL https://aclanthology.org/2023.findings-emnlp.691. 7 1st workshop of “Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI” at ICLR’25 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022. Zijun Y ao, Weijian Qi, Liangming Pan, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, and Juanzi Li. Seakr: Self-aware knowledge retrieval for adaptive ret rieval augmented generation. arXiv preprint arXiv:2406.19215, 2024. 8