RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering Ines Besrourâˆ— ines.besrour@mailbox.tu-dresden.de TU Dresden & ScaDS.AI Dresden/Leipzig Dresden, Germany Jingbo Heâˆ— jingbo.he@mailbox.tu-dresden.de TU Dresden & ScaDS.AI Dresden/Leipzig Dresden, Germany Tobias Schreieder tobias.schreieder@tu-dresden.de TU Dresden & ScaDS.AI Dresden/Leipzig Dresden, Germany Michael FÃ¤rber michael.faerber@tu-dresden.de TU Dresden & ScaDS.AI Dresden/Leipzig Dresden, Germany Abstract We present RAGentA, a multi-agent retrieval-augmented genera- tion (RAG) framework for attributed question answering (QA) with large language models (LLMs). With the goal of trustworthy answer generation, RAGentA focuses on optimizing answer correctness, defined by coverage and relevance to the question and faithfulness, which measures the extent to which answers are grounded in re- trieved documents. RAGentA uses a multi-agent architecture that iteratively filters retrieved documents, generates attributed answers with in-line citations, and verifies completeness through dynamic refinement. Central to the framework is a hybrid retrieval strategy that combines sparse and dense methods, improving Recall@20 by 12.5% compared to the best single retrieval model, resulting in more correct and well-supported answers. Evaluated on a synthetic QA dataset derived from the FineWeb index, RAGentA outperforms standard RAG baselines, achieving gains of 1.09% in correctness and 10.72% in faithfulness. These results demonstrate the effectiveness of our multi-agent RAG architecture and hybrid retrieval strategy in advancing trustworthy QA with LLMs. CCS Concepts â€¢ Information systems â†’ Question answering ; â€¢ Comput- ing methodologies â†’ Multi-agent systems; Natural language generation. Keywords Retrieval-Augmented Generation, Multi-Agent System, Attributed Question Answering, Large Language Model ACM Reference Format: Ines Besrour, Jingbo He, Tobias Schreieder, and Michael FÃ¤rber. 2025. RA- GentA: Multi-Agent Retrieval-Augmented Generation for Attributed Ques- tion Answering. In SIGIR 2025 LiveRAG Challenge, held in conjunction with the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2025), July 13â€“17, 2025, Padua, Italy. âˆ—Both authors contributed equally to this research. This work is licensed under a Creative Commons Attribution 4.0 International License. LiveRAG@SIGIR 2025, Padua, Italy Â© 2025 Copyright held by the owner/author(s). 1 Introduction LLMs are increasingly applied to tasks such as multi-step reason- ing [24], summarization [26], and open-domain QA [ 21]. Never- theless, LLMs often generate hallucinations, meaning factually in- correct or misleading outputs [10]. Because users currently have limited means to verify LLM-generated answers, hallucinated in- formation becomes especially problematic. This lack of verifiability poses a significant challenge for deploying LLMs in high-stakes domains such as science, healthcare, and law, where factuality and trust are essential. RAG addresses this issue by combining traditional information retrieval with LLMs [12]. Relevant documents are retrieved from external sources and used to guide the modelâ€™s generation. This approach reduces the likelihood of unsupported claims by ground- ing answers in curated, up-to-date information. Building on this foundation, Gao et al. [7] show that RAG can also enable attribution, for example via in-line citations. This allows users to verify the sources of generated content and increases trustworthiness. As part of our participation in the SIGIR 2025 LiveRAG Challenge, we propose a multi-agent RAG framework for attributed QA that emphasizes both answer correctness and faithfulness. We extend the MAIN-RAG framework [ 2], which employs three agents to score the relevance of retrieved documents for improved answer correctness. We enhance this architecture by introducing a novel agent that improves the faithfulness of the output through fine- grained attribution and refines the LLM answer when needed. This collaborative setup helps close the gap toward more trustworthy LLM applications for QA. Our key contributions are as follows: â€¢ We proposeRAGentA, a collaborative multi-agent RAG frame- work that improves answer faithfulness through fine-grained citations, especially for multi-source questions. â€¢ We ensure high retrieval quality through a hybrid sparse- dense approach, reinforced by agent-based relevance scoring to select the most suitable documents for generation. â€¢ We test RAGentA against a baseline RAG approach by build- ing a diverse synthetic QA dataset from the FineWeb index to assess retrieval, answer correctness, and faithfulness. 2 Related Work Multi-Agent RAG. Recent developments in RAG have increas- ingly leveraged multi-agent systems to improve performance and arXiv:2506.16988v2 [cs.IR] 1 Sep 2025 LiveRAG@SIGIR 2025, July 13â€“17, 2025, Padua, Italy Ines Besrour, Jingbo He, Tobias Schreieder, and Michael FÃ¤rber scalability in QA tasks. Zhao et al. [27] introduced LongAgent, a col- laborative multi-agent framework designed to enable QA over very long documents. In this approach, queries are decomposed into sub- tasks handled by individual agents, each processing a segment of the input. A leader agent coordinates their outputs and synthesizes the final answer. Building on the idea of agent specialization, Zhu et al. [28] proposed ATM, a dual-agent adversarial RAG framework consisting of a generator and an attacker. The attacker injects chal- lenging distractor documents, forcing the generator to distinguish relevant information even in misleading contexts through adver- sarial training. Expanding the agent interaction paradigm, Yang et al. [25] introduced IM-RAG, which models an â€œinner monologueâ€ among a reasoner, retriever, and refiner. These agents engage in iterative communication to refine both the queries and the retrieved content before generating a final answer. Further emphasizing task specialization, Jang and Li [11] developed AU-RAG, where distinct agents are dedicated to sub-tasks such as query understanding, document retrieval, and answer generation. Similarly, AgentFusion by Saeid and Kopinski [20] assigns specific roles such as planning, verification, and refinement to multiple agents within the genera- tion pipeline, thereby enabling a more modular and robust genera- tion process. Chang et al. [2] proposed MAIN-RAG, a training-free, multi-agent architecture that focuses explicitly on document fil- tering and evidence scoring. This approach significantly improves answer accuracy while mitigating the impact of noisy retrievals. Our proposed framework, RAGentA, builds directly on the insights from MAIN-RAG, aiming to further enhance retrieval robustness and by providing attribution to sources used for text generation. Attributed Text Generation. Also known as citation genera- tion, this task aims to produce text with explicit citations to source documents, thereby enhancing trustworthiness. As described by Huang and Chang [9], LLM attribution is broadly categorized into parametric and non-parametric approaches. Parametric approaches, such as Galactica [22], generate citations only using the modelâ€™s in- ternal knowledge. In contrast, non-parametric methods leverage ex- ternal knowledge sources during generation to provide attribution. Non-parametric approaches can be divided into post-generation and post-retrieval paradigms. Post-generation approaches first gen- erate an answer, then identify supporting evidence. For instance, RARR [6] detects unsupported claims in generated answers and revises them using retrieved documents. Ramu et al. [17] improve attribution in long documents by decomposing answers into coarse- grained segments to better align with evidence. Huang et al . [8] propose fine-grained reward models for training LLMs to gener- ate accurate citations. In this work, we focus on post-retrieval ap- proaches that follow the RAG paradigm, retrieving evidence prior to answer generation. Gao et al. [7] introduced an initial approach and ALCE, a benchmark dataset widely used to evaluate attribution. Fierro et al. [4] propose a planning-based strategy, where models generate citation plans before answering. Berchansky et al . [1] develop CoTAR, applying chain-of-thought reasoning with hier- archical citation to improve attribution granularity. Qi et al. [16] present MIRAGE, which uses internal model representations to trace answer spans back to supporting passages, improving faithful- ness. Finally, Patel et al. [14] demonstrate a method for generating multiple citations per sentence, enabling more nuanced and com- prehensive attribution in long-form generation. 3 Synthetic Dataset Generation We constructed a diverse synthetic QA benchmark for the eval- uation of our RAGentA framework using the DataMorgana plat- form [5]. The dataset comprises 500 QA pairs, each associated with multiple supporting evidence paragraphs. Detailed statistics of the dataset, including the distribution across various question and user categories, are presented in Table 1. Table 1: Distribution of Question and User Categories Category Type Category Name Percent Question: answer-type comparison 49.8 Question: answer-type multi-aspect 28.4 Question: answer-type factoid 21.8 Question: premise without premise 67.4 Question: premise with premise 32.6 Question: formulation concise and natural 37.8 Question: formulation verbose and natural 35.0 Question: formulation short search query 14.2 Question: formulation long search query 13.0 User: user-expertise expert 50.8 User: user-expertise novice 49.2 4 The RAGentA Framework To address the SIGIR LiveRAG challenge, our primary objective is to ensure both the correctness and faithfulness of answers generated by RAG systems. To this end, we propose a multi-agent RAG archi- tecture comprising four specialized agents, each powered by the Falcon-3-10B language model. The overall framework is illustrated in Figure 1. This approach is entirely training-free and broadly applicable to diverse QA tasks and domains without the need for additional fine-tuning or task-specific adaptation. 4.1 Hybrid Retrieval System The first stage of our framework is a hybrid retrieval system that combines both dense and sparse models to ensure broad and con- textually relevant document coverage. Sparse retrieval, based on BM25, is a widely adopted model that relies on exact term match- ing [18, 19]. Our semantic retrieval is powered by dense embeddings using the intfloat/e5-base-v2 model (E5) with Pinecone as the vector store to capture nuanced semantic relationships [23]. These two systems are fused using a tunable interpolation pa- rameter ğ›¼ âˆˆ [ 0, 1], producing a final document score as: ğ‘†hybrid (ğ‘‘) = ğ›¼ Â·ğ‘†sparse (ğ‘‘) + ( 1 âˆ’ ğ›¼) Â· ğ‘†dense (ğ‘‘) (1) We set ğ›¼ = 0.35 to give slightly higher weight to dense retrieval while maintaining meaningful contribution from sparse retrieval. This choice is motivated by the prior findings of Mosquera et al . [13], where a hybrid retrieval with ğ›¼ = 0.35 yielded the best perfor- mance. Their experiments on regulatory texts demonstrated that this balance improves overall retrieval performance. RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering LiveRAG@SIGIR 2025, July 13â€“17, 2025, Padua, Italy Q1: Where is Mount Everest, and what is its height? Q1-D1-A1 0.7 0.2 0.5 The height of MountEverest is 8,848meters [1][2]. Scoring + Filtering Generating Answer + Inline-Citations Generating Answers 0.7 0.5 Q1-D2-A2 Reformulation Retrieval Agent-4Agent-3 Q': In which country is Mount Everest located? A': Mount Everest islocated in Nepal [3]. + Retrieval Generating Answer Agent-1 Agent-2 The height of MountEverest is 8,848meters [1][2]. The height of Mount Everest is8,848 meters [1][2], and it islocated in Nepal [3]. Query-Document- Answer Triplet Q1-D3-A3 Completeness Check Figure 1: Architecture of the RAGentA framework: (1) A hybrid retriever selects top-20 documents. (2) Agent-1 generates an initial answer. (3) Agent-2 filters Query-Document-Answer triplets. (4) Agent-3 produces a final answer with in-line citations. (5) Agent-4 checks completeness, optionally reformulates the query, and merges both answers. 4.2 Multi-Agent Architecture Our multi-agent architecture builds up on the MAIN-RAG frame- work [2], which employs a multi-agent system for document fil- tering and answer generation. In RAGentA, we preserve the core architecture of MAIN-RAG while introducing two key advance- ments: a substantial modification to Agent 3 and the addition of a novel Agent 4 dedicated to answer verification. Specifically, Agent 3 has been fundamentally redesigned to generate fine-grained in-line citations, thereby attributing each statement in the answer to its supporting evidence. In the following, we describe the roles and interactions of the four agents within the RAGentA framework. 4.2.1 Agent-1: Predictor. Following document retrieval, Agent-1 generates query-specific answers for each retrieved document, forming Query-Document-Answer triplets. This agent employs an LLM to analyze each document in the context of the given query and generates an answer based on its content. 4.2.2 Agent-2: Judge. This agent evaluates each documentâ€™s rele- vance by determining its utility in addressing the query. For each triplet, Agent-2 addresses the question: â€œIs this document relevant and supportive for answering the question?â€ generating a binary â€œYesâ€ or â€œNoâ€ assessment. We implement adaptive document filter- ing by calculating a relevance score derived from the difference be- tween logarithmic probabilities: score = log ğ‘ (â€œYesâ€) âˆ’ log ğ‘ (â€œNoâ€). After scoring all documents, the system computes the mean score (ğœğ‘) and standard deviation (ğœ), establishing a dynamic threshold as adjusted_ğœğ‘ = ğœğ‘ âˆ’ ğ‘› Â·ğœ, where ğ‘› is a hyperparameter control- ling filtering stringency. According to empirical analyses from the MAIN-RAG study, optimal performance is achieved at ğ‘› = 0.5. Uti- lizing this dynamic threshold, the system retains documents where score â‰¥ adjusted_ğœğ‘ and filters those where score < adjusted_ğœğ‘. 4.2.3 Agent-3: Final-Predictor. Following the filtering of irrelevant documents, Agent-3 synthesizes a comprehensive answer using the remaining, relevant documents. The principal modification to Agent-3, relative to MAIN-RAG, is the generation of explicit in-line citations. Specifically, Agent-3 is instructed to generate citations in the standardized [ğ‘‹ ] format (where ğ‘‹ denotes the corresponding document identifier) using few-shot citation examples immediately after each factual assertion. As a result, each statement is treated as a discrete "claim", paired with its supporting citation(s), and extracted in a structured format. 4.2.4 Agent-4: Reviser. Agent-4 constitutes our most significant extension to the MAIN-RAG framework, conducting sophisticated multi-stage analysis to determine whether the query has been com- prehensively answered. The reviser operates through a systematic pipeline that begins by decomposing complex queries (containing multiple sub-questions) into constituent components and mapping each generated claim to specific aspects of the original question. This process evaluates whether claims effectively address corre- sponding question components while filtering irrelevant ones. Fol- lowing the initial mapping, Agent-4 employs regular expression functions to parse structured output and assess answer complete- ness by categorizing each component as â€œfully answered, â€ â€œpartially answered, â€ or â€œnot answered. â€ When gaps are identified, Agent-4 initiates a targeted follow-up process: it generates questions to address these gaps and uses the hybrid retrieval system from initial phase, excluding previously retrieved documents to ensure com- plementary information. Agent-4 then generates answers for each follow-up question and integrates the new information into the original answer through answer synthesis. This iterative approach ensures complete coverage of all query components, resulting in a unified final answer that fully addresses the original question. LiveRAG@SIGIR 2025, July 13â€“17, 2025, Padua, Italy Ines Besrour, Jingbo He, Tobias Schreieder, and Michael FÃ¤rber 5 Evaluation We evaluate RAGentA with the 500 QA pairs of our synthetic dataset, which we described in Section 3. The evaluation is a two- stage process, where we first assess retrieval performance and then evaluate the generated answers for correctness and faithfulness. 5.1 Evaluation of Retrieval To assess the performance of our retrieval system, we employ two evaluation metrics: Recall@k and Mean Reciprocal Rank (MRR)@k, with k set to 20, as this corresponds to the number of documents provided to the RAGentA framework. Specifically, Recall@20 quan- tifies the proportion of relevant ground-truth evidence documents that are retrieved within the top 20 results, whereas MRR@20 mea- sures the rank of the first relevant ground-truth document in the retrieved set. We compare the performance of our hybrid retrieval system to that of two baseline models, namely BM25 and E5, to evaluate its relative performance. Table 2: Evaluation of Retrieval Performance Retrieval System MRR@20 Recall@20 BM25 0.4205 0.5020 E5 0.3476 0.4920 Hybrid 0.4290 0.5650 The evaluation results are presented in Table 2. Our hybrid re- trieval system achieves an MRR@20 of 0.4290, outperforming BM25 (0.4205) by +2.0% and E5 (0.3476) by +23.4%. For Recall@20, the hybrid retrieval reaches 0.5650, exceeding BM25 (0.5020) by +12.5% and E5 (0.4920) by +14.8%. These findings demonstrate the strength of the hybrid retrieval approach, which fuses sparse and dense approaches in the RAGentA framework. 5.2 Evaluation of Correctness and Faithfulness We aligned our evaluation of answer correctness and faithfulness closely with the autoevaluator used in the SIGIR LiveRAG chal- lenge. Instead of the closed-source Claude-3.5 Sonnet model, we use Llama-3.3-70B-Instruct as an LLM-as-a-judge, which receives the predicted answer, the ground-truth answer, and the cited passages. The LLM-as-a-judge assigns a correctness score in the range [âˆ’1, 2] and a faithfulness score in the range [âˆ’1, 1]. Correctness. This metric consists of two components: coverage, the portion of vital information in the ground-truth answer that is covered by the generated answer, inspired by the work of Pradeep et al. [15], and relevance, the portion of the generated answer that directly addresses the question, regardless of its factual correctness. Correctness is rated on a four-point scale: 2 The response correctly answers the user question and con- tains no irrelevant content. 1 The response provides a useful answer to the user question, but may contain irrelevant content that does not harm the usefulness of the answer. 0 No answer is provided in the response (e.g., â€œI donâ€™t knowâ€). -1 The response does not answer the question whatsoever. Faithfulness. This metric assesses whether the answer is grounded in the retrieved documents on a three-point scale, following the methodology proposed by Es et al. [3]: 1 Full support: all answer parts are grounded. 0 Partial support: not all answer parts are grounded. -1 No support: all answer parts are not grounded. Table 3: Evaluation of Correctness and Faithfulness Approach Correctness Faithfulness Standard RAG 0.8256 0.6362 RAGentA (Ours) 0.8346 0.7044 Table 3 presents the average correctness and faithfulness scores computed over the full set of 500 questions. We evaluate the per- formance of RAGentA in comparison to a baseline RAG approach (Standard RAG). Both methods utilize the same set of initially re- trieved documents to ensure a controlled comparison. Standard RAG does not incorporate agent-based reasoning and generates answers using a single prompt. The results show that RAGentA achieves a correctness score of 0.8348, slightly outperforming Stan- dard RAG (0.8256) by +1.1%. For faithfulness, RAGentA reaches 0.7044, exceeding Standard RAG (0.6362) by +10.7%. 6 Conclusion In this work, we introduced RAGentA, a multi-agent RAG frame- work designed to enhance the trustworthiness of attributed QA. Our results demonstrate that a hybrid retrieval system combining BM25 and E5 significantly improves Recall@20 (+12.5%) compared to the best single model. RAGentA outperforms the standard RAG base- line, particularly in faithfulness (+10.7%), showing that providing in-line citations and conducting a second-stage retrieval for answer revision yield more relevant documents and well-grounded answers. While correctness sees only modest gains (+1.1%), our analysis indi- cates that the second-stage retrieval currently offers limited added value and may benefit from further refinement, such as adding the document filtering by early-stage agents. Finally, we acknowledge that the four-agent design, while effective in boosting correctness, introduces substantial computational overhead. This trade-off high- lights the need for future research to balance performance and efficiency in multi-agent RAG systems. The source code and exper- imental results presented in this study are publicly available in our Git repository at https://github.com/faerber-lab/RAGentA. Acknowledgments The authors acknowledge the financial support by the Federal Min- istry of Research, Technology and Space of Germany and by SÃ¤ch- sische Staatsministerium fÃ¼r Wissenschaft, Kultur und Tourismus in the programme Center of Excellence for AI-research â€Center for Scalable Data Analytics and Artificial Intelligence Dresden/Leipzigâ€œ, project identification number: ScaDS.AI. The authors also acknowledge computing resources provided by the NHR Center at TU Dresden, supported by the Ministry of Research, Technology and Space and the participating state gov- ernments within the NHR framework. RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering LiveRAG@SIGIR 2025, July 13â€“17, 2025, Padua, Italy References [1] Moshe Berchansky, Daniel Fleischer, Moshe Wasserblat, and Peter Izsak. 2024. CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity. In Findings of the Association for Computational Linguistics: EMNLP 2024 . ACL, Miami, Florida, USA, 236â€“246. https://doi.org/10.18653/v1/2024.findings-emnlp. 13 [2] Chia-Yuan Chang, Zhimeng Jiang, Vineeth Rakesh, Menghai Pan, Chin- Chia Michael Yeh, Guanchu Wang, Mingzhi Hu, Zhichao Xu, Yan Zheng, Ma- hashweta Das, and Na Zou. 2024. MAIN-RAG: Multi-Agent Filtering Retrieval- Augmented Generation. arXiv:2501.00332 [cs.CL] [3] Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. RAGAs: Automated Evaluation of Retrieval Augmented Generation. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations . ACL, St. Julians, Malta, 150â€“158. https: //aclanthology.org/2024.eacl-demo.16/ [4] Constanza Fierro, Reinald Kim Amplayo, Fantine Huot, Nicola De Cao, Joshua Maynez, Shashi Narayan, and Mirella Lapata. 2024. Learning to Plan and Generate Text with Citations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . ACL, Bangkok, Thailand, 11397â€“11417. https://doi.org/10.18653/v1/2024.acl-long.615 [5] Simone Filice, Guy Horowitz, David Carmel, Zohar Karnin, Liane Lewin-Eytan, and Yoelle Maarek. 2025. Generating Diverse Q&A Benchmarks for RAG Evalua- tion with DataMorgana. arXiv:2501.12789 [cs.CL] [6] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Cha- ganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023. RARR: Researching and Revising What Language Models Say, Using Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . ACL, Toronto, Canada, 16477â€“16508. https://doi.org/10.18653/v1/2023.acl-long.910 [7] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling Large Language Models to Generate Text with Citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing . ACL, Singapore, 6465â€“6488. https://doi.org/10.18653/v1/2023.emnlp-main.398 [8] Chengyu Huang, Zeqiu Wu, Yushi Hu, and Wenya Wang. 2024. Training Language Models to Generate Text with Citations via Fine-grained Rewards. In Proceedings of the 62nd Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers) . ACL, Bangkok, Thailand, 2926â€“2949. https://doi.org/10.18653/v1/2024.acl-long.161 [9] Jie Huang and Kevin Chang. 2024. Citation: A Key to Building Responsible and Accountable Large Language Models. In Findings of the Association for Computational Linguistics: NAACL 2024 . ACL, Mexico City, Mexico, 464â€“473. https://doi.org/10.18653/v1/2024.findings-naacl.31 [10] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. ACM Trans. Inf. Syst. 43, 2, Article 42 (2025), 55 pages. https://doi.org/10.1145/3703155 [11] Jisoo Jang and Wen-Syan Li. 2024. AU-RAG: Agent-based Universal Retrieval Augmented Generation. In Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (Tokyo, Japan) (SIGIR-AP 2024). ACM, New York, NY, USA, 2â€“11. https://doi.org/10.1145/3673791.3698416 [12] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems, Vol. 33. Curran Associates, Inc., Red Hook, NY, USA, 9459â€“9474. https: //dl.acm.org/doi/abs/10.5555/3495724.3496517 [13] Jhon Stewar Rayo Mosquera, Carlos RaÃºl De La Rosa Peredo, and Mario Garrido CÃ³rdoba. 2025. A Hybrid Approach to Information Retrieval and Answer Genera- tion for Regulatory Texts. InProceedings of the 1st Regulatory NLP Workshop (Reg- NLP 2025). ACL, Abu Dhabi, UAE, 31â€“35. https://aclanthology.org/2025.regnlp- 1.5/ [14] Nilay Patel, Shivashankar Subramanian, Siddhant Garg, Pratyay Banerjee, and Amita Misra. 2024. Towards Improved Multi-Source Attribution for Long-Form Answer Generation. In Proceedings of the 2024 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) . ACL, Mexico City, Mexico, 3906â€“3919. https://doi.org/10.18653/v1/2024.naacl-long.216 [15] Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick Craswell, and Jimmy Lin. 2025. The Great Nugget Recall: Automating Fact Extrac- tion and RAG Evaluation with Large Language Models. arXiv:2504.15068 [cs.IR] [16] Jirui Qi, Gabriele Sarti, Raquel FernÃ¡ndez, and Arianna Bisazza. 2024. Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Gen- eration. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. ACL, Miami, Florida, USA, 6037â€“6053. https://doi.org/10. 18653/v1/2024.emnlp-main.347 [17] Pritika Ramu, Koustava Goswami, Apoorv Saxena, and Balaji Vasan Srinivasan. 2024. Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing . ACL, Miami, Florida, USA, 17790â€“17806. https://doi.org/10.18653/v1/2024.emnlp-main.985 [18] S. E. Robertson and S. Walker. 1994. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. In ACM SIGIRâ€™94(Dublin, Ireland). Springer-Verlag, Berlin, Heidelberg, 232â€“241. https://doi.org/10.1007/ 978-1-4471-2099-5_24 [19] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In TRECâ€™94, Vol. 500-225. NIST, Gaithersburg, USA, 109â€“126. http://trec.nist.gov/pubs/trec3/papers/city.ps.gz [20] Yasser Saeid and Thomas Kopinski. 2024. AgentFusion: A Multi-Agent Ap- proach to Accurate Text Generation. In2024 International Conference on Electrical and Computer Engineering Researches (ICECER) . IEEE, Gaborone, Botswana, 1â€“8. https://doi.org/10.1109/ICECER62944.2024.10920460 [21] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen R. Pfohl, Heather Cole-Lewis, Darlene Neal, Qazi Mamunur Rashid, Mike Schaekermann, Amy Wang, Dev Dash, Jonathan H. Chen, Nigam H. Shah, Sami Lachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise AgÃ¼era y Arcas, Nenad TomaÅ¡ev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle K. Barral, Dale R. Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2025. Toward expert-level medical question answering with large language models. Nature Medicine 31, 3 (01 Mar 2025), 943â€“950. https://doi.org/10.1038/s41591-024-03423-7 [22] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A Large Language Model for Science. arXiv:2211.09085 [cs.CL] [23] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2024. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv:2212.03533 [cs.CL] [24] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. InProceedings of the 36th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS â€™22). Curran Associates Inc., Red Hook, NY, USA, Article 1800, 14 pages. https://dl.acm.org/doi/10.5555/3600270.3602070 [25] Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang. 2024. IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR â€™24). ACM, New York, NY, USA, 730â€“740. https: //doi.org/10.1145/3626772.3657760 [26] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2024. Benchmarking Large Language Models for News Summarization. Transactions of the Association for Computational Linguistics 12 (2024), 39â€“57. https://doi.org/10.1162/tacl_a_00632 [27] Jun Zhao, Can Zu, Xu Hao, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. LONGAGENT: Achieving Question Answering for 128k- Token-Long Documents through Multi-Agent Collaboration. InProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. ACL, Miami, Florida, USA, 16310â€“16324. https://doi.org/10.18653/v1/2024.emnlp-main.912 [28] Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, and Lei Sha. 2024. ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. ACL, Miami, Florida, USA, 10902â€“10919. https://doi.org/10. 18653/v1/2024.emnlp-main.610