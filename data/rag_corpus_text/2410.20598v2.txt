arXiv:2410.20598v2 [cs.IR] 5 Nov 2024 R3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation Zihan Wang University of Amsterdam Amsterdam, The Netherlands zihanwang.sdu@gmail.com Xuri Ge University of Glasgow Glasgow, United Kingdom x.ge.2@research.gla.ac.uk Joemon M. Jose University of Glasgow Glasgow, United Kingdom joemon.jose@glasgow.ac.uk Haitao Yu University of Tsukuba Tsukuba, Japan yuhaitao@slis.tsukuba.ac.jp Weizhi Ma Tsinghua University Beijing, China mawz12@hotmail.com Zhaochun Ren Leiden University Leiden, The Netherlands z.ren@liacs.leidenuniv.nl Xin Xin Shandong University Shandong, China xinxin@sdu.edu.cn Abstract Retrieval-augmented generation (RAG) has gained wide atte ntion as the key component to improve generative models with exter nal knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and perfor mance of large language model (LLM)-based applications. However , with the comprehensive application of RAG, more and more problem s and limitations have been identiﬁed, thus urgently requiri ng fur- ther fundamental exploration to improve current RAG frameworks. This workshop aims to explore in depth how to conduct reﬁned and reliable RAG for downstream AI tasks. To this end, we propose to organize the ﬁrst R 3AG workshop at SIGIR-AP 2024 to call for participants to re-examine and formulate the basic principles and practical implementation of reﬁned and re- liable RAG. The workshop serves as a platform for both academ ia and industry researchers to conduct discussions, share ins ights, and foster research to build the next generation of RAG syste ms. Participants will engage in discussions and presentations focusing on fundamental challenges, cutting-edge research, and pot ential pathways to improve RAG. At the end of the workshop, we aim to have a clearer understanding of how to improve the reliability and applicability of RAG with more robust information retrieva l and language generation. CCS Concepts • Information systems → Information retrieval ; • Comput- ing methodologies → Natural language generation . Permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided that copies ar e not made or distributed for proﬁt or commercial advantage and that copies bear this n otice and the full cita- tion on the ﬁrst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is pe rmitted. To copy other- wise, or republish, to post on servers or to redistribute to l ists, requires prior speciﬁc permission and/or a fee. Request permissions from permissi ons@acm.org. SIGIR-AP ’24, December 9–12, 2024, Tokyo, Japan © 2024 Copyright held by the owner/author(s). Publication r ights licensed to ACM. ACM ISBN 979-8-4007-0724-7/24/12 https://doi.org/10.1145/3673791.3698435 Keywords Retrieval-Augmented Generation, Information Retrieval,Large Lan- guage Models, Reliability ACM Reference Format: Zihan Wang, Xuri Ge, Joemon M. Jose, Haitao Yu, Weizhi Ma, Zhaochu n Ren, and Xin Xin. 2024. R 3AG: First Workshop on Reﬁned and Reliable Retrieval Augmented Generation. In Proceedings of the 2024 Annual Inter- national ACM SIGIR Conference on Research and Development i n Informa- tion Retrieval in the Asia Paciﬁc Region (SIGIR-AP ’24), December 9–12, 2024, Tokyo, Japan.ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3673791.3698435 1 BACKGROUND AND MOTIV ATIONS RAG (Retrieval-Augmented Generation) has emerged as a new par- adigm for using information retrieval (IR) to improve the generated response from large language models (LLMs). On the one hand, traditional IR systems may encounter diﬃculties to handle m ore and more complex information seeking queries. On the other hand, LLM has shown notable natural language understanding capab il- ity while suﬀering from ﬁctitious or inaccurate generation , also known as the hallucination problem. To this end, RAG has emerged to combine the best of IR and LLM generation. RAG has made sig- niﬁcant progress in improving response quality by ﬁrst retr ieving relevant knowledge from external knowledge base and then ge n- erating responses based on the knowledge retrieval. RAG’s a dvan- tages in handling information queries include but not limit ed to enhanced user experience, enriched information return, im proved response accuracy, and multi-round conversational searchfor com- plex queries. Through integrating IR and language generation, RAG has become the keystone for various AI applications. Existing RAG techniques [7, 14] focused on enhancing language models by integrating additional textual knowledge from ex ter- nal knowledge database. Transformer-based [18] language models have shown great promise in language generation, leading to no- table LLMs like GPTs. LLMs owe their success to advanced arch i- tectures with billions of parameters, pre-trained on vast c orpora from diverse sources, enabling remarkable generalization across SIGIR-AP ’24, December 9–12, 2024, Tokyo, Japan Zihan Wang e t al. various AI applications [4, 8, 12]. However, LLMs also suﬀer from model hallucination [13] and diﬃculty in handling dynamic k nowl- edge updates [19]. RAG alleviates the hallucination proble m by providing LLMs with relevant knowledge using IR techniquesto re- trieve from external databases, achieving more accurate responses for knowledge-intensive generation. The decoupled databa se also supports more lightweight knowledge dynamic updates. For exam- ple, [15] integrates a RAG pipeline in an end-to-end generation sys- tem to improve the factual correctness of LLMs for domain-speciﬁc and time-sensitive queries. While RAG has achieved great advancement, we recognize that there still exists challenges to conduct reﬁned and reliabl e RAG (R3AG). A typical RAG pipeline often includes user intent compr e- hension, knowledge parsing, knowledge retrieval, and resp onse generation. Each pipeline step has speciﬁc challenges and p lays an essential role to accomplish user queries. For example, h ow to understand user query intention under long and complex dialogue context; how to parse complex knowledge documents including ta- bles and ﬁgures; how to conduct reliable knowledge retrieva l; and how to reﬁne the generated response. The workshop is expecte d to help researchers to conduct further investigation on R 3AG. Topic. The topic of this workshop includes interesting points re- lated to the RAG pipelines, i.e., user intent comprehension , knowl- edge parsing, knowledge retrieval, and response generatio n. The detailed topics are described in section 2. Relevance. On the one hand, generative LLMs are hot research topics in the IR communities. The capability of LLMs enriche s the scope of IR and has changed the process of information seeking to a large extend. On the other hand, RAG is one of the most important applications of IR in LLMs. IR plays a new and essential role i n LLM-based downstream tasks. To this end, this workshop of R3AG is highly relevant to the SIGIR-AP conference. Motivation. Recently, the potential of RAG has been veriﬁed in various AI applications. However, there still exists fundamental re- search challenges to further improve current RAG methods. SIGIR- AP is one of the leading conferences with numerous recognized re- search works focusing on IR-related research and applications. We believe that organizing the R 3AG workshop with SIGIR-AP now can (i) stimulate interesting research to meet complex info rma- tion seeking demands; (ii) encourage the community to condu ct in-depth research and practical applications on reﬁned and reli- able RAG; (iii) expand the impact of SIGIR-AP conferences and the workshop. 2 FUNDAMENTAL CHALLENGES AND TOPICS This year we will focus more on fundamental challenges in thi s ﬁeld and expect thorough discussions during the R 3AG workshop. User Intent Comprehension.User intent comprehension directly aﬀects the following retrieval and ﬁnal response generatio n. How- ever, user intent comprehension could be challenging espec ially under long dialogue context. R 3AG is expected to help users clar- ify their information needs during the interaction process. Besides, R3AG should have the capability to split complex queries into s im- ple queries to accomplish user demands. Related methods inc lude query expansion, which introduces hypothetical answer gen era- tion from LLMs into the retrieval process to improve the retr ieval relevance, query summarization [11], query rewrite [16], e tc. Query & Knowledge Encoding. The eﬀectiveness of RAG de- pends heavily on the representation learning of both querie s and knowledge. Mainstream methods use pre-trained feature ext rac- tors [10, 17] to encode them, while there could be a misalignm ent between diverse knowledge and user queries. Currently, ﬁne-turning based query-knowledge encoding plays an important role in R AG. However, there still lacks suﬃcient exploration to conduct more ef- fective query & knowledge encoding for R3AG, especially in terms of eﬃcient ﬁne-tuning to support knowledge updates. RAG for Complex Documents.RAG systems aim to deliver knowl- edge chunks to improve LLM generation. However, existing RA G methods could encounter diﬃculties to parse complex docume nts with embedded tables and ﬁgures. How to parse complex tabula r knowledge is still an open research question. Besides, the c hunks division strategy also aﬀects response generation. Too large chunks introduce irrelevant information while small chunks could lead to incomplete knowledge. In addition, multi-hop document ret rieval is also a challenge. The above three issues need to be further inves- tigated to conduct R 3AG. Reliable Retrieval for RAG. The inclusion of noisy or contradic- tory information during retrieval can signiﬁcantly impair the per- formance of existing RAG models. There has been growing interest in improving the resilience of RAG against harmful or counte rfac- tual inputs, which is now considered a crucial performance i ndi- cator in recent studies [5, 20, 21]. Meanwhile, current rese arch [9] reveals that including irrelevant documents can unexpecte dly in- crease accuracy by more than 30%, challenging the initial be lief that it would degrade quality. These ﬁndings inspire us to organize R3AG for developing specialized strategies for eﬀective retr ieval and to emphasize the ongoing need for further investigation into RAG’s robustness and reliability. Response Evaluation and Reﬁnement. While RAG enhances LLMs by providing additional information, LLMs may still fa ce challenges with unreliable or inaccurate response generation. These challenges may arise from incorrect information in the cont ext [6] or issues with hallucinations [2]. Unfortunately, there is a notice- able gap in understanding how these challenges impact the ou tput quality of RAG, as well as in developing strategies for model s to mitigate these issues and reﬁne their responses. As such, R 3AG is dedicated to comprehensively evaluating and enhancing t he re- sponse quality of RAG-based LLMs across multiple dimension s, such as relevance, faithfulness, negative rejection, info rmation in- tegration, creative generation, and error correction. Multimodal R3AG. Most current research focuses on textual RAG, despite the need for multimodality in many applications. Re cent large-scale models like Flamingo [3], and GPT-4 [1] show sig nif- icant multimodal capability when scaled to tens of billions of pa- rameters and trained on extensive multimodal corpora. These large multimodal models require RAG even more than unimodal textu al LLMs for external knowledge support. Therefore, the worksh op encourage discussions regarding R 3AG for large multimodal mod- els. R3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation SIGIR-AP ’24, December 9–12, 2024, Tokyo, Japan 3 PROGRAM SKETCH 3.1 Workshop Format The workshop is planned to be hosted for half a day, including 2 invited talks and 4 oral research talks. There are two encour aging types of invited talks: (i) academic talks on fundamental re search on the adaptability and reliability of RAG techniques; and ( ii) in- dustrial talks on the practice of designing or applying reﬁn ed and reliable RAG techniques for real-world applications, incl uding in- formation retrieval and generative systems. Each talk should be delivered as a slide-based lecture. A Q&A session will follow the conclusion of each talk. 3.2 Tentative Workshop Schedule The workshop schedule is planned with one half-day session: - 9:00 - 9:10 Welcome & opening - 9:10 - 9:50 Academic invited talk - 9:50 - 10:30 Industrial invited talk - 10:30 - 10:50 Coﬀee break - 10:50 - 11:40 Oral paper talks - 11:40 - 12:00 Panel discussion Tentative Speakers. The tentative speakers include academia re- searchers, such as Dr. Xiangyu Zhao from the City University of Hong Kong, and Prof. Hideo Joho from University of Tsukuba an d industrial staﬀ, such as Dr. Alexandros Karatzoglou from Am azon. Panel Discussion. We are also considering hosting a panel dis- cussion as the ﬁnal part of the workshop. The decision to incl ude this session will depend on the availability of panelists at tending SIGIR-AP and the number of accepted research papers. Contingency Plan. To ensure a successful workshop, our contin- gency plan identiﬁes key requirements (venue, speakers, materials) and assesses risks. We will have backup venues, alternate speakers, and a ready team for onsite support. A designated lead will oversee a communication plan, with speciﬁc roles for last-minute or ganiz- ers to manage registration, technical support, and logisti cs. 3.3 Selection Process Each invited speaker should be highly esteemed within the co m- munity. Invitations should be agreed upon by all organizers with- out any disagreement. The workshop accepts paper submissio ns via a standard peer-review process, expects 6 ∼10 paper submis- sions, and accepts 3 ∼4 papers. Each submission is evaluated by at least two members of the program committee. The senior PCs or workshop organizers will make the ﬁnal decision. Authors will receive detailed review comments and a notiﬁcation letter. Tentative Program Committee. In addition to the current 7 or- ganizers, we also plan to invite the following tentative PC m em- bers: (1) Dr. Chao Huang from University of Hong Kong, (2) Dr. Xiangyu Zhao from the City University of Hong Kong, (3) Dr. An - drew Yates from University of Amsterdam, and (4) Dr. Alexand ros Karatzoglou from Amazon. 3.4 Online Materials A website for the R 3AG workshop will be made available online. All relevant materials, including talk information, presentation slides, referred papers, speaker details, and related open-source projects, will be accessible on this website. 3.5 Workshop Advertisement The R 3AG workshop will be promoted on various social media platforms to increase visibility and encourage paper submi ssions. These platforms include, but are not limited to, Twitter, Fa cebook, and WeChat. Additionally, the organizers will send persona lized emails to further advertise the workshop. 4 RELATED WORKSHOPS List of related workshops: • Information Retrieval’s Role in RAG Systems (SIGIR 20241) • Multimodal Representation and Retrieval (SIGIR 20242) • Information Retrieval Meets Large Language Models (WWW 2024 3) • Large Knowledge-Enhanced Models (IJCAI 20244) • Knowledge Retrieval and Language Models (ICML 20225) The Information Retrieval’s Role in RAG Systems workshop at SI- GIR (2024) explored retrieval’s integral role in RAG framew orks. As multimodal LLMs and RAG gain traction, the Multimodal Rep - resentation and Retrieval workshop at SIGIR (2024) introduced the challenge of multimodal queries and documents. The Informa tion Retrieval Meets LLMs workshop at WWW (2024) addressed issue s like retrieval-generation collaboration and hallucination. The Large Knowledge-Enhanced Models workshop at IJCAI (2024) discus sed integrating LLMs with symbolic knowledge, while the Knowle dge Retrieval and Language Models workshop at ICML (2022) high- lighted the limitations of knowledge retrieval. R 3AG is the ﬁrst workshop to focus on reﬁned and reliable RAG techniques. It w ill feature invited talks, paper presentations, and the releas e of real datasets and code for future practice. 5 ORGANIZERS INFORMATION Prof. Joemon M. Jose is a Professor at the School of Computing Science, University of Glasgow, Scotland and a member of the In- formation Retrieval group. His research focuses on the foll owing three themes: (i) Social Media Analytics; (ii) Multi-modal LLMs for information retrieval; (iii) Multimedia mining and search . He has published over 300 papers with more than 10,000 Google Schol ar citations, and an h-index of 51. He leads the GAIR Lab investigating research issues related to the above themes. He has been serv ing as the program committee chair and member for numerous top international conferences (e.g., SIGIR, WWW, and ECIR). He also serves as a PC chair for SIGIR-AP 2024. 1https://coda.io/@rstless-group/ir-rag-sigir24 2https://mrr-workshop.github.io/ 3https://irmeetsllm.github.io/ 4https://lkm2024.openkg.org/ 5https://knowledge-retrieval-workshop.github.io/ SIGIR-AP ’24, December 9–12, 2024, Tokyo, Japan Zihan Wang e t al. Dr. Zhaochun Ren is an Associate Professor at Leiden University. His research interests focus on joint research in IR and natu ral lan- guage processing, with an emphasis on conversational information seeking, question-answering, and recommender systems. He aims to develop intelligent systems that can address complex use r re- quests and solve core challenges in both information retrie val and natural language processing towards that goal. In addition to his academic experience, he worked on e-commerce search and recom- mendation at JD.com for 2+ years. He has co-organized workshops at SIGIR (2020), WSDM (2019, 2020), and ECIR 2025. Dr. Haitao Yu is a Tenured Associate Professor at University of Tsukuba and leading the Information Intelligence research group. His research focuses on Information Retrieval, Knowledge G raph, and Machine Learning. He has published numerous papers on to p international conferences (e.g., WSDM, CIKM, SIGIR, WWW, ECIR, and AAAI) and journals (e.g., Information Processing and Manage- ment, and Information Retrieval Journal). He is the co-organizer of the NTCIR tasks of Temporalia-2 and AKG. He has been serving as the program committee member for numerous top international conferences (e.g., WSDM, CIKM, SIGIR, and ECIR). Dr. Xin Xin is a Tenure-Track Assistant Professor at the School of Computer Science and Technology of Shandong University. Be- fore that, he earned his Ph.D. degree from the University of G las- gow. His current research interests include information re trieval, natural language processing, and reinforcement learning. He has published more than 40 papers in top conferences (e.g., WWW, SI- GIR, ACL, WSDM) and journals (e.g., TOIS, TKDE), and receive d the Best Paper Honor Mention at WSDM 2024. He has organized the DRL4IR workshop in SIGIR and KEIR workshop in ECIR. Dr. Weizhi Mais a Research Assistant Professor at the Institute for AI Industry Research (AIR) at Tsinghua University. He earne d his B.E. and Ph.D. in the Department of Computer Science and Tech - nology from Tsinghua University. Dr. Ma’s research focuses on information retrieval, recommender systems, and LLM-powe red agents. He has published over 70 papers in leading internati onal conferences and journals, including TOIS, TKDE, SIGIR, KDD, AAAI, IJCAI, WSDM, CIKM, etc. His accolades include the Best Paper Honorable Mention Award at SIGIR 2020 and several other pape r awards. In addition to his research, Dr. Ma is the Secretary o f the Youth Working Committee of the Chinese Information Processing Society of China and serves as an Assistant Editor for ACM TOI S. He is a recipient of the Young Elite Scientists Sponsorship Program by CAST and the Shuimu Tsinghua Scholar Program. Zihan Wang is a fourth-year PhD student at the University of Amsterdam (UvA). He has published over ten papers in prestigious conferences including KDD, SIGIR, CCS, and WSDM. He receive d the Best Student Paper Award at WSDM 2018. His current research focuses on information extraction, knowledge graph embedd ing, and the reliability of large language models. Xuri Ge is a fourth-year PhD student at the University of Glas- gow (UofG). He has published over 15 papers in prestigious co n- ferences and journals, including ACM MM, SIGIR, NeurIPS, IP &M, CIKM, ICME and ACM TIST, etc. His current research focuses on information retrieval, eﬃcient multimodal representatio n learning, and multimodal large language models. Xuri has organized 3DMM workshop in IEEE ICME2024. References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad , Ilge Akkaya, Flo- rencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sa m Altman, Shya- mal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Ni cholas Meade, and Siva Reddy. 2024. Evaluating Correctness and Faithfulness of Instruction- Following Models for Question Answering. Trans. Assoc. Comput. Linguistics 12 (2024), 681–699. [3] Jean-Baptiste Alayrac, Jeﬀ Donahue, Pauline Luc, Antoi ne Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Mal colm Reynolds, Ro- man Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, ZhitaoGong, Sina Saman- gooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borge aud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, R icardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022. Flamingo: a Vi- sual Language Model for Few-Shot Learning. In NeurIPS. [4] Mohammad Alkhalaf, Ping Yu, Mengyang Yin, and Chao Deng. 2024. Applying generative AI with retrieval augmented generation to summa rize and extract key clinical information from electronic health records. Journal of Biomedical Informatics (2024), 104662. [5] Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, an d Sung Ju Hwang. 2023. Knowledge-Augmented Language Model Veriﬁcation. In EMNLP. 1720– 1736. [6] Ning Bian, Hongyu Lin, Peilin Liu, Yaojie Lu, Chunkang Zh ang, Ben He, Xianpei Han, and Le Sun. 2023. Inﬂuence of external information on la rge language models mirrors social cognitive patterns. arXiv preprint arXiv:2305.04812 (2023). [7] Sebastian Borgeaud, Arthur Mensch, Jordan Hoﬀmann, Tre vor Cai, Eliza Ruther- ford, Katie Millican, George Bm Van Den Driessche, Jean-Bap tiste Lespiau, Bog- dan Damoc, Aidan Clark, et al. 2022. Improving language mode ls by retrieving from trillions of tokens. In ICML. 2206–2240. [8] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Be nchmarking large language models in retrieval-augmented generation. In AAAI, Vol. 38. 17754– 17762. [9] Florin Cuconasu, Giovanni Trappolini, Federico Sicili ano, Simone Filice, Ce- sare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The power of noise: Redeﬁning retrieval for rag systems. arXiv preprint arXiv:2401.14887 (2024). [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for langu age understanding. arXiv preprint arXiv:1810.04805 (2018). [11] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Al ex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local t o global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130 (2024). [12] Seﬁka Efeoglu and Adrian Paschke. 2024. Retrieval-Aug mented Generation- based Relation Extraction. arXiv preprint arXiv:2404.13397 (2024). [13] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhang yin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin , et al. 2023. A survey on hallucination in large language models: Principl es, taxonomy, chal- lenges, and open questions. arXiv preprint arXiv:2311.05232 (2023). [14] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fa bio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock- täschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieva l-Augmented Genera- tion for Knowledge-Intensive NLP Tasks. In NeurIPS. [15] Jiarui Li, Ye Yuan, and Zehua Zhang. 2024. Enhancing llm factual accuracy with rag to counter hallucinations: A case study on domain-speciﬁc queries in private knowledge-bases. arXiv preprint arXiv:2403.10446 (2024). [16] Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2024. RaFe: Ranking Feedback Improves Query Rewriting for RAG. arXiv preprint arXiv:2405.14431 (2024). [17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand- hini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin , Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In ICML. 8748–8763. [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. At tention is all you need. In NeurIPS. 5998–6008. [19] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2023. Editing large language models: Prob- lems, methods, and opportunities. In EMNLP. [20] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making retrieval-augmented language models robust to irrelevant context. arXiv preprint arXiv:2310.01558 (2023). R3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation SIGIR-AP ’24, December 9–12, 2024, Tokyo, Japan [21] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hong wei Wang, and Dong Yu. 2023. Chain-of-note: Enhancing robustness in retr ieval-augmented language models. arXiv preprint arXiv:2311.09210 (2023). This figure "acm-jdslogo.png" is available in "png" format from: http://arxiv.org/ps/2410.20598v2 This figure "sample-franklin.png" is available in "png" format from: http://arxiv.org/ps/2410.20598v2