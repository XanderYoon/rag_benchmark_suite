arXiv:2411.11895v1 [cs.IR] 7 Nov 2024 DEPLOYING LARGE LANGUAGE MODELS WITH RETRIEVAL AUGMENTED GENERATION Sonal Prabhune and Donald J. Berndt University of South Florida Tampa, Florida, USA {saprabhune@usf.edu, dberndt@usf.edu} ABSTRACT Knowing that the generative capabilities of large language models (LLM) are sometimes hampered by tendencies to hallucinate or create non-factual respons es, researchers have increasingly focused on methods to ground generated outputs in factual data. Retr ieval-Augmented Generation (RAG) has emerged as a key approach for integrating knowledge from data sources outside of the LLM’s training set, including proprietary and up-to-date inform ation. While many research papers explore various RAG strategies, their true efﬁcacy is tested in real -world applications with actual data. The journey from conceiving an idea to actualizing it in the real world is a lengthy process. We present insights from the development and ﬁeld-testing of a pilot pr oject that integrates LLMs with RAG for information retrieval. Additionally, we examine the impac ts on the information value chain, encom- passing people, processes, and technology. Our aim is to ide ntify the opportunities and challenges of implementing this emerging technology, particularly wi thin the context of behavioral research in the information systems (IS) ﬁeld. The contributions of thi s work include the development of best practices and recommendations for adopting this promising technology while ensuring compliance with industry regulations through a proposed AI governance model. Keywords Retrieval Augmented Generation · Large Language Models · AI Governance · Human Artiﬁcial Intelligence Systems 1 Introduction Recently, the landscape of artiﬁcial intelligence (AI) has changed drastically. With the emergence of large language models and their capabilities of generating textual respon ses based on various prompting techniques. A recent article by Bloomberg Intelligence (BI) [1] on the explosive growth of generative AI in the next d ecade predicts, “With the inﬂux of consumer generative AI programs like Google’s B ARD and OpenAI’s C HATGPT, the generative AI market is poised to explode, growing to $1.3 trillion over the next 1 0 years from a market size of just $40 billion in 2022.” As we develop cutting-edge applications that use large lang uage models, we must explore the various possibilities, understand the shortcomings, and develop ways to circumven t any obstacles to ensure a reliable human-artiﬁcial intelligence system. There are different approaches, even within the narrow scop e of implementing a solution, that use a large language model (LLM) to retrieve information from proprietary unstr uctured documents. Current approaches for retrieving information from unstructured text include ﬁne-tuning an L LM, parameter efﬁcient ﬁne-tuning approaches such as LORA [2], preﬁx tuning [3], prompt tuning [4], p-tuning [5], va rious approaches in prompt engineering such as zero- shot [6], few-shot [7], chain-of-thought [8] and R EACT [9]. It is essential to understand the business case and appl y appropriate approaches, often requiring experimentation such as prototyping and evaluating what works best. V arious libraries and tools have emerged to support the fast develop ment of LLM systems based on information retrieval from proprietary data. [10] [11] [12] These tools enable the LLMs to behave as agents, [13] function as reasoning engines, and invoke various tools supplied to the LLM to generate the required outcome. In addition, the LLM itself can be used to evaluate the efﬁcacy of these generated outcomes. What re mains for those implementing the Retrieval Augmented Generation (RAG) system is to make the right choices of these tools and to design a system that is most effective Deploying Large Language Models with Retrieval Augmented G eneration for their particular use cases. As simple as this might seem, the complexity arises from needing to implement such a system at scale while conforming to the industry’s regulato ry standards. As with any emerging technology, there is a lag between advan cements in large language models (LLMs) with Retrieval-Augmented Generation (RAG), and their adoption and implementation in the industry. We believe that valu- able insights can be gained from the practical application o f RAG with LLMs, which can help advance research and facilitate the next breakthroughs. This paper seeks to shar e these learnings and bridge the gap between the challenges encountered in the industry and the issues being tackled by r esearchers. We draw parallels between the challenges mentioned in a recent paper [14], which discusses key aspect s of human interactions with AI systems, and those we encountered in our ﬁeld study. We suggest best practice reco mmendations for organizations venturing into the space of generative AI with RAG. We hope that this paper, detailing our challenges and insights, will not only serve as a valuable guide for organizations planning to implement RAG-based solutions using LLMs but also provide researchers with a deeper understanding of the real-world challenges fa ced in the industry when adopting new technology. The structure of this paper is as follows: We begin by explori ng the research efforts aimed at enabling LLMs to adapt to domain-speciﬁc contexts, focusing on ﬁne-tuning a nd prompt engineering. Following this, we delve into the work conducted with RAG to date. We then provide the backg round for our ﬁeld study and detail the design of the pilot project, discussing the various challenges faced by the industry in developing such a system and the wide array of choices available for each component of the RAG syst em. Next, we outline the implementation of our pilot project, including the design choices and our approach to ex ecution. We also address the importance and challenges of evaluating such a system, reviewing the different evaluati on strategies, including those we employed. Our ﬁeldwork insights, best practices, and the necessity of AI governanc e are discussed subsequently. Finally, we examine the implications of deploying these systems in production, wit h a particular focus on the challenges of human-artiﬁcial intelligence collaboration, to promote the development of robust, reliable, and compliant AI solutions across variou s industries. The paper concludes with a summary of our ﬁnding s. 2 Related Work Prior to LLMs, the approaches to retrieval of information fr om unstructured text included statistical and machine learning-based approaches. Examples of statistical appro aches include bag-of-words and term frequency-inverse doc - ument frequency (TF-IDF) [15]. For instance, the TF-IDF app roach has been used for extracting ontological data from unstructured text [16]. On the other hand, machine learning approaches are based on generating word embeddings such as W ORD 2VEC [17], which captures the idea of similarity between words wi thin documents. Recently, new technologies have emerged, such as large lang uage models that generate new text and embeddings from existing documents. Although very promising in their c apabilities, these models can suffer from hallucinations, generating responses that are not grounded in facts. LLMs co me up with their responses mainly based on next-word probability [18]. These hallucinatory tendencies impede t he application of LLMs to mainstream industry applications because of obvious questions about the reliability and robu stness of their responses [19] [20]. To address this, a lot of research has been published that suggests a variety of appro aches which are summarized in table 1 Figure 1 provides an overview of the LLM landscape highlight ing the use of both proprietary ( A) and public ( B) documents. An LLM is typically trained on vast amounts of pub lic data but can be customized by additional training on proprietary documents (using full or parameter-efﬁcien t ﬁne-tuning). There could even be an overlap between proprietary and public documents ( A ∩ B) if private documents are released or public documents are r etracted. Of course, the documents generated by an LLM can be re-cycled as training data as well. Using an LLM involves prompt engineering with or without examples, which can be dr awn from seen or unseen documents. Several possible approaches to reduce hallucinations are outlined below. • Full ﬁne-tuning : The pre-trained LLM understands the basic language constr uct, but it is not adapted for domain-speciﬁc context and vocabulary. This approach basi cally modiﬁes the LLM’s weights to adopt the context from the domain-speciﬁc dataset. It is one of the old est approaches for domain-speciﬁc tuning of an LLM. • Parameter-efﬁcient ﬁne-tuning: In this approach, the LLM is ﬁne-tuned by modifying only a sm all number of parameters [2] [4] rather than all of them, as in the case of full ﬁne-tuning. This approach reduces the load with respect to computations, memory use and execution time . • Prompt engineering: Prompt engineering is an approach where the LLM is given ins tructions to generate a certain response. Different approaches have been shown to enhance the quality of responses generated using prompt engineering. Some of them include zero-shot pr ompting [6], where the model is given only instructions but no examples, few-shot prompting where the model is given instructions and a few examples 2 Deploying Large Language Models with Retrieval Augmented G eneration Figure 1: Large Language Model Landscape [7], chain-of-thought prompting [8] where the model is guid ed through steps for problem-solving, R EACT [9] where the model is guided using reasoning and task-speci ﬁc actions in an interleaved manner, or some combination of these different approaches. 2.1 Retrieval Augmented Generation Retrieval-Augmented Generation (RAG) is an approach that e nhances the capabilities of Large Language Models (LLMs) by augmenting their prompts with information retrie ved from a knowledge base that the LLM has not seen during training. This method adds domain-speciﬁc context t o the generated responses, leveraging the LLM’s ability to generate text while incorporating embeddings from existing documents stored in vector databases. These databases are speciﬁcally designed to store, query, and retrieve sentenc e embeddings, making them integral to the RAG approach. The ﬁeld of RAG has rapidly evolved, as highlighted by variou s studies such as those surveying various RAG method- ologies [21], [22], [23], [24], [25] and exploring advanced implementations like RAG- END 2END [26]. The table 1 summarizes various research efforts in the ﬁeld o f Retrieval-Augmented Generation (RAG), each address- ing different aspects and challenges of integrating retrieval mechanisms with large language models (LLMs) as well as the evaluation challenges of RAG systems. Key studies include the foundational work [19] that showed how incorporat- ing retrieval into individual tasks with a single retrieval -based architecture is capable of achieving strong perform ance 3 Deploying Large Language Models with Retrieval Augmented G eneration across several tasks. The survey [21] highlights RAG’s appl ications across multiple NLP tasks but notes areas needing improvement, such as optimization and multi-modalities. O ther surveys [22], [25], and [24] further categorize and review RAG techniques, discussing the limitations and futu re directions for RAG implementation in the industry, as well as discussing different RAG paradigms, including thei r application to various data stores and knowledge graphs. Practical evaluations and benchmarking are addressed in st udies [27] and [28], which propose new methods for as- sessing RAG performance and robustness. V arious studies pr opose performance improvement approaches, including [26], which explores advanced implementations like RAG- END 2END , [29], which proposes a “Hybrid with HyDE” method for retrieval, and [30] proposes a Forward-Looking A ctive REtrieval augmented generation (FLARE). Some studies [31] provide solutions on RAG methods across hetero geneous knowledge. Others explore improved evaluation approaches such as [32], which proposes a Corrective Retrie val Augmented Generation (CRAG), a retrieval evaluator that can be coupled with various RAG approaches to improve th e robustness of generation. These studies underscore the growing academic interest in RAG. Table 1: Review of Researches Exploring Biases in Generativ e AI - De- tails by Research Paper Research Paper Summary Main Findings Models Retrieval Retrieval- Augmented Genera- tion for Knowledge- Intensive NLP Tasks [19] This paper introduces general-purpose ﬁne- tuning for RAG models by combining pre- trained parametric and non-parametric memory, speciﬁcally integrating a pre- trained seq2seq model with a neural retriever. The study ﬁnds that RAG models produce more speciﬁc, diverse, and factual language than a parametric-only seq2seq baseline, reducing "hallu- cinations" by grounding generated text in factual data. BART, BERT, RAG DPR A Survey on Retrieval- Augmented Text Generation [21] This survey summa- rizes key components of retrieval-augmented text generation, includ- ing retrieval metrics, sources, integration paradigms, and appli- cations like dialogue response generation and machine transla- tion. RAG beneﬁts dialogue systems, machine transla- tion, language modeling, and more, but still needs improvement in retrieval sensitivity, optimization, and multi-modalities. NA NA Retrieval- Augmented Gen- eration for AI- Generated Content: A Survey [22] This survey categorizes RAG by augmentation methods for retriev- ers and generators, discusses RAG appli- cations across various modalities and tasks, offering references for researchers and practitioners. This survey categorizes RAG based on the differ- ent methodologies and applications providing diagrammatic represen- tations. It discusses the limitations and provides future directions. NA NA Evaluation of Retrieval- Augmented Gen- eration: A Survey [23] This survey addresses RAG evaluation chal- lenges and proposes a Uniﬁed Evaluation Pro- cess focused on targets, datasets, and measures. The analysis highlights the need for benchmarks that link retrieval ac- curacy with generative quality, considering real-world applications. NA NA Continued on next page 4 Deploying Large Language Models with Retrieval Augmented G eneration Research Paper Summary Main Findings Models Retrieval Retrieval- Augmented Gen- eration for Large Language Models: A Survey [24] This survey explores RAG paradigms (naive, advanced, and modu- lar RAG), key technolo- gies in retrieval, gen- eration, and augmen- tation, and introduces evaluation frameworks and benchmarks. The survey discusses op- timization strategies and retrieval types (iterative, recursive, and adaptive), and explores future prospects for scaling RAG for production- ready systems. NA NA Retrieval- Augmented Gen- eration for Natural Language Process- ing: A Survey [25] This survey reviews different techniques of RAG, especially in the retriever and the retrieval fusions. Provides algorithms for implementing and dis- cusses the RAG training for the application of RAG in representative natural language process- ing tasks and industrial scenarios NA NA Improving the do- main adaptation of retrieval augmented generation (RAG) models for open domain question answering [26] This paper introduces RAG-end2end, which uses an auxiliary training signal to incor- porate domain-speciﬁc knowledge and evalu- ates it across datasets from three domains. The RAG-end2end model shows better performance than the original RAG and improves DPR perfor- mance more effectively than ﬁnetuning. pre-trained BART DPR (Pretrained dense retriever) uses 2 BERT models, FAISS indexing Evaluating Retrieval Quality in Retrieval- Augmented Genera- tion [27] This study presents eRAG, a method where each document in the retrieval list is indi- vidually used by the LLM, and the output is evaluated against ground truth labels for downstream tasks. Experiments show that eRAG improves corre- lation with downstream RAG performance and Kendall’s tau, while also being up to 50 times more efﬁcient in GPU memory usage compared to end-to-end evaluations. T5-small with Fusion-in- Decoder (FiD) as LLM, Mistral for evaluation of retrieved documents BM25, Faiss Benchmarking Large Language Models in Retrieval- Augmented Genera- tion [28] This study evaluates LLMs on noise robust- ness, negative rejection, information integration, and counterfactual ro- bustness for RAG. Findings indicate that LLMs exhibit a certain degree of noise robust- ness, struggle in terms of negative rejection, information integration, and dealing with false information. ChatGPT- 3.5-turbo, ChatGLM-6B, ChatGLM2-6B, Vicuna-7B-v1.3, Qwen-7B-Chat, BELLE-7B-2M Google’s API for web search and an open-source dense retriever (not speciﬁed) Searching for Best Practices in Retrieval- Augmented Gen- eration [29] This study evaluates various solutions for RAG modules and recommends the most effective approach for each. he study ﬁnds that the best performance is achieved using the “Hy- brid with HyDE” method for retrieval, monoT5 for reranking, Reverse for repacking, and Recomp for summarization. GPT-3.5-turbo- 0125, GPT-3.5- turbo-instruct, Zephyr-7b- alpha, monoT5, monoBERT, TILDEv2, Ran- kLLaMA etc. Approaches - HyDE, Hybrid Search, databases - Weaviate, Faiss, Chroma, Qdrant, Milvus, etc. Active Retrieval Augmented Genera- tion [30] This paper introduces FLARE, which gener- ates a temporary next sentence to retrieve rel- evant documents based on low-probability to- kens. Findings show that FLARE performs better than single-time retrieval on four benchmarked datasets Any LLM but they used gpt-3.5- turbo, GPT-3.5 text-davinci-003 FLARE Continued on next page 5 Deploying Large Language Models with Retrieval Augmented G eneration Research Paper Summary Main Findings Models Retrieval Retrieval-augmented Generation across Heterogeneous Knowledge [31] This study provides solutions on RAG methods across hetero- geneous knowledge Solutions like homogeniz- ing different knowledge, multi-virtual hops re- trieval, and reasoning over structured knowl- edge for RAG across heterogeneous data. T5 Dense retriever model DPR Corrective Retrieval Augmented Genera- tion [32] This study introduces Corrective Retrieval Augmented Generation (CRAG) as a retrieval evaluator to enhance the robustness of RAG approaches. Experiments on four datasets show that CRAG signiﬁcantly enhances RAG-based performance in both short- and long- form tasks. s LLaMA2- chat13B, Chat- GPT, CoVE65B LLaMA2- 7B,13B, Alpaca- 7B,13B, SAIL, T5-based Re- trieval Evaluator Despite the extensive research, there is a noticeable lack o f ﬁeld studies and industry-shared experiences that could bridge the gap between theoretical research and practical a pplication. For RAG systems to fully realize their potentia l, research must be conducted in tandem with industry implemen tations. This collaboration would help address real- world challenges, such as managing heterogeneous data sources, integrating separate knowledge bases, and optimizing system performance for production environments. Sharing ﬁ eld studies and insights from industry implementations can signiﬁcantly enhance research, enabling more robust so lutions that cater to the complex demands of real-world applications. Given the complexity of real-world applications, particul arly in industry settings that involve intricate socio-tec hnical systems, more detailed research is needed to study the imple mentation and deployment of RAG systems in the ﬁeld as pilot projects in the industry and their maturation into p roduction. Such research should assess the maturity of this approach, identify and address process gaps, and gener ate ideas to enhance the reliability and adoption of RAG systems. Collaboration between industry and academia is, t herefore, essential, as sharing the challenges faced in the ﬁeld can signiﬁcantly advance research on LLMs and RAG. Furt hermore, it is crucial to establish best practices based on industry learning and to create a governance framework that ensures compliance with regulatory requirements when deploying RAG technologies. 3 Design The advent of large language models (LLMs) like C HATGPT has revolutionized the way information can be retrieved from proprietary documents. However, given the relative im maturity of these models, their integration into socio- technical systems must be meticulously designed to ensure t hat the ﬁnal solution is both reliable and accurate. This design process involves extensive experimentation with pr ompt engineering, innovative strategies for data chunking , indexing, and retrieval, as well as iterative and thorough t esting of the entire system. In this section, we explore the challenges and opportunitie s of implementing RAG systems in production. We outline the key technological decisions required when designing su ch a system, focusing on the various components of RAG. We begin by discussing the need for and background of develop ing an RAG-based system for information retrieval in our ﬁeld study, highlighting the main pain points in the ex isting system and the areas where RAG with LLMs was evaluated to add value. We then detail our design process, em phasizing our approach in light of user readiness for this human-AI system. 3.1 Domain Background In our ﬁeld study, focused on developing a pilot solution for an IT product company, the challenge of integrating LLMs like C HATGPT into socio-technical systems became particularly rele vant. The variability of product versions across different customer environments in IT companies demands a t rained support team capable of delivering prompt and accurate assistance. Addressing this need required a caref ully designed system involving extensive experimentation with prompt engineering, innovative data chunking, and ind exing methods, as well as iterative testing and reﬁnement to ensure the reliability and accuracy of the LLM-based solu tion. In IT organizations, support is often structured in multipl e tiers, including internal and customer-facing support. A l- though the initial design of our system was not meant for dire ct customer access, it was essential to develop it in a way 6 Deploying Large Language Models with Retrieval Augmented G eneration Figure 2: Domain Background Analysis - Identiﬁcation of Pro blem areas in the space that allows customer support teams to efﬁciently retrieve t he necessary information to address customer inquiries and escalations. As the system’s reliability and accuracy are v alidated, there may be potential to explore its direct use by end customers. In Figure 2, we identify key problem areas in the current stat e. The ﬁrst pain point is the frequent back-and-forth communication between the customer support team and the engineer- ing team, which is time-consuming and disruptive. This chal lenge stems from the need for customer support teams to meet Service Level Agreements (SLAs) by providing timely an d accurate responses to customer tickets. Ensuring the accuracy of responses requires veriﬁcation across various product documents, which differ by product and version. By improving the efﬁciency of information retrieval from thes e documents, these pain points can be mitigated, enabling support teams to meet SLAs more effectively. The second pain point is the fragmented nature of informatio n sources within most organizations. Information is often spread across multiple knowledge management systems, tick eting systems, databases, and other formats, each in its own silo. The integration of information across these dispa rate systems is complex and presents a signiﬁcant challenge . 3.2 Challenges and Opportunities In most organizations, retrieving information is not just a bout obtaining data; it’s about doing so quickly, accuratel y, and in a user-friendly manner. The ideal solution would allo w for natural language queries without requiring users to have expertise in complex querying languages like SQL. Ad ditionally, the information retrieved should be easy to understand or even summarized for the user’s convenience. A signiﬁcant challenge lies in the evolution of knowledge ma nagement systems alongside the organization itself. This evolution often results in inconsistencies in data formats , with information being both structured and unstructured. Organizations typically have a wide variety of document for mats, such as .doc, .docx, .pdf, .csv, .xls, .txt, and HTML, among others. Furthermore, this information is often scatt ered across multiple systems, including various knowledge management frameworks, ticketing systems, and databases, each siloed with its own format. Integrating this diverse information into a cohesive system is inherently complex an d requires a nuanced approach. Beyond these technical challenges, several external facto rs inﬂuence the adoption and implementation of emergent technologies like Large Language Models (LLMs): 7 Deploying Large Language Models with Retrieval Augmented G eneration • Legal and governance : Many organizations are cautious about exposing proprieta ry documents to external LLMs due to internal legal and compliance requirements. Thi s includes concerns about data security and the risks associated with hosting sensitive information outsi de the organization’s infrastructure or sharing it with third-party vendors. • Financing: Budgetary constraints often dictate whether an organizat ion can implement a solution in-house or must opt for outsourcing. While outsourcing might provid e a ready-to-use solution, it often comes with recurring subscription costs that can strain organization al budgets. On the other hand, in-house development requires a team with specialized skills for ongoing develop ment, maintenance, and support. • V olatility: The landscape of LLM technologies is rapidly evolving, with new libraries and frameworks emerg- ing frequently. This volatility presents challenges not on ly for in-house development but also when negoti- ating contracts with third-party vendors. Organizations m ust carefully consider the total cost of ownership, including the long-term costs associated with keeping the s ystem up-to-date. When developing a Retrieval-Augmented Generation (RAG) system, particularly when using Large Language Models (LLMs) with proprietary documents, organizations must est ablish clear functional, non-functional, and technical re - quirements. (Please refer to Appendix A). In our ﬁeldwork, w e began this process by understanding the speciﬁc needs and expectations of stakeholders, including end-users, de velopers, and business leaders. The functional requiremen ts focus on the system’s core capabilities, such as natural lan guage understanding, contextual awareness, and integra- tion with existing systems. The non-functional requiremen ts, on the other hand, emphasize performance aspects like scalability, security, and reliability, ensuring the syst em can operate efﬁciently under varying conditions. Techni cal requirements detail the necessary infrastructure, tools, and frameworks that support the system’s development and operation. Stakeholder requirements and acceptance crite ria further reﬁne the project by aligning the system’s fea- tures with user needs and setting clear benchmarks for succe ss. Throughout this process, involving key teams—such as legal, compliance, engineering, customer support, and q uality assurance—ensured that the system not only met technical and functional standards but also adhered to orga nizational policies and regulations. During the initial phase, we conducted feasibility checks t o deﬁne the scope of a Minimum Viable Product (MVP) and to plan a roadmap for incremental development. This appr oach is crucial as it ensures that the system design remains ﬂexible enough to evolve over time, allowing for adj ustments as the technology and organizational needs change. Feasibility analysis and careful planning are crit ical to the overall success of the project, ensuring that the ﬁnal product meets both the technical and business requirem ents of the organization. To ensure the solution met user needs, we assembled a cross-f unctional team to collaboratively design the system. This approach enabled users to actively participate not onl y in deﬁning requirements and conducting ﬁnal acceptance testing but also in the iterative development and evaluatio n phases. By involving users throughout the process, the design was continuously reﬁned, aligning more closely with their expectations and operational realities. 3.3 User Readiness To assess user readiness and the acceptance of the new genera tive AI-based Information Retrieval System, we dis- tributed a pre-survey questionnaire among the pilot group o f users selcted randomly across various departments. This survey was a key step in our process, as it aimed to capture the users’ familiarity with generative AI, their expecta- tions, potential concerns, and overall sentiment towards t he new technology. (Please refer to Table 2 for the pre-surve y analysis summary) • Experience with Generative AI: The survey revealed that 54% of users had prior experience with generative AI, though the level of expertise varied. This indicated a mo derate level of familiarity within the user base, suggesting that additional training might be necessary to b ring all users to a similar level of comfort. • Perceived Beneﬁts: Users highlighted several potential beneﬁts of the AI syste m, including improved accu- racy and relevance of retrieved information, faster access to data, and enhanced troubleshooting capabilities. There was also a strong expectation that the system would str eamline cross-functional communication and reduce the need for frequent meetings. About 8% of users, how ever, were unsure whether AI would signiﬁ- cantly impact their efﬁciency, indicating a need for clear d emonstrations of the system’s value. • Expectations: Enthusiasm for the application was high, with some users ant icipating that it would foster inclusivity, transparency, and collaboration across the o rganization. However, 20% of respondents were uncertain about what to expect, reﬂecting a need for better c ommunication and expectation management. • Concerns: While many users were excited, there were notable concerns, including issues related to ques- tion phrasing, the accuracy and reliability of AI responses , data privacy and security, and the potential for over-reliance on AI. Some users also feared that such a tool c ould eventually replace human jobs. Notably, 8 Deploying Large Language Models with Retrieval Augmented G eneration Table 2: Pre-Survey Analysis Summary Survey Aspect Summary Percentage Departments The pilot users came from various departments within the organization, including engineering, customer support, a nd legal. N/A Experience with Genera- tive AI Users with prior experience in generative AI at various lev- els of expertise. 54% Perceived Beneﬁts Users anticipated that generative AI would improve efﬁ- ciency by providing accurate and relevant information, aid - ing in troubleshooting, and offering fast access to product - speciﬁc data. N/A Unsure About Beneﬁts Users who were unsure if AI can make them or their teams more efﬁcient. 8% Cross-Functional Commu- nication Users expected the AI to streamline communication, reduc- ing the need for meetings and increasing efﬁciency. N/A Unsure but Optimistic Users who were uncertain but optimistic about AI’s impact on cross-functional communication. 8% General Enthusiasm Users who were enthusiastic about the application, be- lieving it could unify the organization and increase trans- parency. N/A Unsure of Expectations Users who were unsure of what to expect from the applica- tion. 20% Concerns Concerns included phrasing questions correctly, AI accu- racy, data privacy, over-reliance on AI, and potential job displacement. N/A No Concerns Users who had no concerns about using the AI application. 41% Data Needs Users wanted the AI to retrieve data from various enterprise systems and document formats. N/A Uncertain About Data Needs Users who were uncertain about their data expectations. 8% Excitement and Gratitude Users who expressed excitement and gratitude for being part of the pilot program. 25% 41% of respondents did not express any concerns, suggesting varying levels of apprehension about the new technology. • Data Needs: The majority of users expressed a need for the system to retri eve data across various enterprise systems, in addition to documents in multiple formats. Abou t 8% of users were uncertain about what data the AI should provide, indicating a need for clearer guidelines on the system’s capabilities. • General Sentiment: Overall, the sentiment was positive, with 25% of users expre ssing excitement and grat- itude for being part of the pilot program. This enthusiasm wi ll likely aid in the system’s adoption, provided that their concerns are adequately addressed through train ing and iterative improvements. This pre-survey analysis underscores the importance of add ressing user concerns and managing expectations while leveraging the enthusiasm and existing familiarity with ge nerative AI to facilitate a smooth rollout of the new system. (Please refer to Appendix B for details on our pre-survey que stions and its analysis). In the development of Retrieval-Augmented Generation (RAG ) systems, selecting the right model, vector database, and cloud provider is critical to ensuring optimal performa nce and alignment with business objectives. The landscape of available technologies is vast, with each offering uniqu e features, scalability options, and integration capabili ties. For instance, different LLMs vary in their context windows, multimodal capabilities, and accessibility, inﬂuencing their suitability for speciﬁc use cases. Similarly, the cho ice of vector databases, directly impacts the system’s abil ity to efﬁciently manage and search through large datasets. Fur thermore, the cloud provider must be carefully chosen to support the desired scalability, security, and complian ce needs. Each of these decisions must be tailored to the speciﬁc requirements of the business case, as the right comb ination can signiﬁcantly enhance the effectiveness of the RAG system while ensuring it meets the operational demands a nd strategic goals of the organization. In the following subsections, we explore the available options for each of th ese key components of the RAG system. 9 Deploying Large Language Models with Retrieval Augmented G eneration 3.4 Model Selection Criteria Table 3: Comparison of Large Language Models (LLMs) LLM Organization Multimodal? Access Parameters T oken context window Open Source GPT OpenAI ✓ Chatbot and API 175B+ for (GPT3). Not Pub- lished for GPT4 GPT-4 (32K ver- sion): Has a context length of 32,768 tokens ✗ Gemini Google ✓ Chatbot and API Upto 27B Parameters Upto 2M ✗ Llama3 Meta ✗ Chatbot and open Upto 405B Customizable model to extend token win- dow from 8k to > 1040K ✓ Claude Anthropic ✓ Chatbot and API Not Pub- lished Upto 200,000 tokens ✗ The table 3 compares several prominent LLMs, including GPT f rom OpenAI, Gemini from Google, Llama from Meta, and Claude from Anthropic. It details key attributes such as whether each model supports multimodal inputs, its access methods, parameter sizes, token context windows, and open- source availability. For instance, GPT is not open-source. In contrast, Llama, developed by Meta, is open-source and su pports up to 405 billion parameters but does not handle multimodal inputs. Gemini and Claude, from Google and Anthr opic, respectively, support multimodal inputs but have varying access methods and token context capabilities. Thi s comparison helps in selecting the most suitable LLM based on speciﬁc requirements such as model size, context wi ndow, and open-source needs. When selecting a large language model (LLM) for deployment, it is essential to begin with a thorough evaluation of models that have been benchmarked on recognized datasets . Resources such as the Open LLM Leaderboard [33] and survey research papers [34] provide valuable insights i nto the performance and suitability of different LLMs for speciﬁc use cases. However, the selection process must also consider the organ ization’s resource constraints and policies. Critical dec i- sions include whether to use locally hosted models like LLaMA or public API models like ChatGPT, as well as whether to opt for open-source models or proprietary solutions. The size of the model and the choice between versions should be guided by the available infrastructure for hosting, the c ost associated with token usage, and the overall billing rat es. While larger models generally excel at complex reasoning tasks, smaller models that have been ﬁne-tuned can often de- liver superior performance for speciﬁc tasks [35]. This mak es model selection not just a technical decision but also one that involves strategic consideration of the trade-offs be tween performance, cost, and alignment with organizationa l goals. 3.5 V ector Database Selection The table 4 provides a comparative overview of several vecto r stores—Weaviate, Faiss, Chroma, Qdrant, and Mil- vus—highlighting their key features, use cases, scalabili ty, and integration capabilities. Each vector store offers dis- tinct advantages tailored to speciﬁc needs in Retrieval-Au gmented Generation (RAG) implementations. For instance, Weaviate excels in hybrid search with ﬂexible schema suppor t and built-in ML model integration, making it suitable for semantic search and knowledge graphs. Faiss, known for i ts high performance and various indexing strategies, is ideal for large datasets and similarity searches. Chroma st ands out with real-time search capabilities and a simple API , which is advantageous for applications requiring real-tim e vector search and personalization. Qdrant offers advance d indexing and distributed deployment, supporting similarity search and recommendation systems. Milvus, with its high performance and multiple indexing options, is well-suited for large-scale vector searches and data analysis. 10 Deploying Large Language Models with Retrieval Augmented G eneration Table 4: Comparison of various V ector Stores V ector Store Key Features Use Cases Scalability Integration Indexing Weaviate Hybrid search (vec- tor + keyword), Scalable, Flexible schema, Built-in ML model support, RESTful API / GraphQL Semantic search, Knowl- edge graphs, Recommenda- tion systems High, supports distributed de- ployment ML frameworks, REST API, GraphQL Hybrid search capabilities Faiss High performance, V arious indexing strategies, Customiz- able, Integrates with ML libraries Similarity search, Near- est neighbor search, Cluster- ing High, optimized for large datasets ML libraries, low- level control Flat, IVFPQ, HNSW Chroma Real-time search, Scalable, Simple API, ML model integration Real-time vector search, Recommenda- tion engines, Personaliza- tion High, designed for horizontal scaling ML models, sim- ple API Optimized for real-time search Qdrant Distributed deploy- ment, Advanced indexing, REST / gRPC APIs, Flexible data support Similarity search, Rec- ommendation systems, Per- sonalization High, supports distributed de- ployment REST & gRPC APIs, ﬂexible integration Advanced indexing techniques Milvus High performance, Distributed & scal- able, Multiple indexing options, Integration with data processing frame- works Large-scale vector search, Machine learn- ing, Data analysis High, supports distributed de- ployment V arious data processing frame- works, ML libraries IVF, HNSW , ANNOY Pinecone Managed serverless service Automatic scaling Compatible with embeddings from any AI model or LLM Hybrid Search - vector search with keyword boosting Rec- ommendation engines Large- scale vector search High, managed serverless service with auto-scaling Easy integration with popular ML frameworks, data sources, and mod- els Live index updates, Mul- tiple indexing strategies tailored for performance Elastic Relevance Search Engine Search Engine) Full-text search Relevance rank- ing Hybrid search (vector + keyword) Extensive ecosystem of plugins Full-text search E- commerce search Hybrid search High, supports distributed de- ployment and scaling Extensive APIs Supports a wide range of data types and ML integra- tions Extensive ecosystem of plugins Inverted in- dex k-NN search Dense vector search through plug- ins Selecting the right vector store is crucial for effectively implementing RAG systems in industry settings. The choice depends on factors such as the required scalability, the nat ure of the data, and integration needs with existing systems . For successful RAG deployment, organizations must match th e vector store’s features with their speciﬁc use cases and operational requirements. For example, businesses needin g robust distributed deployment and ﬂexibility might opt fo r Weaviate or Qdrant, while those focusing on real-time searc h and personalization could beneﬁt from Chroma. Under- standing these differences ensures that the selected vector store aligns with the project’s goals, optimizing perform ance and efﬁciency in handling diverse data sources and retrieva l tasks. 11 Deploying Large Language Models with Retrieval Augmented G eneration 3.6 Cloud V endor Selection and AI Ops Table 5: Comparison of AI Cloud Platforms A WS Microsoft Azure Google Cloud Platform (GCP) IBM Cloud Oracle Cloud Service Breadth and Depth Extensive AI/ML offerings for vari- ous use cases. Extensive AI/ML offerings for vari- ous use cases. Good in AI ser- vices like V ertex AI, Vision AI. Watson offers strong AI ser- vices, especially NLP and ML. Growing AI offer- ings but less com- prehensive. Ease of Use and Integra- tion Highly intuitive, strong integration with AWS ser- vices and other systems. Highly intuitive, strong integration with Azure ser- vices and other systems. Seamless with Google services; challenging with third-party. Developing, good with IBM ecosystem. Decent, but less user-friendly. Performance and Scala- bility Highly scalable AI-optimized infrastructure. Strong scalability with specialized VMs. Excellent per- formance in data-heavy tasks. Limited scalabil- ity, expensive. Decent, but less scalable. Cost Efﬁ- ciency Competitive pric- ing, expensive for large-scale AI. Flexible pricing, competitive. Cost-effective, especially with TPUs. Cost-effective Watson services, but requires higher invest- ment. Competitive, but Oracle-centric. AI- Optimized Hardware Advanced AI- platforms like Inferen- tia, Trainium, NVIDIA GPUs. Wide range of AI- optimized VMs with NVIDIA GPUs. Leading-edge TPUs for AI/ML workloads. PowerAI with NVIDIA GPUs, evolving. Solid hardware options, less innovation. Generative AI capabil- ity Comprehensive AI services, in- cluding Amazon Bedrock, Ama- zon Q, Amazon Lex Comprehensive AI services, ac- cess to OpenAI models (GT-4, Dall-E, Codex). Comprehensive AI services, in- cluding image and music gener- ation. Limited models, focus on textual services. Limited models, focus on chatbots, summarization. Support and Com- munity Large developer community, strong support. Large developer community, strong support. Large developer community, backed by Google AI re- search. Strong support, smaller commu- nity. Good support, smaller commu- nity. The success of any AI application, especially those leverag ing large language models, is closely tied to the infrastruc - ture and services provided by major cloud providers. Studie s [36] show that cloud giants like Amazon, Microsoft, and Google offer a comprehensive suite of AI and infrastruct ure services, including compute power, application de- velopment, security and compliance, and industry-speciﬁc solutions, underscoring the integral role of these cloud platforms in supporting AI development. These services are integral to the development, deployment, and scaling of AI applications. Choosing the right cloud vendor is a critical decision that m ust be made early in the project. This decision impacts not only the development and deployment phases but also the long -term sustainability and scalability of the AI application . Enterprises must plan for the ongoing management of AI opera tions (AI Ops) within their broader cloud strategy, ensuring that AI applications can evolve and adapt in line wi th both technological advancements and business needs. Table 5 provides a comparative analysis of major AI cloud platforms—AWS, Microsoft Azure, Google Cloud Platform (GCP), IBM Cloud, and Oracle Cloud—across several key perfo rmance indicators (KPIs) relevant to AI operations (AI Ops). The table examines the service breadth and depth, e ase of use and integration, performance and scalability, cost efﬁciency, AI-optimized hardware, generative AI capa bility, and the support and community offered by each platform. This comparison aims to highlight the strengths a nd limitations of each provider, helping users select the most appropriate cloud platform for their speciﬁc AI needs a nd operational contexts. 12 Deploying Large Language Models with Retrieval Augmented G eneration It is essential to ensure that the design process lays a robus t foundation for developing a reliable and efﬁcient system. By carefully selecting appropriate models, cloud infrastr ucture, and AI operations strategies, we align the system’s architecture with the organization’s technical requireme nts and user expectations. With the design principles ﬁrmly established, the next phase, the implementation phase, can focus on translating these design elements into a functiona l software prototype. The forthcoming Implementation secti on will detail the construction of the system, including workﬂow diagrams, prompt engineering, and semantic retrie val to enable the transformation of the design into a tangible, operational system. 4 Implementation In the implementation phase, we transitioned from design to development, bringing the planned system architecture to life through a functional software prototype. This phase fo cused on translating the theoretical framework and design principles into a tangible system, ensuring that each compo nent aligned with the speciﬁed requirements. We metic- ulously deﬁned the system architecture, emphasizing the in tegration of various modules and the ﬂow of information between them. The workﬂow, as depicted in Figures 3 and 4, sho wcases the algorithm’s overall structure, guiding the sequence of operations within the system. The subsequent se ctions provide a detailed breakdown of the implemen- tation steps, highlighting key decisions, challenges enco untered, and the strategies employed to achieve the desired functionality. 4.1 Workﬂow After outlining the system architecture and detailing the d esign principles, we focus on the operational workﬂow of the prototype. This workﬂow is central to the system’s funct ionality, ensuring that each component interacts seam- lessly with the others. The steps outlined below provide a cl ear path from data ingestion to the ﬁnal user interaction, illustrating how the system processes, indexes, and retrie ves information to meet user queries efﬁciently. In the following workﬂow subsection, we break down the syste m’s operations step by step, starting from how doc- uments are read and processed to how user queries are handled and responses generated. This structured approach ensures that each stage of the process is clearly understood , enabling a smooth transition from design to fully func- tional implementation. Algorithm 1 Algorithm for Retrieval Augmented Generation using LLM 1: Create a prompt template with basic instructions to the LLM a nd include placeholders for retrieved document, user query and chat history 2: Instantiate splitter to chunk documents 3: docs = Split documents using splitter 4: Instantiate embeddings = OpenAIEmbeddings() 5: Instantiate V ector Database Chroma to store the split docum ents "docs" and their embeddings 6: Specify the vector database search type and number of top mat ching results for retrieval 7: Instantiate retriever 8: Save vector database in a persistent directory 9: while Chat session not exited do 10: Get the query from the application’s UI 11: Generate Embeddings for the query 12: Search top 3 results from the vector database that have embed dings "similar" to the query embeddings 13: Update the prompt template with the query and retrieved docu ment and the chat history 14: Send the prompt to the LLM ChatGPT API 15: Receive the response from the ChatGPT API 16: Save information on query, retrieved document titles and re sponse in a log ﬁle 17: Reformat the response for presentation to the user via UI 18: end while 1. The PDF documents are read from a speciﬁc directory locati on. 2. They are split into chunks, with each chunk containing 100 0 characters and overlap of 50 between chunks. 3. Next, these chunks are indexed. Below is the sample code, w hich shows one approach to implementing this using L ANG CHAIN libraries. [10] 1 from langchain . indexes import V e c t o r s t o r e I n d e x C r e a t o r 13 Deploying Large Language Models with Retrieval Augmented G eneration Figure 3: Workﬂow for Document Pre-processing Figure 4: System Operation Workﬂow 14 Deploying Large Language Models with Retrieval Augmented G eneration 2 from langchain . embeddings . openai import OpenAIEmbedd i ng s 3 from langchain . text_splitter import CharacterT ex t Sp l i tt e r 4 from langchain . vectorstores import Chroma 5 6 text_splitter = Character Te x t Sp l it t er ( chunk_size =1000 , 7 chunk_overlap =50) 8 docs = text_splitter . split_documen t s ( documents ) 9 embeddings = OpenAIEmbed di ng s () 10 db = Chroma . from_document s ( documents = docs , embedding = e mbeddings ) 11 ret = db . as_retriever ( search_type = " similarity " , search_kwargs ={ " k ":3}) Listing 1: Initializing splitter for chunking documents an d also initializing embeddings model database and retrieve r. 4. After that, we create the embeddings. Since we are using C HATGPT as our large language model, we must use OpenAI’s embeddings. Below is the sample code, which sho ws one approach of implementing this. 5. The embeddings are stored in an instance of the C HROMA vector database. 1 from langchain . vectorstores import Chroma 2 from langchain . indexes import V e c t o r s t o r e I n d e x C r e a t o r 3 from langchain . embeddings . openai import OpenAIEmbedd i ng s 4 5 embeddings = OpenAIEmbed di ng s () 6 index = V e c t o r s t o r e I n d e x C r e a t o r( 7 text_splitter = text_splitter , 8 vectorstore _c ls = Chroma 9 ) . from_loaders ([ loader ]) 10 persist_di re ct o ry = ’ ../../ db ’ 11 if ( os . path . isdir ( persist_di re c to ry ) ) : 12 vectordb = Chroma ( 13 persist_dire c to r y = persist_directory , 14 embedding_f un c ti o n = embeddings ) 15 else : 16 vectordb = createDB ( 17 docs = docs , 18 embeddings = embeddings , 19 persist_di re ct o ry = persist_di re c to ry ) 20 retriever = db . as_retriever ( 21 search_type = " similarity " , 22 search_kwargs ={ " k ":3}) 23 r = retriever . g e t _ r e l e v a n t _ d o c u m e n t s( q ) [: top ] Listing 2: Chunking Indexing and Storing the embeddings in t he Chroma vector database and Initializing the retriever. 6. Steps 1 to 5 happen once in a separate application workﬂow. When a user uploads new documents, we repeat the process for those new documents. 7. The application has a React.js frontend and a Python Flask API backend. When a user asks the chatbot, for example, “What features are in the latest release of a particular product?” the question gets sent to the backend Python Flask API in a serialized JSON object along with the co ntext history. Note that here, C HATGPT is expected to understand the meaning of the “latest” release. It would have to scan through all the documents to identify which release is the latest in this case. 8. When the question arrives at the backend, we ﬁrst encode it using the same OpenAI embeddings used for encoding the document. This is necessary for retrieving sim ilar documents from the vector store. 9. We run a semantic search on the vector store, C HROMA , and return the top 3 matching documents. Below is the sample code, which shows one approach of implementing th is. 1 query = " What is in the March release ? " 2 3 retriever = vectordb . as_retriever ( search_type = " similarity " , 4 search_kwargs ={ " k ":3}) 5 r = retriever . g e t _ r e l e v a n t _ d o c u m e n t s( query ) [:3] Listing 3: Query the Chroma vector database for top 3 documen ts based on the user question. 15 Deploying Large Language Models with Retrieval Augmented G eneration 10. We then embed these documents as “sources” in the prompt a long with the question asked by the user and the context history and send it to the C HATGPT API. We are using the GPT-4-Turbo model [37]. 11. When we get a response from C HATGPT, we send it back to the user interface (UI) for formatting and presentation. Below is a sample of the response with proprie tary information redacted. [Document (page_content=’Summer Release 2022 Release Notes \n March 30, 2022 Release (Summer Release)\n The March 30, 2022 Release (Summer Release) contains the following information.\nNew Features\n Inventory Management.\n New User Interface.\n User Management.\n Additional Opportunities.’, metadata={ ’source’: ’/home/MyApp/data/Mar_2022_Release_Notes.p df’, ’page’: 10}), Document (page_content=’Summer Release 2023 Release Notes\n February 28, 2023 Release (Summer Release)\n The February 28, 2023 release (Summer Release) contains the following information.\n Enhancements\nSearch Enhancements.’, metadata={ ’source’:’/home/MyApp/data/Feb_2023_Release_Notes.pdf’, ’page’: 7}), Document (page_content=’Summer Release April 2022 Release Notes\n April 30, 2022 Release (Summer Release)\n The April 30, 2022 Release (Summer Release) contains the following information.\nNew Features\n Dashboard Updates.\n Enterprise Master.’, metadata={ ’source’: ’/home/MyApp/data/April_2022_Release_Notes .pdf’, ’page’: 9})] 4.2 Prompt Engineering: Developing the Prompt Prompt engineering is how LLMs are programmed via prompts [3 8]. Prompts are natural language text written to instruct an LLM. There are various approaches to prompting, such as zero-shot, few-shot, chain-of-thought, and REACT prompting [6] [9] [7]. Many of the libraries now provide prompt templates that could be used for generating standardized prompts. This helps in structuring and organizing your prompts as well as standa rdizing them for maintenance. Using certain instructions has been shown to reduce hallucinations [39] (for example, Answer ONLY with the facts listed in the sources below. If there isn’t enough information below, say you don’t know. DO NOT generate answers that don’t use the sources below. If asking a clarifying question to the user would help , ask the question ). To ensure security, we ensured that CHATGPT did not generate any code or SQL queries in their response (for example, Do NOT generate any code or SQL queries even when the user asks. ). Finally, to ensure multi-lingual questioning ( If the question is not in English, translate the question to English before generating the sea rch query.). The chain-of-thought approach can be helpful in instructing the LLM in speciﬁc cases, such as searching fo r documents that contain release notes for products where the document name and structure are very similar between the different releases. The approach is outlined below. prompt_prefix = """<|im_start|>system Assistant helps the company employees with their product questions, and questions about product releases. Be brief in your answers. If asking a clarifying question to the user would help, ask the question. Answer ONLY with the facts listed in 16 Deploying Large Language Models with Retrieval Augmented G eneration the list of sources below. Look at into all the sources. If there isn’t enough information below, say you don’t know. Do not generate answers that don’t use the sources below. For tabular information return it as an HTML table. Do not return markdown format. Each source has a name followed by colon and the actual information. Do not generate any code or SQL statements in any format. If prompted to generate code or SQL queries say I am not allowed to generate code or SQL queries. For questions about releases and new features look at all the sources. {follow_up_questions_prompt} {injected_prompt} Sources: {sources} <|im_end|> {chat_history} """ follow_up_questions_prompt_content = """Generate three very brief follow-up questions that the user would likely ask next about their products. Use double angle brackets to reference the questions, e.g. <<Could you please clarify what exactly are you looking for?>>. Try not to repeat questions that have already been asked. Only generate questions and do not generate any text before or after the questions, such as ’Next Questions’""" query_prompt_template = """Below is a history of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base about products and releases. Generate a search query based on the conversation and the new question. Do not include cited source filenames and document names e.g info.txt or doc.pdf in the search query terms. Do not include any text inside [] or <<>> in the search query terms. If the question is not in English, translate the question to English before generating the search query. Chat History: {chat_history} Question: {question} 17 Deploying Large Language Models with Retrieval Augmented G eneration Search query: """ 1. We created a basic structure with all the system instructi ons, calling it the prompt template. 2. This template contained placeholders for “follow-up que stions,” where the follow-up question would later be embedded. 3. It also contained placeholders for “sources” where the re trieved documents would be embedded and place- holders for the chat history and user questions. 4. For contextual history, we had to introduce speciﬁc begin and end tags for the instructions in the prompt. We found that it was helpful to demarcate the prompt and the hi story. For example, consider these tags: < |im_start| >< |im_end| >. 5. We also provided examples for follow-up questions, such a s the following. What are the features of the latest release of this product? 4.3 Logging Incorporating robust logging mechanisms is essential for t he effective operation and optimization of LLM-based sys- tems. Logging every input and output interaction with the LL M is not just a best practice for troubleshooting; it is also vital for ﬁne-tuning the model over time. Tracking the numbe r of tokens in each input, context, and response—whether the LLM is hosted locally or accessed via an API—provides val uable insights. This data is crucial for planning infras- tructure requirements and ensuring the scalability of the s ystem, particularly as trafﬁc increases. Logging also aids in understanding the model’s behavior, identifying patter ns of errors or inefﬁciencies, and making informed decision s about future adjustments to the system architecture. 4.4 Semantic Retrieval For the semantic search and retrieval component of our system, we conducted an extensive evaluation of various vector stores, both open-source and commercial, including C HROMA , P INECONE , and E LASTIC SEARCH . A key ﬁnding from our evaluation was that misalignments between the generate d responses and the user’s queries often stemmed from issues in the retrieval process rather than the LLM itself. To address this, we implemented functionality to display links to the documents retrieved during the search. This approach not only provided users with citations for fact-checking but also allowed us to diagnose and reﬁne our retrieval strat egies when incorrect documents were retrieved. We also implement relevance-checking approaches, which we will di scuss in the evaluation section. Initially, we chunked documents without overlap, but furth er experimentation revealed that overlapping chunks im- proved retrieval accuracy. We also varied the number of retr ieved results and tested different retrieval strategies, including similarity search, maximum marginal relevance, and hybrid approaches that combine keyword search with semantic retrieval. This iterative process helped us ident ify the optimal conﬁguration for our speciﬁc documents and use case. Recent research underscores the effectiveness of Retrieva l-Augmented Generation (RAG) approaches, particularly when combining semantic retrieval from vector databases wi th other techniques like knowledge graphs or ﬁne-tuning strategies. Knowledge graphs offer a complementary retrie val method, storing and querying information in the form of entity-attribute-attribute value triples. However, ge nerating knowledge graphs from proprietary documents pose s signiﬁcant challenges, especially when dealing with incon sistently structured documents that have evolved over time within various departments of an organization. Fine-tuning, including Parameter Efﬁcient Fine-Tuning (PEFT), also presents challenges. While PEFT is less resource - intensive than traditional ﬁne-tuning, it still requires c reating a training set, training the model, and evaluating i ts performance. This process can be time-consuming, making it slower to develop compared to simply storing embed- dings in vector databases. Nonetheless, research [40] indi cates that combining ﬁne-tuning with data augmentation can enhance retrieval accuracy, making it a viable option depen ding on the available resources, budget, and timeline. 5 Evaluation Incorporating quality assurance and testing from the early stages of product development is essential, especially whe n dealing with advanced technologies like LLMs and semantic r etrieval, which differ signiﬁcantly from traditional key- word search systems. The testing strategies, cases, and pla ns must be adapted to address the unique challenges posed 18 Deploying Large Language Models with Retrieval Augmented G eneration by LLMs. Additionally, evaluating the sociotechnical aspe cts of the system through user surveys provides valuable insights into the effectiveness and acceptance of the techn ology. To assess the qualitative aspects of the application, we con ducted pre- and post-surveys with identiﬁed pilot users. Th e pre-survey (Appendix B), discussed earlier, was aimed at ev aluating user readiness and expectations. After the pilot run, a post-survey (Appendix C) was conducted to gather feed back on the application’s performance, user satisfaction, and areas for improvement. Detailed results are tabulated i n Table 6 5.1 T est Strategies Testing LLM-based systems necessitates a distinct approac h from traditional software testing methodologies. Estab- lishing a ground truth dataset is crucial for assessing the s emantic accuracy of the generated responses. The testing process must be thorough, addressing both the generative an d retrieval components of the system. When evaluating applications developed for Retrieval-Aug mented Generation (RAG) with Large Language Models (LLMs) on proprietary documents, organizations should imp lement a robust testing strategy to ensure the accuracy, reliability, and adherence to organizational standards. K ey strategies to consider include: • Ground Truth Comparison Develop a robust ground truth dataset consisting of correct and validated responses for a set of queries relevant to the proprietary documents. This dataset serves as a bench mark to evaluate the performance of the RAG application. Regularly compare the outputs of the RAG syste m against the ground truth dataset to assess the accuracy of the generated responses. Focus on key metrics su ch as precision, recall, and relevance. • Scenario-Based T esting Create test scenarios that reﬂect real-world use cases, ens uring that the RAG application can correctly inter- pret and respond to complex queries within the appropriate c ontext. • Fact-Checking Implement automated fact-checking mechanisms to verify th at the RAG system’s generated content accu- rately reﬂects the source documents. One method could inclu de embedding links to the original documents to aid in veriﬁcation. • Consistency To assess the system’s consistency in responding to similar or related queries and to prevent contradictory outputs, an external LLM—often referred to as ’LLM-as-a-ju dge’ [41] —can be utilized. We applied this approach by using the external LLM to evaluate the cosine sim ilarity between responses generated for the same query to ensure consistency. Based on these evaluation s, we were able to adjust parameters such as temperature, top-p, frequency penalty, and presence penal ty. • Performance and Scalability T esting Load T esting: Assess the application’s performance under varying loads t o ensure it can handle high query volumes without degradation in response quality or speed. Scalability T esting:Evaluate the system’s capacity to handle increased data vol ume and user demand, ensur- ing consistent performance as the application expands. Whi le using API-based LLM calls, we encountered rate limits during peak hours, which could degrade applicat ion performance. Frequent rate limit issues may necessitate considering alternatives such as hosted LLMs o r setting token limits for API-based calls. Addi- tionally, scalability testing will inﬂuence the choice of c ontext summarization strategies and determine the hosting infrastructure requirements, including the need f or auto-scaling. • Security and Data Privacy T esting Evaluate the system to conﬁrm that proprietary data is not un intentionally disclosed in generated responses. V erify that the RAG application adheres to data privacy regu lations and organizational policies, especially when dealing with sensitive proprietary information. To ac hieve this, we involved our legal and compliance team from the onset of the project. • User Experience and Feedback Integration User Acceptance T esting (UA T):Engage end-users in the testing process to collect insights on the system’s usability, relevance, and overall performance. This feedb ack is essential for ﬁne-tuning the application. Iterative T esting with Feedback Loops: Establish continuous feedback loops where user input drive s sub- sequent testing and development, ensuring the application evolves to meet changing user requirements. We shared the post-survey responses from the UA T phase and, dur ing the pilot run, encouraged users to actively 19 Deploying Large Language Models with Retrieval Augmented G eneration provide feedback through internal communication channels and emails. We closely tracked the key likes and dislikes about the application. • Long-T erm Monitoring and Evaluation Post-Deployment Monitoring: Implement ongoing monitoring of the RAG system after deploy ment, track- ing performance metrics, user satisfaction, and complianc e with governance standards. Periodic Audits: Conduct regular audits of the system’s performance and comp liance, revisiting and updating the ground truth dataset and testing protocols as necessary . • Ethical and Explainability T esting Explainability Analysis: Assess the system’s capability to offer clear explanations for its outputs, ensur- ing transparency and comprehensibility for end-users. We a chieved this by embedding links to the source documents. Ethical Considerations: Regularly evaluate the application for adherence to ethica l standards, particularly concerning the use and interpretation of proprietary data. The ’ConstitutionalChain’ feature in L ANGCHAIN [10] can be utilized to set predeﬁned rules that the LLM must f ollow. 5.1.1 Local testing During this study, we explored some of the emerging solution s like T RULENS [42], D EEP EVAL [43], L ANG SMITH by L ANG CHAIN [10], LlamaIndex [11] etc., which provide tools and capabil ities for evaluating RAG systems. One signiﬁcant challenge with LLMs is their tendency to "halluc inate" when information retrieval fails. Our study found that irrelevant LLM responses often stemmed from retrieval errors, such as when a document or its relevant chunk was not correctly retrieved in the top-n results. Recent res earch [44] also identiﬁes various points of failure in RAG systems, underscoring the importance of thorough evaluati on. For instance, open-source frameworks like L LAMA INDEX [11] provide tools such as R AGAS for assessing RAG performance metrics, including faithfulness, relevancy, context precision, and context recall. To use these tools, d e- velopers must create a ground truth dataset to evaluate the a pplication’s generated responses. Since such frameworks are still under active development, organizations must car efully consider their reliability before adoption. Additi on- ally, organizations might consider employing a parallel de velopment team to devise an automated testing strategy and methodology. Regardless of the chosen approach, it is cruci al to thoroughly evaluate these metrics. 5.1.2 Red teaming Red teaming, or adversarial testing, is a critical strategy for rigorously evaluating the robustness and security of a R AG system, particularly when deployed in sensitive or high-st akes environments. This approach involves deliberately challenging the system with edge cases, adversarial prompt s, or scenarios designed to provoke biased, harmful, or toxic responses. The goal is to uncover potential vulnerabi lities and weaknesses that might not be evident during standard testing processes. To implement red teaming effectively, organizations can de ploy a secondary LLM, known as an adversarial LLM, to generate challenging inputs. This adversarial LLM can be co nﬁgured to explore the limits of the primary system’s capabilities, testing its response to extreme or unexpecte d situations. For example, it might introduce queries that p ush the boundaries of the system’s ethical guidelines or attemp t to elicit biased or misleading information. The insights gained from red teaming are invaluable for reﬁn ing the system’s safeguards, ensuring that it can handle a wide range of real-world scenarios without compromising o n ethical standards or data integrity. Additionally, this process can help in identifying areas where further trainin g or reinforcement of the LLM is necessary, contributing to the development of a more resilient and trustworthy AI sys tem. Regular red teaming exercises, combined with iterative improvements based on the ﬁndings, are essential for maintaining the long-term security and reliability of RAG systems in any operational environment. 5.1.3 User acceptance test with pilot users To evaluate the practical application of our system, we cond ucted alpha testing with pilot users, measuring both qualitative and quantitative metrics through post-survey questionnaires (Please refer Appendix C). This process hel ped assess the system’s ﬁtness, conﬁdence, and projectability , providing valuable insights into its readiness for broade r deployment. Please refer Table 6 The post-survey results of the RAG application highlighted a mix of opinions, though feedback was generally positive compared to initial expectations. While users were initial ly enthusiastic about the potential of generative AI to im- prove efﬁciency, communication, and transparency, the majority rated their ﬁrst impression of the RAG application as 20 Deploying Large Language Models with Retrieval Augmented G eneration Table 6: Post-Survey Analysis Summary Question No. Summary of Response Percentage 1. First impression of the RAG ap- plication Most users had an “Ok” ﬁrst im- pression of the RAG application. Majority 2. Rating of the RAG applica- tion Users felt the application needed improvements, with varying levels of feedback on the extent of the re- quired improvements. - Some improvements: 52% - Signiﬁcant improvements: 28% - Minor tweaks: 13% - Ready for use: 7% 3. Effectiveness with context estab- lished The application often provided the correct answers when context was established. Majority 4. Effectiveness without con- text The application rarely provided cor- rect answers without context. Majority 5. Helpfulness of citation links All users found the citation links helpful, with suggestions for im- provement. 100% 6. Recommendation to team Most users would recommend the application to their team, though some expressed reservations due to needed modiﬁcations. Majority 7. Productivity improvement Most users believed the application would moderately to signiﬁcantly improve their productivity. Majority "Ok." Speciﬁcally, 52% of users indicated that the applicat ion needed some improvements, 28% believed signiﬁcant enhancements were necessary, and only 7% considered it read y for immediate use. When the context was provided, users frequently received the answers they needed, but the application rarely delivered accurate results without context. Despite these challenge s, all users found the citation links beneﬁcial, with some suggesting further enhancements. Most users were inclined to recommend the application to their team, though a few noted the need for substantial modiﬁcations. Regarding productivity, most users believed the application would moderately to signiﬁcantly boost their efﬁciency, alignin g with the optimistic outlook from the pre-survey. Concerns regarding data privacy, reliability, and potential over-r eliance on AI continued to resonate, similar to the pre-surv ey apprehensions. Overall, the application demonstrated pot ential, but further development is required to fully meet us er expectations and realize its capabilities. 6 AI Governance and Best Practices As artiﬁcial intelligence (AI) advances, the importance of establishing robust governance frameworks and best prac- tices becomes increasingly critical. We review previous re search that highlights this necessity [45] [46] [47], which primarily addresses global issues concerning the developm ent, deployment, and regulation of AI, including its in- tended purposes and limitations. Additionally, research h as explored AI governance at the organizational level, whic h we use as the basis and adopt the deﬁnition stated [48]: "AI governance is a system of rules, practices, processes, a nd technological tools designed to ensure that an organiza- tion’s use of AI technologies aligns with its strategies, ob jectives, and values; complies with legal requirements; an d adheres to the ethical AI principles upheld by the organizat ion." Building on this deﬁnition and insights from our pilot study , we propose an AI governance model, particularly for emerging technologies such as Retrieval-Augmented Generation (RAG). RAG represents a signiﬁcant advancement in AI, enabling large language models (LLMs) to produce more ac curate and contextually relevant outputs by integrating external data sources that are not part of the original train ing data. However, the implementation of RAG in real-world scenarios presents unique challenges, particularly when t ransitioning from research concepts to industry applicati ons. In this section, we share the intersection of AI governance a nd the practical deployment of RAG, drawing on insights from this ﬁeld study. Here, we share the importance of AI gove rnance in ensuring that RAG implementations are not only effective but also aligned with ethical standards, reg ulatory requirements, and organizational goals. The study 21 Deploying Large Language Models with Retrieval Augmented G eneration Figure 5: AI Governance Model underscores the complexities involved in adopting RAG technology, including the need for stringent data management practices, transparency in AI decision-making, and the con tinuous monitoring of AI systems to mitigate risks such as bias, misinformation, and unintended consequences. From what we have learned, we provide a foundation for develo ping best practices that organizations can adopt to navigate the challenges of implementing RAG. These best practices address key areas such as data governance, system design, user engagement, and ongoing evaluation, all of whi ch are essential for maximizing the beneﬁts of RAG while minimizing potential pitfalls. By sharing these insights, we aim to contribute to the broader discourse on AI governance and offer actionable guidance for organizations looking to harness the power of RAG in a responsible and sustainable manner. 6.1 AI Governance In our ﬁeld study, we developed an AI governance model design ed to address the unique challenges posed by AI sys- tems, particularly those involving Retrieval-Augmented G eneration (RAG). Our proposed governance model includes both enhancements to existing governance frameworks and th e introduction of novel governance practices speciﬁ- cally tailored to AI technologies. The model is divided into four broad categories: Architectural and Technological Governance, Risk Governance, Humanitarian Governance, an d Production Governance. (Please refer Figure 5) Each category addresses critical aspects of AI system implement ation, ensuring that organizations can effectively manage the complexities of AI deployment while adhering to ethical , legal, and operational standards. 6.1.1 Architectural and T echnological Governance Given the distinct differences between AI systems and tradi tional information systems, signiﬁcant updates to archite c- tural and technological governance are required: Architectural Review Board For AI implementations, the Architectural Review Board mus t establish processes to differentiate between cloud-based and on-premise infrast ructure. It is essential to make informed decisions regardi ng the use of on-premise AI models versus API-based AI models, c onsidering factors such as data security, performance, and scalability. Standard and Procedural Governance This aspect covers the development and enforcement of standards, guidelines, and best practices for integrating AI capabilities within b oth existing and new applications. Prompt Engineering Governance The effectiveness of an AI system, particularly those lever aging large language models (LLMs), heavily depends on the clarity, structure, a nd ﬂexibility of the prompts used to guide the model’s responses. With LLMs, prompt engineering lies at the heart o f the system. Therefore, it is important to discuss this 22 Deploying Large Language Models with Retrieval Augmented G eneration new governance for prompt engineering in more detail to ensu re consistency, reusability, and maintainability of these prompts. Structuring the prompt templates through standardization is crucial for maintaining consistency across different ap - plications and teams. These templates should be designed to be adaptable, allowing for easy customization while maintaining a core structure that ensures reliability. In a ddition, using a modular design, such as breaking the prompt into sections for preﬁx, context, instructions, and output format, allows teams to reuse and repurpose different secti ons of the prompt across various scenarios, enhancing efﬁcienc y and reducing the need for constant re-engineering. To enable reusability and ﬂexibility, we propose establish ing a library of prompt templates, thus enablimg teams to access a repository of well-tested prompts that can be adapt ed for various use cases. This reduces development time and ensures that best practices are consistently applied ac ross the organization. To manage the evolution of prompt templates, version control should be implemented. This all ows teams to track changes, revert to previous versions if necessary, and ensure that updates are systematically ap plied across all relevant systems. V ersion control also facilitates collaboration, enabling multiple teams to con tribute to and reﬁne prompt templates over time. While stan- dardization is important, prompts must also be ﬂexible enou gh to adapt to speciﬁc contexts or scenarios. Governance should include guidelines on how to modify prompts for diffe rent applications while maintaining the core principles of clarity, accuracy, and efﬁciency. For maintenance and continuous improvement, prompt templa tes should undergo regular audits to ensure they remain effective and aligned with the latest developments in AI tec hnology and organizational needs. These audits should assess the performance of prompts, identifying areas for im provement and updating templates accordingly. Establish- ing feedback mechanisms is crucial for the continuous impro vement of prompt engineering. Teams should collect and analyze feedback from end-users and developers to reﬁne prompts and address any issues that arise during deployment. This iterative process helps to keep prompts relevant and ef fective in changing environments. In addition, comprehen- sive training and documentation are essential for ensuring that all team members understand how to create, use, and maintain prompt templates. Documentation should cover the principles of prompt engineering, provide examples of best practices, and offer guidance on troubleshooting comm on issues. Finally, effective prompt engineering governance require s input from various disciplines, including AI specialists , domain experts, and end-users. This cross-functional coll aboration approach ensures that prompts are not only tech- nically sound but also aligned with the speciﬁc needs of the a pplication and the users it serves. Prompt engineering governance should not be the sole responsibility of a single team. Instead, it should be a shared responsibility across the organization, with clear roles and responsibilities as signed to different teams. This approach ensures that promp t engineering remains a priority and that best practices are c onsistently applied across all projects. CI-CD (Continuous Improvement / Continuous Development) G overnance The rapidly changing landscape of AI technology necessitates a reevaluation of existing CI-CD p ractices. Organizations must adjust their frequency of dat a and code updates to stay aligned with the latest advancement s in AI vendor technologies. Additionally, changes in laws and regulations affecting AI implementations must be i ncorporated into these processes. 6.1.2 Risk Governance Risk governance provides oversight to assess and mitigate t he potential risks associated with AI implementations, ensuring that they do not adversely impact the organization ’s reputation or operational integrity: T echnology Risk Due to the emergent nature of AI, coupled with its rapid evolu tion, AI-based applications face heightened risks. These risks must be regularly evaluated a nd addressed through comprehensive risk management strategies, ensuring that the AI systems remain robust and r eliable over time. V endor ManagementWhile the fundamentals of vendor management remain largely unchanged, the evaluation and onboarding of AI vendors require careful consideration. Or ganizations must assess whether new AI models utilize data responsibly, comply with enterprise contracts, and me et security standards before integration. Evaluation and On-boarding The evaluation of newer models and whether those utilize the data sent for training has to be considered before on-boarding the new AI models and tec hnologies. Evaluating enterprise contracts and security compliance is also part of the evaluation and onboarding gov ernance. Legal and Compliance Keeping abreast of the dynamic regulatory environment surr ounding AI is crucial. Organiza- tions must establish governance frameworks that ensure com pliance with evolving legal requirements and guide the implementation of AI systems in line with these standards. 23 Deploying Large Language Models with Retrieval Augmented G eneration 6.1.3 Humanitarian Governance Humanitarian governance focuses on the socio-technologic al impacts of AI systems, particularly on human-AI inter- actions, ethics, and organizational culture: Ethical Governance AI implementations must adhere to established ethical norm s. This governance ensures that AI systems are developed and deployed in a manner that respects human rights, fairness, and transparency. Explainability Governance To foster trust and accountability, it is imperative that AI systems are explainable. This governance sets the required level of explainability for AI systems within the organization and mandates adherence to these standards across all AI-based solutions. Bias Governance Addressing biases in AI systems is critical. This governanc e requires the analysis of potential biases in AI implementations and the development of mitigat ion strategies to ensure fairness and equity in AI-driven decisions. Training and Education While training is an existing governance model, AI systems d emand specialized training for teams involved in their development and use. This includes e ducating allied teams and customers on how to effectively interact with AI systems. Organizational Communication and Change Management Existing governance models for communication and change management must be adapted to meet the speciﬁc requir ements of AI systems. Psycho-social Aspects of AI This includes addressing the psycho-social aspects of AI, s uch as managing staff and user anxieties through targeted training and support progr ams, thereby enhancing the adoption of AI technologies. 6.1.4 Production Governance Production governance ensures that AI systems are managed e ffectively once deployed, encompassing monitoring, disaster recovery, business continuity, incident management production support, etc. Data Governance Data governance for AI systems must be restructured to accou nt for the dynamic nature of data that can modify AI models and outputs. This involves implementin g stringent data quality controls and ensuring that the data feeding into AI models is reliable and accurate. Operational Governance While operational governance practices such as staff backu ps, rotation schedules, and in- cident management remain largely applicable to AI systems, they must be ﬁne-tuned to accommodate the speciﬁc operational needs of AI technologies. Production Support and Assurance AI-based systems require specialized production support. Organizations must equip their production support teams with the necessary tra ining and resources to handle the unique challenges posed by AI systems, ensuring that they can provide effective and t imely support. This AI governance model is based on our learnings from this ﬁ eld study and is designed to provide a comprehensive framework that organizations can adopt to manage the complexities of AI deployment. By integrating these governance practices, organizations can ensure that their AI systems a re not only effective but also ethical, compliant, and resil ient in the face of evolving technological and regulatory landsc apes. 6.2 Best Practices In our ﬁeld study, we experimented with various prompt patte rns [38] and explored different aspects of Retrieval- Augmented Generation (RAG) to develop best practices for bu ilding LLM-based systems. The following recommen- dations are grounded in lessons learned from our build-test cycles and are designed to address key areas such as data governance, system design, user engagement, and ongoing ev aluation. 6.2.1 Prompt Engineering Prompt Structure: RAG allows LLMs to stay focused and minimize hallucinationsby providing clear context through the prompt. We developed structured templates that enhance readability and ﬂexibility during development. These templates also segment instructions into distinct section s, which is crucial for maintaining clarity. Incorporating a preﬁx section in the prompt helps deﬁne the LLM’s persona and provides structural information, ensuring the model understands the organization of the prompt. Custom begin an d end tags (e.g., <|im-start|>, <|im-end|>) facilitate eas y parsing of the prompt’s sections. 24 Deploying Large Language Models with Retrieval Augmented G eneration Prompt Writing: Clarity and speciﬁcity in prompt instructions are vital for guiding the LLM effectively. For instance, instructions like "If there is not enough information, say ’ I don’t know’" or "Be brief in your answers" help the model deliver more accurate responses. Capitalizing key directi ves (e.g., "DO NOT," "ONLY") and numbering instructions can further emphasize critical points. When expecting outp ut in a speciﬁc format (e.g., JSON), providing an example is beneﬁcial. Additionally, using the function-calling fe ature of LLMs, such as in ChatGPT, can help ensure the output follows a particular pattern. 6.2.2 Data Governance V ector Databases: Experimenting with various vector databases and search mec hanisms is crucial for optimizing RAG systems. We tested databases like Chroma, Pinecone, and Elastic Relevance Search Engine and compared dif- ferent search strategies, including exact-match and seman tic search, or a combination of both. Proper data governance involves selecting and managing these databases to ensure d ata accuracy, consistency, and privacy throughout the re- trieval process. Establishing clear guidelines for data ha ndling, storage, and retrieval is essential for maintainin g the integrity of the information used by RAG systems. 6.2.3 System Design Modular Architecture: Given the rapidly evolving nature of AI technologies, desig ning the application in modular, decoupled components is advisable. This approach allows fo r easier updates and integration of new features without disrupting the entire system. Thorough planning of the arch itecture before building the prototype is essential. Ex- tensive logging should be implemented to track which docume nts were retrieved for speciﬁc user queries and what responses were generated. This not only aids in debugging bu t also supports ongoing evaluation and improvement of the system. LLM Considerations: Selecting the right AI provider is critical. Enterprise acc ounts offer more reliable API access and support, which is particularly important for large-sca le applications. It’s beneﬁcial to compare the capabilitie s of cloud-based LLMs (e.g., those offered by Google V ertex AI or Microsoft Azure OpenAI) with locally hosted, ﬁne- tuned models on proprietary data. Each option has distinct a dvantages, depending on the business case, and should be evaluated in terms of scalability, cost, and data security. 6.2.4 User Engagement Human Factors: Engaging end users early in the development process is key to creating a system that meets their needs. Request users to provide a set of frequently used ques tions for quality testing and unit testing. This approach helps mitigate developers’ bias and avoids tunnel vision, e nsuring the system is tested against real-world scenarios that users will encounter. Continuous feedback from users s hould be integrated into the development cycle to reﬁne the system’s performance. 6.2.5 Ongoing Evaluation Continuous Monitoring and Feedback: Implementing a robust system for ongoing evaluation is crucial for maintain- ing the effectiveness and relevance of the RAG system. This i ncludes regular monitoring of the system’s performance, user satisfaction, and compliance with data governance pol icies. Feedback loops should be established to capture insights from users and operational data, allowing for iter ative improvements. Regular updates to the system based on these evaluations will help ensure that it continues to meet evolving user needs and technological advancements. By following these best practices that we contribute based o n the insights from our ﬁeld study, organizations can successfully implement RAG-based solutions using LLMs, en suring that the technology is applied in a way that is effective, responsible, and aligned with industry standar ds and user expectations. 7 Discussion and Implications This research and ﬁeld study uncovered several technologic al, enterprise, and human factors that must be addressed as Retrieval-Augmented Generation (RAG) systems continue to evolve. From a technological perspective, one major challenge encountered was the limitations associated with using external AI models like C HATGPT via APIs, such as rate limits, delayed responses, and token constraints. T hese limitations directly impact the performance and scal- ability of RAG applications, requiring careful considerat ion of model selection, token management, and response optimization. 25 Deploying Large Language Models with Retrieval Augmented G eneration On the enterprise front, the use of external APIs for large la nguage models (LLMs) necessitates a rigorous assessment of the proprietary information being exposed to these model s, particularly in sensitive contexts where data security i s paramount. Gaining stakeholder buy-in and securing budget allocations are critical steps in developing systems based on emerging technologies. This highlights the importance o f robust AI governance frameworks, which we addressed in our study by proposing governance models that account for th e unique requirements of AI systems, including prompt engineering, risk management, and legal compliance. Our pilot phase underscored the importance of managing user expectations for the successful deployment of AI-based applications. We found that user training is critical, part icularly in question-and-answer interactions, where poor ly framed questions can result in inaccurate or misleading res ponses. By educating users on how to interact effectively with AI systems, many of these issues can be mitigated, leadi ng to improved overall system performance and user satisfaction. In addressing the challenges we faced, we encountered issues similar to those documented by Hevner et al. [14] in their research on the design of human-artiﬁcial intelligence sys tems (HAIS). These challenges included selecting appropri - ate technologies amid legal and compliance constraints, ma naging the complexity of integrating diverse data sources, and evaluating the effectiveness of LLM-based application s. Each of these issues required careful consideration and iterative problem-solving, such as modular design, user fe edback through pre-surveys, and the strategic use of tools like T RULENS [42], D EEP EVAL [43], L ANG SMITH by L ANG CHAIN [10], etc., for evaluation. In addition, the implementation of Retrieval-Augmented Ge neration (RAG) systems in real-world scenarios necessi- tates a robust AI governance framework to ensure ethical, le gal, and operational integrity. As AI technologies evolve, especially with advancements like RAG, governance models must adapt to address new challenges, including architec- tural design, risk management, and the socio-technical imp acts on human-AI interactions. Our ﬁeld study insights laid the groundwork for developing t he proposed AI governance model tailored speciﬁcally to RAG systems, with a focus on key areas such as prompt engine ering, data governance, and continuous evaluation. This governance model emphasizes the importance of maintai ning transparency, ensuring ethical AI deployment, and managing risks associated with the rapid evolution of AI tec hnologies. By adopting these governance practices, orga- nizations can deploy RAG systems effectively while adherin g to compliance standards and mitigating potential risks such as bias or misinformation. Integrating these insights into AI governance contributes to the broader discourse on responsible AI deployment and offers a practical framework for organizations aiming to harness the potential of RAG systems. Given the complex challenges revealed by our ﬁeld study, it i s essential for industry and academia to collaborate more closely to advance AI technologies in a manner that is both in novative and responsible. By sharing the real-world difﬁculties encountered during implementations, researc h in the ﬁeld of LLM and RAG systems can be signiﬁcantly accelerated. This collaboration will not only reﬁne best pr actices and governance models but also foster the develop- ment of robust, reliable, and compliant AI solutions that ca n be widely adopted across various industries. This study enhances the growing body of knowledge on RAG syst ems by providing a comprehensive analysis of the technical, organizational, and human challenges invol ved in their implementation. Our ﬁndings highlight the importance of a holistic approach that integrates AI govern ance, user engagement, and continuous evaluation, offering a roadmap for organizations looking to adopt RAG-based solu tions in a dynamic technological landscape. 8 Conclusion This study has highlighted the key challenges and opportuni ties associated with implementing Retrieval-Augmented Generation (RAG) systems using large language models (LLMs ). While RAG is emerging as a signiﬁcant application of LLMs, our ﬁndings indicate that it remains in its early sta ges, with many organizations still navigating the complex- ities of its deployment. The rapid evolution of underlying t echnologies adds to the challenge, making it a continuously moving target. In addition to addressing technical challenges, our resear ch has also contributed to AI governance frameworks and best practices essential for successfully integrating RAG systems. We proposed governance models that account for the unique demands of AI systems, emphasizing the need for st ructured prompt engineering, comprehensive risk management, ethical considerations, and ongoing system ev aluation. These governance recommendations provide organizations with a robust foundation for ensuring that th eir AI implementations are effective and compliant with emerging standards and regulations. By sharing the lessons learned and best practices from our ﬁe ld study, we aim to offer a practical framework that organizations can use to overcome these obstacles. The insi ghts gained from our research offer valuable guidance for 26 Deploying Large Language Models with Retrieval Augmented G eneration both technical and governance aspects for those embarking o n similar ventures, helping to ensure that the challenges of integrating RAG into production environments can be more effectively addressed. We hope that these contributions will support the broader adoption and reﬁnement of RAG syste ms, advancing both industry practice and academic research in this area. References [1] Oktavia Catsaros. Bloomberg. https://www.bloomberg.com/company/press/generative- ai-to-become-a-1-3-trilli Accessed: 2023-12-16. [2] Edward J Hu, Y elong Shen, Phillip Wallis, Zeyuan Allen-Z hu, Y uanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021. [3] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Y ang, and Jie Tang. P-tuning v2: Prompt tuning can be comparable to ﬁne-tuning universal ly across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. [4] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. [5] Xiao Liu, Y anan Zheng, Zhengxiao Du, Ming Ding, Y ujie Qia n, Zhilin Y ang, and Jie Tang. GPT understands, too. AI Open, 2023. [6] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Y utaka Matsuo, and Y usuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems , 35:22199–22213, 2022. [7] Archit Parnami and Minwoo Lee. Learning from few example s: A summary of approaches to few-shot learning. arXiv preprint arXiv:2203.04291 , 2022. [8] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large lan guage models. Advances in Neural Information Pro- cessing Systems, 35:24824–24837, 2022. [9] Shunyu Y ao, Jeffrey Zhao, Dian Y u, Nan Du, Izhak Shafran, Karthik Narasimhan, and Y uan Cao. React: Syner- gizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 , 2022. [10] Harrison Chase. LangChain, October 2022. [11] Jerry Liu. LlamaIndex, November 2022. [12] Microsoft. semantic-kernel, 2023. [13] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaoku n Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applica tions via multi-agent conversation framework. arXiv preprint arXiv:2308.08155 , 2023. [14] Alan Hevner and V eda Storey. Research challenges for the design of human-artiﬁcial intelligence systems (HAIS). ACM Transactions on Management Information Systems , 14(1):1–18, 2023. [15] Juan Ramos et al. Using TF-IDF to determine word relevan ce in document queries. In Proceedings of the ﬁrst instructional conference on machine learning , volume 242, pages 29–48. Citeseer, 2003. [16] Donald J Berndt, James A McCart, and Stephen L Luther. Us ing ontology network structure in text mining. In AMIA Annual Symposium Proceedings , volume 2010, page 41. American Medical Informatics Associ ation, 2010. [17] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean . Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 , 2013. [18] Stephen Wolfram. What Is ChatGPT Doing... and Why Does It W ork? Stephen Wolfram, 2023. [19] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio P etroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Re trieval-augmented generation for knowledge- intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474, 2020. [20] Y ejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang D ai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Y u, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023 , 2023. [21] Huayang Li, Yixuan Su, Deng Cai, Y an Wang, and Lemao Liu. A survey on retrieval-augmented text generation, 2022. 27 Deploying Large Language Models with Retrieval Augmented G eneration [22] Penghao Zhao, Hailin Zhang, Qinhan Y u, Zhengren Wang, Y unteng Geng, Fangcheng Fu, Ling Y ang, Wen- tao Zhang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey. arXiv preprint arXiv:2402.19473, 2024. [23] Hao Y u, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zha ofeng Liu. Evaluation of retrieval-augmented generation: A survey. arXiv preprint arXiv:2405.07437 , 2024. [24] Y unfan Gao, Y un Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Y uxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models : A survey. arXiv preprint arXiv:2312.10997 , 2023. [25] Shangyu Wu, Ying Xiong, Y ufei Cui, Haolun Wu, Can Chen, Y e Y uan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, et al. Retrieval-augmented generation for natura l language processing: A survey. arXiv preprint arXiv:2407.13193, 2024. [26] Shamane Siriwardhana, Rivindu Weerasekera, Elliott W en, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguis tics, 11:1–17, 2023. [27] Alireza Salemi and Hamed Zamani. Evaluating retrieval quality in retrieval-augmented generation. arXiv preprint arXiv:2404.13781, 2024. [28] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchm arking large language models in retrieval- augmented generation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligen ce, volume 38, pages 17754–17762, 2024. [29] Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yi xin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, et al. Searching for best practic es in retrieval-augmented generation. arXiv preprint arXiv:2407.01219, 2024. [30] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian L iu, Jane Dwivedi-Y u, Yiming Y ang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983 , 2023. [31] Wenhao Y u. Retrieval-augmented generation across het erogeneous knowledge. In Proceedings of the 2022 conference of the North American chapter of the association for computational linguistics: human language technologies: student research workshop , pages 52–58, 2022. [32] Shi-Qi Y an, Jia-Chen Gu, Y un Zhu, and Zhen-Hua Ling. Cor rective retrieval augmented generation. arXiv preprint arXiv:2401.15884, 2024. [33] Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. Open llm leaderboard v2. https://huggingface.co/spaces/open-llm-leaderboard/ open_llm_leaderboard, 2024. [34] Y upeng Chang, Xu Wang, Jindong Wang, Y uan Wu, Linyi Y ang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large lang uage models. ACM Transactions on Intelligent Systems and T echnology, 15(3):1–45, 2024. [35] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, E Buchatskaya, T Cai, E Rutherford, DdL Casas, LA Hendricks, J Welbl, A Clark, et al. Training compute-opti mal large language models. arxiv 2022. arXiv preprint arXiv:2203.15556, 10, 2022. [36] Fernando van der Vlist, Anne Helmond, and Fabian Ferrar i. Big ai: Cloud infrastructure dependence and the industrialisation of artiﬁcial intelligence. Big Data & Society , 11(1):20539517241232630, 2024. [37] OpenAI. Introducing apis for gpt-3.5 turbo and whisper , April 9, 2024. [Accessed 06-08-2024]. [38] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Car los Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern cata log to enhance prompt engineering with ChatGPT. arXiv preprint arXiv:2302.11382 , 2023. [39] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawt e, Aman Chadha, and Amitava Das. A comprehen- sive survey of hallucination mitigation techniques in larg e language models. arXiv preprint arXiv:2401.01313 , 2024. [40] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Ten ghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efﬁcient ﬁne-tuning is better and chea per than in-context learning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022. [41] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhua ng, Zhanghao Wu, Y onghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge wit h mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. 28 Deploying Large Language Models with Retrieval Augmented G eneration [42] Anupam Datta, Matt Fredrikson, Klas Leino, Kaiji Lu, Sh ayak Sen, Ricardo Shih, and Zifan Wang. Exploring conceptual soundness with TruLens. In Douwe Kiela, Marco Ci ccone, and Barbara Caputo, editors, Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track , volume 176 of Proceedings of Machine Learning Research, pages 302–307. PMLR, 06–14 Dec 2022. [43] Jeffrey Ip, ColabDog, Kritin V ongthongsri, Anindyade ep, V asilije, Pratyush K. Patnaik, agokrani, lplcor, fschuh, Jonathan Bennion, Andrea Romano, Simon Podhajsky, Andrés, Philip Nuzhnyi, Jonas, Fabian Greavu, Ananya Raval, João Felipe Pizzolotto Bini, pedroallenrevez, ofte nfrequent, Y udhiesh Ravindranath, Rohinish, Michael Leung, Jeroen Overschie, Donald Wasserman, Brian DeRenzi, V aibhav Kubre, Vinicius Mesel, Wenjie Fu, and nictuku. conﬁdent-ai/deepeval, 8 2024. [44] Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. Seven fail- ure points when engineering a retrieval augmented generati on system. In Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engi neering for AI, pages 194–199, 2024. [45] Araz Taeihagh. Governance of artiﬁcial intelligence. Policy and society, 40(2):137–157, 2021. [46] James Butcher and Irakli Beridze. What is the state of ar tiﬁcial intelligence governance globally? The RUSI Journal, 164(5-6):88–96, 2019. [47] Allan Dafoe. Ai governance: a research agenda. Governance of AI Program, Future of Humanity Institute, University of Oxford: Oxford, UK , 1442:1443, 2018. [48] Matti Mäntymäki, Matti Minkkinen, Teemu Birkstedt, an d Mika Viljanen. Deﬁning organizational ai governance. AI and Ethics , 2(4):603–609, 2022. A Appendix A.1 Functional Requirements 1. Natural Language Understanding (NLU): The application s hould be able to understand and interpret user inputs accurately, even when the language used is informal o r contains typos. 2. Contextual Understanding: The application should maint ain the context of a conversation, allowing it to provide relevant responses and follow up on previous intera ctions. 3. Multi-Platform Compatibility: The application archite cture should be compatible with various platforms, including websites, mobile apps, and social media platform s. 4. Integration with Existing Systems: The application shou ld be able to integrate with existing enterprise sys- tems to access and provide relevant information. 5. It should support various interfaces to various document formats, such as (a) Written documents (PDFs, Word, Excel, etc.) (b) Organization’s internal communicator data. (c) Integrate various existing databases. 6. Self-Learning: The application should consume individu al documents or documents placed in a speciﬁc directory. 7. When new documents are placed into a speciﬁed directory, t he application shall check on an interval time for new documents to ingest and add to the indexes. 8. User Authentication: The application should be able to au thenticate users to provide personalized services and handle sensitive information securely. 9. User sessions should be separated so that one user’s quest ions and answers are not mixed between conversa- tions. 10. Authentication should be done as a Single Sign On (SSO) wi th the organization’s Active Directory 11. The application should support various authorization l evels at individual and group levels. 12. Response Time: The application should respond to user in puts within a reasonable time frame, ensuring a smooth and efﬁcient user experience. 29 Deploying Large Language Models with Retrieval Augmented G eneration A.2 Non-Functional Requirements 1. Scalability: The application should be able to handle man y simultaneous conversations without any signiﬁ- cant degradation in performance. 2. Security: The application should comply with all relevan t data protection and privacy regulations. User data should be encrypted and stored securely. 3. Reliability: The application should be reliable, with a h igh uptime and minimal errors or failures. 4. Usability: The application should be easy to use, with a us er-friendly interface and clear, understandable responses. 5. Maintainability: The application should be easy to updat e and maintain, allowing for continuous improvement and the addition of new features. A.3 T echnical Requirements 1. V ector Database – Chroma DB 2. UI - Self-hosted React.js Frontend 3. API – Flask API 4. Software language and toolkits – Python, Langchain A.4 Stakeholder Requirements 1. As a user, I want the application to understand my queries, so that I can get accurate and relevant responses. 2. As a user, I want the application to remember previous inte ractions so I don’t have to repeat information. 3. As a user, I want the application to provide citations to do cument sources so I can conﬁrm and expand upon the generated information. 4. As a business owner, I want the application to integrate wi th my existing systems to access and provide up-to-date information. A.5 Acceptance Criteria The AI application will be considered complete when it meets all the functional and non-functional requirements outlined and successfully fulﬁlls the stakeholder require ments. A.6 Stakeholders 1. Solution Architect team 2. Development team 3. Quality Assurance team 4. Performance testing team 5. Technology risk management team 6. Enterprise production support teams 7. Network and infrastructure teams 8. Shared services teams 9. Customer support end users 10. Legal, compliance, and regulatory teams 11. Executive leadership team, including heads of engineer ing, strategy, marketing, training, and documentation organizations within the company. 30 Deploying Large Language Models with Retrieval Augmented G eneration A.7 Measures of effectiveness A.7.1 T echnical • Response should be relevant to the question asked. • Response should be accurate, factual, and generated based on the existing documentation. • Each response should provide citation links to the documen tation from which the response was generated. • Responses should be consistent every time the same questio n is asked. The response can be aesthetically different (worded differently), but the content should be t he same. A.7.2 Human • The system should be easy to use, user-friendly, and intuit ive. • The system should be reliable and robust so users can use it w ithout double-checking the cited documents. • The users should be provided with a consistent interface su ch that any changes to the back-end technology should be agnostic to the user. B Pre-survey Questionnaire 1. The ﬁrst few questions are related to personal informatio n such as name, team, age, department, years of service in the organization, designation, etc. 2. Have you used a generative AI application? 3. How do you think using generative AI would help you and your team to be more efﬁcient? 4. How do you think generative AI can help with cross-functio nal teams? 5. What are your expectations from the Information Retrieva l System using Generative AI? 6. Do you have any concerns with using generative AI applicat ions? 7. What data or information do you expect the generative AI ap plication to give? 8. Any additional inputs, comments, or notes. B.1 Analysis from the pre-survey 1. Question 1: This data is redacted for privacy reasons. The team of pilot users was across various departments in the organization. 2. Question 2: 54% of users have some prior experience using g enerative AI at various levels of expertise. 3. Question 3: Users are looking for overall accuracy and rel evance of retrieved information, help with trou- bleshooting, fast access to information, speciﬁcs about pr oducts and releases, etc. About 8% of the users were not sure if AI can make them or their teams more efﬁcient. 4. Question 4: Users are looking to reduce cross-functional team communications and meetings, as with the AI application, everyone would have quick access to everythin g. The users are expecting a signiﬁcant increase in efﬁciency. About 8% of users are “unsure but excited to ﬁnd out.” 5. Question 5: Most users were excited about the new applicat ion. Some even believed in endless possibilities, and some were excited about how it would bring the entire orga nization together and “all-inclusive” and increase transparency. About 20% of the users did not know wh at to expect from such an application. 6. Question 6: Concerns were about phrasing questions to the AI and some known issues with the current AI models available in the market, such as taking questions out of context, accuracy, and reliability of responses, data privacy and security, too much dependency on the AI appl ication, and even such a tool replacing human jobs. About 41% of the users did not have any concerns. 7. Question 7: Most users wanted data to be retrieved across v arious enterprise systems in addition to documents in different formats. About 8% of users were not sure. 8. Question 8: Few comments in notes. About 25% of users were e xcited and grateful to be included in the pilot-run program. 31 Deploying Large Language Models with Retrieval Augmented G eneration C Post Survey Questionnaire 1. What is your ﬁrst impression of the RAG application in use? (a) Bad (b) Ok (c) Good 2. How would you rate the RAG application? (a) Bad – Discontinue further development efforts (b) Not Bad – Needs a lot of improvements (c) OK – Needs some improvements (d) Good – Needs some tweaks, but overall good (e) Excellent – It is remarkable, can use it right now 3. How often did the RAG application provide the answers you n eeded when the context was established? (a) Never (b) Rarely (c) Sometimes (d) Often (e) Every time 4. How often did the RAG application provide the answers you n eeded when the context was NOT established? (a) Never (b) Rarely (c) Sometimes (d) Often (e) Every time 5. Are the citation links helpful? Y es/No/Maybe 6. Would you recommend the RAG application to your team? Y es/ No/Maybe 7. Do you think the RAG application would improve your produc tivity? (a) Not at all (b) Less than 30% - Slightly (c) 30 - 60% – Moderately (d) 60 - 100% – Signiﬁcantly 8. What are the top 3 items you want improved? 9. What feature/s would you like to see added to generative AI application 2.0? 10. Do you have any concerns about using the RAG application? If so, what? C.1 Analysis from the post-survey 1. Question 1: Most users chose the option B for ok. 2. Question 2: 52% of users thought the application needed so me improvements. 28% of users selected the option that the application needed a lot of improvements. 13 % of users selected the option that the application needed some minor tweaks. 7% of users selected the option tha t the application was ready for use. 3. Question 3: Most users selected option C or D, which meant t hat when the context was provided, the applica- tion gave results in some or most cases. 4. Question 4: Most users selected option B, which meant that with less or no context provided, the application rarely gave the answers correctly. 5. Question 5: All users thought the citations were helpful. We also received some good feedback on improving the citations by highlighting the text or showing the user th e exact page or paragraph from the document. 6. Question 6: Most users selected Y es, and some selected May be for recommending the application to their team, but a few did select No for this answer. The reason for selecting No was that they thought the application needed a lot of modiﬁcations before it was ready for use. 32 Deploying Large Language Models with Retrieval Augmented G eneration 7. Question 7: Most users selected options C and D, and some se lected B for recommending the application to their team, but a few did select No for this answer. The reason for this, in most users’ opinion, was that the application needed a lot of modiﬁcations before it was ready for use. 8. Question 8, 9, 10: The answers for these three questions ar e associated closely with the proprietary applica- tion, its functioning, and the context of this particular ﬁe ld study, and hence, this data has been redacted from the survey analysis for privacy reasons. 33