HypRAG: Hyperbolic Dense Retrieval for Retrieval Augmented Generation Hiren Madhu 1 Ngoc Bui 1 Ali Maatouk 1 Leandros Tassiulas1 Smita Krishnaswamy 1 Menglin Yang2 Sukanta Ganguly 3 Kiran Srinivasan 3 Rex Ying 1 Abstract Embedding geometry plays a fundamental role in retrieval quality, yet dense retrievers for retrieval- augmented generation (RAG) remain largely con- fined to Euclidean space. However, natural lan- guage exhibits hierarchical structure from broad topics to specific entities that Euclidean embed- dings fail to preserve, causing semantically distant documents to appear spuriously similar and in- creasing hallucination risk. To address these limi- tations, we introduce hyperbolic dense retrieval, developing two model variants in the Lorentz model of hyperbolic space: HyTE-FH, a fully hyperbolic transformer, and HyTE-H, a hybrid architecture projecting pre-trained Euclidean em- beddings into hyperbolic space. To prevent rep- resentational collapse during sequence aggrega- tion, we introduce the Outward Einstein Mid- point, a geometry-aware pooling operator that provably preserves hierarchical structure. On MTEB, HyTE-FH outperforms equivalent Eu- clidean baselines, while on RAGBench, HyTE-H achieves up to 29% gains over Euclidean base- lines in context relevance and answer relevance us- ing substantially smaller models than current state- of-the-art retrievers. Our analysis also reveals that hyperbolic representations encode document specificity through norm-based separationâ€”with over 20% radial increase from general to specific conceptsâ€”a property absent in Euclidean embed- dings, underscoring the critical role of geometric inductive bias in faithful RAG systems1. 1. Introduction Dense retrieval forms the backbone of retrieval-augmented generation (RAG) systems (Lewis et al., 2020; Fan 1Yale University, USA2Hong Kong University of Science and Technology (Guangzhou), China 3NetApp, USA. Correspondence to: Rex Ying <rex.ying@yale.edu>. Preprint. February 10, 2026. 1The code is available at: https://anonymous.4open. science/r/HypRAG-30C6 Figure 1.Hierarchies in Text.(A) Documents naturally organize into branching hierarchies where general topics spawn increasingly specific subtopics. Euclidean spaces distort such hierarchies due to crowding effects, while hyperbolic geometry preserves hierar- chical relationships through exponential volume growth. (B) Ricci curvature analysis of document embeddings from strong baselines reveals predominantly negative curvature, indicating tree-like se- mantic structure. et al., 2024), where embedding quality directly determines whether generated responses are grounded in evidence or hallucinated. By retrieving relevant documents and condi- tioning generation on this context, RAG systems produce responses that are more attributable and aligned with ver- ifiable sources (Ni et al., 2025). Yet, despite advances in retrieval architectures, current systems continue to rely on Euclidean embeddings, a choice inherited from standard neural networks rather than from language structure itself. Natural language inherently exhibits strong hierarchical or- ganization (He et al., 2025b; Robinson et al., 2024), with semantic structure giving rise to locally tree-like neigh- borhoods. Euclidean spaces struggle to represent such branching hierarchies due to polynomial volume growth (He et al., 2025b), introducing shortcuts between hierarchically distinct regions that distort semantic relationships. In re- trieval settings, these distortions can cause semantically dis- tant documents to appear spuriously similar (Radovanovic et al., 2010; Bogolin et al., 2022), degrading retrieval preci- sion (Reimers & Gurevych, 2021): a query about a specific subtopic may retrieve documents from sibling or parent cat- egories that share similarity but lack the required specificity. To further see why geometry matters for retrieval, consider a query about transformer attention mechanisms (Figure 1A). Relevant documents form a natural hierarchyâ€”from general concepts like NLP, to transformers, to specific components like multi-head attentionâ€”inducing tree-like semantic struc- 1 arXiv:2602.07739v1 [cs.IR] 8 Feb 2026 HypRAG ture. Euclidean embeddings struggle to preserve this organi- zation: representing both broad topics and specialized de- scendants forces a trade-off between semantic proximity and fine-grained separation, causing neighborhood crowding and distortion. Hyperbolic geometry resolves this tension through exponential volume growth, allowing general con- cepts to remain compact while specific documents spread outward. To test whether such structure appears empirically, we analyze Ollivierâ€“Ricci curvature (Ni et al., 2019)â€”a measure of local geometry where negative values indicate tree-like branchingâ€”on graphs built from MS MARCO document embeddings (Bajaj et al., 2016). Across several strong models (Linq Embed Mistral, LLaMA Nemotron 8B, Qwen3 Embedding 4B), curvature distributions are predom- inantly negative (Figure 1B), providing empirical evidence that retrieval-relevant embeddings exhibit intrinsic hyper- bolic structure and motivating hyperbolic geometry as a natural inductive bias for dense retrieval. Recent work has begun exploring hyperbolic geometry for language modeling and RAG systems, though with different focus areas. HELM (He et al., 2025a) introduces a family of hyperbolic language models that operate entirely in hyper- bolic space, but these models target text generation rather than retrieval. In the RAG setting, HyperbolicRAG (Cao et al., 2025) projects embeddings into the PoincarÃ© ball to encode hierarchical depth within a static, pre-built knowl- edge graph, using dual-space retrieval that fuses Euclidean and hyperbolic rankings. However, HyperbolicRAG relies on Euclidean encoders to produce the initial embeddings, leaving the fundamental geometric mismatch. Moreover, ag- gregating token embeddings into document representations poses a challenge that existing work in hyperbolic learning does not address (Yang et al., 2024). As we show in Proposi- tion 4.3, naively averaging tokens in Euclidean space before projecting to hyperbolic space causes representations to col- lapse toward the origin, destroying the hierarchical structure that is meant be to preserved. To this end, we introduce hyperbolic dense retrieval for RAG, framing embedding geometry as a design choice for improving evidence selection and grounding at the repre- sentation level. We study this through two complementary instantiations. First, HyTE-FH (Hyerbolic Text Encoder, Fully Hyperbolic) operates entirely in the Lorentz model of hyperbolic space, enabling end-to-end representation learn- ing. Second, HyTE-H (Hybrid) maps embeddings from off-the-shelf Euclidean encoders into hyperbolic space, al- lowing us to build on existing pre-trained Euclidean models. The Lorentz modelâ€™s intrinsic geometry enables parameter- efficient scaling: HyTE-H outperforms Euclidean baselines several times (2-3x) its size, reducing memory footprint in resource-constrained settings. To address the aggregation challenge in both instantiations, we introduce the Outward Einstein Midpoint, a geometry-aware pooling operator that amplifies tokens farther from the origin, provably preserving hierarchical structure during pooling. Through extensive evaluation on RAGBench, we demon- strate that both hyperbolic variants consistently outper- form Euclidean baselines in answer relevancy across multi- ple datasets, while achieving competitive performance on MTEB. Our experiments validate three key findings: (1) hy- perbolic retrieval substantially improves RAG performance, with up to 29% gains over Euclidean baselines in context relevance and answer relevance; (2) hyperbolic models nat- urally encode concept-level hierarchies in their radial struc- ture, with the fully hyperbolic model achieving a 20.2% radius increase from general to specific concepts, while Eu- clidean models fail to capture this organization; and (3) our theoretically grounded Outward Einstein Midpoint pooling preserves this hierarchical structure during aggregation. 2. Related Works Text Embeddings and Dense Retrieval.Dense retrieval embeds queries and documents into a shared vector space and ranks candidates by similarity (e.g., dot product or co- sine). Transformer bi-encoders (e.g., BERT (Devlin et al., 2019)) are widely used in this context due to their scalabil- ity with maximum inner product search (Karpukhin et al., 2020; Reimers & Gurevych, 2019). Most methods train with contrastive objectives using in-batch and hard negatives (Gao et al., 2021; Izacard et al., 2021; Xiong et al., 2021), often following large-scale pretraining plus task-specific fine-tuning (Wang et al., 2022; Li et al., 2023; Nussbaum et al., 2025). More recently, decoder-only embedding mod- els initialize from LLMs to exploit their pretrained linguis- tic knowledge (Muennighoff et al., 2024; Lee et al., 2024; Zhang et al., 2025). However, most retrievers remain reliant on inner products or distances in Euclidean geometry-an in- ductive bias often misaligned with the hierarchical structure of language and document collections. We address this gap by introducing hyperbolic geometry for text embeddings to better capture such a hierarchy. Retrieval Augmented Generation.RAG grounds LLMs in retrieved evidence to improve factuality and access external knowledge (Oche et al., 2025). It typically retrieves top-k contexts (often via dense retrieval) and conditions genera- tion on them (Lewis et al., 2020). Since the context window is limited, retrieval quality is a key bottleneck for relevance and faithfulness (Friel et al., 2024a). Several methods im- prove reliabilityafterretrieval: Self-RAG (Asai et al., 2024) and CRAG (Yan et al., 2024) use learned critics to filter or re-rank evidence, while GraphRAG (Han et al., 2024) lever- ages knowledge graphs for structured subgraph retrieval. These approaches operate downstream of the embedding space and are complementary to ours geometric approach. Our goal is to improve RAG upstream by enhancing the 2 HypRAG retriever representations so that the initial top-k evidence is more reliable under realistic efficiency constraints. Hyperbolic Representation Learning.Hyperbolic ge- ometry is primarily known for its ability to better cap- ture hierarchical, tree-like structures (Yang et al., 2023; Peng et al., 2021), which enhances performance in vari- ous tasks, including molecular generation (Liu et al., 2019), recommendation (Yang et al., 2021; Li et al., 2021), im- age retrieval (Khrulkov et al., 2020; Wei et al., 2024; Bui et al., 2025), and knowledge graph embedding (Ganea et al., 2018a; Dhingra et al., 2018). More recently, hyperbolic ge- ometry has also shown promise for multi-modal embedding models (Desai et al., 2023; Ibrahimi et al., 2024; Pal et al., 2024) and foundation models (Yang et al., 2025; He et al., 2025a). In contrast to these works, we study how hyper- bolic representations can improve retrieval in RAG systems. Concurrently, Cao et al. (2025) use hyperbolic geometry to improve RAG rankings, but obtain hyperbolic embed- dings via a simple projection from Euclidean encoders; by contrast, we build on fully hyperbolic encoders trained end- to-end and address key challenges in this setting, including providing the theoretically grounded geometry-aware pool- ing for document-level representations. 3. Hyperbolic Space Preliminaries In this section, we go over all the preliminaries of Lorentz model of hyperbolic space and introduce the basic building blocks that create HyTE-FH. 3.1. Lorentz Model of Hyperbolic Space We represent all embeddings in d-dimensional hyperbolic space Hd K with constant negative curvature K <0 using the Lorentz (hyperboloid) model. In the Lorentz model, hyperbolic space is realized as the upper sheet of a two- sheeted hyperboloid embedded inR d+1, Hd K =  xâˆˆR d+1 âŸ¨x,xâŸ© L = 1 K , x 0 >0  , where the Lorentzian inner product is defined as âŸ¨x,yâŸ© L = âˆ’x0y0 +Pd i=1 xiyi. This formulation admits closed-form expressions for geodesic distances, barycentric operations, and parallel transport, and expresses similarity directly through Lorentzian inner products. The geodesic distance between two points x,yâˆˆH d K is given by dK(x,y) = 1âˆšâˆ’K coshâˆ’1(KâŸ¨x,yâŸ© L), which is a monotone function of the Lorentzian inner product. To support optimization, we make use of exponential and logarithmic maps between the manifold and its tangent spaces. For a point xâˆˆH d K, the logarithmic map logx(Â·) maps nearby points to the tangent space TxHd K, while the exponential map expx(Â·) maps tangent vectors back to the manifold. These operators are used only where necessary for gradient-based updates, ensuring that all representations remain on Hd K and preserving the hierarchical structure induced by negative curvature. 3.2. Hyperbolic Transformer Components Standard operations cannot be applied directly in hyperbolic space, as they may violate the manifold constraint (Yang et al., 2024). To address this, we introduce hyperbolic com- ponents that serve as the building blocks for our embedding model. These operations are performed via a re-centering procedure that applies Euclidean operations in a latent space and maps the result back to the Lorentz model. By doing so, the resulting vector is constructed to satisfy the Lorentz constraint, thereby preserving the hyperbolic structure of representations. We present these operations as follows. Lorentz Linear Layer.Given curvatures K1, K2, and parameters WâˆˆR (n+1)Ã—m and bâˆˆR m with z= |WâŠ¤x+b| , the Lorentzian linear transformation (Yang et al., 2024) is the map HLT:L K1,n â†’L K2,m given by, HLT(x;W,b) = r K2 K1 Â· " p âˆ¥zâˆ¥2 âˆ’1/K 2,z # Hyperbolic Layer Normalization.Given token embed- dings X={x i}n i=1 âŠ‚H d K, hyperbolic layer normalization is defined as HypLayerNorm(X) = r K1 K2 âˆ¥zâˆ¥2 2 âˆ’ 1 K2 , r K1 K2 z ! where z=f LN xi,[1:d]  , fLN(Â·) denotes standard Eu- clidean LayerNorm applied to the spatial components of the embedding, and K1, K2 >0 are input and output curva- ture respectively. Lorentz Residual Connection.Let x, f(x)âˆˆL K,n where x is an input vector and f(x) is the output of a neural network f. Then, the Lorentzian residual connection (He et al., 2025d) is given byxâŠ• L f(x) =Î± 1x+Î± 2y, where Î±i =w i/ âˆš âˆ’Kâˆ¥w 1x+w 2f(x)âˆ¥ L  ,foriâˆˆ {0,1}, where Î±1, Î±2 are weights parametrized by constants (w1, w2)âˆˆR 2 \ {(0,0)}. Hyperbolic Self-Attention.In hyperbolic attention, simi- larity is governed by hyperbolic geodesic distance (Ganea et al., 2018b). Given token embeddings X={x i}n i=1 âŠ‚ Hd K, queries, keys, and values are computed via Lorentz- linear transformations as Q= HLT(X;W Q,b Q), K= HLT(X;W K,b K), and V= HLT(X;W V ,b V ), where HLT(Â·) denotes a linear map in Lorentz space. Attention 3 HypRAG Figure 2.HyTE Architecture. A) HyTE-FH Encoder Block, B) HyTE-FH architecture, C) HyTE-H Architecture. weights are computed using squared hyperbolic geodesic distances (He et al., 2025c; Chen et al., 2022) as Î½i,j = exp âˆ’d2 K(qi,k j)/âˆšm  Pn l=1 exp(âˆ’d2 K(qi,k l)/âˆšm) , with head dimension m. This prioritizes geodesic proximity rather than angular similarity. The attended representation is obtained via a Lorentzian weighted midpoint AttL(x)i = Pn j=1 Î½i,jÎ»jvj âˆš âˆ’K Pn j=1 Î½i,jÎ»jvj L , where Î»j =v j,0 is the Lorentz factor. Unlike Euclidean averaging, this aggregation remains on Hd K and preserves radial structure during contextualization. 4. Method We now outline our approach to hyperbolic dense retrieval. We begin by introducing the two proposed HyTE architec- tures, followed by an analysis of why naÃ¯ve pooling strate- gies fail in hyperbolic space, and conclude by presenting our geometry-aware aggregation operator. 4.1. Architecture The hyperbolic encoder components described in Section 3 form the building blocks (Figure 2A) of HyTE-FH, our fully hyperbolic transformer (Figure 2B). By operating entirely within hyperbolic geometry, HyTE-FH preserves hierar- chical structure throughout token-level contextualization, aggregation, and similarity computation, with semantic ab- straction and specificity encoded along radial dimensions. HyTE-H (Figure 2C) instead projects pretrained Euclidean representations into hyperbolic space, which allows hyper- bolic geometry to be leveraged with a strong initialization and avoiding the need to train a fully hyperbolic encoder from scratch. While hyperbolic self-attention enables geometry-consistent contextualization at the token level, dense retrieval re- quires aggregating variable-length sequences into fixed- dimensional representations. Standard approaches map representations to tangent space, aggregate in Euclidean space, then map back to the manifold (Yang et al., 2024; Desai et al., 2023), but this distorts hierarchical structure encoded in radial depth in both the models. In the following subsections, we analyze this failure mode formally and in- troduce a pooling operator designed to preserve hierarchical information. 4.2. Failure of NaÃ¯ve Hyperbolic Pooling NaÃ¯ve pooling strategies that aggregate in Euclidean space (Yang et al., 2024; Desai et al., 2023) systemati- cally contract representations toward the origin. This fol- lows from hyperbolic convexity: for any {xi}n i=0 âŠ‚H d K, the barycenter lies strictly closer to the origin than the maximum-radius point unless all points coincide. Conse- quently, document-level embeddings lose the radial separa- tion that encodes document specificity through hierarchical depth. To address this failure mode, we first establish no- tation for projecting ambient vectors onto the hyperboloid and measuring radial depth. Definition 4.1(Lorentz Projection).For vâˆˆR d+1 with âŸ¨v,vâŸ© L <0 and v0 >0 , let Î K(v) = vâˆš KâŸ¨v,vâŸ© L denote the unique positive rescaling satisfying âŸ¨Î K(v),Î  K(v)âŸ©L = 1/K. Definition 4.2(Radial Depth).The radial depth of xâˆˆH d K is r(x) =x 0. Since x0 = 1âˆšâˆ’K cosh( âˆš âˆ’K Ï) where Ï=d K(o,x) , ordering by x0 is equivalent to ordering by intrinsic hyperbolic distance from the origin. Semantically, radial depth encodesconcept specificity: gen- eral concepts should lie near the origin while fine-grained entities should have larger radii. This provides a measurable signature for evaluating whether models learn meaningful hierarchical structure. The simplest aggregation strategy is Euclidean averaging in the ambient space followed by reprojection. However, this approach provably contracts rep- resentations toward the origin (Ganea et al., 2018a; Chami et al., 2019), destroying hierarchical structure encoded in radial depth. We formalize this in the following proposition. Proposition 4.3(Euclidean Mean Contracts).Let {xi}n i=1 âŠ‚H d K with nâ‰¥2 . Define the Euclidean mean Â¯x= 1 n Pn i=1 xi and its projection onto the hyperboloid mEuc = Î K(Â¯x). Then, we have r(mEuc)â‰¤ 1 n nX i=1 r(xi), with equality if and only if allx i are identical. 4 HypRAG Figure 3.Outward Einstein Midpoint. Size of token shows its contribution towards aggregation. The proof of this Proposition is available in Appendix A.2. This failure motivates a precise characterization of desirable pooling behavior. We formalize the requirement that pooling should preserve, rather than collapse, radial structure. Definition 4.4(Outward Bias).A pooling operator P: (Hd K)n â†’H d K isoutward-biasedif r(P({x i}n i=1))â‰¥Â¯r , whereÂ¯ris the weighted mean radius. A natural alternative is a weighted aggregation scheme in which token contributions are modulated by their relative im- portance. For example, Zhu et al. (2020) adopt the Einstein midpoint, the canonical barycenter in hyperbolic space (Gul- cehre et al., 2019), to emphasize semantically specific to- kens during pooling: since points near the boundary receive higher weight via the Lorentz factor Î»i =x i,0, more infor- mative content should dominate the aggregate. However, we show this intuition is misleading: the implicit radial weight- ing is fundamentally insufficient to counteract hyperbolic contraction at the document level. Proposition 4.5(Implicit Radial Weighting is Insufficient). The Einstein midpoint weights points by the Lorentz factor Î»i =x i,0, but this weighting grows as exp( âˆš âˆ’KÏ) while hyperbolic volume grows as exp((dâˆ’1) âˆš âˆ’KÏ). Specifi- cally, for a point xâˆˆH d K at hyperbolic distance Ï from the origino= (1/ âˆš âˆ’K,0, . . . ,0), we have x0 = 1âˆš âˆ’K cosh âˆš âˆ’K Ï  âˆ¼ 1 2 âˆš âˆ’K exp âˆš âˆ’K Ï  as Ïâ†’ âˆž . Thus, the Lorentz factor weighting undercom- pensates for the exponential growth of hyperbolic balls at large radii by a factor ofexp (dâˆ’2) âˆš âˆ’K Ï  . These results establish that neither Euclidean averaging nor the standard Einstein midpoint satisfies the outward- bias property required for hierarchy-preserving aggregation. This motivates the design of a pooling operator with ex- plicit radial amplification. The proof of this Proposition is available in Appendix A.3. 4.3. Outward Einstein Midpoint Pooling To mitigate radial contraction during aggregation, we in- troduce theOutward Einstein Midpoint, a geometry-aware pooling operator that explicitly amplifies the contribution of tokens with larger hyperbolic radius. Let {xi}n i=1 âŠ‚H d K denote a sequence of token embeddings, with optional atten- tion weights wi â‰¥0 , and Î»i denoting the Lorentz factors.We define a radius-dependent weighting function Ï•(xi) =x p i,0, p >0, which is monotone in the radial coordinate. The Outward Einstein Midpoint is then given by mOEM K,p = Pn i=1 (wi Ï•(xi))Î» ixiPn i=1 (wi Ï•(xi))Î» i , followed by reprojection onto the hyperboloidH d K. As shown in Figure 3, by construction, this operator assigns disproportionately higher weight to tokens located farther from the origin, counteracting the contraction inherent to naÃ¯ve averaging. We now establish theoretical guarantees for the Outward Einstein Midpoint, showing that it system- atically improves upon the standard Einstein midpoint in preserving radial structure. Theorem 4.6(OEM Pre-Projection Bound).Let Ëœv=Pn i=1 Ëœwixi where Ëœwi âˆw ixp+1 i,0 are the normalized OEM weights. Then, forpâ‰¥0, we have Ëœv0 = Pn i=1 wixp+2 i,0 Pn i=1 wixp+1 i,0 â‰¥ Pn i=1 wixi,0Pn i=1 wi = Â¯rw. We apply Chebyshevâ€™s sum inequality to the co-monotonic sequences ai =x p+1 i,0 and bi =x i,0 to prove this. Full proof can be found in Appendix A.4. While projection onto Hd K contracts the radial coordinate, the OEMâ€™s concentration of weight on high-radius tokens inflates the pre-projection average, counteracting this effect. Theorem 4.6 establishes that OEM increases the pre-projection radial coordinate. The following theorem shows a stronger result: OEM prov- ably dominates the standard Einstein midpoint in preserving radial structure. Theorem 4.7(OEM Outward Bias).Let mEin K denote the standard Einstein midpoint (p= 0 ) and mOEM K,p the Outward Einstein Midpoint. Then, for allpâ‰¥1: r(mOEM K,p )â‰¥r(m Ein K ). The OEM weights Ëœwi âˆw ixp+1 i,0 concentrate more mass on high-radius points than the Einstein weights wixi,0, in- creasing the pre-projection time component while reducing pairwise dispersion. Full proof in Appendix A.5. Together, these results establish that the Outward Einstein Midpoint provably preserves hierarchical structure during aggregation, in contrast to both Euclidean averaging and the standard Einstein midpoint. We validate this empirically through 5 HypRAG concept-level hierarchy analysis (Section 5.2), showing that models using OEM pooling maintain monotonically increas- ing radii across semantic specificity levelsâ€”a property ab- sent in Euclidean baselines. 4.4. Training Methodology We train the hyperbolic encoder in three stages, with all objectives operating directly on the Lorentz manifold using geodesic-based similarity. Stage 1: Hyperbolic Masked Language Modeling.We initialize via masked language modeling (MLM), following the standard BERT objective in hyperbolic space. Contex- tualization is performed through hyperbolic self-attention, with all intermediate representations on the hyperboloid. Predictions are produced using a Lorentzian multinomial logistic regression (LorentzMLR) (Bdeir et al., 2024) head, which defines class logits via Lorentzian inner products. Only HyTE-FH is trained on MLM, while for HyTE-H we choose a pre-trained Euclidean model as the MLM base to leverage a sronger initialization in low-resource settings. Stage 2: Unsupervised Contrastive Pre-Training.We fine-tune the resulting MLM model on queryâ€“document pairs by minimizing unsupervised contrastive loss. Sim- ilarity is defined as negative geodesic distance s(q, d) = âˆ’d K(q, d).The contrastive loss over in-batch negatives is Lctr =âˆ’ 1 N NX i=1 log exp(s(qi,d i)/Ï„), whereÏ„ >0is a temperature parameter. Stage 3: Supervised Contrastive Learning Fine-tuning. In the final stage of training, we further fine-tune the encoder using supervised contrastive learning on labeled queryâ€“ document data. Given a query qi, a set of relevant doc- uments D+ i , and a set of non-relevant documents Dâˆ’ i , the supervised contrastive objective encourages the query rep- resentation to be closer to all relevant documents than to non-relevant ones Lsup =âˆ’ 1 N NX i=1 log P d+âˆˆD+ i exp(s(qi,d +)/Ï„) P dâˆˆD+ i âˆªDâˆ’ i exp(s(qi,d)/Ï„) , where Ï„ >0 is a temperature parameter. This stage explic- itly aligns hyperbolic distances with supervised relevance signals, refining retrieval behavior beyond unsupervised co-occurrence structure. Retrieval-Augmented Generation.At inference time, the trained hyperbolic encoder is used to retrieve the top-k doc- uments C for a given queryt. These retrieved documents are then provided as context to a downstream generative lan- guage model. Prompt formatting and generation follow stan- dard practice and are provided in Appendix B. We present runtime and computational complexity in Appendix D. Table 1.Performance on MTEB benchmark. We report mean scores across tasks and task types. HyTE-FH performs best among the three models. Model Mean (Task) Mean (TaskType) EucBERT 54.11 51.31 HyTE-HEuc 54.57 53.71 HyTE-FH 56.41 53.75 5. Experiments and Results 5.1. Experimental Setup Datasets.We pre-train our models using publicly available corpora following the data curation and filtering protocols introduced in nomic-embed (Nussbaum et al., 2025). For masked language modeling (MLM), we use the high-quality 2023 Wikipedia dump, which provides broad topical cover- age and long-form text suitable for learning general-purpose semantic representations. For contrastive pre-training, we leverage approximately 235 million text pairs curated and filtered as described in (Nussbaum et al., 2025), designed to encourage semantic alignment across paraphrases and re- lated content at scale. Finally, for task-specific fine-tuning, we use the training splits of the BEIR benchmark (Thakur et al., 2021), which comprises a diverse collection of re- trieval tasks spanning multiple domains and query styles. Evaluation Benchmarks.We evaluate our approach on two complementary benchmarks: (1) the Massive Text Em- bedding Benchmark (MTEB) (Muennighoff et al., 2023) to assess embedding quality across diverse tasks, and (2) RAGBench (Friel et al., 2024b) for end-to-end RAG sys- tem evaluation. In MTEB, we particularly use the English part of the benchmark. RAGBench evaluates RAG systems on domain-specific question-answering datasets including CovidQA, Cuad, Emanual, DelucionQA, and ExpertQA. Baselines.We adopt different baseline strategies for our two models based on their training paradigms. For HyTE- FH, which is pre-trained from scratch, we train a fully Euclidean equivalent called EucBERT using the same ar- chitecture and training setup. This controlled compari- son isolates the contribution of hyperbolic geometry. We also evaluate HyTE-HEuc, a hybrid hyperbolic model ini- tialized with EucBERT. The three models are evaluated on MTEB and RAGBench. For HyTE-Hbert, which is fine-tuned with modernbert-base (Warner et al., 2024) as base model, we compare against state-of-the-art em- bedding models smaller than 500M parameters, includ- ing gte-multilingual-base (Zhang et al., 2024), KaLM- embedding-multilingual-mini-v1 (Hu et al., 2025), and embeddinggemma-300m (Vera et al., 2025). Metrics.For MTEB, we report mean scores across tasks and task types. For RAG evaluation, we measure three key metrics using RAGAS (Es et al., 2024): (1)Faithfulness, 6 HypRAG Table 2.RAG benchmark results comparing our model variants. Average CovidQA Cuad Emanual DelucionQA ExpertQA Model F CR AR F CR AR F CR AR F CR AR F CR AR F CR AR EucBERT0.596 0.798 0.647 0.685 0.863 0.582 0.654 0.644 0.641 0.642 0.646 0.674 0.5250.9680.679 0.475 0.872 0.662 HyTE-HEuc 0.706 0.814 0.739 0.708 0.868 0.668 0.7870.652 0.710 0.679 0.835 0.814 0.737 0.857 0.773 0.623 0.859 0.728 HyTE-FH0.732 0.848 0.765 0.764 0.916 0.694 0.747 0.674 0.752 0.6600.807 0.704 0.7890.906 0.861 0.702 0.936 0.814 F = Faithfulness, CR = Context Relevance, AR = Answer Relevance. Best results in bold. Table 3.RAG benchmark results comparing our hybrid model with state-of-the-art embedding models. HyTE-H demonstrates competitive performance particularly in context relevance and answer relevance. Average CovidQA Cuad Emanual DelucionQA ExpertQA Model F CR AR F CR AR F CR AR F CR AR F CR AR F CR AR ModernBert*0.617 0.748 0.6320.656 0.895 0.53780.632 0.709 0.7460.567 0.715 0.6390.655 0.6657 0.51830.575 0.758 0.718 GTE 0.659 0.701 0.6500.695 0.840 0.538 0.733 0.599 0.7790.546 0.608 0.6860.648 0.725 0.549 0.672 0.731 0.698 Gemma 0.603 0.735 0.6840.685 0.760 0.497 0.724 0.600 0.7780.555 0.884 0.6870.612 0.643 0.705 0.442 0.791 0.755 KaLM-mini-v10.624 0.719 0.5910.656 0.787 0.528 0.742 0.789 0.7160.565 0.776 0.6160.553 0.581 0.573 0.607 0.666 0.522 HyTE-Hbert 0.763 0.904 0.8320.797 0.974 0.755 0.760 0.683 0.8040.688 0.943 0.8990.829 0.965 0.871 0.739 0.958 0.834 F = Faithfulness, CR = Context Relevance, AR = Answer Relevance. Best results in bold. which assesses whether generated answers are grounded in the retrieved context; (2)Context Relevance, which mea- sures how relevant the retrieved documents are to the query; and (3)Answer Relevance, which evaluates how well the generated answer addresses the userâ€™s question. Implementation.We implement all hyperbolic models using HyperCore (He et al., 2025e) and train on NVIDIA H100 GPUs. All three models, HyTE-FH, HyTE-H, and Eu- cBERT, share the same architecture, each containing 149M parameters with 12 transformer layers and 768-dimensional embeddings. For generation and judging, we use Llama- 3.1-8B-Instruct (Weerawardhena et al., 2025). For RAG benchmarks, we fix the retrieval context window size to 5 for all models to ensure a controlled comparison; we additionally report ablations with larger context sizes in Appendix Table A3. 5.2. Results MTEB Benchmark.Table 1 reports performance on the MTEB benchmark. HyTE-FH achieves the highest mean score across tasks (56.41), outperforming both EucBERT (54.11) and HyTE-HEuc (54.57). On the task-type mean, HyTE-FH and HyTE-HEuc perform comparably (53.75 and 53.71, respectively), with both surpassing EucBERT (51.31). These results demonstrate that hyperbolic representations not only improve RAG retrieval but also remain competitive on general-purpose embedding benchmarks. We present task-wise results in Table A1. RAG Benchmark Results.Table 2 presents RAG bench- mark results across five datasets. HyTE-FH achieves the best average performance across all three metrics: faithful- ness (0.732), context relevance (0.848), and answer rele- vance (0.765). HyTE-HEuc ranks second overall, with both hyperbolic variants substantially outperforming EucBERT. On individual datasets, HyTE-FH leads on CovidQA, Cuad, DelucionQA, and ExpertQA, while HyTE-HEuc achieves the best context and answer relevance on Emanual. These results demonstrate that hyperbolic geometry consistently improves retrieval quality for RAG across diverse domains. Table 3 reports RAG performance across five datasets. HyTE-Hbert consistently outperforms strong Euclidean em- bedding baselines across all metrics, with particularly large gains in context relevance and answer relevance. These improvements indicate that hyperbolic representations are more effective at retrieving structurally relevant evidence, which is critical for downstream generation quality in RAG pipelines. In qualitative case studies shows in Appendix E.1, we observe that Euclidean models frequently fail to retrieve key supporting passages altogether, whereas hyperbolic model recover relevant evidence more reliably, leading to more faithful and contextually grounded answers. Concept-Level Hierarchy Analysis.A central motivation for hyperbolic embeddings is their capacity to preserve hi- erarchical relationships (Section 4.2). To understand how models capture document hierarchy, we analyze learned radii (distances from the origin in the PoincarÃ© ball) across five hierarchical levels: from Level 1 (most general, e.g., document-level topics) to Level 5 (most specific, e.g., fine- grained entities). Figure 4 presents these results. The fully hyperbolic model demonstrates clear hierarchical organi- zation with radii increasing monotonically from Level 1 (2.902) to Level 5 (3.488, +20.2%). This shows the model naturally places general concepts near the origin and spe- cific details toward the boundary, consistent with hyper- bolic geometry, where proximity to the origin represents generality. Euclidean models show flat or decreasing dis- tributions. Baselines maintain constant norms across levels 7 HypRAG Figure 4.Empirical validation of hierarchical encoding. Left:Euclidean models show flat or decreasing norms.Middle:HyTE-H demonstrate increasing norms with fine-tuning enhancing this trend.Right:HyTE-FH achieves +20.2% total increase from L1 to L5. Bottom:Normalized comparison and percent change summary highlighting the contrasting behaviors of different geometric approaches. or decreases norm by 30%, reflecting inverted structure. Hybrid models exhibit substantially larger radii from the hy- perbolic component. The fine-tuned hybrid increases from 116.9 to 146.7, showing that fine-tuning induces structured hierarchy. We have attached the dataset for this case study in the supplementary material. The concept level hierarchy data is available in Appendix C. Ablation Studies.We compare two pooling strategies for aggregating token embeddings into document representa- tions: CLS token pooling and OEM pooling. CLS pool- ing uses the representation of a special classification token, while OEM pooling performs geometry-aware aggregation directly in hyperbolic space. Table 4 shows that OEM pool- ing yields higher performance across both mean task and mean task-type metrics on MTEB retrieval tasks, indicating more effective document-level aggregation in the hyperbolic setting. We also show that using geodesic distance in the contrastive objective outperforms the Lorentz inner product (Appendix Table A2), suggesting better alignment of repre- sentations on the manifold. Additionally, hyperbolic models maintain strong performance with smaller retrieval budgets, whereas Euclidean baselines require larger context windows to achieve comparable results (Appendix Table A3). Table 4.Comparison of pooling strategies on MTEB tasks. OEM pooling leverages hyperbolic geometry for improved performance. Pooling Strategy Mean (Task) Mean (TaskType) CLS Token 49.33 48.90 OEM 56.41 53.75 6. Conclusion We introduced hyperbolic dense retrieval for RAG, show- ing that aligning embedding geometry with the hierarchical structure of language improves faithfulness and answer qual- ity. Our approach preserves document-level structure during aggregation through a geometry-aware pooling operator, ad- dressing a key failure mode of Euclidean retrieval pipelines. Across evaluations, we observe consistent gains using mod- els substantially smaller than current state-of-the-art retriev- ers, highlighting the effectiveness of hyperbolic inductive bias over scale alone. Case studies further show that hy- perbolic representations organize documents by specificity through norm-based separation, a property absent in Eu- clidean embeddings. These findings suggest that embedding geometry is a central design choice for reliable retrieval in RAG systems, with implications for future scalable and multimodal retrieval architectures. 8 HypRAG Impact Statement This paper presents work whose goal is to advance the field of Machine Learning, specifically dense retrieval for retrieval-augmented generation systems. By improving the geometric fidelity of document embeddings, our approach aims to reduce retrieval errors that can lead to hallucinated or poorly grounded responses in RAG systems. We believe more accurate retrieval contributes positively to the reliabil- ity of AI-generated content. Additionally, our fully hyper- bolic model demonstrates improved parameter efficiency, which may reduce computational costs and environmental impact associated with training and deploying embedding models. There are many potential societal consequences of our work, none which we feel must be specifically high- lighted here. References Asai, A., Wu, Z., Wang, Y ., Sil, A., and Hajishirzi, H. Self- RAG: Learning to retrieve, generate, and critique through self-reflection. InThe Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=hSyW5go0v8. Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., McNamara, A., Mitra, B., Nguyen, T., et al. Ms marco: A human generated machine reading comprehension dataset.arXiv preprint arXiv:1611.09268, 2016. Bdeir, A., Schwethelm, K., and Landwehr, N. Fully hyper- bolic convolutional neural networks for computer vision. InThe Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=ekz1hN5QNh. Bogolin, S.-V ., Croitoru, I., Jin, H., Liu, Y ., and Albanie, S. Cross modal retrieval with querybank normalisation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5194â€“5205, 2022. Bui, N., Yang, M., Chen, R., Neves, L., Ju, M., Ying, R., Shah, N., and Zhao, T. Learning along the arrow of time: Hyperbolic geometry for backward-compatible representation learning. InForty-second International Conference on Machine Learning, 2025. URL https: //openreview.net/forum?id=KUphSx7PAC. Cao, L., Wang, R., Li, J., Zhou, Z., and Yang, M. Hyperbolicrag: Enhancing retrieval-augmented gener- ation with hyperbolic representations.arXiv preprint arXiv:2511.18808, 2025. Chami, I., Ying, Z., RÃ©, C., and Leskovec, J. Hyperbolic graph convolutional neural networks.Advances in neural information processing systems, 32, 2019. Chen, W., Han, X., Lin, Y ., Zhao, H., Liu, Z., Li, P., Sun, M., and Zhou, J. Fully hyperbolic neural networks. InPro- ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5672â€“5686, 2022. Desai, K., Nickel, M., Rajpurohit, T., Johnson, J., and Vedantam, S. R. Hyperbolic image-text representations. InInternational Conference on Machine Learning, pp. 7694â€“7731. PMLR, 2023. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. InProceedings of the 2019 confer- ence of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 4171â€“4186, 2019. Dhingra, B., Shallue, C., Norouzi, M., Dai, A., and Dahl, G. Embedding text in hyperbolic spaces. In GlavaÅ¡, G., So- masundaran, S., Riedl, M., and Hovy, E. (eds.),Proceed- ings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12), pp. 59â€“69, New Orleans, Louisiana, USA, June 2018. Asso- ciation for Computational Linguistics. doi: 10.18653/v1/ W18-1708. URL https://aclanthology.org/ W18-1708/. Es, S., James, J., Anke, L. E., and Schockaert, S. Ragas: Automated evaluation of retrieval augmented generation. InProceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pp. 150â€“158, 2024. Fan, W., Ding, Y ., Ning, L., Wang, S., Li, H., Yin, D., Chua, T.-S., and Li, Q. A survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining, pp. 6491â€“6501, 2024. Friel, R., Belyi, M., and Sanyal, A. Ragbench: Explainable benchmark for retrieval-augmented generation systems. arXiv preprint arXiv:2407.11005, 2024a. Friel, R., Belyi, M., and Sanyal, A. Ragbench: Explainable benchmark for retrieval-augmented generation systems. arXiv preprint arXiv:2407.11005, 2024b. Ganea, O., BÃ©cigneul, G., and Hofmann, T. Hyperbolic neu- ral networks.Advances in neural information processing systems, 31, 2018a. Ganea, O.-E., BÃ©cigneul, G., and Hofmann, T. Hyperbolic neural networks. InProceedings of the 32nd International Conference on Neural Information Processing Systems, NIPSâ€™18, pp. 5350â€“5360, Red Hook, NY , USA, 2018b. Curran Associates Inc. 9 HypRAG Gao, T., Yao, X., and Chen, D. SimCSE: Sim- ple contrastive learning of sentence embeddings. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.),Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6894â€“6910, Online and Punta Cana, Dominican Republic, November 2021. Association for Computa- tional Linguistics. doi: 10.18653/v1/2021.emnlp-main. 552. URL https://aclanthology.org/2021. emnlp-main.552/. Gulcehre, C., Denil, M., Malinowski, M., Razavi, A., Pas- canu, R., Hermann, K. M., Battaglia, P., Bapst, V ., Ra- poso, D., Santoro, A., and de Freitas, N. Hyperbolic atten- tion networks. InInternational Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=rJxHsjRqFQ. Han, H., Wang, Y ., Shomer, H., Guo, K., Ding, J., Lei, Y ., Halappanavar, M., Rossi, R. A., Mukherjee, S., Tang, X., et al. Retrieval-augmented generation with graphs (graphrag).arXiv preprint arXiv:2501.00309, 2024. He, N., Anand, R., Madhu, H., Maatouk, A., Krishnaswamy, S., Tassiulas, L., Yang, M., and Ying, R. HELM: Hy- perbolic large language models via mixture-of-curvature experts. InThe Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025a. URL https: //openreview.net/forum?id=RnbJPkakkm. He, N., Liu, J., Zhang, B., Bui, N., Maatouk, A., King, I., Yang, M., Weber, M., and Ying, R. Position: Be- yond euclidean â€“ foundation models should embrace non- euclidean geometries. InThe Fourth Learning on Graphs Conference, 2025b. URL https://openreview. net/forum?id=WoK4o90lln. He, N., Madhu, H., Bui, N., Yang, M., and Ying, R. Hy- perbolic deep learning for foundation models: A survey. InProceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V . 2, pp. 6021â€“ 6031, 2025c. He, N., Yang, M., and Ying, R. Lorentzian resid- ual neural networks. InProceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V .1, KDD â€™25, pp. 436â€“447, New York, NY , USA, 2025d. Association for Computing Machinery. ISBN 9798400712456. doi: 10.1145/ 3690624.3709292. URL https://doi.org/10. 1145/3690624.3709292. He, N., Yang, M., and Ying, R. Hypercore: The core framework for building hyperbolic foundation models with comprehensive modules.arXiv preprint arXiv:2504.08912, 2025e. Hu, X., Shan, Z., Zhao, X., Sun, Z., Liu, Z., Li, D., Ye, S., Wei, X., Chen, Q., Hu, B., et al. Kalm-embedding: Superior training data brings a stronger embedding model. arXiv preprint arXiv:2501.01028, 2025. Ibrahimi, S., Atigh, M. G., Noord, N. V ., Mettes, P., and Worring, M. Intriguing properties of hyperbolic embeddings in vision-language models.Transactions on Machine Learning Research, 2024. ISSN 2835- 8856. URL https://openreview.net/forum? id=P5D2gfi4Gg. Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense infor- mation retrieval with contrastive learning.arXiv preprint arXiv:2112.09118, 2021. Karpukhin, V ., Oguz, B., Min, S., Lewis, P. S., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. InEMNLP (1), pp. 6769â€“6781, 2020. Khrulkov, V ., Mirvakhabova, L., Ustinova, E., Oseledets, I., and Lempitsky, V . Hyperbolic image embeddings. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6418â€“6428, 2020. Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catan- zaro, B., and Ping, W. Nv-embed: Improved techniques for training llms as generalist embedding models.arXiv preprint arXiv:2405.17428, 2024. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V ., Goyal, N., KÃ¼ttler, H., Lewis, M., Yih, W.-t., RocktÃ¤schel, T., et al. Retrieval-augmented generation for knowledge- intensive nlp tasks.Advances in neural information pro- cessing systems, 33:9459â€“9474, 2020. Li, Y ., Chen, H., Sun, X., Sun, Z., Li, L., Cui, L., Yu, P. S., and Xu, G. Hyperbolic hypergraphs for sequential recom- mendation. InProceedings of the 30th ACM international conference on information & knowledge management, pp. 988â€“997, 2021. Li, Z., Zhang, X., Zhang, Y ., Long, D., Xie, P., and Zhang, M. Towards general text embeddings with multi-stage contrastive learning.arXiv preprint arXiv:2308.03281, 2023. Liu, Q., Nickel, M., and Kiela, D. Hyperbolic graph neural networks. In Wallach, H., Larochelle, H., Beygelzimer, A., d'AlchÃ©-Buc, F., Fox, E., and Garnett, R. (eds.),Advances in Neural Information Process- ing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ 103303dd56a731e377d01f6a37badae3-Paper. pdf. 10 HypRAG Muennighoff, N., Tazi, N., Magne, L., and Reimers, N. MTEB: Massive text embedding benchmark. In Vla- chos, A. and Augenstein, I. (eds.),Proceedings of the 17th Conference of the European Chapter of the Asso- ciation for Computational Linguistics, pp. 2014â€“2037, Dubrovnik, Croatia, May 2023. Association for Compu- tational Linguistics. doi: 10.18653/v1/2023.eacl-main. 148. URL https://aclanthology.org/2023. eacl-main.148/. Muennighoff, N., Hongjin, S., Wang, L., Yang, N., Wei, F., Yu, T., Singh, A., and Kiela, D. Generative representa- tional instruction tuning. InThe Thirteenth International Conference on Learning Representations, 2024. Ni, B., Liu, Z., Wang, L., Lei, Y ., Zhao, Y ., Cheng, X., Zeng, Q., Dong, L., Xia, Y ., Kenthapadi, K., et al. Towards trust- worthy retrieval augmented generation for large language models: A survey.arXiv preprint arXiv:2502.06872, 2025. Ni, C.-C., Lin, Y .-Y ., Luo, F., and Gao, J. Community detection on networks with ricci flow.Scientific reports, 9(1):1â€“12, 2019. Nussbaum, Z., Morris, J. X., Mulyar, A., and Duderstadt, B. Nomic embed: Training a reproducible long context text embedder. InTransactions on Machine Learning Research, 2025. URLhttps://openreview.net/ forum?id=IPmzyQSiQE. Oche, A. J., Folashade, A. G., Ghosal, T., and Biswas, A. A systematic review of key retrieval-augmented generation (rag) systems: Progress, gaps, and future directions.arXiv preprint arXiv:2507.18910, 2025. Pal, A., van Spengler, M., di Melendugno, G. M. D., Fla- borea, A., Galasso, F., and Mettes, P. Compositional entailment learning for hyperbolic vision-language mod- els. 2024. URL https://arxiv.org/abs/2410. 06912. Peng, W., Varanka, T., Mostafa, A., Shi, H., and Zhao, G. Hyperbolic deep neural networks: A survey.IEEE Trans- actions on pattern analysis and machine intelligence, 44 (12):10023â€“10044, 2021. Radovanovic, M., Nanopoulos, A., and Ivanovic, M. Hubs in space: Popular nearest neighbors in high-dimensional data.Journal of Machine Learning Research, 11(sept): 2487â€“2531, 2010. Reimers, N. and Gurevych, I. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Inui, K., Jiang, J., Ng, V ., and Wan, X. (eds.),Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982â€“3992, Hong Kong, China, November 2019. Association for Computational Lin- guistics. doi: 10.18653/v1/D19-1410. URL https: //aclanthology.org/D19-1410/. Reimers, N. and Gurevych, I. The curse of dense low- dimensional information retrieval for large index sizes. InProceedings of the 59th Annual Meeting of the Associ- ation for Computational Linguistics and the 11th Interna- tional Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 605â€“611, 2021. Robinson, M., Dey, S., and Sweet, S. The structure of the token space for large language models.arXiv preprint arXiv:2410.08993, 2024. Thakur, N., Reimers, N., RÃ¼cklÃ©, A., Srivastava, A., and Gurevych, I. BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Process- ing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum? id=wCu6T5xFjeJ. Vera, H. S., Dua, S., Zhang, B., Salz, D., Mullins, R., Pa- nyam, S. R., Smoot, S., Naim, I., Zou, J., Chen, F., et al. Embeddinggemma: Powerful and lightweight text repre- sentations.arXiv preprint arXiv:2509.20354, 2025. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. Text embeddings by weakly-supervised contrastive pre-training.arXiv preprint arXiv:2212.03533, 2022. Warner, B., Chaffin, A., ClaviÃ©, B., Weller, O., HallstrÃ¶m, O., Taghadouini, S., Gallagher, A., Biswas, R., Ladhak, F., Aarsen, T., Cooper, N., Adams, G., Howard, J., and Poli, I. Smarter, better, faster, longer: A modern bidi- rectional encoder for fast, memory efficient, and long context finetuning and inference, 2024. URL https: //arxiv.org/abs/2412.13663. Weerawardhena, S., Kassianik, P., Nelson, B., Saglam, B., Vellore, A., Priyanshu, A., Vijay, S., Aufiero, M., Goldblatt, A., Burch, F., et al. Llama-3.1-foundationai- securityllm-8b-instruct technical report.arXiv preprint arXiv:2508.01059, 2025. Wei, R., Liu, Y ., Song, J., Xie, Y ., and Zhou, K. Explor- ing hierarchical information in hyperbolic space for self- supervised image hashing.IEEE Transactions on Image Processing, 33:1768â€“1781, 2024. doi: 10.1109/TIP.2024. 3371358. Xiong, L., Xiong, C., Li, Y ., Tang, K.-F., Liu, J., Ben- nett, P. N., Ahmed, J., and Overwijk, A. Approximate nearest neighbor negative contrastive learning for dense 11 HypRAG text retrieval. InInternational Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=zeFrfgyZln. Yan, S.-Q., Gu, J.-C., Zhu, Y ., and Ling, Z.-H. Cor- rective retrieval augmented generation.arXiv preprint arXiv:2401.15884, 2024. Yang, H., Chen, H., Li, L., Philip, S. Y ., and Xu, G. Hyper meta-path contrastive learning for multi-behavior recom- mendation. In2021 IEEE International Conference on Data Mining (ICDM), pp. 787â€“796. IEEE, 2021. Yang, M., Zhou, M., Ying, R., Chen, Y ., and King, I. Hyper- bolic representation learning: Revisiting and advancing. InInternational Conference on Machine Learning, pp. 39639â€“39659. PMLR, 2023. Yang, M., Verma, H., Zhang, D. C., Liu, J., King, I., and Ying, R. Hypformer: Exploring efficient trans- former fully in hyperbolic space. InProceedings of the 30th ACM SIGKDD Conference on Knowledge Dis- covery and Data Mining, KDD â€™24, pp. 3770â€“3781, New York, NY , USA, 2024. Association for Comput- ing Machinery. ISBN 9798400704901. doi: 10.1145/ 3637528.3672039. URL https://doi.org/10. 1145/3637528.3672039. Yang, M., B, R. S. B., Feng, A., Xiong, B., Liu, J., King, I., and Ying, R. Hyperbolic fine-tuning for large language models. InThe Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https: //openreview.net/forum?id=TkEdQv0bXB. Zhang, X., Zhang, Y ., Long, D., Xie, W., Dai, Z., Tang, J., Lin, H., Yang, B., Xie, P., Huang, F., et al. mgte: Gen- eralized long-context text representation and reranking models for multilingual text retrieval. InProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp. 1393â€“1412, 2024. Zhang, Y ., Li, M., Long, D., Zhang, X., Lin, H., Yang, B., Xie, P., Yang, A., Liu, D., Lin, J., et al. Qwen3 embed- ding: Advancing text embedding and reranking through foundation models.arXiv preprint arXiv:2506.05176, 2025. Zhu, Y ., Zhou, D., Xiao, J., Jiang, X., Chen, X., and Liu, Q. HyperText: Endowing FastText with hy- perbolic geometry. In Cohn, T., He, Y ., and Liu, Y . (eds.),Findings of the Association for Computa- tional Linguistics: EMNLP 2020, pp. 1166â€“1171, On- line, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp. 104. URL https://aclanthology.org/2020. findings-emnlp.104/. 12 HypRAG A. Proofs Throughout, we work in the Lorentz model with curvatureK <0, where Hd K ={xâˆˆR d+1 :âŸ¨x,xâŸ© L = 1/K, x 0 >0} andâŸ¨x,yâŸ© L =âˆ’x 0y0 +Pd i=1 xiyi denotes the Lorentzian inner product. A.1. Auxiliary Lemma Lemma A.1(Lorentzian Inner Product Bound).For any x,yâˆˆH d K, we have KâŸ¨x,yâŸ© L â‰¥1 , with equality if and only if x=y. Proof.The geodesic distance onH d K satisfies dK(x,y) = 1âˆš âˆ’K coshâˆ’1(KâŸ¨x,yâŸ© L). Since coshâˆ’1 : [1,âˆž)â†’[0,âˆž) and dK(x,y)â‰¥0 with equality if and only if x=y , we conclude KâŸ¨x,yâŸ© L â‰¥1 with equality if and only ifx=y. 2 A.2. Proof of Proposition 4.3 Proposition A.2(Euclidean Mean Contracts).Let {xi}n i=1 âŠ‚H d K with nâ‰¥2 . Define the Euclidean mean Â¯x= 1 n Pn i=1 xi and its projection onto the hyperboloidm Euc = Î K(Â¯x). Then: r(mEuc)â‰¤ 1 n nX i=1 r(xi) with equality if and only if allx i are identical. Proof.We first verify that the projection is well-defined, then establish the contraction inequality. We must showâŸ¨ Â¯x, Â¯xâŸ©L <0andÂ¯x 0 >0. The latter is immediate sinceÂ¯x 0 = 1 n P i xi,0 >0. For the former, compute: KâŸ¨Â¯x, Â¯xâŸ©L =K * 1 n X i xi, 1 n X j xj + L = 1 n2 X i,j KâŸ¨x i,x jâŸ©L. By Lemma A.1, each term satisfiesKâŸ¨x i,x jâŸ©L â‰¥1. Therefore: KâŸ¨Â¯x, Â¯xâŸ©L â‰¥ 1 n2 Â·n 2 = 1>0. SinceK <0, this impliesâŸ¨ Â¯x, Â¯xâŸ©L <0, confirming projectability. The projection is given bym Euc = Â¯x/ p KâŸ¨Â¯x, Â¯xâŸ©L, so the radial depth satisfies: r(mEuc) = Â¯x0p KâŸ¨Â¯x, Â¯xâŸ©L . From Step 1, we haveKâŸ¨ Â¯x, Â¯xâŸ©L â‰¥1, hence p KâŸ¨Â¯x, Â¯xâŸ©L â‰¥1. Therefore: r(mEuc) = Â¯x0p KâŸ¨Â¯x, Â¯xâŸ©L â‰¤Â¯x0 = 1 n nX i=1 xi,0 = 1 n nX i=1 r(xi). Equality holds if and only if p KâŸ¨Â¯x, Â¯xâŸ©L = 1, which by Step 1 requiresKâŸ¨x i,x jâŸ©L = 1for all pairsi, j. By Lemma A.1, this occurs if and only ifx i =x j for alli, j, i.e., all points are identical.2 A1 HypRAG A.3. Proof of Proposition 4.5 Proposition A.3(Implicit Radial Weighting is Insufficient).The Einstein midpoint weights points by the Lorentz factor Î»i =x i,0, but this weighting grows as exp( âˆš âˆ’K Ï) while hyperbolic volume grows as exp((dâˆ’1) âˆš âˆ’K Ï). The Lorentz factor weighting therefore undercompensates by a factor ofexp((dâˆ’2) âˆš âˆ’K Ï)fordâ‰¥3. Proof. We establish the asymptotic growth rates of the Lorentz factor and hyperbolic volume separately, then compare them. Step 1: Lorentz factor asymptotics.The hyperbolic distance from the origin o= (1/ âˆš âˆ’K,0, . . . ,0) to a point xâˆˆH d K is: Ï=d K(o,x) = 1âˆš âˆ’K coshâˆ’1(KâŸ¨o,xâŸ© L). Computing the inner product: âŸ¨o,xâŸ© L =âˆ’ x0âˆš âˆ’K , soKâŸ¨o,xâŸ© L =âˆ’KÂ·(âˆ’x 0/ âˆš âˆ’K) = âˆš âˆ’K x0. Thus: Ï= 1âˆš âˆ’K coshâˆ’1( âˆš âˆ’K x0). Inverting this relation: x0 = 1âˆš âˆ’K cosh( âˆš âˆ’K Ï). For largeÏ, usingcosh(t)âˆ¼ 1 2 et: x0 âˆ¼ 1 2 âˆš âˆ’K exp( âˆš âˆ’K Ï). Hence the Lorentz factorÎ»=x 0 grows asexp( âˆš âˆ’K Ï). Step 2: Hyperbolic volume asymptotics.The volume of a geodesic ball of radiusÏinH d K is: Vol(BÏ) = Ï‰dâˆ’1 (âˆ’K) (dâˆ’1)/2 Z Ï 0 sinhdâˆ’1( âˆš âˆ’K t)dt, whereÏ‰ dâˆ’1 = 2Ï€d/2/Î“(d/2)is the surface area of the unit(dâˆ’1)-sphere. For largeÏ, usingsinh(t)âˆ¼ 1 2 et: Vol(BÏ)âˆ¼C d exp((dâˆ’1) âˆš âˆ’K Ï), whereC d is a dimension-dependent constant. Step 3: Compensation deficit.The ratio of volume growth to Lorentz factor growth is: Vol(BÏ) Î» âˆ¼ exp((dâˆ’1) âˆš âˆ’K Ï) exp( âˆš âˆ’K Ï) = exp((dâˆ’2) âˆš âˆ’K Ï). For dâ‰¥3 , this ratio diverges as Ïâ†’ âˆž , demonstrating that the Lorentz factor provides insufficient compensation for the exponential growth of hyperbolic space at large radii.2 A.4. Proof of Theorem 4.6 Theorem A.4(OEM Pre-Projection Bound).Let Ëœv= Pn i=1 Ëœwixi where Ëœwi âˆw ixp+1 i,0 are the normalized OEM weights. Then, forpâ‰¥0: Ëœv0 = Pn i=1 wixp+2 i,0 Pn i=1 wixp+1 i,0 â‰¥ Pn i=1 wixi,0Pn i=1 wi = Â¯rw. A2 HypRAG Proof.We apply Chebyshevâ€™s sum inequality. Define sequences: ai =x p+1 i,0 , b i =x i,0. Since xi,0 >0 for all i (points lie on the upper sheet of the hyperboloid) and pâ‰¥0 , both sequences are strictly positive. Moreover, the sequences are co-monotonic: for anyi, j, xi,0 â‰¥x j,0 â‡ â‡’x p+1 i,0 â‰¥x p+1 j,0 , sincet7â†’t p+1 is strictly increasing on(0,âˆž). Chebyshevâ€™s sum inequality states that for co-monotonic sequences {ai}, {bi} and non-negative weights {wi} withP i wi >0: X i wiaibi ! X i wi ! â‰¥ X i wiai ! X i wibi ! . Substitutinga i =x p+1 i,0 andb i =x i,0: X i wixp+2 i,0 ! X i wi ! â‰¥ X i wixp+1 i,0 ! X i wixi,0 ! . Dividing both sides by P i wixp+1 i,0  (P i wi)>0: P i wixp+2 i,0 P i wixp+1 i,0 â‰¥ P i wixi,0P i wi . The left-hand side equals Ëœv0 and the right-hand side equals Â¯rw, completing the proof. Equality holds if and only if all xi,0 are identical. 2 A.5. Proof of Theorem 4.7 Theorem A.5(OEM Outward Bias).Let mEin K denote the standard Einstein midpoint (p= 0 ) and mOEM K,p the Outward Einstein Midpoint. Then for allpâ‰¥1: r(mOEM K,p )â‰¥r(m Ein K ). Proof.For a general exponentqâ‰¥0, define the weighted average with weights proportional tow ixq+1 i,0 : v(q) = P i wixq+1 i,0 xi P i wixq+1 i,0 . The projected point ism (q) = Î K(v(q))with radial depth: r(m(q)) = v(q) 0p KâŸ¨v (q),v (q)âŸ©L . The Einstein midpoint corresponds toq= 0and the OEM toq=p. We show that q7â†’r(m (q)) is non-decreasing for qâ‰¥0 . Define the normalized weights Î±(q) i =w ixq+1 i,0 /P j wjxq+1 j,0 . As q increases, these weights concentrate toward indices with largerx i,0: ifx i,0 > x j,0, then Î±(q) i Î±(q) j = wi wj  xi,0 xj,0 q+1 is strictly increasing inq. A3 HypRAG The radial depth after projection satisfies: r(m(q))2 = (v(q) 0 )2 KâŸ¨v (q),v (q)âŸ©L . We analyze numerator and denominator separately. Numerator:We havev (q) 0 =P i Î±(q) i xi,0. By Chebyshevâ€™s inequality (Theorem 4.6), this is non-decreasing inq. Denominator:We have KâŸ¨v (q),v (q)âŸ©L = X i,j Î±(q) i Î±(q) j KâŸ¨x i,x jâŸ©L. By Lemma A.1, KâŸ¨x i,x jâŸ©L â‰¥1 with equality iff xi =x j. As weights concentrate on fewer points (larger q), the sum decreases toward 1. Thus as q increases: the numerator (v(q) 0 )2 increases, while the denominator KâŸ¨v (q),v (q)âŸ©L decreases. Both effects increase r(m(q))2, establishing monotonicity. Forpâ‰¥1>0, we concluder(m OEM K,p ) =r(m (p))â‰¥r(m (0)) =r(m Ein K ).2 B. RAG Prompt For each retrieval-augmented generation (RAG) query, we construct a single inference prompt by concatenating the top-|C| retrieved documents into the context window of the language model. The documents are provided verbatim, without re-ranking or compression, and are ordered according to their retrieval score. The language model is then instructed to generate an answer conditioned solely on the retrieved context and the user query, ensuring that any factual content in the response must be supported by the retrieved evidence. RAG Prompt Based on t h e f o l l o w i n g c o n t e x t , answer t h e q u e s t i o n . C o n t e x t : {Doc [ 0 ] , Doc [ 1 ] , . . . , Doc [ | C | ] } Q u e s t i o n : { q u e r y } Answer : C. Hierarchical Document Probe Dense retrieval models implicitly induce a geometry over documents. While Euclidean encoders often cluster documents based on surface-level similarity, they struggle to faithfully represent hierarchical relationships that arise naturally in language and knowledge organization. Hyperbolic spaces, by contrast, are well suited for embedding tree-like and taxonomic structures due to their exponential volume growth. To qualitatively assess whether hyperbolic encoders recover such hierarchical organization, we construct a controlled document set that exhibits a clear semantic hierarchy while ensuring that individual documents remain self-contained and independent. We design a synthetic yet semantically natural hierarchy consisting of five levels of increasing specificity: Scienceâ†’ Mathematicsâ†’Algebraâ†’Linear Algebraâ†’Linear Transformations. At each level, we generate five independent paragraphs that are topically coherent but do not explicitly reference parent or child topics. Importantly, documents at deeper levels refine the semantic scope of higher-level topics without sharing explicit lexical markers or cross-document A4 HypRAG dependencies. This construction isolates hierarchical structure as a latent semantic property rather than an artifact of explicit cues. All paragraphs are embedded independently with no hierarchical supervision. We analyze the resulting embeddings by inspecting their relative organization in the learned space. Hyperbolic models are expected to organize documents according to semantic specificity, with broader concepts closer to the origin and more specific concepts at increasing radial depth. We present a few of them here and we have attached the data file in supplementary material. Hierarchical Document Texts (Verbatim) [SCIENCE] Science is a systematic way of understanding the natural world through observation , experimentation , and reasoning . It encompasses a wide range of disciplines that study phenomena from the smallest subatomic particles to the vast structure of the universe . At its core , science seeks patterns , explanations , and predictive principles that help humans make sense of reality . [MATHEMATICS] Mathematics also plays a central role in modeling complex systems. By formalizing assumptions and relationships , mathematical models help clarify underlying mechanisms and enable precise predictions under wellâˆ’defined conditions . [ALGEBRA] Modern algebra emphasizes structural relationships over explicit computation. Rather than focusing on individual equations , it studies entire systems of elements and operations , revealing patterns that persist across different mathematical settings . [LINEAR ALGEBRA] The power of linear algebra lies in its balance between abstraction and computation. While grounded in rigorous theory , it offers efficient numerical techniques that scale to highâˆ’dimensional problems. [LINEAR TRANSFORMATIONS] Linear transformations form the foundation of many applied systems, from computer graphics to neural networks. Understanding their behavior is essential for analyzing stability , expressiveness , and computational efficiency . D. Runtime and Computational Complexity In this section, we analyze the computational complexity of the proposed hyperbolic dense retrieval system and compare it to a standard Euclidean transformer-based retriever. Let n denote the input sequence length, d the hidden dimension, h the number of attention heads, andLthe number of transformer layers. Hyperbolic Transformer Encoder.The HyTE-FH encoder follows the standard transformer structure, with all linear, normalization, attention, and residual operations replaced by their Lorentzian counterparts. Crucially, these operations preserve the same asymptotic complexity as their Euclidean analogues. Each Lorentz linear transformation (HLT) consists of a matrix multiplication followed by a constant number of scalar operations and a reprojection. This incurs O(nd2) time per layer, identical to a Euclidean linear layer up to constant factors. A5 HypRAG Hyperbolic self-attention computes pairwise geodesic distances between queries and keys. In the Lorentz model, each geodesic distance dK(qi,k j) is computed using a Lorentzian inner product, which costs O(d). Thus, attention score computation scales asO(n 2d)per layer, matching standard dot-product attention. The Lorentzian weighted midpoint used for value aggregation requires a weighted sum and normalization in Rd+1, contributingO(nd)time per token, and is dominated by the attention score computation. Overall, the time complexity of a single HyTE-FH layer is O(n2d+nd 2), and the total encoder complexity is O(L(n2d+ nd2)), which matches the asymptotic complexity of a standard Euclidean transformer. HyTE-H introduces an additional projection from Euclidean to hyperbolic space at the input, costing O(nd), which is negligible compared to the encoder cost. Pooling via Outward Einstein Midpoint.Given a sequence of n token embeddings, the Outward Einstein Midpoint computes radius-dependent weights, a weighted sum in Rd+1, and a single reprojection. This requires O(nd) time and O(d) memory, identical in order to standard mean pooling or the Einstein midpoint. Training Objectives.All training objectives operate on fixed-dimensional query and document embeddings. Geodesic similarity computation costsO(d)per queryâ€“document pair. Unsupervised contrastive pre-training with in-batch negatives of sizeN requires O(N 2d) per batch, matching the complexity of standard contrastive learning. Supervised contrastive fine-tuning has the same asymptotic cost. Masked language modeling introduces no additional asymptotic overhead beyond the encoder forward pass. Retrieval and RAG Inference.At inference time, dense retrieval over a corpus of size |D| requires computing hyperbolic distances between a query embedding and document embeddings, with total cost O(|D|d) . This matches Euclidean dense retrieval up to constant factors. Approximate nearest neighbor indexing can be applied without modification, as retrieval relies only on pairwise distance computations. In summary, the proposed hyperbolic dense retrieval system has the same asymptotic computational complexity as a Euclidean transformer-based retriever: O(L(n2d+nd 2)) for encoding, and O(|D|d) for retrieval. The additional cost of hyperbolic geometry manifests only as constant-factor overhead from Lorentzian inner products and reprojection, while enabling geometry-aware modeling of hierarchical structure. E. Additional results Full MTEB Results.Table A1 presents the performance of our proposed models on the MTEB benchmark across seven task types. All models share the same architecture with 149M parameters, 768 dimensions, and 12 layers. HyTE-H achieves the best overall performance with a Mean (Task) score of 59.89, outperforming both HyTE-FH and the Euclidean baselines. Notably, HyTE-H ranks first in six out of seven task categories, demonstrating the effectiveness of the hybrid hyperbolic-Euclidean approach. HyTE-FH shows competitive performance in clustering tasks, securing second place, which suggests that hyperbolic geometry is particularly beneficial for capturing hierarchical relationships. The Euclidean equivalent (ModernBert-embed*) achieves second place in most categories but falls short of HyTE-H, indicating that incorporating hyperbolic components provides meaningful improvements over purely Euclidean representations. Table A1.MTEB Benchmark Results. * Euclidean equivalent of HyTE Model Mean (Task) Mean (Type) Class. Clust. Retr. Rerank. STS Pair Class. Summ. EucBERT 54.11 51.31 69.07 35.31 37.01 35.18 75.02 80.02 27.62 ModernBert-embed* 58.32 55.59 69.25 40.87 43.28 44.29 78.00 83.8428.37 HyTE-HEuc 54.57 53.71 68.76 38.67 34.26 44.54 73.33 80.0236.45 HyTE-Hbert 56.41 53.75 68.77 41.95 40.54 42.05 74.39 79.70 28.87 HyTE-H59.89 57.15 72.71 44.83 43.56 46.01 78.3883.82 30.87 Similarity function.We evaluate two distance metrics for the contrastive loss in hyperbolic space: the Lorentz inner product and hyperbolic geodesic distance. While the Lorentz inner product provides a computationally convenient similarity A6 HypRAG measure, geodesic distance directly reflects intrinsic distances on the manifold. As shown in Table A2, using geodesic distance in the contrastive objective leads to improved performance across both mean task and mean task-type metrics, suggesting more effective alignment of representations in hyperbolic space. Table A2.Comparison of loss functions for hyperbolic embeddings. Both Lorentz inner product and geodesic distance are evaluated for their effectiveness in learning hierarchical representations. Loss Function Mean (Task) Mean (TaskType) Lorentz Inner Product 52.59 51.60 Geodesic Distance 56.41 53.75 Context size.Table A3 shows the effect of increasing the retrieval context size from |C|= 5 to |C|= 10 . The Euclidean baseline (Gemma) exhibits a large performance gain across all metrics with the larger context window, but still falls short of the fully hyperbolic HyTE-H model. HyTE-FH also improves with increased context, while HyTE-H remains comparatively stable, achieving consistently strong performance at both context sizes. This suggests that fully hyperbolic retrieval is less sensitive to context expansion and maintains effectiveness even under smaller retrieval budgets. Table A3.Average performance on RAG Bench with varying context window size. |C|= 5 |C|= 10 Model F CR AR F CR AR Gemma 0.603 0.735 0.684 0.756 0.846 0.836 HyTE-FH 0.732 0.848 0.765 0.770 0.912 0.784 HyTE-H 0.763 0.904 0.832 0.787 0.913 0.847 F = Faithfulness, CR = Context Relevance, AR = Answer Relevance. Best results in bold. E.1. Case Study: The Impact of Retrieval Geometry on Answer Quality To illustrate how embedding geometry affects end-to-end RAG performance, we analyze a representative query from the Emanuel dataset where models exhibit markedly different behaviors. Query.â€œWhat is the feature of Bixby guide?â€ This query requires retrieving documentation about a specific Samsung TV feature: the Bixby tutorial that appears when users first interact with the voice assistant. The correct answer is contained in a single passage within the hierarchically organized e-Manual. Quantitative Comparison.Table A4 presents the evaluation metrics for each model. Only HyTE-H successfully retrieves relevant context and generates a faithful response. Qualitative Analysis.We identify four distinct outcomes based on retrieval quality and LLM response behavior: (1) Successful Retrieval (HyTE-H).The hyperbolic hybrid model retrieves the exact passage describing the Bixby guide: â€œWhen you press the button for the first time, the Using Bixby button appears... a tutorial on using Bixby is shown. â€This enables a concise, accurate response: â€œThe feature of Bixby guide is a tutorial on using Bixby, which is shown when you press the button on your Samsung Smart Remote after the first time, and then press the Select button. â€ (2) Retrieval Collapse with Honest Refusal (GTE).GTE retrieves its default â€œhubâ€ documentsâ€”generic content about picture quality, SmartThings, and antenna connectionsâ€”regardless of the query. The LLM correctly identifies the context mismatch: â€œUnfortunately, the provided context does not mention the Bixby guide. â€ A7 HypRAG Table A4.Case study metrics for the queryâ€œWhat is the feature of Bixby guide?â€CR = Context Relevance, F = Faithfulness, AR = Answer Relevancy. Model CR F AR Failure Mode HyTE-H1.0 1.0 1.0None (Success) GTE 0.5 1.0 0.0 Honest Refusal Gemma 1.0 0.25 0.94 Ungrounded Extrapolation While this response is faithful (F=1.0), it provides no utility (AR=0.0). This pattern, where Euclidean embeddings collapse to retrieving the same generic documents, occurred for 100% of queries in GTEâ€™s results. (3) Partial Context with Topic Drift (Gemma).Gemma retrieves tangentially related content about the Samsung Smart Remote that mentions Bixby but not the Bixbyguide. The LLM provides information about running Bixby generally rather than the guide feature: â€œUnfortunately, the provided context does not mention the Bixby guide. However, based on general knowledge, the Bixby guide is a feature in Samsung TVs that provides users with a personalized interface to access various features and settings. It offers a simplified and intuitive way to navigate the TVâ€™s menu... â€ This response achieveshighanswer relevancy (AR=0.94) because it sounds plausible, butlowfaithfulness (F=0.25) because the claims are fabricated. This â€œconfident hallucinationâ€ failure mode is particularly dangerous: users receive authoritative-sounding misinformation. JSON Representation.The complete retrieval outputs are:HyTE-H (Successful Retrieval): { "query": "What is the feature of Bixby guide?", "model": "HyTE-H", "metrics": { "context_relevance": 1.0, "faithfulness": 1.0, "answer_relevancy": 1.0 }, "top_retrieved_context": "Running Bixby. Press and hold the button on your Samsung Smart Remote... To view the Bixby guide, press the button once: When you press the button for the first time, the Using Bixby button appears at the bottom of the screen. Press the Select button. The Using Bixby popup window appears and a tutorial on using Bixby is shown.", "response": "The feature of Bixby guide is a tutorial on using Bixby, which is shown when you press the button on your Samsung Smart Remote after the first time, and then press the Select button.", "outcome": "success" } GTE (Retrieval Collapse): { "query": "What is the feature of Bixby guide?", "model": "GTE", "metrics": { "context_relevance": 0.5, "faithfulness": 1.0, "answer_relevancy": 0.0 }, "top_retrieved_context": "Adjusting the Picture Quality. Change the Picture Mode and adjust Expert Settings.", "response": "Unfortunately, the provided context does not mention the Bixby guide.", "outcome": "retrieval_collapse_honest_refusal" } Gemma (Ungrounded Extrapolation): A8 HypRAG { "query": "What is the feature of Bixby guide?", "model": "Gemma", "metrics": { "context_relevance": 0.5, "faithfulness": 0.25, "answer_relevancy": 0.94 }, "top_retrieved_context": "Removing channels from a favorites list. Select channels in a favorites list, and then select the icon.", "response": "Unfortunately, the provided context does not mention the Bixby guide. However, based on general knowledge, the Bixby guide is a feature in Samsung TVs that provides users with a personalized interface to access various features and settings. It offers a simplified and intuitive way to navigate the TV's menu and access popular features.", "outcome": "ungrounded_extrapolation", "warning": "High answer_relevancy (0.94) masks unfaithful content (0.25)" } A9