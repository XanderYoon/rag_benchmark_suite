HeteRAG: A Heterogeneous Retrieval-augmented Generation Framework with Decoupled Knowledge Representations Peiru Yang1, Xintian Li 1, Zhiyang Hu 2, Jiapeng Wang3, Jinhua Yin1, Huili Wang1, Lizhi He4, Shuai Yang4, Shangguang Wang3, Y ongfeng Huang1, Tao Qi3,* 1Tsinghua University, 2Xinjiang University, 3Beijing University of Posts and Telecommunications, 4JD Health International Inc. Abstract Retrieval-augmented generation (RAG) meth- ods can enhance the performance of LLMs by incorporating retrieved knowledge chunks into the generation process. In general, the re- trieval and generation steps usually have differ- ent requirements for these knowledge chunks. The retrieval step benefits from comprehen- sive information to improve retrieval accuracy, whereas excessively long chunks may intro- duce redundant contextual information, thereby diminishing both the effectiveness and effi- ciency of the generation process. However, existing RAG methods typically employ iden- tical representations of knowledge chunks for both retrieval and generation, resulting in sub- optimal performance. In this paper, we pro- pose a heterogeneous RAG framework (Het- eRAG) that decouples the representations of knowledge chunks for retrieval and generation, thereby enhancing the LLMs in both effective- ness and efficiency. Specifically, we utilize short chunks to represent knowledge to adapt the generation step and utilize the correspond- ing chunk with its contextual information from multi-granular views to enhance retrieval accu- racy. We further introduce an adaptive prompt tuning method for the retrieval model to adapt the heterogeneous retrieval augmented gener- ation process. Extensive experiments demon- strate that HeteRAG achieves significant im- provements compared to baselines. 1 Introduction Retrieval Augmented Generation (RAG) technol- ogy is a powerful technique for building capable and reliable AI systems (Lewis et al., 2020). By in- corporating external knowledge chunks into LLMsâ€™ generation process, RAG enables more accurate responses and effectively mitigates the occurrence of hallucinations. RAG systems first segment the knowledge corpus into limited-size chunks, then Retrieval Embedding Model [Knowledge Chunk] Malaria is characterized by symptom clusters including ..., which exhibit both pathological overlaps and distinctive diagnostic indicators when compared to COVID-19. Generative Language Model Retrieval Embedding Model Generative Language Model [Knowledge Chunk] Three common clusters of symptoms have been identified: a respiratory symptom cluster â€¦; a musculoskeletal symptom cluster with â€¦; and a cluster of digestive symptoms â€¦ [Context] The manifestatio ns of COVID-19 vary depending on the type of variant contract- ed â€¦â€¦ [Context] Published data on the neuropath ological changes related with COVID-19 â€¦ â€¢ Wiki Page Title: COVID-19 â€¢ Section Title: Symptoms and Signs[Metadata] Naive RAG HeteRAG Generation Generation ğ‘„ğ‘„: What are the typical symptom clusters of COVID-19? Multi-Granular Contextual Signals COVID-19 Chunk Chunk Corpus Retrieved Chunks Retrieval (Highly Relevant)Retrieval (Less Relevant) Malaria Chunk Chunk Corpus Retrieved Chunks Figure 1: Naive RAG suffers retrieval inaccuracy due to identical chunk representations for retrieval/generation. The decoupled architecture of HeteRAG addresses this via contextual signal- and metadata-enhanced retrieval. retrieve relevant chunks by calculating the simi- larity between the user query and chunk encoded representations using the retrieval model. These re- trieved chunks are subsequently incorporated into the prompt for the LLM, allowing it to generate contextually informed responses. In typical RAG architectures, the retrieval and generation phases demonstrate distinct require- ments regarding knowledge chunk granularity. As interactive objects of the retriever, knowledge chunks are required to accurately match user queries to help the retrieval model find the most relevant information. Therefore, the retrieval step requires semantically complete information to en- sure retrieval accuracy. Conversely, excessively long chunks may introduce redundant or irrelevant information. This may potentially induce hallu- cinations in LLMs (Huang et al., 2023), thereby compromising the efficacy and efficiency of the generation process. Hence, knowledge chunks are expected to provide the most precise information to answer the userâ€™s questions. However, most exist- ing RAG methods employ identical representations of knowledge chunks for both retrieval and genera- 1 arXiv:2504.10529v1 [cs.IR] 12 Apr 2025 tion, and thus face challenges in jointly optimizing the performance of both stages caused by the identi- cal granularity of knowledge chunk representation. To address this problem, we propose HeteRAG, a heterogeneous RAG framework that decouples the representations of knowledge chunks for re- trieval and generation stages. As shown in Fig 1, we employ a context-enriched modeling strategy at retrieval side to integrate both multi-granular contextual signals and global structured metadata, enhancing the retrieval accuracy. Meanwhile, we utilize standalone knowledge chunks for the gen- eration process, enabling LLMs to generate with high efficiency and precision. This architecture facilitates joint optimization of both stages. Build- ing on this, we further propose an adaptive prompt tuning strategy that enables the retrieval model to dynamically align with our context-enriched mod- eling strategy. It facilitates the specialization of off-the-shelf embedding models, allowing them to effectively handle diverse, structurally complex real-world knowledge corpus. We conduct ex- tensive experiments on retrieval tasks and end-to- end RAG pipelines to evaluate the effectiveness of HeteRAG. Experimental results demonstrate that HeteRAG achieves significant improvements compared to baselines. The consistent gains in retrieval and QA accuracy confirm HeteRAG ef- fectively resolves the two-stage optimization con- flict, thereby enhancing the real-world applicabil- ity of RAG. Our codes are available at: https: //anonymous.4open.science/r/HeteRAG/. Our contributions can be summarized as follows: â€¢ We introduce a novel heterogeneous RAG framework that decouples knowledge repre- sentations for retrieval and generation step. â€¢ We design a prompt tuning strategy that adap- tively aligns pre-trained models with the het- erogeneous RAG process. â€¢ Extensive experiments on 3 knowledge bases, 5 datasets, 4 retrieval model, and 3 foundation models demonstrate that HeteRAG effectively outperforms baseline RAG methods. 2 Related Works 2.1 Retrieval Models Retrieval models aim to retrieve relevant informa- tion from a corpus based on queries. Modern ap- proaches predominantly employ transformer-based pre-trained embedding models for dense retrieval, a paradigm that learns latent space representations of queries and chunks through neural encoding. Recent progress features several impactful em- bedding models that demonstrate state-of-the-art performance across various benchmarks. The E5 family (Wang et al., 2022) train text embeddings in a contrastive manner using weak supervision from a large-scale text pair dataset. Jina Embed- dings (GÃ¼nther et al., 2023) focus on long text input and extend token limits, effectively handling long documents without the need for truncation or paragraph splitting. BGE embedding family (Xiao et al., 2024; Chen et al., 2024a) is a versatile em- bedding model trained through multi-stages that exhibits highly competitive performance in multi- lingual and cross-lingual retrieval tasks. These ver- satile embedding models are capable of uniformly supporting a variety of tasks, providing support for multiple applications, including RAG. Note that our work is orthogonal to these embedding models; it can be implemented in any embedding model to enhance their performance in retrieval tasks. 2.2 Retrieval Augmented Generation Since the RAG framework was first proposed (Lewis et al., 2020; Guu et al., 2020), it has be- come an important supporting technology in the real-world applications of LLMs. By providing re- liable and up-to-date external knowledge to LLMs, RAG effectively enhances their generation perfor- mance. In recent years, many works have improved the retrieval stage of RAG through various opti- mization methods. Yu et al. (2023) introduce an augmentation-adapted retriever which is trained to learn unseen LLMsâ€™ preferences from a known source language model. Shi et al. (2024) append re- trieved documents to the input of a frozen language model, differentiating itself from previous methods that train language models to adapt to retrievers. Overall, these works still employ identical repre- sentations of knowledge chunks for both retrieval and generation stages. Some works have decoupled retrieval and gen- eration representations to a certain extent. For ex- ample, late chunking method (GÃ¼nther et al., 2024) utilizes long context embedding models to first em- bed all tokens before applying chunking, resulting in chunk embeddings that preserve full contextual information and improve performance on retrieval tasks. However, the effectiveness of late chunk- ing is limited when using conventional embedding 2 Retrieval Embedding Model â„› Generative Language Model ğ’¢ğ’¢ Identical Representations for ğ“¡ğ“¡and ğ“–ğ“– ğ‘„ğ‘„ Decoupling Representations for ğ“¡ğ“¡and ğ“–ğ“– ğœ™ğœ™(ğªğª, ğğğ‘–ğ‘– (ğ‘—ğ‘—)ï¼‰ ğªğª ğ¶ğ¶ğ‘–ğ‘– (ğ‘—ğ‘—): ğ‘–ğ‘–-th Chunk of Doc. ğ‘—ğ‘— Retrieval Embedding Model â„› Generative Language Model ğ’¢ğ’¢ ğ‘„ğ‘„ ğğğ‘–ğ‘– (ğ‘—ğ‘—) ğªğª ğœ™ğœ™(ğªğª, ğğğ‘–ğ‘– (ğ‘—ğ‘—) ï¼‰ ğ¶ğ¶1 (ğ‘—ğ‘—) ğ¶ğ¶ğ‘ğ‘ğ‘—ğ‘— (ğ‘—ğ‘—) ğ·ğ·ğ‘—ğ‘—: ğ‘—ğ‘—-th Doc. in Corpus ğ¶ğ¶ğ‘–ğ‘– (ğ‘—ğ‘—) â€¦â€¦ Multi-Granular Contextual Signals ğœ“ğœ“ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘{ğ¶ğ¶ğ‘–ğ‘–ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘—ğ‘—} Global Structured Metadata of ğ·ğ·ğ‘—ğ‘— ğ·ğ·ğ‘—ğ‘—: Title Sub-title 1 Sub-title ğ‘–ğ‘–ğ‘†ğ‘† Sub-title ğ‘ğ‘ğ‘†ğ‘†â€¦ â€¦ Paragraph 1 Paragraph ğ‘–ğ‘–ğ‘ƒğ‘ƒ Paragraph ğ‘ğ‘ğ‘ƒğ‘ƒâ€¦ â€¦ ğœ“ğœ“ğ‘šğ‘šğ‘šğ‘šğ‘ğ‘ğ‘šğ‘šğ‘€ğ‘€ğ‘–ğ‘– ğ‘—ğ‘— ğ¶ğ¶t (ğ‘—ğ‘—) ğ¶ğ¶s (ğ‘—ğ‘—) {ğ¶ğ¶ğ‘–ğ‘–Â±ğ‘˜ğ‘˜ (ğ‘—ğ‘—) } ğğğ‘–ğ‘– (ğ‘—ğ‘—) = â„› ğœ“ğœ“ğ¶ğ¶ğ‘–ğ‘– ğ‘—ğ‘—âŠ• ğœ“ğœ“ğ‘šğ‘šğ‘šğ‘šğ‘ğ‘ğ‘šğ‘šğ‘€ğ‘€ğ‘–ğ‘– ğ‘—ğ‘—âŠ• ğœ“ğœ“ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘{ğ¶ğ¶ğ‘–ğ‘–ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘—ğ‘—}ğğğ‘–ğ‘– (ğ‘—ğ‘—) = â„›(ğ¶ğ¶ğ‘–ğ‘– (ğ‘—ğ‘—)) Chunking Figure 2: The overall framework of HeteRAG. The left shows naive RAG using identical representations of knowledge chunks for retrieval and generation. The right depicts HeteRAGâ€™s framework: retrieval incorporates global metadata and multi-granular context, while generation maintains standalone chunk usage. models or in scenarios involving extremely long documents. Raina and Gales (2024) introduce a zero-shot adaptation of dense retrieval by decom- posing chunks into atomic statements and generat- ing synthetic questions for improved chunk recall. Chen et al. (2024b) propose a multi-document QA framework with cascading metadata integration and multi-route retrieval for multi-document environ- ments. Anthropic (2024) present a method for gen- erating contextualized chunk embeddings by using a large language model (LLM) to augment chunk text with relevant context from the entire document before embedding. These works might face latency caused by the generation of KG or summaries by large models, which limits their effectiveness in online settings and with larger corpora. 3 Methods In this section, we first give a problem formulation of retrieval and generation process of RAG. Then we elaborate on HeteRAG framework in detail. 3.1 Problem Formulation Given a document corpus {D1, ..., DM } and user query Q, an RAG system operates through three coordinated phases: document chunking, dense vector retrieval, and conditional generation. A chunking strategy is used to first decompose each document Dj into text chunks through a chunk- ing strategy C: {C(j) 1 , ..., C(j) Nj } = C(Dj) âˆ€j âˆˆ {1, ..., M}. C(j) i denotes the i-th chunk from docu- ment Dj, resulting in a global chunk collectionSM j=1{C(j) 1 , ..., C(j) Nj }. Then the retriever R en- codes both the query and all chunks into a shared embedding d-dimensional vector space: q = R(Q), e(j) i = R(C(j) i ) (1) The system computes pairwise similarity scores Ï•(q, e(j) i ) between the query embedding and chunk embeddings, typically implemented as cosine simi- larity. The top-k most relevant chunks are passed to LLM G to generate the final response. 3.2 Knolwedge Representation Decoupling Our HeteRAG framework addresses the represen- tation dilemma by decoupling the representations of knowledge chunks for retrieval and generation. As illustrated in Fig 2, the architecture establishes dual pathways for retrieval-oriented and generation- oriented knowledge chunks, enabling specialized optimization for each stage. After chunking the document corpus {Dj}M j=1 into the global chunk collectionSM j=1{C(j) 1 , ..., C(j) Nj }, we model the re- trieval and generation stages separately. The retrieval side aims to precisely align user queries with relevant documents, necessitating comprehensive information from the retrieval side to sufficiently model and compute semantic sim- ilarity. RAG systems across different tasks and domains typically operate on heterogeneous cor- pora with distinct structural characteristics. For in- stance, corpora may exhibit hierarchical tree struc- tures (e.g., Wikipedia articles with nested sections), linear sequences (e.g., news articles with temporal dependencies), or graph-based organizations (e.g., knowledge bases with entity-relation networks). To effectively leverage such diversity, multi-granular information integration at the retrieval side be- comes crucial â€“ this typically encompasses raw knowledge chunks, multi-granular contextual sig- nals, and global structured metadata, each con- tributing complementary perspectives for robust retrieval. For a knowledge chunk C(j) i in document Dj, We formulate the modeling procedure at the 3 retrieval side as follows: e(j) i = R h Ïˆ(C(j) i ) âŠ•Ïˆctx({C(j) ictx}) âŠ•Ïˆmeta(M(j) i ) i (2) where M (j) i represents the global metadata of C(j) i in Dj, including but not limited to sub- ject, abstract, document title, section title, subsec- tion title, related keywords, etc. And {C(j) ictx} = {C(j) t , C(j) s , {C(j) iÂ±k}} represents the multi-granular contextual signals, which can provide the retrieval model with contextual information at different lev- els. Ïˆ(Â·), Ïˆctx(Â·), and Ïˆmeta(Â·) are the semantic encoders for different components, and âŠ• denotes the fusion operation. The generation side aims to keep the representa- tion of the knowledge chunk as concise as possible for the sake of efficiency and precision, avoiding re- dundant or unnecessary information. Therefore, in contrast to the retrieval side, we only provide C(j) i itself to the generative model on the generation side to maintain task-specific precision: Ans = G  T (Q, C(j) i )  (3) Where T (Â·, Â·) refers to the prompt template accus- tomed to the generative language model G. In this way, the representations are decoupled between retrieval and generation. 3.3 Adaptive Prompt tuning Strategy In many cases where RAG systems are applied to specific domains, the retrieval embedding model is fine-tuned to adapt to the corresponding domain. To specialize the retrieval model for heterogeneous document structures, we introduce an adaptive fine- tune strategy. Prompt tuning (Lester et al., 2021) has been widely adopted in various fields, as it uses task- specific instructions to improve performance on targeted tasks. To enable the retrieval model to better leverage contextual signals and structured metadata, as well as to adapt to the characteris- tics of different corpora, we propose a fine-tuning strategy based on prompt tuning. Specifically, we prepend instructions to different information units of a certain chunk. For a chunk C(j) i with contex- tual signals {C(j) iÂ±k} and global metadata M (j) i , we formulate the instruction input as: ËœCh = [INSTh] âŠ• C (4) where [INSTh] denotes the instruction embedding specific to hierarchy level h, implemented as soft prompts through continuous token vectors. The retrieval model R then encodes both the original query Q and prompted chunks { ËœCh} into an adap- tive embedding space: q = R(Q), Ëœeh = R( ËœCh) (5) Following the conventional paradigm of con- trastive learning, we construct positive and negative samples. Given a user query Q, the positive pair (Q, C+) is directly derived from human-annotated relevance data. For negative pairs (Q, Câˆ’), both in-batch negatives {Câˆ’ j }jÌ¸=iand random negatives Câˆ’ rand are employed for training. Similarity between Q and C is measured by scaled cosine similarity: Ï•(Q, C) = qâŠ¤Ëœeh âˆ¥qâˆ¥âˆ¥Ëœehâˆ¥ Â· Ï„ âˆ’1 (6) where Ï„ denotes the temperature hyperparameter controlling the softness of the similarity distribu- tion. The model is trained using an InfoNCE loss (Oord et al., 2018): L=âˆ’1 N NX i=1 log es(Qi,C+ i ) PN j=1es(Qi,C+ j ) +PK k=1es(Qi,Câˆ’ k ) (7) 4 Experiments and Analysis 4.1 Experimental Datasets and Settings We utilize three information retrieval datasets for evaluation in the BEIR benchmark (Thakur et al., 2021). SciFact (Wadden et al., 2020) pro- vides expert-written scientific claims with evidence- annotated research abstracts for claim verifica- tion. NF-Corpus (Boteva et al., 2016) focuses on medical information retrieval, while Trev-COVID (V oorhees et al., 2021) specializes in COVID-19- related retrieval. Three widely-used embedding models are employed: E5-base-v2 (Wang et al., 2022), BGE-base-en-v1.5 (Xiao et al., 2024), and Jina-embeddings-v2-small (GÃ¼nther et al., 2023). We also utilize a specialized embedding model MedEmbed-small-v0.1 (Balachandran, 2024) for medical and clinical corpus. Among them, Jina is a long text embedding model with a capacity of 8192 tokens, while both E5, BGE, and MedEmb are regular models with a capacity of 512 tokens. We conducted our end-to-end RAG experiments on five widely-used datasets: PopQA (Mallen et al., 2023) is a curated question set from diverse on- line platforms. NQ dataset (Kwiatkowski et al., 4 DatasetEmbModelMethod chunk size=16 chunk size=32 chunk size=64 chunk size=128 nDCG@1 nDCG@10nDCG@1 nDCG@10nDCG@1 nDCG@10nDCG@1 nDCG@10 SciFact Jina Naive 45.33% 58.74% 53.33% 64.23% 56.00% 66.29% 53.00% 64.79% Late 55.00% 66.63% 55.33% 66.86% 54.00% 66.05% 54.67% 66.12% HeteRAG58.67% 68.90% 57.33% 68.51% 57.00% 67.83% 56.00% 67.50% BGE Naive 53.67% 66.76% 57.33% 69.68% 59.00% 70.87% 61.33% 73.09% Late 60.00% 72.10% 59.33% 71.91% 59.00% 71.70% 59.33% 71.94% HeteRAG63.00% 74.54% 64.33% 75.89% 64.00% 75.54% 60.33% 73.49% E5 Naive 44.00% 58.53% 52.33% 64.03% 51.33% 63.75% 47.67% 58.90% Late 53.00% 66.79% 53.67% 66.77% 53.00% 66.79% 52.67% 66.56% HeteRAG60.33% 71.74% 60.00% 71.04% 58.67% 70.16% 52.67% 66.82% MedEmb Naive 50.33% 62.94% 56.00% 66.80% 56.00% 68.41% 58.33% 69.85% Late 57.33% 68.96% 57.33% 69.03% 57.67% 68.82% 57.00% 68.47% HeteRAG66.59% 71.18% 62.00% 72.31% 61.33% 72.15% 60.00% 71.70% NF-Corpus Jina Naive 32.51% 25.24% 29.10% 24.00% 31.27% 24.40% 30.96% 24.01% Late 40.56% 31.20% 41.33% 30.84% 39.94% 30.73% 39.47% 30.33% HeteRAG41.95% 31.98% 43.65% 32.07% 40.25% 30.92% 39.78% 29.81% BGE Naive 41.95% 33.49% 43.34% 34.68% 44.12% 35.16% 41.64% 35.47% Late 46.29% 36.68% 45.98% 36.60% 45.67% 36.46% 44.89% 36.41% HeteRAG48.45% 37.65% 49.38% 37.66% 47.52% 37.65% 46.75% 37.01% E5 Naive 39.63% 30.72% 32.97% 29.42% 32.51% 28.50% 32.35% 26.08% Late 39.01% 31.15% 38.70% 30.93% 37.31% 30.69% 35.91% 30.17% HeteRAG43.81% 36.07% 44.58% 35.49% 44.89% 35.84% 43.34% 34.86% MedEmb Naive 44.12% 33.44% 43.50% 33.25% 43.96% 33.03% 41.33% 32.55% Late 43.19% 34.51% 42.42% 34.39% 41.49% 34.26% 41.02% 33.82% HeteRAG46.59% 35.62% 47.37% 35.83% 43.96% 35.18% 43.65% 34.94% Trec-COVID Jina Naive 56.00% 51.82% 55.00% 52.82% 58.00% 60.55% 65.00% 64.16% Late 74.00% 66.91% 65.00% 66.23% 73.00% 67.66% 77.00% 67.12% HeteRAG73.00% 69.26% 72.00% 69.79% 77.00% 71.77% 81.00% 70.31% BGE Naive 68.00% 62.60% 66.00% 62.37% 65.00% 65.06% 66.00% 67.07% Late 70.00% 64.93% 67.00% 46.30% 73.00% 70.01% 69.00% 67.62% HeteRAG78.00% 76.60% 86.00% 75.33% 87.00% 77.30% 82.00% 75.97% E5 Naive 67.00% 57.03% 63.00% 54.75% 58.00% 51.62% 55.00% 51.66% Late 57.00% 46.50% 61.00% 34.16% 59.00% 49.70% 60.00% 51.28% HeteRAG69.00% 63.23% 68.00% 61.99% 56.00% 57.32% 55.00% 54.96% MedEmb Naive 57.00% 60.77% 67.00% 65.51% 67.00% 67.03% 75.00% 72.14% Late 73.00% 66.19% 75.00% 46.41% 72.00% 65.37% 76.00% 67.32% HeteRAG79.00% 74.58% 81.00% 76.17% 87.00% 79.47% 83.00% 78.81% Table 1: Evaluation of different chunk representation methods on retrieval tasks. HeteRAG significantly improves retrieval accuracy in the majority of settings. 2019) is a collection of real user queries paired with Wikipedia passages. SQuAD (Rajpurkar et al., 2018) is a widely-used benchmark dataset for machine comprehension, consisting of questions on a set of Wikipedia articles. TriviaQA (Joshi et al., 2017) contains 95K trivia-based QA pairs, while HotpotQA (Yang et al., 2018) offers 113K Wikipedia QA pairs for multi-hop reasoning chal- lenges. We used three state-of-the-art open-source LLMs as generative models: Llama3-8b-Instruct (Dubey et al., 2024), Mistral-8B-Instruct (Jiang et al., 2024), and Gemma-9b-Instruct (Team et al., 2024). The end-to-end RAG code implementation refers to Jin et al. (2024). Next, we introduce the experimental settings. For retrieval experiments, we use commonly used ranking metrics ndcg@1 and ndcg@10. For the adaptive tuning process, we conducted fine-tuning using the training partition of the SciFact dataset, followed by performance evaluation on the desig- nated test partition. For generation experiments, we use commonly used metrics in QA systems, namely EM (Exact Match) and token-level F1. The token-level F1 metric refers to the harmonic mean of token-level precision and recall, calculated by comparing shared tokens between the response and golden answer. In the retrieval corpus, we choose the widely-used Wiki2018 corpus, which is com- patible with the five QA datasets used in the experi- ment. To streamline the experiments, we select the first 1,000 samples from the test or development set of all QA datasets. For vector database index 5 Dataset Emb Method chunk size=16 chunk size=32 chunk size=64 chunk size=128 ndcg@1 ndcg@10 ndcg@1 ndcg@10 ndcg@1 ndcg@10 ndcg@1 ndcg@10 SciFact Jina Naive 47.33% 61.87% 52.67% 64.70% 51.00% 66.06% 51.33% 65.53% Late 55.67% 70.89% 56.00% 70.70% 56.33% 70.33% 54.33% 69.76% HeteRAG 56.67% 70.21% 58.00% 71.98% 59.00% 72.01% 60.00% 71.87% BGE Naive 49.00% 65.15% 58.67% 70.78% 62.67% 73.91% 63.33% 74.93% Late 61.33% 73.40% 62.00% 73.67% 61.67% 73.45% 61.00% 73.22% HeteRAG 64.67% 77.11% 65.33% 77.34% 65.00% 77.59% 65.00% 77.46% e5 Naive 47.67% 63.05% 53.00% 68.01% 55.33% 70.40% 58.00% 71.66% Late 56.67% 70.11% 56.00% 69.65% 55.33% 69.41% 55.00% 69.12% HeteRAG 59.33% 73.06% 61.67% 74.61% 63.00% 74.54% 62.67% 74.85% Table 2: Evaluation of our proposed adaptive fine-tune strategy on retrieval tasks. While fine-tuning generally enhances retrieval task performance, HeteRAG still achieves superior results compared to the fine-tuned baselines. building, we employ the Faiss library (Douze et al., 2024). All experiments were conducted on four RTX 5000 GPUs. 4.2 Performance Evaluation We conduct comprehensive experiments to evaluate the effectiveness of HeteRAG on the BeIR bench- mark, comparing against two baseline retrieval methods: naive RAG and late chunking. Late chunking method (GÃ¼nther et al., 2024) embeds all tokens in a document before applying chunk- ing with a long text embedding model, to preserve full contextual information and improve retrieval performance. For the Jina model, since it is specifi- cally designed for long texts, late chunking can be applied directly. For the other two models, a variant called long late chunking is used, which employs a sliding window approach to concatenate embed- dings. Table 1 presents the retrieval performance across three representative datasets (SciFact for sci- entific claims, nfCorpus for medical information, and TREC COVID for COVID-19-related articles) using three embedding models with distinct archi- tectures: Jina-v2 (long-text optimized), E5-v2, and BGE-v1.5 (both standard-length models). From the experimental results, we made the fol- lowing observations: First, HeteRAG consistently outperforms baseline methods in almost all cases. Our method achieves average improvements of 9.43% (nDCG@1) and 7.76% (nDCG@10) over naive RAG across all datasets and models, with par- ticularly notable gains on TrecCOVID (+11.73% nDCG@10). While the absolute performance of all three embedding models varies due to their inher- ent capacity differences, HeteRAG maintains sta- ble relative advantages regardless of the backbone model, suggesting effective decoupling of knowl- edge chunk modeling strategy from fundamental capabilities of embedding model. This may be be- cause of the context-enriched strategy of HeteRAG on the retrieval side successfully models more com- prehensive and rich information, thereby increasing recall accuracy. Second, the late chunking method shows better performance on long text embedding models (Jina-v2) compared to naive RAG; however, on regular embedding models (E5-v2 and BGE- v1.5), the performance of the late chunking method declines. We attribute this to the mismatch be- tween the full-document encoding of late chunking (which Jina-v2 natively supports) and the sequence length constraints of regular models. Furthermore, the late chunking method only applies to embed- ding models that use mean pooling and performs poorly on CLS-pooling models. In contrast, Het- eRAG achieves better model-agnostic robustness. Third, varying chunk sizes from 16 to 128 tokens cause fluctuations in the performance of naive RAG. Overall, smaller chunk sizes lead to lower retrieval performance due to the reduced amount of infor- mation. Late chunking is less affected by chunk size due to its global modeling characteristics. Het- eRAG also demonstrates strong stability through its multi-granular retrieval side modeling. In other words, HeteRAG can effectively adapt to differ- ent chunking sizes and strategies corresponding to various corpora. These findings collectively vali- date advantages of HeteRAG in cross-domain gen- eralization, model compatibility, and operational robustness for real-world retrieval scenarios. 4.3 Evaluation on Adaptive Prompt Tuning As demonstrated in Table 2, the experimental re- sults validate the effectiveness of the fine-tuning strategy described in Section 3.3 for HeteRAG. We 6 Model Dataset w/o RAG Naive RAG HeteRAG EM F1 Recall EM F1 Recall EM F1 Recall Llama3-8b PopQA 18.70% 22.96% 25.80% 24.00% 39.75% 58.66% 32.70% 52.25% 76.19% HotpotQA 19.70% 28.03% 28.06% 21.70% 30.56% 32.53% 30.80% 42.48% 43.32% TriviaQA 51.60% 58.94% 60.47% 52.40% 61.04% 63.65% 58.70% 68.56% 71.87% Squad 20.40% 27.09% 28.48% 28.90% 36.49% 40.11% 32.60% 40.34% 44.17% NQ 22.40% 32.61% 37.45% 29.80% 40.25% 47.01% 36.10% 48.24% 57.46% Mistral-8b PopQA 20.10% 22.51% 22.69% 32.70% 45.77% 58.94% 46.20% 61.40% 76.04% HotpotQA 18.60% 26.63% 26.30% 26.80% 37.21% 36.96% 36.60% 47.99% 47.91% TriviaQA 47.30% 53.90% 54.67% 55.40% 63.51% 64.87% 61.30% 69.53% 71.48% Squad 15.50% 21.75% 22.78% 33.30% 40.40% 42.52% 37.20% 44.24% 46.35% NQ 17.00% 24.68% 27.91% 33.00% 42.38% 47.22% 40.20% 51.54% 56.69% gemma-9b PopQA 15.00% 16.20% 16.40% 38.60% 48.16% 58.58% 52.00% 63.27% 75.51% HotpotQA 16.70% 24.39% 23.85% 25.10% 33.74% 33.07% 34.90% 45.40% 44.67% TriviaQA 52.40% 58.01% 58.09% 58.10% 64.79% 65.41% 63.60% 71.31% 72.22% Squad 16.30% 21.23% 22.03% 34.50% 39.63% 40.67% 37.90% 43.36% 44.61% NQ 21.60% 31.02% 32.70% 33.20% 42.72% 45.71% 39.80% 50.34% 54.22% Table 3: The performance evaluation of different methods on five datasets and three LLMs. Across all datasets and models, HeteRAG demonstrates higher QA accuracy on all evaluation metrics. Size=16 Size=32 Size=64 Size=12855 59 63 67nDCG@10 (SciFact) Jina Size=16 Size=32 Size=64 Size=12855 60 65 70 75 BGE Size=16 Size=32 Size=64 Size=12850 55 60 65 70 MedEmb Size=16 Size=32 Size=64 Size=12850 55 60 65 70nDCG@10 (TrecCOVID) Size=16 Size=32 Size=64 Size=12850 56 62 68 74 Size=16 Size=32 Size=64 Size=12850 56 62 68 74 chunk chunk + contextual signals chunk + metadata HeteRAG Figure 3: Effect of contextual signals and structured metadata in HeteRAG framework. The ablation results show that both contribute significantly to the retrieval performance of HeteRAG. implement contrastive learning-based fine-tuning for fair comparison on baseline methods follow- ing the same protocol. Fine-tuned variants consis- tently outperform their non-fine-tuned counterparts across all datasets. When trained with identical op- timization steps, HeteRAG achieves superior per- formance compared to fine-tuned baseline meth- ods, confirming the benefits of our proposed fine- tuning strategy. These findings demonstrate that HeteRAG maintains compatibility with standard embedding model fine-tuning strategies, exhibiting strong adaptation capabilities. 4.4 End-to-End RAG Performance The experimental results of our end-to-end RAG framework, as shown in Table 3, demonstrate con- sistent performance improvements across three generative language models (Llama3-8b-Instruct, Mistral-8B-Instruct, and Gemma-9b-Instruct) and five benchmark datasets (NQ, PopQA, SQuAD, TriviaQA, and HotpotQA). The table presents the results of retrieval top-5 knowledge chunks from the Wiki corpus. HeteRAG significantly outper- forms other baseline methods across all models and datasets. These gains might be attributed to Wikipediaâ€™s inherent tree-like hierarchical struc- ture, which enables HeteRAG to holistically model document-level dependencies as metadata. 4.5 Ablation Study We evaluate the effectiveness of contextual signals and structured metadata through ablation studies by 7 1 3 5 10 15 T op K Value 25 30 35 40 45F1 Score (%) Naive RAG (Llama3-8b) HeteRAG (Llama3-8b) Naive RAG (Mistral-8b) HeteRAG (Mistral-8b) Naive RAG (Gemma-9b) HeteRAG (Gemma-9b) PopQA HotpootQA TriviaQA Squad NQ0 20 40 60F1 Score(%) T opK=1 T opK=3 T opK=5 T opK=10 T opK=15 Figure 4: The RAG results under varying retrieval numbers (top-k). The left side shows the results of three LLMs on the HotpotQA dataset as they vary with top-k, using both naive RAG and HeteRAG. The right side displays the performance variation of HeteRAG across five different datasets under various top-k settings. removing the corresponding representations from the retrieval formulation in Eq 2. Ablation results are visualized in Fig 3, from which we have sev- eral findings. First, the complete HeteRAG frame- work (with both contextual signals and structured metadata) consistently outperforms its variant us- ing only the representations of knowledge chunks themselves. This demonstrates that explicitly mod- eling document-level, multi-granular context and metadata strengthens retrieval-side semantics, par- ticularly enhancing recall capability through com- plementary information fusion. Second, the rela- tive importance of these components varies across domains: contextual signals contribute more to per- formance gain on SciFact, while document-level metadata is more useful for TrevCOVID and NF- Corpus datasets. The results of our ablation study confirm that HeteRAG â€™s multi-channel encoding effectively leverages both latent contextual patterns and explicit structural knowledge. 4.6 Top- k Retrieval Analysis Fig. 4 presents the experimental results under vary- ing Top-k retrieval settings, from which we draw the following observations. First, the left panel of Fig. 4 demonstrates the performance trajectories of different models and methods on the same dataset as k increases. We evaluate several commonly used k values in RAG systems (1, 3, 5, 10, 15). The re- sults reveal that compared to naive RAG, our Het- eRAG maintains consistent performance improve- ments across all k values. Furthermore, naive RAG exhibits noticeable performance degradation with larger k values, likely due to excessive redundant information in retrieved content. Second, the right panel of Fig. 4 illustrates the performance variation of HeteRAG across different datasets. Notably, our method demonstrates positive correlation between larger k values and improved answer F1 scores on most datasets. These experimental results indicate that HeteRAG effectively balances comprehensive retrieval with generation efficiency and accuracy, successfully mitigating the common performance deterioration issue observed in baseline methods when processing larger retrieval sets. 5 Conclusion In this paper, we identify a limitation in existing RAG methods: the use of identical knowledge chunk representations for both retrieval and genera- tion, despite their distinct requirements. To address this, we propose HeteRAG, a heterogeneous RAG framework that decouples knowledge representa- tions to optimize retrieval accuracy as well as gen- eration efficiency and efficacy simultaneously. By leveraging multi-granular contextual signals and metadata for retrieval and concise chunks for gen- eration, our approach mitigates redundancy while preserving critical knowledge. Furthermore, we propose an adaptive prompt-tuning strategy for the retrieval model to adapt the heterogeneous retrieval augmented generation process. Extensive experi- ments across retrieval tasks and end-to-end genera- tion pipelines validate that HeteRAG significantly outperforms baseline methods. These results high- light the importance of tailoring knowledge repre- sentations to the unique demands of retrieval and generation steps. In general, this work provides a principled direction for advancing RAG systems by harmonizing the dual objectives of retrieval pre- cision and generation quality. 8 Limitations While HeteRAG demonstrates promising results, this work has two main limitations that suggest di- rections for future research. First, the experimental validation currently focuses on several widely-used benchmark datasets from selected domains. Al- though these datasets represent important applica- tion areas for RAG systems, our findings may not fully generalize to emerging domains with distinct knowledge characteristics. Future work should validate the of HeteRAG across more diverse do- mains and emerging application contexts. Second, our framework primarily focuses on optimizing the retrieval side knowledge chunk representations, while employing relatively straightforward repre- sention for generation side. Prompt token compres- sion techniques could potentially better preserve critical information while further improving genera- tion efficiency. This presents a promising direction for subsequent research to enhance the generation- side optimization while maintaining the decoupling paradigm of our framework. We exclusively uti- lize generative AI to refine the writing and verify grammatical accuracy in this paper. References Anthropic. 2024. Introducing contextual retrieval. Ac- cessed: 2025-02-01. Abhinand Balachandran. 2024. Medembed: Medical- focused embedding models. Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. 2016. A full-text learning to rank dataset for medical information retrieval. In Ad- vances in Information Retrieval: 38th European Con- ference on IR Research, ECIR 2016, Padua, Italy, March 20â€“23, 2016. Proceedings 38, pages 716â€“722. Springer. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024a. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216. Xinyue Chen, Pengyu Gao, Jiangjiang Song, and Xi- aoyang Tan. 2024b. Hiqa: A hierarchical contextual augmentation rag for massive documents qa. arXiv preprint arXiv:2402.01767. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel MazarÃ©, Maria Lomeli, Lucas Hosseini, and HervÃ© JÃ©gou. 2024. The faiss library. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Michael GÃ¼nther, Isabelle Mohr, Daniel James Williams, Bo Wang, and Han Xiao. 2024. Late chunking: con- textual chunk embeddings using long-context embed- ding models. arXiv preprint arXiv:2409.04701. Michael GÃ¼nther, Jackmin Ong, Isabelle Mohr, Alaed- dine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, et al. 2023. Jina embeddings 2: 8192-token general-purpose text embeddings for long documents. arXiv preprint arXiv:2310.19923. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International confer- ence on machine learning , pages 3929â€“3938. PMLR. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bam- ford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv e-prints, pages arXivâ€“2401. Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. 2024. Flashrag: A modular toolkit for efficient retrieval-augmented generation research. CoRR, abs/2405.13576. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehen- sion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (V ol- ume 1: Long Papers), pages 1601â€“1611. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken- ton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:453â€“ 466. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 3045â€“3059. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim Rock- tÃ¤schel, et al. 2020. Retrieval-augmented generation 9 for knowledge-intensive nlp tasks. Advances in Neu- ral Information Processing Systems, 33:9459â€“9474. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric mem- ories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (V ol- ume 1: Long Papers), pages 9802â€“9822. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. Vatsal Raina and Mark Gales. 2024. Question-based retrieval using atomic units for enterprise rag. arXiv preprint arXiv:2405.12363. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you donâ€™t know: Unanswerable ques- tions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (V olume 2: Short Papers), pages 784â€“789. Weijia Shi, Sewon Min, Michihiro Yasunaga, Min- joon Seo, Richard James, Mike Lewis, Luke Zettle- moyer, and Wen-tau Yih. 2024. Replug: Retrieval- augmented black-box language models. In Proceed- ings of the 2024 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies (V olume 1: Long Papers), pages 8364â€“8377. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane RiviÃ¨re, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295. Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Ab- hishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous benchmark for zero-shot evalua- tion of information retrieval models. arXiv preprint arXiv:2104.08663. Ellen V oorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021. Trec-covid: constructing a pandemic information re- trieval test collection. In ACM SIGIR F orum, vol- ume 54, pages 1â€“12. ACM New York, NY , USA. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534â€“7550. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly- supervised contrastive pre-training. arXiv preprint arXiv:2212.03533. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen- nighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th international ACM SIGIR conference on research and development in informa- tion retrieval, pages 641â€“649. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo- pher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing , pages 2369â€“2380. Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. 2023. Augmentation-adapted retriever improves gen- eralization of language models as generic plug-in. arXiv preprint arXiv:2305.17331. 10