Identifying Origins of Place Names via Retrieval Augmented Generation Alexis HORDE VO a,1, Matt DUCKHAM a, Estrid HE a and Rafe BENLI b a School of Computing Technologies, RMIT University, Melbourne, Australia b Geographic Names Victoria, Victoria State Government, Australia ORCiD ID: Alexis HORDE VO https://orcid.org/0009-0004-3324-3380, Matt DUCKHAM https://orcid.org/0000-0002-7249-6709, Estrid HE https://orcid.org/0000-0002-8994-9532 Abstract. Who is the Batman behind ‚ÄúBatman Street‚Äù in Melbourne? Understanding the historical, cultural, and societal narratives behind place names can reveal the rich context that has shaped a community. Although place names serve as essential spatial references in gazetteers, they often lack information about place name origins. Enriching these place names in today‚Äôs gazetteers is a time- consuming, manual process that requires extensive exploration of a vast archive of documents and text sources. Recent advances in natural language processing and language models (LMs) hold the promise of significant automation of identifying place name origins due to their powerful capability to exploit the semantics of the stored documents. This chapter presents a retrieval augmented generation pipeline designed to search for place name origins over a broad knowledge base, DBpedia. Given a spatial query, our approach first extracts sub-graphs that may contain knowledge relevant to the query; then ranks the extracted sub-graphs to generate the final answer to the query using fine-tuned LM-based models (i.e., ColBERTv2 and Llama2). Our results highlight the key challenges facing automated retrieval of place name origins, especially the tendency of language models to under-use the spatial information contained in texts as a discriminating factor. Our approach also frames the wider implications for geographic information retrieval using retrieval augmented generation. Keywords. geographic information retrieval, open domain question answering, retrieval augmented generation, place name, gazetteer 1. Introduction Although place names are officially recorded by place naming authorities around the world, gazetteers commonly lack information regarding place name origins. To understand the history behind place names, one must typically consult external sources, such as historical archives or web pages. Understanding that history is becoming increasingly important to communities, for example, in better reflecting the contributions of marginalized groups, such as women or Indigenous people, to a place. To increase diversity with commemorative places in Victoria (Australia), for example, gender 1Corresponding Author: Alexis HORDE VO, alexis.horde.vo@student.rmit.edu.au arXiv:2509.01030v2 [cs.IR] 3 Sep 2025 Horde Vo, Duckham, He, and Benli equality policies face questions such as: what are the existing places named in honor of women? This question implies the detection of such place names as well as the generation of relevant arguments; in other words, what is the origin of a place name? Place names may refer to multiple objects (streets, buildings, cities or even natural features) but streets are most frequently encountered in daily life, e.g. when addressing a parcel or locating a restaurant near our workplaces. Accordingly, there is a growing need for automated tools that can efficiently retrieve relevant information about the origins of a given place name. With its rich available archives, this chapter evaluates these automated tools within the streets of Melbourne, the historic center of Victoria. Developing such automated tools is challenging for at least two reasons. First, spatial queries can be highly ambiguous. For example, place names in Australia often consist of a single identifying word rather than a whole name (e.g., Batman Street in Melbourne, contrary to Avenue Simone Veil in Nice, France). To deal with ambiguity, it is often necessary to clarify the spatial context for a place name, such as the city, state, and country; the neighboring streets and the neighborhood. Our expectations about the most likely name origins vary spatially, for example, depending on whether the name appears in Melbourne, Victoria (where John Batman is a well-known historical figure who played a role in the founding of Melbourne, as well as in numerous massacres of Indigenous Australians) or in Los Angeles, California (where the comic-book character might be more relevant to the spatial context). Second, place name origins often fall within the domain of ‚Äúlong-tail knowledge‚Äù: discovering origins may rely on piecing together multiple low-frequency but salient occurrences in a knowledge base. For example, in a knowledge base such as DBpedia the name ‚ÄúBatman‚Äù is likely to appear much more frequently in association with the popular superhero than the historical figure. Identifying such long-tail instances often requires a chain of sophisticated reasoning to uncover correct answers. In this chapter, we develop a geographic information retrieval (GIR) system to automatically and effectively identify the origins of place names. We formulate the problem as a retrieval augmented generation task, allowing us to combine the strengths of traditional information retrieval and the generative capabilities of advanced language models (LMs). In our approach, relevant data is first retrieved from external knowledge sources (i.e., DBpedia), and then used to guide the generation of responses by pre- trained LMs (i.e., ColBERTv2 [1] and Llama2 [2]). Consequently, the presented system is capable of processing user queries formulated in natural language and providing responses that are both contextually accurate and linguistically coherent. In order to enhance the spatial understanding of LMs, we inject spatial knowledge to ColBERTv2 by fine-tuning it on a dataset crafted from GeoNames. Figure 1 presents an overview of our approach: the searcher retrieves relevant data from DBpedia, the indexer and the ranker filters the retrieved data via a fine-tuned ColBERTv2, and thegenerator produces the final answer via Llama2. In this study of how best to leverage LMs for GIR, we focus in particular on two key questions: Can traditional LMs adequately reflect spatial relationships and geographic context? We perform controlled experiments to evaluate various components of our model, such as semantic and spatial understanding. When specially tuned to spatial understanding, LMs become more limited in general semantic understanding. Further, these models do not Identifying Origins of Place Names Via Retrieval Augmented Generation Searcher Indexer Ranker Generator Who is Batman Street most likely named after, in Melbourne, Victoria, Australia? 1 2 3 4 SELECT ?object ... WHERE ... FILTER( CONTAINS(?object, ‚ÄúBatman‚Äù) ) Top-10 matching Llama2-13b-Chat ‚ÄúBased on the knowledge graph #3, Batman Street is named after John Batman.‚Äù DS:QALD9RDF/XML DS:GeoNames DS:GeoNamesCountry Training:Datasets From the dataset: DS:Gazetteer Embeddings In-context spatial /f_ilters Coarse (geo_aus)Fine-grained (geo_vic) C0, C1, C2a, C2b Chunks Question: origin of a place name (sem) ColBERT XML/RDF Figure 1. An overview of our approach. reliably capture spatial containment, nor prioritize spatially proximal objects. Further, introducing in-context spatial filters only helps to partially orient the predictions. Can the use of external knowledge bases increase response accuracy? By providing a context, the generator is driven to produce an output that relates more strongly to the provided ground knowledge, rather than fabricating (i.e., so-called ‚Äúhallucination‚Äù) nonexistent objects. However, we still observe ‚Äúmirages‚Äù: real but spatially distal objects that appear closer in terms of embeddings, and by consequence, acquire greater relevance for language models. Finally, the use of external knowledge restricts the solution space in a way that potentially increases the chance of correct answers; nonetheless, the models still need to learn how to navigate in this space. 2. Background and related work Scope of information retrieval over knowledge bases. Information retrieval aims to find ‚Äú(query, answer)‚Äù pairs. In knowledge bases, data can have an unstructured format with text; a semi-structured format with tables; or a structured format with knowledge graphs [3]. With the latter, knowledge graphs store information in atomic triplets ‚Äú( subject, predicate, object )‚Äù and respect ontologies for semantic interoperability aspects. For unstructured data, conventional approaches rely on pre-trained language models that capture and compare dense representations with text embeddings [4]. Querying semi- structured data typically implies converting the query to SQL. For structured data, current approaches either seek to convert the query to SPARQL or to find similarities with graph embeddings [5]. Horde Vo, Duckham, He, and Benli Modular architectures for information retrieval architectures. DBpedia is a widely used semantic knowledge base. However, it lacks consistency, mixing structured and unstructured data, such as paragraphs in natural language. As a result, reasoning and generalization with such knowledge bases is challenging. To tackle the problem, current architectures for information retrieval use multiple modules, such as an indexer to optimize the knowledge base for queries and a ranker to find the best matches [6]. Neural rankers then use low-dimensional encoders for text and to calculate similarities. A generator module can be added in retrieval augmented generation (RAG) with a generative language model to align the outputs with the input question in a final step [4]. Despite the increasing number of parameters, generators are not standalone models as they lack ground knowledge [7][8] and tend to fabricate information (‚Äúhallucination‚Äù). Recent language models can manipulate both natural language and programming languages [2], which increases the capacities of information retrieval systems. An example of machine learning for street names. Despite its limited information in comparison with DBpedia, Wikidata is an atomic knowledge graph used in StreetToPerson [9]. This model restricts the candidates for the origin of place names to persons only: given a place name X Street, StreetToPerson extracts vectors of attributes (e.g., name, occupation, place) for persons having X in their name and then trains a random forest classifier. 3. Methodology and architecture Our model is designed to identify the origin of a place name by extracting related knowledge from a spatial knowledge graph. Anchor-question. An example question that our model aims to solve is: ‚ÄúWho is the person that Batman Street in Melbourne is named after?‚Äù We formulate such a question into the following format: ‚Äú Who (A) is [Xplace] most likely named after (A), in [city([Xplace])] (B), [country([Xplace])] (C)? If it is not a person (A), find any other origin (A). ‚Äù Here, Xplace represents the target place name. As our model is based on language model generation, the template above will focus more on a person origin of Xplace ( A ). It also implies a spatial context involving a coarse spatial filter ( C ) and a finer-grained spatial filter ( B ). To retrieve the answer, the model first uses a searcher to extract candidate answers from DBpedia. Then, it uses an indexer-ranker to rank these candidates based on their closeness with the query. Finally, a generator module produces the final answer from the ranked list of candidates. These three modules are detailed below, with reference to the overall architecture illustrated in Figure 1. 3.1. Searcher. Given a query for the origin of a place name, the searcher module aims to extract any relevant data from an external spatial knowledge graph. In this work, we used DBpedia Identifying Origins of Place Names Via Retrieval Augmented Generation as the external knowledge source, where all knowledge is stored as triplets, i.e., (subject, predicate, object). Let Xplace be the place name mentioned in the query. We used the DBpedia SPARQL endpoint to extract ksearcher triplets from the DBpedia knowledge graph. Each extracted triplet satisfies two constraints: (1) the subject must contain the place name X ‚àó place name; and (2) the predicate must belong to a predefined relation set Frel, where Frel = {abstract, children, comment, country, date, geo, label, location, occupation, parent, place, spouse }. In particular, {abstract, comment} are included in Frel because triplets with these predicates usually include rich descriptions about their subjects. Extracted triplets are stored in a RDF/XML format. In order to improve the compactness of RDF/XML, all the URIs 2 were mapped with their prefixes, allowing LMs used in later modules to focus on those meaningful tokens in the URIs. 3.2. Indexer and ranker. An indexer and a ranker were developed to identify those triplets that are more relevant to the query. We developed the indexer and ranker based on ColBERTv2 [1], a pre- trained LM designed to compute the closeness between a pair of query and a triplet document in terms of semantic similarity. Here, the texts associated with a set of triplets related to one subject can be treated as one triplet document, and thus the semantic similarity between a query and a triplet document is measured as the similarity between their latent embeddings produced by a text encoder. ColBERTv2 enhances the efficiency of similarity computation by grouping and indexing the triplet documents into several clusters, where passages within the same cluster are more similar to each other. The ranker ranks all triplet documents based on their similarities to the query and returns the top-kranker documents as the result. The model is trained to rank documents related to the query (positive samples) higher than the documents that are irrelevant to the query (negative samples). We chose ColBERTv2 because it offers a balance between language understanding and computational efficiency. However, most LMs are not specifically trained to understand spatial concepts; ColBERTv2 is trained on general knowledge such as texts crawled from Wikipedia. To answer spatial queries, a system needs to handle spatial concepts accurately. Continuing our example query, ‚ÄúWho is the person that the Batman Street in Melbourne is named after?‚Äù; an accurate query response should respect common spatial knowledge such as ‚ÄúMelbourne is contained in Victoria‚Äù and ‚ÄúMelbourne is a city in Australia.‚Äù To inject spatial knowledge to the indexer-ranker, we fine-tuned the base model ColBERTv2 on two datasets curated for spatial understanding: DS:GeoNames and DS:GeoNamescountry. Further, we note that the base ColBERTv2 model was pre-trained on pure natural language text. Although RDF/XML format presents a structure that is mostly human readable, it still involves RDF/XML-specific syntax in its format, which may be harder for the base model to interpret. Hence, we fine-tuned the indexer-ranker on a dataset that we curated, namely DS:QALD9RDF/XML. 2Uniform Resource Identifier Horde Vo, Duckham, He, and Benli Fine-tuning the indexer-ranker. We fine-tuned the base language model ColBERTv2 using three datasets. DS:GeoNames and DS:GeoNames country were utilized for improving spatial understanding, while DS:QALD9RDF/XML was used for improving the understanding of RDF/XML formats. DS:GeoNamescountry. This dataset contains questions and answer pairs about countries that are neighboring to one another. We extracted countries from Geonames and constructed a graph Gcountry = {V country,Ecountry}. Here,V country is the node set with each node representing one country, and Ecountry represents the set of edges. Given country [countryi] and [country j] that share borders, we added an edgeecountry i, j to set E to represent the adjacency relationship. We generated one question-answer pair for edgeecountry i, j using the following template:  X = ‚Äú Give a country that shares a border with [countryi]. ‚Äù y+ = ‚Äú [country j] shares a border with [countryi].‚Äù  ‚àÄecountry i, j ‚ààEcountry (1) Here, y represents a country that is a positive answer to the question X. To fine-tune ColBERTv2, we also generated negative samples to the same question X, so that the model can learn to discriminate the spatial context involved in X. Specifically, we randomly sampled a country [countryk] that does not share border with [countryi] and generated a negative sample: y‚àí = ‚Äú [countryk] shares a border with [countryi].‚Äù (2) DS:GeoNames. This dataset completes DS:GeoNames country by also capturing the closeness among cities. We extracted cities from Geonames, keeping only cities with a population of at least nhab. For these cities, we computed their pairwise spherical distance. Similar to DS:GeoNamescountry, we constructed a city graph denoted as Gcity = {V city,Ecity} where each node represents one city. Given city [cityi] and [city j], if their distance is less than or equal to the threshold dcity, we added an edge ecity i, j to the edge set, and generated a question answer pair using the following template:  X = ‚ÄúGive a city near [cityi] in [country(cityi)].‚Äù y+ = ‚Äú[city j] in [country(city j)] is a neighbor of [cityi] in [country(cityi)].‚Äù  (i, j) (3) The negative samples in DS:GeoNames were generated in a similar way to DS:GeoNamescountry. We omit the details for conciseness. DS:QALD9RDF/XML. This dataset covers questions on general knowledge rather than focusing on spatial knowledge. Given a question X, its answer is curated by extracting sub-knowledge graphs from DBpedia in RDF/XML format. We built this dataset based on QALD9, which includes a range of questions and their corresponding SPARQL queries on DBpedia [10], denoted as {(X,ySPARQL)}. Given the i-th pair consisting question Xi and query ySPARQL,i, we executed the query and extracted the knowledge graph y+ KG,i, forming the new pair {(Xi,y+ KG,i)}. For training, we also identified negative samples y‚àí KG,i by retrieving nodes in DBpedia that contained one keyword (provided by QALD9) in the query, but that are not returned by executing ySPARQL. Identifying Origins of Place Names Via Retrieval Augmented Generation 3.3. Generator From the top-kranker documents, a generative language model, Llama2 [2], was used to choose the top-1 document and generate the final answer. The top-kranker documents were concatenated in the prompt. To concatenate these documents, we experimented with two ways of ordering these documents: ordering by increasing similarity with the query  ‚Üìkranker 1  and decreasing rank  ‚Üì1 kranker  . In our experiments, we observe that positioning the most relevant information near the tail of the prompt improves the results: due to limitations with long-distance dependencies, the model focuses on the most recent input to the language model, i.e., the tail of the prompt. We applied in-context learning into the design of our prompt, where an example of how to solve the task is provided to the language model. The prompt is designed as follows to retrieve the top-kgenerator(kgenerator = 1) document. For [kranker] knowledge graphs, an extract from the RDF/XML file is provided; the names of the knowledge graphs are: [subject 1]. . .[subjectkranker]. We want to find an answer to: ‚Äú[ANCHOR QUESTION]‚Äù. These are the extracts: [KG 1]. . .[KGkranker]‚Äù. Give me a simple answer to ‚Äú[ANCHOR QUESTION]‚Äù. [INSTRUCTIONSa] aInstructions: ‚ÄúOnly use the provided information. First, choose the extract that best allows you to answer among: [title 1]. . .[titlekranker]. Delimit your chosen answer with the tags ‚ü®CHOICE‚ü© ‚ü®/CHOICE‚ü© . Second, give your answer by delimiting it with the tags ‚ü®ANSWER‚ü© ‚ü® /ANSWER‚ü©. Your answer should be concise. If it is a person, I need the first name and the last name. For example, to ‚ÄúWho is Rue Madame Curie in Beirut, Lebanon named after?‚Äù, write: ‚Äú ‚ü®CHOICE‚ü© [write here your chosen source] ‚ü®/CHOICE‚ü© ‚ü® ANSWER‚ü© Marie Curie ‚ü®/ANSWER‚ü© Based on the provided information, ...‚Äù Here, the generated prompts were sent to a non fine-tuned Llama2-13B-Chat model, frozen with a 4-bit quantization. This maximum size of context is 4096, allowing us to concatenate all the chosen documents without truncation. 4. Experimental design Each place name Xplace in the gazetteer is treated independently as a sample: the results of one sample is not re-used for another place name. 4.1. Datasets We curated a dataset for evaluating the presented framework. Let DS:Gazetteer = {(X i place,yi origin)}N i=1 be the dataset containing N data samples, where the i-th sample is a pair of location X i place and its ground-truth origin yi origin. This dataset is derived from the Vicnames, a comprehensive database containing rich information about place names owned by Victorian State Government register of Geographic Place Names, Australia. In this work, we filtered this dataset to keep only street names in the city of Melbourne to perform a focused study. In addition, for each place name, we extracted its root name by removing any prefix and street type, e.g., converting Little Bourke Street to Bourke. The resultant dataset contains 248 entries, i.e., N = 248. Horde Vo, Duckham, He, and Benli As mentioned in Section 3, three datasets are curated to fine-tune the underlying language model, ColBERTv2, of the indexer-ranker: DS:GeoNames country, DS:GeoNames, and DS:QALD9 RDF/XML. To prepare DS:GeoNames country, we kept the ratio between positive and negative answers as 1:100. To prepare the dataset DS:GeoNames, we set nhab = 50K, dcity = 50 km. For each question, we kept the ratio between positive and negative answers as 1:5 at maximum. The statistics of these datasets are presented in Table 1. 4.2. Evaluation Using DS:Gazetteer, we evaluate the model in terms of its capability in two aspects: (1) understanding semantic meaning of a query; and (2) processing spatial contexts involved in a query. The following measures are utilized to evaluate the relevance of the retrieved answer to the query: ‚Ä¢ Semantic relevance: sem. In this measurement, we focus on the semantic similarity of the retrieved answer and the textual description of the place name Xplace in the gazetteer. Suppose thatXplace is Nancy Adams Placein Melbourne that is named after a local person in Melbourne. We evaluate if the retrieved answer is semantically relevant to Xplace. For example, if the system identifies the origin as Nancy Adams who lived in Victoria, then the answer is semantically overlapped with the query, and hence, it is deemed as semantically correct to the query. In contrast, the botanist Nancy Adams in New Zealand is considered as erroneous. ‚Ä¢ Spatial relevance: geo aus, geo vic. Similarly, we measure the spatial similarity of the retrieved answer and the place name Xplace in the query. Specifically, if the retrieved origin is related to the spatial context ofXplace (e.g., both retrieved origin and Xplace are related if they share a same spatial parent from a hierarchical spatial representation for containment; the parent can be the ‚Äúname of the country‚Äù ‚Äì equal to Australia for geo aus ‚Äì or the ‚Äúname of the state‚Äù ‚Äì equal to Victoria for geo vic), we treat it as a correct answer. The evaluation is conducted by one annotator. We report the accuracy of each module in our framework in terms of HR@ k: for N data samples in DS:Gazetteer, on average, what is the probability of the ground-truth answer being in the top- k retrieved answers. Since the indexer-ranker provides an ordered list of retrieved answers, we report the quality of the returned list by reporting three more metrics: mean reciprocal rank (MRR@k), normalized discounted cumulative gain (nDCG@k), and precision@k. Here, the MRR@k computes the average rank of the ground-truth item within the list of top-k answers; the nDCG@k is similar to hit ratio but penalizes the result if the ground-truth answer is ranked low; the precision@ k (P@k) reports the average number of answers that are related to the query. As benchmark models, the results after the generator are compared to the scores given by gpt-4o-mini and StreetToPerson [9]. 4.3. Experiments Searcher. We only extracted triplets whose language of the object is English or not specified. The Virtuoso SPARQL endpoint for DBpedia was used with a limitation Identifying Origins of Place Names Via Retrieval Augmented Generation Table 1. Statistics of the three datasets used for fine-tuning the indexer-ranker. In GeoNames, overseas territories of a country are considered as an independent entry for countries (e.g. French Guiana and France have two distinct entries), which explains the high number of countries. Dataset Positive pairs Negative pairs Comments DS:QALD9RDF/XML /commen‚óés647 64 401 408 questions DS:GeoNamescountry /gl‚å¢be-asia650 25 751 252 ‚Äùcountries‚Äù DS:GeoNames /gl‚å¢be-asia/ci‚óéy320 958 746 938 252 ‚Äùcountries‚Äù + 10 572 cities of ksearcher = 10K triplets. Additionally, we fixed a maximum of 1000 subjects. For reproducibility, the query was executed once, with all extracted knowledge graphs saved locally for all the experiments. Indexer and ranker. We set kranker as 10, meaning that the indexer-ranker will select the top-10 triplets from the 10K triplets retrieved from the searcher. The knowledge graph extracted by the searcher was split by subject in the triple; each chunk is regarded as a document for ColBERTv2. The maximum length for input is set to 256 tokens. As presented in Table 2, we developed five versions of ColBERT (C0, C1, C2a, C2b, C2c) that underwent different fine-tuning procedures. C0 [‚àÖ] is the original ColBERT without fine-tuning; C1 [/commen‚óés] is fine-tuned on DS:QALD9RDF/XML; C2a [/gl‚å¢be-asia+ /commen‚óés] is fine-tuned on DS:GeoNamescountry and then DS:QALD9RDF/XML; C2b [/gl‚å¢be-asia/ci‚óéy+ /commen‚óés] is fine-tuned on DS:GeoNames and then DS:QALD9RDF/XML; and C2c [/gl‚å¢be-asia/commen‚óés] is fine-tuned on both DS:GeoNames and DS:QALD9RDF/XML, where the two datasets are fed into ColBERTv2 simultaneously with shuffling. Table 2. Versions of ColBERT that are fine-tuned on different combination of datasets, where ‚àÖ represents the corresponding dataset is not utilized. The notations 1‚Éù and 2‚Éù represent that the corresponding dataset is utilized for fine-tuning, where 1/2 represents the order of the dataset introduced during fine-tuning. Version of The fine-tuning is carried out on the following datasets: ColBERT DS:GeoNamescountry /gl‚å¢be-asiaDS:GeoNames /gl‚å¢be-asia/ci‚óéyDS:QALD9RDF/XML /commen‚óés C0 [‚àÖ] ‚àÖ ‚àÖ ‚àÖ C1 [/commen‚óés] ‚àÖ ‚àÖ 1‚Éù C2a [/gl‚å¢be-asia+ /commen‚óés] 1‚Éù ‚àÖ 2‚Éù C2b [/gl‚å¢be-asia/ci‚óéy+ /commen‚óés] ‚àÖ 1‚Éù 2‚Éù C2c [/gl‚å¢be-asia/commen‚óés] 1‚Éù ‚àÖ 1‚Éù StreetToPerson. This model successfully extracted 261 compatible pairs ‚Äústreet- person‚Äù in Australia from the English Wikidata and Wikipedia as a training dataset. Even if the streets of Melbourne are part of the test dataset, we decided to include the 22 streets detected in Melbourne in the training dataset for three reasons. First, we want to characterize the streets in Melbourne to improve inference, hence it is necessary to have data for this area. Second, the dataset for Australia is 18 times smaller than in the original paper for Germany. Third, our objective is not to find originsex nihilo but rather to detect more origins than those already recorded in gazetteers: we accept that the model is biased by having seen the training dataset. Moreover, the pre-trained LMs used in our model are also biased as they might have seen also the training dataset. The parameters for StreetToPerson are unchanged. Horde Vo, Duckham, He, and Benli Table 3. Description of DS:Gazetteer Count % Number of streets 248 ... with an origin in the gazetteer 230 / 248 .927 ... that commemorates a named person 143 / 248 .577 ... that commemorates an unnamed person 68 / 248 .274 ... where the searcher successfully extracts a knowledge graph 222 / 248 .895 ... and whose origin is mentioned in the knowledge graph (HR@10K for the searcher) 93 / 222 .375 5. Results 5.1. Searcher Table 3 presents the results for the searcher. The gazetteer provides 248 streets in DS:Gazetteer with a known origin in 92.7% of the cases. Only 57.7% of the streets commemorate a person. Of those, 27.4% commemorate a local inhabitant of the area, such as a merchant or a former land owner, even though they are not explicitly named in the document3. The searcher successfully extracts a knowledge graph from DBpedia for 89.5% of the streets: the missing graphs are due to a lexical gap (e.g., Abeckett Street in the gazetteer instead of A‚ÄôBeckettStreet) or the absence of information in DBpedia. On average, the resulting dataset has 291 objects per knowledge graph. Among the extracted knowledge graphs, only 37.5% (HR@10 k) contain a mention of the origin (see Table 4). A mention does not necessarily mean that there is an explicit link between a candidate and the naming origin; only that the origin appears in the text. This relatively low score for the searcher indicates that not all the required information is easily accessible in DBpedia, first due to the limitations of the SPARQL endpoint and second due to the prevalence of unnamed persons. 5.2. Ranker Table 4 and Figure 2 present the results for the indexer-ranker, where the subscript sem represents evaluation results in terms of semantic understanding and the subscriptgeo vic and geo aus represent results in terms of spatial understanding (i.e., if the answer is respectively at least within Victoria or Australia). General. Initially pre-trained on English, a non fine-tuned ColBERTv2 C0 [ ‚àÖ] can already understand the RDF/XML format of knowledge graphs. However, the quality of the top-k candidates is not high with respect to their spatial distribution, particularly with the nDCG compared with fine-tuned models. To evaluate the models in detail, we must distinguish two cases: firstly, can a model retrieve an information that does exist? Secondly, if missing, can a model retrieve information that is at least spatially related to the query? 3For example, where the documentation specifies ‚Äúthis street is named after a former person who lived in the street‚Äù but does not explicitly name that person. This qualification was subjectively defined by the annotator. Identifying Origins of Place Names Via Retrieval Augmented Generation Table 4. Evaluation of the performances of C0, C1, C2a, C2b and C2c after the ranker (@ k = @10) for the different types of observations (relevance of the semantic evaluation sem, or spatial evaluations geo aus or geo vic). First, the scores are calculated for all the data in DS:Gazetteer and second, on the a subset where the extracted knowledge graph KG from the searcher contains a mention of the origin: these scores are marked with ‚àó. The notation x denotes averaged results on the dataset; MAP is the mean average precision (namely P@10). Each street in DS:Gazetteer ( N = 248) represents one independent sample; results are averaged on N = 248 elements. Ranker Scores on DS:Gazetteer (N = 248) ... restricted to the streets where @k = @10 the KG mentions an origin (N‚àó = 93) Model Type MRR nDCG MAP HR MRR‚àó nDCG‚àó MAP‚àó HR‚àó C0 [‚àÖ] sem .232 .170 .063 .216 .445 .412 .143 .506 geo aus .684 .777 .501 .883 .831 .854 .578 .933 geo vic .476 .539 .243 .703 .628 .646 .313 .753 C1 [/commen‚óés] sem .253 .186 .080 .243 .475 .421 .177 .528 geo aus .769 .854 .588 .896 .885 .908 .656 .944 geo vic .585 .637 .294 .739 .728 .729 .356 .786 C2a [/gl‚å¢be-asia] +[ /commen‚óés] sem .229 .162 .076 .221 .431 .382 .173 .506 geo aus .780 .858 .585 .892 .889 .909 .664 .944 geo vic .602 .657 .306 .748 .740 .745 .366 .798 C2b [ /gl‚å¢be-asia/ci‚óéy] + [/commen‚óés] sem .237 .169 .071 .213 .450 .400 .160 .489 geo aus .797 .891 .635 .900 .887 .934 .699 .955 geo vic .555 .620 .283 .733 .673 .698 .340 .773 C2c [ /gl‚å¢be-asia/commen‚óé] sem .232 .166 .073 .225 .418 .366 .159 .483 geo aus .792 .881 .618 .901 .890 .926 .679 .955 geo vic .615 .674 .318 .743 .717 .743 .375 .786 When a knowledge graph mentions an origin. In these cases, evident answers are already strong markers, without the need to attend to spatial understanding. Indeed, the models does not discard the answer for the query in the top-10 in more than 48% of the cases, as indicated by the HR ‚àó sem@k. Moreover, MRR‚àó sem is over 41% which means that the first evident answer appears quickly in the highest ranks. We observe that MAP ‚àó sem is between .1 and .2: an interpretation is that, on average, 1 or 2 retrieved items among the set of top-10 contain relevant information. Nonetheless, certain names like Flinders Street are more specific thanRainbow Alley, which increases the chances to have more than 2 items instead of 0. Mentions toAustralia or Victoria are good markers as the top-10 is likely to contain at least one related element in more than 75% of the recommendations when we consider HR‚àó geo vic or 93% with HR‚àó geo aus, as well as the first two elements out of 10 might be linked to these places (MRR‚àó geo aus@10 ‚â• .6). When the knowledge graph does not mention any relevant origin. In these cases, the sem score has no useful information but in contrast, the ranker is expected to manipulate more information related to geo aus or geo vic. In practice, we observe that the models do not fully exploit the spatial filters written in the context (in the anchor-question) as a discriminating criterion. An efficient ranker should first prioritize passages that do contain the origin of a place name. This origin can be explicitly given in the passage, or inferred with multiple passages eventually with the internal knowledge of language models. As a second order Horde Vo, Duckham, He, and Benli (a) C0 ( ‚àÖ) - not trained on a spatial dataset (b) C2a ( /gl‚å¢be-asia+ /commen‚óés) - trained on a spatial dataset Figure 2. Retrieved items for 15 place names regarding geo aus and geo vic. A fine-tuning is expected to highlight spatially related candidates at the highest ranks, which is characterized by the nDCG@10. of priority, human understanding would focus on spatial similarities, as indicated in the anchor-question, by giving better ranks for items related to Melbourne, Victoria, or Australia. This characteristic is not fully respected by our results. For all the models, the scores for the nDCGgeo ...@k and MAPgeo ...@k are worse than nDCGgeo ...@‚àók and MAP‚àó geo ... with a systematic difference of more than .05. This indicates that the models tend to provide more attention to the semantic aspect than the two spatial in-context Identifying Origins of Place Names Via Retrieval Augmented Generation filters. In other words, mentioning in Melbourne, Victoria, Australia in the anchor- question does not act as a filter. Effect of training. In these cases, fine-tuning is mainly useful to read the RDF format and to find similarities based of the global meanings, with a compromise with spatial understanding. Training with DS:QALD9RDF/XML offers better models for the evaluation sem than the baseline C0 [ ‚àÖ] since ColBERT now understands the RDF/XML format. However, there is a compromise between the semantic and the spatial scores: a previous fine- tuning on DS:GeoNames country and DS:GeoNames reduced the scores on sem to offer better scores for geo aus or geo vic as shown in Figure 2 for example. As a surprising result, training on a fine-grained grid of locations with DS:GeoNames does not improve scores on fine locations geo vic but only on coarse locations with geo aus. We propose two explanations to this observation: first, DS:GeoNames contains more mentions to countries that strengthen the similarities between countries and second, the mention of one city is long-tail information that has little impact on the back propagation of the loss. In contrast, only training on DS:GeoNames country at a coarser spatial level keeps a certain capacity for the language models to generalize. In Figure 2, we show that the fine- tuning on spatial pairs does not fully improve the rankings: qualitatively, the distribution of the results still keeps a high entropy. We then assume that a continual learning first on spatial then RDF/XML understanding may lead to ‚Äúcatastrophic forgetting‚Äù of spatial knowledge in C2a [/gl‚å¢be-asia+ /commen‚óés] and C2b [/gl‚å¢be-asia/ci‚óéy+ /commen‚óés]: that is why we define C2c [/gl‚å¢be-asia/commen‚óés] with a unique training on both skills. Finally, C2c [ /gl‚å¢be-asia/commen‚óés] does not necessarily lead to better performance. By consequence, the paradigm of fine-tuning hardly captures both semantic and spatial understanding in a neural information retrieval only based on a language model. 5.3. Generator Figure 3 and Figure 4 present the results for the generator, discussed further below. Comparison with the baselines. StreetToPerson does not generalize to Australia; gpt-4o-mini provides insightful answers only for common knowledge whereas our models offer better and specialized answers in the long-tail knowledge. Indeed, for StreetToPerson, HR...@1 and HR‚àó ...@1 are significantly lower than in our approach. With gpt-4o-mini, HRsem@1 is better than StreetToPerson but still lower than our approach; moreover, gpt-4o-mini correctly retrieves an origin for streets that are named after common elements (Plover4 Lane or Bridleway5 Walk) contrary to our model. Capacity of generalization. With limited supervision, the use of a generator aims to align a final answer with an initial question, while discriminating information. In our experiments, the generator fills this role but is not optimized to take decisions. The generator tends to systematically answer the query without hallucination; however, it seldom rejects an irrelevant top- kranker set: we only encounter the answer ‚ÄúThere is no relevant information to answer the question‚Äù once or twice. In our experiments, limited scores for the HR@1 in Figure 3 are found. We believe the reasons 4A bird 5Path originally used by people riding horses or trails Horde Vo, Duckham, He, and Benli sem geo_aus geo_vic Type of evaluation 0.00 0.25 0.50 0.75HR@1 Evaluation of the hit ratio HR@1 after the generator gpt-4o-mini StreetToPerson C0 C1 C2a C2b C2c (a) Scores on DS:Gazetteer ( n = 248) sem geo_aus geo_vic Type of evaluation 0.00 0.25 0.50 0.75HR * @1 Evaluation of the hit ratio HR * @1 after the generator gpt-4o-mini StreetToPerson C0 C1 C2a C2b C2c (b) ... restricted to the streets where the KG mentions an origin ( n‚àó = 93) Figure 3. Evaluation of the hit ratio on the types sem, geo aus and geo vic after the generator for the models C0, C1, C2a, C2b, C2c and comparison with the baselines gpt-4o-mini and StreetToPerson. C0 C1 C2a C2b C2c Model 0.50 0.25 0.00 HR * HR* after the ... ... searcher ... ranker ... generator Figure 4. Relative change ‚àÜHR‚àó(modulei,modulei‚àí1) with module i ‚àà { searcher, ranker, generator }. The more ‚àÜHR ‚Üí 0, the more relevant information is selected without loss. can be twofold. First, the model does not understand the background knowledge of naming conventions, e.g., some rules abouthow places are named. During the evaluation, the model tends to over-estimate persons from the world of sport, particularly football and rugby. Second, the information ground-truth answer may be absent in the dataset and the model has to infer from the existing knowledge. For example with Athenaeum Place, no extracted information mentionsAthena but surprisingly, the generator correctly Identifying Origins of Place Names Via Retrieval Augmented Generation infers the correct answer from its own knowledge. This capacity of generalization can be improved with a better quality of the top-10 items provided in the dynamic prompt. In Figure 4, we observe that the generator loses less information after the ranker if the top- kranker has good scores on geo vic in C2a [/gl‚å¢be-asia+ /commen‚óés], which compensates lower results after the ranker: through in-context learning, the generator tends to favor answers related to Victoria or Australia. However, there is still a compromise between spatial or semantic scores, as shown in Figure 3. 6. Discussion and conclusions Our experiment underlines the important differences between spatial proximity and semantic proximity. In our GIR architecture, later modules aim to counterbalance weaknesses of earlier ones. By exploiting grounded knowledge with different approaches, the final output is consolidated. In summary, the architecture aims to support the principle that maps still speak louder than words. However, our architecture exhibits two stubborn weaknesses. First, spatial information is under-used; and second, losses of information propagate through our system. These weaknesses are particularly impactful for retrieval of long-tail information, such as the origin of place names. Spatially proximal information might be semantically distal in the language models. Other preliminary findings are also suggested by our results. Co-dependencies with structured graphs instead of texts. In our process, we treat each candidate independently in the ranking and we do not consider that each candidate might mutually contribute to better understand each other. An interesting direction is to develop multimodal information retrieval, that combines texts and spatial knowledge graphs [5]. Their graph representation is able to create dependencies in a corpus in the ranker module while offering low-dimension representations for fast rankings. Qualitative spatial reasoning. Disambiguation is a key factor to improve geographic information retrieval, particularly to help to associate a footprint with the mentions ‚Äúnear Collins Street‚Äù or ‚Äúarrived in Australia.‚Äù Recent works try to tackle this intrinsic nature of spatial information [11] in language models. Nevertheless, the domain is still an open problem. Further development. In this work, the results can be extended at larger scales, notably other cities in Australia or in France for example where the ambiguity behind a place name might be limited. In this chapter, we focused on a task that requires high resources in terms of annotations for the evaluation. In a first step, the creation of a gold dataset between a place name and its origin and, in a second step, the annotation of each result in the top-10 and top-1. Despite being encapsulated in texts, spatial containment relations are better captured within hierarchies. In this aim, the high level of representation conveyed by knowledge graphs is more promising than prosaic texts. Techniques commonly applicable for natural language processing systems partially fail with spatial information: indeed, geographic information retrieval needs to know which footprints are impacted rather than which words are. Horde Vo, Duckham, He, and Benli Acknowledgments This project was undertaken with the assistance of computing resources from RACE (RMIT AWS Cloud Supercomputing). We also acknowledge the long-term work of Geographic Names Victoria with its past and current members. We also thank the two anonymous reviewers whose comments contributed to this chapter. References [1] Santhanam K, Khattab O, Saad-Falcon J, Potts C, Zaharia M. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. arXiv; 2022. Available from: https://doi.org/10. 48550/arXiv:2112.01488. [2] Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y , et al.. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv; 2023. Available from: https://doi.org/10.48550/arXiv: 2307.09288. [3] Purves RS, Clough P, Jones CB, Hall MH, Murdock V . Geographic Information Retrieval: Progress and Challenges in Spatial Search of Text. Foundations and Trends in Information Retrieval. 2018;12(2‚Äì 3):164-318. Available from: http://dx.doi.org/10.1561/1500000034. [4] Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V , Goyal N, et al.. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv; 2021. Available from: https://doi.org/10.48550/ arXiv:2005.11401. [5] Lan Y , He G, Jiang J, Jiang J, Zhao WX, Wen JR. A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions. arXiv; 2021. Available from:https://doi.org/10. 48550/arXiv.2105.11644. [6] Gao Y , Xiong Y , Gao X, Jia K, Pan J, Bi Y , et al.. Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv; 2024. Available from:https://doi.org/10.48550/arXiv.2312.10997. [7] Ma Y , Cao Y , Hong Y , Sun A. Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!. arXiv; 2023. Available from:https://doi.org/10.48550/ arXiv.2303.08559. [8] Sun K, Xu YE, Zha H, Liu Y , Dong XL. Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?. arXiv; 2024. Available from: https://arxiv.org/abs/2308.10168. [9] Gurtovoy D, Gottschalk S. Linking Streets in OpenStreetMap to Persons in Wikidata. In: Companion Proc. Web Conference. New York, NY , USA: Association for Computing Machinery; 2022. p. 294-7. Available from: https://doi.org/10.1145/3487553.3524267. [10] Usbeck R, Gusmita R, Saleem M, Ngonga Ngomo AC. 9th Challenge on Question Answering over Linked Data (QALD-9). In: Choi KS, Anke LE, Declerck T, Gromann D, Kim JD, Ngomo ACN, et al., editors. Joint Proc. ISWC 2018 Workshops SemDeep-4 and NLIWOD-4; 2018. p. 58-64. Available from: https://ceur-ws.org/Vol-2241/paper-06.pdf . [11] Beydokhti MK, Tao Y , Duckham M, Griffin AL. Integrating Large Language Models and Qualitative Spatial Reasoning. In: Big Data: Techniques and Technologies in Geoinformatics. CRC Press; 2025. p. 316-33.