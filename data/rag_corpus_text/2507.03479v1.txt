arXiv:2507.03479v1 [cs.IR] 4 Jul 2025 Explainable Information Retrieval in the Audit Domain Alexander Frummet dab: GmbH Deggendorf, Germany alexander.frummet@dab-gmbh.de Emanuel Slany dab: GmbH Deggendorf, Germany emanuel.slany@dab-gmbh.de Jonas Amling dab: GmbH Deggendorf, Germany jonas.amling@dab-gmbh.de Moritz Lang dab: GmbH Deggendorf, Germany moritz.lang@dab-gmbh.de Stephan Scheele OTH Regensburg Regensburg, Germany stephan.scheele@oth-regensburg.de ABSTRACT Conversational agents such as Microsoft Copilot and Google Gemini assist users with complex search tasks but often generate mislead- ing or fabricated references. This undermines trust, particularly in high-stakes domains such as medicine and finance. Explainable information retrieval (XIR) aims to address this by making search re- sults more transparent and interpretable. While most XIR research is domain-agnostic, this paper focuses on auditing – a critical yet underexplored area. We argue that XIR systems can support audi- tors in completing their complex task. We outline key challenges and future research directions to advance XIR in this domain. KEYWORDS explainable IR, audit, information needs 1 INTRODUCTION Recently, agents like Microsoft Copilot and Google Gemini have gained popularity by combining the capabilities of Large Language Models (LLMs) with search systems to support users in completing complex search tasks [ 41, 48]. To promote trust, these systems often include references to the sources their answers are based on [48]. However, these references are frequently fabricated or misleading [45], which undermines the trustworthiness of such agents [4, 50, 53]. This issue becomes particularly critical in high- stakes domains such as medicine [ 13] and business [ 55], where decisions based on inaccurate or misleading information can have serious consequences. For instance, if doctors rely on incorrect agent responses to epilepsy [27] or cancer-related [37] queries, they may make harmful treatment decisions [22]. Similarly, businesses that base their strategies on misleading information risk significant financial losses [12]. Therefore, building trust in these systems is essential. To this end, researchers argue that the search results underlying the agents’ answers must be made more transparent, interpretable, and ultimately explainable [4, 11]. To achieve this, research on explainable AI (XAI) and explain- able information retrieval (XIR) has introduced methods to help users interpret model decisions [2, 43]. XAI techniques aim to pro- vide insights into the behaviour of models [ 38]. Local XAI tech- niques explain model decisions given specific input instances to users. Famous examples include feature attribution methods such as LIME [35] or SHAP [31] or counterfactual explanations [44, 47]. Similarly, XIR research focuses on making search systems more transparent and interpretable [2, 39]. This includes explaining how the system interprets user queries [3, 46, 54], developing evalua- tion metrics to assess explainability [7, 20], and providing expla- nations for search results [ 6, 28–30]. Methods for search result explanations range from explaining the ranking of retrieved evi- dences [6, 29, 34, 52] to providing additional information on the retrieved sources to enhance transparency and build trust in the sys- tem [28]. Explanations can take the form of visualisations or textual descriptions [1, 28] offering users varying levels of detail [1, 33, 51]. Most existing research focuses on domain-agnostic use cases. Search system explainability in specific domains, however, remains underexplored. While some studies have examined explainable IR in healthcare and medical applications [22], other critical fields, such as finance, law, and audit, have received little attention. Studying domain-specific applications is crucial, as each field has unique challenges and requirements. For instance, information needs in domains like cooking [17, 18] are expressed differently to those in web [5, 24] and mobile search [10]. This paper focuses on the audit domain, a critical yet underex- plored area in the IR community. Audits involve examining financial transactions for inconsistencies and fraud, requiring auditors to gather data from multiple sources, such as SAP tables and financial reports, and compile accurate reports detailing their findings. Audit- ing is complex, as it involves analysing large datasets and making informed decisions. Since every major company undergoes an an- nual audit, an explainable search system that supports auditors’ needs could provide significant benefits. By offering transparent and interpretable results, such a system could improve efficiency, enhance accuracy, and aid in fraud detection, ultimately saving costs and ensuring financial integrity. Therefore, we believe that explainable IR systems can assist auditors in generating comprehensive audit reports. The system should enable auditors to express complex information needs while providing transparent and interpretable retrieval results. This would help auditors assess the relevance of key financial documents and tables in their investigations. To advance research in this domain, we propose future research directions (see Section 2) and outline key challenges in XIR research for the audit context (see Section 3). 2 RESEARCH DIRECTIONS In this section, we outline key research directions to advance re- search on XIR in the audit domain. These include understanding auditor information needs (see Section 2.1), designing suitable expla- nations (see Section 2.2), and developing interpretable knowledge representations (see Section 2.3). Frummet et al. 2.1 Information Needs in the Audit Domain Understanding user information needs is a critical first step in designing effective information retrieval (IR) systems, as it influ- ences how search systems should be structured [ 19, 26, 49]. For instance, Frummet and Elsweiler [15] found that users in the cook- ing domain tend to prefer concise answers to simple, fact-based questions (e.g., “How much sugar do I need?”) and more detailed responses for complex, competence-oriented queries (e.g., “How do I chop an onion?”). Similarly, the audit domain likely involves a broad spectrum of information needs. It seems plausible that information needs in this domain could range from simple fact- checking (e.g., “What was the invoice amount for transaction X?”) to complex procedural or investigative queries (e.g., “How was this cost justified and approved across departments?”). The nature of the information need can influence how explanations should be presented – whether as a direct answer, a chain of events, or links to supporting documents. Future research needs to investigate in detail the type of information needs that can occur and how these are best supported through appropriate explanations. 2.2 Explanations in Audit-Related IR Once information needs are understood, the next step is to design explanations that effectively support them. Prior work has explored various explanation formats, including visualisations [ 1, 34], de- tailed descriptions [34, 46] (e.g., knowledge graphs showing rela- tionships between relevant documents) and textual highlights (e.g., key sentences in relevant documents) [ 33]. In the audit context, the preferred explanation format may vary depending on the task. For example, when verifying an expense claim, auditors might pre- fer a short textual summary linking the claim to relevant policy documents and approval records. In contrast, when investigating a complex revenue recognition issue, a visual explanation showing dependencies across contracts, revenue entries, and timeline events could be more useful. Understanding these preferences is key to building IR systems that enhance auditor efficiency and trust. 2.3 Representing Audit-Related Knowledge From a system perspective, knowledge representation plays a cen- tral role in supporting explainability. Research suggests that knowl- edge graphs are well-suited for this, as their structure inherently conveys relationships between entities [6, 9, 25]. In the audit do- main, many elements are interdependent. For instance, a revenue entry in the general ledger might depend on multiple underlying sales contracts, which in turn relate to delivery records and ap- proval workflows. Representing these relationships in a knowledge graph allows for intuitive navigation and explanation – for example, showing how a revenue figure is supported by a chain of contractual and operational evidence. 3 CHALLENGES In this section, we highlight the challenges of conducting XIR re- search in the audit domain, which relate to gathering realistic in- formation needs (Section 3.1), addressing data availability issues (Section 3.2), and determining appropriate ground truth explana- tions (Section 3.3). 3.1 Getting Realistic Information Needs As highlighted in Section 2.1, information needs are typically cap- tured through search queries and categorised using information need taxonomies. In open domains like web search or cooking, collecting such queries is relatively straightforward through crowd- sourcing platforms such as Prolific or MTurk [ 8, 16], logging in- teractions with search interfaces [ 23, 32], or conducting in-situ observational studies [18]. However, in specialised and high-stakes domains such as medicine and auditing, this becomes more difficult. These fields are closed, expert-driven environments where access is limited, making it challenging to log real user interactions or rely on crowdsourcing methods in a large scale. 3.2 Barriers to Using Real-World Audit Data As discussed in Section 2.3, developing effective retrieval algorithms requires careful consideration of how audit-relevant documents are structured. However, financial data is highly confidential, and com- panies operate under strict regulatory and compliance standards. As a result, they may be unwilling or not allowed to share real- world data for XIR research. This makes it challenging to design and evaluate explainable retrieval systems using realistic datasets. Releasing such data for reproducibility is equally difficult and may be only possible in anonymised form. Therefore, it is crucial to en- sure that anonymised datasets still retain enough fidelity to reflect real-world scenarios. For example, anonymised employee expense reports – preserving categories like travel, meals, or lodging, along with approval status and justifications – can be used to simulate retrieval tasks, such as finding similar past expenses. 3.3 Ground Truth Explanations As emphasised in Section 2.2, designing helpful explanations is an important research direction in this domain. A major challenge lies in providing appropriate ground truth explanations for evalu- ation [36, 38, 40, 42]. Research in XAI has shown that the quality of explanations is context-dependent [21] and that different user groups might prefer different explanation modalities [14]. For exam- ple, a junior auditor might need detailed textual evidence pointing to inconsistencies in SAP exports, while a manager might prefer a visual overview linking anomalies across reports. These varying needs, combined with the opacity of existing systems, highlight the importance of developing and evaluating multi-modal, post-hoc explanation methods tailored to audit tasks. REFERENCES [1] Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, and Gerhard Weikum. 2017. QUINT: Interpretable Question Answering over Knowledge Bases. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017 - System Demonstrations, Lucia Specia, Matt Post, and Michael Paul (Eds.). Association for Computational Linguistics, 61–66. https://doi.org/10.18653/v1/d17-2011 [2] Avishek Anand, Lijun Lyu, Maximilian Idahl, Yumeng Wang, Jonas Wallat, and Zijian Zhang. 2022. Explainable Information Retrieval: A Survey. CoRR abs/2211.02405 (2022). https://doi.org/10.48550/arXiv.2211.02405 [3] Avishek Anand, Venktesh V, Abhijit Anand, and Vinay Setty. 2023. Query Understanding in the Age of Large Language Models. CoRR abs/2306.16004 (2023). https://doi.org/10.48550/arXiv.2306.16004 [4] Leif Azzopardi, Charles L. A. Clarke, Paul Kantor, Bhaskar Mitra, Johanne R. Trippas, Zhaochun Ren, Mohammad Aliannejadi, Negar Arabzadeh, Raman Chan- drasekar, Maarten de Rijke, Panagiotis Eustratiadis, William Hersh, Jin Huang, Evangelos Kanoulas, Jasmin Kareem, Yongkang Li, Simon Lupart, Kidist Amde Explainable Information Retrieval in the Audit Domain Mekonnen, Adam Roegiest, Ian Soboroff, Fabrizio Silvestri, Suzan Verberne, David Vos, Eugene Yang, and Yuyue Zhao. 2024. Report on the Search Fu- tures Workshop at ECIR 2024. SIGIR Forum 58, 1 (Aug. 2024), 1–41. https: //doi.org/10.1145/3687273.3687288 [5] Andrei Broder. 2002. A taxonomy of web search. SIGIR Forum 36, 2 (Sept. 2002), 3–10. https://doi.org/10.1145/792550.792552 [6] Boqi Chen, Kua Chen, Yujing Yang, Afshin Amini, Bharat Saxena, Cecilia Chávez- García, Majid Babaei, Amir Feizpour, and Dániel Varró. 2023. Towards Improving the Explainability of Text-based Information Retrieval with Knowledge Graphs. CoRR abs/2301.06974 (2023). https://doi.org/10.48550/arXiv.2301.06974 [7] Catherine Chen and Carsten Eickhoff. 2024. Evaluating Search System Ex- plainability with Psychometrics and Crowdsourcing. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024 , Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.). ACM, 1051–1061. https://doi.org/10.1145/3626772.3657796 [8] Jason Ingyu Choi, Saar Kuzi, Nikhita Vedula, Jie Zhao, Giuseppe Castellucci, Marcus D. Collins, Shervin Malmasi, Oleg Rokhlenko, and Eugene Agichtein. 2022. Wizard of Tasks: A Novel Conversational Dataset for Solving Real-World Tasks in Conversational Settings. In Proceedings of the 29th International Confer- ence on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (Eds.). International Committee on Computational Linguistics, 3514–3529. https://aclanthology.org/2022.coling-1.310 [9] Philipp Christmann, Rishiraj Saha Roy, and Gerhard Weikum. 2023. Explainable Conversational Question Answering over Heterogeneous Sources via Iterative Graph Neural Networks. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023 , Hsin-Hsi Chen, Wei-Jou (Edward) Duh, Hen- Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete (Eds.). ACM, 643–653. https://doi.org/10.1145/3539618.3591682 [10] Karen Church, Mauro Cherubini, and Nuria Oliver. 2014. A large-scale study of daily information needs captured in situ. ACM Trans. Comput.-Hum. Interact. 21, 2, Article 10 (Feb. 2014), 46 pages. https://doi.org/10.1145/2552193 [11] J. Shane Culpepper, Fernando Diaz, and Mark D. Smucker. 2018. Research Frontiers in Information Retrieval: Report from the Third Strategic Workshop on Information Retrieval in Lorne (SWIRL 2018). SIGIR Forum 52, 1 (2018), 34–90. https://doi.org/10.1145/3274784.3274788 [12] Eva Eigner and Thorsten Händler. 2024. Determinants of LLM-assisted Decision- Making. CoRR abs/2402.17385 (2024). https://doi.org/10.48550/arXiv.2402.17385 [13] Marcos Fernández-Pichel, Juan C. Pichel, and David E. Losada. 2025. Evaluating search engines and large language models for answering health questions. npj Digital Medicine 8, 1 (2025). https://doi.org/10.1038/s41746-025-01546-w [14] Bettina Finzel, David E. Tafler, Stephan Scheele, and Ute Schmid. 2021. Expla- nation as a Process: User-Centric Construction of Multi-level and Multi-modal Explanations. In KI 2021: Advances in Artificial Intelligence , Stefan Edelkamp, Ralf Möller, and Elmar Rueckert (Eds.). Springer International Publishing, Cham, 80–94. doi:10.1007/978-3-030-87626-5_7 [15] Alexander Frummet and David Elsweiler. 2024. Decoding the Metrics Maze: Nav- igating the Landscape of Conversational Question Answering System Evaluation in Procedural Tasks. In Proceedings of the Fourth Workshop on Human Evaluation of NLP Systems (HumEval) @ LREC-COLING 2024 , Simone Balloccu, Anya Belz, Rudali Huidrom, Ehud Reiter, Joao Sedoc, and Craig Thomson (Eds.). ELRA and ICCL, Torino, Italia, 81–90. https://aclanthology.org/2024.humeval-1.8/ [16] Alexander Frummet and David Elsweiler. 2024. QookA: A Cooking Question Answering Dataset. In Proceedings of the 2024 ACM SIGIR Conference on Human Information Interaction and Retrieval, CHIIR 2024, Sheffield, United Kingdom, March 10-14, 2024 , Paul D. Clough, Morgan Harvey, and Frank Hopfgartner (Eds.). ACM, 406–410. doi:10.1145/3627508.3638311 [17] Alexander Frummet, David Elsweiler, and Bernd Ludwig. 2019. Detecting Domain-specific Information needs in Conversational Search Dialogues. In Proceedings of the 3rd Workshop on Natural Language for Artificial Intelligence co-located with the 18th International Conference of the Italian Association for Artificial Intelligence (AIIA 2019), Rende, Italy, November 19th-22nd, 2019 (CEUR Workshop Proceedings, Vol. 2521) , Mehwish Alam and Valerio Basile and Fe- lice Dell’Orletta and Malvina Nissim and Nicole Novielli (Ed.). CEUR-WS.org. https://ceur-ws.org/Vol-2521/paper-02.pdf [18] Alexander Frummet, David Elsweiler, and Bernd Ludwig. 2022. "What Can I Cook with these Ingredients?" - Understanding Cooking-Related Information Needs in Conversational Search. ACM Trans. Inf. Syst. 40, 4 (2022), 81:1–81:32. doi:10.1145/3498330 [19] Alexander Frummet, Alessandro Speggiorin, David Elsweiler, Anton Leuski, and Jeff Dalton. 2024. Cooking with Conversation: Enhancing User Engagement and Learning with a Knowledge-Enhancing Assistant. ACM Trans. Inf. Syst. 42, 5, Article 122 (2024), 29 pages. https://doi.org/10.1145/3649500 [20] Shuyu Guo, Shuo Zhang, Weiwei Sun, Pengjie Ren, Zhumin Chen, and Zhaochun Ren. 2023. Towards Explainable Conversational Recommender Systems. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, Hsin-Hsi Chen, Wei-Jou (Edward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete (Eds.). ACM, 2786–2795. https://doi.org/10. 1145/3539618.3591884 [21] Louisa Heidrich, Emanuel Slany, Stephan Scheele, and Ute Schmid. 2023. Fair- Caipi: A Combination of Explanatory Interactive and Fair Machine Learning for Human and Machine Bias Reduction. Machine Learning and Knowledge Extraction 5, 4 (2023), 1519–1538. doi:10.3390/make5040076 [22] Andreas Holzinger, Chris Biemann, Constantinos S. Pattichis, and Douglas B. Kell. 2017. What do we need to build explainable AI systems for the medical domain? CoRR abs/1712.09923 (2017). http://arxiv.org/abs/1712.09923 [23] Bernard J. Jansen. 2006. Search log analysis: What it is, what’s been done, how to do it. Library & Information Science Research 28, 3 (2006), 407–432. https://doi.org/10.1016/j.lisr.2006.06.005 [24] Bernard J. Jansen, Danielle Booth, and Brian Smith. 2009. Using the taxon- omy of cognitive learning to model online searching. Information Processing & Management 45, 6 (2009), 643–663. https://doi.org/10.1016/j.ipm.2009.05.004 [25] Zhen Jia, Soumajit Pramanik, Rishiraj Saha Roy, and Gerhard Weikum. 2021. Complex Temporal Question Answering on Knowledge Graphs. InCIKM ’21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021 , Gianluca Demartini, Guido Zuccon, J. Shane Culpepper, Zi Huang, and Hanghang Tong (Eds.). ACM, 792–802. https://doi.org/10.1145/3459637.3482416 [26] Diane Kelly, Jaime Arguello, Ashlee Edwards, and Wan-ching Wu. 2015. Devel- opment and Evaluation of Search Tasks for IIR Experiments using a Cognitive Complexity Framework. In Proceedings of the 2015 International Conference on The Theory of Information Retrieval (Northampton, Massachusetts, USA) (IC- TIR ’15). Association for Computing Machinery, New York, NY, USA, 101–110. https://doi.org/10.1145/2808194.2809465 [27] Hyun-Woo Kim, Dong-Hyeon Shin, Jiyoung Kim, Gha-Hyun Lee, and Jae Wook Cho. 2024. Assessing the performance of ChatGPT’s responses to questions related to epilepsy: A cross-sectional study on natural language processing and medical information retrieval. Seizure - European Journal of Epilepsy 114 (2024), 1–8. https://doi.org/10.1016/j.seizure.2023.11.013 [28] Weronika Lajewska, Damiano Spina, Johanne Trippas, and Krisztian Balog. 2024. Explainability for Transparent Conversational Information-Seeking. In Proceed- ings of the 47th International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024 , Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.). ACM, 1040–1050. https://doi.org/10.1145/3626772.3657768 [29] Jurek Leonhardt, Koustav Rudra, and Avishek Anand. 2023. Extractive Explana- tions for Interpretable Text Ranking.ACM Trans. Inf. Syst. 41, 4 (2023), 88:1–88:31. https://doi.org/10.1145/3576924 [30] Michael Llordes, Debasis Ganguly, Sumit Bhatia, and Chirag Agarwal. 2023. Explain Like I am BM25: Interpreting a Dense Model’s Ranked-List with a Sparse Approximation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Tai- wan, July 23-27, 2023 , Hsin-Hsi Chen, Wei-Jou (Edward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete (Eds.). ACM, 1976–1980. https://doi.org/10.1145/3539618.3591982 [31] Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 4765–4774. https://proceedings.neurips.cc/paper/2017/hash/ 8a20a8621978632d76c43dfd28b67767-Abstract.html [32] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. In Proceedings of the Workshop on Cogni- tive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773) , Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. https://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf [33] Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, and Junji Tomita. 2019. Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , Anna Korhonen, David R. Traum, and Lluís Màrquez (Eds.). Association for Computational Linguistics, 2335–2345. https://doi.org/10.18653/v1/p19-1225 [34] Sayantan Polley, Atin Janki, Marcus Thiel, Juliane Hoebel-Mueller, and Andreas Nuernberger. 2021. ExdocS: Evidence Based Explainable Document Search. InThe 1st International Workshop on Causality in Search and Recommendation (CSR’21) Frummet et al. (Online). CEUR Workshop Proceedings, 1–7. https://doi.org/10.1145/nnnnnnn. nnnnnnn [35] Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I Trust You?": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016 , Balaji Krishnapuram, Mohak Shah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi (Eds.). ACM, 1135–1144. doi:10.1145/2939672.2939778 [36] Avi Rosenfeld. 2021. Better Metrics for Evaluating Explainable Artificial In- telligence. In AAMAS ’21: 20th International Conference on Autonomous Agents and Multiagent Systems, Virtual Event, United Kingdom, May 3-7, 2021 , Frank Dignum, Alessio Lomuscio, Ulle Endriss, and Ann Nowé (Eds.). ACM, 45–50. doi:10.5555/3463952.3463962 [37] Nicholas R. Rydzewski, Deepak Dinakaran, Shuang G. Zhao, Eytan Ruppin, Baris Turkbey, Deborah E. Citrin, and Krishnan R. Patel. 2024. Comparative Evaluation of LLMs in Clinical Oncology. NEJM AI 1, 5 (2024). https://doi.org/10.1056/ aioa2300151 [38] Gesina Schwalbe and Bettina Finzel. 2023. A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts. Data Mining and Knowledge Discovery (2023). doi:10.1007/s10618- 022-00867-8 [39] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019, Melbourne, VIC, Aus- tralia, February 11-15, 2019 , J. Shane Culpepper, Alistair Moffat, Paul N. Bennett, and Kristina Lerman (Eds.). ACM, 770–773. https://doi.org/10.1145/3289600. 3290620 [40] Emanuel Slany, Stephan Scheele, and Ute Schmid. 2024. Bayesian CAIPI: A Probabilistic Approach to Explanatory and Interactive Machine Learning. InArti- ficial Intelligence. ECAI 2023 International Workshops , Sławomir Nowaczyk, Prze- mysław Biecek, Neo Christopher Chung, Mauro Vallati, Paweł Skruch, Joanna Jaworek-Korjakowska, Simon Parkinson, Alexandros Nikitas, Martin Atzmüller, Tomáš Kliegr, Ute Schmid, Szymon Bobek, Nada Lavrac, Marieke Peeters, Roland van Dierendonck, Saskia Robben, Eunika Mercier-Laurent, Gülgün Kayakutlu, Mieczyslaw Lech Owoc, Karl Mason, Abdul Wahid, Pierangela Bruno, Francesco Calimeri, Francesco Cauteruccio, Giorgio Terracina, Diedrich Wolter, Jochen L. Leidner, Michael Kohlhase, and Vania Dimitrova (Eds.). Springer Nature Switzer- land, Cham, 285–301. doi:10.1007/978-3-031-50396-2_16 [41] Siddharth Suri, Scott Counts, Leijie Wang, Chacha Chen, Mengting Wan, Tara Safavi, Jennifer Neville, Chirag Shah, Ryen W. White, Reid Andersen, Georg Buscher, Sathish Manivannan, Nagu Rangan, and Longqi Yang. 2024. The Use of Generative Search Engines for Knowledge Work and Complex Tasks. CoRR abs/2404.04268 (2024). arXiv:2404.04268 https://doi.org/10.48550/arXiv.2404. 04268 [42] Stefano Teso and Kristian Kersting. 2019. Explanatory Interactive Machine Learning. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019, Honolulu, HI, USA, January 27-28, 2019 , Vincent Conitzer, Gillian K. Hadfield, and Shannon Vallor (Eds.). ACM, 239–245. doi:10.1145/ 3306618.3314293 [43] Suzan Verberne. 2018. Explainable IR for Personalizing Professional Search. In Joint Proceedings of the First International Workshop on Professional Search (ProfS2018); the Second Workshop on Knowledge Graphs and Semantics for Text Retrieval, Analysis, and Understanding (KG4IR); and the International Workshop on Data Search (DATA:SEARCH’18) Co-located with (ACM SIGIR 2018), Ann Arbor, Michigan, USA, July 12, 2018 (CEUR Workshop Proceedings, Vol. 2127), Laura Dietz, Laura Koesten, and Suzan Verberne (Eds.). CEUR-WS.org, 35–42. https://ceur- ws.org/Vol-2127/paper4-profs.pdf [44] Sandra Wachter, Brent D. Mittelstadt, and Chris Russell. 2017. Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. (2017). arXiv:1711.00399 [45] William H. Walters and Esther Isabelle Wilder. 2023. Fabrication and errors in the bibliographic citations generated by ChatGPT. Scientific Reports 13, 1 (2023), 14045. https://doi.org/10.1038/s41598-023-41032-5 [46] Yumeng Wang, Xiuying Chen, and Suzan Verberne. 2024. QUIDS: Query Intent Generation via Dual Space Modeling. CoRR abs/2410.12400 (2024). https: //doi.org/10.48550/arXiv.2410.12400 [47] Adam White and Artur S. d’Avila Garcez. 2020. Measurable Counterfactual Local Explanations for Any Classifier. In ECAI 2020 - 24th European Conference on Artificial Intelligence, 29 August-8 September 2020, Santiago de Compostela, Spain, August 29 - September 8, 2020 - Including 10th Conference on Prestigious Applications of Artificial Intelligence (PAIS 2020) (Frontiers in Artificial Intelligence and Applications, Vol. 325). IOS Press, 2529–2535. doi:10.3233/FAIA200387 [48] Ryen W. White. 2023. Tasks, Copilots, and the Future of Search: A Keynote at SIGIR 2023. SIGIR Forum 57, 2 (2023), 4:1–4:8. doi:10.1145/3642979.3642985 [49] Barbara Wildemuth, Luanne Freund, and Elaine G. Toms. 2014. Untangling search task complexity and difficulty in the context of interactive information retrieval studies. Journal of Documentation 70, 6 (2014), 1118–1140. https: //doi.org/10.1108/JD-03-2014-0056 [50] Haoyi Xiong, Jiang Bian, Yuchen Li, Xuhong Li, Mengnan Du, Shuaiqiang Wang, Dawei Yin, and Sumi Helal. 2024. When Search Engine Services Meet Large Lan- guage Models: Visions and Challenges. IEEE Transactions on Services Computing 17, 6 (2024), 4558–4577. doi:10.1109/TSC.2024.3451185 [51] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, 2369–2380. https://aclanthology.org/D18-1259/ [52] Puxuan Yu, Razieh Rahimi, and James Allan. 2022. Towards Explainable Search Results: A Listwise Explanation Generator. In SIGIR ’22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022 , Enrique Amigó, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai (Eds.). ACM, 669–680. https://doi.org/10.1145/3477495.3532067 [53] ChengXiang Zhai. 2024. Large Language Models and Future of Information Retrieval: Opportunities and Challenges. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’24). Association for Computing Machinery, New York, NY, USA, 481–490. https://doi.org/10.1145/3626772.3657848 [54] Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng. 2020. Query Understanding via Intent Description Generation. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management (Virtual Event, Ireland) (CIKM ’20). Association for Computing Machinery, New York, NY, USA, 1823–1832. https://doi.org/10.1145/3340531.3411999 [55] Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Hanqi Jiang, Yi Pan, Junhao Chen, Yifan Zhou, Gengchen Mai, Ninghao Liu, and Tianming Liu. 2024. Revolutionizing Finance with LLMs: An Overview of Applications and Insights. CoRR (2024). arXiv:2401.11641 [cs.CL] https://arxiv.org/abs/2401.11641