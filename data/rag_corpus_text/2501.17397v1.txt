arXiv:2501.17397v1 [cs.CL] 29 Jan 2025 Leveraging In-Context Learning and Retrieval-Augmented Generation for Automatic /Q_uestion Generation in Educationa l Domains Subhankar Maity∗ IIT Kharagpur West Bengal, India subhankar.ai@kgpian.iitkgp.ac.in Aniket Deroy IIT Kharagpur West Bengal, India roydanik18@kgpian.iitkgp.ac.in Sudeshna Sarkar IIT Kharagpur West Bengal, India sudeshna@cse.iitkgp.ac.in ABSTRACT Question generation in education is a time-consuming and co gni- tively demanding task, as it requires creating questions th at are both contextually relevant and pedagogically sound. Current auto- mated question generation methods often generate question s that are out of context. In this work, we explore advanced techniq ues for automated question generation in educational contexts , focus- ing on In-Context Learning (ICL), Retrieval-Augmented Gen era- tion (RAG), and a novel Hybrid Model that merges both methods . We implement GPT-4 for ICL using few-shot examples and BART with a retrieval module for RAG. The Hybrid Model combines RAG and ICL to address these issues and improve question quality. Eval- uation is conducted using automated metrics, followed by hu man evaluation metrics. Our results show that both the ICL appro ach and the Hybrid Model consistently outperform other methods , in- cluding baseline models, by generating more contextually accurate and relevant questions. CCS CONCEPTS • Computing methodologies → Natural language generation; • Applied computing → Education. KEYWORDS Automatic Question Generation (AQG), Large Language Model s (LLMs), In-Context Learning (ICL), Retrieval-Augmented G enera- tion (RAG) ACM Reference Format: Subhankar Maity, Aniket Deroy, and Sudeshna Sarkar. 2025. Leverag ing In-Context Learning and Retrieval-Augmented Generation for Automatic Question Generation in Educational Domains. In Proceedings of ACM Con- ference (Conference’17).ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION The demand for personalized and adaptive learning systems i n ed- ucation has grown signiﬁcantly in recent years [50]. Educat ional ∗Corresponding Author Permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided that copies ar e not made or distributed for proﬁt or commercial advantage and that copies bear this n otice and the full cita- tion on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.or g. Conference’17, July 2017, Washington, DC, USA © 2025 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn technologies, particularly those powered by Artiﬁcial Int elligence in Education (AIED), are reshaping how we create learning ma te- rials, assess student knowledge, and provide customized in struc- tion [37]. A crucial element in these systems is Automatic Qu es- tion Generation (AQG), which refers to the automatic creati on of questions from textual content, such as textbook passages, lecture notes, or online educational materials [7, 28, 30, 44]. AQG h as nu- merous applications, ranging from generating quizzes and a ssess- ments to enhancing student engagement through interactivelearn- ing platforms [11]. Despite recent advancements, there rem ains a signiﬁcant gap in generating high-quality, contextually r elevant, and pedagogically meaningful questions for diverse educat ional settings [19]. With the advent of neural networks and large language models (LLMs), AQG systems have made signiﬁcant strides [3]. Model s such as T5 [38], BART [22], and GPT-3 [6] have demonstrated re - markable success in tasks such as summarization [39], trans lation [12], question-answering [49], and dialogue generation [14]. These models, when ﬁne-tuned on question-answer datasets, can ge ner- ate more diverse and grammatically correct questions. Howe ver, they still face challenges in the educational domain, parti cularly when it comes to generating pedagogically appropriate ques tions that align with the instructional goals of a given curriculum [3]. Ad- ditionally, the success of these models often depends on the avail- ability of large, domain-speciﬁc training datasets, which are scarce in educational contexts [3]. Recent advances in In-Context Learning (ICL) [6] and Retrieval- Augmented Generation (RAG) [23] provide promising new avenues for tackling the limitations of traditional AQG methods. Un like conventional ﬁne-tuning, ICL enables large pre-trained la nguage models to generate task-speciﬁc outputs by providing a few i nput- output examples without updating the model’s parameters. T his makes ICL particularly useful in educational settings with limited domain-speciﬁc data, where the few-shot examples guide themodel to generate contextually relevant questions for new passages. How- ever, the quality of the generated questions relies heavily on the representativeness of the provided examples. On the other h and, RAG, as proposed by [23], enhances the generation process by in- tegrating a retrieval mechanism into the generation pipeli ne. It retrieves semantically relevant external documents and co mbines them with the input text to generate more informed and contex tu- ally rich questions. This is especially beneﬁcial in educat ional con- texts where the provided passage may lack suﬃcient informat ion, and the retrieval of supplementary resources can add depth a nd detail to the generated questions. RAG is useful when additi onal Conference’17, July 2017, Washington, DC, USA Maity et al. context from sources like textbooks, research papers, or on line en- cyclopedias is needed to create high-quality, domain-speciﬁc ques- tions. Despite their promise, both ICL and RAG have certain limita- tions when applied independently in the educational AQG domain. ICL is limited by the quality and coverage of the few-shot exa m- ples, often generating questions that are too similar to the input examples or fail to address the nuanced details of the input p as- sage. RAG, while eﬀective at incorporating external knowle dge, can sometimes retrieve irrelevant or redundant documents [9], lead- ing to questions that are either oﬀ-topic or overly complica ted. In this paper, we propose that combining the strengths of ICL and RAG can signiﬁcantly improve the quality of educational ques- tion generation. We hypothesize that a hybrid approach, whi ch ﬁrst retrieves relevant external documents (as in RAG) and t hen uses a few-shot learning mechanism (as in ICL) to guide quest ion generation, will result in questions that are not only conte xtually relevant but also pedagogically meaningful. By leveragingexternal information through retrieval and guiding the model’s outp ut with carefully selected examples, we aim to address the shortcomings of each individual method and generate a broader range of quest ions that target various cognitive levels in educational assess ments. This paper makes several key contributions to the ﬁeld of AQG , particularly in the context of educational applications: • A comparative analysis of In-Context Learning (ICL) and Retrieval-Augmented Generation (RAG) for automated ques- tion generation in the educational domain. We evaluate how each method performs individually in generating questions from textbook passages and assess their respective strengths and weaknesses. • A novel hybrid model that combines ICL and RAG to gener- ate higher-quality, contextually accurate, and pedagogically aligned questions. Our hybrid approach leverages external knowledge through retrieval and incorporates in-context learn- ing to guide the generation process. • A comprehensive evaluation of the proposed methods using both automated metrics (e.g., BLEU-4, ROUGE-L, METEOR, ChRF, BERTScore) and human evaluation by educators. We assess the grammaticality, appropriateness, relevance, com- plexity, and answerability, providing insights into the peda- gogical value of each approach. The rest of the paper is structured as follows: Section 2 revi ews related work in AQG, ICL, and RAG. Section 3 outlines the task deﬁnition and mathematical formulation of the problem. Sec tion 4 discusses the dataset used in our study. In Section 5, we desc ribe the methodology, detailing the implementation of ICL, RAG, and our proposed hybrid model. The experimental setup is presen ted in Section 6. Section 7 covers the automated and human evalua - tion metrics used in this study. Results and analysis are pre sented in Section 8, where we compare the models using both automati c and human evaluations and provide a detailed analysis of the re- sults. Finally, Section 9 concludes the paper and discussespotential future work. 2 RELATED WORK 2.1 Automatic Question Generation (AQG) The ﬁeld of Automatic Question Generation (AQG) has evolved from early rule-based systems to more sophisticated neuralnetwork- based approaches. Initial research in AQG primarily focuse d on template-based methods [8, 33, 43], which generate questio ns by applying syntactic and semantic rules to predeﬁned templat es. For example, [13] used syntactic transformations to generate f actual questions from declarative sentences. However, template-based meth- ods such as [43] lack ﬂexibility and are highly dependent on p re- deﬁned rules, which limit their applicability to diﬀerent d omains and complex question types. With the rise of neural networks, sequence-to-sequence (Seq2Seq) models such as T5 [38] and BART [22] have been successfully ap - plied to AQG [10, 18, 27]. These models are trained end-to-en d to map a given passage to a corresponding question, generati ng more diverse and grammatically correct questions. However , neu- ral models often struggle with domain-speciﬁc knowledge, e spe- cially when training data is limited. Moreover, they tend to gen- erate questions that are either too generic or not pedagogic ally aligned with the content’s educational goals. 2.2 In-Context Learning (ICL) In-Context Learning (ICL) was introduced by [6] as part of th e capabilities of the GPT-3 language model. Unlike ﬁne-tunin g ap- proaches, where a model is trained on a speciﬁc task using a la rge dataset, ICL allows large language models to perform a task b ased on a few examples provided as input [25]. The model processes these examples and generalizes them to generate appropriat e out- puts for unseen instances. ICL has demonstrated success in various tasks, including text classiﬁcation [31], translation [2] , and sum- marization [15, 45], making it particularly valuable in sit uations where data is scarce. However, in the context of AQG, ICL’s performance has not been extensively studied. The challenge lies in crafting eﬀ ective few-shot prompts that guide the model toward generating ped a- gogically meaningful questions. Existing work has shown that ICL is sensitive to prompt design, and small changes in the examp les provided can lead to signiﬁcantly diﬀerent outputs [25]. Fu rther- more, ICL’s reliance on in-context examples means it may fai l to capture deeper contextual information that is not present i n the input passage but is essential for generating complex educa tional questions. 2.3 Retrieval-Augmented Generation (RAG) Retrieval-Augmented Generation (RAG), introduced by [23] , ad- dresses the limitations of purely generative models by inco rporat- ing an external retrieval mechanism. In a typical RAG framew ork, the model ﬁrst retrieves relevant documents from an externa l cor- pus based on the input text [16]. The retrieved documents pro - vide additional context, which is used to generate more accu rate and contextually enriched outputs [41]. This approach is pa rticu- larly beneﬁcial for knowledge-intensive tasks where the input text alone does not provide suﬃcient information. Leveraging ICL and RAG for Automatic /Q_uestion Generation in Ed ucational Domains Conference’17, July 2017, Washington, DC , USA RAG has been applied successfully to various tasks, such as knowledge- based question answering [34, 42, 47] and code summarization [26]. Nevertheless, RAG’s performance in the context of AQG has no t been widely explored. In the context of AQG, RAG can retrievesup- plementary information from external sources, such as text books or research papers, to generate questions that are not only c ontex- tually aligned with the input passage but also pedagogicall y rele- vant. However, the retrieval process may introduce noise if irrel- evant or redundant documents are retrieved [9], which can ne ga- tively impact the quality of the generated questions. 3 TASK DEFINITION 3.1 Problem Statement The task of Automatic Question Generation (AQG) can be formu- lated as follows: given an input passage /u1D443 , the objective is to gen- erate a question /u1D444 that is relevant to the content of /u1D443 , contextually accurate, and aligned with educational goals. 3.2 Input and Output Representation The input to the model is a passage /u1D443 , which can be a sentence, a paragraph, or a longer text excerpt from educational materi al. The output is a question /u1D444 that is relevant to /u1D443 and suitable for use in an educational setting. Formally, we deﬁne the task as learn ing a function: /u1D453( /u1D443 ) = /u1D444 where /u1D443 is the input passage, and /u1D444 is the generated question. 3.3 In-Context Learning (ICL) In the In-Context Learning (ICL) paradigm, given an input passage /u1D443 new and a set of/u1D458 few-shot examples {( /u1D443 1, /u1D444 1) , ( /u1D443 2, /u1D444 2) , . . . , ( /u1D443 /u1D458, /u1D444 /u1D458)} , the model generates a new question /u1D444 new corresponding to /u1D443 new: /u1D444 new = /u1D453ICL ( /u1D443 new, {( /u1D443 /u1D456, /u1D444 /u1D456)} /u1D458 /u1D456=1) Here, the few-shot examples serve as prompts to guide the ques- tion generation process for the new passage. 3.4 Retrieval-Augmented Generation (RAG) For Retrieval-Augmented Generation (RAG), the task is extended by incorporating an external retrieval mechanism. Given a pas sage /u1D443 , the model retrieves a set of relevant documents { /u1D4451, /u1D445 2, . . . , /u1D445 /u1D458} from an external corpus. These documents provide additiona l con- text, and the ﬁnal question /u1D444 is generated as: /u1D444 = /u1D453RAG( /u1D443, { /u1D445/u1D456} /u1D458 /u1D456=1) 3.5 Hybrid Model Our proposed Hybrid Model combines the advantages of both ICL and RAG. The model ﬁrst retrieves a set of documents { /u1D445/u1D456} /u1D458 /u1D456=1 for the input passage /u1D443 , and then uses few-shot learning to generate the question /u1D444 based on both the passage and retrieved documents: /u1D444 = /u1D453Hybrid ( /u1D443, { /u1D445/u1D456} /u1D458 /u1D456=1, {( /u1D443 /u1D456, /u1D444 /u1D456)} /u1D45A /u1D456=1) Here, the retrieval step enriches the context for question g en- eration, while the few-shot examples (i.e., /u1D45A examples) help guide the model towards generating pedagogically relevant quest ions. 4 DATASET We used the EduProbe dataset [27], which comprises 3,502 question- answer pairs across various subjects: 858 pairs related to H istory, 861 pairs related to Geography, 802 pairs related to Economics, 606 pairs related to Environmental Studies, and 375 pairs related to Sci- ence. The dataset was curated from segments of varying lengt hs, extracted from a diverse range of chapters in National Counc il of Educational Research and Training (NCERT) 1 textbooks across several subjects, covering standards from 6/u1D461ℎto 12/u1D461ℎ. Each entry in the dataset includes a context (or passage), a long prompt, a short prompt, and a question. For our experiments, we extracted only the context (or passage) and question, focusing on evaluating how well the models generate questions based on the provided context s (or passages). We used the same training and test datasets as in [ 27]. 5 METHODOLOGY In this section, we describe the methodologies employed for ques- tion generation using In-Context Learning (ICL), Retrieval-Augmented Generation (RAG), and a Hybrid Model that combines both ap- proaches. 5.1 In-Context Learning (ICL) Approach For the ICL approach, we use the GPT-4 [1] model to generate ques- tions based on a few-shot prompt. Each prompt consists of /u1D458 ex- ample input-output pairs {( /u1D443 1, /u1D444 1) , . . . , ( /u1D443 /u1D458, /u1D444 /u1D458)} , where each pair consists of a passage and a corresponding question. Given a n ew passage /u1D443 new, the model generates a question /u1D444 new using the few- shot examples as context. The general structure of the ICL pr ompt is as follows: ⟨/u1D443 1, /u1D444 1⟩, ⟨/u1D443 2, /u1D444 2⟩, . . . , /u1D443 new → /u1D444 new We experimented with diﬀerent numbers of examples /u1D458 and op- timized the structure of the prompt for maximum performance . 5.2 Retrieval-Augmented Generation (RAG) Approach The RAG model employs the BART architecture as its generative backbone, further reﬁned by a retrieval module. This retrie val sys- tem, implemented using FAISS [17], searches through an extensive external corpus of educational materials 2 speciﬁcally tailored for school-level subjects such as History, Geography, Economi cs, Sci- ence, and Environmental Studies. For a given passage/u1D443 , the system identiﬁes the most relevant documents from this curated cor pus. These retrieved documents denoted as { /u1D445/u1D456} /u1D458 /u1D456=1, are concatenated with /u1D443 , and the combined input is processed through a ﬁne-tuned BART model trained on the EduProbe training set [27] to gener - ate a question /u1D444 . The ﬁne-tuning of BART on EduProbe ensures that the model is adept at generating questions based on educ a- tional content, further enhancing its performance. Formal ly, ques- tion generation in RAG is deﬁned as: /u1D444 = /u1D453RAG( /u1D443, { /u1D445/u1D456} /u1D458 /u1D456=1) 1https://en.wikipedia.org/wiki/National_Council_of_Educational_Research_and_Training 2https://ncert.nic.in/textbook.php Conference’17, July 2017, Washington, DC, USA Maity et al. The retrieval module enriches the context, allowing the mod el to generate questions that are better aligned with the conte nt and objectives of the passage. 5.3 Hybrid Model Our Hybrid Model combines the retrieval-based context enrichment of RAG with the few-shot learning mechanism of ICL using GPT-4. First, relevant documents are retrieved for a given passage/u1D443 . Then, the few-shot learning mechanism uses these retrieved docum ents alongside the input passage to generate a more contextually accu- rate and pedagogically meaningful question. The hybrid app roach can be mathematically deﬁned as: /u1D444 = /u1D453Hybrid ( /u1D443, { /u1D445/u1D456} /u1D458 /u1D456=1, {( /u1D443 /u1D456, /u1D444 /u1D456)} /u1D45A /u1D456=1) Here, /u1D443 is the input passage, { /u1D445/u1D456} /u1D458 /u1D456=1 are the retrieved docu- ments, and {( /u1D443 /u1D456, /u1D444 /u1D456)} /u1D45A /u1D456=1 are the few-shot examples used to guide the question generation process. 6 EXPERIMENTAL SETUP Our experiments evaluate the eﬀectiveness of the In-Context Learn- ing (ICL), Retrieval-Augmented Generation (RAG), and Hybrid Model for Automatic Question Generation (AQG). We aim to assess an d compare these models’ performance using a variety of automa ted and human evaluation metrics. 6.1 Baseline Models Following [10, 27], we ﬁne-tune the best-performing models(based on automated evaluation), such as the T5-large and BART-lar ge architectures, on the EduProbe training dataset [27]. The T 5-large model, implemented from the Hugging Face Transformers library3, uses a sequence-to-sequence framework with attention mechanisms to align passages with questions, providing a robust baseli ne for comparison. Similarly, the BART-large model, also from Hug ging Face, employs a transformer encoder-decoder structure to g ener- ate questions from passages, serving as another strong base line for our evaluation. 6.2 In-Context Learning (ICL) We implemented ICL using GPT-44, providing it with diﬀerent few- shot settings (/u1D458 = 3, 5, 7). In this setup, the model uses a set of exam- ple passage-question pairs to generate questions for new pa ssages. We tested various numbers of examples to determine the optim al few-shot settings for generating relevant and coherent que stions. 6.3 Retrieval-Augmented Generation (RAG) For RAG, we utilized BART-large 5 as the backbone model and in- tegrated a retrieval module with FAISS for eﬃcient document re- trieval. For each input passage, the retrieval module ident iﬁes and fetches the top /u1D458 = 5 relevant documents from a large corpus of educational material. These documents are concatenated wi th the input passage, and BART-large generates questions based on the combined context. 3https://huggingface.co/models 4https://platform.openai.com/docs/models/gpt-4-turbo -and-gpt-4 5https://huggingface.co/facebook/bart-large 6.4 Hybrid Model The Hybrid Model combines retrieval and in-context learning tech- niques. We ﬁrst retrieve /u1D458 = 5 relevant documents for each pas- sage, then use a few-shot prompt with /u1D45A = 5 examples to generate questions. This model leverages both the additional contex t from retrieved documents and the few-shot learning capabilitie s to pro- duce high-quality questions. 7 EV ALUATION METRICS The performance of the models was evaluated using several au to- mated evaluation metrics: BLEU-4 [35], ROUGE-L [24], METEO R [21], ChRF [36], and BERTScore [48]. We use the implementati ons of these metrics provided by the SummEval package 6. Acknowledging the limitations of automated metrics in textgen- eration research [4, 5, 32, 40], we conducted a human evaluat ion involving three high school teachers and two high school students. Each evaluator reviewed a total of 1,400 questions across seven set- tings: T5-large (baseline), BART-large (baseline), ICL ( /u1D458 = 3, 5, 7), RAG (/u1D458 = 5), and Hybrid model ( /u1D458 = 5, /u1D45A = 5). They rated each question on a scale from 1 (worst) to 5 (best) based on ﬁve crit e- ria: Grammaticality [27, 29, 46] (correctness of grammar indepen- dent of context), Appropriateness [27] (semantic correctness irre- spective of context), Relevance [27] (alignment with the given con- text), Complexity [27] (level of reasoning required to answer), and Answerability [29, 46] (whether the question can be answered from the provided context). To evaluate the level of agreement among the ﬁve raters for each generated question, we use Fleiss’s kappa as the measure of i nter- rater agreement. The resulting kappa scores are 0.51, 0.48, 0.45, 0.45, and 0.49 for grammaticality, appropriateness, relevance, com- plexity, and answerability, respectively. These kappa values sug- gest a moderate level of agreement across all human evaluati on metrics [20]. 8 RESULTS AND ANALYSIS In the automated evaluation (see Table 1), In-Context Learn ing (ICL) with /u1D458 = 7 demonstrates the best overall performance, sur- passing other conﬁgurations such as RAG ( /u1D458 = 5) and the Hybrid Model ( /u1D458 = 5, /u1D45A = 5). ICL with /u1D458 = 7 excels in ROUGE-L, ME- TEOR, CHrF, and BERTScore, while ICL with /u1D458 = 5 achieves the highest BLEU-4 score. This advantage can be attributed to IC L’s ability to eﬀectively leverage multiple examples to genera te more contextually relevant questions. In contrast, RAG and the H ybrid Model depend on external document retrieval, which sometim es retrieves content misaligned with the input passage, reducing their eﬀectiveness in generating closely related questions. Nev ertheless, both RAG and the Hybrid Model consistently outperform the ﬁn e- tuned baseline models (i.e., T5-large and BART-large) acro ss all automated evaluation metrics, demonstrating their overal l supe- riority in question generation tasks. The ﬁne-tuned models , while reliable, lack the additional context provided by retrieva l or few- shot examples, making them less eﬀective than the more advanced techniques (i.e., ICL, RAG or Hybrid Models). As shown in Table 2, the human evaluation results indicate that the Hybrid Model ( /u1D458 = 5, /u1D45A = 5) consistently outperforms other 6https://github.com/Yale-LILY/SummEval Leveraging ICL and RAG for Automatic /Q_uestion Generation in Ed ucational Domains Conference’17, July 2017, Washington, DC , USA Table 1: Automatic evaluation results for diﬀerent models. The highest value for each metric, achieved by any model, is h igh- lighted in blue, and values marked with * are statistically signiﬁcant base d on student’s /u1D461 -test at the 95% conﬁdence interval compared to the lowest corresponding baseline value. Model BLEU-4 ROUGE-L METEOR ChRF BERTScore T5-large (Baseline) [27] 21.59 53.90 32.20 57.03 71.80 BART-large (Baseline) [27] 20.05 51.60 31.90 54.96 74.20 ICL (/u1D458 = 3) 22.65 54.24 32.98 58.47 74.93 ICL (/u1D458 = 5) 22.87 54.84 33.58 59.42* 75.60* ICL (/u1D458 = 7) 22.69 55.95* 34.62 60.48* 75.92* RAG (/u1D458 = 5) 20.76 52.60 32.07 56.93 70.20 Hybrid Model (/u1D458 = 5, /u1D45A = 5) 21.45 53.79 33.69 57.78 71.45 Table 2: Human evaluation results for generated questions a cross diﬀerent models on grammaticality (Gramm), appropriate- ness (Appr), relevance (Rel), complexity (Comp), and answerability (Answ). The highest value for each metric, achieved by any model, is highlighted in blue, and values marked with * are statistically signiﬁcant base d on student’s /u1D461 -test at the 95% conﬁ- dence interval compared to the lowest corresponding baseli ne value. Model Gramm Appr Rel Comp Answ T5-large (Baseline) [27] 4.65 4.45 3.92 3.57 3.21 BART-large (Baseline) [27] 3.81 3.98 3.60 3.60 3.15 ICL (/u1D458 = 3) 4.67* 4.50* 3.97* 3.65 3.20 ICL (/u1D458 = 5) 4.72* 4.56* 4.03* 3.78 3.24 ICL (/u1D458 = 7) 4.76* 4.62* 4.08* 3.84 3.31 RAG (/u1D458 = 5) 3.90 4.10 3.70 3.74 2.90 Hybrid Model (/u1D458 = 5, /u1D45A = 5) 4.84* 4.74* 4.25* 4.02* 3.20 models across key metrics such as grammaticality, appropriateness, relevance, and complexity. The Hybrid Model achieves the highest scores in most human evaluation metrics, suggesting its abi lity to generate more linguistically correct, contextually appropriate, and pedagogically complex questions. In terms of relevance and com- plexity, the Hybrid Model demonstrates a clear advantage, likely due to its integration of both retrieval and few-shot learni ng tech- niques, which allows it to create questions that are well-al igned with the passage and involve deeper reasoning. This is follo wed by the ICL with /u1D458 = 7, which also performs strongly across all metrics, particularly in grammaticality, appropriateness, and rele- vance, but slightly lags behind the Hybrid Model in complexity. However, ICL with /u1D458 = 7 outperforms other models in generat- ing more answerable questions, as it leverages multiple examples to better align the questions with the context, making them m ore straightforward to answer. RAG ( /u1D458 = 5), however, shows weaker performance, particularly in answerability, where it scores lower compared to both the ICL and Hybrid models. This could be at- tributed to its reliance on external document retrieval, wh ich may introduce content that is less directly tied to the passage, thus af- fecting the relevance and answerability of the generated questions. The ﬁne-tuned baseline models, T5-large and BART-large, perform adequately but fall behind the more advanced models. T5-lar ge exhibits a stronger performance compared to BART-large acr oss most metrics, particularly in grammaticality and appropriateness, but neither model matches the performance of the ICL or Hybri d approaches. These ﬁndings underscore the beneﬁts of incorp orat- ing retrieval and few-shot examples for generating more con textu- ally relevant and complex educational questions. Tables 3 and 4 show two data samples (context and gold stan- dard question) from the EduProbe dataset [27], covering His tory and Economics. They also display the corresponding questions gen- erated by T5-large (baseline), BART-large (baseline), ICL(/u1D458 = 3, 5, 7), RAG (/u1D458 = 5), and the Hybrid model ( /u1D458 = 5, /u1D45A = 5). It can be ob- served that while baseline models sometimes produce questi ons that are irrelevant or unanswerable from the context, the proposed techniques, particularly ICL, generate questions that aremore closely aligned with the provided context. 9 CONCLUSIONS AND FUTURE WORK In this study, we explored advanced methods for automated qu es- tion generation in educational contexts, focusing on In-Co ntext Learning (ICL), Retrieval-Augmented Generation (RAG), and a novel Hybrid Model that combines both techniques. Our results rev eal that ICL with /u1D458 = 7 demonstrates superior performance in au- tomated evaluation metrics, such as ROUGE-L, METEOR, CHrF, Conference’17, July 2017, Washington, DC, USA Maity et al. Table 3: A sample (context and gold standard question) from t he History subject of the EduProbe dataset [27], including questions generated by various models and settings. The highlighted color indicates irrelevant or unanswerable questions according to the context. Context: During the medieval period in India, Islamic rulers held sig niﬁcant power, leading to the blending of Indian and Islamic cultures, which can still be observed in the archite cture and artwork created at that time. The country was gover ned and administered by notable rulers who made exceptional contributions in the ﬁelds of art, culture, literature, and architecture during this period. Gold Standard Question: What is the contribution of the medieval period to Indian his tory? Model Generated Question T5-large (Baseline) [27] What happened in the medieval period in India, which saw a str ong control of Islamic rulers? BART-large (Baseline) [27] What was the main difference between the medieval period in India and the Chalcolithic period? ICL (/u1D458 = 3) What were the major cultural inﬂuences of the Islamic rulers during India’s medieval period? ICL (/u1D458 = 5) How did Islamic rulers contribute to the architectural deve lopment in India during the medieval era? ICL (/u1D458 = 7) What were the key impacts of the medieval Islamic rulers on In dian art and architecture, and how are these contributions reﬂected today? RAG (/u1D458 = 5) How did medieval Indian rulers inﬂuence the country’s cultu ral and artistic heritage? Hybrid Model (/u1D458 = 5, /u1D45A = 5) In what ways did the Islamic rulers of medieval India shape th e cultural and architectural landscape, and what are some prominent examples of their contributions? Table 4: A sample (context and gold standard question) from t he Economics subject of the EduProbe dataset [27], includin g questions generated by various models and settings. The highlighted color indicates irrelevant or unanswerable questions according to the context. Context: Purchasing power parity (PPP) is an economic indicator that signiﬁes the purchasing power of the currencies of various nations of the world against each other. It helps i n comparing living standards between diﬀerent countries an d estimating economic productivity. Gold Standard Question: What does purchasing power parity do? Model Generated Question T5-large (Baseline) [27] What is the importance of a country’sdevelopment index? How does it help the poor? BART-large (Baseline) [27] What is the importance of ease of living standards in simple words, and why is it important? ICL (/u1D458 = 3) What is the function of purchasing power parity (PPP) in comp aring currencies? ICL (/u1D458 = 5) How does purchasing power parity (PPP) help in comparing liv ing standards between countries? ICL (/u1D458 = 7) What role does purchasing power parity (PPP) play in compari ng economic productivity between diﬀerent countries? RAG (/u1D458 = 5) What is the purpose of purchasing power parity (PPP) in asses sing economic productivity? Hybrid Model (/u1D458 = 5, /u1D45A = 5) How does purchasing power parity (PPP) assist in comparing l iving standards and currencies across nations? and BERTScore. On the other hand, the Hybrid Model ( /u1D458 = 5, /u1D45A = 5) shows superior performance in human evaluation metrics, i n- cluding grammaticality, appropriateness, relevance, and complexity. The combination of retrieval and in-context learning techn iques allows the Hybrid Model to generate questions with greater d epth and alignment with educational contexts. Overall, both ICL and the Hybrid Model signiﬁcantly outperform the previous base line models, T5-large and BART-large, in both automated and huma n evaluations. These advanced models surpass baselines in ge nerat- ing more relevant and contextually appropriate questions, demon- strating their enhanced capabilities in generating educational ques- tions. In future work, we plan to expand evaluations to diverse datasets and educational domains to better assess the generalizability of our models. We also aim to incorporate feedback from educators a nd students to reﬁne our approaches, ensuring they meet curric ulum Leveraging ICL and RAG for Automatic /Q_uestion Generation in Ed ucational Domains Conference’17, July 2017, Washington, DC , USA standards and eﬀectively support learning outcomes. In add ition, we plan to explore other state-of-the-art LLMs, such as Gemi ni and Llama-2-70B, for in-context learning. These initiatives will im- prove the development of more eﬀective and contextually accurate AQG systems. ACKNOWLEDGMENTS The ﬁrst two authors gratefully acknowledge the Ministry of Hu- man Resource Development (MHRD), Government of India, for funding their doctoral research. The authors also thank the anony- mous reviewers for their valuable feedback and insightful s ugges- tions, which signiﬁcantly enhanced this work. Additionall y, they acknowledge the use of ChatGPT for proofreading, polishing , and enhancing the clarity and style of their initial drafts, as w ell as for reviewing the ﬁnal manuscript prior to submission. REFERENCES [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad , Ilge Akkaya, Flo- rencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sa m Altman, Shya- mal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemo yer, and Marjan Ghazvininejad. 2022. In-context Examples Selection for Ma chine Translation. arXiv:2212.02437 [cs.CL] https://arxiv.org/abs/2212.0 2437 [3] Said Al Faraby, Adiwijaya Adiwijaya, and Ade Romadhony. 2023. Review on neural question generation for education purposes. International Journal of Ar- tiﬁcial Intelligence in Education (2023), 1–38. [4] Fernando Alva-Manchego, Carolina Scarton, and Lucia Sp ecia. 2021. The (Un)Suitability of Automatic Evaluation Metrics for Te xt Sim- pliﬁcation. Computational Linguistics 47, 4 (Dec. 2021), 861–889. https://doi.org/10.1162/coli_a_00418 [5] Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pen gfei Liu, and Graham Neubig. 2020. Re-evaluating Evaluation in Text Summ arization. In Proceedings of the 2020 Conference on Empirical Methods in Na tural Lan- guage Processing (EMNLP) , Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistic s, Online, 9347–9359. https://doi.org/10.18653/v1/2020.emnlp-main.751 [6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah , Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Giri sh Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeﬀrey Wu, Cl emens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben- jamin Chess, Jack Clark, Christopher Berner, Sam McCandlis h, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learn- ers. arXiv:2005.14165 [cs.CL] https://arxiv.org/abs/20 05.14165 [7] Aniket Deroy, Subhankar Maity, and Sudeshna Sarkar. 202 4. Mirror: A novel ap- proach for the automated evaluation of open-ended question generation. arXiv preprint arXiv:2410.12893 (2024). [8] Alexander Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nalla pati, and Bing Xiang. 2020. Template-Based Question Generation from Retrieved S entences for Im- proved Unsupervised Question Answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Associa tion for Computational Linguistics, Online, 4508–4513. https://doi.org/10.186 53/v1/2020.acl-main.413 [9] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengy un Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A Survey on RAG Meeting LLMs : To- wards Retrieval-Augmented Large Language Models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Minin g (Barcelona, Spain) (KDD ’24). Association for Computing Machinery, New York, NY, USA, 6491–6501. https://doi.org/10.1145/3637528.3671470 [10] Huanli Gong, Liangming Pan, and Hengchang Hu. 2022. KHA NQ: A Dataset for Generating Deep Questions in Education. In Proceedings of the 29th Inter- national Conference on Computational Linguistics , Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohas hi, Patrizia Pag- gio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (Eds.). International Com- mittee on Computational Linguistics, Gyeongju, Republic o f Korea, 5925–5938. https://aclanthology.org/2022.coling-1.518 [11] Guher Gorgun and Okan Bulut. 2024. Exploring quality cr iteria and evaluation methods in automated question generation: A comprehensive survey. Education and Information Technologies (2024), 1–32. [12] Lifeng Han, Serge Gladkoﬀ, Gleb Erofeev, Irina Sorokin a, Betty Galiano, and Goran Nenadic. 2024. Neural machine translation of clinical text: an empirical in- vestigation into multilingual pre-trained language models and transfer-learning. Frontiers in Digital Health 6 (2024). https://doi.org/10.3389/fdgth.2024.1211564 [13] Michael Heilman and Noah A. Smith. 2010. Good Question! Statistical Rank- ing for Question Generation. In Human Language Technologies: The 2010 An- nual Conference of the North American Chapter of the Associa tion for Compu- tational Linguistics , Ron Kaplan, Jill Burstein, Mary Harper, and Gerald Penn (Eds.). Association for Computational Linguistics, Los An geles, California, 609– 617. https://aclanthology.org/N10-1086 [14] Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Sem ih Yavuz, and Richard Socher. 2020. A Simple Language Model for Task- Oriented Dialogue. In Advances in Neural Information Process- ing Systems , H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 20179–2 0191. https://proceedings.neurips.cc/paper_ﬁles/paper/202 0/ﬁle/e946209592563be0f01c844ab2170f0c-Paper [15] Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sat hyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chuntin g Zhou. 2023. Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. In Findings of the Association for Computa- tional Linguistics: ACL 2023 . Association for Computational Linguistics. https://doi.org/10.18653/v1/2023.ﬁndings-acl.537 [16] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi- Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Acti ve Retrieval Aug- mented Generation. arXiv:2305.06983 [cs.CL] https://arx iv.org/abs/2305.06983 [17] Jeﬀ Johnson, Matthijs Douze, and Hervé Jégou. 2017. Bil lion-scale similarity search with GPUs. arXiv:1702.08734 [cs.CV] https://arxiv .org/abs/1702.08734 [18] Kalpesh Krishna and Mohit Iyyer. 2019. Generating Ques tion-Answer Hi- erarchies. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , Anna Korhonen, David Traum, and Lluís Màrquez (Eds.). Association for Computational Linguistics, Flore nce, Italy, 2321–2334. https://doi.org/10.18653/v1/P19-1224 [19] Ghader Kurdi, Jared Leo, Bijan Parsia, Uli Sattler, and Salam Al-Emari. 2020. A systematic review of automatic question generation for edu cational purposes. International Journal of Artiﬁcial Intelligence in Educat ion 30 (2020), 121–204. [20] J Landis. 1977. The Measurement of Observer Agreement f or Categorical Data. Biometrics (1977). [21] Alon Lavie and Abhaya Agarwal. 2007. METEOR: An Automat ic Metric for MT Evaluation with High Levels of Correlation with Human Jud gments. In Proceedings of the Second Workshop on Statistical Machine Tr anslation, Chris Callison-Burch, Philipp Koehn, Cameron Shaw Fordyce, and C hristof Monz (Eds.). Association for Computational Linguistics, Pragu e, Czech Republic, 228– 231. https://aclanthology.org/W07-0734 [22] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine jad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer . 2020. BART: Denoising Sequence-to-Sequence Pre-training for NaturalLanguage Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Compu tational Linguistics, Online, 7871–7880. https://doi.org/10.18653/v1/2020.a cl-main.703 [23] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio P etroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen- tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. R etrieval- Augmented Generation for Knowledge-Intensive NLP Tasks. I n Advances in Neural Information Processing Systems , H. Larochelle, M. Ranzato, R. Had- sell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associa tes, Inc., 9459–9474. https://proceedings.neurips.cc/paper_ﬁles/paper/202 0/ﬁle/6b493230205f780e1bc26945df7481e5-Paper [24] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Eval uation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 74–81. https://aclanthology.org/W04- 1013 [25] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Ten ghao Huang, Mohit Bansal, and Colin A Raﬀel. 2022. Few-Shot Parameter-E ﬃcient Fine- Tuning is Better and Cheaper than In-Context Learning. In Advances in Neural Information Processing Systems , S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 1950–1965. https://proceedings.neurips.cc/paper_ﬁles/paper/202 2/ﬁle/0cde695b83bd186c1fd456302888454c-Paper- [26] Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, and Y ang Liu. 2021. Retrieval-Augmented Generation for Code Summarization vi a Hybrid GNN. arXiv:2006.05405 [cs.LG] https://arxiv.org/abs/2006.0 5405 [27] Subhankar Maity, Aniket Deroy, and Sudeshna Sarkar. 20 24. Harnessing the Power of Prompt-based Techniques for Generating School -Level Ques- tions using Large Language Models. In Proceedings of the 15th Annual Meet- ing of the Forum for Information Retrieval Evaluation (Panjim, India) (FIRE ’23). Association for Computing Machinery, New York, NY, USA, 30 –39. https://doi.org/10.1145/3632754.3632755 [28] Subhankar Maity, Aniket Deroy, and Sudeshna Sarkar. 20 24. Investigating Large Language Models for Prompt-Based Open-Ended Question Gene ration in the Conference’17, July 2017, Washington, DC, USA Maity et al. Technical Domain. SN Computer Science 5, 8 (2024), 1–32. [29] Subhankar Maity, Aniket Deroy, and Sudeshna Sarkar. 20 24. A Novel Multi- Stage Prompting Approach for Language Agnostic MCQ Generation Using GPT. In Advances in Information Retrieval , Nazli Goharian, Nicola Tonellotto, Yulan He, Aldo Lipani, Graham McDonald, Craig Macdonald, and Iadh Ounis (Eds.). Springer Nature Switzerland, Cham, 268–277. [30] Subhankar Maity, Aniket Deroy, and Sudeshna Sarkar. 20 25. Can large language models meet the challenge of generating school-level questions? Computers and Education: Artiﬁcial Intelligence (2025), 100370. [31] Aristides Milios, Siva Reddy, and Dzmitry Bahdanau. 20 23. In-Context Learning for Text Classiﬁcation with Many Labels. In Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP, Dieuwke Hupkes, Verna Dankers, Khuyagbaatar Batsuren, Koustuv Sin ha, Amirhos- sein Kazemnejad, Christos Christodoulopoulos, Ryan Cotte rell, and Elia Bruni (Eds.). Association for Computational Linguistics, Singapore, 173–184. https://doi.org/10.18653/v1/2023.genbench-1.14 [32] Preksha Nema and Mitesh M. Khapra. 2018. Towards a Bette r Metric for Evalu- ating Question Generation Systems. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing , Ellen Riloﬀ, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for C omputational Linguis- tics, Brussels, Belgium, 3950–3959. https://doi.org/10. 18653/v1/D18-1429 [33] Samiran Pal, Kaamraan Khan, Avinash Kumar Singh, Subha sish Ghosh, Tapas Nayak, Girish Palshikar, and Indrajit Bhattacharya. 2022. Weakly Supervised Context-based Interview Question Generation. In Proceedings of the 2nd Work- shop on Natural Language Generation, Evaluation, and Metri cs (GEM) , Antoine Bosselut, Khyathi Chandu, Kaustubh Dhole, Varun Gangal, Sebastian Gehrmann, Yacine Jernite, Jekaterina Novikova, and Laura Perez-Beltrachini (Eds.). Associa- tion for Computational Linguistics, Abu Dhabi, United Arab Emirates (Hybrid), 43–53. https://doi.org/10.18653/v1/2022.gem-1.4 [34] Feifei Pan, Mustafa Canim, Michael Glass, Alﬁo Gliozzo , and James Hendler. 2022. End-to-End Table Question Answering via Retrieval-A ugmented Genera- tion. arXiv:2203.16714 [cs.CL] https://arxiv.org/abs/2 203.16714 [35] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jin g Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. I n Proceed- ings of the 40th Annual Meeting of the Association for Comput ational Lin- guistics, Pierre Isabelle, Eugene Charniak, and Dekang Lin (Eds.). A ssocia- tion for Computational Linguistics, Philadelphia, Pennsy lvania, USA, 311–318. https://doi.org/10.3115/1073083.1073135 [36] Maja Popović. 2015. chrF: character n-gram F-score for automatic MT eval- uation. In Proceedings of the Tenth Workshop on Statistical Machine Tra ns- lation, Ondřej Bojar, Rajan Chatterjee, Christian Federmann, Bar ry Had- dow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pa vel Pecina (Eds.). Association for Computational Linguistics, Lisbo n, Portugal, 392–395. https://doi.org/10.18653/v1/W15-3049 [37] Muh. Putra Pratama, Rigel Sampelolo, and Hans Lura. 202 3. REVOLUTION- IZING EDUCATION: HARNESSING THE POWER OF ARTIFICIAL INTELL I- GENCE FOR PERSONALIZED LEARNING. KLASIKAL : JOURNAL OF ED- UCATION, LANGUAGE TEACHING AND SCIENCE 5, 2 (Aug. 2023), 350–357. https://doi.org/10.52208/klasikal.v5i2.877 [38] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee , Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Ex plor- ing the Limits of Transfer Learning with a Uniﬁed Text-to-Te xt Trans- former. Journal of Machine Learning Research 21, 140 (2020), 1–67. http://jmlr.org/papers/v21/20-074.html [39] Mathieu Ravaut, Aixin Sun, Nancy Chen, and Shaﬁq Joty. 2 024. On Con- text Utilization in Summarization with Large Language Mode ls. In Proceed- ings of the 62nd Annual Meeting of the Association for Comput ational Linguis- tics (Volume 1: Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangk ok, Thailand, 2764– 2781. https://aclanthology.org/2024.acl-long.153 [40] Ehud Reiter. 2018. A Structured Review of the Validity o f BLEU. Computational Linguistics 44, 3 (Sept. 2018), 393–401. https://doi.org/10.1162/col i_a_00322 [41] Alireza Salemi and Hamed Zamani. 2024. Evaluating Retr ieval Quality in Retrieval-Augmented Generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Informatio n Retrieval (Wash- ington DC, USA) (SIGIR ’24). Association for Computing Machinery, New York, NY, USA, 2395–2400. https://doi.org/10.1145/3626772.36 57957 [42] Shamane Siriwardhana, Rivindu Weerasekera, Elliott W en, Tharindu Kalu- arachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Impro ving the Domain Adaptation of Retrieval Augmented Generation (RAG) Modelsfor Open Domain Question Answering. Transactions of the Association for Computational Linguis - tics 11 (2023), 1–17. https://doi.org/10.1162/tacl_a_00530 [43] Katira Soleymanzadeh. 2017. Domain Speciﬁc Automatic Question Genera- tion from Text. In Proceedings of ACL 2017, Student Research Workshop , Allyson Ettinger, Spandana Gella, Matthieu Labeau, Cecilia Ovesdo tter Alm, Marine Carpuat, and Mark Dredze (Eds.). Association for Computati onal Linguistics, Vancouver, Canada, 82–88. https://aclanthology.org/P17 -3014 [44] Tim Steuer, Anna Filighera, Thomas Tregel, and André Mi ede. 2022. Educa- tional Automatic Question Generation Improves Reading Com prehension in Non-native Speakers: A Learner-Centric Case Study. Frontiers in Artiﬁcial In- telligence 5 (2022). https://doi.org/10.3389/frai.2022.900304 [45] Yuting Tang, Ratish Puduppully, Zhengyuan Liu, and Nan cy Chen. 2023. In- context Learning of Large Language Models for Controlled Di alogue Summa- rization: A Holistic Benchmark and Empirical Analysis. In Proceedings of the 4th New Frontiers in Summarization Workshop , Yue Dong, Wen Xiao, Lu Wang, Fei Liu, and Giuseppe Carenini (Eds.). Association for Computa tional Linguistics, Singapore, 56–67. https://doi.org/10.18653/v1/2023.ne wsum-1.6 [46] Asahi Ushio, Fernando Alva-Manchego, and Jose Camacho -Collados. 2022. Gen- erative Language Models for Paragraph-Level Question Gene ration. In Proceed- ings of the 2022 Conference on Empirical Methods in Natural L anguage Pro- cessing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). As sociation for Computational Linguistics, Abu Dhabi, United Arab Emir ates, 670–688. https://doi.org/10.18653/v1/2022.emnlp-main.42 [47] Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang , Manasi Desh- pande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-Augmen ted Gen- eration with Knowledge Graphs for Customer Service Questio n Answer- ing. In Proceedings of the 47th International ACM SIGIR Conference o n Re- search and Development in Information Retrieval (Washington DC, USA) (SIGIR ’24). Association for Computing Machinery, New York, NY, USA, 29 05–2909. https://doi.org/10.1145/3626772.3661370 [48] Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text Generatio n with BERT. In International Conference on Learning Representations . https://openreview.net/forum?id=SkeHuCVFDr [49] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2023. ToolQA: A Dataset for LLM Question Answering with External Tools. In Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 5011 7–50143. https://proceedings.neurips.cc/paper_ﬁles/paper/202 3/ﬁle/9cb2a7495900f8b602cb10159246a016-Paper- [50] Indre Zliobaite, Albert Bifet, Mohamed Gaber, Bogdan G abrys, Joao Gama, Leandro Minku, and Katarzyna Musial. 2012. Next challenges for adap- tive learning systems. SIGKDD Explor. Newsl. 14, 1 (dec 2012), 48–55. https://doi.org/10.1145/2408736.2408746