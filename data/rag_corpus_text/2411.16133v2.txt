Context Awareness Gate For Retrieval Augmented Generation Mohammad Hassan Heydari Computer Engineering Faculty University of Isfahan Isfahan, Iran mheydarii@mehr.ui.ac.ir Arshia Hemmat Computer Engineering Faculty University of Isfahan Isfahan, Iran arshiahemmat@mehr.ui.ac.ir Erfan Naman Computer Engineering Facu. University of Isfahan Isfahan, Iran erfannaman@mehr.ui.ac.ir Afsaneh Fatemi Computer Engineering Faculty University of Isfahan Isfahan, Iran a fatemi@eng.ui.ac.ir Abstract—Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach to mitigate the limitations of large language models (LLMs) in answering domain-specific questions. Previous research has predominantly focused on improving the accuracy and quality of retrieved data chunks to enhance the overall performance of the generation pipeline. However, despite ongoing advancements, the critical issue of retrieving irrelevant information—which can impair a model’s ability to utilize its internal knowledge effectively—has received minimal attention. In this work, we investigate the impact of retrieving irrelevant information in open-domain question answering, highlighting its significant detrimental effect on the quality of LLM outputs. To address this challenge, we propose the Context Awareness Gate (CAG) architecture, a novel mechanism that dynamically adjusts the LLM’s input prompt based on whether the user query necessitates external context retrieval. Additionally, we introduce the Vector Candidates method, a core mathematical component of CAG that is statistical, LLM-independent, and highly scalable. We further examine the distributions of relationships between contexts and questions, presenting a statistical analysis of these distributions. This analysis can be leveraged to enhance the context retrieval process in retrieval-augmented generation (RAG) systems 1. Index Terms —Retrieval-Augmented Generation, Hallucina- tion, Large Language Models, Open Domain Question Answering I. I NTRODUCTION Retrieval-augmented generation (RAG) has emerged as a leading approach for implementing question-answering sys- tems that require intensive domain-specific knowledge [1]. This method allows for the utilization of customized datasets to generate answers, grounded in the information provided by those datasets. However, the effectiveness of the retrieval component within RAG pipelines is critical, as it directly influences the reliability and quality of the generated outputs [2], [3]. In efforts to enhance the quality of the retrieval component in RAG pipelines, research has demonstrated that transforming the user’s input query into varying levels of abstraction before conducting the document search can significantly improve the relevance of the retrieved data. Several methods have been proposed, including query expansion into multi-query searches, chain of verification [4], [5], pseudo-context search 1The code is publicly available at: https://github.com/heydaari/CAG [6] and query transformation [7], [8], [9], [10]. These ap- proaches contribute to more accurate and effective retrieval of information. Despite ongoing efforts to develop more reliable retrieval methods for extracting relevant data chunks, many question- answering systems do not solely rely on local or domain- specific datasets for answering user queries. In addition to domain-specific user queries, many input queries do not necessitate retrieval from local datasets, which reduces the scalability and reliability of question-answering systems [11]. To tackle this limitation, retrieval methods based on query classification and routing mechanisms have proven effective in enhancing retrieval accuracy by directing the search toward a set of documents closely related to the user’s query [10]. However, in our study, we demonstrate that even with semantic routing, the probability of retrieving irrelevant information remains non-negligible, particularly when dealing with a broad domain of potential queries. Due to the inherently local search mechanism of Retrieval- Augmented Generation (RAG) systems [1], [12], even for queries that are largely irrelevant, the pipeline will still return a set number of passages. While existing research has made strides in addressing the challenge of imperfect data retrieval [11], [13], [14], the issue of broad-domain question answering in RAG systems has received relatively little attention. Many queries submitted to RAG-enhanced question- answering (QA) systems do not require data retrieval, such as daily conversations, general knowledge questions, or questions that large language models (LLMs) themselves can answer using their internal knowledge [10], [11], [15], [16]. Retrieving passages for all input queries, especially in these cases, can significantly diminish the retrieval precision [11], [16] and the context relevancy [17], often rendering them entirely irrelevant. To address this issue, we propose a novel context-aware gate architecture for RAG-enhanced systems which is highly scalable of dynamically routing the LLM input prompt to increase the quality of pipeline outputs. For better comprehension of our work, we highlight three main contributions in this study: • Context Awareness Gate (CAG) : We introduce a novel gate architecture that significantly broadens the domain arXiv:2411.16133v2 [cs.LG] 6 Jan 2025 accessibility of RAG systems. CAG leverages both query transformation and dynamic prompting to enhance the reliability of RAG pipelines in both open-domain and closed-domain question answering tasks. • Vector Candidates (VC) : We propose a statistical se- mantic analysis algorithm that improves semantic search and routing by utilizing the concept of pseudo-queries and in-dataset embedding distributions. • Context Retrieval Supervision Benchmark (CRSB) Dataset: Alongside our technical and statistical investiga- tions, we introduce the CRSB dataset, which consists of data from 17 diverse fields. We study the inner context- query distributions of this rich dataset and demonstrate the effectiveness and scalability of Vector Candidates on practical QA systems 2 . Fig. 1. Context Awareness Gate (CAG) architecture for open domain questions answering II. R ELATED WORK improving both retrieval quality and the outputs of large language models. Query2Doc [18] and HyDE [6] generate pseudo-documents based on the input query and utilize these for semantic search instead of the query itself. RQ-RAG [19] decomposes complex queries into simpler sub-queries, enhancing retrieval performance. The Rewrite-Retrieve-Read framework [7] employs query rewriting to improve the match between queries and relevant documents. Additionally, some studies suggest that for queries answerable by the large lan- guage model (LLM) based on its internal knowledge, query classification using a smaller language model can benefit overall pipeline performance [10]. In terms of improving model output quality, RobustRAG [3] investigates the vulnerability of RAG-based systems to mali- cious passages injected into the knowledge database. Conflict- Disentangle Contrastive Decoding (CD2) [20] proposes a 2The CRSB dataset is available at: https://huggingface.co/datasets/ heydariAI/CRSB framework to reconcile conflicts between an LLM’s internal knowledge and external knowledge stored in a database. Yu et al. (2024) [21] argue that simply adding more context to the LLM input prompt does not necessarily improve performance. In a recent study, Wang et al. (2024) [11] show that when retrieval precision is below 20%, RAG is not beneficial for QA systems. They highlight that when retrieval precision approaches zero, the RAG pipeline performs significantly worse than a pipeline without RAG. As a relevant method to our work, Adaptive-RAG [16] uses a smaller language model to adaptively switch the pipeline inputs whether it needs context retrieval for RAG or the question answering language model itself can answer the input query based on its internal knowledge. III. A PPROACH To address the challenges associated with retrieving irrele- vant information [11], we propose the Context Awareness Gate (CAG) architecture, which utilizes Vector Candidates as its primary statistical method for query classification. CAG sig- nificantly improves the performance of open-domain question- answering systems by dynamically adjusting the input prompt for the LLM, transitioning from RAG-based context prompts to Few Shot, Chain-of-Thought (CoT) [4], [5], and other methodologies. Consequently, the LLM responds to user queries based on its internal knowledge base. A. Context Awareness Gate (CAG) To address the issue of retrieving irrelevant data chunks for each input query, one solution is to ask a supervising large language model (LLM) to classify whether the query should prompt a retrieval-augmented generation (RAG) or a RAG-free response [16]. This involves determining whether the input query falls within the scope of the local database. However, a significant limitation of this approach is the high computational cost of using an LLM with billions of parame- ters for a relatively simple task like query classification. Even smaller language models come with their own challenges, such as hallucination and limited reasoning capabilities [11]. To mitigate these issues,we propose an efficient yet highly effective statistical approach, known as Vector Candidates. The key idea behind Vector Candidates is to generate pseudo- queries for each document in the set, then calculate the dis- tribution of embeddings and their similarities. By comparing the input query to this distribution, it is possible to estimate whether context retrieval is necessary with a certain level of probability. If the input query is far from this distribution, it is recommended not to retrieve any context and instead reformulate the LLM input prompt into a simpler few-shot question-answering task, rather than utilizing RAG. The over- all architecture of CAG is presented in figure 1 The limitation of this approach may appear to be the necessity of generating numerous pseudo-queries for a local dataset. However, when comparing it to LLM supervision [16], the complexity of the Vector Candidates method, which operates on a set of contexts with C contexts and N pipeline input requests, reveals a significant advantage. Specifically, the complexity of the Vector Candidates method is O(1), as it relies solely on the number of contexts, regardless of the number of input requests ( This happens when we disable the query transformation as one of the CAG steps ) . In contrast, the complexity of LLM supervision [16] scales linearly with the number of input requests, represented as O(N ). This indicates that while generating pseudo-queries may seem cumbersome, the overall efficiency of the Vector Candidates approach is superior in scenarios with multiple input requests. Alongside all the steps involved in the Vector Candidates approach, the process begins with transforming the user’s input query into a more appropriate format to enhance the quality of semantic search. This query transformation is critical as it ensures that the input is optimized for better alignment with the embeddings used in the retrieval process. After this transformation, the Vector Candidates method is applied to assess the relevance of context retrieval. Following query transformation and Vector Candidates anal- ysis, the Context Awareness Gate (CAG) system dynamically adapts the input query into a suitable prompt. This involves determining whether context retrieval is necessary or if the LLM can answer the query based on its internal knowledge, utilizing techniques like Chain of Thought (CoT) reasoning [4], [5], agents, or other methods. B. Vector Candidates To address the issues of using a LLM for context su- pervision , we propose a statistical approach based on the distributions of emmbedings of contexts and pseudo-queries. Algorithm 1 Vector Candidates Algorithm Require: Contexts C, Pseudo Queries Q, Policy P , Threshold T , Input Query q Ensure: A classification (True or False) 1: Compute dataset distributions based on cosine similarity: D ← C · Q ∥C∥∥Q∥ 2: Compute input query similarities with contexts: d ← C · q ∥C∥∥q∥ 3: if max(d) > P (D) − T then 4: return True 5: else 6: return False 7: end if Based on the proposed method in Algorithm (1), we first calculate the cosine similarity distributions between the con- texts and pseudo-queries. Then, we compute the similarity between the user’s original query and each context in the dataset. If the maximum similarity found between the orig- inal query and the contexts falls within the distribution of context-pseudo-query similarities, this suggests that retrieval- augmented generation (RAG) might be beneficial. Otherwise, it is more efficient to exclude RAG from the pipeline. This approach is grounded in our statistical analysis and the results presented by Wang et al. [11]. To measure the relevancy between the described distribu- tions and the user query, we apply a policy P , which is a hyperparameter derived from common statistical metrics such as minimum, mean, median, or quartiles. Additionally, we define a threshold T , which serves as another hyperparameter, to create a risk range for decision-making. This threshold helps in determining the confidence level for whether context retrieval should be applied, balancing the trade-off between precision and recall in the retrieval process. C. Context Retrieval Supervision Bench (CRSB) We introduce the Context Retrieval Supervision Bench (CRSB) dataset, which can be used to evaluate the per- formance of context-aware systems and retrieval-augmented generation (RAG) semantic routers. The CRSB contains 17 different topics, with each context associated with 3 pseudo- queries. This design allows the CRSB to encompass a total of 5,100 question-answer pairs. With a correct permutation, CRSB can offer more than 83000 context-query pairs to evaluate the context awareness systems and semantic routing pipelines.. IV. E XPERIMENTS To analyze the statistical relationships between relevant and irrelevant context-query pairs, we examine the distribution of collected contexts and generated pseudo-queries. We begin by gathering 1,700 contexts across 17 distinct topics. For each context, we prompt the Gemma 2 9B language model [22] to generate three pseudo-queries.We applied all−mpnet−base− v2 as our embedding model and create a vector database of contexts and pseudo-queries embeddings [23]. We then calculate the similarity distributions for Positive (relevant) context-query pairs, where the queries require con- text retrieval, as well as for Negative (irrelevant) context- query pairs. With appropriate permutations, we analyze 83,000 Positive and Negative context-query pairs. Various statistical metrics are applied to these distributions, and the results are presented in Table I. As demonstrated in Table I, over 95% of positive context- question pairs exhibit a cosine similarity greater than 0.55, while more than 95% of negative context-query pairs have a cosine similarity lower than 0.21. The median value for the positive set exceeds 0.71, whereas the median for the negative set is below 0.04. Although the maximum value of the negative set is higher than the minimum value of the positive set, the density of the positive distribution is greater than that of the negative distribution approximately 98.7% of the time. These statistics provide a comprehensive understanding that, by utilizing these metrics as a policy, we can develop TABLE I STATISTICAL ANALYSIS ON CRSB Policy Positive Negative Minimum 0.110 -0.193 5th Percentile 0.554 -0.052 1st Quartile 0.662 -0.000 Mean 0.705 0.047 Median 0.716 0.039 3rd Quartile 0.762 0.086 95th Percentile 0.836 0.219 Maximum 0.912 0.654 a statistical method that is highly effective in classifying user queries to establish dynamic prompts, as discussed in previous sections. Due to the algebraic nature of our method, we have in- tegrated advanced high-performance techniques for parallel computing and accelerated linear algebra through the JAX framework [24]. Leveraging JAX’s ability to handle automatic differentiation and just-in-time compilation (JIT) seamlessly, we are able to optimize the underlying computations for both CPU and GPU architectures. This not only allows for faster execution but also ensures scalability across large datasets and complex models. Our approach significantly improves the efficiency in computing the distributions of the dataset, offering a more streamlined and scalable solution for high- dimensional data analysis. V. R ESULTS To evaluate the capabilities and performance of the Context Awareness Gate (CAG), we applied this architecture to the SQuAD dataset [25] and our proposed benchmark, CRSB. We implemented an open-domain question-answering pipeline to assess the outcomes of CAG under a specified scenario: • Setting CRSB as the local database while querying from SQuAD [25]. The pipeline should identify irrelevant queries to this local database and refrain from using RAG, instead generating a few-shot response using the LLM input prompt. In this scenario, we evaluate the pipeline outputs using two metrics from RAGAS: context relevancy and answer relevancy [17].Due to the absence of retrieved context for irrelevant queries to the dataset, we ask our model to generate a pseudo-context that answers the query and then calculate the context relevancy based on this generated context. Our question-answering base model was Qwen-2.5-7B [26] and we applied Llama-3.2-3B as our answer relevancy evaluator [27]. To demonstrate the effectiveness of the proposed pipeline, we compare the results of the classic RAG [1], HyDE [6], query transformation [7], Adaptive-RAG [16] and the proposed CAG. For the evaluation step, we applied both RAG and CAG. We set 95% density distribution as the Policy P and we set the threshold T to 0 as the Vector Candidates hyperparameters. Our experimental results shown in Table II clearly demon- strate that classic Retrieval-Augmented Generation (RAG) [1] TABLE II EVALUATION OF CONTEXT AWARENESS GATE (CAG) ON SQUAD AND CRSB Context Relevancy Answer Relevancy RAG [1] 0.016 0.206 Query Rewriting [7] 0.026 0.147 HyDE [6] 0.043 0.228 Adaptive-RAG [16] 0.334 0.613 CAG (Ours) 0.338 0.709 is unable to generalize effectively for open-domain question answering. Both Query Rewriting [7] and HyDE [6], while providing improvements in context retrieval, share the same limitation as classic RAG [1] in failing to address the broader challenges posed by open-domain queries. As illustrated in our results, the context relevancy scores for these methods are close to zero (0.016 for RAG [1], 0.026 for Query Rewriting [7], and 0.043 for HyDE [6]), indicating that they are unable to adapt the pipeline outputs in scenarios where the input query does not align with the local dataset. Consequently, their ability to generate accurate and relevant answers is also compromised, as reflected by their low answer relevancy scores (0.206 for RAG [1], 0.147 for Query Rewriting [7], and 0.228 for HyDE [6]). In contrast, Adaptive-RAG [16] and CAG (our proposed method) show a marked improvement in both context rele- vancy and answer relevancy. The results for Adaptive-RAG [16] ( context relevancy: 0.334, answer relevancy: 0.613) demonstrate its ability to adaptively decide when to trigger context retrieval based on query classification. Similarly, CAG achieves even higher performance (context relevancy: 0.338, answer relevancy: 0.709), suggesting that both methods suc- cessfully enhance the pipeline’s ability to dynamically adjust the input to the language model, either by incorporating external context or by relying solely on the internal knowledge of the model, depending on the query’s nature. However, a close examination of the computational costs associated with each approach reveals key differences. While Adaptive-RAG [16] relies on an additional large language model (LLM) to supervise the decision of whether context re- trieval is needed, this introduces significant overhead in terms of memory usage and inference time. As detailed in previous works, such LLM-based supervision requires additional model loading, which can considerably slow down the pipeline and increase resource consumption. On the other hand, our CAG method offers a more efficient and scalable solution by eliminating the need for LLM super- vision. Instead, CAG uses a statistical approach based on the distributions of the local database, allowing it to make context retrieval decisions in a highly efficient manner. This method is LLM-free and leverages a lightweight, statistical computation that can be hundreds or even thousands of times faster than LLM-based adaptations, offering significant advantages in both speed and scalability without compromising accuracy. Thus, the results not only demonstrate the effectiveness of CAG in addressing the challenges of open-domain question answering, but also highlight its computational efficiency, making it a highly scalable solution for practical deployments of RAG-based systems. VI. F UTURE DIRECTION This work opens up several promising avenues for further research and enhancement of Context Awareness Gate (CAG) in open-domain question answering systems: • Incorporating Best Practices in Information Retrieval: Future work could focus on integrating the methodologies outlined in [15] to refine CAG’s information retrieval pipeline. Specifically, these practices could optimize how CAG filters and ranks relevant information, leading to even more precise data selection. By enhancing the gran- ularity of relevance scoring during retrieval, CAG could further improve its ability to identify and utilize the most contextually pertinent chunks of information, boosting both retrieval accuracy and downstream performance in generating high-quality responses. • Replacing Pseudo-Context Search with Pseudo-Query Search: While the pseudo-context search strategy pro- posed in HyDE [6] has been effective, this study intro- duces the concept of pseudo-query search as a potentially more robust alternative. Future research could explore the efficacy of this approach across various datasets and domains. A systematic evaluation of the pseudo-query search could reveal whether it generalizes better across different question-answering tasks, especially in complex or multi-turn dialogues, where context awareness is cru- cial for maintaining conversation coherence. REFERENCES [1] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval- augmented generation for knowledge-intensive nlp tasks,” Advances in Neural Information Processing Systems , vol. 33, pp. 9459–9474, 2020. [2] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language models in retrieval-augmented generation,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 17 754– 17 762. [3] C. Xiang, T. Wu, Z. Zhong, D. Wagner, D. Chen, and P. Mittal, “Certifiably robust rag against retrieval corruption,” arXiv preprint arXiv:2405.15556, 2024. [4] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. Weston, “Chain-of-verification reduces hallucination in large language models,” arXiv preprint arXiv:2309.11495 , 2023. [5] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le, D. Zhou et al. , “Chain-of-thought prompting elicits reasoning in large language models,” Advances in neural information processing systems , vol. 35, pp. 24 824–24 837, 2022. [6] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval without relevance labels,” arXiv preprint arXiv:2212.10496 , 2022. [7] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, “Query rewrit- ing for retrieval-augmented large language models,” arXiv preprint arXiv:2305.14283, 2023. [8] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, D. Xu, T. Xu, and E. Chen, “Large language model based long-tail query rewriting in taobao search,” in Companion Proceedings of the ACM on Web Conference 2024, 2024, pp. 20–28. [9] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le, and D. Zhou, “Take a step back: Evoking reasoning via abstraction in large language models,” arXiv preprint arXiv:2310.06117 , 2023. [10] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” arXiv preprint arXiv:2312.10997 , 2023. [11] F. Wang, X. Wan, R. Sun, J. Chen, and S. ¨O. Arık, “Astute rag: Overcoming imperfect retrieval augmentation and knowledge conflicts for large language models,” arXiv preprint arXiv:2410.07176 , 2024. [12] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval augmented language model pre-training,” in International conference on machine learning . PMLR, 2020, pp. 3929–3938. [13] Z. Jin, P. Cao, Y . Chen, K. Liu, X. Jiang, J. Xu, Q. Li, and J. Zhao, “Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models,” arXiv preprint arXiv:2402.14409, 2024. [14] A. Mallen, A. Asai, V . Zhong, R. Das, D. Khashabi, and H. Hajishirzi, “When not to trust language models: Investigating effectiveness of para- metric and non-parametric memories,” arXiv preprint arXiv:2212.10511, 2022. [15] X. Wang, Z. Wang, X. Gao, F. Zhang, Y . Wu, Z. Xu, T. Shi, Z. Wang, S. Li, Q. Qian et al., “Searching for best practices in retrieval-augmented generation,” arXiv preprint arXiv:2407.01219 , 2024. [16] S. Jeong, J. Baek, S. Cho, S. J. Hwang, and J. C. Park, “Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity,” arXiv preprint arXiv:2403.14403 , 2024. [17] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Au- tomated evaluation of retrieval augmented generation,” arXiv preprint arXiv:2309.15217, 2023. [18] L. Wang, N. Yang, and F. Wei, “Query2doc: Query expansion with large language models,” arXiv preprint arXiv:2303.07678 , 2023. [19] C.-M. Chan, C. Xu, R. Yuan, H. Luo, W. Xue, Y . Guo, and J. Fu, “Rq-rag: Learning to refine queries for retrieval augmented generation,” arXiv preprint arXiv:2404.00610 , 2024. [20] Z. Jin, P. Cao, Y . Chen, K. Liu, X. Jiang, J. Xu, Q. Li, and J. Zhao, “Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models,” arXiv preprint arXiv:2402.14409, 2024. [21] Y . Yu, W. Ping, Z. Liu, B. Wang, J. You, C. Zhang, M. Shoeybi, and B. Catanzaro, “Rankrag: Unifying context ranking with retrieval- augmented generation in llms,” arXiv preprint arXiv:2407.02485, 2024. [22] G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ram ´e et al. , “Gemma 2: Improving open language models at a practical size,” arXiv preprint arXiv:2408.00118, 2024. [23] N. Reimers and I. Gurevych, “Sentence-bert: Sentence embeddings using siamese bert-networks,” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 11 2019. [Online]. Available: https: //arxiv.org/abs/1908.10084 [24] A. Roberts, H. W. Chung, G. Mishra, A. Levskaya, J. Bradbury, D. Andor, S. Narang, B. Lester, C. Gaffney, A. Mohiuddin et al. , “Scaling up models and data with t5x and seqio,” Journal of Machine Learning Research, vol. 24, no. 377, pp. 1–8, 2023. [25] P. Rajpurkar, “Squad: 100,000+ questions for machine comprehension of text,” arXiv preprint arXiv:1606.05250 , 2016. [26] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang et al. , “Qwen2 technical report,” arXiv preprint arXiv:2407.10671, 2024. [27] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al. , “The llama 3 herd of models,” arXiv preprint arXiv:2407.21783 , 2024.