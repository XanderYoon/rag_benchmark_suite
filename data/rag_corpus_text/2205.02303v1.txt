Analysing the Robustness of Dual Encoders for Dense Retrieval Against Misspellings Georgios Sidiropoulos University of Amsterdam Amsterdam, The Netherlands g.sidiropoulos@uva.nl Evangelos Kanoulas University of Amsterdam Amsterdam, The Netherlands e.kanoulas@uva.nl ABSTRACT Dense retrieval is becoming one of the standard approaches for doc- ument and passage ranking. The dual-encoder architecture is widely adopted for scoring question-passage pairs due to its efficiency and high performance. Typically, dense retrieval models are evaluated on clean and curated datasets. However, when deployed in real-life applications, these models encounter noisy user-generated text. That said, the performance of state-of-the-art dense retrievers can substantially deteriorate when exposed to noisy text. In this work, we study the robustness of dense retrievers against typos in the user question. We observe a significant drop in the performance of the dual-encoder model when encountering typos and explore ways to improve its robustness by combining data augmentation with contrastive learning. Our experiments on two large-scale passage ranking and open-domain question answering datasets show that our proposed approach outperforms competing approaches. Ad- ditionally, we perform a thorough analysis on robustness. Finally, we provide insights on how different typos affect the robustness of embeddings differently and how our method alleviates the effect of some typos but not of others. CCS CONCEPTS â€¢Information systems â†’ Retrieval models and ranking . KEYWORDS dense retrieval; dual-encoder; robustness; typos; misspellings ACM Reference Format: Georgios Sidiropoulos and Evangelos Kanoulas. 2022. Analysing the Robust- ness of Dual Encoders for Dense Retrieval Against Misspellings. In Proceed- ings of the 45th International ACM SIGIR Conference on Research and Devel- opment in Information Retrieval (SIGIR â€™22), July 11â€“15, 2022, Madrid, Spain. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3477495.3531818 1 INTRODUCTION With the advances in neural language modeling [3], learning dense representations for text has become a vital component in many in- formation retrieval (IR) tasks. In passage ranking and open-domain question answering, dense retrieval has become a new paradigm to Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR â€™22, July 11â€“15, 2022, Madrid, Spain Â© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-8732-3/22/07. . . $15.00 https://doi.org/10.1145/3477495.3531818 retrieve relevant passages [6, 7, 9]. In contrast to traditional term- based IR models (TF-IDF and BM25) that fail to capture beyond lexical matching, dense retrieval learns dense representations of questions and passages for semantic matching. A typical approach for dense retrieval involves learning a dual- encoder for embedding the questions and passages [ 6]. A dual- encoder model consists of two separate neural networks optimized to score relevant (i.e., positive) question-passage pairs higher than irrelevant (i.e., negative) ones. At inference time, the score of a question-passage pair is computed as the inner product of the cor- responding question and passage embeddings. Due to their high efficiency, dual-encoders are popular first-stage rankers in large- scale settings (in contrast to cross-encoders where even though they can achieve higher performance, they are not indexable and therefore are used as re-rankers [4, 15]). The whole corpus can be encoded and indexed offline, while at inference time, high-scoring passages with respect to a question can be found using efficient maximum inner product search [5]. So far, dense retrieval models have been evaluated on clean and curated datasets. However, these models will encounter user- generated noisy questions when deployed in real-life applications. Questions can include typos because of users mistyping words, such as keyboard typos (additional/missing character and character substitution), phonetic typing errors due to the close pronunciation, and misspellings. How these typos affect the encoding of questions and whether dense dual-encoder retrieval models are robust to them is not studied yet. Works on text classification have shown that deep neural lan- guage models such as BERT are not robust against typos [13, 16], even though they apply the WordPiece tokenization. Ma et al. [10] and Zhuang et al. [17] showed that typos can confuse even advanced BERT-based cross-encoders for re-ranking [2, 4, 12] and proposed data augmentation training for building typo-robust re-rankers. Additionally, Ma et al. [10] showed that bringing closer in the latent space the representations of the positive question-passage pairs of different questions while being far apart from negative ones can increase robustness. While the aforementioned works studied robustness for the case of re-ranking, improving the robustness of dense retrieval for first- stage ranking has not been explored in-depth yet. Intuitively, if typos cause an inferior first-stage ranking, that will already nega- tively affect the performance of the re-ranker. Therefore, robustness for first-stage ranking is crucial for the overall performance. To the best of our knowledge, Zhuanget al. [17] is the only work that stud- ied the first-stage ranking and used data augmentation to improve the robustness of a BERT-based Siamese encoder. arXiv:2205.02303v1 [cs.IR] 4 May 2022 In this paper, we study in-depth the robustness of dense retrieval for the case of dual-encoder architecture. We propose an approach that combines data augmentation with contrastive learning for robustifying dual-encoders against questions with typos. In detail, alongside augmenting questions with typos, we propose to use a contrastive loss that brings the representation of a question close to its typoed variations in the latent space while keeping it distant from other distinct questions. We aim to answer the following research questions: RQ1 Can data augmentation, contrastive learning, and their com- bination improve the robustness of dense retrieval to typos? RQ2 Do certain typoed words affect the robustness of the question encoding more than others? RQ3 Do the proposed method improve the robustness of the ques- tion encoding by ways other than simply learning to ignore the typoed word? Our main contributions are the following: (i) we propose an approach for robustifying dense retrievers towards typos in user questions that combines data augmentation with contrastive learn- ing and performs better than applying each component separately, (ii) we perform a thorough analysis on the robustness of dense retrieval, and (iii) show that typos in various words influence per- formance differently. 1 2 EXPERIMENTAL SETUP In this section, we discuss the datasets, the metrics, and the robust- ness methods we experiment with to answer our research questions. 2.1 Datasets We conduct our experiments on two large-scale datasets, namely, MS MARCO passage ranking [11], and Natural Questions [8]. In MS MARCO passage ranking, the goal is to rank passages based on their relevance to a question (i.e., the probability of including the answer). The data collection consists of 8.8 million passages; the questions were selected from Bing search logs. Natural Questions is a large-scale dataset for open-domain QA over Wikipedia, and its questions were selected from Google search logs. Table 1 shows the statistics of the two datasets. Table 1: Number of questions in each dataset, and the aver- age length of question. Train Dev Test Avg. q length MS MARCO 502,939 6,980 6,837 5.94 Natural Questions 79,168 8,757 3,610 9.20 2.2 Metrics To measure the retrieval performance on MS MARCO, we use the official metric MRR (@10) alongside the commonly reported Recall (R) at top-k ranks [ 7, 14].2 Following previous work on Natural 1https://github.com/GSidiropoulos/dense-retrieval-against-misspellings. 2Similar to previous works, we report the metrics on MSMARCO (Dev) since the correct answers for the test set are not available to the public. Questions, we use answer recall (AR) at the top-K retrieved passages [6, 14]. Answer recall measures whether at least one of the top-k retrieved passages contains the ground-truth answer. 2.3 Methods In this section, we describe the dual-encoder model we use for our experiments. Moreover, we present the three approaches we apply as extensions to this model in order to increase robustness. Dense Retriever (DR) is a dual-encoder BERT-based model used for scoring question-passage pairs [6]. Given a questionğ‘, a rel- evant passage ğ‘+ and a set of irrelevant passages {ğ‘âˆ’ 1 , ğ‘âˆ’ 2 , . . . , ğ‘âˆ’ğ‘› }, the model learns to rank ğ‘+ higher than the negative passages via the optimization of the negative log-likelihood of the relevant passage: L1 (ğ‘ğ‘–, ğ‘+ ğ‘– , ğ‘âˆ’ ğ‘–,1, Â· Â· Â·, ğ‘âˆ’ ğ‘–,ğ‘›) (1) = âˆ’ log ğ‘’sim(ğ‘ğ‘–,ğ‘+ ğ‘– ) ğ‘’sim(ğ‘ğ‘–,ğ‘+ ğ‘– ) + Ãğ‘› ğ‘—=1 ğ‘’sim(ğ‘ğ‘–,ğ‘âˆ’ ğ‘–,ğ‘— ) . Data Augmentation (DR + Data augm.) is one of the tradi- tional approaches for robustifying neural models. By exposing DR on questions with and without typos, the model learns to be in- variant to typos. Similar to Zhuang et al. [17], for each original correctly written question, on training time, we draw an unbiased coin. If the result is heads, we use the original question for training. If the result is tails, we use one of its typoed variations. Contrastive learning (DR + CL) of representations works by maximizing the agreement between differently augmented views of the same object. We propose a contrastive loss that compares the similarity between a question and its typoed variations and other distinct questions. In contrast with data augmentation, which explicitly trains on typoed question-passage pairs, here such pairs are seen implicitly only. In detail, in addition to Equation 1, we introduce a loss that enforces that a question, ğ‘ and its typoed variations ğ‘+ are close together in the latent space while being far apart from other distinct questions {ğ‘âˆ’ 1 , ğ‘âˆ’ 2 , . . . , ğ‘âˆ’ğ‘› }: L2 (ğ‘ğ‘–, ğ‘+ ğ‘– , ğ‘âˆ’ ğ‘–,1, Â· Â· Â·, ğ‘âˆ’ ğ‘–,ğ‘›) (2) = âˆ’ log ğ‘’sim(ğ‘ğ‘–,ğ‘+ ğ‘– ) ğ‘’sim(ğ‘ğ‘–,ğ‘+ ğ‘– ) + Ãğ‘› ğ‘—=1 ğ‘’sim(ğ‘ğ‘–,ğ‘âˆ’ ğ‘–,ğ‘— ) . The final loss is a weighted average of the two losses: L = ğ‘¤ 1 Â· L1 + ğ‘¤ 2 Â· L2 (3) The weights ğ‘¤ 1 and ğ‘¤ 2 are hyper-parameters and therefore need to be defined. Giving equal weights to the two losses is an effective and straightforward combination method which we used in our experiments. Combination (DR + Data augm. + CL) method consists of data augmentation combined with contrastive learning. Specifically, we propose alongside augmenting questions with typos to use the contrastive loss of Equation 2 that brings the representation of a question close to its typoed variations while keeping it distant from other distinct questions. The final loss is a weighted average of the three losses: L = ğ‘¤ 1 Â· L1 + ğ‘¤ 2 Â· L2 + ğ‘¤ 3 Â· L3, (4) where L3 represents the data augmentation and is computed simi- larly to Equation 1 but for the typoed variation ğ‘+ of the original question ğ‘. For our experiments, we use an equal weighting setting for the weights ğ‘¤ 1,ğ‘¤ 2, and ğ‘¤ 3. 2.4 Simulating Typos To assess the robustness of the proposed methods, a large-scale dataset for passage retrieval with typoed questions is necessary. Unfortunately such a dataset does not exist, and therefore we build one by simulating typos over the original Natural Questions and MS MARCO datasets. In detail, we simulate typos produced by humans by augmenting the original questions in the dataset with synthetically generated typoed ones. In order to simulate typos, we apply the following transforma- tions that often occur in human-generated questions. â€¢ Random: Inserts, deletes, swaps, or substitutes a random character; e.g., committee â†’ { copmmittee, commttee, comimt- tee, commitlee}. â€¢ Keyboard: Swaps a random character with those close to each other on the QWERTY keyboard; e.g., committee â†’ comnittee. â€¢ Common misspellings: Replaces words with misspelled ones, defined in a dictionary of common user-generated misspellings; e.g., committee â†’ comittee. 2.5 Implementation Details The DR model used in our experiments is trained using the in-batch negative setting described in [6]. The question and passage BERT encoders are trained for 50ğ¾ steps, with a batch size of 48. The learning rate is set to 2ğ‘’-5 using Adam, and the rate of the linear scheduling with a warm-up is set to0.1. Moreover, we use the same hyper-parameters for the three robustifying methods described in Section 2.3, in order to ensure a fair comparison. For generating typos in the training phase as well as building the typo-robustness test set, we use the open-source Aug library [1]. Each word in a question gets transformed with a probability of 0.2, and the transformation type (Section 2.4) gets chosen at random. 3 RESULTS In this section, we present our experimental results that answer our research questions. We aim to answer RQ1 by comparing the retrieval performance of the methods we consider (Section 2.3) for two settings: clean questions and questions with typos. In Table 2, we obverse that on clean questions, data augmentation as well as our two proposed approaches, namely, contrastive learning and data augmentation combined with contrastive learning do not harm the performance. Moreover, all the approaches for robustifying DR are performing significantly better compared to the original DR, on questions with typos. That indicates that they successfully robustify the underlying dual-encoder model. That said, our proposed data augmentation combined with contrastive learning approach holds the best performance. Following previous works, we randomly introduce typos to ques- tions. However, we want to investigate if the performance of the approaches we consider remains the same irrespectively of the word in which the typo appears. To answer RQ2, we create two addi- tional test settings for the case of questions with typos. Specifically, we create (i) a setting where typos appear only in non-stopwords, and (ii) a setting where typos appear only in utterances with lexical match with the relevant passage.3 In detail, we consider the overlap- ping consecutive words between the ground-truth passage and the question (e.g., â€œWho was the president of the united states during wwi?â€, and â€œWoodrow Wilson, a leader of the Progressive Move- ment, was the 28th President of the United States (1913-1921). After a policy of neutrality at the outbreak of World War I, he led Amer- ica into war. â€ mark the â€œpresident of the united statesâ€ as available utterance to introduce typos). The highly discriminative utterances obtained through this heuristic are typically entity mentions. As we can see in Table 3 and by comparing the numbers with the results in Table 2, the effectiveness of the methods varies across the three settings. Particularly, robustness deteriorates when typos do not appear randomly. In detail, the most significant losses occur when typos appear on discriminative utterances. To this extent, our proposed data augmentation combined with contrastive learning approach remains the best performing one across all setting. To better understand the discrepancy in robustness between the three settings of questions with typos, we conduct the following analysis. For the setting where typos randomly appear on ques- tions, we study how the frequency on the training set of the typoed words at test time affects robustness. As shown in Figure 1, there is a strong connection between the frequency of the typoed words and the retrieval performance. As the frequency of the typoed words decreases, the performance drops significantly. To this extent, our proposed data augmentation combined with contrastive learning approach remains the best performing one, with the performance gap increasing as the frequency of the typoed word decreases. The results in Figure 1 can also explain why we observe the highest losses in performance on the setting with typos in discriminative ut- terances. In general, the discriminative utterances (entity mentions) diversity between the dataset splits is higher compared to other words appearing in questions (e.g., interrogative, linking words). [105-) [104-105) [103-104) [102-103) [0-102) Frequency of typoed words in train set. 0.0 0.2 0.4 0.6 0.8 1.0 R@50 DR DR + CL (ours) DR + Data augm. DR + Data augm. + CL (ours) Figure 1: Retrieval performance (Average Recall@50) w.r.t the frequency on the training set, of the typoed words at test- time; on MS MARCO (Dev). Questions are split into bins w.r.t the frequency of their typoed words. 3We build the new settings using the same probability for introducing typos (Section 2.5), and we do not retrain the models on the new settings. Table 2: Retrieval results for the settings of (i) clean questions (Original), and (ii) questions with typos (Typos in Random Words). Statistical significance difference with paired t-test (ğ‘ < 0.05) DR=d; DR+ Data augm.=a; DR+CL=c. Natural Questions (Test) MS MARCO (Dev) Original Typos in Random Words Original Typos in Random Words AR@5 AR@20 AR@100 AR@5 AR@20 AR@100 MRR@10 R@50 R@1000 MRR@10 R@50 R@1000 DR 67.31 78.22 85.42 49.52 63.98 76.12 28.11 73.46 93.36 15.11 46.47 74.02 DR + Data augm. 66.45 79.03 85.56 60.69 73.76 83.37 28.26 72.66 93.07 22.00 61.68 86.49 DR + CL (ours) 66.31 77.42 85.45 55.51 69.27 80.52 28.95 73.01 93.64 19.37 55.08 80.69 DR + Data augm. + CL (ours) 67.47 78.83 85.67 62.13ğ‘‘ğ‘ğ‘ 74.87ğ‘‘ğ‘ğ‘ 83.26ğ‘‘ğ‘ 29.14 73.85 93.69 22.84ğ‘‘ğ‘ğ‘ 63.21ğ‘‘ğ‘ğ‘ 87.52ğ‘‘ğ‘ğ‘ Table 3: Retrieval results for the settings of (i) questions with typos in non-stopwords (Typos in Non-stopwords), and (ii) questions with typos in highly discriminative utterances (Typos in Discriminative Utterances). Stat. sig. difference w/ paired t-test (ğ‘ < 0.05) DR=d; DR+Data augm.=a; DR+CL=c. Natural Questions (Test) MS MARCO (Dev) Typos in Non-stopwords Typos in Discriminative UtterancesTypos in Non-stopwords Typos in Discriminative Utterances AR@5 AR@20 AR@100 AR@5 AR@20 AR@100 MRR@10 R@50 R@1000 MRR@10 R@50 R@1000 DR 40.60 55.48 68.72 38.89 53.37 68.00 11.83 38.98 66.16 10.51 34.17 59.71 DR + Data augm.56.14 69.94 80.19 51.68 66.12 78.08 18.51 54.82 81.92 16.51 49.06 77.47 DR + CL (ours) 49.47 64.12 76.48 44.04 59.00 72.43 15.44 46.69 73.27 12.44 39.17 66.69 DR + Data augm. + CL (ours) 57.78ğ‘‘ğ‘ğ‘ 70.77ğ‘‘ğ‘ğ‘ 81.38ğ‘‘ğ‘ğ‘ 53.15ğ‘‘ğ‘ğ‘ 67.28ğ‘‘ğ‘ğ‘ 78.61ğ‘‘ğ‘ğ‘ 19.47ğ‘‘ğ‘ğ‘ 56.22ğ‘‘ğ‘ğ‘ 83.61ğ‘‘ğ‘ğ‘ 17.58ğ‘‘ğ‘ğ‘ 50.81ğ‘‘ğ‘ğ‘ 79.51ğ‘‘ğ‘ğ‘ For RQ3, we consider a simple baseline where the typoed words are identified and removed from the question before being fed to the original DR model. We compare our best-performing approach against the aforementioned baseline. If our proposed approach only learns to ignore words with typos, then we argue that the performance of the baseline should be competitive to ours. In many cases, removing the typoed word can be a valid approach since the importance of words in a question varies. For instance, considering the question â€œWhere was president Lincoln born?â€ we see that â€œLincolnâ€ is crucial for the meaning of the question while â€œwasâ€ adds no information. With that in mind, we study RQ3 with respect to the relative importance of the typoed words within the question.4 Our method does not simply learn to ignore words with typos since, as we can see from Figure 2, it consistently outperforms the baseline. Furthermore, Figure 2 highlights that when the importance of typoed words is low, simply ignoring them is a highly competitive approach. On the other hand, as the importance of the typoed words increases, the effectiveness of just ignoring these words decreases dramatically, to the extent that keeping the typoed words performs better. That can be attributed to the application of the WordPiece tokenizer (by the underlying BERT model) that allows DR to recover from some typos, such as when the character n-gram splits remain intact despite the typos. For instance originalrobustness and typoed robustnessd will be split into [robust, ##ness] and [robust, ##ness, ##d] respectively. 4We define a wordâ€™s relevant importance as the ratio of its IDF to the sum of the IDFs of every word in the question. [0-0.1) [0.1-0.2) [0.2-0.3) [0.3-0.4) [0.4-0.5) [0.5-0.6) [0.6-0.7] Importance of typoed words 0.0 0.2 0.4 0.6 0.8 1.0 R@50 DR DR (+ Delete words with typos) DR + Data augm. + CL (ours) Figure 2: Retrieval results w.r.t the relevant importance of the typoed words; on MS MARCO (Dev). Questions are split into bins w.r.t the relevant importance their typoed words. 4 CONCLUSIONS In this work, we provided insights on the robustness of dual-encoders to typos of user questions for dense retrieval. We proposed an ap- proach for robustifying dual-encoders that combines data augmen- tation with contrastive learning. Our experimental results showed that our proposed method not only improves robustness but also performs better than separately applying data augmentation or contrastive learning. Analysis of the methods we explored showed that typos in various words do not influence performance equally. In particular, typos on words that are less frequent on the training set and more important for a question are harder to address. Our proposed technique remains the best performing one in these set- tings, however the performance deteriorates significantly compared to a clean question. There is a significant question that has risen throughout our study: What could a dual-encoder actually learn to fix the problem? The WordPiece tokenizer, applied by BERT, allows models to recover from some typos. However, it would be ideal if embeddings could be learned at the character n-gram level to allow recovery from typical character substitution, deletion, etc. Further- more, word-to-word interactions during training (e.g., through a late interaction model [7]) could also allow implicitly to learn the â€œcorrect spellingâ€ of a typoed word during training. We leave these directions as future work. ACKNOWLEDGMENTS This research was supported by the NWO Innovational Research Incentives Scheme Vidi (016.Vidi.189.039), the NWO Smart Culture - Big Data / Digital Humanities (314-99-301), the H2020-EU.3.4. - SOCIETAL CHALLENGES - Smart, Green And Integrated Transport (814961). All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors. REFERENCES [1] Joanna Bitton and Zoe Papakipos. 2021. AugLy: A data augmentations library for audio, image, text, and video. https://github.com/facebookresearch/AugLy. https://doi.org/10.5281/zenodo.5014032 [2] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019 , Benjamin Piwowarski, Max Chevalier, Ã‰ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and Falk Scholer (Eds.). ACM, 985â€“988. https://doi.org/10.1145/3331184.3331303 [3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers) , Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computa- tional Linguistics, 4171â€“4186. https://doi.org/10.18653/v1/n19-1423 [4] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. Rethink Training of BERT Rerankers in Multi-stage Retrieval Pipeline. In Advances in Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021, Proceedings, Part II (Lecture Notes in Computer Science, Vol. 12657) , Djoerd Hiemstra, Marie-Francine Moens, Josiane Mothe, Raffaele Perego, Martin Potthast, and Fabrizio Sebastiani (Eds.). Springer, 280â€“286. https://doi.org/10. 1007/978-3-030-72240-1_26 [5] Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2021. Billion-Scale Similarity Search with GPUs. IEEE Trans. Big Data 7, 3 (2021), 535â€“547. https://doi.org/10. 1109/TBDATA.2019.2921572 [6] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, No- vember 16-20, 2020, Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 6769â€“6781. https://doi.org/10.18653/ v1/2020.emnlp-main.550 [7] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Re- trieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 39â€“48. https://doi.org/10.1145/3397271.3401075 [8] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob De- vlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: a Benchmark for Question Answering Research.Trans. Assoc. Comput. Linguistics 7 (2019), 452â€“466. https://transacl.org/ojs/index.php/tacl/article/view/ 1455 [9] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, Dense, and Attentional Representations for Text Retrieval. Trans. Assoc. Comput. Linguistics 9 (2021), 329â€“345. https://transacl.org/ojs/index.php/tacl/article/view/ 2383 [10] Xiaofei Ma, CÃ­cero Nogueira dos Santos, and Andrew O. Arnold. 2021. Con- trastive Fine-tuning Improves Robustness for Neural Rankers. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021 (Findings of ACL, Vol. ACL/IJCNLP 2021) , Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, 570â€“582. https://doi.org/10.18653/v1/2021.findings-acl.51 [11] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. In Proceedings of the Workshop on Cogni- tive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773) , Tarek Richard Besold, Antoine Bordes, Artur S. dâ€™Avila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf [12] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. CoRR abs/1901.04085 (2019). arXiv:1901.04085 http://arxiv.org/abs/1901.04085 [13] Danish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton. 2019. Combating Adversarial Misspellings with Robust Word Recognition. InProceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , Anna Korhonen, David R. Traum, and LluÃ­s MÃ rquez (Eds.). Association for Computational Linguistics, 5582â€“5591. https://doi.org/10.18653/v1/p19-1561 [14] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxi- ang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2021 Conference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , Kristina Toutanova, Anna Rumshisky, Luke Zettle- moyer, Dilek Hakkani-TÃ¼r, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, 5835â€“5847. https://doi.org/10.18653/v1/2021.naacl-main.466 [15] Georgios Sidiropoulos, Nikos Voskarides, Svitlana Vakulenko, and Evangelos Kanoulas. 2021. Combining Lexical and Dense Retrieval for Computationally Efficient Multi-hop Question Answering. InProceedings of the Second Workshop on Simple and Efficient Natural Language Processing . Association for Computational Linguistics, Virtual, 58â€“63. https://doi.org/10.18653/v1/2021.sustainlp-1.7 [16] Lichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip S. Yu, and Caiming Xiong. 2020. Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT. CoRR abs/2003.04985 (2020). arXiv:2003.04985 https://arxiv.org/abs/2003.04985 [17] Shengyao Zhuang and Guido Zuccon. 2021. Dealing with Typos for BERT-based Passage Retrieval and Ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, 2836â€“2842. https://aclanthology.org/2021.emnlp-main.225