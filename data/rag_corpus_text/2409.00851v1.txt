Dissecting Temporal Understanding in Text-to-Audio Retrieval Andreea-Maria Oncescu Visual Geometry Group, University of Oxford Oxford, United Kingdom andreea-maria.oncescu@sjc.ox.ac.uk JoÃ£o F. Henriques Visual Geometry Group, University of Oxford Oxford, United Kingdom joao@robots.ox.ac.uk A. Sophia Koepke TÃ¼bingen AI Center, University of TÃ¼bingen TÃ¼bingen, Germany a-sophia.koepke@uni-tuebingen.de Abstract Recent advancements in machine learning have fueled research on multimodal tasks, such as for instance text-to-video and text- to-audio retrieval. These tasks require models to understand the semantic content of video and audio data, including objects, and characters. The models also need to learn spatial arrangements and temporal relationships. In this work, we analyse the temporal order- ing of sounds, which is an understudied problem in the context of text-to-audio retrieval. In particular, we dissect the temporal under- standing capabilities of a state-of-the-art model for text-to-audio retrieval on the AudioCaps and Clotho datasets. Additionally, we introduce a synthetic text-audio dataset that provides a controlled setting for evaluating temporal capabilities of recent models. Lastly, we present a loss function that encourages text-audio models to fo- cus on the temporal ordering of events. Code and data are available at https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/dtu/. CCS Concepts â€¢Information systems â†’ Speech / audio search; Retrieval effec- tiveness; â€¢Computing methodologies â†’ Temporal reasoning. Keywords text-to-audio retrieval, temporal understanding ACM Reference Format: Andreea-Maria Oncescu, JoÃ£o F. Henriques, and A. Sophia Koepke. 2024. Dis- secting Temporal Understanding in Text-to-Audio Retrieval. InProceedings of the 32nd ACM International Conference on Multimedia (MM â€™24), October 28-November 1, 2024, Melbourne, VIC, Australia. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3664647.3681690 1 Introduction The continued improvement of models and the increase in data available have led to impressive advances on various text-video tasks [1, 2, 13, 15, 17, 20, 21] and text-audio tasks [10], including text- to-audio retrieval [5, 18, 24, 26, 31, 35, 43], audio captioning [8, 9, 22] and recently, text-to-audio generation [14, 19, 42]. Understanding details, such as the temporal ordering of events, is important if we want our systems to give the best search results or generate reliable content for a text query. Recently, [ 34] showed that text-audio models do not use temporal cues available in text-audio datasets. This work is licensed under a Creative Commons Attribution- NonCommercial International 4.0 License. MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Â© 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0686-8/24/10 https://doi.org/10.1145/3664647.3681690 In this work, we build on [ 34] and examine the limitations of current state-of-the-art text-audio models, particularly in their use of temporal information. Different from [ 34] that considers a text-audio model containing a CNN-based audio encoder, our analysis uses the recent transformer-based audio encoder HTS- AT [4] that serves as a component of state-of-the-art text-to-audio retrieval models [24, 35]. We assess whether the model contain- ing a transformer-based audio encoder results in better temporal understanding abilities than a CNN-based one. Additionally, we investigate the experimental designs and datasets used to determine if poor temporal understanding in current state-of-the-art models is caused by the training data or by the model architecture. To determine whether commonly used text-audio datasets, such as AudioCaps [16] and Clotho [ 9], are suitable for training and evaluating the ability of current models to comprehend time, we examine the relative distribution of audio descriptions that con- tain temporal cues. In particular, we plot the frequency of specific temporal cues in relation to the total number of descriptions. Our analysis shows that both the AudioCaps and Clotho datasets suffer from biases caused by the way humans describe events. That is, we tend to describe events in the order they appear. When first hearing the sounds of a dog barking and then the sound of a human speaking, we describe this as â€˜A dog barking followed by a human speakingâ€™ rather than â€˜A dog barking before a human speaksâ€™. To try to address the lack of some temporal examples, in [ 34], the authors generate new text-audio pairs that enhance the existing text-audio data. They concatenate the audio files in a specific order and then generate a description that reflects that. For instance, if the generated sound isğ‘†ğ‘œğ‘¢ğ‘›ğ‘‘ 1, ğ‘†ğ‘œğ‘¢ğ‘›ğ‘‘2, the description is â€˜<Original description of ğ‘†ğ‘œğ‘¢ğ‘›ğ‘‘ 1> before <Original description of ğ‘†ğ‘œğ‘¢ğ‘›ğ‘‘ 2>â€™. This increases the size of the training data by 40%. Different to [34], we rephrase existing text descriptions to obtain a more uniform distribution of textual temporal cues whilst preserving the content (AudioCapsğ‘¢ğ‘›ğ‘– ). Furthermore, we investigate the impact of more uniform training data on the text-to-audio retrieval performance, reporting results on the original test data and on rephrased test data (TempTestğ‘Ÿğ‘’ğ‘£ and TempTestğ‘Ÿğ‘’ğ‘ ). The rephrasing of text descriptions is illustrated in Fig. 1. Furthermore, we present an empirical evaluation of the correct- ness and completeness of AudioCaps descriptions by leveraging a Large Language Model (LLM). More specifically, we provide an LLM with (a subset of) AudioCaps descriptions and the start and end times of the sounds that make up the audio files (provided by [38]). We ask the LLM to classify the sentences into correct â€“ if the descrip- tion contains all the sounds and the correct ordering, incomplete â€“ if the description is missing sounds or is missing temporal con- text, and incorrect â€“ if the description contradicts the provided grounded sounds. We observe that about 23% of the descriptions arXiv:2409.00851v1 [cs.IR] 1 Sep 2024 MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Oncescu, et al. Figure 1: Analysing and improving the understanding of temporal cues in text-to-audio retrieval model. Top left: AudioCaps ğ‘¢ğ‘›ğ‘– is a variant of the AudioCaps dataset with modified descriptions that have a more uniform distribution of textual temporal cues. Additionally (bottom left), we generate test sets with reversed temporal ordering or replaced temporal conjunctions (TempTestğ‘Ÿğ‘’ğ‘£ and TempTestğ‘Ÿğ‘’ğ‘ ). Middle: Generation of audio-text pairs in the SynCaps dataset that provides a controlled evaluation setting. Right: Our text-text contrastive loss improves temporal understanding using positive descriptions (green) for the same sound ordering and negative examples (red) for the opposite temporal meaning. are incomplete or incorrect. This can contribute to models trained on AudioCaps not being able to understand temporal ordering. To gain further insights into the temporal understanding capa- bilities, we propose a synthetic dataset that provides a controlled setting for analysing text-audio models. This dataset contains 10 second long audios to be consistent with the general setting that current models have been trained on. We show that the considered model struggles to use temporal cues in the synthetic dataset, too, confirming the findings from [34] in a controlled setting. This al- lows us to decouple the bad temporal performance of the model from the data not being suited for the task. Lastly, we propose a simple text-based contrastive loss function (see Fig. 1) and show that it results in the model paying more attention to the temporal ordering of events. This gives improvements in the overall retrieval results on the synthetic dataset. In summary, we make the following contributions: (i) We show why existing text-to-audio retrieval datasets are not good indicators of a text-audio modelâ€™s ability to understand temporal ordering, (ii) We propose a more uniform version of AudioCaps that is better suited for obtaining temporal understanding in models trained on this data. Additionally, its test subset allows for a better analysis of temporal understanding in existing models. This uniform version of AudioCaps keeps the audios intact and only requires changing the text descriptions. We provide benchmarks and an analysis of the behaviour of one current state-of-the art model on the original and more uniform versions of this dataset. (iii) We propose a synthetic dataset and use it to evaluate the modelâ€™s understanding of time. (iv) We investigate an additional loss term to encourage the model to focus on text-based temporal cues. 2 Related work Text-to-audio retrieval.Text-to-audio retrieval involves matching a textual query with its most relevant audio file in a database of audio samples. The task of searching through audio databases can be approached in multiple ways. One simple approach is to match the text query with the title or the metadata of the audio file, provided it exists. However, for unlabelled databases, the aim is to find an audio file that has the content specified by the user through a text query. This is called semantic search. For many years, text-audio semantic retrieval has used audio class labels that consist of individual or few words as text queries [11, 12, 30, 33]. More recently, [18, 26] proposed new benchmarks where the text query is a free-form text description rather than a pre-defined class label, allowing for more control over the retrieved audio content. Collecting new text- audio pairs for training and using state-of-the-art transformer- based audio encoders has proven beneficial on the text-to-audio retrieval benchmarks [24, 35, 43]. As the annotation of audio files with descriptions is time consuming, some of the text-audio pairs collected by [ 35] and [ 24] contain short audio labels instead of descriptions. To overcome this, [35] employed the T5 [29] model to generate descriptions starting from audio labels, whilst [24] used ChatGPT [27]. [24] also used ChatGPT to clean audio descriptions from datasets such as BBC Sound Effects 1 by removing vision- based content. Another line of works considered metric learning objectives for text-to-audio retrieval [ 23, 37]. Other concurrent research pushed the text-to-audio retrieval results even further by training models with additional modalities, such as video and speech [6, 32]. Recently, [25] introduced new text-to-audio retrieval benchmarks on egocentric video data. Text-audio grounding. [38] proposes a new set of data annota- tions for a subset of the AudioCaps dataset [ 16] with the aim of grounding each sound to a time interval. For this, annotators la- belled the start and end times of all relevant sounds in each audio clip. [39, 40] investigated the task of weakly supervised text-to- audio grounding. The audio grounded dataset has also been used for learning to align sounds and text in an unsupervised manner [36]. [3] used the grounded sounds to introduce new metrics for audio 1https://sound-effects.bbcrewind.co.uk/ Dissecting Temporal Understanding in Text-to-Audio Retrieval MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia captioning. In this work, we use this subset to provide an empiri- cal evaluation of the quality of existing AudioCaps captions. More specifically, we give the grounded sounds and their corresponding AudioCaps descriptions to an LLM and ask it to evaluate if the descriptions are correct and complete. Temporal understanding in text-audio models. [34] show that text-audio models do not pay attention to temporal cues in text queries, such as â€˜followed byâ€™, or â€˜afterâ€™. One example of an experi- ment in [34] is replacing temporal cues with words that represent a wrong ordering, e.g. replacing â€˜thenâ€™ with â€˜asâ€™. Then, the modelâ€™s performance on the â€˜wrongâ€™ descriptions is evaluated, revealing that this performance is similar to when the temporal ordering in the text queries is correct. In their study, [34] utilize CNNs for audio process- ing and identify a critical limitation of CNN-based models: applying temporal pooling across all embeddings can result in the loss of tem- poral information. To mitigate this issue, they augment the CNN architecture with several transformer layers to preserve temporal dynamics. In contrast, contemporary models built on transformers inherently incorporate mechanisms to handle temporal data more effectively. Different to [34], we investigate the temporal under- standing of a transformer-based state-of-the-art audio-text retrieval model. In particular, we analyse if a transformer-based model also ignores temporal cues. Additionally, the approach proposed by [34] for helping models better understand time does not improve the overall performance on downstream retrieval benchmarks. In this work, we investigate an alternative for guiding the model to fo- cus on temporal cues. Furthermore, we present a detailed analysis of descriptions in text-audio datasets in the context of temporal understanding. Concurrent work [41] claims that commonly used text-audio datasets only contain simple audio descriptions that lack information about temporal cues, the number of times a sound can be heard, or details about sounds overlapping. To address this, [41] introduce a synthetic text-audio datasets by merging â€˜atomicâ€™ sounds in a controlled way. Differently from [41] that focuses on more varied audio details such as loudness, or number of times a sound can be heard, we generate audio files and descriptions that showcase clear temporal relations between the composed sounds. In addition to that, [41] uses an LLM to generate descriptions which are varied but could include hallucinated content in contrast to the descriptions in our synthetic dataset which are rule-based. 3 Analysis of temporal understanding in text-to-audio retrieval In this section, we take a close look at the AudioCaps and Clotho datasets in the context of understanding temporal information. We present a detailed evaluation of text-audio retrieval models on those datasets. 3.1 AudioCaps dataset The AudioCaps [16] dataset contains paired audio clips and text descriptions. The training set consists of one text description for each audio file. The validation and test sets contain five descriptions for each audio file. In the AudioCaps evaluation setting, if at least one of the five text descriptions matches the audio clip, it counts as 100% retrieval accuracy. Figure 2: Distribution of temporal conjunctions and prepo- sitions in the full AudioCaps [ 16] dataset. Most temporal sentences contain future temporal cues, such as â€˜Followed byâ€™. There is only a small proportion ofpast cues, e.g. â€˜Beforeâ€™. Figure 3: Distribution of temporal conjunctions and preposi- tions in the full Clotho [ 9] dataset. Most temporal sentences contain joint cues (e.g. â€˜Asâ€™, â€˜Whileâ€™), followed byfuture ones (e.g. â€˜Thenâ€™). Fewer sentences containpast cues (e.g. â€˜Beforeâ€™). The AudioCaps dataset is employed in all related text-to-audio retrieval works for training and evaluation. We want to understand the temporal characteristics of the AudioCaps dataset to gauge if the data available for training text-to-audio retrieval models is a part of the problem of models not understanding temporal cues [34]. First, we analyse the distribution of temporal conjunctions and prepositions in all the audio captions in the AudioCaps dataset in Fig. 2. We observe that most conjunctions and temporal preposi- tions suggest future events, i.e. â€˜Followed byâ€™, â€˜Thenâ€™. This is closely followed by the joint occurrence of audio events, i.e. â€˜Asâ€™ and â€˜Whileâ€™. However, almost no examples contain the temporal prepositions â€˜Beforeâ€™ or â€˜Preceded byâ€™ which is reasonable as humans would not naturally describe events in that order. A similar analysis is per- formed by [34], providing the distribution for â€˜Followed byâ€™, â€˜Thenâ€™, MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Oncescu, et al. â€˜Beforeâ€™ and â€˜Afterâ€™. However, in [34], this distribution describes their training and test data which combines multiple datasets in- cluding AudioCaps [16] and Clotho [9]. Here, we consider all words in the AudioCaps descriptions that represent temporal ordering. Given the distribution in Fig. 2, expecting a model trained on this data to understand the meaning of reverse temporal prepositions is unreasonable. At the same time, the test data also suffers from the same problem, therefore, using AudioCaps benchmarks for deciding if models understand temporal ordering is not optimal either. Next, we empirically evaluate the correctness and completeness of AudioCaps descriptions by using the grounded sound time inter- vals provided by [38]. Through manual inspection, we notice that many AudioCaps descriptions are composed of multiple sounds. For instance, a 10 second audio file with a bird singing from second 0 to second 6 and a dog barking from second 4 to second 10 can be described as â€˜Bird singing and/as dog barksâ€™. Alternatively, this could be described as â€˜Bird singing followed by dog barkingâ€™. Both descriptions are correct, however, a more complete version of these descriptions would be, for example, â€˜Bird singing, soon joined by a dog barking. Their sounds overlap briefly before the bird stops, while the dog continues barking. â€™. If the description is not complete, however, how could a model learn the difference between â€˜asâ€™ and â€˜followed byâ€™ when they describe the same audio clip? To empirically evaluate the completeness and quality of the de- scriptions in AudioCaps with grounded sound sources, we use an LLM, specifically GPT-4 [27]. We provide the LLM with the Audio- Caps description, the grounded sources and their time intervals. We use one-shot prompting to give the model an example, such that it better understands the task. We then task the LLM to decide if the AudioCaps description is â€˜correctâ€™, â€˜incompleteâ€™, or â€˜wrongâ€™ based on the sound source information. Details for our prompt are shown in Tab. 1. To check how reliable the GPT-4 outputs are for this task, we manually checked 40 randomly selected descriptions, their grounded sounds and the GPT-4 evaluation. We found that the GPT-4 evaluation was correct 85% of the time, with the major- ity of the mistakes being in favour of AudioCaps, i.e. evaluating a sentence as correct when in fact it was incomplete. We show the resulting proportions of correct, incomplete, and wrong descrip- tions as identified by the LLM in the subset of AudioCaps in Tab. 2. On average, 23% of the descriptions are incomplete or wrong. This percentage increases for descriptions containing future and past temporal cues. The use of future and past refers to the fact that if â€˜Sound 1â€™ and â€˜Sound 2â€™ are connected by afuture temporal cue, then that means that â€˜Sound 1â€™ comes first and is followed by â€˜Sound 2â€™. If a past cue is used, then â€˜Sound 1â€™ comes after â€˜Sound 2â€™, e.g. â€˜Bird sings after dog barksâ€™.Future cues include â€˜Followed byâ€™, â€˜Beforeâ€™ and â€˜Thenâ€™, e.g. â€˜Bird sings before dog barksâ€™. Forpast, we consider â€˜Preceded byâ€™ and â€˜Afterâ€™. Based on the significant proportion of incomplete or wrong descriptions, and the distribution of temporal textual cues, we conclude that AudioCaps is not well-suited for analysing if text-audio models understand temporal ordering in audio clips and descriptions. 3.2 Clotho dataset We examine whether the Clotho [9] dataset, another popular text- audio retrieval dataset, faces similar temporal limitations as Au- dioCaps. Clotho contains pairs of audio clips and text descriptions. Unlike AudioCaps, the train, validation, and test subsets contain five descriptions for each audio file. For a more uniform experimen- tal setting, in the next sections, instead of pairs of one audio and 5 corresponding text descriptions, we split the Clotho training pairs into 5 pairs of one audio (repeated) and one corresponding text description. We investigate the distribution of sentences containing temporal cues in Clotho. We notice in Fig.3 that, similarly to AudioCaps, Clotho contains a non-uniform distribution of temporal cues. How- ever, for Clotho, descriptions mainly feature simultaneous actions, with the words â€˜Asâ€™ and â€˜Whileâ€™ being used most often. This is followed by future temporal cues such as â€˜Thenâ€™ or â€˜Followed byâ€™. There are three times fewer sentences containing past temporal cues. Therefore, we expect that models trained on Clotho are biased towards simultaneous or in-order (future) events. 3.3 Model performance on AudioCaps and Clotho In this section, we investigate the performance of a state-of-the-art model for text-to-audio retrieval on AudioCaps and Clotho in detail. 3.3.1 Evaluation metrics. Throughout all experiments, we use the standard evaluation metrics for retrieval: recall at rank ğ‘˜ (R@ğ‘˜). This measures the percentage of targets retrieved within the top ğ‘˜ ranked results. Higher numbers are better. We report results for text-to-audio (T â†’ A) and audio-to-text retrieval (A â†’ T). We report the mean of three runs that use different random seeds. 3.3.2 Model. We employ the state-of-the-art text-audio model by [24], utilising an HTS-AT audio encoder [4], and a pre-trained BERT encoder for text. After encoding audio and text inputs, an MLP projects the embeddings into the same space. We use the model variant pre-trained on the joint dataset of WavCaps [ 24] + AudioCaps [16] + Clotho [9]. In our experiments, we finetune the model for 40 epochs on the dataset of interest (e.g. AudioCaps, Clotho) and use the same setup as [24]. The best model is selected using the highest average validation retrieval accuracy R@1 on the dataset used for experiments. We run these experiments on A6000s. 3.3.3 Loss function. We use the same loss as [24] - a normalised temperature scaled bidirectional cross-entropy loss (NT-Xent) [7]. We refer to this as Lğ‘ğ‘¡ with ğ‘ ğ‘ğ‘–ğ‘¡ ğ‘— = ğ‘“ (ğ‘ğ‘– ) Â· ğ‘”(ğ‘¡ ğ‘— ) âˆ¥ğ‘“ (ğ‘ğ‘– ) âˆ¥2 âˆ¥ğ‘”(ğ‘¡ ğ‘— ) âˆ¥2 , (1) Lğ‘¡ğ‘ = âˆ’ 1 2ğµ Ãğµ ğ‘–=1  log  exp(ğ‘ ğ‘ğ‘–ğ‘¡ğ‘– /ğœ)Ãğµ ğ‘—=1 exp(ğ‘ ğ‘ğ‘–ğ‘¡ğ‘— /ğœ)  + log  exp(ğ‘ ğ‘ğ‘–ğ‘¡ğ‘– /ğœ)Ãğµ ğ‘—=1 exp(ğ‘ ğ‘ğ‘— ğ‘¡ğ‘– /ğœ)  . (2) Here ğ‘“ (Â·) is the audio encoder and ğ‘”(Â·) the text encoder. ğ‘ ğ‘ğ‘–ğ‘¡ ğ‘— is the cosine similarity, ğ‘ğ‘– the audio input, ğ‘¡ ğ‘— the text input, ğµ the batch size, and ğœ is a temperature parameter. More details can be found in [24]. 3.3.4 Data used. For our analysis, we construct a more uniform version of the AudioCaps dataset with descriptions having a more Dissecting Temporal Understanding in Text-to-Audio Retrieval MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Table 1: Methodology for evaluating the quality of the temporally grounded subset of AudioCaps using an LLM. Our input prompt includes setting the scene, one-shot prompting with an example for generating the evaluation of descriptions in the AudioCaps dataset, followed by the generation of new examples. Prompt Given descriptions of audio files and detailed temporal information about specific sounds within these files, where a sound may be present during multiple, distinct time intervals, your task is to evaluate the accuracy of each description with a primary focus on the timing and sequence of these sounds. Each audio file is 10 seconds long. For every description, assess its accuracy specifically in terms of how well it captures the chronological order and exact timing of sounds. Classify your evaluation into one of three categories: â€˜Correctâ€™, â€˜Incompleteâ€™, or â€˜Wrongâ€™. If necessary, provide a corrected description that not only fixes inaccuracies related to timing but also maintains the original writing style of the description. Your analysis should critically examine the temporal details provided, ensuring your assessment is primarily guided by the accuracy of these temporal sequences. Keep in mind the following: Pay attention to whether the description matches the start and end times of sounds accurately. Consider if the sequence of described sounds follows the actual sequence in the audio file. Evaluate if the description misses any sounds within the specified time frames or includes sounds that do not occur within these times. Use similar vocabulary as the original audio description. Example: Input: Original audio description: A power tool motor running then revving Localized components and their start and end times: revving: 2.154, 10.02; a power tool motor running: 0.0, 10.02; Output: Evaluation: Incomplete Corrected description: A power tool motor running throughout, with revving starting early on and continuing alongside the motorâ€™s running sound until the end. Table 2: Proportion of correct, incomplete and wrongly cap- tioned AudioCaps data as determined by an LLM. First row contains the total numbers of temporally grounded descrip- tions. The other rows show proportions for specific temporal cues. Almost a quarter of temporal sentences are incomplete or wrong. Preposition Correct Incomplete Wrong Total (#) 3835 636 503 As (%) 75.3 13.9 10.8 Followed by (%) 60.6 15.3 24.1 Then (%) 62.1 15.9 22.0 While (%) 72.0 15.4 12.6 Before (%) 58.8 9.8 31.4 After (%) 54.5 6.1 39.4 Proceeded by (%) 50.0 0.0 50.0 During (%) 53.3 13.3 33.3 And (%) 75.6 13.5 10.9 balanced distribution of temporal conjunctions and prepositions. In particular, we rephrase AudioCaps descriptions to preserve the original meaning while varying the use of temporal conjunctions and prepositions. We investigate if this improves temporal under- standing. Specifically, we generate the ğ´ğ‘¢ğ‘‘ğ‘–ğ‘œğ¶ğ‘ğ‘ğ‘  ğ‘¢ğ‘›ğ‘– dataset with corresponding ğ‘‡ ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘¢ğ‘›ğ‘– , ğ‘‰ ğ‘ğ‘™ğ‘¢ğ‘›ğ‘– and ğ‘‡ ğ‘’ğ‘ ğ‘¡ğ‘¢ğ‘›ğ‘– subsets based on the AudioCaps dataset. We use two approaches to re-writing the de- scriptions. One is to replace the temporal cues with something that has the same meaning, e.g. â€˜Bird singing followed by dog barkingâ€™ is equivalent to â€˜Bird singing before dog barkingâ€™. The second ap- proach is to re-order the text location of events and also change the temporal cue, e.g. â€˜Bird singing followed by dog barkingâ€™ becomes â€˜Dog barking after bird singingâ€™. We present the distribution of temporal cues in the original AudioCaps dataset and its uniform variant in Fig. 4, Fig. 5, and Fig. 6. In addition to the more uniform AudioCaps train, validation and test sets, we create a test subset where at least one of the five text descriptions contains future and past temporal cues (as explained in Sec. 3.1). We do this to evaluate the performance of the model on sentences that actually contain temporal cues of interest. We call this subset TempTest. MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Oncescu, et al. Figure 4: Distribution of temporal conjunctions and preposi- tions in the AudioCaps training data. We compare the propor- tion of temporal textual cues in the original training dataset (Train) and our proposed variant with a more uniform distri- bution of temporal textual cues ( ğ‘‡ ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘¢ğ‘›ğ‘– ). Figure 5: Distribution of temporal conjunctions and preposi- tions in the AudioCaps [16] validation dataset (Val) compared to our proposed variant with a more uniform distribution of temporal textual cues (ğ‘‰ ğ‘ğ‘™ğ‘¢ğ‘›ğ‘– ). For Clotho, we use its original data in our experiments. Differ- ently from our AudioCaps experiments, we do not create a dataset variant with more uniformly distributed temporal conjunctions, since the â€˜futureâ€™ and â€˜pastâ€™ cues only account for less than 15% of the Clotho data. However, as for AudioCaps, we evaluate the model on multiple test subsets that allow us to analyse the temporal understanding capabilities. 3.3.5 Experiments. We consider three main experiments. First, we conduct the standard evaluation for text-to-audio retrieval on the AudioCaps and Clotho test sets. The performance on the standard Figure 6: Distribution of temporal conjunctions and preposi- tions in the AudioCaps [ 16] test dataset (Test) compared to our proposed variant with a more uniform distribution of temporal textual cues (ğ‘‡ ğ‘’ğ‘ ğ‘¡ğ‘¢ğ‘›ğ‘– ). test set serves as a point of reference for the training and evaluation on different variants of the data. The second experiment involves reversing the ordering of sounds in the text queries of the test set. We refer to this asğ‘‡ ğ‘’ğ‘ ğ‘¡ğ‘Ÿğ‘’ğ‘£ . The purpose of this experiment is to see what happens if the temporal text descriptions keep the same temporal preposition or conjunction but the sound sources are reversed, resulting in a wrongly ordered description, e.g. Birds singing before dog barks becomes Dog barks before birds singing. If the model understands the temporal ordering of events, the modelâ€™s performance should drop for the â€˜wronglyâ€™ ordered events as compared to the original test set performance. For the third experiment, we replace the temporal cue in a de- scription with its opposite, thus changing the order of events with- out changing their spatial position in the text description, e.g. Birds singing before dog barks becomes Birds singing after dog barks. We refer to this as Testğ‘Ÿğ‘’ğ‘ . More concretely, we make the following re- placements: (â€˜afterâ€™â†’â€˜beforeâ€™), (â€˜beforeâ€™â†’â€˜afterâ€™), (â€˜thenâ€™â†’â€˜beforeâ€™), (â€˜followed byâ€™â†’â€˜preceded byâ€™), and (â€˜preceded byâ€™â†’â€˜followed byâ€™). If the model does not understand temporal cues, we expect it to perform similarly well on Test, Testğ‘Ÿğ‘’ğ‘£ and ğ‘‡ ğ‘’ğ‘ ğ‘¡ğ‘Ÿğ‘’ğ‘ . Conversely, if it understands temporal ordering, the Test performance should be considerably higher than Testğ‘Ÿğ‘’ğ‘£ and Testğ‘Ÿğ‘’ğ‘ . We would also expect Testğ‘Ÿğ‘’ğ‘£ and Testğ‘Ÿğ‘’ğ‘ to be similar, as the meaning of the sentence is the same but opposite of the correctTest sentences. We additionally consider rev and rep subsets of the temporal subset TempTest. In Tab. 3, we take the checkpoint provided by [24] which was pre-trained on the WavCaps [24], AudioCaps [16], and Clotho [9], and finetune it on the original AudioCaps dataset (similar to [24]). We notice that on the reversed TempTestğ‘Ÿğ‘’ğ‘£ set the model per- forms worse, indicating that the model understands temporal or- dering. However, on TempTestğ‘Ÿğ‘’ğ‘ which contains the replacement of temporal cues, the model performs similarly to onTempTest. This is interesting, as the temporal ordering of both ğ‘‡ ğ‘’ğ‘šğ‘ğ‘‡ ğ‘’ğ‘ ğ‘¡ğ‘Ÿğ‘’ğ‘£ and TempTestğ‘Ÿğ‘’ğ‘ is reversed and wrong as compared to TempTest. The Dissecting Temporal Understanding in Text-to-Audio Retrieval MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Table 3: Text-to-audio retrieval and audio-to-text retrieval on the AudioCaps and AudioCaps ğ‘¢ğ‘›ğ‘– datasets for the model fine-tuned on AudioCaps (Train). We report retrieval accu- racies R@1. Reversing the order of events ( rev, rep) does not generally result in a drop in performance, showing that the model does not understand temporal ordering. Lower per- formance on TempTestğ‘Ÿğ‘’ğ‘£ might be due to the bias induced by data being described in a future fashion. When this bias is removed from the test set in AudioCaps ğ‘¢ğ‘›ğ‘– , reversing the order of events does not change performance. Eval Dataset Subset Tâ†’A A â†’T R@1 R@1 AudioCaps Test 43.71 55.44 TempTest 50.06 62.95 TempTestğ‘Ÿğ‘’ğ‘£ 44.11 57.27 TempTestğ‘Ÿğ‘’ğ‘ 49.12 63.12 AudioCapsğ‘¢ğ‘›ğ‘– Test 41.61 53.80 TempTest 48.04 61.74 TempTestğ‘Ÿğ‘’ğ‘£ 47.29 62.29 TempTestğ‘Ÿğ‘’ğ‘ 47.24 62.23 only difference is that for the former, the actual positional text loca- tions of the sounds are swapped, whilst for the latter the meaning is reversed by changing the temporal connector. This leads us to believe that at best, the model learns text location-based ordering rather than the ordering given by the text connector. In the left- hand side of Tab.4 we observe a similar behaviour for the Clotho temporal test sets, with the model performing similarly well on TempTestğ‘Ÿğ‘’ğ‘ and a bit worse on TempTestğ‘Ÿğ‘’ğ‘£ . This indicates that the model at best learns the text location-based ordering, but does not really understand temporal cues. We do the same experiments on the test sets of AudioCapsğ‘¢ğ‘›ğ‘– and find that the model is unable to identify the text-based order of sound events, with results on â€˜correctâ€™ (TempTest) and â€˜wrongâ€™ (TempTestğ‘Ÿğ‘’ğ‘£ and TempTestğ‘Ÿğ‘’ğ‘ ) splits being comparable. This is due to the test set not being biased anymore towards future events. Next, we examine whether the modelâ€™s understanding of tem- poral ordering is limited due to a lack of variety in the training examples. We take the same pre-trained model as before [24], and finetune it on the ğ´ğ‘¢ğ‘‘ğ‘–ğ‘œğ¶ğ‘ğ‘ğ‘  ğ‘¢ğ‘›ğ‘– ğ‘‡ ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘¢ğ‘›ğ‘– set. We notice that the overall performance on the ğ´ğ‘¢ğ‘‘ğ‘–ğ‘œğ¶ğ‘ğ‘ğ‘  ğ‘¢ğ‘›ğ‘– test sets and the corre- sponding temporal subsets is higher when finetuning on a more uniform distribution of temporal cues (left half of Tab. 5) than when finetuning on the original training data (bottom of Tab. 3). Thus, the lack of understanding temporal ordering is in part due to the train- ing data not containing examples of past temporal cues. We also notice some signs of better temporal understanding, with a slightly bigger drop in performance on the TempTestğ‘Ÿğ‘’ğ‘£ and TempTestğ‘Ÿğ‘’ğ‘ sets relative to TempTest. 4 Analysis of temporal understanding in controlled setting We analyse the text-audio modelâ€™s temporal understanding in a controlled setting where we can guarantee correct alignment of text-audio pairs. Table 4: Text-to-audio retrieval and audio-to-text-retrieval on the Clotho dataset for the model fine-tuned on Clotho (Train). We report retrieval accuracies R@1. Reversing the order of events does not change performance when using only the text-audio contrastive loss. When training with the added text-text contrastive loss, evaluation on the wrong order of events (i.e. rev, rep) yields a drop in performance. Subset Loss T â†’A A â†’T Loss T â†’A A â†’T R@1 R@1 R@1 R@1 Test Lğ‘¡ğ‘ 18.79 24.35 +ğœ†Lğ‘¡ğ‘¡ 18.17 21.56 TempTest Lğ‘¡ğ‘ 30.84 39.33 +ğœ†Lğ‘¡ğ‘¡ 30.02 36.24 TempTestğ‘Ÿğ‘’ğ‘£ Lğ‘¡ğ‘ 30.02 38.48 +ğœ†Lğ‘¡ğ‘¡ 24.57 34.93 TempTestğ‘Ÿğ‘’ğ‘ Lğ‘¡ğ‘ 30.30 39.33 +ğœ†Lğ‘¡ğ‘¡ 24.34 33.52 Table 5: Text-to-audio retrieval and audio-to-text retrieval results on the temporally uniform AudioCaps ğ‘¢ğ‘›ğ‘– dataset for the model fine-tuned on AudioCapsğ‘¢ğ‘›ğ‘– (Trainğ‘¢ğ‘›ğ‘– ). Improved results on Testğ‘¢ğ‘›ğ‘– . Using the text-text contrastive loss, the model better understands temporal ordering (larger drop for ğ‘Ÿğ‘’ğ‘£ and ğ‘Ÿğ‘’ğ‘ compared to TempTest). Subset Loss Tâ†’A A â†’T Loss Tâ†’A A â†’T R@1 R@1 R@1 R@1 Test Lğ‘¡ğ‘ 43.52 53.40 +ğœ†Lğ‘¡ğ‘¡ 42.00 52.25 TempTest Lğ‘¡ğ‘ 50.71 62.41 +ğœ†Lğ‘¡ğ‘¡ 48.32 59.37 TempTestğ‘Ÿğ‘’ğ‘£ Lğ‘¡ğ‘ 46.82 59.43 +ğœ†Lğ‘¡ğ‘¡ 39.38 52.43 TempTestğ‘Ÿğ‘’ğ‘ Lğ‘¡ğ‘ 46.99 58.58 +ğœ†Lğ‘¡ğ‘¡ 38.42 51.52 4.1 Data generation We use the ESC-50 [28] environmental sound classification dataset to generate a synthetic dataset for text-to-audio retrieval with a focus on temporal understanding capabilities. ESC-50 is a dataset of 2000 audio samples from 50 classes. As this dataset is clean and contains â€˜atomicâ€™ sounds (i.e. 5 second audios containing only one sound), we use it for synthetic data generation. We first task an LLM to take the sound labels from ESC-50 and generate textual descriptions in the style of AudioCaps (e.g. â€˜dogâ€™â†’ â€˜dog barkingâ€™). To generate the text-audio pairs, we take two sounds and their LLM-generated labels and concatenate them based on an arbitrarily selected temporal order (see Fig. 1 in the middle). We call this dataset SynCaps. To avoid any confusion, we only use future and past temporal cues. This is because synchronous temporal cues such as â€˜asâ€™ or â€˜dur- ingâ€™ are ambiguous, especially in a noisily labelled dataset. They can be used for sounds that completely overlap, or for partial overlaps of sounds, ignoring the actual order in which the sounds appear. The test set contains unique sound components that are not used in the training and validation sets. This leads to 485 test examples of 10 second long audio clips. For training and validation, we allow the same 5 seconds sound component to appear on average five times. We apply five different types of augmentation that include time shifting, volume adjustment, pitch shift, time stretch, and the MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Oncescu, et al. addition of noise. We also allow for an overlap between the files of up to one second. This results in a total of 4400 training samples, 485 validation samples, and 485 test samples. 4.2 SynCaps experiments We analyse the temporal understanding of the text-audio model in the controlled setting of the SynCaps dataset. For this, we take the same pre-trained model from [24] and finetune it on SynCaps using the original Lğ‘¡ğ‘ loss. We observe that evaluating on the â€˜reversedâ€™(rev) and â€˜replacedâ€™ (rep) datasets gives almost the same results as using the correct (original) test data (left half of Tab. 6). This shows that the model indeed does not understand temporal cues even on a simple dataset. 5 Text-text contrastive loss We propose a loss function Lğ‘¡ğ‘¡ that aims to enhance the under- standing of temporal information. It is formulated as a text-text contrastive loss, which relies on pairs of positive examples (that have the same temporal significance as the original sentence) and negative text examples (that have the opposite temporal meaning). Concretely, given the original descriptionBird sings followed by dog barks, one positive example is Bird sings before dog barks and one negative example would be Bird sings after dog barks. We provide the model with two positive text examples and two negative text examples for each text description containing the previously defined future and past temporal textual cues. Positive and negative text examples can be generated once, before training the model. We searched for the temporal cues we are interested in and automatically generated multiple positives and negatives by changing the temporal cues and/or the ordering of the sounds. The contrastive loss for each query and a margin ğ›¼ is: Lğ‘¡ğ‘¡ = 1 2ğ‘ ğ‘âˆ‘ï¸ ğ‘›=1 2âˆ‘ï¸ ğ‘˜=1 max(0, ğ›¼ âˆ’ ğ‘ ğ‘¡ğ‘›ğ‘¡ğ‘˜,ğ‘ğ‘œğ‘  + ğ‘ ğ‘¡ğ‘›ğ‘¡ğ‘˜,ğ‘›ğ‘’ğ‘” ), (3) where ğ‘ ğ‘¡ğ‘›ğ‘¡ğ‘˜,ğ‘ğ‘œğ‘  is the cosine similarity (see Eq. 1) between the ğ‘›- th text query and its ğ‘˜-th positive example, ğ‘ ğ‘¡ğ‘›ğ‘¡ğ‘˜,ğ‘›ğ‘’ğ‘” is the cosine similarity between the ğ‘›-th query and its ğ‘˜-th negative example. Our full loss then becomes: L = Lğ‘¡ğ‘ + ğœ†Lğ‘¡ğ‘¡ . (4) In our experiments that use the text-text contrastive loss, we set ğœ† = 10, and ğ›¼ = 0.2. 5.1 Performance using text-text contrastive loss We evaluate the same model pre-trained on the joint WavCaps+ AudioCaps + Clotho, and finetuned on SynCaps, AudioCapsğ‘¢ğ‘›ğ‘– and Clotho using our additional text-text contrastive loss. For SynCaps, we observe in Tab. 6, that the model performs better on the origi- nal test set when the additional text-text contrastive loss is used, whilst at the same time showing a big drop in performance on the â€˜reversedâ€™ and â€˜replacedâ€™ data. This shows that employing a simple additional loss can help the model better understand time, at least in the controlled setting of the SynCaps dataset. When finetuning the WavCaps-pretrained model on AudioCapsğ‘¢ğ‘›ğ‘– with the text-text contrastive loss, we see in the right-hand side of Tab. 5 that the drop is larger between the TempTest and theğ‘Ÿğ‘’ğ‘£ Table 6: Text-to-audio and audio-to-text retrieval on the Syn- Caps dataset for the model fine-tuned on SynCaps with the text-audio loss Lğ‘¡ğ‘ , and with the text-text loss Lğ‘¡ğ‘¡ . When using only Lğ‘¡ğ‘ , there is almost no drop in performance on the wrongly ordered test sets ğ‘Ÿğ‘’ğ‘£ , ğ‘Ÿğ‘’ğ‘ . With the additional Lğ‘¡ğ‘¡ , the drop becomes much more significant, confirming that the model understands temporal ordering. Subset Loss T â†’A A â†’T Loss T â†’A A â†’T R@1 R@1 R@1 R@1 Test Lğ‘¡ğ‘ 67.22 65.23 +ğœ†Lğ‘¡ğ‘¡ 69.35 69.90 Testğ‘Ÿğ‘’ğ‘£ Lğ‘¡ğ‘ 67.35 65.84 +ğœ†Lğ‘¡ğ‘¡ 40.55 41.03 Testğ‘Ÿğ‘’ğ‘ Lğ‘¡ğ‘ 66.94 63.92 +ğœ†Lğ‘¡ğ‘¡ 44.33 45.43 and ğ‘Ÿğ‘’ğ‘ test sets than when just using the text-audio original loss function. At the same time, the performance on the full Testğ‘¢ğ‘›ğ‘– and TempTest remains competitive when using the additional loss as compared to only using the original one. Lastly, we show that on the original form of the Clotho dataset, when finetuning using the text-text contrastive loss, we obtain a bigger drop on the ğ‘Ÿğ‘’ğ‘£ and ğ‘Ÿğ‘’ğ‘ test subsets. This indicates that although Clotho contains considerably more simultaneous actions and the number of future and past temporal cues is greatly dispro- portionate, the loss still helps the model better focus on temporal cues when present. This is observed in Tab. 4. 6 Conclusion In this work, we dissected the temporal understanding capabilities of a current state-of-the-art text-audio model. We first performed an in-depth analysis of the AudioCaps and Clotho text-audio datasets. We concluded that these datasets are not well-suited for obtaining and evaluating temporal understanding capabilities in a text-audio retrieval model. As a result, we proposed a variant of the Audio- Caps dataset, namely AudioCapsğ‘¢ğ‘›ğ‘– , that contains a more uniform distribution of different temporal cues. We then showed that using AudioCapsğ‘¢ğ‘›ğ‘– reduces biases learnt by the model and improves performance on the AudioCapsğ‘¢ğ‘›ğ‘– test set. Furthermore, we intro- duced a synthetic dataset (SynCaps), showing that indeed models fail to use the temporal cues even in a controlled data setting. Lastly, we proposed a simple loss that results in better text-to-audio re- trieval results on SynCaps, whilst also putting more emphasis on the temporal content of the audio and text data in all datasets analysed. Acknowledgments This work was supported by an EPSRC DTA Studentship, by the Royal Academy of Engineering (RF\201819\18\163), by the DFG: SFB 1233, project number: 276693517, and by the DFG EXC number 2064/1 â€“ project number 390727645. We are very grateful to Samuel Albanie and Bruno Korbar for helpful feedback and suggestions. Dissecting Temporal Understanding in Text-to-Audio Retrieval MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia References [1] Max Bain, Arsha Nagrani, GÃ¼l Varol, and Andrew Zisserman. 2021. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. In IEEE International Conference on Computer Vision . [2] Max Bain, Arsha Nagrani, GÃ¼l Varol, and Andrew Zisserman. 2022. A CLIP- Hitchhikerâ€™s Guide to Long Video Retrieval. arXiv preprint arXiv:2205.08508 (2022). [3] Swapnil Bhosale, Rupayan Chakraborty, and Sunil Kumar Kopparapu. 2023. A Novel Metric For Evaluating Audio Caption Similarity. InInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) . [4] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2022. HTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection. International conference on acoustics, speech and signal processing (ICASSP) . [5] Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, Jinhui Tang, and Jing Liu. 2023. VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset. arXiv preprint arXiv:2304.08345 (2023). [6] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. 2024. Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset. Advances in Neural Information Processing Systems (NeurIPS) (2024). [7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In Interna- tional Conference on Machine Learning (ICML) . [8] Soham Deshmukh, Benjamin Elizalde, Dimitra Emmanouilidou, Bhiksha Raj, Rita Singh, and Huaming Wang. 2024. Training Audio Captioning Models without Audio. In International Conference on Acoustics, Speech and Signal Processing (ICASSP). [9] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. 2020. Clotho: an Audio Captioning Dataset. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP). [10] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. 2023. Clap learning audio concepts from natural language supervision. In Inter- national Conference on Acoustics, Speech and Signal Processing (ICASSP) . [11] Benjamin Elizalde, Shuayb Zarar, and Bhiksha Raj. 2019. Cross modal audio search and retrieval with joint embeddings based on text and audio. In International Conference on Acoustics, Speech and Signal Processing (ICASSP) . [12] Asif Ghias, Jonathan Logan, David Chamberlin, and Brian C. Smith. 1995. Query by humming: musical information retrieval in an audio database. In Association for Computing Machinery (ACM) International Conference on Multimedia . [13] Tengda Han, Max Bain, Arsha Nagrani, GÃ¼l Varol, Weidi Xie, and Andrew Zis- serman. 2024. AutoAD III: The Prequel - Back to the Pixels. In Conference on Computer Vision and Pattern Recognition (CVPR) . [14] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. 2023. Make-An-Audio: Text- To-Audio Generation with Prompt-Enhanced Diffusion Models. InInternational Conference on Machine Learning (ICLR) . [15] Vladimir Iashin and Esa Rahtu. 2020. Multi-Modal Dense Video Captioning. In Conference on Computer Vision and Pattern Recognition (CVPR) Workshops . [16] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. 2019. AudioCaps: Generating captions for audios in the wild. InNorth American Chapter of the Association for Computational Linguistics (NAACL) . [17] Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, and Seong Tae Kim. 2024. Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 13894â€“13904. [18] A. Sophia Koepke, Andreea-Maria Oncescu, JoÃ£o F. Henriques, Zeynep Akata, and Samuel Albanie. 2022. Audio retrieval with natural language queries: A benchmark study. IEEE Transactions on Multimedia (2022). [19] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre DÃ©fossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. 2023. AudioGen: Textually Guided Audio Generation. InInternational Conference on Learning Representations (ICLR). [20] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use What You Have: Video retrieval using representations from collaborative experts. In British Machine Vision Conference (BMVC) . [21] Esa Rahtu Mayu Otani, Yuta Nakahima and Janne HeikkilÃ¤. 2020. Uncovering Hid- den Challenges in Query-Based Video Moment Retrieval. In The British Machine Vision Conference (BMVC). [22] Xinhao Mei, Xubo Liu, Qiushi Huang, Mark D. Plumbley, and Wenwu Wang. 2021. Audio Captioning Transformer. In Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021) . [23] Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark D. Plumbley, and Wenwu Wang. 2022. On Metric Learning for Audio-Text Cross-Modal Retrieval. InINTERSPEECH. [24] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou, and Wenwu Wang. 2023. WavCaps: A ChatGPT- Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multi- modal Research. arXiv preprint arXiv:2303.17395 (2023). [25] Andreea-Maria Oncescu, JoÃ£o F. Henriques, Andrew Zisserman, Samuel Albanie, and A. Sophia Koepke. 2024. A Sound Approach: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval. InInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) . [26] Andreea-Maria Oncescu, A. Sophia Koepke, JoÃ£o F. Henriques, Zeynep Akata, and Samuel Albanie. 2021. Audio Retrieval with Natural Language Queries. In INTERSPEECH. [27] OpenAI. [n. d.]. GPT. https://platform.openai.com/playground/. Accessed February, March 2024. [28] Karol J. Piczak. 2015. ESC: Dataset for Environmental Sound Classification. In Association for Computing Machinery (ACM) Conference on Multimedia . [29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.Journal of Machine Learning Research (2020). [30] Malcolm Slaney. 2002. Semantic-audio retrieval. In International conference on acoustics, speech and signal processing (ICASSP) . [31] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang Zhou. 2023. ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities. arXiv preprint arXiv:2305.11172 (2023). [32] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, and Limin Wang. 2024. Intern- Video2: Scaling Video Foundation Models for Multimodal Video Understanding. arXiv preprint abs/2403.15377 (2024). [33] Erling Wold, Thom Blum, Douglas Keislar, and James Wheaten. 1996. Content- based classification, search, and retrieval of audio. IEEE Multimedia (1996). [34] Ho-Hsiang Wu, Oriol Nieto, Juan Pablo Bello, and Justin Salamon. 2023. Audio- Text Models Do Not Yet Leverage Natural Language. International conference on acoustics, speech and signal processing (ICASSP) . [35] Yusong Wu*, Ke Chen*, Tianyu Zhang*, Yuchen Hui*, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023. Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation. In International Conference on Acoustics, Speech and Signal Processing (ICASSP) . [36] Huang Xie, Okko RÃ¤sÃ¤nen, Konstantinos Drossos, and Tuomas Virtanen. 2022. Unsupervised Audio-Caption Aligning Learns Correspondences Between Individ- ual Sound Events and Textual Phrases. In International Conference on Acoustics, Speech and Signal Processing (ICASSP) . [37] Yifei Xin, Dongchao Yang, and Yuexian Zou. 2023. Improving Text-Audio Re- trieval by Text-Aware Attention Pooling and Prior Matrix Revised Loss. InInter- national Conference on Acoustics, Speech and Signal Processing (ICASSP) . [38] Xuenan Xu, Heinrich Dinkel, Mengyue Wu, and Kai Yu. 2021. Text-to-Audio Grounding: Building Correspondence Between Captions and Sound Events. In- ternational conference on acoustics, speech and signal processing (ICASSP) . [39] Xuenan Xu, Ziyang Ma, Mengyue Wu, and Kai Yu. 2024. Towards Weakly Supervised Text-to-Audio Grounding. arXiv preprint arXiv:2401.02584 (2024). [40] Xuenan Xu, Mengyue Wu, and Kai Yu. 2023. Investigating Pooling Strategies and Loss Functions for Weakly-Supervised Text-to-Audio Grounding via Contrastive Learning. In International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW). [41] Xuenan Xu, Xiaohang Xu, Zeyu Xie, Pingyue Zhang, Mengyue Wu, and Kai Yu. 2024. A Detailed Audio-Text Data Simulation Pipeline using Single-Event Sounds. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). [42] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. 2023. Diffsound: Discrete Diffusion Model for Text-to-Sound Generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing 31 (2023). [43] Ching-Feng Yeh, Po-Yao Huang, Vasu Sharma, Shang-Wen Li, and Gargi Gosh. 2023. Flap: Fast Language-Audio Pre-Training. In Automatic Speech Recognition and Understanding Workshop (ASRU).