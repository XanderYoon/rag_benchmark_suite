ArchRAG: Attributed Community-based Hierarchical Retrieval-Augmented Generation Shu Wang1, Yixiang Fang1, Yingli Zhou1, Xilin Liu2, Yuchi Ma2, 1The Chinese University of Hong Kong, Shenzhen 2Huawei Cloud Computing Technologies CO., LTD. {shuwang3, yinglizhou}@link.cuhk.edu.cn, fangyixiang@cuhk.edu.cn, {liuxilin3, mayuchi1}@huawei.com Abstract Retrieval-Augmented Generation (RAG) has proven effective in integrating external knowledge into large language models (LLMs) for solving question-answer (QA) tasks. The state- of-the-art RAG approaches often use the graph data as the external data since they capture the rich semantic informa- tion and link relationships between entities. However, exist- ing graph-based RAG approaches cannot accurately identify the relevant information from the graph and also consume large numbers of tokens in the online retrieval process. To address these issues, we introduce a novel graph-based RAG approach, called A ttributed C ommunity-based H ierarchical RAG (ArchRAG), by augmenting the question using at- tributed communities, and also introducing a novel LLM- based hierarchical clustering method. To retrieve the most rel- evant information from the graph for the question, we build a novel hierarchical index structure for the attributed communi- ties and develop an effective online retrieval method. Experi- mental results demonstrate that ArchRAG outperforms exist- ing methods in both accuracy and token cost. Introduction Retrieval-Augmented Generation (RAG) has emerged as a core approach for enhancing large language models (LLMs) by enabling access to domain-specific and real-time updated knowledge beyond their pre-training corpus (2024; 2023; 2024; 2024; 2024b; 2024; 2024). By improving the trustwor- thiness and interpretability of LLMs, RAG has been widely adopted across a broad range of applications (2024a; 2024a; 2024; 2024; 2023; 2024; 2024b). Current state-of-the-art RAG approaches often use graph-structured data as exter- nal knowledge, due to its ability to capture the rich seman- tics and relationships. Given a question Q, the key idea of graph-based RAG is to retrieve relevant information (e.g., nodes, subgraphs, or textual information) from the graph, incorporate them with Q as the prompt, and feed them into the LLM, as illustrated in Figure 1. Several recent meth- ods (2024; 2024; 2024; 2024a; 2024c; 2024; 2024) address two common question answering (QA) tasks: abstract ques- tions, which require reasoning over high-level themes (e.g., “What are the potential impacts of LLMs on education?”), and specific questions, which focus on entity-centric factual details (e.g., “Who won the Turing Award in 2024?”). Copyright © 2026, Association for the Advancement of Artificial Figure 1: The general workflow of graph-based RAG, which retrieves relevant information (e.g., nodes, subgraphs, or tex- tual information) to facilitate the LLM generation. In the past year, a surge of graph-based RAG meth- ods (2025; 2024c; 2024; 2024; 2024; 2023) has emerged, each proposing different retrieval strategies to extract de- tailed information for response generation. Among them, GraphRAG (2024), proposed by Microsoft, is the most prominent and the first to leverage community summariza- tion for abstract QA. It builds a knowledge graph (KG) from the external corpus, detects communities using Lei- den (2019), and generates a summary for each community using LLMs. For abstract questions that require high-level information, it adopts a Global Search approach, traversing all communities and using LLMs to retrieve the most rel- evant summaries. In contrast, for specific questions, it em- ploys a Local Search method to retrieve entities, relevant text chunks, and low-level communities, providing the multi-hop detailed information for accurate answers. Although some methods claim that GraphRAG underper- forms and is difficult to apply in practice (2024; 2024a), our re-examination shows that it is primarily constrained by the following limitations ( L): L1. Low community qual- ity: GraphRAG uses the Leiden (2019) algorithm to detect communities, but this approach relies solely on graph struc- ture and ignores the rich semantics of nodes and edges. As a result, the detected communities often consist of differ- ent themes, which leads to the poor quality of community summaries and further decreases its performance. L2. Lim- ited compatibility : While GraphRAG employs Global and Intelligence (www.aaai.org). All rights reserved. arXiv:2502.09891v3 [cs.IR] 8 Aug 2025 Local Search strategies, each retrieves graph elements at only one granularity, making it inadequate for simultane- ously addressing both abstract and specific questions and limiting its applicability in real-world open-ended scenar- ios. L3. High generation cost : Although GraphRAG per- forms well on abstract questions, analyzing all communities with LLMs is both time- and token-consuming. For exam- ple, GraphRAG detects 2,984 communities in the Multihop- RAG (2024) dataset, and answering just 100 questions in- curs a cost of approximately $650 and 106 million tokens 1, which is an impractical overhead. To tackle the above limitations of GraphRAG, in this paper, we propose a novel graph-based RAG approach, called Attributed Community-based Hierarchical RAG (ArchRAG). ArchRAG leverages attributed communities (ACs) and introduces an efficient hierarchical retrieval strat- egy to adaptively support both abstract and specific ques- tions. To mitigateL1, we detect high-quality ACs by exploit- ing both links and the attributes of nodes, ensuring that each AC comprises nodes that are not only densely connected but also share similar semantic themes (2009). We further pro- pose a novel LLM-based iterative framework for hierarchi- cal AC detection, which can incorporate any existing com- munity detection methods (2009; 2019; 2007; 2007; 2016). In each iteration, we detect ACs based on both attribute sim- ilarity and connectivity, summarize each AC using an LLM, and construct a higher-level graph by treating each AC as a node, connecting pairs with similar summaries. By iterating the above steps multiple times, we obtain a set of ACs that can be organized into a hierarchical tree structure. To effectively address L2, we organize all ACs and en- tities into a hierarchical index and retrieve relevant ele- ments from all levels to support both abstract and specific questions. Entities offer fine-grained details, while LLM- generated AC summaries capture relational structures and provide high-level condensed overviews (2024; 2018), mak- ing them suitable for both multi-hop reasoning and abstract insight extraction. To support efficient retrieval across levels, we propose C-HNSW (Community-based HNSW), a novel hierarchical index inspired by the HNSW algorithm (2018) for approximate nearest neighbor (ANN) search. To mitigate the high generation cost caused by travers- ing all communities ( L3), we propose a hierarchical search with adaptive filtering to efficiently select the most relevant ACs and entities while maintaining performance. Specifi- cally, we design an efficient hierarchical retrieval algorithm over the proposed C-HNSW index, which supports top- k nearest neighbor search across multiple levels, thereby fa- cilitating access to multi-level relevant information. Further- more, the adaptive filtering mechanism identifies the most informative results at each level, making the retrieved infor- mation complementary. We have extensively evaluated ArchRAG on real-world datasets, and the results show that it consistently outper- forms existing methods in both abstract and specific QA tasks. ArchRAG achieves a 10% higher accuracy than 1The cost of GPT-4o is $10/M tokens for output and $2.50/M tokens for input (for details, please refer to OpenAI pricing). state-of-the-art graph-based RAG methods on specific ques- tions and shows notable gains on abstract QA. Moreover, ArchRAG is very token-efficient, saving up to 250 times the token usage compared to GraphRAG (2024). In summary, our main contributions are as follows: • We present a novel graph-based RAG approach by using ACs that are organized hierarchically and detected by an LLM-based hierarchical clustering method. • To index ACs, we propose a novel hierarchical index structure called C-HNSW and also develop an efficient online retrieval method. • Extensive experiments show that ArchRAG is both highly effective and efficient, and achieves state-of-the- art performance on both abstract and specific QA tasks. Related Work In this section, we review the related works, includ- ing Retrieval-Augmentation-Generation (RAG) approaches, and LLMs for graph mining and learning. • RAG approaches. RAG has been proven to excel in many tasks, including open-ended question answer- ing (2024; 2023), programming context (2024b; 2023), SQL rewrite (2025; 2024), and data cleaning (2024; 2022; 2024). The naive RAG technique relies on retrieving query- relevant contexts from external knowledge bases to mit- igate the “hallucination” of LLMs. Recently, many RAG approaches (2025; 2024; 2024; 2024; 2024c; 2024; 2024; 2024) have adopted graph structures to organize the infor- mation and relationships within documents, leading to im- proved performance. For more details, please refer to the re- cent survey of graph-based RAG methods (2024). • LLM for graph mining. Recent advances in LLMs have offered opportunities to leverage LLMs in graph min- ing. These include using LLMs for KG construction (2024), addressing complex graph mining tasks (2024a; 2024; 2024; 2024b), and employing KG to enhance the LLM reason- ing (2023; 2024c; 2023; 2023; 2024; 2023; 2025). For in- stance, RoG (2023) proposes a planning-retrieval-reasoning framework that retrieves reasoning paths from KGs to guide LLMs conducting faithful reasoning. StructGPT (2023) and ToG (2023) treat LLMs as agents that interact with KGs to find reasoning paths leading to the correct answers. Our Approach ArchRAG We begin by presenting the overall workflow and design ra- tionale of ArchRAG, followed by detailed descriptions of each component. As illustrated in Figure 2, our proposed ArchRAG consists of two phases. In the offline indexing phase, ArchRAG first constructs a KG from the corpus, then detects ACs by a novel LLM-based hierarchical clustering method, and finally builds the C-HNSW index. During the online retrieval phase, ArchRAG first converts the question into a query vector, then retrieves relevant information from the C-HNSW index, and finally generates answers through an adaptive filtering-based generation process Our ArchRAG detects ACs by exploiting both links and attributes, and organizes ACs and entities into a novel hierar- chical index, C-HNSW, yielding the following advantages: Figure 2: ArchRAG consists of two phases: offline indexing and online retrieval. For the online retrieval phase, we show an example of using ArchRAG to answer a question in the HotpotQA dataset. 1) Each AC group densely connected entities with shared themes and a high-quality summary. 2) The hierarchical structure captures multiple levels of abstraction: lower-level entities and communities encode detailed KG knowledge, while higher-level communities provide global context, en- abling ArchRAG to address questions at varying granular- ity. and 3) The C-HNSW index efficiently retrieves relevant information across levels, supporting fast and accurate re- sponses to both abstract and specific questions. Offline Indexing KG construction. ArchRAG builds a KG by prompting the LLM to extract entities and relations from each chunk of the text corpus D. Specifically, all text contexts are seg- mented into chunks based on specified chunk length, en- abling the LLM to extract entities and relations from each chunk using in-context learning (2020), thus forming sub- graphs. These subgraphs are then merged, with entities and relations that appear repeatedly across multiple subgraphs being consolidated by the LLM to generate a complete de- scription. Finally, we get a KG, denoted by G(V, E), where V and E are sets of vertices and edges, respectively, and each vertex and edge is associated with textual attributes. LLM-based hierarchical clustering. We propose an it- erative LLM-based hierarchical clustering framework that supports arbitrary graph augmentation (e.g., KNN connec- tions and CODICIL (2013)) and graph clustering algorithms (e.g., weighted Leiden (2019), weighted spectral clustering, and SCAN (2007)). Specifically, we propose to augment the KG by linking entities if their attribute similarities are larger than a threshold, and then associate each pair of linked en- tities with a weight denoting their attribute similarity value. Next, we generate the ACs using any given graph clustering algorithm. In this way, both node attributes and structural links are jointly considered during community detection. Algorithm 1 shows the above iterative clustering process. Given a graph augmentation method Aug, clustering algo- rithm GCluster and stopping condition T , we perform the following steps in each iteration: (1) augmenting the graph (line 3); (2) computing the edge weights (lines 4-5); (3) clus- tering the augmented graph (line 6); (4) generating a sum- mary for each community using LLM (line 7); and (5) build- ing a new attributed graph where each node denotes an AC and two nodes are linked if their community members are connected (line 9). We repeat the iterations until the stop- ping condition T (such as insufficient nodes or reaching the specified level limit) is met. Since each iteration corresponds to one layer, all the ACs HC can be organized into a multi- layer hierarchical tree structure, denoted by ∆, where each community in one layer includes multiple communities in the next layer. Appendix also provides more details. C-HNSW index. Given a query, to efficiently identify the most relevant information from each layer of the hierarchical tree ∆, a naive method is to build a vector database for the ACs in each layer, which is costly in both time and space. To tackle this issue, we propose to build a single hierar- Algorithm 1: LLM-based hierarchical clustering input : G(V, E), Aug, GCluster, T 1 T ← False, HC ← ∅; 2 repeat 3 G′(V, E ′) ← Aug(G(V, E)); 4 for each e′ = (u, v) ∈ E′ do 5 update the weight of e′ as 1 − cos(zu, zv); 6 C ← GCluster(G′(V, E ′)); 7 for each c ∈ C do generate summary of c by LLM ; 8 HC ← HC ∪ C; 9 G(V, E) ← build a new graph using C and E′; // update T according to G(V, E); 10 until T =True; 11 return HC ; chical index for all the communities. Recall that the ACs in ∆ form a tree structure, and the number of nodes de- creases as the layer level increases. Since this tree struc- ture is similar to the HNSW (Hierarchical Navigable Small World) index which is the most well-known index for ef- ficient ANN search (2018), we propose to map entities and ACs of∆ into high-dimensional nodes, and then build a uni- fied Community-based HNSW (C-HNSW) index for them. • The structure of C-HNSW. Conceptually, the C- HNSW index is a list of simple graphs with links between them, denoted by H = ( G, Linter) with G = {G0 = (V0, E0), G1 = (V1, E1), · · · , GL = (VL, EL)}, where Gi is a simple graph and each node of the simple graph cor- responds to an attributed community or entity. The number L of layers in H is the same as that of ∆. Specifically, for each attributed community or entity in the i-th layer of ∆, we map it to a high-dimensional node in the i-th layer of H by using a language model (e.g., nomic-embed-text (2024)). We next establish two types of links between these high- dimensional nodes, i.e., intra-layer and inter-layer links: • Intra-layer links: These links exist between nodes in the same layers. Specifically, for each node in each layer, we link it to at least M nearest neighbors within the same layer, where M is a predefined value, and the nearest neighbors are determined according to a given distance metric d. Thus, all the intra-layer links are edges in all the simple graphs: Lintra = SL i=0 Ei. • Inter-layer links: These links cross two adjacent lay- ers. Specifically, we link each node in each layer to its nearest neighbor in the next layer. As a result, all the inter-layer links can be represented as Linter =SL i=1{(v, ψ(v))|v ∈ Vi, ψ(v) ∈ Vi−1}, where ψ(·) : Vi → Vi−1 is the injective function that identifies the nearest neighbor of each node in the lower layer. For example, in Figure 2, the C-HNSW index has three layers (simple graphs), incorporating all the ACs. Within each layer, each node is connected to its two nearest neigh- bors via intra-layer links, denoted by undirected edges. The inter-layer links are represented by arrows, e.g., the green community at layer L1 is connected to the green entity (its nearest neighbor at layer L0). Intuitively, since the two types of links above are es- tablished based on nearest neighbors, C-HNSW allows us to quickly search the relevant information for a query by traversing along with these links. Note that C-HNSW is dif- ferent from HNSW since it has intra-layer links and each node exists in only one layer. • The construction of C-HNSW.We propose a top-down approach to build a C-HNSW. Specifically, by leveraging the query process of C-HNSW, which will be introduced in on- line retrieval, nodes are progressively inserted into the in- dex starting from the top layer, connecting intra-layer links within the same layer and updating the inter-layer links. For lack of space, we give the details of the construction algo- rithm in the appendix. Online retrieval In the online retrieval phase, after obtaining the query vector for a given question, ArchRAG generates the final answer by first conducting a hierarchical search on the C-HNSW index and then analyzing and filtering the retrieved information. Hierarchical search. We propose an efficient and fast re- trieval algorithm, hierarchical search, to retrieve nodes from each layer of the C-HNSW structure. Intuitively, retrieving nodes from a given layer in C-HNSW requires starting from the top layer and searching downward through two types of links (i.e., intra-layer and inter-layer links) to locate the nearest neighbors at the given layer. In contrast, our hierar- chical search algorithm accelerates this process by reusing intermediate results, the nearest neighbors found in higher layers, as the starting node for lower layers. This approach avoids redundant computations that would otherwise arise from repeatedly searching from the top layer, thereby en- abling efficient multi-layer retrieval. Algorithm 2 illustrates hierarchical search. Given the C- HNSW H, query point q, and the numberk of nearest neigh- bors to retrieve at each layer, the hierarchical search algo- rithm can be implemented by the following iterative process: 1. Start from a random node at the highest layer L, which serves as the starting node for layer L (line 1). 2. For each layer i from the top layer L down to layer 0, the algorithm begins at the starting node and performs a greedy traversal (i.e., the SearchLayer procedure) to find the set Ri of the k nearest neighbors of q. The set Ri is then merged into the final result set R (lines 4–5). 3. The closest neighbor c of q is then obtained from Ri, and the algorithm proceeds to the next layer by traversing the inter-layer link of c, using it as the starting node for the subsequent search (lines 6–7). Specifically, the greedy traversal strategy compares the distance between the query point and the visited nodes dur- ing the search process. It maintains a candidate expansion queue Q and a dynamic nearest neighbor set K containing k elements, along with a stopping condition: • Expansion Queue Q: For each neighbor x of a visited node, if d(x, q) < d (f, q), where f is the furthest node from R to q, then x is added to the expansion queue. Algorithm 2: Hierarchical search input : H = (G, Linter), q, k. 1 s ← a random node in the highest layer L; 2 R ← ∅; 3 for i ← L, · · · , 0 do 4 Ri ← SearchLayer (Gl = (Vl, El), q, s, k); 5 R ← R ∪ Ri; 6 c ← get the nearest node from Ri; 7 s ← find the node in layer i − 1 via the inter-layer links of c; 8 return R; 9 Procedure SearchLayer(Gi = (Vi, Ei), q, s, k): 10 V ← {s}, K ← {s}, Q ← initialize a queue containing s; 11 while |K| > 0 do 12 c ← nearest node in Q; 13 f ← furthest node in K; 14 if d(c, q) > d(f, q) then break ; 15 for each neighbor x ∈ N (c) in Gi do 16 if x ∈ V then continue; 17 V ← V ∪ {x}; 18 f ← furthest node in K; 19 if d(x, q) < d(f, q) or |K| < k then 20 Q ← Q ∪ {x}, K ← K ∪ {x}; 21 if |K| > k then remove f from K; 22 return K; • Dynamic Nearest Neighbor Set K: Nodes added to C are used to update K, ensuring that it maintains no more than k elements, where k is the number of query results. • Stopping Condition: The traversal terminates if a node x expanded from Q satisfies d(n, q) > d(n, f), where f is the furthest node in K from the query point q. After completing the hierarchical search and obtaining the ACs and entities from each layer, we further extract their associated textual information. In particular, at the bottom layer, we also extract the relationships between the retrieved entities, resulting in the textual subgraph representation de- noted as R0. We denote all the retrieved textual information from each layer as Ri, where i ∈ 0, 1, . . . , L, which will be used in the adaptive filtering-based generation process. Adaptive filtering-based generation. While some opti- mized LLMs support longer text inputs, they may still en- counter issues such as the “lost in the middle” dilemma (Liu et al. 2024b). Thus, direct utilization of retrieved informa- tion comprising multiple text segments for LLM-based an- swer generation risks compromising output accuracy. To mitigate this limitation, we propose an adaptive filtering-based method that harnesses the LLM’s inherent reasoning capabilities. We first prompt the LLM to extract and generate an analysis report from the retrieved informa- tion, identifying the parts that are most relevant to answer- ing the query and assigning relevance scores to these reports. Then, all analysis reports are integrated and sorted, ensuring that the most relevant content is used to summarize the final response to the query, with any content exceeding the text limit being truncated. This process can be represented as: Ai = LLM(Pf ilter||Ri) (1) Output = LLM(Pmerge||Sort({A0, A1, · · · , An})) (2) where Pf ilter and Pmerge represent the prompts for extract- ing relevant information and summarizing, respectively, Ai, i ∈ 0 · · · n denotes the filtered analysis report. The sort func- tion orders the content based on the relevance scores from the analysis report. Experiments In this section, we conduct a comprehensive evaluation of our ArchRAG, focusing on both efficiency and performance. Setup Table 1: Datasets used in our experiments. Acc, Rec, Blue, Met, and Rou denote Accuracy, Recall, BLEU-1, METEOR, and ROUGE-L F1. Dataset Multihop-RAG HotpotQA NarrativeQA Passages 609 9,221 1,572 Tokens 1,426,396 1,284,956 121,152,448 Nodes 23,353 37,436 650,571 Edges 30,716 30,758 679,426 Questions 2,556 1,000 43,304 Metrics Acc, Rec Acc, Rec Blue, Met, Rou Datasets. We evaluate ArchRAG on both specific and abstract QA tasks. For specific QA, we use Multihop- RAG (2024), HotpotQA (2018), and NarrativeQA (2018), all of which are extensively utilized within the QA and Graph-based RAG research communities (2022; 2024; 2022; 2024; 2024; 2023). For abstract QA, we follow the GraphRAG (2024) method and reuse the Multihop-RAG corpus, prompting LLM to generate questions that convey a high-level understanding of dataset contents. The statistics of these datasets are reported in Table 1. Baselines. Our experiments consider three configurations: • Inference-only: Using an LLM to answer questions with- out retrieval, i.e., Zero-Shot and CoT (2022). • Retrieval-only: Retrieval models extract relevant chunks from all documents and use them as prompts for LLMs. We select strong and widely used retrieval models: BM25 (1994) and Vanilla RAG. • Graph-based RAG: These methods leverage graph data during retrieval. We select RAPTOR (2024), HippoRAG (2024), GraphRAG (2024), and LightRAG (2024). Partic- ularly, GraphRAG has two versions, i.e., GGraphRAG and LGraphRAG, which use global and local search methods, respectively. Similarly, LightRAG integrates local search, global search, and hybrid search, denoted by LLightRAG, HLightRAG, and HyLightRAG, respectively. In GGraphRAG, all communities below the selected level are first retrieved, and then the LLM is used to filter out ir- relevant communities. This process can be viewed as uti- lizing the LLM as a retriever to find relevant communities VR LR C1 C2 AR VR 50 46 18 18 1 LR 54 50 21 29 16 C1 82 79 50 86 18 C2 82 71 14 50 16 AR 99 84 82 84 50 (a) Comprehensiveness VR LR C1 C2 AR VR 50 64 3 12 8 LR 36 50 52 63 33 C1 97 48 50 52 46 C2 88 37 48 50 42 AR 92 67 54 58 50 (b) Diversity VR LR C1 C2 AR VR 50 39 15 21 8 LR 61 50 59 30 35 C1 85 41 50 60 42 C2 79 70 40 50 38 AR 92 65 58 62 50 (c) Empowerment VR LR C1 C2 AR VR 50 46 14 18 4 LR 54 50 48 31 22 C1 86 52 50 70 31 C2 82 69 30 50 30 AR 96 78 69 70 50 (d) Overall Figure 3: Head-to-head win rates for abstract QA, comparing each row method against each column (higher is better). VR, LR, and AR denote Vanilla RAG, HyLightRAG, and ArchRAG, respectively. within the corpus. According to the selected level of commu- nities (2024), GGraphRAG can be further categorized into C1 and C2, representing high-level and intermediate-level communities, respectively, with C2 as the default. Metrics & Implementation. For the specific QA tasks, we use Accuracy and Recall to evaluate performance on the first two datasets based on whether gold answers are in- cluded in the generations instead of strictly requiring exact matching, following (2024; 2022; 2023). We also use the official metrics of BLEU, METEOR, and ROUGE-l F1 in the NarrativeQA dataset. For the abstract QA task, we fol- low prior work (2024) and adopt a head-to-head compari- son approach using an LLM evaluator (GPT-4o). Overall, we utilize four evaluation dimensions: Comprehensiveness, Diversity, Empowerment, and Overall. For implementation, we mainly use Llama 3.1-8B (2024) as the default LLM and use nomic-embed-text (2024) as the text embedding model. We use KNN for graph augmentation and the weighted Lei- den algorithm for community detection. For retrieval item k, we search the same number of items at each layer, with k = 5 as the default. All methods are required to complete index construction and query execution within 3 days, re- spectively. Additional details are provided in the appendix, and our codes are provided in the supplementary material. Overall results We compare our method with baseline methods in solving both abstract and specific QA tasks. • Results of abstract QA tasks. We compare ArchRAG against baselines across four dimensions on the Multihop- RAG dataset. For the LightRAG, we only compare the Hy- LightRAG method, as it represents the best version (2024). As shown in Figure 3, GGraphRAG outperforms other base- line methods, while our method achieves comparable perfor- mance on the diversity and empowerment dimensions and significantly surpasses it on the comprehensive dimension. Overall, by leveraging ACs, ArchRAG demonstrates supe- rior performance in addressing abstract QA tasks. • Results of specific QA tasks. Table 2 reports the performance of each method on three datasets. Note that GGraphRAG fails to complete querying on the NarrativeQA dataset within the 3-day time limit. RAPTOR is unable to build the index on datasets like HotpotQA, which con- tains a large number of text chunks. Its Gaussian Mix- ture Model (GMM) clustering algorithm requires prohibitive computational time and suffers from non-termination issues during clustering. Clearly, ArchRAG demonstrates a sub- stantial performance advantage over other baseline meth- ods on these datasets. The experimental results suggest that not all communities are suitable for specific QA tasks, as the GGraphRAG performs poorly. Furthermore, GraphRAG does not consider node attributes during clustering, which causes the community’s summary to become dispersed, making it difficult for the LLM to extract relevant informa- tion from a large number of communities. Thus, we gain an interesting insight: LLM may not be a good retriever, but is a good analyzer.We further analyze the reasons behind the un- derperformance of each graph-based RAG method and sup- port our claims with empirical evidence in the appendix. Zero-Shot CoT BM25 Vanilla RAG RAPTOR HippoRAG LLightRAG HLightRAG HyLightRAG LGraphRAG GGraphRAG ArchRAG Multihop-RAG HotpotQA102103104105 N/A time (s) (a) Time cost Multihop-RAG HotpotQA100101102103 N/A token (M) (b) Token cost Figure 4: Comparison of query efficiency. • Efficiency of ArchRAG.We compare the time cost and token usage of ArchRAG with those of other baseline meth- ods. As shown in Figure 4, ArchRAG demonstrates signifi- cant time and cost efficiency for online queries. For example, token usage on the HotpotQA dataset is cut by 250× with ArchRAG compared to GraphRAG-Global, from 1,394M tokens down to 5.1M tokens. To further evaluate ArchRAG, we test the efficiency of hierarchical search, indexing performance, and effective- ness on an additional dataset, RAG-QA Arena (2024). Re- sults show that ArchRAG achieves up to5.4× faster retrieval than basic HNSW, maintains efficient indexing, and achieves state-of-the-art performance on the RAG-QA Arena dataset. Additional details are provided in the appendix. Table 2: Performance comparison of different methods across various datasets for solving specific QA tasks. The best and second-best results are marked in bold and underlined. OOT: Not finished within 3 days. Baseline Type Method Multihop-RAG HotpotQA NarrativeQA (Accuracy) (Recall) (Accuracy) (Recall) (BLEU-1) (METEOR) (ROUGE-L F1) Inference-only Zero-shot 47.7 23.6 28.0 31.8 8.0 7.9 8.6 CoT 54.5 28.7 32.5 39.7 5.0 8.1 6.4 Retrieval-only BM25 37.6 19.4 49.7 53.6 2.0 4.9 2.8 Vanilla RAG 58.6 31.4 50.6 56.1 2.0 4.9 2.8 Graph-based RAG RAPTOR 59.1 34.1 N/A N/A 5.5 12.5 9.1 HippoRAG 38.9 19.1 51.3 56.8 2.2 5.0 2.8 LLightRAG 44.1 25.1 34.1 41.8 4.5 8.7 6.6 HLightRAG 48.5 28.7 25.6 33.3 4.4 8.1 6.1 HyLightRAG 50.3 30.3 35.6 43.3 5.0 9.4 7.0 LGraphRAG 40.1 23.8 29.7 35.5 3.9 3.3 3.5 GGraphRAG 45.9 28.4 33.5 42.6 OOT OOT OOT Our proposed ArchRAG 68.8 37.2 65.4 69.2 11.5 15.6 17.6 Detailed Analysis To better understand the effectiveness of our proposed ArchRAG, we perform the following ablation study and ex- periment with a GGraphRAG variant. • Ablation study. To evaluate the contributions of different components, we design several ArchRAG vari- ants and conduct ablation experiments. These include two modifications to the LLM-based hierarchical clus- tering framework, three targeting core design elements in ArchRAG—attributes, hierarchy, and communities—and one direct prompting variant, as detailed below: • Spec.: Spectral clustering instead of weighted Leiden. • Spec. (No Aug): Spectral clustering without graph aug- mentation. • Leiden: Replaces our clustering framework with Leiden. • Single-Layer: Replaces our hierarchical index and search with a single-layer community. • Entity-Only: Generate the response using entities only. • Direct Prompt: Direct prompts the LLM to generate the response without the adaptive filtering-based generation. As shown in Table 3, the performance of ArchRAG on specific QA tasks decreases when each feature is removed, with the removal of the community component resulting in the most significant drop. Additionally, the direct vari- ant demonstrates that the adaptive filtering-based generation process can effectively extract relevant information from re- trieved elements. • Impact of Attributed Communities in RAG. We propose a new variant, GraphRAG+AC, which replaces the original Leiden-based communities in GraphRAG with our ACs, while preserving the original Global Search pipeline. As shown in Table 4, this variant results in a significant per- formance improvement compared to the original approach. Specifically, on the HotpotQA dataset, GraphRAG+AC im- proves accuracy by 51% compared to GGraphRAG. We further test ArchRAG under different LLM back- bones, various top-k retrieval settings, and evaluate the Table 3: Comparing the performance of different variants of ArchRAG on the specific QA tasks. Acc and Rec denote Accuracy and Recall, respectively. Method variants Multihop-RAG HotpotQA (Acc) (Rec) (Acc) (Rec) ArchRAG 68.8 37.2 65.4 69.2 - Spec. 67.1 36.7 60.5 63.7 - Spec. (No Aug) 62.8 34.2 64.8 63.2 - Leiden 63.2 34.1 61.7 64.8 - Single-Layer 63.8 36.4 60.1 63.6 - Entity-Only 61.2 34.7 59.9 63.1 - Direct Prompt 59.9 29.6 40.7 45.4 Table 4: Results of GraphRAG variants using our ACs. Method Multihop-RAG HotpotQA (Acc) (Rec) (Acc) (Rec) GGraphRAG 45.9 28.4 33.5 42.6 GraphRAG+AC 49.3 31.4 50.6 (51.0% ↑) 52.8 ArchRAG 68.8 37.2 65.4 69.2 community quality of its LLM-based hierarchical cluster- ing. ArchRAG consistently achieves state-of-the-art perfor- mance across all backbones, remains robust under vary- ing top-k values, and demonstrates high-quality community detection. We also provide additional case studies of our ArchRAG, which illustrate the detailed workflow. Detailed experiments are provided in the appendix. Conclusion In this paper, we propose ArchRAG, a novel graph-based RAG approach, by augmenting the question using attributed communities from the knowledge graph built on the external corpus, and building a novel index for efficient retrieval of relevant information. Our experiments show that ArchRAG is highly effective and efficient. In the future, we will explore fast parallel graph-based RAG methods to process large- scale external corpus. References Angelidis, S.; and Lapata, M. 2018. Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised. arXiv preprint arXiv:1808.08858. Asai, A.; Wu, Z.; Wang, Y .; Sil, A.; and Hajishirzi, H. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511. Brown, T. B. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Cali´nski, T.; and Harabasz, J. 1974. A dendrite method for cluster analysis. Communications in Statistics-theory and Methods, 3(1): 1–27. Cao, Y .; Han, S.; Gao, Z.; Ding, Z.; Xie, X.; and Zhou, S. K. 2024. Graphinsight: Unlocking insights in large language models for graph structure understanding. arXiv preprint arXiv:2409.03258. Charikar, M. S. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth an- nual ACM symposium on Theory of computing, 380–388. Chen, N.; Li, Y .; Tang, J.; and Li, J. 2024a. Graphwiz: An instruction-following language model for graph computa- tional problems. In KDD. Chen, S.; He, Y .; Cui, W.; Fan, J.; Ge, S.; Zhang, H.; Zhang, D.; and Chaudhuri, S. 2024b. Auto-Formula: Recommend Formulas in Spreadsheets using Contrastive Learning for Ta- ble Representations. Proceedings of the ACM on Manage- ment of Data, 2(3): 1–27. Chen, S.; Tang, N.; Fan, J.; Yan, X.; Chai, C.; Li, G.; and Du, X. 2023. Haipipe: Combining human-generated and machine-generated pipelines for data preparation. Proceed- ings of the ACM on Management of Data, 1(1): 1–26. Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Edge, D.; Trinh, H.; Cheng, N.; Bradley, J.; Chao, A.; Mody, A.; Truitt, S.; and Larson, J. 2024. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130. Fan, W.; Ding, Y .; Ning, L.; Wang, S.; Li, H.; Yin, D.; Chua, T.-S.; and Li, Q. 2024. A survey on rag meeting llms: To- wards retrieval-augmented large language models. In Pro- ceedings of the 30th ACM SIGKDD Conference on Knowl- edge Discovery and Data Mining, 6491–6501. Gao, Y .; Xiong, Y .; Gao, X.; Jia, K.; Pan, J.; Bi, Y .; Dai, Y .; Sun, J.; and Wang, H. 2023. Retrieval-augmented gen- eration for large language models: A survey. arXiv preprint arXiv:2312.10997. Ghimire, A.; Prather, J.; and Edwards, J. 2024. Gen- erative AI in Education: A Study of Educators’ Aware- ness, Sentiments, and Influencing Factors. arXiv preprint arXiv:2403.15586. Grover, A.; and Leskovec, J. 2016. node2vec: Scalable fea- ture learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, 855–864. Guo, Z.; Xia, L.; Yu, Y .; Ao, T.; and Huang, C. 2024. Ligh- tRAG: Simple and Fast Retrieval-Augmented Generation. arXiv e-prints, arXiv–2410. Guti´errez, B. J.; Shu, Y .; Gu, Y .; Yasunaga, M.; and Su, Y . 2024. HippoRAG: Neurobiologically Inspired Long- Term Memory for Large Language Models. arXiv preprint arXiv:2405.14831. Han, R.; Zhang, Y .; Qi, P.; Xu, Y .; Wang, J.; Liu, L.; Wang, W. Y .; Min, B.; and Castelli, V . 2024. RAG-QA Arena: Eval- uating Domain Robustness for Long-form Retrieval Aug- mented Question Answering. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 4354–4374. He, X.; Tian, Y .; Sun, Y .; Chawla, N. V .; Laurent, T.; LeCun, Y .; Bresson, X.; and Hooi, B. 2024. G-retriever: Retrieval- augmented generation for textual graph understanding and question answering. arXiv preprint arXiv:2402.07630. Hu, Y .; and Lu, Y . 2024. Rag and rau: A survey on retrieval- augmented language model in natural language processing. arXiv preprint arXiv:2404.19543. Hu, Z.; Xu, Y .; Yu, W.; Wang, S.; Yang, Z.; Zhu, C.; Chang, K.-W.; and Sun, Y . 2022. Empowering language models with knowledge graph reasoning for question answering. arXiv preprint arXiv:2211.08380. Huang, Y .; and Huang, J. 2024. A Survey on Retrieval- Augmented Text Generation for Large Language Models. arXiv preprint arXiv:2404.10981. Huang, Y .; Zhang, S.; and Xiao, X. 2025. KET-RAG: A Cost-Efficient Multi-Granular Indexing Framework for Graph-RAG. arXiv preprint arXiv:2502.09304. Jeong, S.; Baek, J.; Cho, S.; Hwang, S. J.; and Park, J. C. 2024. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. arXiv preprint arXiv:2403.14403. Jiang, J.; Zhou, K.; Dong, Z.; Ye, K.; Zhao, W. X.; and Wen, J.-R. 2023. Structgpt: A general framework for large lan- guage model to reason over structured data. arXiv preprint arXiv:2305.09645. Koˇcisk`y, T.; Schwarz, J.; Blunsom, P.; Dyer, C.; Hermann, K. M.; Melis, G.; and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. Transactions of the As- sociation for Computational Linguistics, 6: 317–328. Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa, Y . 2022. Large language models are zero-shot reason- ers. Advances in neural information processing systems, 35: 22199–22213. Li, D.; Yang, S.; Tan, Z.; Baik, J. Y .; Yun, S.; Lee, J.; Chacko, A.; Hou, B.; Duong-Tran, D.; Ding, Y .; et al. 2024. DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer’s Disease Questions with Scientific Literature. arXiv preprint arXiv:2405.04819. Li, Y .; Wang, S.; Ding, H.; and Chen, H. 2023. Large lan- guage models in finance: A survey. In Proceedings of the fourth ACM international conference on AI in finance, 374– 382. Li, Z.; Yuan, H.; Wang, H.; Cong, G.; and Bing, L. 2025. LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency.Proceedings of the VLDB Endowment, 1(18): 53–65. Liu, L.; Yang, X.; Lei, J.; Liu, X.; Shen, Y .; Zhang, Z.; Wei, P.; Gu, J.; Chu, Z.; Qin, Z.; et al. 2024a. A Survey on Medical Large Language Models: Technology, Applica- tion, Trustworthiness, and Future Directions. arXiv preprint arXiv:2406.03712. Liu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua, M.; Petroni, F.; and Liang, P. 2024b. Lost in the middle: How language models use long contexts.Transactions of the Association for Computational Linguistics, 12: 157–173. Luo, L.; Li, Y .-F.; Haffari, G.; and Pan, S. 2023. Reasoning on graphs: Faithful and interpretable large language model reasoning. arXiv preprint arXiv:2310.01061. Ma, S.; Xu, C.; Jiang, X.; Li, M.; Qu, H.; Yang, C.; Mao, J.; and Guo, J. 2024. Think-on-Graph 2.0: Deep and Faith- ful Large Language Model Reasoning with Knowledge- guided Retrieval Augmented Generation. arXiv preprint arXiv:2407.10805. Malkov, Y . A.; and Yashunin, D. A. 2018. Efficient and ro- bust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4): 824–836. Mallen, A.; Asai, A.; Zhong, V .; Das, R.; Khashabi, D.; and Hajishirzi, H. 2022. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511. Mavromatis, C.; and Karypis, G. 2024. GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning. arXiv preprint arXiv:2405.20139. Naeem, Z. A.; Ahmad, M. S.; Eltabakh, M.; Ouzzani, M.; and Tang, N. 2024. RetClean: Retrieval-Based Data Clean- ing Using LLMs and Data Lakes. Proceedings of the VLDB Endowment, 17(12): 4421–4424. Narayan, A.; Chami, I.; Orr, L.; and R ´e, C. 2022. Can Foundation Models Wrangle Your Data? Proceedings of the VLDB Endowment, 16(4): 738–746. Nie, Y .; Kong, Y .; Dong, X.; Mulvey, J. M.; Poor, H. V .; Wen, Q.; and Zohren, S. 2024. A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges. arXiv preprint arXiv:2406.11903. Nussbaum, Z.; Morris, J. X.; Duderstadt, B.; and Mulyar, A. 2024. Nomic Embed: Training a Reproducible Long Con- text Text Embedder. arXiv:2402.01613. Peng, B.; Zhu, Y .; Liu, Y .; Bo, X.; Shi, H.; Hong, C.; Zhang, Y .; and Tang, S. 2024. Graph retrieval-augmented genera- tion: A survey. arXiv preprint arXiv:2408.08921. Qian, Y .; He, Y .; Zhu, R.; Huang, J.; Ma, Z.; Wang, H.; Wang, Y .; Sun, X.; Lian, D.; Ding, B.; et al. 2024. UniDM: A Unified Framework for Data Manipulation with Large Lan- guage Models. Proceedings of Machine Learning and Sys- tems, 6: 465–482. Robertson, S. E.; and Walker, S. 1994. Some simple effec- tive approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR’94: Proceedings of the Sev- enteenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organ- ised by Dublin City University, 232–241. Springer. Ruan, Y .; Fuhry, D.; and Parthasarathy, S. 2013. Efficient community detection in large networks using content and links. In Proceedings of the 22nd international conference on World Wide Web, 1089–1098. Sarthi, P.; Abdullah, S.; Tuli, A.; Khanna, S.; Goldie, A.; and Manning, C. D. 2024. Raptor: Recursive abstrac- tive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059. Schick, T.; Dwivedi-Yu, J.; Dess`ı, R.; Raileanu, R.; Lomeli, M.; Hambro, E.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36. Siriwardhana, S.; Weerasekera, R.; Wen, E.; Kaluarachchi, T.; Rana, R.; and Nanayakkara, S. 2023. Improving the do- main adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics, 11: 1–17. Sun, J.; Xu, C.; Tang, L.; Wang, S.; Lin, C.; Gong, Y .; Shum, H.-Y .; and Guo, J. 2023. Think-on-graph: Deep and respon- sible reasoning of large language model with knowledge graph. arXiv preprint arXiv:2307.07697. Sun, Z.; Zhou, X.; and Li, G. 2024. R-Bot: An LLM-based Query Rewrite System. arXiv preprint arXiv:2412.01661. Tang, J.; Zhang, Q.; Li, Y .; and Li, J. 2024. Grapharena: Benchmarking large language models on graph computa- tional problems. arXiv preprint arXiv:2407.00379. Tang, Y .; and Yang, Y . 2024. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. arXiv preprint arXiv:2401.15391. Traag, V . A.; Waltman, L.; and Van Eck, N. J. 2019. From Louvain to Leiden: guaranteeing well-connected communi- ties. Scientific reports, 9(1): 1–12. V on Luxburg, U. 2007. A tutorial on spectral clustering. Statistics and computing, 17: 395–416. Wang, J.; Fu, J.; Wang, R.; Song, L.; and Bian, J. 2025. PIKE-RAG: sPecIalized KnowledgE and Rationale Aug- mented Generation. arXiv preprint arXiv:2501.11551. Wang, J.; Ning, H.; Peng, Y .; Wei, Q.; Tesfai, D.; Mao, W.; Zhu, T.; and Huang, R. 2024a. A Survey on Large Language Models from General Purpose to Medical Appli- cations: Datasets, Methodologies, and Evaluations. arXiv preprint arXiv:2406.10303. Wang, K.; Duan, F.; Wang, S.; Li, P.; Xian, Y .; Yin, C.; Rong, W.; and Xiong, Z. 2023. Knowledge-driven cot: Exploring faithful reasoning in llms for knowledge-intensive question answering. arXiv preprint arXiv:2308.13259. Wang, S.; Xu, T.; Li, H.; Zhang, C.; Liang, J.; Tang, J.; Yu, P. S.; and Wen, Q. 2024b. Large language mod- els for education: A survey and outlook. arXiv preprint arXiv:2403.18105. Wang, Y .; Lipka, N.; Rossi, R. A.; Siu, A.; Zhang, R.; and Derr, T. 2024c. Knowledge graph prompting for multi- document question answering. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, 19206– 19214. Wu, J.; Zhu, J.; Qi, Y .; Chen, J.; Xu, M.; Menolascina, F.; and Grau, V . 2024a. Medical graph rag: Towards safe medi- cal large language model via graph retrieval-augmented gen- eration. arXiv preprint arXiv:2408.04187. Wu, S.; Xiong, Y .; Cui, Y .; Wu, H.; Chen, C.; Yuan, Y .; Huang, L.; Liu, X.; Kuo, T.-W.; Guan, N.; et al. 2024b. Retrieval-augmented generation for natural language pro- cessing: A survey. arXiv preprint arXiv:2407.13193. Xu, S.; Pang, L.; Yu, M.; Meng, F.; Shen, H.; Cheng, X.; and Zhou, J. 2024. Unsupervised Information Refinement Train- ing of Large Language Models for Retrieval-Augmented Generation. arXiv preprint arXiv:2402.18150. Xu, X.; Yuruk, N.; Feng, Z.; and Schweiger, T. A. 2007. Scan: a structural clustering algorithm for networks. In Pro- ceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, 824–833. Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y .; Cohen, W. W.; Salakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answer- ing. arXiv preprint arXiv:1809.09600. Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y . 2022. React: Synergizing reasoning and act- ing in language models. arXiv preprint arXiv:2210.03629. Yu, H.; Gan, A.; Zhang, K.; Tong, S.; Liu, Q.; and Liu, Z. 2024. Evaluation of Retrieval-Augmented Generation: A Survey. arXiv preprint arXiv:2405.07437. Zhang, N.; Choubey, P. K.; Fabbri, A.; Bernadett-Shapiro, G.; Zhang, R.; Mitra, P.; Xiong, C.; and Wu, C.-S. 2024a. SiReRAG: Indexing Similar and Related Information for Multihop Reasoning. arXiv preprint arXiv:2412.06206. Zhang, Q.; Hong, X.; Tang, J.; Chen, N.; Li, Y .; Li, W.; Tang, J.; and Li, J. 2024b. Gcoder: Improving large lan- guage model for generalized graph problem solving. arXiv preprint arXiv:2410.19084. Zhao, P.; Zhang, H.; Yu, Q.; Wang, Z.; Geng, Y .; Fu, F.; Yang, L.; Zhang, W.; and Cui, B. 2024. Retrieval-augmented generation for ai-generated content: A survey.arXiv preprint arXiv:2402.19473. Zheng, Y .; Gan, W.; Chen, Z.; Qi, Z.; Liang, Q.; and Yu, P. S. 2024. Large language models for medicine: a survey. International Journal of Machine Learning and Cybernet- ics, 1–26. Zhou, Y .; Cheng, H.; and Yu, J. X. 2009. Graph clustering based on structural/attribute similarities. Proceedings of the VLDB Endowment, 2(1): 718–729. Zhu, Y .; Wang, X.; Chen, J.; Qiao, S.; Ou, Y .; Yao, Y .; Deng, S.; Chen, H.; and Zhang, N. 2024. Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities. World Wide Web, 27(5): 58. Method details of ArchRAG LLM-based hierarchical clustering For example, as shown in the second step of offline index- ing in Figure 2, LLM-based hierarchical clustering first en- hances the original KG at layer L0 by adding similar edges. Next, the weight of each edge is calculated based on the strength of the relationship between the node embeddings, and a weighted clustering algorithm is applied to obtain four communities. Based on the existence of links between nodes within a community, the topology of communities at layer L1 is constructed, resulting in a graph of communities. This process is repeated, ultimately generating a clustering result consisting of three layers of hierarchical communities. More details of C-HNSW • Introduction of HNSW. We provide a brief introduc- tion to HNSW, an efficient Approximate Nearest Neighbor Search (ANNS) technique for vector databases. Definition 1 (Hierarchical Navigable Small World (HNSW) (Malkov and Yashunin 2018)) . HNSW is a graph-based ANNS algorithm that consists of a multi-layered index struc- ture, where each node uniquely corresponds to a vector in the database. Given a set S containing n vectors, the con- structed HNSW can be represented as a pair H = ( G, C). G = {G0, G1, . . . , GL} is a set of simple graphs (also called layers) Gi = ( Vi, Ei), where i ∈ { 0, 1, . . . , L} and VL ⊂ VL−1 ⊂ · · · ⊂ V1 ⊂ V0 = S. C records the inter- layer mappings of edges between the same node across ad- jacent layers C = SL−1 i=0 {(v, ϕ(v))|v ∈ Vi, ϕ(v) ∈ Vi+1}, where ϕ(v) : Vi → Vi+1 is the mapping function for the same node across two adjacent layers. The nodes in the multi-layer graph of HNSW are orga- nized in a nested structure, where each node at each layer is connected to its nearest neighbors. During a query, the search begins at the top layer and quickly identifies the node closest to q through a greedy search. Then, through inter- layer mapping, the search proceeds to the next lower layer. This process continues until all approximate nearest neigh- bors are identified in G0. • The construction of C-HNSW. A naive approach to build the C-HNSW index is to build nodes first and then establish the two types of links by finding the nearest neigh- bors of each node. However, the process of finding the near- est neighbor is costly. To accelerate the construction, we pro- pose a top-down approach by borrowing the idea of HNSW construction. The construction of C-HNSW is illustrated in Algorithm 3. The construction algorithm of C-HNSW fol- lows a top-down approach. Using the query process of C- HNSW, we obtain the M nearest neighbors of each node in its layer, and the inter-layer links are continuously updated during this process. When inserting a node x at layer i , if the nearest neighbor at the layer i + 1 is cj, the inter-layer link of cj is updated in the following two cases: • Node cj does not have a inter-layer link to the layer i. • The distance from node cj to node x is smaller than the distance from cj to its previous nearest neighbor x′, i.e., d(cj, x) < d(cj, x′). After all nodes at layer i (i < L ) have been inserted, we check each node at layer i + 1 to ensure it has a inter-layer link, confirming the traverse from the higher layer to the lower layer. Algorithm 3: C-HNSW construction input : The Hierarchical community HC, KG G(V, E), maximum number of connections for each node M. 1 H ← ∅ ; 2 V ← {HC ∪ V } // Get all nodes of each layer. 3 for each layer l ← L · · · 0 do 4 for each node v ∈ Vl do 5 if l ̸= L then 6 R ← SearchLayer(Gl = (Vl, El), q, s, 1); 7 c ← get the nearest node from R; 8 s ← node in layer l − 1 via c’s inter-layer link; 9 if s is null or d(c, s) > d(c, v) then 10 update v as c’s inter-layer link 11 if l = L or s is null then s ← random node in layer l; 12 R ← SearchLayer(Gl = (Vl, El), q, s, M); 13 add edges between v and R, update El; 14 H ← H ∪ Gl = (Vl, El) 15 return H; Complexity analysis of ArchRAG We now analyze the complexity of our ArchRAG approach. Since the token cost is more important in the era of LLM, we replace the space complexity with the token cost. The offline indexing process of ArchRAG includes the KG construction, hierarchical clustering, and C-HNSW con- struction. The time complexity and token usage are as fol- lows: Lemma 1. Given a large text corpus or a large set of text documents with a total of D tokens, the time complexity of the offline indexing process of ArchRAG isO(I D w + 1−aL 1−a (n∗ t + I D w + π(m) + n log n)), where I is the generation time of the LLM for a single inference, w is the specified token size of one chunk, n and m are the number of entities and relations in the extracted KG,L is the height of the resulting hierarchical community structure, anda is the average ratio of the number of nodes between two consecutive layers, with 0 < a < 1. For a given embedding model, the computation time for the embedding of an entity description is denoted by t, while a specific clustering method is typically a function of m, represented as π(m). Proof. For the corpus D, we use the LLM to infer and ex- tract the KG from each chunk of sizew, resulting in a cost of O(I D w ) for constructing the KG. Since the size of all com- munity summaries in a single layer would not exceed the length of the corpus, their LLM inference time is also less than O(I D w ). For each layer of clustering, the embedding of each point must be computed, and clustering is performed with time complexity of π(m). In the C-HNSW construc- tion, each point requires O(nlogn) time to perform the k- nearest neighbor search and establish connections, which is similar to the proof in (Malkov and Yashunin 2018). For an L-layer multi-layer graph structure, where the number of nodes decreases by a factor of a between two consecutive layers, the increase in clustering and C-HNSW construction time is given by: 1−aL 1−a . Lemma 2. Given a large text corpus or a large set of text documents with a total of D tokens, the number of to- kens used in the offline indexing process of ArchRAG is O(D(1 + 1−aL 1−a )), where L is the height of the resulting hi- erarchical community structure anda is the average ratio of the number of nodes between two consecutive layers, with 0 < a < 1. Proof. Based on the above proof, the token cost for con- structing the KG is O(D), while the token cost for the sum- maries does not exceed O(D 1−aL 1−a ). Generally, L is O(log n), where n is the number of ex- tracted entities. However, due to the constraints of commu- nity clustering, L is typically constant, usually no greater than 5. Next, we analyze the time complexity and token usage in the online query process of the ArchRAG. Lemma 3. Given a C-HNSW withL layers constructed from a large text corpus or a large set of text documents, the time complexity of a sequentially executed single online retrieval query in ArchRAG is O(e + LkI + Lk log(n)), where I is the generation time of the LLM for a single inference, e is the time cost of computing the query embedding, k is the number of nodes retrieved at each layer, andn is the number of nodes at the lowest layer in C-HNSW. Proof. ArchRAG first computes the embedding of the query, which takes O(e) time. For each layer, querying one nearest neighbor takes no more thanO(log(n)) time, similar to the proof in (Malkov and Yashunin 2018). In the Adap- tive filtering-based generation, the content of each query is analyzed and inferred, requiring O(LkI) time. Therefore, the total time for the online retrieval query is O(e + LkI + Lk log(n)). Lemma 4. Given a C-HNSW withL layers constructed from a large text corpus or a large set of text documents, the number of tokens used for a single online retrieval query in ArchRAG is O(kL(c + P ))), where k is the number of neighbors retrieved at each layer, c is the average token of the retrieved content, and P is the token of the prompt. Proof. The token consumption for analyzing all retrieved in- formation is O(kL(c + P ))), while the token consumption for generating the final response is of constant order. There- fore, the total token consumption for the online retrieval is O(kL(c + P ))). In practice, multiple retrieved contents are combined, al- lowing the LLM to analyze them together, provided the total token count does not exceed the token size limit. As a result, the time and token usage for online retrieval are lower than those required for analysis. Experimental details Metrics This section provides additional details on the metrics. • Metrics for specific QA tasks. We choose accuracy as the evaluation metric based on whether the gold answers are included in the model’s generations rather than strictly requiring an exact match, following (Schick et al. 2024; Mallen et al. 2022; Asai et al. 2023). This is because LLM outputs are typically uncontrollable, making it difficult for them to match the exact wording of standard answers. Sim- ilarly, we choose recall as the metric instead of precision, as it better reflects the accuracy of the generated responses. Additionally, when calculating recall, we adopt the same ap- proach as previous methods (Guti´errez et al. 2024; Asai et al. 2023): if the golden answer or the generated output contains “yes” or “no”, the recall for that question is set to 0. There- fore, the recall metric is not perfectly correlated with accu- racy. • Metrics for abstract QA tasks. Following exist- ing works, we use an LLM to generate abstract questions, with the prompts shown in Figure 13, defining ground truth for abstract questions, particularly those involving complex high-level semantics, poses significant challenges. We build on existing works (Edge et al. 2024; Guo et al. 2024) to address this and adopt an LLM-based multi-dimensional comparison method (including comprehensiveness, diver- sity, empowerment, and overall). We employ a robust LLM, specifically GPT-4o, to rank each baseline against our method. Figure 14 shows the evaluation prompt we use. • Metrics of community quality. We select the follow- ing metrics to evaluate the quality of the community: 1. Calinski-Harabasz Index (CHI) (Cali´nski and Harabasz 1974): A higher value of CHI indicates better cluster- ing results because it means that the data points are more spread out between clusters than they are within clusters. It is an internal evaluation metric where the assessment of the clustering quality is based solely on the dataset and the clustering results and not on external ground-truth la- bels. The CHI is calculated by between-cluster separation and within-cluster dispersion: CHI = N − C C − 1 PC i=1 ni||ci − c||2 PC i=1 P x∈Ci ||x − ci||2 . (3) N is the number of nodes. C is the number of clusters.ni is the number of nodes in clusteri. Ci is the i−th cluster. ci is the centroid of cluster Ci. c is the overall centroid of the datasets. x is the feature of the target node. 2. Cosine Similarity (Sim) (Charikar 2002): Cosine similar- ity is a measure of similarity between two non-zero vec- tors defined in an inner product space. In this paper, for each cluster, we calculate the similarity between the cen- troid of this cluster and each node in this cluster: Sim = 1 N CX i=1 X x∈Ci Cosine(x, ci). (4) N is the number of nodes. C is the number of clusters. Ci is the i-th cluster. ci is the centroid of cluster Ci. x is the feature of the target node. Cosine(x, y) = Pn i=1 xiyipPn i=1 x2 i pPn i=1 y2 i . (5) Implementation details We implement our ArchRAG in Python, while C-HNSW is implemented in C++ and provides a Python interface for integration. We implement C-HNSW using the FAISS framework and employ the inner product metric to mea- sure the proximity between two vectors. All the experi- ments were conducted on a Linux operating system run- ning on a machine with an Intel Xeon 2.0 GHz CPU, 1024GB of memory, and 8 NVIDIA GeForce RTX A5000 GPUs, each with 24 GB of memory. All methods uti- lize 10 concurrent LLM calls, and to maintain consis- tency, other parallel computations in the method, such as embedding calculations, also use 10 concurrent threads. Figure 15 demonstrates the prompt used in Adaptive filtering-based generation. Please refer to our repository (https://anonymous.4open.science/r/H-CAR-AG-1E0B/) to view the detailed prompts. • Details of clustering methods. The graph augmen- tation methods we choose are the KNN algorithm, which computes the similarity between each node, and CODI- CIL (Ruan, Fuhry, and Parthasarathy 2013), which selects and adds the top similar edges to generate better clustering results. In the KNN method, we set the K value as the aver- age degree of nodes in the KG. Additional experiments • Efficiency of hierarchical search (C-HNSW). To evalu- ate the efficiency of C-HNSW, we conduct experiments on a synthetic hierarchical dataset comprising 11 layers. The bot- tom layer (Layer 0) contains 10 million nodes, and the num- ber of nodes decreases progressively across higher layers by randomly dividing each layer’s size by 3 or 4. The top layer (Layer 10), for example, contains only 74 nodes. This hier- archical structure simulates the process of LLM-based hier- archical clustering. Each node is assigned a randomly gener- ated 3072-dimensional vector, simulating high-dimensional embeddings such as those produced by text or image en- coders (e.g., text-embedding-3-large, used in Chat- GPT, can generate 3072-dimensional vectors, and text- embedding-v3 can generate 1024-dimensional vectors). For each layer, we generate 200 random queries and compute the top-5 nearest neighbors for each query. Both C-HNSW and Base-HNSW are configured with identical parameters: M = 32, ef Search = 100, and ef Construction = 100. Importantly, our method maintains comparable retrieval ac- curacy to Base-HNSW, with recall of 0.5537 and 0.6058, respectively. We compare our hierarchical search based on C-HNSW with a baseline approach (Base-HNSW), which indepen- dently builds a vector index for attributed communities at each layer and performs retrieval separately for each. As shown in Figure 5, on the large-scale synthetic dataset, C- HNSW achieves up to a 5.4× speedup (On level 1, C-HNSW takes 1.861 seconds, while Base-HNSW takes 10.125 sec- onds.) and is on average 3.5× faster than Base-HNSW. Ad- ditional details are provided in the appendix. Base-HNSW C-HNSW 0 1 2 3 4 5 6 7 8 9 10Avg.051015 Level time (s) Figure 5: C-HNSW and Base-HNSW query efficiency. We also conducted experiments on a synthetic dataset of 1024-dimensional vectors, keeping all other parameters un- changed. The results are shown in Figure 6. As 1024 di- mensions are more representative of commonly used high- dimensional embeddings, our method still achieves over 5× speedup in the best case and an average speedup of 3× com- pared to the baseline. Base-HNSW C-HNSW 0 1 2 3 4 5 6 7 8 9 10Avg.051015 Level time (s) Figure 6: C-HNSW and Base-HNSW query efficiency on 1024-dimensional vector dataset. • Efficiency of indexing phrase. Figure 7 shows the in- dex construction time and token usage for different methods. The cost of building an index for ArchRAG is similar to that of GraphRAG, but due to the need for community summa- rization, both take a higher time cost and token usage than HippoRAG. RAPTOR HippoRAG LightRAG GraphRAG ArchRAG Multihop-RAGHotpotQA103 104 OOTtime (s) (a) Time cost Multihop-RAGHotpotQA1 10 100 OOT token (M) (b) Token cost Figure 7: Comparison of indexing efficiency. Table 5: Comparing ArchRAG with other RAG methods on the specific QA tasks under different LLM backbone models. LLM backbone Methods Multihop-RAG HotpotQA (Accuracy) (Recall) (Accuracy) (Recall) Llama3.1-8B Vanilla RAG 58.6 31.4 50.6 56.1 HippoRAG 38.9 19.1 51.3 56.8 RAPTOR 59.1 34.1 N/A N/A ArchRAG 68.8 37.2 65.4 69.2 GPT-3.5-turbo Vanilla RAG 65.9 32.8 60.7 65.9 HippoRAG 68.9 31.4 58.0 62.3 RAPTOR 64.4 34.6 N/A N/A ArchRAG 67.2 31.5 62.8 65.0 GPT-4o-mini Vanilla RAG 71.4 32.8 68.2 70.1 HippoRAG 70.5 31.6 65.0 68.5 RAPTOR 70.1 32.6 N/A N/A ArchRAG 77.3 33.8 69.9 73.8 To further demonstrate the effectiveness of ArchRAG, we conduct the following experiments: • Effectiveness of LLM backbones. Given the limited budget, we restrict our evaluation to GPT-4o-mini and GPT- 3.5-turbo as the LLM backbones, and compare a representa- tive subset of strong RAG methods on the HotpotQA and Multihop-RAG datasets. As strong LLMs with hundreds of billions of parameters (e.g., GPT-3.5-turbo) possess en- hanced capabilities, our proposed ArchRAG may also ben- efit from performance improvement. As shown in Table 5, the results of Llama 3.1-8B are similar to those of GPT-3.5- turbo, as Llama3.1’s capabilities are comparable to those of GPT-3.5-turbo (Dubey et al. 2024). GPT-4o-mini performs better than other LLM backbones because of its exceptional reasoning capabilities. Besides, we have compared several strong RAG baselines under different LLM backbones. As LLMs’ parameters and reasoning capabilities increase, all RAG approaches benefit from performance gains, especially HippoRAG. ArchRAG consistently achieves state-of-the-art performance across most settings. • Community quality of different clustering meth- ods. We evaluate the community quality of our proposed LLM-based hierarchical clustering framework using four clustering algorithms (weighted Leiden, Spectral Cluster- ing (V on Luxburg 2007), SCAN (Xu et al. 2007), and node2vec (Grover and Leskovec 2016) with KMeans), com- bined with two graph augmentation techniques (the KNN algorithm and CODICIL (Ruan, Fuhry, and Parthasarathy 2013)). The resulting communities are assessed using the Calinski-Harabasz Index (CHI) (Cali ´nski and Harabasz 1974) and Cosine Similarity (Sim) (Charikar 2002), where higher values indicate better quality. Further details on the clustering implementation and evaluation metrics can be found in the appendix. Figures 8 and 9 show that combin- ing KNN or CODICIL with the weighted Leiden algorithm significantly enhances community detection quality. • Effectiveness of our attributed clustering algorithm. To further demonstrate the effectiveness of our attributed KNN CODICIL LeidenSpectralSCANNode2Vec024CHI (a) Multihop-RAG LeidenSpectralSCANNode2Vec024CHI (b) HotpotQA Figure 8: Community quality evaluated by CH Index. KNN CODICIL LeidenSpectralSCANNode2Vec0.40.60.81Sim (a) Multihop-RAG LeidenSpectralSCANNode2Vec0.6 0.8 1Sim (b) HotpotQA Figure 9: Community quality evaluated by Cosine Similar- ity. clustering algorithm, we evaluate the quality of communities (i.e., CHI and Cosine Similarity) generated by our attributed clustering algorithm compared to those produced by the Lei- den algorithm, which is used in GraphRAG for structural clustering. As shown in Table 7, our attribute-based cluster- ing consistently yields higher-quality communities. • More experiments on the additional dataset. We further conduct experiments on the RAG-QA Arena dataset (Han et al. 2024), a high-quality, multi-domain benchmark featuring human-annotated, coherent long-form answers. To the best of our capability, we use publicly avail- able data from five domains (including lifestyle, recreation, science, technology, and writing), selecting 200 questions per domain. Following prior work, we employ LLMs as evaluators to compare the RAG-generated responses with Table 6: Comparing ArchRAG with other RAG methods on the RAG-QA Area dataset. Each entry denotes the win ratio and win + tie ratio of the corresponding method against the ground-truth annotations, based on LLM evaluation. Method Lifestyle Recreation Science Technology Writing Vanilla RAG 17.5 / 20.5 17.0 / 25.0 32.5 / 37.0 28.5 / 34.0 15.0 / 16.5 HippoRAG 26.5 / 26.5 29.5 / 30.5 49.5 / 49.5 42.0 / 42.5 21.0 / 21.5 RAPTOR 17.0 / 19.5 18.5 / 24.5 39.0 / 45.0 33.0 / 35.5 25.0 / 26.5 ArchRAG 49.5 / 50.0 41.5 / 41.5 56.0 / 56.0 59.0 / 59.5 45.0 / 45.0 Table 7: Comparison of Community Quality between Our Attributed Clustering Method and Leiden Method Multihop-RAG HotpotQA (CHI) (Sim) (CHI) (Sim) Leiden 3.02 0.71 3.42 0.71 Ours 4.68 0.89 4.82 0.88 ground-truth answers in terms of win ratio and win + tie ratio. We evaluate the top-performing methods (including Vanilla RAG, HippoRAG, RAPTOR, and our ArchRAG) and present the results in terms of the win ratio and win + tie ratio against the ground-truth annotations in the table be- low. As shown in Table 6, ArchRAG consistently achieves state-of-the-art performance across all evaluated settings. Accuracy Recall 2 3 5 7 920406080Metrics (a) Multihop-RAG 2 3 5 7 920406080Metrics (b) HotpotQA Figure 10: Comparative analysis of the different numbers of retrieval elements in ArchRAG. • Effect of k values. We compare the performance of ArchRAG under different retrieved elements. As shown in Figure 10, the performance of ArchRAG shows little vari- ation when selecting different retrieval elements (i.e., com- munities and entities in each layer). This suggests that the adaptive filtering process can reliably extract the most rele- vant information from the retrieval elements and integrate it to generate the answer. • Case study. We present an additional case study from Multihop-RAG. As shown in Figure 11, only our method generates the correct answer, while others either provide in- correct or irrelevant information. We only show the core output for brevity, with the remaining marked as “¡/¿”. We also show the retrieval and adaptive filtering process of ArchRAG in Figure 12. The results demonstrate that ArchRAG effectively retrieves relevant information and fil- ters out noise, leading to a correct final answer. • Discussion of the performance of other graph-based RAG methods. On the Multihop-RAG dataset, HippoRAG Table 8: Distribution of HippoRAG’s ER Errors Datasets Null Entity Rate Low-Quality Entity Rate Multihop-RAG 1.3% 11.9% HotpotQA 5.0% 15.8% performs worse than retrieval-only methods while outper- forming retrieval-based methods on the HotpotQA dataset. This is mainly because, on the HotpotQA dataset, passages are segmented by the expert annotators; that is, passages can provide more concise information, whereas on Multihop- RAG, passages are segmented based on chunk size, which may cause the LLM to lose context and produce incorrect answers. Besides, HippoRAG also suffers from inaccurate entity recognition. In addition, we also provide a brief anal- ysis of why some graph-based RAG methods underperform. Specifically, they often fail to retrieve relevant information during the search process. For example: • GraphRAG: The suboptimal performance of GraphRAG has been widely observed in research. For instance, Zhang et al. (Zhang et al. 2024a) find that GraphRAG tends to respond with ”I don’t know” when retrieving ir- relevant content, indicating that it prefers not to give a concrete answer. PIKE-RAG (Wang et al. 2025) identi- fies that GraphRAG tends to echo the query and include meta-information about answers within its graph struc- ture. Through manual verification, we confirm the pres- ence of similar issues in our experimental results. In par- ticular, GraphRAG-Local Search also frequently returns irrelevant content. • HippoRAG/LightRAG: We attribute part of HippoRAG’s suboptimal performance to entity recognition issues. HippoRAG first extracts relevant entities from questions and then retrieves chunks based on these entities. How- ever, it sometimes fails to identify appropriate entities (Entity Recognition Errors, ER Errors), and we conduct corresponding experiments, as shown in Table 8. For ex- ample, on HotpotQA, HippoRAG fails to detect entities in 5.0% of questions and identifies low-quality ones in 15.8% (e.g., less than 3 characters, numeric-only terms, or sentence entities with more than 5 words). These is- sues make it difficult to match accurate entities in the KG, resulting in irrelevant retrieval and suboptimal per- formance. Similar issues also impair LightRAG’s perfor- mance because it requires accurate topic identification from questions. We also observe that as the LLM’s ca- pability improves, the performance of these methods in- creases significantly, as shown in the experiment Effec- tiveness of LLM backbone. Question & Ground-truth Question: Who is the individual associated with generative AI technology that was reportedly ousted from a leading AI organization, but is recognized for brilliance and generosity in the industry, and is currently planning to launch a new venture according to reports from TechCrunch and Fortune? Ground-truth: Sam Altman Responses of Different Methods Zero-shot Jesse Schell. He is the CTO of Calm and was the lead designer on the “Star Wars” theme park ride at Disney. Vanilla RAG It seems like there are multiple articles and topics discussed in this text. Here’s a brief summary of each section: 1.Robotics predictions. The article discusses the future of robotics, with predictions made by Brian Heater. · · · 2.TechCrunch’s Week in Review.This section summarizes various tech industry happenings. · · · 3.Bumble gets a new CEO. Bumble announces a change in leadership. · · · HippoRAG There is no question in the provided text. It appears to be a collection of news articles and updates related to OpenAI’s ChatGPT. · · · GraphRAG-Global Key Points and Implications The individual associated with generative AI technology who was reportedly ousted is Andrew Ng. · · · According to multiple analysts, Andrew Ng [Data: Reports (5, 6)] · · · Implications of the New Venture· · · ArchRAG Sam Altman. This information is supported by multiple analysts’ reports, which highlight Sam Altman’s involvement in various AI- related projects. The direct answer is simply “Sam Altman”. · · · Note: I have not included any information from Analyst 2, 3, or 7 as their reports are not relevant. · · · Figure 11: Case study of responses by different RAG methods on a question from the Multihop-RAG dataset. RED TEXT indicates incorrect content, CYAN TEXT denotes correct content, and · · · marks omitted irrelevant parts. Top Retrieved Entities: - Entity 1058: OPENAI Description: OpenAI is a tech company founded by Sam Altman ... - Community 4097: Microsoft, OpenAI, and AI Regulation Summary: Discussion between Satya Nadella and Sam Altman ... Adaptive Filtering Result: - Sam Altman is among the backers of an AI startup. - Score: 80.0 (Reports: 1) ... Figure 12: ArchRAG Retrieval & Filtering Output. ... marks omitted irrelevant parts. Prompt for generating abstract questions Prompt: Given the following description of a dataset: {description} Please identify 5 potential users who would engage with this dataset. For each user, list 5 tasks they would perform with this dataset. Then, for each (user, task) combination, generate 5 questions that require a high-level understanding of the entire dataset. Output the results in the following structure: - User 1: [user description] - Task 1: [task description] - Question 1: - Question 2: - Question 3: - Question 4: - Question 5: - Task 2: [task description] ... - Task 5: [task description] - User 2: [user description] ... - User 5: [user description] ... Note that there are 5 users and 5 tasks for each user, resulting in 25 tasks in total. Each task should have 5 questions, resulting in 125 questions in total. The Output should present the whole tasks and questions for each user. Output: Figure 13: The prompt for generating abstract questions. Prompt for LLM-based multi-dimensional comparison Prompt: You will evaluate two answers to the same question based on three criteria: Comprehensiveness, Diversity, Empowerment, and Directness. • Comprehensiveness: How much detail does the answer provide to cover all aspects and details of the ques- tion? • Diversity: How varied and rich is the answer in providing different perspectives and insights on the question? • Empowerment: How well does the answer help the reader understand and make informed judgments about the topic? • Directness: How specifically and clearly does the answer address the question? For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these four categories. Here is the question: Question: {query} Here are the two answers: Answer 1: {answer1} Answer 2: {answer2} Evaluate both answers using the four criteria listed above and provide detailed explanations for each criterion. Output your evaluation in the following JSON format: { "Comprehensiveness": { "Winner": "[Answer 1 or Answer 2]", "Explanation": "[Provide one sentence explanation here]" }, "Diversity": { "Winner": "[Answer 1 or Answer 2]", "Explanation": "[Provide one sentence explanation here]" }, "Empowerment": { "Winner": "[Answer 1 or Answer 2]", "Explanation": "[Provide one sentence explanation here]" }, "Overall Winner": { "Winner": "[Answer 1 or Answer 2]", "Explanation": "[Briefly summarize why this answer is the overall winner]" } } Output: Figure 14: The prompt for the evaluation of abstract QA. Prompt for Adaptive filtering-based generation Filter Prompt: # Role You are a helpful assistant responding to questions about data in the tables provided. # Goal Generate a response consisting of a list of key points that respond to the user’s question, summarizing all relevant information in the input data tables. You should use the data provided in the data tables below as the primary context for generating the response. If you don’t know the answer or if the input data tables do not contain sufficient information to provide an answer, just say so. Do not make anything up. Each key point in the response should have the following element: • Description: A comprehensive description of the point. • Importance Score: An integer score between 0-100 that indicates how important the point is in answering the user’s question. An ‘I don’t know’ type of response should have a score of 0. The response should be JSON formatted as follows: {"points": [ { "description": "Description of point 1", "score": score_value }, // ... more points]} # User Question {user_query} # Data tables {context_data} Output: Merge Prompt: # Role You are a helpful assistant responding to questions and may use the provided data as a reference. # Goal You should incorporate insights from all the reports from multiple analysts who focused on different parts of the dataset to support your answer. Please note that the provided information may contain inaccuracies or be unrelated. If the provided information does not address the question, please respond using what you know: • A response that utilizes the provided information, ensuring that all irrelevant details from the analysts’ reports are removed. • A response to the user’s query based on your existing knowledge when ¡Analyst Reports¿ is empty. The final response should merge the relevant information into a comprehensive answer that clearly explains all key points and implications, tailored to the appropriate response length and format. Note that the analysts’ reports provided below are ranked in the descending order of importance . Do not include information where the supporting evidence for it is not provided. # Target response length and format {response_format} # User Question {user_query} # Analyst Reports {report_data} Output: Figure 15: The prompt for adaptive filtering-based generation.