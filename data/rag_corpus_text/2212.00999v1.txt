Information Retrieval from the Digitized Books Riya Gupta IIIT Hyderabad India riyagupta771995@gmail.com C.V . Jawahar IIIT Hyderabad India jawahar@iiit.ac.in Abstract Extracting the relevant information out of a large num- ber of documents is a challenging and tedious task. The quality of results generated by the traditionally available full-text search engine and text-based image retrieval sys- tems is not optimal. Information retrieval (IR) tasks become more challenging with the nontraditional language scripts, as in the case of Indic scripts. The authors have devel- oped OCR (Optical Character Recognition) Search Engine to make an Information Retrieval & Extraction (IRE) system that replicates the current state-of-the-art methods using the IRE and Natural Language Processing (NLP) techniques. Here we have presented the study of the methods used for performing search and retrieval tasks. The details of this system, along with the statistics of the dataset (source: Na- tional Digital Library of India or NDLI), is also presented. Additionally, the ideas to further explore and add value to research in IRE are also discussed. Keywords Optical Character Recognition, Information Retrieval & Extraction, Metadata, Search Engine, Search Engine Opti- mization 1. Introduction Often the information retrieval systems designed to pro- vide and assist the general populace with texts in different languages that are either not optimized or lack advanced language features, like getting similar results to a query. Even for the big digital database like Google Books, the contextual and semantic information is not available for the desired query. Short queries might not explain the full in- tent behind the search. However, suppose the Information Retrieval and Extraction systems (IRE) can be made capa- ble in terms of obtaining semantically and contextually rich information from the ocean of data. In that case, it will im- prove the user experience. It will also help in the direction of extracting the required query. This work focuses on multimedia retrieval (image and text in Indic languages) in this IRE system, where the user enters a text query for which the system returns books that are indexed and ranked. Multilingual OCR [8, 11] is used to achieve this task. We know that optical Character Recog- nition (OCR) is the process of conversion of text or text- containing documents such as handwritten text, printed or scanned text images into a digital format that can be used for further processes. In this system, text recognition is per- formed on ancient Indic language documents. The ﬁnal in- formation retrieval system’s goal is to demonstrate the high accuracy of the Indic OCR and the retrieval of the ancient texts in Indic languages. The challenges faced by IRE systems in Indic language scenarios are discussed in Section 2. Section 3 consists of the details and functionalities of the OCR Search Engine designed by the authors. Section 4 represents the ideas that can be implemented on top of the existing system. 2. Practical Challenges associated with Indian IRE systems Modern state-of-the-art IRE systems suffer from vari- ous challenges for Indian data. These challenges can vary among different areas within information retrieval. These areas majorly include cross-lingual information retrieval (CLIR) [10], Web search, user modeling, ﬁltering, classiﬁ- cation, summarization, question answering, metasearch, se- mantic, and context-based search and extraction [1]. Based on the current trends and research work, these are some of the challenges that the community faces at the moment: 2.1. Limited Research Aid According to the latest Indian Census, India has 24 of- ﬁcially recorded languages. These include Hindi and En- glish. Although most of these languages have a similar grammatical structure and layout, the semantics and script differ drastically between them. Furthermore, a particular language varies widely in fonts, rendering schemes, dialects and other aspects depending on the region. 1 arXiv:2212.00999v1 [cs.IR] 2 Dec 2022 Moreover, the number of researchers contributing in the ﬁeld of information retrieval and text-based challenges for Indian languages (Hindi, Bangla, Sanskrit, Tamil, Telugu and Malayalam) are comparatively smaller due to the diver- sity in them, as well as due to the involvement of various modalities for instance images, text or audio. 2.2. Limited amount of available data One of the reasons for the lack of research in building quality IRE systems for Indian languages is the lack of data for information retrieval. Even when the data is publicly available, it is not readily usable and requires extensive cleaning and preprocessing to make it suitable for the re- trieval models. Current state-of-the-art methods like Word- Net [13], ConceptNet [18] are trained on large datasets for Roman and Latin scripts. Similar datasets are tedious to build due to the reasons stated in 2.1. Thus, the unavailabil- ity of quality data for this task hinders the progress in the ﬁeld. Recently, several groups from NDLI, IIT Kharagpur, have converted and digitalized a large number of historical documents like ”Bhagawadgitha”. These documents can act as a large dataset if annotated. However, even if it is present, the dataset is majorly noisy. Modern text recognition algo- rithms in this space are not designed to handle such noisy data and hence perform poorly. Figure 1: In this system, old and degraded document images are passed through an OCR, which hampers the quality of the generated text. The ﬁgure shows an example of a sample degraded document in Bangla language. Figure 1 shows a few examples of the data from the Bangla documents. The noise might include the warped lines within the documents, watermarked pages and traces of ink from the backside of the page. 2.3. Semantics and Context based Retrievals Web Search Engines and Information Retrieval are two aspects where web search is just a spectrum of Information retrieval. The information which is given as an input to the search bar does not always represent the practical and re- quired information that a user demands. Understanding the rich semantic and context behind a text query is extremely important for the search engines to return relevant results. For instance, in the search engines Yahoo and Bing, if a per- son enters a search query ’Nokia’, the results generated will be all the links containing Nokia, irrespective of their us- ability for the user. Whereas in Google search, the links are ranked in order of details about Nokia and currently avail- able cellphones of Nokia showing a higher semantic under- standing of the Google search engine than that of Yahoo and Bing. Similar features are needed to be implemented for the Indian language based IRE system. However, due to the complexity of Indian languages, newer techniques speciﬁcally designed for them are re- quired for generating rich semantic embedding. Hence, building algorithms that can extract rich information from Indic texts and later combine them with user search history and search patterns can be beneﬁcial. 3. What is OCR Search Engine? In recent years, there has been a lot of research in the ﬁeld of text recognition, and efforts have been put into achieve an OCR that can perform well on Indian scripts. In- dic scripts pose unique challenges for state-of-the-art OCR algorithms trained in English. For example, Sandhi rules and the difference in glyphs make it harder for the state-of- the-art OCR algorithms to work for such languages. makes it harder for the State-of-the-art OCR algorithms to work for such languages. Unavailability of IRE algorithms and dic- tionaries in Indic languages that can fetch similar words due to differences in the script and colossal amount of data to a given query hampers the search engine’s performance. The developed OCR Search Engine that demonstrates the accu- racy of the multilingual Indic OCR along with the retrieval accuracy. This OCR Search Engine is an IRE system that uses Op- tical Character Recognition designed for Indian scripts to generate the text from the document images in the Indic languages. Currently, it supports Hindi, Tamil, and Telugu languages. The system is built using Python and PSQL, forming the back-end and HTML/CSS being used as the front end. It uses the text generated by the Indic OCR developed at IIIT Hyderabad, line-level segmentation, and document im- ages to form the database of the system. The database is indexed using Elasticsearch, which is an open-source dis- tributed Java package. 2 Figure 2: Basic pipeline followed by the OCR Search En- gine 3.1. Functionalities The IRE system designed by us performs retrieval tasks on multiple levels : • Query Level Search The task is performed by entering the keyword/text in the search bar. There are multiple levels within the query level search like: – Single Query Search: Search based on one key- word search – Multiple Query Search: Search based on more than one keywords – Exact Query Search : Search based on retrieving exact entered query (by using double quotes) – Query Processing: Removal of stopwords and implementation of tokenization and stemming. • Relevance and Visualization – Relevance: Based on the number of occurrence of a particular keyword in the database and num- ber of hits for a particular book from users. – Visualization: Highlighted query outputs within the resulting books using segmentation method. – Pages with ﬁgures and varying font styles and sizes retrievable. • Language Level Search This level of search is completely based on the lan- guage chosen to retrieve the query. – Transliteration: A method of mapping from one system of writing to another based on phonetic similarity. – Single language search – Multilingual Search: Based on the search in more than one language and retrieval of multilingual books. • Search Engine Based – Primary Filters: Filtering based on the languages (Hindi, Tamil and Telugu) and the other ﬁlters including content based ﬁlter (default), ISBN Code, Author name and Book title. – Secondary Filters: Filtering based on Genre and Source provider of the book. 3.2. Dataset Table 1 shows the dataset and its details. Currently, the IRE system supports around 1.5 million documents. The detailed statistics are given in the following table. Language Hindi Tamil Telugu Books 2287 2030 2574 Pages 5,18,500 5,18,000 5,07,070 Table 1: Statistics of the dataset 3.3. Design The OCR Search Engine is structured on the following basis : 1. User End : Home, Search page, View, See Results About. 2. Server End: Login, Database Status, Manual Updates to Database. The following subsection represents the brief description of these ends. 3.3.1 User The user side is the client-side for retrieving the query entered by the user. The Home page is the launching page where the users can choose the ﬁlters for their search choices. This homepage presents the total number of docu- ments that this IRE system supports. Users can select the ﬁlters, whether it be language (Hindi, Tamil, Telugu) or search criteria (title, ISBN, author, default: content). These will fetch the results based on the content present within the book and use text output generated from the Indic OCR. The other ﬁlters present are termed ’secondary ﬁlters’ that include genre and the source if the book is extracted 3 Figure 3: Example of the metadata provided for one of the Hindi books Figure 4: Clicking ’See results about’ opens up a pop up with highlighted lines in a document where the query is present from the metadata provided. One of the examples of the metadata provided is given. The search page displays all the retrieved results. Every result consists of the cover page of the book, author, title, language, ISBN, short abstract (if available), an option to View the book and See results about , clicking on which a pop-up appears that has all the pages of a book with the searched query present in them. As the current OCR system uses line-level segmentation, the line in which the query is present is highlighted for making it easier for the users to search it. 3.3.2 Server End The Admin can Login inside the IRE system and assign various roles to the other admins, whether they can alter the entries in the database or just monitor them. Database Sta- tus can be viewed as a graph that gives the total number of pages and the total number of books added within the entire Figure 5: Clicking ”View” will open a new taskbar where users can view and read the entire book. database over a period. Featured datasets lists the datasets present within the system (currently by the National Digital Library Data). Manual Updates to the database are also possible. The Admin can edit/delete the books’ details, including Ab- stract, ISBN and Author. 3.4. System Implementation The backend of the IRE system is built over the top of Django/python, and the frontend uses HTML/CSS and Javascript. The pages are indexed using Elasticsearch. The dynamic components of these IRE systems include the Search results, ﬁlters, visualization where the query searched is highlighted, admin authentication, and manu- ally updating the database. The admin/supervisor has the right to completely mod- ify/delete/edit the dataset/database manually/automatically depending on his/her needs. This will not affect the already available dataset/database present in the system. Workﬂow: Users can view the retrieved results (retrieved books w.r.t. to the query searched), the highlighted queries, meta details, and the complete book(s). The download op- tions are currently unavailable. The users do not need to log in to use the system, and this makes the search and the generated results accessible for everyone to view and read. 4. Future Work These are the few proposed ideas that can be imple- mented over the top of the existing system that has been built and discussed here. 4.0.1 Abstract Generation Abstracts are necessary when a user cannot dedicate his time reading the complete book. It allows a user to un- derstand the summary of the book, thus enable a user to ﬁlter out books of his/her interest. This becomes more rele- vant in the case of Indic languages where there are religious 4 texts in different languages; for instance, it might be the case that the user would not want to read the entire drama book in Hindi but the gist of a particular page. State-of-the- art attention-based models [19, 20, 23] can be used to gen- erate a ﬂuent, concise, and understandable summary of the complete book. This can even be used to create summaries of chapters or pages within the book, which will make the user experience more smooth and increase the system’s us- age. Relevant keywords, frequency within the document, and readability will contribute to the abstract generation. This feature can further be enhanced by adding an audio option, i.e., Text-To-Speech (TTS), which will recite the ab- stract of the book to the user. 4.0.2 Semantic and Context Based IRE Usually, the searched queries are short or lack of intent be- hind the search. These queries might generate ambiguous results that the users do not expect. The current system de- signs fetch results using text processing techniques like tok- enization and lemmatization in the Indic query entered or by extracting the exact matching query. However, it does not generate similar results or semantically rich results. For ex- ample, suppose the query ’Ram aur unke putra’ is entered. In that case, the IRE system should be able to identify the semantics and relationship between the components ’Ram’ and ’Putra’ and generate relevant results containing details about ’Luv’ and ’Kush’ [4, 17, 24]. By using natural language processing technologies, word similarity techniques, ontology techniques, etc., to under- stand the nature of the words and queries, the search results can be improved. Controlled vocabularies, dictionaries, the- saurus, and taxonomies are some of the basic ontology tech- niques that are used during the annotation. In text-based re- trieval, these techniques improve the extraction of the infor- mation easily. The other method that can be used to achieve semantic Information retrieval is treating words as entities or concepts. In concept indexing, instances(entities) and ab- stract ideas(concepts) are identiﬁed within a text document and are linked to the ontological concepts. Methods like WordNet [13], OpenCyc [12] and ConceptNet [9] can be used to achieve some results in the case of Indic languages. Additionally, the search results in the current system can be improved by using Query Expansion(QE) [2] [3]. QE is performed by broadening the query by introducing additional keywords within the originally entered query. These usually include abbreviations, synonyms, antonyms, hyponyms, and hypernyms. Unsupervised or Supervised methods can be used to achieve this. Doc2Vec and Word2Vec can be used to capture the surface and seman- tic level similarity and perform Query Expansion. 4.0.3 System Based Multiple features can be added to the system to enhance it. Machine Translation(MT) [5, 7, 14, 15, 22] is one of the features that can be added. A user can type the query in one language and retrieve the results based on the meaning of that query in another language. For example, ”Mahabharat” searched in ”Hindi” should fetch ”Mahabharatha” results from the languages Tamil and Telugu too. Image and Audio based search methods can also be added where the entered query can either be audio or im- age. The audio search method is Speech to Text conver- sion, where a user can speak, and the query will automati- cally be translated and entered into the search bar. Image- based/Document Information Retrieval [16, 21] is another search method that can be implemented. In this work [6] by VGG group, the retrieval is performed over the object category on British Library data. Additionally, adding the most popular queries searched by the users over time and generation of related articles, magazines, as well as books concerning the book clicked by users are the few features that can be added over the top of the currently existing OCR Search Engine developed at CVIT, to make it more accessible and handy during usage. 4.1. Conclusion The end goal is to develop and deploy an enhanced Indic IRE system that can exploit the large corpora provided by the NDLI, and the output generated by the Indic OCR grew at CVIT. This system will improve the user search experi- ence for the Indic languages and increase the popularity of reading digitized books amongst the general populace. This system is currently being developed by adding more func- tionalities and newer languages like Bangla and Sanskrit. References [1] James Allan, J. Aslam, Nicholas Belkin, Chris Buckley, J. Callan, B. Croft, S. Dumais, Norbert Fuhr, Donna Harman, D.J. Harper, Djoerd Hiemstra, Thomas Hofmann, Wessel Kraaij, J. Lafferty, V . Lavrenko, David Lewis, Liz Liddy, R. Manmatha, and C.X. Zhai. Challenges in information re- trieval and language modeling: report of a workshop held at the center for intelligent information retrieval, university of massachusetts amherst. september 2002.ACM SIGIR F orum, 37, 01 2003. [2] Hiteshwar Kumar Azad and Akshay Deepak. Query expan- sion techniques for information retrieval: a survey. CoRR, abs/1708.00247, 2017. [3] Rafael E. Banchs, Monojit Choudhury, Paolo Rosso, Parth Gupta, and Kalika Bali. Query expansion for mixed-script information retrieval. pages 677–686. ACM - Association for Computing Machinery, July 2014. [4] Youssef Bassil and Paul Semaan. Semantic-sensitive web information retrieval model for HTML documents. CoRR, abs/1204.0186, 2012. 5 [5] Himanshu Choudhary, Aditya Kumar Pathak, Rajiv Ratan Saha, and Ponnurangam Kumaraguru. Neural machine trans- lation for English-Tamil. In Proceedings of the Third Con- ference on Machine Translation: Shared Task Papers , pages 770–775, Belgium, Brussels, Oct. 2018. Association for Computational Linguistics. [6] Elliot J. Crowley and Andrew Zisserman. In search of art. In Lourdes Agapito, Michael M. Bronstein, and Carsten Rother, editors, Computer Vision - ECCV 2014 Workshops , pages 54–70, Cham, 2015. Springer International Publishing. [7] Ayan Das, Pranay Yerra, Ken Kumar, and Sudeshna Sarkar. A study of attention-based neural machine translation model on Indian languages. In Proceedings of the 6th Workshop on South and Southeast Asian Natural Language Processing (WSSANLP2016), pages 163–172, Osaka, Japan, Dec. 2016. The COLING 2016 Organizing Committee. [8] Deepayan Das, Jerin Philip, Minesh Mathew, and C. V . Jawa- har. A cost efﬁcient approach to correct OCR errors in large document collections. CoRR, abs/1905.11739, 2019. [9] Ming-Hung Hsu, Ming-Feng Tsai, and Hsin-Hsi Chen. Query expansion with conceptnet and wordnet: An intrinsic comparison. In Proceedings of the Third Asia Conference on Information Retrieval Technology , AIRS’06, page 1–13, Berlin, Heidelberg, 2006. Springer-Verlag. [10] Jagadeesh Jagarlamudi and A. Kumaran. Cross-lingual in- formation retrieval system for indian languages. volume 5152, pages 80–87, 09 2007. [11] M. Mathew, A. K. Singh, and C. V . Jawahar. Multilingual ocr for indic scripts. In 2016 12th IAPR Workshop on Document Analysis Systems (DAS), pages 186–191, April 2016. [12] Cynthia Matuszek, John Cabral, Michael Witbrock, and John DeOliveira. An introduction to the syntax and content of cyc. pages 44–49, 01 2006. [13] Vuong M. Ngo, Tru H. Cao, and Tuan M. V . Le. Wordnet- based information retrieval using common hypernyms and combined features. CoRR, abs/1807.05574, 2018. [14] Jerin Philip, Vinay P. Namboodiri, and C. V . Jawahar. A baseline neural machine translation system for indian lan- guages. CoRR, abs/1907.12437, 2019. [15] Sukanta Sen, Kamal Kumar Gupta, Asif Ekbal, and Push- pak Bhattacharyya. Multilingual unsupervised NMT using shared encoder and language-speciﬁc decoders. In Proceed- ings of the 57th Annual Meeting of the Association for Com- putational Linguistics , pages 3083–3089, Florence, Italy, July 2019. Association for Computational Linguistics. [16] S. Senda, M. Minoh, and K. Ikeda. Document image re- trieval system using character candidates generated by char- acter recognition process. In Proceedings of 2nd Interna- tional Conference on Document Analysis and Recognition (ICDAR ’93), pages 541–546, Oct 1993. [17] Rossitza Setchi, Qiao Tang, and Ivan Stankov. Semantic- based information retrieval in support of concept design.Ad- vanced Engineering Informatics, 25(2):131–146, 2011. [18] Robyn Speer, Joshua Chin, and Catherine Havasi. Concept- net 5.5: An open multilingual graph of general knowledge. CoRR, abs/1612.03975, 2016. [19] Tomek Strzalkowski, Gees C Stein, G Bowden Wise, and Amit Bagga. Towards the next generation information re- trieval. In RIAO, pages 1196–1207, 2000. [20] Lu Wang and Wang Ling. Neural network-based ab- stract generation for opinions and arguments. CoRR, abs/1606.02785, 2016. [21] K. L. Wiggers, A. S. Britto, L. Heutte, A. L. Koerich, and L. E. S. Oliveira. Document image retrieval using deep fea- tures. In 2018 International Joint Conference on Neural Net- works (IJCNN), pages 1–8, July 2018. [22] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural machine translation system: Bridg- ing the gap between human and machine translation. CoRR, abs/1609.08144, 2016. [23] Pengcheng Yin and Graham Neubig. Tranx: A transition- based neural abstract syntax parser for semantic parsing and code generation. arXiv preprint arXiv:1810.02720, 2018. [24] Hamed Zamani, Susan Dumais, Nick Craswell, Paul Ben- nett, and Gord Lueck. Generating clarifying questions for information retrieval. In The Web Conference 2020 (formerly WWW conference), April 2020. 6