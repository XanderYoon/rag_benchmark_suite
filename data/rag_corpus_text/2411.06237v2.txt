Leveraging Retrieval-Augmented Generation for Persian University Knowledge Retrieval 1st Arshia Hemmat dept. Computer Engineering University of Isfahan Isfahan, Iran arshiahemmat@mehr.ui.ac.ir 1st Kianoosh Vadaei dept. Computer Engineering University of Isfahan Isfahan, Iran k.vadaei@mehr.ui.ac.ir 1st Mohammad Hassan Heydari dept. Computer Engineering University of Isfahan Isfahan, Iran mheydarii@mehr.ui.ac.ir 2nd Afsaneh Fatemi dept. Computer Engineering University of Isfahan Isfahan, Iran a fatemi@eng.ui.ac.ir Abstract— This paper introduces an innovative approach using Retrieval-Augmented Generation (RAG) pipelines with Large Language Models (LLMs) to enhance information retrieval and query response systems for university-related question answering. By systematically extracting data from the university’s official website, primarily in Persian, and employing advanced prompt engineering techniques, we generate accurate and contextually relevant responses to user queries. We developed a comprehensive university benchmark, Univer- sityQuestionBench (UQB), to rigorously evaluate our system’s performance. UQB focuses on Persian-language data, assessing accuracy and reliability through various metrics and real-world scenarios. Our experimental results demonstrate significant im- provements in the precision and relevance of generated responses, enhancing user experiences, and reducing the time required to obtain relevant answers. In summary, this paper presents a novel application of RAG pipelines and LLMs for Persian-language data retrieval, sup- ported by a meticulously prepared university benchmark, offer- ing valuable insights into advanced AI techniques for academic data retrieval and setting the stage for future research in this domain.1 Index Terms —LLMs, Local Datasets, Knowledge Retrieval, Academic Question Answering I. I NTRODUCTION Large Language Models (LLMs), including cutting-edge ones like OpenAI GPTs and Google Gemini models, often face significant challenges when it comes to extracting and utilizing local data, particularly from specialized datasets such as universities archives. These models are typically trained on broad, diverse datasets, which can result in a lack of specificity and accuracy when applied to niche domains. The challenges include the inability to access and process localized data ef- fectively, leading to issues like hallucinations and inaccuracies in generated content. Additionally, the models’ reliance on pre-existing knowledge limits their capability to incorporate newly acquired, domain-specific information without extensive retraining [1], [2]. We extend our gratitude to our fellow group members within the UIAI Com- munity at the University of Isfahan—Kiana Fakhrian, Amirhossein Moalemi, Amirhossein Ala, and Zahra Mortazavi—for their dedicated efforts in inte- grating our dataset and diligently scraping documents from the University of Isfahan website. Their contributions were instrumental to the progress of this work. 1Dataset is publicly available at https://huggingface.co/datasets/UIAIC/ UQB Retrieval-Augmented Generation (RAG) offers a robust solution to the challenges faced by LLMs in processing local documents. By integrating retrieval mechanisms with genera- tion capabilities, RAG pipelines enable models to access and utilize specific, relevant information from extensive datasets. Our proposed pipeline leverages a two-stage RAG approach combined with a Persian Large Language Model (PLM) and advanced prompt engineering techniques. Initially, queries are categorized to identify the most relevant documents, after which the appropriate LLM is engaged to generate accurate and contextually relevant responses. This method significantly enhances the precision and utility of LLMs in handling local- ized, domain-specific queries [3], [4]. We developed the ”UniversityQuestionBench” dataset, cre- ated from the most frequently asked questions by students across various disciplines. This dataset is designed to evaluate the performance of Persian LLMs integrated with RAG using the RAGAS evaluation metrics, which includes three key mea- sures: Faithfulness, Answer Relevance and Context Relevance. By employing these metrics, we ensure that the model provides accurate, relevant, and contextually appropriate responses. The dataset and evaluation processes aim to benchmark the effectiveness of our pipeline in addressing the specific needs of universities students [5], [6], [7]. Our contributions to this paper are as follows: • Development of a two-stage RAG pipeline integrated with Persian LLMs for handling localized queries. • Creation of the UniversityQuestionBench dataset, tailored to the most common queries from university students. • Leveraging the RAGAS evaluation metrics to rigorously assess the performance of our models. • Demonstration of significant improvements in Faithful- ness, Answer Relevance and Context Relevance of Re- sponses generated by our pipeline. II. R ELATED WORK A. Introduction to Retrieval-Augmented Generation Retrieval-Augmented Generation (RAG) is a novel paradigm that enhances the performance of large language models by incorporating information retrieval processes into the generation mechanism. This approach aims to arXiv:2411.06237v2 [cs.IR] 1 Dec 2024 Fig. 1. Our Proposed Pipeline improve the accuracy and robustness of generated content by utilizing relevant external data sources. Recent studies have demonstrated the effectiveness of RAG frameworks in various applications in AI and machine learning [1], [8]. B. Recent Advances and Techniques in RAG Recent advancements in RAG have focused on innovative techniques and methodologies to optimize retrieval and gener- ation processes. Lewis et al. (2020) highlight the power of RAG in knowledge-intensive NLP tasks, demonstrating its potential to solve complex information retrieval challenges [1]. Shahul et al. (2023) introduced RAGAS, a framework for automated evaluation of RAG pipelines, emphasizing the importance of reference-free evaluation metrics to enhance the evaluation process of RAG systems. Siriwardhana et al. (2022) developed RAG-end2end, which optimizes RAG for domain-specific knowledge bases, significantly improving performance in specialized domains such as healthcare and news [4]. Yu (2022) explored the use of retrieval-augmented generation across heterogeneous knowledge, addressing the challenges of retrieving information from diverse sources [6]. Nakhod (2023) proposed applying RAG to elevate low-code developer skills by integrating domain-specific knowledge into large language models, thereby improving their practical utility [9]. Melz (2023) introduced ARM-RAG, a system that enhances large language models’ intelligence through storing and retrieving reasoning chains, demonstrating significant im- provements in problem-solving tasks [10]. Chen et al. (2023) provided a comprehensive evaluation of the impact of RAG on large language models, highlighting the potential bottlenecks and challenges in applying RAG across different tasks [7]. Heydari et al. (2024) proposed the Context Awareness Gate (CAG) architecture, a novel mechanism that dynamically ad- justs the LLM’s input prompt based on whether the user query necessitates external context retrieval, thereby enhancing the efficiency and accuracy of RAG systems [11]. C. Applications and Case Studies The versatility of RAG is evident in its wide range of applications. For example, RAG has been successfully applied to AI-generated content, enhancing the quality and contex- tual relevance of the outputs. Another significant application is the integration of RAG across heterogeneous knowledge bases, which has proven effective in generating coherent and contextually appropriate responses [12]. Practical applications in high-performance computing for code development fur- ther demonstrate its versatility [13]. Specifically, Graph-based approaches like GNN-RAG and KG-RAG have significantly improved handling complex queries and enhancing factual consistency [14], [15]. D. Frameworks and Implementations Several frameworks and implementations have been pro- posed to facilitate the deployment of RAG systems. The University of Massachusetts introduced Stochastic RAG, an end-to-end framework that leverages stochastic methods for retrieval and generation, ensuring high relevance and diver- sity in the outputs [16]. Additionally, the Spring AI project demonstrates the practical application of RAG using Azure OpenAI, providing valuable insights into the integration of RAG with cloud-based AI services [17]. The Semantic Kernel framework by Microsoft offers another robust implementation for RAG [18]. KRAGEN, a knowledge graph-enhanced RAG framework, has also been developed for biomedical problem- solving, illustrating the application of RAG in specialized domains [19]. E. Leveraging RAG in Specific Domain Tasks RAG has shown significant potential in addressing specific domain tasks, such as enhancing university information sys- tems. By crawling university websites and creating datasets tailored to students’ queries, RAG can effectively answer questions related to different departments and services. This approach not only improves the accuracy of information retrieval but also enhances the overall user experience by providing precise and relevant answers to specific queries [13], [20]. Graph-based approaches have further enhanced RAG’s capabilities in handling domain-specific tasks by integrating structured knowledge representations [21], [22]. F . Challenges and Future Directions Despite the promising advancements, several challenges remain in the implementation of RAG. A survey by Semantic Scholar identifies key obstacles, including the complexity of integrating retrieval mechanisms with generation models and ensuring the scalability of such systems [23]. The tutorial by IJCAI further discusses recent advances and outlines future research directions to overcome these challenges, emphasizing the need for continued innovation in this field [24]. III. D ATASET GENERATION A. Data Production Process The data generation process for our UniversityQuestion- Bench dataset involves several key steps to ensure the col- lection of relevant and frequently asked questions by students. We initiated the process by identifying the most important data from the University of Isfahan’s website. 2 . This data extraction step is crucial for forming the foundational dataset. Step 1: We start by collecting data D from the university’s official website, focusing on the most critical and frequently accessed information by students. The first step is represented in the Equation 1: D = {datai}N i=1 (1) where datai represents individual pieces of information and N is the total number of data items extracted. Next, we surveyed students to gather insights on the most common questions they encounter. The survey results were recorded and analyzed to identify patterns and frequently asked questions. Step 2: In the second stage, we collected the questions from students through two primary approaches to ensure comprehensive coverage of frequently asked questions: 1) Student Surveys: We utilized data collected from 60 students to refine and enhance the quality of the doc- uments, ensuring they accurately reflected the most relevant and practical information based on real student experiences and inquiries. We asked students to fill out a form regarding the most frequent questions they en- counter at the university. The form included open-ended sections where students could describe the challenges they face when navigating university resources. From the responses, we identified a correspondence corpus of frequently asked questions that were representative of common student concerns from either the university website or the relevant documents in the different chan- nels. 2University of Isfahan website: https://ui.ac.ir 2) Web Scraping: In addition to the student surveys, we scraped the official Isfahan University website for relevant information. This included extracting questions derived from useful content such as department-specific pages, contact information (e.g., email addresses of pro- fessors), course descriptions, and administrative proce- dures. These questions were designed to complement the student-provided data and fill in any gaps in coverage. The collected data from these two sources forms the initial set of questions, denoted as Qstudent. This step is formulated in Equation 2: Qstudent = {questionj}M j=1 (2) where questionj represents each individual question and M is the total number of questions collected. where questionj represents each individual question and M is the total number of questions collected. To supplement the student-provided questions, we used GPT-4 to generate additional questions. We implemented a Python script to automate the generation of a comprehensive question set, ensuring a diverse and complete dataset. Step 3: Let Qgpt represent the set of questions generated by GPT-4. This is formulated in the Equation 3: Qgpt = {questionk}P k=1, (3) where questionk represents each individual question generated by GPT-4 and P is the total number of generated questions. Step 4: The combined set of questions, Qcombined, includes both student-provided and GPT-generated questions, this equa- tion can be shown in the Equation 4. Qcombined = Qstudent ∪ Qgpt, (4) To ensure the accuracy and relevance of the answers, we manually curated the responses with the help of human feed- back. This iterative process involved verifying and updating the answers based on student feedback. Step 5: The set of validated question-answer pairs, QAvalid, is established in the Equation 5. QAvalid = {(questionj, answerj)}T j=1, (5) where T corresponds to the Total number of GPT and Human Questions.Additionally, answerj corresponds to the manually validated answer for question j. The final dataset comprises over 500 documents, encom- passing a wide range of contexts to ensure comprehensive coverage. These documents include detailed information about the academic groups and sections of each department, pro- fessors along with their contact emails, and various aspects of the university, such as administrative procedures, facilities, and other relevant resources. This extensive collection ensures that the dataset accurately represents the diverse informational needs of students across the university. We illustrate the process of the Dataset Generation in the figure 2. Fig. 2. Data Generation Procedure - In this figure we has shown the question and answer generation. IV. P IPELINE GENERATION A. Pipeline Setting As previously discussed, large language models (LLMs) encounter challenges when responding to queries that they were not well-trained on, particularly due to the absence of specific training data related to those queries. Additionally, some queries directly pertain to local or private datasets. The retrieval-augmented generation (RAG) pipeline offers a solution to this issue without requiring extensive fine-tuning of the entire model on the specific dataset . Our pipeline is constructed based on the following steps: Step 1: Pass the crawled data, D, through the model, regardless of whether the data exists in the question or not. This is represented in the Equation 6: D = {datai}N i=1, (6) where datai represents individual pieces of information and N is the total number of data items extracted. Step 2: Identify the type of question and determine the relevant department using the DORNA Model, which is a fine-tuned version of Llama-3 on Presian data [25], [26]. Specifically, we employed the 8-bit quantized version of Dorna utilizing QLoRA [27]. This approach enables us to load our base model with significantly reduced memory requirements. Let Tdept(q) be the function that assigns a question q to a specific department. This can be represented in the Equation 7: Tdept(q) = DORNAclassify(q), (7) Step 3: Split paragraphs from the texts of each department. Let P denote the set of paragraphs split from D, The formula can be written in the Equation 8: P = {paraj}M j=1, (8) where para j represents each paragraph and M is the total number of paragraphs. Step 4: Use FAISS to find the similarity function. FAISS is a library for efficient similarity search and clustering of dense vectors, crucial for retrieving similar texts [28]. We utilize the Persian embedding model named persian-sentence-transformer-news- wiki-pairs-v3 for embedding the paragraphs. Let E(·) be the embedding function provided by the Persian sentence transformer. The embeddings for the paragraphs are in the Equation 9: E(P) = {E(paraj)}M j=1, (9) Step 5: For a given query q, its embedding is denoted as E(q). We then retrieve the first 3 closest documents based on the text similarity between the question and the retrieved contents. This similarity search is represented in the Equation 10: R(q) = TopK(FAISS(E(q), E(P)), 3), (10) where R(q) represents the set of top 3 retrieved paragraphs similar to the query q. Step 6: Create a prompt template and pass it to the LLMs. Our LLM, DORNA, is a fine-tuned version on Llama-3 of persian data. The prompt template T is designed to incorporate the retrieved paragraphs and the query. Step 7: Pass the generated prompt to the LLMs to produce the final answer. Let A denote the answer generated by DORNA in the Equation 11: A = DORNA(T (q, R(q))), (11) These steps constitute our pipeline, leveraging RAG and advanced embedding techniques to ensure accurate and rele- vant responses from localized data sources. We illustrate our Pipeline schema in figure 1. V. E XPERIMENTS To comprehensively assess the effectiveness of our RAG pipeline and LLMs, we utilize three key metrics as defined in the RAGAS paper: Faithfulness, Answer Relevance and Context Relevance. Each metric is described in detail below. A. Faithfulness Faithfulness evaluates how accurately the generated answer reflects the content of the retrieved documents. This metric is crucial to ensure that the model does not introduce hal- lucinations or incorrect information. Let F denote pipeline faithfulness, calculated in the equation 12: F = 1 N NX i=1 Faithfulness(Ai, R(qi)), (12) Where: • Ai is the answer generated for query qi. TABLE I EVALUATION METRICS FOR DIFFERENT MODELS AND EMBEDDINGS Model Embedding Faithfulness Answer Relevancy Context Relevancy GPT 4o OpenAI Embeddings 0.6333 0.6395 0.1154 GPT 3.5-turbo OpenAI Embeddings 0.8497 0.5604 0.1849 GPT 3.5-turbo Persin-Sentence-Embedding-V3 0.8113 0.493 0.223 GPT 4o Persin-Sentence-Embedding-V3 0.6578 0.6564 0.1848 Dorna (Persian version of Llama3) Dorna Embeddings 0.839 0.823 0.216 • R(qi) is the set of documents retrieved for query qi. • Faithfulness(Ai, R(qi)) measures the relevancy of Ai against the information in R(qi). The faithfulness function can be further defined in the equation 13: Faithfulness(Ai, R(qi)) = P|A(i)| j=1 Rel(Aij, R(qi)) |A(i)| (13) where Ai is the set of statements in the answer and Rel(Aij, R(qi)) determines that the statement Aij from an- swer Ai is supported by the document R(qi) or not. B. Answer Relevance Answer relevance measures how well the generated answer addresses the query. This metric ensures that the answer is directly relevant to the question asked. We first initialize a set of questions q that can directly address from the Answer Ai. Let Rans denote answer relevance, calculated in the equation 14: Rans = 1 m mX j=1 Sim(Q, qj) (14) Where: • m is the number of questions in the set of questions q • Q is the users query which model generated Ai based on it. • Ai is the answer generated for users query Q. • Sim(Q, qi) calculates the similarity of Q and qj embed- ding vectors. In our study, we used cosine similarity to calculate the output of this function. C. Context Relevance Context relevance assesses how well the retrieved doc- uments match the query, ensuring that the documents are contextually appropriate and useful for generating an accurate answer. Let Rctx denote context relevance, calculated in the equation 15: Rctx = 1 N NX i=1 Relevance(R(qi), qi) (15) Where: • R(qi) is the set of documents retrieved for query qi. • Relevance(R(qi), qi) evaluates how well the set of re- trieved documents R(qi) addresses the query qi. The relevance function for context is further defined in the equation 16: Relevance(R(qi), qi) = P|R(qi)| j=1 P(Rj(qi), qi) |R(qi)| (16) where P (Rj(qi), qi) is a measure which indicates the po- tential of Rj(qi) in the set of retrieved documents R(qi) for answering the query qi , VI. R ESULTS A. Pipeline Performance On UQB Our pipeline’s performance was evaluated on a dataset of 300 questions and answers using the test set of the UniversityQuestionBench (UQB) dataset. We used our base model, Dorna, to compute the evaluation metrics. As previ- ously discussed, the evaluation focused on three key metrics: faithfulness, answer relevance, and context relevance. The calculated results are demonstrated in Table I. • Faithfulness: – Faithfulness measures the factual accuracy of the responses, reflecting the system’s ability to generate outputs consistent with the underlying data source. – The highest faithfulness score (0.8497) is achieved by GPT-3.5-turbo with OpenAI Embeddings , un- derscoring the robustness of general-purpose embed- dings in generating accurate responses. – The performance of Dorna with Dorna Embed- dings (0.839) is competitive, highlighting the capa- bility of localized embeddings specifically designed for Persian-language content. – Models utilizing Persian-Sentence-Embedding-V3 (e.g., GPT-3.5-turbo, 0.8113 ) exhibit slightly re- duced faithfulness compared to OpenAI Embed- dings, suggesting limitations in the current iteration of these embeddings for ensuring strict fidelity to source information. • Answer Relevancy: – Answer relevancy assesses the alignment of the gen- erated response with the user’s query, a critical factor for user satisfaction. – Dorna with Dorna Embeddings outperforms other models with a score of 0.823 , demonstrating its strength in producing highly relevant responses tai- lored to Persian-language queries. This indicates the effectiveness of the Dorna model in embedding semantic understanding of user intent in Persian. – The lowest score for this metric is observed with GPT-3.5-turbo using Persian-Sentence- Embedding-V3 (0.493) , suggesting potential challenges in aligning query semantics with response generation when utilizing this embedding. • Context Relevancy: – Context relevancy measures how well the generated response incorporates broader contextual understand- ing, ensuring a coherent and comprehensive answer. – GPT-3.5-turbo with Persian-Sentence- Embedding-V3 achieves the highest context relevancy score ( 0.223), demonstrating its relative strength in capturing and incorporating contextual nuances, despite lower performance in other metrics. – Dorna with Dorna Embeddings also performs well (0.216), reflecting its capacity to balance context incorporation with other aspects of performance. Observations and Insights: • Trade-offs between Models and Embeddings: – GPT-3.5-turbo with OpenAI Embeddings delivers the best performance in faithfulness, reflecting the general applicability and robustness of these embed- dings across various datasets. – Conversely, Dorna with Dorna Embeddings ex- hibits superior performance in answer relevancy, em- phasizing the importance of leveraging embeddings specifically designed for Persian text in achieving domain-specific objectives. • Performance Variability of Persian-Specific Embed- dings: While Persian-Sentence-Embedding-V3 demon- strates strengths in context relevancy, its relatively lower performance in faithfulness and answer relevancy indi- cates the need for further optimization and training on diverse Persian datasets to improve its applicability for information retrieval tasks. B. Quality of Outputs To assess the acceptability of the pipeline outputs from a human perspective, we conducted a qualitative evaluation involving 10 reviewers, including members of the University of Isfahan Artificial Intelligence Community and university students. The evaluators rated the generated answers based on clarity, coherence, and overall satisfaction. Feedback indicated that the majority of the answers were clear, well-structured, and effectively addressed the questions, demonstrating high acceptability. Moreover, the consistency of the answers was noted, with evaluators observing a uniform level of quality across different questions. This consistency highlights the robustness of our pipeline, confirming its ability to produce reliable and high- quality answers suitable for practical use in a university setting. VII. C ONCLUSION In this study, we developed a question-answering pipeline based on Retrieval-Augmented Generation (RAG) using a quantized version of Dorna model, a fine-tuned version of LLaMA-3 on Persian data, and our custom dataset, Univer- sityQuestionBench (UQB). Our pipeline demonstrated strong performance across three key metrics: faithfulness, answer relevance, and context relevance. The quantitative results were complemented by a qualitative assessment, which confirmed the high acceptability and consistency of the generated an- swers from a human perspective. The findings underscore the efficacy of our RAG-based approach in addressing university-level questions, highlighting the potential of fine-tuned language models with context retrieval pipeline and custom datasets in enhancing educational tools. VIII. F UTURE DIRECTIONS Several future contributions are envisioned to further en- hance the proposed pipeline’s performance and scope, includ- ing improvements to the model and dataset. These contribu- tions aim to expand the dataset’s diversity, incorporate data from various universities, and establish real-time connectivity with course selection departments for more accurate and up- to-date information. Below are the key future contributions: A. Contribution 1: Expanding Dataset Diversity The first improvement involves expanding the dataset to include more diverse questions and answers, which can in- crease the robustness of the model across a wider array of academic subjects and contexts. This can be achieved by crowdsourcing data contributions from a broader range of stu- dents and institutions, ensuring the dataset contains both valid and new question-answer pairs. By increasing the diversity of the dataset, the model’s generalizability and effectiveness in handling various academic queries are expected to improve significantly. B. Contribution 2: Incorporating Multi-University Data To enhance the generalization and versatility of the dataset, it is proposed that question-answer data from multiple uni- versities be incorporated. Each university can have its unique structure, curriculum, and question patterns, which, when combined, create a more generalized dataset suitable for a wide range of academic scenarios. By pooling datasets from multiple institutions, the model will gain exposure to a wider variety of scholarly discourse, which can improve its perfor- mance across different contexts and subject areas. C. Contribution 3: Integrating Real-Time Updates Integrating the dataset with course selection departments for real-time updates is crucial to maintaining its relevance and accuracy. This connection ensures that new questions and answers reflecting the most current course offerings and content are continuously added to the dataset. This approach will ensure that the model stays up-to-date with the latest academic developments, enhancing its utility and accuracy for real-time student inquiries. By implementing these contributions, the pipeline will be- come more dynamic and capable of handling a wider range of questions in various academic contexts while ensuring up-to-date information is always available to users. These advancements are expected to lead to a more effective and adaptive question-answer system for students and educators alike. REFERENCES [1] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. K ¨uttler, M. Lewis, W. tau Yih, T. Rockt ¨aschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive nlp tasks,” 2021. [Online]. Available: https://arxiv.org/abs/2005.11401 [2] J. Zhao and J. Smith, “Utilisation of retrieval-augmented generation techniques for ai-generated content,” arXiv, 2024. [Online]. Available: https://arxiv.org/pdf/2404.19543 [3] NeurIPS, “Benchmarking large language models in retrieval-augmented generation,” 2020. [Online]. Available: https://arxiv.org/pdf/2404.19543 [4] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana, and S. Nanayakkara, “Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering,” 2022. [Online]. Available: https://arxiv.org/abs/2210.02627 [5] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Automated evaluation of retrieval augmented generation,” 2023. [Online]. Available: https://arxiv.org/abs/2309.15217 [6] W. Yu, “Retrieval-augmented generation across heterogeneous knowledge,” in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, D. Ippolito, L. H. Li, M. L. Pacheco, D. Chen, and N. Xue, Eds. Hybrid: Seattle, Washington + Online: Association for Computational Linguistics, Jul. 2022, pp. 52–58. [Online]. Available: https://aclanthology.org/2022.naacl-srw.7 [7] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language models in retrieval-augmented generation,” 2023. [Online]. Available: https://arxiv.org/abs/2309.01431 [8] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, M. Wang, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” 2024. [Online]. Available: https://arxiv.org/abs/2312.10997 [9] O. Nakhod, “Using retrieval-augmented generation to elevate low-code developer skills,” Artificial Intelligence, no. 3, pp. 126–130, 2023. [10] E. Melz, “Enhancing llm intelligence with arm-rag: Auxiliary rationale memory for retrieval augmented generation,” 2023. [Online]. Available: https://arxiv.org/abs/2311.04177 [11] M. H. Heydari, A. Hemmat, E. Naman, and A. Fatemi, “Context awareness gate for retrieval augmented generation,” arXiv preprint arXiv:2411.16133, 2024. [12] A. Anthology, “Retrieval-augmented generation across heterogeneous knowledge,” 2022. [Online]. Available: https://aclanthology.org/2022. naacl-srw.7 [13] H. Petty, G. Gupta, and S. Johnson, “Advanced ai and retrieval- augmented generation for code development in high-performance com- puting,” NVIDIA Developer Blog , 2024. [14] C. Mavromatis and G. Karypis, “Gnn-rag: Graph neural retrieval for large language model reasoning,” arXiv, 2024. [Online]. Available: https://arxiv.org/abs/2404.19543 [15] D. Sanmartin, “Kg-rag: Bridging the gap between knowledge and creativity,” arXiv, 2024. [Online]. Available: https://arxiv.org/abs/2405. 07437 [16] U. of Massachusetts, “Stochastic rag: End-to-end retrieval-augmented generation,” 2022. [Online]. Available: https://maroo.cs.umass.edu/pub/ web/getpdf.php?id=1496 [17] S. AI, “Spring ai retrieval augmented generation with azure openai,” 2022. [Online]. Available: https://github.com/rd-1-2022/ ai-azure-retrieval-augmented-generation [18] Microsoft, “Demystifying retrieval-augmented generation with .net,” 2023. [Online]. Available: https://devblogs.microsoft.com/dotnet/ demystifying-retrieval-augmented-generation-with-dotnet/ [19] N. Matsumoto, J. Moran, H. Choi, M. E. Hernandez, M. Venkatesan, P. Wang, and J. H. Moore, “Kragen: A knowledge graph-enhanced rag framework for biomedical problem solving using large language models,” Bioinformatics, vol. 40, no. 6, p. btae353, 2024. [Online]. Available: https://doi.org/10.1093/bioinformatics/btae353 [20] AtomCamp, “What is retrieval augmented generation (rag)? a 2024 guide,” 2024. [Online]. Available: https://www.atomcamp.com/ what-is-retrieval-augmented-generation-rag-a-2024-guide/ [21] D. Edge, H. Trinh, N. Cheng, J. Bradley, A. Chao, A. Mody, S. Truitt, and J. Larson, “From local to global: A graph rag approach to query-focused summarization,” arXiv, 2024. [Online]. Available: https://arxiv.org/abs/2405.07437 [22] J. Dong, B. Fatemi, B. Perozzi, L. F. Yang, and A. Tsitsulin, “Don’t forget to connect! improving rag with graph-based reranking,” arXiv, 2024. [Online]. Available: https://arxiv.org/abs/2404.19543 [23] S. Scholar, “A survey on retrieval-augmented text generation for large language models,” 2024. [Online]. Available: https://www.semanticscholar.org/paper/ A-Survey-on-Retrieval-Augmented-Text-Generation-for-Huang-Huang/ 94034fd2ed4b6cf41113abb7adc9ae469313c958 [24] IJCAI, “Recent advances in retrieval-augmented text genera- tion,” 2022. [Online]. Available: https://twitter.com/IJCAIconf/status/ 1538815418562801664 [25] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al. , “The llama 3 herd of models,” arXiv preprint arXiv:2407.21783 , 2024. [26] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, “Llama: Open and efficient foundation language models,” arXiv, 2023. [Online]. Available: https://arxiv.org/abs/2302.13971 [27] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient finetuning of quantized llms,” Advances in Neural Information Processing Systems, vol. 36, 2024. [28] J. Johnson, M. Douze, and H. J ´egou, “Faiss: A library for efficient similarity search and clustering of dense vectors,” arXiv, 2017. [Online]. Available: https://arxiv.org/abs/1702.08734 APPENDIX Examples of Dataset Questions and Answers: Below are sample questions from the dataset and their corresponding answers: Fig. 3. QA samples