Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation Aparajitha Allamraju, Maitreya Prafulla Chitale, Hiranmai Sri Adibhatla, Rahul Mishra, and Manish Shrivastava International Institute of Information Technology, Hyderabad, India Abstract.Document chunking is a crucial component of Retrieval- Augmented Generation (RAG), as it directly affects the retrieval of rel- evant and precise context. Conventional fixed-length and recursive split- ters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similar- ity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMeddatausingthreedifferentembeddingmodels.Wefurtherpresent an evaluation framework that measures the effect of chunking on both re- trieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (↑24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response- time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consis- tently deliver superior performance. Keywords:Semantic Chunking·RAG Evaluation·Question Answer- ing 1 Introduction Retrieval-Augmented Generation (RAG) [8] has become a key approach for en- hancing Large Language Models (LLMs) in high-stakes domains such as finance, law, and healthcare. By integrating external, domain-relevant context, RAG im- proves the factual grounding and reliability of model outputs. However, its ef- fectiveness depends heavily on the quality of the retrieval stage. A central factor in retrieval performance is chunking [2], which determines how documents are segmented for indexing. Effective chunking preserves seman- tic coherence, enabling the retrieval of contextually complete and relevant seg- ments and improving downstream tasks like question answering. Yet mainstream arXiv:2512.00367v1 [cs.IR] 29 Nov 2025 2 Authors Suppressed Due to Excessive Length RAG frameworks largely rely on fixed-length, sentence-based, or paragraph- based methods, which frequently yield incoherent or overly broad chunks that weaken retrieval precision. We propose two semantically aware, domain-trained chunking methods, Pro- jected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), which usedistinctboundary-detectionmechanismsandaretrainedonaugmentedbiomed- ical corpora. Our results show substantial improvements in both retrieval and generationcomparedtofixed-length,sentence-level,semantic,andrecursivebase- lines.Furthermore,thechunkersgeneralizewelltoout-of-domaindatasets,demon- strating robustness beyond the biomedical domain. All code, trained models, and augmentation scripts will be released upon acceptance. 2 Background Chunking is a critical preprocessing step in Retrieval-Augmented Generation (RAG),playingafundamentalroleinimprovingboththeefficiencyandrelevance of the retrieved context for downstream tasks. Existing approaches typically basedonfixed-sizesegmentsorrigidpredefinedrules,oftenfailtocaptureseman- tic boundaries effectively. While benchmarks exist for evaluating retrieval [15] and generation [5,14,17,18], along with established metrics for both tasks [1], most studies evaluate the retriever and generator modules independently. The influence of chunking on retrieval and generation performance, however, remains comparatively underexplored. Semantic chunking addresses boundary segmentation by producing coher- ent, contextually aligned text segments instead of arbitrary fixed-length chunks. Some existing library implementations [7] attempt to solve the limitations of uniform chunking by employing embedding-based strategies. In such methods, candidate chunks are identified by measuring the cosine similarity between adja- cent sentences (or merged segments) and splitting when the similarity falls below a threshold, indicating a topic shift. However, cosine similarity has known short- comings for downstream tasks such as information retrieval [23] and often lacks robust, domain-specific performance, as our experiments demonstrate. The cost of use of the library implementation of semantic chunking was elaborated in [11] and concluded that the existing semantic chunker may not always be worth the computation cost. Additionally, recent research [22] has explored predicting op- timal chunk granularity, but to our knowledge, no recent work has proposed new semantic chunking algorithms or systematically examined their joint impact on retrieval and generation. We propose an approach that better aligns with human judgment than ex- isting baselines, particularly because the gold data used for evaluation, both for our models and the baselines, is human-authored. Our methods not only deliver higher retrieval and generation quality but also achieve measurable computa- tional efficiency gains compared to current chunking strategies. Breaking It Down 3 3 Methodology In this section, we detail the process of training and employing our semantic chunkers using domain-specific documents. The hypothesis behind our methods is that two sentences similar in ideas would occur together while two sentences with not so close ideas would occur in different paragraphs and thereby be in separate chunks. Instead of just calculating distance between sentence embed- dings, we train our models to be able to amplify the distances and give us scores which we then use to chunk. We train the chunkers using positive and negative samples created from the PQA-U dataset as described in Section 4.1. 3.1 Semantic Chunking The two methods that we introduce are distinctive in how they define the se- mantic boundaries of a chunk, ultimately determining the separation points be- tween two chunks. In both the methods, we employ a 1:1 negative sampling of data as mentioned in Section 4.1, to ensure that the learned representations and boundaries are robust. We choose simple light-weight approaches and models to ensure that performance is optimized for downstream tasks and the chunking it- self doesn’t take much time or compute. The shared methodology between both of our chunking strategies is explained below. Given a sentence pairS i andS j, we encode them using state-of-the-art sen- tence transformers [12]:all-MiniLM-L6-v2(MiniLM) [20],all-mpnet-base- v2(mpnet) [13], ande5-large-v2(e5) [19] to have a total of three chunking models per strategy. The resulting embeddingsE i andE j undergo a linear transformation. The training enables the model to specifically tune the embedding space for boundary detection. InProjected Similarity Chunking(PSC), the dot-product similarity be- tween embeddings is obtained and the resultant final scalar similarity score is used to determine the boundary InMetric F usion Chunking(MFC), we employ a combined metric of eval- uating similarity between two given embeddings. We employ an equally weighted combination of Dot Product Similarity, Euclidean Distance, and Manhattan Dis- tance which is passed through a single neural layer to obtain the final similarity score. The scalar similarity scoreSi,j is normalized into a probability range(0,1) using sigmoid. This value represents the model’s final prediction. If the output probability is greater than or equal to 0.5 (determined emperically), the individ- ual model predicts that the two sentences occur in the same section. Otherwise, if the probability is below 0.5, it predicts that the sentences should not occur one consecutively and we use that to draw our segment boundary or create the chunks. 4 Authors Suppressed Due to Excessive Length 4 Experiments This section presents experiments conducted using the proposed model on our augmented PubMedQA dataset and the RAGBench [3] datasets. It is organized into five subsections: dataset, evaluation metrics, baselines, experimental setup, and evaluation setup. 4.1 Dataset Currently, no publicly available datasets exist for jointly evaluating retrieval and generation performance. To address this gap, we repurposed the PubMedQA (PQA) dataset [6] by augmenting it with corresponding full-text articles from PubMed Central (PMC). The full texts were downloaded in XML format fol- lowing the official instructions1 . PubMedQA is a carefully curated biomedical question-answering dataset de- rived from PubMed articles whose titles are phrased as questions. The dataset is divided into labeled (PQA-L), unlabeled (PQA-U), and artificial (PQA-A) subsets. PQA-L and PQA-U are human-annotated, whereas PQA-A is auto- matically labeled. In the original dataset, the title serves as the question, the context consists of selected sentences from the abstract that support the conclu- sion, and the conclusion itself serves as the long-form answer. Upon downloading the corresponding full-text XML files, we observed that not all PQA entries have available articles on PMC. Specifically, from the original 1k samples in PQA-L, only 115 have full-text matches; PQA-U has 6.45k matches instead of 61.2k; and PQA-A has 51.2k matches from an original 211.3k. The XML files were cleaned to remove figures, tables, captions, references, appendices, and other supplementary materials, retaining only the abstract and main article text. For our experiments, we kept the original question from the dataset and used the cleaned full text. The original “context" sentences were used to evaluate retriever performance, while the generated answers were compared to the original long answers from the conclusions. We used PQA-A’s full-text data to train our semantic chunking models and reserved PQA-L for evaluation. Training data was prepared by splitting PQA-A articles into pairs. To help the model distinguish between sequentially related and unrelated sentences, we applied negative sampling. For each positive pair (sentences occurring consecutively), one negative pair was added, yielding a bal- anced set of 50.3% positive and 49.7% negative samples. Positive pairs were defined as sentences grouped by the human author within the same section of the text. Negative pairs were created by randomly selecting sentences from differ- ent sections within the same document that never co-occur in any section. This design ensures that the model learns to identify contextually related sentences based on human-defined structure. The final dataset contained 93M sentence pairs for training our models. 1 https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/ Breaking It Down 5 Theoriginaldatasetcolumnsincluded:pubid,question,context,long_answer, andfinal_decision. In our augmented version, the schema is updated to include full_text. We do not usefinal_decision, focusing exclusively on long-form an- swering. By introducing full-text articles alongside the original question answer pairs, our augmented PQA dataset enables a unified framework for evaluating both retrieval quality and answer generation in a controlled setting. The curated con- texts allow us to measure retriever precision, while the long-form conclusions serve as a robust reference for generation metrics, making it uniquely suited for end-to-end RAG evaluation. 4.2 Evaluation metrics Weevaluatethesystemattwolevels:retrievalqualityandanswergeneration.For the retriever, we employ Hits@k and Mean Reciprocal Rate (MRR) to measure theeffectivenessandrankingefficiencyoftheretrievedchunks.Forthegenerator, we evaluate the outputs using standard generation metrics such as BLEU [10], ROUGE [9], and BERTScore [21]. Additionally, we measure query response time and time-to-first-token to assess the responsiveness of our chunker models. 4.3 Baselines In our experiments, we compare several popular chunking strategies against our proposedmethodstoestablishabaselineforperformance.Allchunkingstrategies areimplementedusingLangChain 2 library.Thesentencesandchunksderivedare then encoded using one of three sentence transformer [12] models: all-MiniLM- L6-v2 [20], all-mpnet-base-v2 [13], and e5-large-v2 [19]. The performance of each chunker is evaluated based on its impact on retrieval and generation tasks. Character Chunking(Fixed-Length)(Char)splitstextintofixed-sizeblocks, often ignoring natural linguistic boundaries and reducing coherence [2,4,8]. We adopt a 1000-token chunk size with 200-token overlap, chosen because two con- secutive chunks typically capture a complete semantic unit while avoiding unnec- essary redundancy.Sentence Chunking(Sent) groups a fixed number of full sentences, offering better linguistic consistency than character-level splitting. We use three sentences with a one-sentence overlap, roughly matching the 1000 and 200-token configuration, though this produces variable chunk lengths that com- plicate uniform processing.Recursive Chunking(Rec) segments documents hierarchically (e.g., paragraphs to sentences). While more structured, it requires additional computation to identify segmentation levels. We use the same 1000- token and 200-token setups for comparability.Semantic Chunking(Sem) uses sentence embeddings to detect semantic shifts and form meaningfully aligned chunks. Although conceptually strong, LangChain’s default SemanticChunker under-performs in our evaluations relative to the proposed methods. 2 https://www.langchain.com 6 Authors Suppressed Due to Excessive Length 4.4 Experimental setup We trained our two chunker architectures, PSC and MFC, on three selected sen- tence embedding models, MiniLM, mpnet, and E5, using the augmented PQA-A dataset. The augmented PQA-A dataset contains approximately 93M training samples. All model training was performed on a single NVIDIA RTX 6000 GPU with 48 GB of VRAM. Each embedding–chunker combination was trained for 5 epochs, and empirical evaluation indicated that models trained up to epoch 4 achieved the best overall performance. We used the BCEWithLogitsLoss ob- jective function, which combines a sigmoid activation with binary cross-entropy loss. Training time for each model combination was under 12 hours, resulting in a total cumulative GPU usage of roughly 72 hours across all experiments. For generation and inference, we used a pair of NVIDIA RTX 4090 GPUs (24 GB VRAM each). The PQA-L dataset, comprising 115 documents, was used for in-domain evaluation, requiring about 1 hour per model. Out-of-domain evalua- tion on RagBench averaged 14 hours per model. This setup enabled consistent, large-scale experimentation while maintaining practical computational require- ments. 4.5 Evaluation Setup We evaluate the impact of chunking strategies on both retrieval quality and answer generation. Using the augmented PQA-L dataset, gold-standard chunks and answers enable direct comparison against retrieved contexts and generated outputs. Alongside our PSC and MFC chunkers, we benchmark four baselines: Character, Sentence, Semantic, and Recursive Chunkers. For out-of-domain testing, we use 12 RAGBench [3] datasets to measure generation quality across chunkers. Since RAGBench lacks gold context labels, we evaluate only generated answers, which though LLM-produced, are reliable for benchmarking. This setup allows us to test how well domain-trained chunkers generalize to varied document types. Retriever evaluation is conducted solely on the PubMedQA extension using MRR and Hits@k. PSC and MFC produce naturally variable chunk lengths (averaging 471 tokens), which we find improves retrieval and downstream answer generation compared to fixed-size segmentation. For generation, we retrieve the top-5 chunks and pass them, along with the query, to a Llama-3.1-8B [16] instruction-tuned model with fixed decoding pa- rameters across all datasets. Outputs are evaluated against gold answers using ROUGE [9], BLEU [10], and BERTScore [21]. Keeping the retriever and gener- ator constant ensures all performance differences arise solely from the chunking strategy. Each dataset, chunker, and encoder configuration follows a unified workflow: thedocumentissegmented,itsembeddingsareindexed,themostrelevantchunks are retrieved for a given query, and a generator model produces the final answer. Breaking It Down 7 Model Chunker Hits@3 Hits@5 MRR Query Time TTFT MiniLM Char0.0 0.0 0.0 0.01 5.91 Sent0.0 0.0 0.0 0.01 5.91 Sem0.00 * 0.00* 0.01 0.01 0.56 Rec0.0 0.0 0.0 0.01 0.19 PSC0.10 0.13 0.30 0.00 0.12 MFC0.09 0.12 0.26 0.01 0.13 mpnet Char0.0 0.0 0.0 0.02 5.86 Sent0.0 0.0 0.0 0.01 0.34 Sem0.00 * 0.00* 0.01 0.01 0.62 Rec0.0 0.0 0.0 0.01 0.19 PSC0.11 0.15 0.31 0.010.13 MFC0.10 0.14 0.29 0.01 0.15 e5 Char0.0 0.0 0.0 0.02 5.58 Sent0.0 0.0 0.0 0.01 0.34 Sem0.00 * 0.00* 0.01 0.01 0.63 Rec0.0 0.0 0.0 0.01 0.19 PSC0.13 0.16 0.36 0.010.14 MFC0.12 0.15 0.34 0.01 0.20 Table 1: Retrieval performance measured with Hits@3, Hits@5 and MRR across the three embedding models, show significant improvement, on using our chunkers.* indi- cates infinitesimal non-zero values. The p-test values between PSC and Sem across the three embeddings and metrics were in the order ofe−11 and t-test values greater than 7. Query Time and Time to First Token (TTFT) indicate that PSC is the fastest. This integrated design removes redundant pipelines, enforces consistent evalua- tion across setups, and offers a scalable framework for systematically comparing RAG components. 5 Analysis Our trained chunkers significantly outperform commonly used methods, MRR improves from 0.0086 (recursive) to 0.2914 (PSC in e5) and Hits@5 from 0.0028 to 0.1234 (PSC in e5), as shown in Table 1. The library implementation of semantic chunker under performs in retrieval (MRR: 0.0130 vs. 0.2914 for PSC) due to its reliance on cosine similarity for chunk boundary detection, which often results in suboptimal segmentation. These results confirm that chunk quality strongly influences retrieval effectiveness. Generation results on PubMedQA (Table 2) show PSC and MFC performing competitivelyandoftensurpassingbaselines.MFCwiththeE5embeddingmodel achieves the strongest BLEU, ROUGE, and BERTScore, while PSC and MFC with mpnet also achieve high ROUGE scores. This indicates that our chunkers yield contextually aligned outputs. 8 Authors Suppressed Due to Excessive Length Embedding Chunker BLEU ROUGE1 ROUGE2 ROUGEL BER TScore MiniLM Char28.10 30.63 13.16 22.26 87.43 Sent21.84 22.64 8.37 15.97 84.72 Sem24.73 27.19 10.67 19.75 86.57 Rec22.68 24.33 9.08 17.53 85.73 PSC24.20 27.08 10.00 19.45 86.92 MFC22.55 25.56 9.08 18.07 86.48 mpnet Char23.31 25.64 9.81 18.52 86.80 Sent23.19 25.08 9.50 17.39 86.45 Sem22.01 25.3010.5018.44 86.83 Rec20.22 23.26 8.73 16.95 84.17 PSC23.1426.9410.21 19.68 85.91 MFC23.6026.61 9.8319.71 87.38 e5 Char24.06 26.32 10.11 18.77 86.53 Sent19.42 21.66 8.32 15.14 85.92 Sem22.19 23.88 8.70 17.19 85.93 Rec22.10 24.78 10.09 18.22 86.56 PSC23.41 26.52 9.59 18.77 87.42 MFC25.34 27.68 10.31 19.94 87.46 Table 2: Evaluation metrics (BLEU, ROUGE-1, ROUGE-2, ROUGE-L, and BERTScore) for different embedding models and chunking strategies. Results show Character chunking with miniLM, MFC with mpnet and E5 perform the best. The p- test values between PSC and Char across the three embeddings and five metrics were less than0.3and t-test values around than1. Evaluation reveals a mismatch between conventional metrics and actual gen- erationquality.StandardmetricslikeBLEU,ROUGE,andBERTScoreprioritize lexical overlap over semantic nuance and factual accuracy. Consequently, they fail to capture the superiority of PSC and MFC chunkers, which produce con- cise, context-focused responses free from the verbosity and drift found in other methods. A two-tailed independent samples t-test was conducted to evaluate the sta- tistical significance of the performance metrics between all the chunkers. The difference between the reported means of MRR, Hits@3, Hits@5, BLEU and ROUGE was found to be statistically significant, as indicated by a p-value and t-value in the captions of Tables 1 and 2. This suggests that the improvement of the PSC Chunker over the next best chunker is not due to random chance with a confidence of 99.9% for retrieval, and 90% for generation. For out-of-domain tests, the generation results with BLEU and ROUGE- L scores on every dataset from RAGBench are summarized in Tables 3.1, 3.2 and 3.3. Despite our chunkers being trained only on PubMed dataset, we can see Breaking It Down 9 a clear improvement in the performance across all datasets with our chunkers, especially PSC. Embed Chunker CovidQA CUAD DelucionQA EManual ExpertQA FinQA HAGRID HotpotQA MS Marco PubMedQA T A T-QA T echQA BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L BLEU R-L MiniLM Char 1.95 3.16 9.86 9.82 2.91 4.9 3.98 6.22 16.34 12.49 2.13 3.44 2.63 4.17 1.03 1.94 1.63 2.53 8.49 10.21 3.07 5.01 13.48 12.05Sent 2.51 4.22 5.83 6.61 5.73 9.28 5.52 8.65 9.29 8.73 4.11 6.52 2.97 4.95 1.26 2.48 3.16 5.15 9.61 11.95 3.51 6.04 9.51 10.59Sem 3.48 5.44 5.64 5.82 7.35 10.93 8.32 11.13 10.32 8.59 4.64 6.72 4.59 6.95 2.07 3.88 4.33 6.34 14.36 15.90 4.83 7.12 5.67 5.91Rec 4.62 7.04 10.73 10.61 9.43 13.36 9.78 12.75 17.17 13.01 6.03 8.79 4.57 6.81 2.03 3.85 5.01 7.33 14.77 16.43 4.69 7.20 14.38 12.64 PSC6.31 8.83 11.34 10.75 15.69 17.25 13.85 14.4420.83 14.329.76 12.30 7.22 9.714.00 7.278.32 10.71 24.33 21.33 6.40 8.82 18.01 13.91MFC 6.16 8.63 9.90 9.78 15.67 17.12 13.40 13.6920.93 14.339.15 11.64 7.06 9.644.06 7.408.1710.7123.72 21.18 5.83 8.21 17.19 13.34 mpnet Char 1.92 3.16 9.71 9.73 2.84 4.82 3.97 6.26 16.11 12.43 2.10 3.39 2.60 4.15 1.03 1.96 1.63 2.53 8.40 10.13 2.90 4.75 13.18 12.02Sent 2.48 4.20 5.90 6.69 5.64 9.24 5.45 8.71 9.20 8.63 4.19 6.49 2.97 4.96 1.23 2.43 3.14 5.13 9.55 11.89 3.53 6.07 9.50 10.72Sem 3.47 5.38 5.7 6.11 6.91 10.57 8.18 11.46 10.49 8.66 4.19 6.08 4.55 7.00 2.00 3.74 4.24 6.29 14.09 15.66 4.39 6.62 5.59 5.95Rec 4.58 6.97 10.72 10.35 9.4 13.49 9.21 12.42 17.07 13.05 5.898.554.62 6.93 2.02 3.82 5.01 7.37 14.68 16.384.48 6.8814.07 12.56 PSC6.76 9.39 13.14 11.65 12.69 15.7818.1017.88 19.28 13.69 6.168.507.02 9.68 3.696.648.33 10.91 24.47 21.624.19 6.3615.78 12.82MFC 6.66 9.23 12.9 11.5 12.67 15.5318.6117.84 18.47 13.30 5.48 7.67 6.65 9.34 3.656.727.99 10.53 23.91 21.49 3.89 5.96 15.39 12.61 E5 Char 1.80 3.01 9.44 9.32 2.79 4.71 3.86 6.04 15.96 12.06 2.07 3.46 2.14 3.40 1.04 1.97 1.58 2.44 8.43 10.18 3.25 5.36 12.52 12.04Sent 2.46 4.23 6.18 7.02 5.73 9.3 5.60 8.56 9.27 8.63 4.22 6.80 2.81 4.67 1.23 2.44 3.15 5.16 9.69 12.05 3.52 6.12 9.65 10.99Sem 3.30 5.33 6.29 6.05 7.67 11.15 8.50 11.69 10.23 8.56 4.55 6.73 4.24 6.45 1.74 3.28 5.27 7.59 14.46 16.05 4.80 7.24 4.92 5.54Rec 4.65 7.34 11.04 10.61 9.59 13.58 9.40 12.58 17.22 12.81 6.00 8.97 4.51 6.74 1.92 3.63 5.01 7.33 14.95 16.67 4.56 7.0913.87 12.82 PSC6.31 9.24 13.96 12.23 6.52 9.27 6.14 8.7518.16 13.179.25 11.99 6.65 9.273.21 6.057.42 10.12 25.00 22.20 6.81 9.4013.39 11.77MFC 6.20 9.09 9.99 9.32 5.42 8.22 6.06 8.6819.17 13.456.97 9.56 6.14 8.713.73 6.906.92 9.49 24.49 22.10 5.25 7.77 10.65 9.83 Table 3.1: Evaluation metrics BLEU and ROUGE-L (R-L) for different embedding models and chunking strategies on 12 datasets from RAGBench. The results in bold show that our chunkers perform better across the board despite being trained only one domain data. Embed Chunker CovidQA CUAD DelucionQA EManual ExpertQA FinQA R1 R2 BScore R1 R2 BScore R1 R2 BScore R1 R2 BScore R1 R2 BScore R1 R2 BScore MiniLM Char 3.78 2.6883.3015.14 5.42 79.23 5.67 4.2684.817.72 5.2484.1323.49 9.8082.794.55 2.40 79.78Sent 4.95 3.55 82.67 9.88 3.6980.4410.97 7.93 83.98 10.89 7.12 83.03 15.30 6.61 82.16 8.62 4.7080.13Sem 6.60 4.55 83.19 8.89 3.25 79.71 13.20 9.36 84.75 14.95 9.07 83.51 16.00 6.95 82.66 9.13 4.85 79.71Rec 8.61 5.95 83.10 16.475.9379.22 16.65 11.56 83.86 17.32 10.59 83.29 24.42 10.11 82.75 11.77 6.33 79.47 PSC11.03 6.8782.4616.875.89 79.8823.16 13.6882.9721.11 10.7982.6127.4210.56 82.6017.10 9.1079.88MFC 10.79 6.72 82.55 15.12 5.31 79.81 23.05 13.47 82.93 20.16 9.94 82.72 27.3910.6782.51 16.15 8.53 79.85 mpnet Char 3.75 2.6883.1915.10 5.38 78.87 5.55 4.1485.087.70 5.2784.3323.20 9.59 82.78 4.46 2.3779.74Sent 4.94 3.54 82.49 10.01 3.5880.1710.87 7.92 84.13 10.82 7.19 83.18 15.20 6.54 82.03 8.70 4.59 79.72Sem 6.56 4.47 83.00 9.15 3.44 79.75 12.62 9.17 84.80 14.96 9.57 83.71 16.21 7.02 82.71 8.29 4.35 79.57Rec 8.50 5.74 82.97 16.27 5.78 78.99 16.66 11.57 83.88 16.52 10.18 83.31 24.27 9.98 82.78 11.57 6.16 79.29 PSC11.75 7.4082.5518.65 6.4379.7920.49 13.0183.72 26.46 12.72 81.7725.98 10.52 82.81 11.71 6.1279.54MFC 11.57 7.20 82.44 18.49 6.27 79.70 20.07 12.61 83.6526.75 12.9181.91 25.23 10.3282.8110.53 5.48 79.54 E5 Char 3.56 2.6183.6414.56 5.16 79.19 5.43 4.0784.977.46 5.14 84.17 22.74 9.40 82.64 4.47 2.47 79.93Sent 4.94 3.62 82.88 10.62 3.8480.0510.98 8.00 84.03 11.08 7.06 82.79 15.15 6.57 81.90 8.91 4.9580.07Sem 6.37 4.53 83.58 9.39 3.39 79.55 13.60 9.51 84.67 15.41 9.58 83.46 15.80 6.93 82.57 9.02 4.91 79.81Rec 8.78 6.23 83.48 16.70 5.84 79.2516.88 11.6983.8716.91 10.3883.44 24.00 9.7682.6711.92 6.60 79.57 PSC11.29 7.4382.9919.61 6.5779.47 11.27 7.89 84.62 11.27 7.3084.1824.87 10.07 82.6116.42 8.9479.99MFC 11.10 7.34 82.99 14.66 5.06 79.49 9.82 7.11 84.76 11.22 7.10 83.7925.58 10.2982.66 12.94 7.03 79.86 Table 3.2: Evaluation metrics ROUGE1 (R1), ROUGE2 (R2) and BERTScore (BScore) on 6 datasets from RAGBench. We also evaluate the response time of the various chunkers by calculating the Query Response Time for retrieval and Time to First Token(TTFT) for gener- ation. Since the models compute the similarity between sentences for chunking, there is a slight increase of 0.7s against Character chunker and 0.4s against Se- mantic chunker in indexing time on an average. As indexing is an offline one-time operation, we do not believe that the increase is of importance in this study. The 10 Authors Suppressed Due to Excessive Length Embed Chunker HAGRID HotpotQA MS Marco PubMedQA T A T-QA T echQA R1 R2 BScore R1 R2 BScore R1 R2 BScore R1 R2 BScore R1 R2 BScore R1 R2 BScore MiniLM Char 5.09 3.5883.382.12 1.67 82.53 3.18 2.31 84.00 14.81 8.9185.876.21 3.3080.1220.95 9.31 81.18Sent 5.99 4.23 82.00 2.69 2.12 81.30 6.24 4.44 82.97 17.01 10.27 84.69 7.55 4.07 79.49 17.70 8.29 81.05Sem 8.63 6.00 82.96 4.28 3.3082.558.21 5.8284.0423.17 13.83 85.50 9.13 4.78 80.00 9.97 4.5781.23Rec 8.50 5.83 82.68 4.19 3.29 82.44 9.44 6.61 83.70 23.79 14.21 85.02 9.01 4.87 80.03 22.09 9.71 81.13 PSC12.53 8.0182.27 8.15 6.03 81.7214.688.98 83.3433.38 18.0284.6311.39 5.9379.9025.16 10.3481.19MFC 12.37 7.92 82.258.26 6.0981.70 14.469.0083.37 32.92 17.90 84.71 10.51 5.46 79.90 24.02 9.98 81.14 mpnet Char 5.06 3.5783.422.13 1.68 82.61 3.17 2.30 84.01 14.68 8.8485.865.87 3.1380.0320.82 9.32 81.03Sent 6.01 4.23 82.06 2.62 2.07 81.30 6.23 4.44 82.98 16.90 10.20 84.71 7.68 4.04 79.05 17.69 8.34 80.92Sem 8.62 6.01 83.04 4.13 3.2282.628.06 5.7484.0322.83 13.66 85.51 8.35 4.38 79.92 9.89 4.6681.19Rec 8.61 5.89 82.75 4.18 3.26 82.41 9.44 6.61 83.74 23.70 14.17 85.048.67 4.6079.73 21.80 9.73 80.94 PSC12.37 8.0682.47 7.42 5.61 82.1614.75 9.2483.4633.64 18.3984.69 8.01 4.20 79.9022.69 9.7480.96MFC 11.84 7.80 82.567.52 5.6482.11 14.25 8.98 83.53 33.20 18.27 84.77 7.48 3.92 79.95 22.34 9.66 81.07 E5 Char 4.17 2.9383.452.14 1.68 82.67 3.08 2.2484.0314.73 8.8985.886.59 3.5980.2420.42 9.37 80.95Sent 5.66 3.99 82.08 2.64 2.11 81.35 6.23 4.45 82.97 17.15 10.37 84.73 7.54 4.17 79.58 18.25 8.57 80.60Sem 7.99 5.58 83.09 3.60 2.8182.709.89 6.90 83.94 23.34 13.99 85.53 9.07 4.87 80.08 9.03 4.3581.17Rec 8.38 5.77 82.79 3.95 3.11 82.49 9.42 6.59 83.73 24.06 14.43 85.03 8.82 4.83 80.1122.05 10.0080.98 PSC11.79 7.7182.61 6.65 5.13 82.3213.39 8.7883.6434.32 18.9384.7312.06 6.3680.03 20.38 9.09 80.87MFC 11.08 7.34 82.727.71 5.8182.11 12.55 8.31 83.78 33.89 18.91 84.84 9.78 5.23 80.14 16.97 7.49 80.98 Table 3.3: Evaluation metrics ROUGE1 (R1), ROUGE2 (R2) and BERTScore (BScore) on remaining 6 datasets from RAGBench. results in Table 1 indicate a clear improvement in both the retrieval and genera- tion time with PSC chunker. This shows that the chunking strategy is not only improving retrieval and generation but also lowering the time taken to retrieve relevant chunks and generate an answer. This reiterates our claim for using light and simple models for fast and improved chunking. 6 Conclusion Our approach demonstrates a substantial improvement in retrieval effectiveness, with the MFC chunker trained on the E5 embedding model achieving an MRR increase of up to 24 times over baseline methods. This performance gain high- lights the power of domain-specific semantic chunking in optimizing retrieval- augmented generation (RAG) pipelines. By producing chunks that naturally align with human-authored section boundaries, our method preserves contex- tual coherence and facilitates more accurate retrieval, which in turn benefits downstream generation quality. Beyond PubMedQA, our results on out-of-domain datasets indicate strong potential for generalization, suggesting that the principles of semantic chunking we present are transferable across domains. This work not only provides a con- crete, reproducible methodology for chunking optimization but also introduces a freshperspectiveondocumentsegmentation,acriticalyetunderexploredcompo- nent in RAG systems. We believe that our findings can serve as a foundation for future research in retrieval-aware chunking strategies, adaptive chunk size deter- mination, and domain-adaptive retrieval optimization. The lightweight models introduced in this paper can be integrated into complex models resulting in more sophisticated approaches. Breaking It Down 11 References 1. Es, S., James, J., Espinosa Anke, L., Schockaert, S.: RAGAs: Automated eval- uation of retrieval augmented generation. In: Aletras, N., De Clercq, O. (eds.) Proceedings of the 18th Conference of the European Chapter of the Associa- tion for Computational Linguistics: System Demonstrations. pp. 150–158. As- sociation for Computational Linguistics, St. Julians, Malta (Mar 2024),https: //aclanthology.org/2024.eacl-demo.16/ 2. Finardi, P., Avila, L., Castaldoni, R., Gengo, P., Larcher, C., Piau, M., Costa, P., Caridá, V.: The chronicles of rag: The retriever, the chunk and the generator. arXiv preprint arXiv:2401.07883 (2024) 3. Friel, R., Belyi, M., Sanyal, A.: Ragbench: Explainable benchmark for retrieval- augmented generation systems. arXiv preprint arXiv:2407.11005 (2024) 4. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang, H., Wang, H.: Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.109972(1) (2023) 5. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., Steinhardt, J.: Measuring massive multitask language understanding. In: International Con- ference on Learning Representations (2021),https://openreview.net/forum?id= d7KBjmI3GmQ 6. Jin, Q., Dhingra, B., Liu, Z., Cohen, W., Lu, X.: Pubmedqa: A dataset for biomed- ical research question answering. In: Proceedings of the 2019 Conference on Em- pirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2567–2577 (2019) 7. Kamradt, G.: Semantic chunking.https://github.com/charlespwd/ project-title(2024) 8. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küt- tler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval- augmented generation for knowledge-intensive nlp tasks. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 9459–9474. Curran Associates, Inc. (2020),https://proceedings.neurips.cc/paper_files/paper/2020/file/ 6b493230205f780e1bc26945df7481e5-Paper.pdf 9. Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: Text sum- marization branches out. pp. 74–81 (2004) 10. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311–318 (2002) 11. Qu, R., Tu, R., Bao, F.S.: Is semantic chunking worth the computational cost? In: Chiruzzo, L., Ritter, A., Wang, L. (eds.) Findings of the Associa- tion for Computational Linguistics: NAACL 2025. pp. 2155–2177. Association for Computational Linguistics, Albuquerque, New Mexico (Apr 2025).https:// doi.org/10.18653/v1/2025.findings-naacl.114,https://aclanthology.org/ 2025.findings-naacl.114/ 12. Reimers, N., Gurevych, I.: Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In: Inui, K., Jiang, J., Ng, V., Wan, X. (eds.) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process- ing and the 9th International Joint Conference on Natural Language Process- ing (EMNLP-IJCNLP). pp. 3982–3992. Association for Computational Linguis- 12 Authors Suppressed Due to Excessive Length tics, Hong Kong, China (Nov 2019).https://doi.org/10.18653/v1/D19-1410, https://aclanthology.org/D19-1410/ 13. Song, K., Tan, X., Qin, T., Lu, J., Liu, T.Y.: Mpnet: Masked and permuted pre- training for language understanding. Advances in neural information processing systems33, 16857–16867 (2020) 14. Su, L., Duan, N., Cui, E., Ji, L., Wu, C., Luo, H., Liu, Y., Zhong, M., Bharti, T., Sacheti, A.: GEM: A general evaluation benchmark for multimodal tasks. In: Zong, C., Xia, F., Li, W., Navigli, R. (eds.) Findings of the Association for Com- putational Linguistics: ACL-IJCNLP 2021. pp. 2594–2603. Association for Com- putational Linguistics, Online (Aug 2021).https://doi.org/10.18653/v1/2021. findings-acl.229,https://aclanthology.org/2021.findings-acl.229/ 15. Thakur, N., Reimers, N., Rücklé, A., Srivastava, A., Gurevych, I.: BEIR: A het- erogeneous benchmark for zero-shot evaluation of information retrieval models. In: Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021),https://openreview.net/forum?id= wCu6T5xFjeJ 16. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023),https:// doi.org/10.48550/arXiv.2302.13971 17. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.: Superglue: A stickier benchmark for general-purpose language under- standing systems. Advances in neural information processing systems32(2019) 18. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: GLUE: A multi-task benchmark and analysis platform for natural language understand- ing. In: International Conference on Learning Representations (2019),https: //openreview.net/forum?id=rJ4km2R5t7 19. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., Wei, F.: Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022) 20. Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., Zhou, M.: Minilm: Deep self- attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems33, 5776–5788 (2020) 21. Zhang*, T., Kishore*, V., Wu*, F., Weinberger, K.Q., Artzi, Y.: Bertscore: Eval- uating text generation with bert. In: International Conference on Learning Repre- sentations (2020),https://openreview.net/forum?id=SkeHuCVFDr 22. Zhong, Z., Liu, H., Cui, X., Zhang, X., Qin, Z.: Mix-of-granularity: Optimize the chunking granularity for retrieval-augmented generation. In: Rambow, O., Wanner, L.,Apidianaki,M.,Al-Khalifa,H.,Eugenio,B.D.,Schockaert,S.(eds.)Proceedings of the 31st International Conference on Computational Linguistics. pp. 5756–5774. Association for Computational Linguistics, Abu Dhabi, UAE (Jan 2025),https: //aclanthology.org/2025.coling-main.384/ 23. Zhou, K., Ethayarajh, K., Card, D., Jurafsky, D.: Problems with cosine as a measure of embedding similarity for high frequency words. In: Muresan, S., Nakov, P., Villavicencio, A. (eds.) Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 2: Short Pa- pers). pp. 401–423. Association for Computational Linguistics, Dublin, Ire- land (May 2022).https://doi.org/10.18653/v1/2022.acl-short.45,https:// aclanthology.org/2022.acl-short.45/