arXiv:2403.10939v1 [cs.IR] 16 Mar 2024 Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning Georgios Sidiropoulos [0000− 0002− 6486− 089X] and Evangelos Kanoulas[0000− 0002− 8312− 0694] University of Amsterdam, Amsterdam, The Netherlands {g.sidiropoulos, e.kanoulas }@uva.nl Abstract. Dense retrieval has become the new paradigm in passage re- trieval. Despite its eﬀectiveness on typo-free queries, it is not robust when dealing with queries that contain typos. Current works on im proving the typo-robustness of dense retrievers combine (i) data augme ntation to ob- tain the typoed queries during training time with (ii) addit ional robusti- fying subtasks that aim to align the original, typo-free que ries with their typoed variants. Even though multiple typoed variants are a vailable as positive samples per query, some methods assume a single pos itive sample and a set of negative ones per anchor and tackle the robustify ing subtask with contrastive learning; therefore, making insuﬃcient u se of the multi- ple positives (typoed queries). In contrast, in this work, w e argue that all available positives can be used at the same time and employ co ntrastive learning that supports multiple positives (multi-positiv e). Experimental results on two datasets show that our proposed approach of le verag- ing all positives simultaneously and employing multi-posi tive contrastive learning on the robustifying subtask yields improvements i n robustness against using contrastive learning with a single positive. Keywords: Dense retrieval · Typo-robustness · Contrastive learning. 1 Introduction Dense retrieval has become the new paradigm in passage retrieval. It has demon- strated higher eﬀectiveness than traditional lexical-based metho ds due to its abil- ity to tackle the vocabulary mismatch problem [5]. Even though dens e retrievers are highly eﬀective on typo-free queries, they can witness a drama tic perfor- mance decrease when dealing with queries that contain typos [11,12,1 4]. Recent works on robustifying dense retrievers against typos utilize data a ugmentation to obtain typoed versions of the original queries at training time. Mo reover, they introduce additional robustifying subtasks to minimize the rep resentation discrepancy between the original query and its typoed variants. Sidiropoulos and Kanoulas [11] applied an additional contrastive loss t o en- force the latent representations of the original, typo-free quer ies to be closer to their typoed variants. Zhuang and Zuccon [14] utilized a self-teach ing training 2 Georgios Sidiropoulos and Evangelos Kanoulas strategy to minimize the diﬀerence between the score distribution o f the origi- nal query and its typoed variants. Alternatively, Tasawong et al. [13] employed dual learning in combination with self-teaching [14] and contrastively trained the dense retriever on the prime task of passage retrieval and th e dual task of query retrieval (learns the query likelihood to retrieve queries for passages). Despite the improvements in robustness, the existing typo-robus t methods do not always make optimal use of the available typoed queries. In de tail, they address the robustifying subtasks with contrastive learning, ass uming a single positive sample (query) and a set of negative ones per anchor (dep ending on the approach, the anchor can be either a query or a passage). Howev er, alongside the original query, its multiple typoed variants are available. Hence, there is more than one positive sample per anchor. As a result, we can levera ge all the available positives simultaneously and apply multi-positive contrastive learning instead (i.e., contrastive learning that supports multiple positives). For instance, Tasawong et al. [13] computes the contrastive loss for the query retrieval subta sk using only the original, typo-free query as relevant for a given pass age. Given a passage, we argue that both the original query and its typoed var iations can be considered as relevant and adopt a multi-positive contrastive loss in stead. Literature on contrastive learning has shown that including multiple p ositives can enhance the ability of the model to discriminate between signal a nd noise (negatives)[6,8]. Intuitively, multiple negatives focus on what makes the anchor and the negatives dissimilar, while multiple positives focus on what make s the anchor and the positives similar. To this end, contrasting among mult iple pos- itives and negatives can bring an anchor and all its positives closer to gether in the latent space while keeping them far from the negatives. In this work, we revisit recent methods in typo-robust dense retr ieval and unveil that, in many cases, they do not suﬃciently utilize the multiple p ositives that are available. Speciﬁcally, when tackling the robustifying subta sks, they ignore that multiple positives are available per anchor and consider co ntrastive learning with a single positive. In contrast, we suggest leveraging all the available positives and adopting a multi-positive contrastive learning approac h. We aim to answer the following research questions: RQ1 Can our multi-positive contrastive learning approach increase the r obust- ness of dense retrievers that use contrastive learning with a single positive? RQ2 Does our multi-positive contrastive learning variant outperform its single- positive counterpart regardless of the number of positives? Our experimental results on two datasets show that our propose d approach of employing multi-positive contrastive learning yields improvements in ro bustness compared to contrastive learning with a single positive. 1 1 https://github.com/GSidiropoulos/typo-robust-multi-positive-DR Typo-Robust Dense Retrieval via Multi-positive Contrasti ve Learning 3 2 Methodology Contrastive learning is a vital component for training an eﬀective de nse retriever. Current typo-robust dense retrievers use contrastive learning with a single posi- tive sample and multiple negative ones for both the main task of passa ge retrieval and the robustifying subtasks. In detail, given an anchor x, a positive sample x+, and a set of negative samples X − , the contrastive prediction task aims to bring the positive sample closer to the anchor than any other negat ive sample: LCE (x, x+, X− ) = − log ef (x,x+) ef (x,x+) + ∑ x−∈ X − ef (x,x−) , (1) where f is a similarity function (e.g., dot product). However, in many cases, multiple positive samples are available per anc hor and can be used simultaneously to increase the discriminative perfor mance of the model. As opposed to the aforementioned contrastive loss that su pports a single positive, we propose employing a multi-positive contrastive loss to be neﬁt from all the available positives. Given an anchor x, multiple positive samples X +, and multiple negatives X − , a multi-positive contrastive loss [6] is computed as: LM CE(x, X+, X− ) = − 1 |X +| ∑ x+∈ X + log ef (x,x+) ef (x,x+) + ∑ x−∈ X − ef (x,x−) . (2) This work aims to identify cases in typo-robust dense retrieval met hods where the robustifying subtasks consider only a single positive sample, eve n though multiple ones are available, and optimize a contrastive loss. Next, we r eplace the contrastive loss with its multi-positive alternative to beneﬁt from all the avail- able positives. Below we present the typo-robust dense retrieval methods we build upon followed by our multi-positive variants. We focus on dense retr ievers that follow the dual-encoder architecture [5]. A traditional dense retrie ver, DR, is op- timized only with the passage retrieval task. Given a query q, a positive/relevant passage p+, and a set of negative/irrelevant passages P − = {p− i }N i=1, the learning task trains the query and passage encoders via minimizing the softm ax cross- entropy: Lp CE = LCE (q, p+, P − ). Positive query-passage pairs are encouraged to have higher similarity scores and negative pairs to have lower scores . 2.1 Dense Retriever with Self-supervised Contrastive Lear ning DR+CL alternates DR with an additional contrastive loss that maximizes the agreement between diﬀerently augmented views of the same query [11]. This loss enforces that a query q and its typoed variation q′, sampled from a set of available typoed variations Q′ = {q′ i}K i=1, are close together in the latent space and distant from other distinct queries Q− = {q− i }M i=1: Lt CE = LCE (q, q′, Q− ). The ﬁnal loss is computed as a weighted summation, L = w1Lp CE + w2Lt CE . 4 Georgios Sidiropoulos and Evangelos Kanoulas DR+CLM is our multi-positive variant of DR+CL. Given a query q, instead of sampling a diﬀerent typoed variant q′ from a set Q′ at each update, we propose simultaneously employing all typoed variants. To do so, we replace Lt CE with the following multi-positive contrastive loss that accounts for multip le positives: Lt M CE = LM CE(q, Q′, Q− ). The ﬁnal loss is: L = w1Lp CE + w2Lt M CE. 2.2 Dense Retriever with Dual Learning DR+DL trains a robust, dense retriever via a contrastive dual learning me cha- nism [7]. In contrast to classic DR, which is optimized for passage retr ieval only (Lp CE ), DR+DL is optimized for the prime task of passage retrieval (i.e., lea rns to retrieve relevant passages for queries) and the dual task of q uery retrieval (i.e., learns to retrieve relevant queries for passages). Therefore, g iven a passage p, a positive query q+, and a set of negative queries Q− = {q− i }M i=1, it further mini- mizes the loss for the dual task: Lq CE = LCE (p, q+, Q− ). The dual training loss is added to the prime training loss to conduct contrastive dual learn ing and train the dense retriever. Speciﬁcally, the ﬁnal loss is computed as L = Lp CE + wLq CE , where w is used to weight the dual task loss. DR+DLM is our multi-positive variant of DR+DL. Contrary to DR+DL, we propose that for the query retrieval task, given a passage p, we can have a set of relevant queries consisting of the typo-free query and its typo ed variants, Q = {q+, q′ 1, q′ 2, . . . , q′ K}. Thus, we replace the contrastive loss of Lq CE with a multi-positive contrastive loss, which can account for multiple releva nt queries at the same time. We deﬁne the multi-positive contrastive loss for th e dual task as: Lq M CE = LM CE(p, Q, Q− ). The ﬁnal loss is computed as L = Lp CE +wLq M CE. 2.3 Dense Retriever with Dual Learning and Self-T eaching DR+ST+DL trains a dense retriever with dual learning and self-teaching [13]. Similar to DR+DL, it minimizes the Lp CE and Lq CE for the main task of pas- sage retrieval and the subtask of query retrieval, respectively. The additional self-teaching mechanism distills knowledge from a typo-free query q into its ty- poed variants Q′ = {q′ i}K i=1 by forcing the model to match score distributions of misspelled queries to the score distribution of the typo-free query for both the passage retrieval and query retrieval task. This is achieved by min imizing the KL- divergence losses: (i) Lp KL = 1 K ∑K k=1 LKL (s′k p , sp), where {s′1 p , s′2 p , . . . , s′K p } and sp is the score distribution in a passage-to-queries direction (passag e retrieval) for the typoed queries and the typo-free query, respectively an d (ii) Lq KL = 1 K ∑K k=1 LKL (s′k q , sq), where {s′1 q , s′2 q , . . . , s′K q } and sq is the score distribution in a query-to-passages direction (query retrieval) for the typoe d queries and the typo-free query, respectively. The ﬁnal loss is computed as the w eighted summa- tion of the four losses, L = (1 −β)((1−γ)Lp CE +γLq CE )+ β((1−σ)Lp KL +σLq KL ). DR+ST+DLM is our multi-positive variant of DR+ST+DL. Even though DR+ST+DL simultaneously uses all the available typo variations of a qu ery in order to calculate the KL divergence losses for the prime passage retrieval Typo-Robust Dense Retrieval via Multi-positive Contrasti ve Learning 5 task and the dual query retrieval, it uses only the typo-free quer y to compute the contrastive loss for query retrieval ( Lq CE ). To fully beneﬁt from the multi- ple available typoed queries per typo-free query, we replace the co ntrastive loss for query retrieval Lq CE with a multi-positive variant that supports samples with multiple positives Lq M CE. The ﬁnal loss is computed as the weighted summation, L = (1 − β)((1 − γ)Lp CE + γLq M CE) + β((1 − σ)Lp KL + σLq KL ). 3 Experimental Setup Query augmentation. From the aforementioned methods, those employing queries with typos in their training scheme are augmentation-based . In detail, during training, the typoed queries are generated from the origina l, typo-free queries through a realistic typo generator [9]. The typo generator applies the following transformations that often occur in human-generated q ueries: random character insertion, deletion and substitution, swapping neighbor ing characters, and keyboard-based character swapping [4]. Datasets and evaluation. We conduct our experiments on MS MARCO pas- sage ranking [10] and DL-Typo [14] on their typo-free and typoed versions. Both datasets use the same underlying corpus of 8 .8 million passages and ∼ 400K training queries but diﬀer in evaluation queries. DL-Typo provides 60 real-world queries with typos alongside their manually corrected typo-free ve rsion. The development set of MS MARCO consists of 6 , 980 queries (the test set is not publicly available). Following previous works [13,14], we obtain typo var iations for each typo-free query via a synthetic typo generation model a nd repeat the typo generation process 10 times. To measure the retrieval perf ormance, we re- port the oﬃcial metrics of each dataset. For the evaluation on the typo version of MS MARCO, we report the metrics averaged for each repeated e xperiment since typoed queries are generated 10 times for each original quer y. Implementation details. We follow an in-batch negative training setting with 7 hard negative passages per query and a batch size of 16 to train t he dense re- trievers. 2 We use AdamW optimizer with a 10 − 5 learning rate, linear scheduling with 10 K warm-up steps, and decay over the rest of the training steps. We train up to 150 K steps. We implement the query and passage encoders with BERT [2]. When applicable, we set the query augmentation size to 40. For the r emaining hyperparameters speciﬁc to each method (e.g., weight w in DR+CL), we use the values initially proposed by the creators of each method. We use the Tevatron toolkit [3] to train the models and the Ranx library [1] to evaluate the r etrieval performance. Finally, we use the typo generators from the TextA ttack toolkit [9] for all the methods we experiment with to augment the training qu eries. 2 The original methods and our proposed counterparts employ t he same number of original, typo-free query-passage pairs per batch. Howeve r, our method leverages multiple typoed variants for each query; therefore, the bat ch we need to ﬁt in the GPU memory is larger. 6 Georgios Sidiropoulos and Evangelos Kanoulas 4 Results To answer RQ1, we compare the retrieval performance of our multi-positive con- trastive learning approaches against the original models. From Tab le 1, we see that employing our multi-positive contrastive learning approach yield s improve- ments in robustness against typos upon the original methods that use contrastive learning with a single positive. As expected, the more dramatic improvement comes when applying m ulti- positive contrastive learning on DR+DL since the original work only co nsiders the typo-free query as positive when computing the contrastive lo ss for query re- trieval (see Section 2.2). In contrast, in our DR+DL M , we consider the typo-free query and all its available typoed variants as positives and use a multi- positive contrastive loss for query retrieval. We also see improvements whe n compar- ing DR+CL vs. our DR+CL M . In detail, employing all available positives (ty- poed queries) at once and using multi-positive contrastive loss outp erforms sam- pling a diﬀerent positive at each update and using a single positive cont rastive loss (see Section 2.1). The improvements are held even when compar ing our DR+DL+STM against DR+DL+ST, a model that already uses multiple posi- tives. As seen in Section 2.3, DR+DL+ST uses a contrastive loss with a single positive for the query retrieval dual task (i.e., Lq CE ) while considering multiple positives simultaneously to compute the KL-divergence losses (i.e., Lp KL , Lq KL ). Table 1. Retrieval results for the settings of (i) clean queries (Cle an), and (ii) queries with typos (Typo). Statistical signiﬁcant gains (two-tail ed paired t-test with Bonferroni correction, p < 0.05) obtained from models with multi-positive contrastive l oss (ours) over their original version with standard contrastive loss are indicated by †. Model Multi-positive contrastive loss MS MARCO DL-Typo Clean Typo Clean Typo MRR@10 R@1000 MRR@10 R@1000 nDCG@10 MRR MAP nDCG@10 MRR MAP DR ✗ .331 .953 .140 .698 .677 .850 .555 .264 .395 .180 DR+DL ✗ .332 .953 .140 .698 .679 .826 .557 .269 .411 .186 DR+DLM ✓ .335 .958 .213 † .866† .699 .864 .585 .347 † .452 .259 † DR+CL ✗ .321 .957 .170 .787 .659 .797 .535 .284 .411 .207 DR+CLM ✓ .322 .956 .178 † .811† .652 .847 .539 .290 .447 .215 DR+ST+DL ✗ .334 .951 .259 .893 .681 .868 .567 .412 .543 .315 DR+ST+DLM ✓ .335 .955 .261 .902 † .687 .870 .579 .426 † .583 .342 † At this point, we want to explore how the diﬀerent numbers of positiv es aﬀect our multi-positive approach ( RQ2). To do so, we compare our DR+DL+ST M against DR+DL+ST. In its training, the latter already employs multiple posi- tives simultaneously to compute the KL-divergence losses. Howeve r, our multi- positive approach fully beneﬁts from the multiple available positives by incor- porating them when computing the contrastive loss for query retr ieval (Lq CE → Lq M CE). Table 2 unveils that our multi-positive variant consistently outper forms the original model for the diﬀerent numbers of typoed variants pe r query. Typo-Robust Dense Retrieval via Multi-positive Contrasti ve Learning 7 Table 2. Retrieval results for diﬀerent query augmentation sizes ( K). We report the results in the format “ R@1000 ( M RR@10)” on MS MARCO with typos. Multi-positive contrastive loss K 1 10 20 30 40 DR+ST+DL ✗ .884 (.251) .892 (.258) .894 (.258) .893 (.259) .893 (.259) DR+ST+DLM ✓ .884 (.251) .898 (.260) .900 (.260) .902 (.261) .902 (.261) 5 Conclusions In this work, we revisit recent studies in typo-robust dense retrie val and showcase that they do not always make suﬃcient use of multiple positive samples . In de- tail, they assume a single positive sample and multiple negatives per anc hor and use contrastive learning for the robustifying subtasks. Opposed to this, we pro- pose to leverage all the available positives and employ multi-positive co ntrastive learning. Experimentation on two datasets shows that following a mu lti-positive contrastive learning approach yields improvements in the robustne ss of the un- derlying dense retriever upon contrastive learning with a single posit ive. Acknowledgements This research was supported by the NWO Innovational Research Incentives Scheme Vidi (016.Vidi.189.039). All content re presents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors. References 1. Bassani, E.: ranx: A blazing-fast python library for rank ing evaluation and com- parison. In: Hagen, M., Verberne, S., Macdonald, C., Seifer t, C., Balog, K., Nørv ˚ ag, K., Setty, V. (eds.) Advances in Information Retri eval - 44th Euro- pean Conference on IR Research, ECIR 2022, Stavanger, Norwa y, April 10- 14, 2022, Proceedings, Part II. Lecture Notes in Computer Sc ience, vol. 13186, pp. 259–264. Springer (2022). https://doi.org/10.1007/9 78-3-030-99739-7 30, https://doi.org/10.1007/978-3-030-99739-7_30 2. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-t raining of deep bidirec- tional transformers for language understanding. In: Burst ein, J., Doran, C., Solorio, T. (eds.) Proceedings of the 2019 Conference of the North Ame rican Chapter of the Association for Computational Linguistics: Human Lang uage Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volum e 1 (Long and Short Papers). pp. 4171–4186. Association for Computation al Linguistics (2019). https://doi.org/10.18653/v1/n19-1423, https://doi.org/10.18653/v1/n19-1423 3. Gao, L., Ma, X., Lin, J., Callan, J.: Tevatron: An eﬃcient a nd ﬂex- ible toolkit for neural retrieval. In: Chen, H., Duh, W.E., H uang, H., Kato, M.P., Mothe, J., Poblete, B. (eds.) Proceedings of the 46th In- ternational ACM SIGIR Conference on Research and Developme nt in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23 -27, 2023. pp. 3120–3124. ACM (2023). https://doi.org/10.1145/3539 618.3591805, https://doi.org/10.1145/3539618.3591805 8 Georgios Sidiropoulos and Evangelos Kanoulas 4. Hagen, M., Potthast, M., Gohsen, M., Rathgeber, A., Stein , B.: A large-scale query spelling correction corpus. In: Kando, N ., Sakai, T., Joho, H., Li, H., de Vries, A.P., White, R.W. (eds.) Proceedi ngs of the 40th International ACM SIGIR Conference on Research and Devel- opment in Information Retrieval, Shinjuku, Tokyo, Japan, A ugust 7-11, 2017. pp. 1261–1264. ACM (2017). https://doi.org/10.1145 /3077136.3080749, https://doi.org/10.1145/3077136.3080749 5. Karpukhin, V., Oguz, B., Min, S., Lewis, P.S.H., Wu, L., Ed unov, S., Chen, D., Yih, W.: Dense passage retrieval for open-domain questi on answering. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings o f the 2020 Conference on Empirical Methods in Natural Language Proces sing, EMNLP 2020, Online, November 16-20, 2020. pp. 6769–6781. Associa tion for Com- putational Linguistics (2020). https://doi.org/10.1865 3/v1/2020.emnlp-main.550, https://doi.org/10.18653/v1/2020.emnlp-main.550 6. Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Is ola, P., Maschinot, A., Liu, C., Krishnan, D.: Supervised contrastive learning . In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advance s in Neural In- formation Processing Systems 33: Annual Conference on Neur al Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020 , virtual (2020), https://proceedings.neurips.cc/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html 7. Li, Y., Liu, Z., Xiong, C., Liu, Z.: More robust dense retri eval with con- trastive dual learning. In: Hasibi, F., Fang, Y., Aizawa, A. (eds.) IC- TIR ’21: The 2021 ACM SIGIR International Conference on the T he- ory of Information Retrieval, Virtual Event, Canada, July 1 1, 2021. pp. 287–296. ACM (2021). https://doi.org/10.1145/347115 8.3472245, https://doi.org/10.1145/3471158.3472245 8. Ma/suppress lki´ nski, M., Ma´ ndziuk, J.: Multi-label contrastive learning for abstract visual reasoning. IEEE Transactions on Neural Networks and Learni ng Systems pp. 1–13 (2022). https://doi.org/10.1109/TNNLS.2022.3185949 9. Morris, J.X., Liﬂand, E., Yoo, J.Y., Grigsby, J., Jin, D., Qi, Y.: Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP. In: Liu, Q., Schlangen, D. (eds.) Proceedings of the 202 0 Conference on Em- pirical Methods in Natural Language Processing: System Dem onstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020. pp. 119–126. Ass ociation for Com- putational Linguistics (2020). https://doi.org/10.1865 3/v1/2020.emnlp-demos.16, https://doi.org/10.18653/v1/2020.emnlp-demos.16 10. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng, L.: MS MARCO: A human generated machine reading comprehensi on dataset. In: Besold, T.R., Bordes, A., d’Avila Garcez, A.S., Wayne, G . (eds.) Proceed- ings of the Workshop on Cognitive Computation: Integrating neural and sym- bolic approaches 2016 co-located with the 30th Annual Confe rence on Neu- ral Information Processing Systems (NIPS 2016), Barcelona , Spain, Decem- ber 9, 2016. CEUR Workshop Proceedings, vol. 1773. CEUR-WS. org (2016), http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf 11. Sidiropoulos, G., Kanoulas, E.: Analysing the robustne ss of dual en- coders for dense retrieval against misspellings. In: Amig´ o, E., Castells, P., Gonzalo, J., Carterette, B., Culpepper, J.S., Kazai, G. (eds.) SI- GIR ’22: The 45th International ACM SIGIR Conference on Rese arch and Development in Information Retrieval, Madrid, Spain, J uly 11 - 15, Typo-Robust Dense Retrieval via Multi-positive Contrasti ve Learning 9 2022. pp. 2132–2136. ACM (2022). https://doi.org/10.1145 /3477495.3531818, https://doi.org/10.1145/3477495.3531818 12. Sidiropoulos, G., Vakulenko, S., Kanoulas, E.: On the im pact of speech recog- nition errors in passage retrieval for spoken question answ ering. In: Hasan, M.A., Xiong, L. (eds.) Proceedings of the 31st ACM Internati onal Conference on Information & Knowledge Management, Atlanta, GA, USA, Oc tober 17- 21, 2022. pp. 4485–4489. ACM (2022). https://doi.org/10.1 145/3511808.3557662, https://doi.org/10.1145/3511808.3557662 13. Tasawong, P., Ponwitayarat, W., Limkonchotiwat, P., Ud omcharoenchaikit, C., Chuangsuwanich, E., Nutanong, S.: Typo-robust represe ntation learn- ing for dense retrieval. In: Rogers, A., Boyd-Graber, J.L., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Assoc ia- tion for Computational Linguistics (Volume 2: Short Papers ), ACL 2023, Toronto, Canada, July 9-14, 2023. pp. 1106–1115. Associati on for Com- putational Linguistics (2023). https://doi.org/10.1865 3/v1/2023.acl-short.95, https://doi.org/10.18653/v1/2023.acl-short.95 14. Zhuang, S., Zuccon, G.: Characterbert and self-teachin g for improving the robustness of dense retrievers on queries with typos. In : Amig´ o, E., Castells, P., Gonzalo, J., Carterette, B., Culpepper, J.S. , Kazai, G. (eds.) SIGIR ’22: The 45th International ACM SIGIR Conference on Re search and Development in Information Retrieval, Madrid, Spain, J uly 11 - 15, 2022. pp. 1444–1454. ACM (2022). https://doi.org/10.1145 /3477495.3531951, https://doi.org/10.1145/3477495.3531951