arXiv:2506.00054v1 [cs.IR] 28 May 2025 This is a preprint under review at ACM TOIS. Do not redistribute the final version without permission. Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers CHAITANYA SHARMA, Independent Researcher, United States Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance large language models (LLMs) by conditioning generation on external evidence retrieved at inference time. While RAG addresses critical limitations of parametric knowledge storage‚Äîsuch as factual inconsistency and domain inflexibility‚Äîit introduces new challenges in retrieval quality, grounding fidelity, pipeline efficiency, and robustness against noisy or adversarial inputs. This survey provides a comprehensive synthesis of recent advances in RAG systems, offering a taxonomy that categorizes architectures into retriever-centric, generator-centric, hybrid, and robustness-oriented designs. We systematically analyze enhancements across retrieval optimization, context filtering, decoding control, and efficiency improvements, supported by comparative performance analyses on short-form and multi-hop question answering tasks. Furthermore, we review state-of-the-art evaluation frameworks and benchmarks, highlighting trends in retrieval-aware evaluation, robustness testing, and federated retrieval settings. Our analysis reveals recurring trade-offs between retrieval precision and generation flexibility, efficiency and faithfulness, and modularity and coordination. We conclude by identifying open challenges and future research directions, including adaptive retrieval architectures, real-time retrieval integration, structured reasoning over multi-hop evidence, and privacy-preserving retrieval mechanisms. This survey aims to consolidate current knowledge in RAG research and serve as a foundation for the next generation of retrieval-augmented language modeling systems. CCS Concepts: ‚Ä¢Information systems ‚Üí Retrieval models and ranking; Evaluation of retrieval results; Information retrieval query processing; Retrieval tasks and goals ; Document representation. Additional Key Words and Phrases: Retrieval-Augmented Generation, Query Reformulation, Context Filtering, Reranking, Multi-hop Reasoning, Hallucination Mitigation, Robustness, Dynamic Retrieval, Evaluation Benchmarks, Federated Retrieval, Faithfulness, Efficiency Optimization, Document Ranking, LLM Alignment, Open-Domain QA 1 Introduction Large Language Models (LLMs) have demonstrated impressive generalization across natural language tasks, but their reliance on static, parametric knowledge remains a fundamental limitation. This restricts their ability to handle queries requiring up-to-date, verifiable, or domain-specific information, often resulting in hallucinations or factual inconsistencies [19, 40]. Retrieval-Augmented Generation (RAG) addresses this issue by coupling pretrained language models with non- parametric retrieval modules that fetch external evidence during inference. By conditioning generation on retrieved documents, RAG systems offer greater transparency, factual grounding, and adaptability to evolving knowledge bases. These properties have made RAG central to tasks such as open-domain QA, biomedical reasoning, knowledge-grounded dialogue, and long-context summarization. However, integrating retrieval with generation introduces unique challenges: retrieval noise and redundancy can degrade output quality; misalignment between retrieved evidence and generated text can lead to hallucinations; and pipeline inefficiencies and latency make deployment costly at scale. Moreover, balancing modularity with tight retrieval‚Äìgeneration interaction remains an open architectural trade-off. In this survey, we first present a high-level taxonomy of RAG architectures based on where core innovations occur‚Äîwithin the retriever, the generator, or through their joint coordination (Section 3). We begin with a background Author‚Äôs Contact Information: Chaitanya Sharma, Independent Researcher, United States. Manuscript submitted to ACM 1 Retrieval-Augmented Generation: A Survey 2 Fig. 1. Retrieval-Augmented Generation (RAG) workflow. A user query is processed by the retriever, which may perform query expansion before retrieving documents from external knowledge sources (e.g., databases, APIs, or document stores). Retrieved documents are re-ranked by relevance, and the Top-K are passed to the generator as factual context. The generator synthesizes a response conditioned on both the query and retrieved content. An optional post-processing step (e.g., ranking, rewriting, or fact-checking) may further refine the output, enhancing factual consistency, real-time adaptability, and overall response quality in large language models (LLMs). on RAG‚Äôs mathematical formulation and components (Section 2.2), and then explore advances in retrieval strategies, filtering, and control mechanisms (Section 4). We further analyze how RAG systems are benchmarked (Section 6), compare prominent frameworks (Section 5), and conclude with open research challenges and future directions (Section 7). 2 Background and foundations of retrieval-augmented generation Retrieval-Augmented Generation (RAG) is a framework that augments large language models (LLMs) with external knowledge access via document retrieval. It builds on the intuition that generating grounded and verifiable responses requires not only parametric knowledge stored in model weights, but also non-parametric access to a dynamic evidence corpus. This section outlines the core components of RAG systems and presents the mathematical formulation that underpins their design. 2.1 Components of a RAG System At a high level, a RAG system consists of three modules: Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 3 Query Encoder: Encodes the input ùë• into a query representation ùëû, which is used to retrieve relevant documents. This can be either a neural encoder or a rule-based template. Retriever: Given the query ùëû, the retriever fetches a ranked list of documents ùëë1, ùëë2, . . . , ùëëùëò from a corpus C. Retrievers may be sparse (e.g., BM25 [54]), dense (e.g., DPR [36]), hybrid, or generative. Generator: The generator conditions on the input ùë• and the retrieved documents ùëëùëñ to produce the final output ùë¶. This is typically a pretrained transformer model (e.g., T5 [51], BART [39], GPT [5]). 2.2 Mathematical Formulation Formally, the generation process in Retrieval-Augmented Generation (RAG) can be expressed as modeling the conditional distribution: ùëÉ (ùë¶ | ùë•) = ‚àëÔ∏Å ùëë ‚àà C ùëÉ (ùë¶ | ùë•, ùëë) ¬∑ ùëÉ (ùëë | ùë•) (1) where: ‚Ä¢ ùë• is the input (e.g., a question or prompt), ‚Ä¢ ùëë is a retrieved document from corpus C, ‚Ä¢ ùë¶ is the generated response. In practice, the summation is approximated by retrieving the top-ùëò documents ùëë1, . . . , ùëëùëò, yielding: ùëÉ (ùë¶ | ùë•) ‚âà ùëò‚àëÔ∏Å ùëñ=1 ùëÉ (ùë¶ | ùë•, ùëëùëñ ) ¬∑ ùëÉ (ùëëùëñ | ùë•) (2) This decomposition reflects two key probabilities: ‚Ä¢ ùëÉ (ùëëùëñ | ùë•): the relevance score of document ùëëùëñ given the input ùë•, often derived from a retriever or reranker. ‚Ä¢ ùëÉ (ùë¶ | ùë•, ùëëùëñ ): the probability of generating output ùë¶ conditioned on ùë• and document ùëëùëñ, modeled by the language model. Variants of RAG differ in how they estimate and combine these components. Some use a fixed retriever and let the generator handle noisy inputs, while others jointly optimize retrieval and generation to maximize downstream utility. 3 Taxonomy of RAG Architectures To contextualize recent advances in Retrieval-Augmented Generation (RAG), we propose a taxonomy that categorizes existing systems based on their architectural focus‚Äîretriever-centric, generator-centric, hybrid, and robustness-oriented designs. This classification highlights key design patterns and illustrates how different frameworks tackle the core challenges of retrieval, grounding, and reliability. 3.1 Retriever-Based RAG Systems Retriever-based Retrieval-Augmented Generation (RAG) systems delegate architectural responsibility primarily to the retriever, treating the generator as a passive decoder. These systems operate under the premise that the fidelity and relevance of the retrieved context are the most critical factors for generating accurate and grounded outputs. Innovations in this space typically fall into one of three design patterns: input-side query enhancement, retriever-side adaptation, and retrieval granularity optimization. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 4 Fig. 2. Figure 2: Taxonomy of Retrieval-Augmented Generation (RAG) Systems.This taxonomy categorizes RAG frameworks based on their primary architectural orientation‚Äîretriever-based, generator-based, hybrid, and robustness-focused designs. Retriever- based models are further grouped into query-centric, retriever-centric, and granularity-aware approaches, while generator-based models include faithfulness-aware decoding, context compression, and retrieval-guided generation. Hybrid systems are organized by retrieval dynamics (e.g., multi-round, utility-driven), and robustness-oriented models address challenges such as noise, hallucination, and adversarial vulnerabilities. This structure highlights the diverse innovations shaping the RAG landscape. Query-Driven Retrieval: A prominent strategy focuses on refining and structuring user intent before retrieval to maximize alignment with relevant corpus segments. This includes decomposition, rewriting, generative reformulation, and the incorporation of structured priors to guide retrieval. Notable examples include RQ-RAG (Refine Query for RAG) [6], which decomposes multi-hop queries into latent sub-questions, and GMR (Generative Multi-hop Retrieval) [38], which uses a generative LLM to autoregressively formulate complex multi-hop queries. RAG-Fusion [ 50] further improves recall by combining results from multiple reformulated queries through reciprocal rank fusion [13]. Structured approaches have also emerged: KRAGEN (Knowledge Retrieval Augmented Generation ENgine) [43] introduces graph-of- thoughts prompting to decompose complex queries into subproblems, retrieving relevant subgraphs to guide multi-hop reasoning. Additionally, LQR (Layered Query Retrieval) [25] implements hierarchical planning over multi-hop questions, while Sparse Context Selection [90] emphasizes efficient sparse reformulations for both recall and speed. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 5 Retriever-Centric Adaptation: Another line of work modifies the retriever itself through architectural enhance- ments or task-specific learning. Re2G (Retrieve, Rerank, Generate) [20] blends symbolic and neural retrieval via reranking layers, while SimRAG (Self-Improving RAG) [73] employs self-training over synthetic QA pairs to improve domain generalization. Frameworks like RankRAG [83] and uRAG (unified RAG) [57] emphasize retriever versatility‚Äîeither by unifying reranking and generation within a single backbone or by training general-purpose retrievers across varied downstream tasks. Additionally, systems such as ToolRerank [88] dynamically adapt retrieval strategies based on query semantics, optimizing tool selection in specialized retrieval settings. Relatedly, SEER (Self-Aligned Evidence Extraction for RAG) [87] focuses on post-retrieval adaptation, advancing corpus-based evidence extraction by aligning evidence selection with faithfulness, helpfulness, and conciseness criteria, thereby improving evidence quality for open-domain and temporally sensitive queries. Granularity-Aware Retrieval: This pattern addresses retrieval precision by optimizing the unit of retrieval‚Äîfrom full documents to fine-grained, semantically aligned segments. LongRAG [31] exemplifies this by retrieving compressed long-context chunks, constructed through document grouping, to better exploit long-context language models. Similarly, FILCO (Filter Context) [69] enhances retrieval granularity by filtering irrelevant or low-utility spans from retrieved passages before generation, improving the faithfulness and efficiency of RAG outputs. In parallel, the Sufficient Context analysis framework [34] offers a complementary lens, evaluating whether retrieved contexts contain enough information to support accurate generation, thereby highlighting the importance of granular retrieval quality for system robustness. Each of these patterns anchors its innovation in the retriever, preserving modularity and interpretability. However, they also introduce trade-offs in latency, redundancy, and sensitivity to ambiguous or underspecified queries. Sec- tion 4 elaborates on how downstream enhancements‚Äîsuch as reranking, adaptive filtering, and utility-based context selection‚Äîfurther address these limitations. 3.2 Generator-Based RAG Systems Generator-based RAG systems concentrate architectural innovation on the decoding process, assuming the retrieved content is sufficiently relevant and shifting the burden of factual grounding and integration to the language model. These systems enhance output quality through mechanisms for self-verification, compression, and controlled generation. We identify three recurring design patterns within this category: faithfulness-aware decoding, context compression and utility filtering, and retrieval-conditioned generation control. Faithfulness-Aware Decoding: To reduce hallucinations and improve factual grounding, several systems embed mechanisms for self-reflection, verification, or correction during generation. SELF-RAG (Self-Reflective RAG) [ 1] introduces a critique‚Äìgenerate loop, allowing the model to assess and revise its outputs before finalization. SelfMem [9] builds on this by incorporating a self-memory module that enables the model to revisit and refine prior generations. INFO-RAG [76] treats the LLM as a denoising module trained with contrastive objectives. Collectively, these systems decouple output faithfulness from retrieval fidelity, enabling recovery even when retrieval is suboptimal. Context Compression and Utility Filtering: To address context window limitations, several systems optimize retrieval inputs into denser or more structured forms. FiD-Light [24], a streamlined variant of the Fusion-in-Decoder (FiD) architecture [27], improves decoding efficiency by compressing encoder outputs across retrieved passages and pruning cross-passage attention without modifying retrieval mechanisms. xRAG [10] projects document embeddings directly into the model‚Äôs representation space, minimizing token overhead through modality fusion. Rich Answer Encoding (RAE) [26] enhances retrieval relevance by embedding answer-aligned semantics into retriever outputs rather than relying on token overlap. GenRT [75] further refines retrieval utility by reranking and dynamically truncating Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 6 retrieved lists, retaining only the most contextually valuable candidates for generation. Complementing these designs, an information bottleneck-based filtering approach [89] selectively preserves evidence most informative for generation. Together, these strategies advance decoding efficiency and context quality, particularly for long-form and multi-hop RAG tasks. Retrieval-Guided Generation: A third strategy modulates generation based on retrieval metadata, task-specific cues, or agentic decision-making. AU-RAG (Agent-based Universal RAG) [28] exemplifies this by using an agent to decide dynamically between retrieved and parametric knowledge across diverse data environments. RAG-Ex [ 62] perturbs retrieval context to analyze how variability influences model behavior and reliance on external evidence.R2AG (Retrieval information into RAG) [82] extends this by recursively reranking candidates during generation, dynamically prioritizing evidence based on the evolving answer state. In high-stakes domains, Confidence-Calibrated RAG [48] shows that document ordering and prompt structure affect output certainty, highlighting the need for calibration alongside factual accuracy. These architectures are particularly suited to domains where factual correctness, reasoning transparency, or structured output formats are essential‚Äîsuch as biomedical QA, finance, and enterprise workflows. While they leave the retriever fixed, many of their techniques are complementary to retrieval-side enhancements and can be layered atop other RAG variants. Section 4.2 further explores compression, reranking, and decoding control strategies in these systems. 3.3 Hybrid RAG Systems Hybrid RAG systems tightly couple the retriever and generator, moving beyond modular architectures to treat retrieval and generation as co-adaptive reasoning agents. These systems emphasize iterative feedback, utility-aware coordination, and dynamic control over retrieval actions, particularly in open-domain, multi-hop, and evolving knowledge contexts. We identify three dominant architectural patterns: iterative or multi-round retrieval, utility-driven joint optimization, and retrieval-aware generation control. Iterative or Multi-Round Retrieval: These systems interleave retrieval and generation across multiple reasoning steps, allowing for evidence refinement and progressive answer construction. IM-RAG (Inner Monologue RAG) [80] simulates an ‚Äúinner monologue‚Äù by alternating between generation and retrieval phases, supporting multi-step reasoning. Generate-Then-Ground (GenGround) [59] follows a similar philosophy, generating a provisional answer first and then retrieving supporting evidence to substantiate or revise it‚Äîimproving factuality and interpretability in multi-hop settings. G-Retriever [22] retrieves graph-structured subcomponents as generation unfolds, enhancing complex reasoning over textual graphs. Utility-Driven Joint Optimization: Several frameworks seek to align retriever outputs with their downstream utility for generation through joint objectives or reinforcement learning. Stochastic RAG [84] treats retrieval as an expected utility maximization problem, updating both retriever and generator end-to-end using REINFORCE-based gradients. M-RAG [70] applies multi-agent reinforcement learning, coordinating distributed retrievers and generators via shared memory and task-specific roles. MedGraphRAG [72] integrates knowledge graphs into the joint learning loop, facilitating domain-specific reasoning with structured priors. These systems improve factuality and answer consistency, particularly in biomedical and enterprise domains. Dynamic Retrieval Triggering: A growing class of systems dynamically controls when and how to retrieve, condi- tioned on generation uncertainty, task complexity, or intermediate outputs. DRAGIN (Dynamic Retrieval Augmented Generation based on the Information Needs of LLMs) [ 61] triggers retrieval at the token level using entropy-based confidence signals, while FLARE (Forward-Looking Active REtrieval augmented generation () [32] selectively retrieves Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 7 based on low-confidence predictions during sentence generation. SELF-ROUTE [41] dynamically routes tasks between retrieval and generation modules based on model self-assessed difficulty, and AU-RAG [28] leverages agentic decision- making to mediate between diverse retrieval sources and procedural knowledge. TA-ARE (Time-Aware Adaptive REtrieval) [86] introduces a retrieval trigger classifier that adaptively determines when retrieval is necessary and adjusts the granularity of evidence based on query needs. A related approach, CRAG (Corrective RAG) [79], evaluates retrieved evidence quality before generation and dynamically decides whether to proceed with generation, re-trigger retrieval, or decompose the input into simpler sub-queries. This corrective mechanism positions CRAG within the hybrid class, as it tightly coordinates retrieval assessment with adaptive generation pathways under uncertainty. These architectures reflect a broader trend toward treating retrieval as a controllable, contextualized act rather than a fixed preprocessing step. Their strength lies in adaptivity, coordination, and the capacity to handle under-specified or evolving queries. However, they introduce new challenges in training stability, latency, and system transparency‚Äî especially when retrieval is performed mid-decoding. These trade-offs, as well as efficiency-oriented enhancements like pipelining and reranking, are further explored in Section 4. 3.4 Robustness and Security-Oriented RAG Systems Robustness- and security-oriented RAG systems are designed to preserve output quality in the face of noisy, irrelevant, or adversarially manipulated retrieval contexts. Unlike models that optimize retrieval or generation under ideal assumptions, these systems explicitly address worst-case scenarios‚Äîsuch as hallucination under misleading evidence, retrieval failures, or corpus poisoning. We identify three major design strategies in this category: noise-adaptive training, hallucination-aware decoding constraints, and adversarial robustness. Noise-Adaptive Training Objectives: These systems aim to make RAG outputs resilient to degraded or spurious input evidence by training under perturbed, irrelevant, or misleading contexts. RAAT [18] classifies retrieved passages into relevant, irrelevant, or counterfactual categories and introduces an adversarial training objective to maximize worst- case performance. Bottleneck Noise Filtering [89] applies information bottleneck theory to identify the intersection of useful and noisy information, compressing retrieved context into minimal, high-utility representations. These approaches are particularly effective in retrieval-heavy pipelines where context precision cannot be guaranteed. Hallucination-Aware Decoding Constraints: To mitigate factual inaccuracies in generation, several systems in- troduce decoding-time constraints or architectures designed to enforce grounding. RAGTruth [46] provides benchmarks and evaluation protocols for hallucination detection, guiding system-level design. Structured retrieval-based approaches have also been explored: one method [24] retrieves executable templates (e.g., JSON workflows) to constrain output generation, minimizing reliance on generative interpolation and reducing domain-specific hallucination. RAG-Ex [62] simulates retrieval variability by injecting perturbed documents during training, improving robustness to inconsistent or adversarial context. In high-stakes domains such as healthcare, Confidence-Calibrated RAG [ 48] explores how document ordering and prompt design affect both answer accuracy and model certainty. Adversarial Robustness and Security: Emerging work also highlights new vulnerabilities. BadRAG [ 78] and TrojanRAG [8] demonstrate that adversarially poisoned passages can serve as semantic backdoors, triggering spe- cific behaviors in LLM outputs even when base models remain unmodified. These attacks rely on stealthy corpus manipulations that are hard to detect and pose significant threats in open-domain or API-exposed RAG systems. Collectively, these systems complement retrieval- and generation-oriented architectures by offering essential safety guarantees in real-world deployments. Their robustness strategies‚Äîranging from retrieval verification and context compression to constrained generation‚Äîare modular and often integrable into existing RAG pipelines. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 8 4 Enhancements in RAG Recent advancements in Retrieval-Augmented Generation (RAG) increasingly focus on targeted enhancements across the retrieval‚Äìgeneration pipeline. Beyond architectural baselines, these enhancements address key limitations in retrieval quality, context integration, computational efficiency, robustness to perturbations, and ranking precision. This section delineates five core areas of optimization‚Äîretrieval, filtering, efficiency, robustness, and reranking‚Äîeach contributing to the development of more reliable and performant RAG systems, and collectively summarized in Table 1, which compares representative methods based on their mechanisms, strengths, limitations, and ideal use cases. 4.1 Retrieval Enhancement RAG systems have increasingly adopted smarter retrieval strategies to mitigate inefficiencies such as redundant lookups, irrelevant context, and computational overhead. These improvements can be categorized into four major families: adaptive retrieval, multi-source retrieval, query refinement, and hybrid or structured retrieval. Each addresses a distinct bottleneck in the retrieval pipeline, offering trade-offs in latency, scalability, and faithfulness. Adaptive retrieval dynamically adjusts when to retrieve based on model uncertainty or predictive confidence. TA-ARE replaces static thresholds with a learned estimator, reducing redundant retrievals by 14.9% in short-form tasks. DRAGIN takes this further by applying retrieval at the token level, using entropy signals to detect knowledge gaps and triggering retrieval through a self-attentive query formulation process. Though it improves multi-hop QA precision, DRAGIN introduces notable inference costs, mitigated through adaptive frequency thresholds. FLARE proactively anticipates knowledge needs before uncertainty arises, improving faithfulness but requiring careful thresholding to avoid excessive retrieval. Multi-source retrieval targets adaptability across evolving corpora or specialized domains. AU-RAG introduces agent-based retrieval, dynamically selecting sources based on metadata heuristics. This improves domain coverage but necessitates hierarchical pipelines to manage source prioritization. SimRAG enhances retrieval precision using self-supervised learning on synthetic QA pairs, filtered via round-trip consistency. While it achieves 1.2‚Äì8.6% accuracy gains across datasets, it risks overfitting, mitigated by human-in-the-loop validation. Query refinement techniques enhance retrieval relevance by modifying ambiguous or underspecified queries. RQ-RAG uses perplexity-driven decomposition and rewriting to improve relevance, especially in multi-fact scenarios. However, this incurs inference overhead, mitigated through selective refinement based on query ambiguity. R2AG improves post-retrieval alignment by injecting retrieval metadata into prompts, bridging the retriever‚Äìgenerator semantic gap. Though effective, it adds computational cost, addressed by only enabling metadata prompting when retrieval scores fall below a relevance threshold. Hybrid and structured retrieval approaches improve coherence by integrating unstructured and structured sources. M-RAG clusters knowledge into semantic partitions, with dual agents selecting and refining content. It reduces noise but introduces latency, mitigated by dynamic partition expansion. KRAGEN retrieves subgraphs from knowledge graphs, using Graph-of-Thoughts prompting for relational reasoning. This reduces hallucinations by 20‚Äì30%, though it increases memory overhead, controlled via selective node expansion. Extending hybrid retrieval designs, the Dual-Pathway KG-RAG framework [ 74] combines structured retrieval from knowledge graphs with unstructured corpus retrieval in parallel, enhancing factual consistency and reducing hallucinations by 18% in biomedical QA tasks. Similarly, Graph RAG [16] constructs entity-centric graphs from retrieved passages and uses community summarization to scale RAG to large corpora, improving multi-hop QA recall by 6.4 Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 9 points compared to baseline retrieval. Likewise, Customer Service QA [77] integrates RAG with knowledge graphs constructed from issue-tracking tickets, achieving a 77.6% improvement in retrieval MRR and a 28.6% reduction in resolution time when deployed at LinkedIn‚Äôs customer service team. In a complementary direction, Doan et al. [ 15] propose a lightweight hybrid retrieval strategy that combines unstructured text embeddings with structured knowledge graph embeddings without requiring complex retriever re-training, achieving up to 13.1% improvements in retrieval correctness and ranking precision in domain-specific RAG deployments. 4.2 Enhancing Context Relevance through Filtering Despite advances in retrieval models, RAG systems often integrate irrelevant, redundant, or semantically noisy docu- ments that degrade generation quality. Filtering techniques aim to reduce hallucinations and improve answer relevance by selecting only contextually appropriate content. These methods vary in supervision, granularity, and efficiency, and can be categorized into three groups: lexical/statistical filters, information-theoretic optimizers, and self-supervised passage scoring. Lexical filters such as FILCO apply word overlap and statistical relevance scoring. Using STRINC and CXMI metrics, FILCO removes low-relevance passages and reduces hallucinations by up to 64%, improving EM by +8.6. However, its reliance on lexical similarity limits its adaptability across domains and query styles. Information-theoretic methods like IB Filtering [89] use principles from the information bottleneck framework to retain only high-utility input features while discarding noise. Though computation-heavy, IB Filtering improves EM by +3.2 with a 2.5% compression ratio, offering a balance between precision and conciseness. Similarly, Stochastic Filtering models retrieval as an expected utility maximization problem and re-ranks passages based on marginal value, achieving consistent retrieval effectiveness gains with minimal retriever changes. Self-supervised methods like SEER and RAG-Ex use internal feedback signals to filter noisy retrievals. SEER applies label-free training and generates pseudo-relevance judgments, improving F1 by 13.5% and achieving a 9.25√ó reduction in context length. RAG-Ex perturbs retrieved passages and compares generation outcomes, selecting those that maximize semantic consistency. It aligns with human-assessed faithfulness 76.9% of the time and is model-agnostic, though computationally expensive due to multiple inference passes. Collectively, these methods balance retrieval compression, answer faithfulness, and domain adaptability. While lexical filters are efficient, self-supervised models provide deeper semantic filtering and support long-form reasoning. 4.3 Efficiency Enhancements While Retrieval-Augmented Generation (RAG) significantly enhances factual consistency in large language models (LLMs) by integrating external document retrieval, it introduces new inefficiencies. These include increased memory overhead, latency from retrieval-processing pipelines, and redundancy in passage selection. This section synthesizes key research efforts aimed at improving retrieval efficiency across four areas: sparse retrieval and context selection, inference acceleration, caching and redundancy reduction, and retrieval faithfulness. Sparse context selection and retrieval-aware generation techniques aim to reduce the input length and improve semantic alignment without sacrificing output quality. Sparse RAG addresses this by filtering low-relevance content before self-attention, retaining only high-signal tokens via parallel encoding. While it builds on Fusion-in-Decoder (FiD), it improves efficiency by avoiding dense input concatenation. However, it may discard useful context under suboptimal retrieval, requiring fine-tuning to maintain robustness. R2AG takes a complementary approach by embedding retrieval Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 10 representations directly into the LLM‚Äôs context space, enhancing semantic alignment. Unlike prompt-based methods (e.g., REPLUG [58]), R2AG bypasses explicit concatenation, reducing redundant processing. Both approaches enhance efficiency at different stages but require retriever fine-tuning and increase model complexity. Inference acceleration strategies focus on reducing decoding latency in autoregressive models by minimizing redundant token processing. FiD-Light achieves this through token-level passage compression, which lowers decoding time while preserving key information. Though effective, aggressive filtering can marginally reduce retrieval precision. Speculative Pipelining [71] further reduces latency by overlapping retrieval and generation. It incrementally processes top-ùëò candidates before retrieval completes, lowering time-to-first-token (TTFT) by 20‚Äì30%. However, it risks speculative hallucinations unless controlled by fallback mechanisms and selective decoding checkpoints. This line of work opens the door for future speculative decoding architectures‚Äîdiscussed in Section 8‚Äîthat balance responsiveness and reliability in low-latency applications. Caching and redundancy reduction techniques aim to eliminate recomputation overhead in repetitive or high- throughput workloads. RAGCache [33] introduces a hierarchical caching system that stores key-value tensors from prior retrievals. PGDSF extends this with prefix-aware eviction that prioritizes frequent and important documents. While these methods significantly improve efficiency in common-query settings, their impact diminishes on long-tail distributions and introduces cache complexity. Retrieval faithfulness and answer relevance methods go beyond lexical similarity to ensure that retrieved documents are factually aligned with the generated output. Rich Answer Encoding (RAE) addresses this using a Retriever-as-Answer Classifier (RAC) and Dense Knowledge Similarity (DKS), which rescore documents based on their plausibility. RAE reduces hallucinations and improves grounding but requires retriever retraining, increasing cost. Taken together, these optimization strategies enhance efficiency across the RAG pipeline: Sparse RAG andR2AG improve alignment between retrieved documents and generation; FiD-Light and Speculative Pipelining reduce latency during inference; RAGCache and PGDSF minimize recomputation in high-throughput environments; and RAE advances retrieval faithfulness. Collectively, they represent a move toward more scalable, accurate, and computationally efficient RAG systems. 4.4 Enhancing Robustness RAG systems improve factual accuracy in language models by retrieving external information. However, they remain vulnerable to retrieval noise, hallucinations, and adversarial attacks. While past research has addressed these challenges separately‚Äîsuch as noise resilience, hallucination control, and retrieval security‚Äîa unified perspective is essential. This section groups robustness techniques into three areas: noise mitigation, hallucination reduction, and security defenses. Empirical studies further support this need for a unified view; a recent study identifies seven recurrent failure points in operational RAG systems, spanning retrieval errors, context consolidation failures, hallucinated outputs, and incomplete answers [4]. Noise mitigation strategies target irrelevant, misleading, or adversarial content that can degrade RAG accuracy. However, recent work challenges the assumption that all retrieval noise is detrimental; Cuconasu et al. [14] demonstrate that carefully positioned random documents can paradoxically improve LLM reasoning and answer quality by promoting evidence selection behaviors. Two contrasting approaches address this: RAAT and CRAG. RAAT uses adversarial pretraining to expose models to subtle and counterfactual retrieval noise, improving F1/EM scores by 20‚Äì30%. Its high training cost limits it to static, high-stakes domains. CRAG filters low-confidence retrievals at inference time and Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 11 works well in real-time systems, reducing retrieval errors by 12‚Äì18%. However, it struggles with ‚Äúsoft noise, ‚Äù where superficially relevant content misleads the model. Hallucination reduction techniques such as Structured RAG [2] and IM-RAG aim to improve the faithfulness of generated content. Structured RAG constrains retrieval to verified corpora, lowering hallucination rates by 30‚Äì40% with minimal compute cost. Its drawback is poor adaptability, requiring manual updates. IM-RAG uses iterative retrieval refinement, achieving +5.3 F1 / +7.2 EM on HotPotQA. Though more accurate in evolving domains, it is computationally intensive and slower at inference. Security defenses focus on adversarial threats such as data poisoning and backdoor attacks. Research into BadRAG shows that poisoning just 0.04% of a corpus can lead to a 98.2% attack success rate and 74.6% system failure. Defenses like cryptographic document signing or adversarial filtering are only partially effective. TrojanRAG embeds backdoors in retrieval embeddings, bypassing traditional sanitization. Stronger mitigations‚Äîsecure training and integrity validation‚Äî are needed but require proactive design. Beyond adversarial attacks, privacy vulnerabilities in RAG systems have also been identified; Zeng et al. [85] show that both retrieval databases and pretraining corpora can be exploited through structured prompting, although retrieval can paradoxically help reduce memorization leakage by acting as a grounding mechanism. 4.5 Enhancements and Optimizations in Reranking Reranking plays a vital role in improving the relevance and faithfulness of Retrieval-Augmented Generation (RAG) outputs. While initial retrieval stages often return noisy results, reranking refines document ordering before passage selection and generation, reducing hallucinations and improving response accuracy. Recent work advances reranking across three key areas: adaptive reranking, unified pipelines, and fusion-based reranking. Adaptive reranking methods dynamically adjust the number of documents reranked based on query complexity. RLT [44] uses ranked list truncation to improve MRR/nDCG while reducing retrieval noise by 15%. ToolRerank further adapts reranking depth based on familiarity with seen vs. unseen tools, boosting recall by 12% in hierarchical retrieval tasks. These methods optimize computation by avoiding unnecessary reranking in low-complexity scenarios. Unified reranking pipelines combine retrieval, document ranking, and generation within a single architecture. RankRAG fine-tunes a language model to jointly score documents and generate answers, improving MRR@10 by 7.8% while reducing latency. uRAG extends this to multiple tasks‚Äîlike QA and fact verification‚Äîusing shared reranking logic and user-feedback signals, improving cross-task generalization by 8% MRR@10. These approaches eliminate the overhead of separate ranking modules and increase retrieval consistency. Fusion-based reranking strategies aggregate evidence from multiple query variants to improve answer robustness. RAG-Fusion generates multiple subqueries and applies reciprocal rank fusion, improving answer accuracy by 9%.R2AG refines these rankings iteratively, reducing irrelevant retrievals by 15% through recursive feedback. These models are especially effective for multi-hop and ambiguous tasks. Reranking methods significantly boost the efficiency and faithfulness of RAG systems. Future work may explore hybrid approaches combining adaptive truncation with fusion-based aggregation, as well as domain-adaptive reranking for enterprise scalability. As RAG expands to more tasks and domains, reranking will remain essential to enabling context-aware, trustworthy generation. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 12 Table 1. Summary of RAG System Enhancements. This table categorizes enhancements across five dimensions‚Äîretrieval, filtering, efficiency, robustness, and reranking. Each entry specifies the enhancement type, method, mechanism, key strengths, known limitations, and ideal use cases. Enhancement Type Category Method Mechanism Strengths Limitations Best Use Case Retrieval Adaptive TA-ARE Dynamic confidence estima- tion Reduces redundant retrieval Estimator latency Short-form QA Adaptive DRAGIN Token-level entropy-based trig- gers Improves multi-hop QA preci- sion High inference cost Multi-hop QA Adaptive FLARE Preemptive uncertainty detec- tion Enhances faithfulness Risk of over-retrieval Long-form generation Multi-source AU-RAG Agent-based source selection High domain adaptability Source management overhead Evolving corpora Multi-source SimRAG Synthetic QA + round-trip fil- tering Cross-domain accuracy gains Overfitting risk Specialized domains Query RQ-RAG Perplexity-based query rewrit- ing Improves query clarity and rel- evance Additional inference steps Multi-fact queries Query R 2AG Retrieval-aware prompt injec- tion Enhances factual grounding Prompt expansion overhead Low-confidence queries Hybrid M-RAG Semantic partitioning + dual agents Reduces retrieval noise Partition latency Context-heavy reasoning Hybrid KRAGEN Knowledge graph subgraph re- trieval Improves structured reasoningMemory and compute inten- sive Biomedical, graph-based tasks Filtering Lexical FILCO STRINC + CXMI scoring +8.6 EM, 64% hallucination re- duction Query-style bias Structured QA Info-Theoretic IB Filtering Bottleneck-based compression +3.2 EM, 2.5% compression Computation overhead High-precision QA Info-TheoreticStochastic Filtering Utility-maximizing re-ranking Improves effectiveness Needs custom scoring Lightweight retrieval tasks Self-Supervised SEER Pseudo-relevance via self- training +13.5% F1, 9.25x context reduc- tion High training cost Open-domain QA Self-Supervised RAG-Ex Generation perturbation com- parison 76.9% human-aligned faithful- ness Multiple inference passes Faithful generation Efficiency Sparse Selection Sparse RAG Retains high-signal tokensReduces memory, improves rel- evance May discard useful docs Long-context tasks Sparse Selection R2AG Context-aware retrieval injec- tion Enhances coherence, lowers re- dundancy Retriever fine-tuning needed Knowledge-intensive QA Inference Acceler- ation FiD-Light Compresses passages Faster decoding Slight loss in recall Low-latency applications Caching Speculative Pipelining Overlaps retrieval and genera- tion 20‚Äì50% TTFT reduction Risk of hallucination Real-time applications Caching RAGCache Hierarchical cache w/ PGDSF Eliminates recomputation Cache complexity in long-tail High-throughput workloads Retrieval Quality RAE Retriever-as-answer scorerBoosts grounding and preci- sion Requires scoring/retraining Factual QA Robustness Noise Mitigation RAAT Adversarial training +20‚Äì30% F1/EM High training cost Offline pretraining Noise Mitigation CRAG Inference-time filtering +12‚Äì18% precision gain Ineffective on ‚Äúsoft‚Äù noise Real-time support Hallucination Control Structured RAG Curated corpus retrieval30‚Äì40% hallucination reductionLow adaptability Static domains Hallucination Control IM-RAG Iterative retrieval refinement +5.3 F1 / +7.2 EM Inference latency Multi-hop QA Security BadRAG Adversarial retrieval poisoningDemonstrates corpus-level threat Needs stronger filtering Security evaluation Security TrojanRAG Embedding-level backdoor Persistent attack vector Requires secure training Security-sensitive pipelines Reranking Adaptive RLT Dynamic list truncation +15% noise reduction Heuristic tuning needed Real-time QA Adaptive ToolRerank Familiarity-aware reranking +12% recall for unseen toolsComplexity for un- seen/frequent tools Tool-aware retrieval Unified Pipeline RankRAG Joint rerank + generate +7.8% MRR@10 Domain-specific tuning End-to-end QA systems Unified Pipeline uRAG Shared reranking engine +8% MRR@10, task generaliza- tion Higher setup cost Multi-task enterprise RAG Fusion-based RAG-Fusion Reciprocal rank fusion +9% accuracy Query explosion risk Complex multi-hop QA Fusion-based R2AG Recursive reranking refine- ment 15% irrelevant retrieval reduc- tion Higher latency Iterative reasoning Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 13 Table 2. Comparative Performance of Retrieval-Augmented Generation Frameworks Across Multi-Hop and Short-Form QA Benchmarks. This table reports relative performance improvements achieved by each RAG framework over two baselines: (i) the raw backbone language model (B) and (ii) the same model augmented with a standard retrieval module (B+R). Results are shown across multi-hop benchmarks (HotpotQA [81], 2Wiki [23], MuSiQue [67]) and short-form QA datasets (PopQA [42], TriviaQA [35], ARC-Challenge [12], NQ [ 37]), with metrics including F1, Exact Match (EM), and Accuracy (Acc). Frameworks are grouped by architectural category: retriever-based, generator-based, and hybrid. A ‚Äú‚Äì‚Äù indicates that the corresponding score was not reported in the original publication. Backbone LLMs referenced in this table include LLaMA 2 [66], LLaMA 3 [21], GPT-3.5/4 [47], Vicuna [11], Mistral [29], Mixtral [30], Gemini [53], and Gemma [64]. Framework Backbone HotpotQA 2Wiki MusiQue PopQA TriviaQA ARC-Challenge NQ B/B+R B/B+R B/B+R B/B+R B/B+R B/B+R B/B+R Retriever-Based RAG RQ-RAG LLaMA2-7B 8.485/2.749 (F1) 1.8/1.396 (F1) 12.9/4.635 (F1) 2.884/0.434 (Acc) -/- 2.133/1.379 (Acc) -/- SimRAG LLaMA3-8B -/- -/- -/- -/- -/- 0.145/- (Acc) -/- SimRAG Gemma2-27B -/- -/- -/- -/- -/- 0.034/- (Acc) -/- SEER LLaMA2-7B-Chat 0.104/0.037 (F1) -/- -/- -/- -/- -/- -/- RankRAG LLaMA3-8B -/0.079 (F1) -/0.323 (F1) -/- -/- -/- -/- -/- RankRAG LLaMA3-70B -/0.242 (F1) -/0.376 (F1) -/- -/- -/- -/- -/- LQR LLaMA3-8B 2.081/0.516 (F1) 0.706/0.141 (F1) 2.922/0.841 (F1) -/- -/- -/- -/- LongRAG GPT-4o 0.517/- (EM) -/- -/- -/- -/- -/- -/- LongRAG Gemini-1.5-Pro 0.696/- (EM) -/- -/- -/- -/- -/- -/- FILCO LLaMA2-7B -/0.057 (EM) -/- -/- -/- -/0.056 (EM) -/- -/0.298 (EM) Re2G BART Large -/- -/- -/- -/- 0.251/- (Acc) -/- 0.144/- Generator-Based RAG xRAG Mistral-7B 0.26/-0.122 (EM) -/- -/- -/- 0.152/-0.002 (EM) -/- 0.293/-0.085 (EM) xRAG Mixtral-8x7B 0.207/-0.087 (EM) -/- -/- -/- 0.043/0.054 (EM) -/- 0.126/0.047 (EM) INFO-RAG LLaMA2-7B 0.182/- (EM) -/- 0.163/- (EM) -/- -/- -/- -/- INFO-RAG LLaMA2-13B 0.222/- (EM) -/- 0.358/- (EM) -/- -/- -/- -/- INFO-RAG LLaMA2-13B-chat 0.011/- (EM) -/- 0.018/- (EM) -/- -/- -/- -/- SELF-RAG LLaMA2-7B -/- -/- -/- 2.735/0.437 (Acc) 1.177/0.562 (Acc) 2.092/0.404 (Acc) -0.505/- (Acc) SELF-RAG LLaMA2-13B -/- -/- -/- 2.796/0.221 (Acc) 0.8/0.474 (Acc) -/- -/- FiD-Light FiD+DPR -/- -/- -/- -/- 0.185/- (EM) -/- 0.27/- (EM) R2AG LLaMA2-7B 3.231/- (F1) 34.52/4.445 (F1) -/- -/- -/- -/- 0.824/- (Acc) Hybrid RAG DRAGIN LLaMA2-7B-chat 0.218/0.338 (F1) 0.311/0.148 (F1) -/- -/- -/- -/- -/- DRAGIN LLaMA2-13B-chat 0.368/0.144 (F1) 0.445/0.169 (F1) -/- -/- -/- -/- -/- DRAGIN Vicuna-13B-v1.5 0.279/0.179 (F1) 0.575/0.371 (F1) -/- -/- -/- -/- -/- FLAREdirect GPT-3.5 -/- 0.622/0.223 (F1) -/- -/- -/- -/- -/- FLAREinstruct GPT-3.5 -/- 0.353/0.02 (F1) -/- -/- -/- -/- -/- GenGround GPT-3.5 0.236/0.093 (F1) 0.219/0.122 (F1) 0.359/0.361 (F1) -/- -/- -/- -/- Stochastic RAG FiD-Light (T5-Base) 0.066/- (F1) -/- -/- -/- -/0.036 (EM) -/- -/0.013 (EM) Stochastic RAG FiD-Light (T5-XL) 0.065/- (F1) -/- -/- -/- -/0.016 (EM) -/- -/0.037 (EM) CRAG LLaMA2-7B -/- -/- -/- 3.034/0.471 (Acc) -/- 1.514/0.173 (Acc) 0.045/- (Acc) Self-CRAG LLaMA2-7B -/- -/- -/- 3.204/0.533 (Acc) -/- 2.083/0.439 (Acc) -/- TA-ARE GPT-3.5 -/- -/- -/- -/- -/- -/- -/- TA-ARE GPT-4 -/- -/- -/- -/- -/- -/- -/- TA-ARE LLaMA2-7B -/- -/- -/- -/- -/- -/- -/- 5 Comparative Analysis To assess the empirical effectiveness of design innovations in Retrieval-Augmented Generation (RAG), this section presents a comparative analysis of representative frameworks across three key evaluation settings: short-form question answering, multi-hop reasoning, and robustness under retrieval perturbations. Results are reported as relative improve- ments over both raw and retrieval-augmented baselines, normalized for model and dataset variability. Additionally, we review ablation studies from the literature to disentangle the contributions of specific components such as retrieval triggers, filtering layers, reranking mechanisms, and robustness modules. These insights offer a clearer understand- ing of which enhancements most significantly impact performance, faithfulness, and efficiency across diverse RAG configurations. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 14 5.1 Comparative Analysis of Framework Performance on Short-Form QA This section presents a comparative analysis of Retrieval-Augmented Generation (RAG) frameworks in short-form question answering, emphasizing their relative improvements over raw large language model (LLM) baselines and retrieval-augmented baselines. As shown in Table 2, these comparisons focus on relative gains (e.g., a value of 2.7 indicates a 270% improvement) rather than absolute performance metrics, which normalize for variations in backbone architectures, prompting strategies, and evaluation protocols. This approach enables a meaningful comparison across diverse experimental setups. Among generator-based RAG systems primarily optimized for accuracy, SELF-RAG consistently demonstrates substantial gains across multiple datasets. It achieves over a 270% improvement from the raw LLM baseline on PopQA [42] and over 200% on ARC-Challenge [12], illustrating the effectiveness of deep context integration for enhancing short- form factual recall. FiD-Light, although also a generator-side enhancement, adopts a different optimization philosophy centered on lightweight, efficient fusion of retrieved documents during decoding, yielding more moderate improvements of 18‚Äì27% across TriviaQA [35] and NQ [37]. R2AG, another generator-based approach, shows promising gains, with over 80% improvement from the baseline on NQ, further validating the benefits of integrating retrieval signals within generation. We note that generator-based frameworks primarily designed for efficiency, such as xRAG, are discussed separately due to their distinct optimization focus. Retriever-based frameworks such as RQ-RAG and SimRAG also demonstrate notable gains. RQ-RAG achieves a 288% improvement on PopQA and over 210% on ARC-Challenge, reaffirming the importance of retrieval quality in evidence-centric QA. SimRAG also shows strong improvements on ARC, although gains are more modest (approximately 14%). Additionally, retriever-side re-ranking approaches like FILCO deliver moderate but meaningful gains, with 5‚Äì30% improvements across NQ and TriviaQA, further highlighting the incremental value of retrieval refinement strategies. Hybrid frameworks exhibit a more heterogeneous pattern. CRAG and Self-CRAG achieve impressive gains, with Self-CRAG delivering a 320% improvement on PopQA and a 208% improvement on ARC-Challenge, suggesting that combining retrieval refinement with generation adaptation can be highly effective when well aligned. However, TA-ARE, despite achieving a significant 28√ó improvement over raw baselines on RetrievalQA, occasionally underperforms relative to the standard retrieval baseline, indicating that retrieval frequency reduction strategies, while efficient, may introduce trade-offs. Stochastic RAG frameworks, meanwhile, display relatively modest gains (typically under 4%), reflecting that introducing retrieval randomness increases diversity without consistently boosting short-form QA accuracy. Efficiency-focused generator-based systems such as xRAG exhibit mixed results. While xRAG achieves 10‚Äì29% improvements over raw LLM baselines on datasets such as NQ and TriviaQA, its gains over retrieval baselines are marginal or occasionally negative. This suggests that while resource-efficient designs are promising for scaling RAG systems, further optimization is needed to maintain competitive factual accuracy in short-form tasks. Finally, robustness-oriented frameworks such as RAAT demonstrate strong performance, with a 116% improvement from the raw baseline and over 27% gain compared to retrieval on RAG-Bench‚Äîa robustness-focused variant of NQ, WebQ, and TriviaQA. Although evaluated under challenging retrieval noise conditions, RAAT‚Äôs results suggest that robustness-driven retrieval strategies can effectively complement factual QA objectives. Overall, retrieval- and generation-enhanced frameworks deliver substantial relative gains in short-form QA, while hybrid and efficiency-focused approaches offer promising but variable results depending on dataset and retrieval complexity. These findings underscore the critical role of retrieval optimization and generation-adaptive strategies in advancing retrieval-augmented short-form question answering. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 15 5.2 Comparative Analysis of Framework Performance on Multi-Hop QA A comparative evaluation of various Retrieval-Augmented Generation (RAG) frameworks reveals distinct patterns in their ability to enhance multi-hop question answering, assessed through improvements over both raw large language models (LLMs) and standard retrieval-augmented baselines. Similar to the previous section, this analysis focuses on relative gains rather than absolute scores to normalize for architectural and experimental variations. The results, summarized in Table 2, enable a consistent comparison of framework contributions across diverse multi-hop QA settings. Among retrieval-based RAG systems, models such as RQ-RAG, RankRAG, LQR, and LongRAG demonstrate substantial relative gains. Notably, RQ-RAG achieves over an 800% improvement from its raw LLM baseline on HotpotQA [81], and a 275% improvement over standard retrieval, highlighting the effectiveness of sophisticated query decomposition techniques in multi-hop settings. Similarly, LQR achieves a 292% improvement from the raw baseline and an 84% improvement over retrieval in the MuSiQue dataset [67], suggesting that intelligent retrieval-ranking substantially boosts multi-hop reasoning. LongRAG also exhibits strong performance, improving by over 50% from the raw LLM baseline on HotpotQA, further emphasizing the value of extended retrieval for complex question answering. These patterns collectively affirm that optimizing retrieval quality remains a dominant driver of performance gains in multi-hop RAG applications. Generator-based RAG frameworks, including R2AG, INFO-RAG, and xRAG, display more varied relative improve- ments. R2AG shows consistent strong gains, improving by over 300% relative to the baseline on HotpotQA, demonstrating the benefits of tightly integrating retrieval signals into the generation process. In contrast, INFO-RAG exhibits more modest improvements, with relative gains around 16‚Äì35% across different backbones and datasets, suggesting that while generator-side augmentations enhance output faithfulness, their standalone effect may be limited without concurrent retrieval refinement. xRAG, while improving from raw baselines by approximately 20‚Äì26%, shows negative or marginal gains compared to the retrieval baseline in some settings, indicating that extreme context compression, although efficient, may compromise the model‚Äôs ability to utilize retrieved evidence effectively for complex multi-hop reasoning. Hybrid RAG frameworks, such as DRAGIN, FLARE, GenGround, and Stochastic RAG, present a diverse range of outcomes. DRAGIN frameworks achieve moderate improvements, typically ranging between 22‚Äì44% over raw LLMs and 14‚Äì34% over retrieval baselines, reflecting the incremental gains from dynamically adapting retrieval to evolving information needs. FLAREdirect stands out, achieving a 62% improvement from the raw LLM and a 22% improvement over standard retrieval on 2Wiki [23], suggesting that model-guided active retrieval significantly strengthens multi-hop evidence gathering. GenGround reports relatively smaller improvements (13‚Äì36% from the baseline) but is evaluated against already-strong baselines, which partially accounts for the more conservative gains. Stochastic RAG frameworks offer consistent yet modest gains (6%), indicating that introducing randomness into retrieval can modestly diversify and enhance evidence coverage without destabilizing performance. Overall, retrieval-based RAG frameworks demonstrate the most consistent and substantial improvements across multi-hop QA tasks, particularly when retrieval quality, ranking, and query decomposition are optimized. Generator- based adaptations, while beneficial in specific cases, often require complementary retrieval-side enhancements to realize their full potential. Hybrid frameworks offer promising but more variable results, underscoring the challenge of harmonizing retrieval and generation strategies dynamically. These findings highlight retrieval optimization as a critical lever for advancing complex reasoning capabilities in RAG systems. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 16 5.3 Comparative Robustness Analysis: Framework Gains Over Retrieval-Only Baselines To assess robustness in Retrieval-Augmented Generation (RAG) systems, we report incremental improvements each framework achieves over its retrieval-augmented LLM baseline. This isolates the added value of mechanisms such as critique, reranking, and filtering, independent of the baseline retrieval gain. Evaluations span multiple datasets and focus on gains in precision, recall, and FactScore. By standardizing on relative improvements, the analysis enables fair comparisons across models with differing backbone architectures. A summary of these results is provided in Table 3. Among hybrid systems, the most substantial gains in factual consistency are observed. Self-CRAG yields the highest FactScore improvement‚Äî+0.456 on the Biography dataset [ 45]‚Äîsignificantly surpassing other frameworks, most of which report ‚â§0.05 gains. The multi-sentence compositional nature of the Biography task likely benefits from Self- CRAG‚Äôs feedback-based reranking and correction loop, which aligns generation with retrieved evidence. Comparable improvements are evident with Self-RAG and CRAG, reporting +0.372 and +0.252 gains on the same dataset, underscoring the importance of evidence-aware generation refinement. On 2Wiki, Flare-Direct improves both precision and recall by +21.6%, while Flare-Instruct‚Äîdespite using the same retrieval backbone‚Äîoffers negligible gains, illustrating how prompt design alone can meaningfully impact robustness in multi-hop settings. In contrast, Stochastic RAG shows only marginal FactScore gains (‚â§+0.008) on Fever, suggesting that entropy-driven retrieval without subsequent verification may be insufficient to ensure factual reliability. Generator-based systems present more variable, task-dependent performance. SELF-RAG, evaluated on ASQA [60], achieves sizable improvements in precision (+22‚Äì30%) and recall (+16‚Äì19%), though its FactScore gains remain modest (+0.03‚Äì0.04), implying improved evidence usage without equivalent advances in factual accuracy. DRAGIN similarly improves precision and recall by +9‚Äì22% on HotPotQA, leveraging entropy-based token-level triggers suited for multi- hop reasoning. However, lacking reported FactScore, its contribution to factual consistency remains indeterminate. Other generator-oriented systems, including GenRT and Rich Answer Encoding, achieve smaller recall gains (‚â§+0.1) on datasets such as TriviaQA, KILT-WoW [49], and MSMARCO [3]. These modest improvements suggest better document selection but limited post-retrieval validation, constraining their robustness impact. Retriever-based systems exhibit consistent yet comparatively modest gains. Re2G reports +17.8% precision and +15.9% recall on TriviaQA, reflecting the benefits of retrieval-aware prompt optimization. FILCO, by contrast, improves precision by +3.25% on Fever but fails to enhance recall or FactScore, indicating that filtering irrelevant context improves selectivity, but without downstream verification, its robustness contribution is limited. Not all frameworks report all three metrics across datasets; while relative improvement facilitates normalization, incomplete coverage‚Äîparticularly of FactScore‚Äîmay obscure the full extent of a system‚Äôs capabilities. In sum, Self-CRAG on Biography delivers the strongest FactScore gain (+0.456), while SELF-RAG on ASQA achieves the best precision (+29.56%) and recall (+18.81%) improvements. Flare-Direct, outperforming Flare-Instruct by over 20% on 2Wiki, highlights the sensitivity of robustness to prompt design. At the lower end, Stochastic RAG on FEVER [65] records the smallest impact ( ‚â§+0.008 FactScore), reinforcing the necessity of combining retrieval strategies with downstream verification to enhance factual fidelity. Collectively, these findings affirm that retrieval alone is insufficient for robust generation. The most effective frameworks tightly couple retrieval, generation, and verification in iterative loops, ensuring that generation is guided by critique and alignment rather than treated as a terminal step. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 17 Table 3. Comparative Robustness Analysis of RAG Frameworks Across Architectures.Relative improvements in precision, recall, and FactScore over retrieval-augmented baselines across multiple datasets. A dash (‚Äì) denotes missing values in the original paper. Taxonomy Framework Backbone Dataset Precision Recall FactScore Retriever-based RAG Re2G KGI0 NQ 0.096984 0.074569 ‚Äì Re2G KGI1 TriviaQA 0.177981 0.159062 ‚Äì Re2G KGI2 Fever 0.120986 0.073732 ‚Äì FILCO RAG Fever 3.25 ‚Äì ‚Äì Generation-based RAG SELF-RAG LLaMA2-7B ASQA 22.06897 15.95 0.041026 SELF-RAG LLaMA2-7B ASQA 29.56522 18.80556 0.034839 Rich Answer Encoding RAG MSMARCO ‚Äì 0.086957 ‚Äì Rich Answer Encoding RAG KILT-WoW ‚Äì 0.107293 ‚Äì DRAGIN LLaMA2-13B HotPotQA 0.185934 0.09893 ‚Äì DRAGIN VICUNA-13B HotPotQA 0.222447 0.105114 ‚Äì GenRT RAG NQ ‚Äì 0.023232 ‚Äì GenRT RAG TriviaQA ‚Äì 0.026239 ‚Äì Hybrid RAG CRAG LLaMA2-7B Biography ‚Äì ‚Äì 0.251689 Self-CRAG LLaMA2-7B Biography ‚Äì ‚Äì 0.456081 Flare-Instruct GPT-3.5 2Wiki 0.010288 0.019417 ‚Äì Flare-Direct GPT-3.5 2Wiki 0.216049 0.215534 ‚Äì Stochastic RAG FiD-Light (T5-Base) Fever ‚Äì ‚Äì 0.008685 Stochastic RAG FiD-Light (T5-XL) Fever ‚Äì ‚Äì 0.00355 5.4 Ablation Studies Ablation studies serve as a crucial methodological lens for disentangling the contributions of individual components in Retrieval-Augmented Generation (RAG) frameworks. Across the surveyed literature, these studies primarily target retrieval triggers, filtering layers, reranking strategies, compression modules, and corrective mechanisms, offering empirical insights into performance, efficiency, and robustness. Adaptive Retrieval and Query Reformulation. Frameworks such as TA-ARE, FLARE, and IM-RAG demonstrate that retrieval adaptivity is central to long-form and multi-hop reasoning. Ablating dynamic query triggers (e.g., forward- looking or self-reflective prompts) consistently results in degraded factual accuracy and increased hallucinations, confirming the value of retrieval-awareness throughout the generation process. Filtering, Reranking, and Evidence Quality. Systems like SEER, CRAG, and Re2G show that context filtering, reranking, and correction layers significantly influence downstream performance. Ablations reveal that removing context evaluators or decomposing mechanisms leads to verbosity and reduced grounding fidelity. Notably, reranking- truncation co-designs (e.g., in GenRT and ToolRerank) outperform static top- ùëò approaches by improving answer faithfulness and retrieval precision. Compression and Efficiency Trade-offs. FiD-Light and RAGCache demonstrate that passage compression and caching can substantially reduce latency without compromising accuracy. Ablating vector sparsity or caching mecha- nisms (e.g., speculative pipelining or prefix-aware replacement) increases inference time up to 4√ó, underscoring the operational significance of architectural optimization in production RAG systems. Robustness and Security. Studies like BadRAG and TrojanRAG emphasize that security-focused ablations reveal novel vulnerabilities. Even lightweight retrieval poisoning or trigger crafting can steer model outputs, while mitigation strategies (e.g., summarization, distance thresholds) offer partial resilience but require further study. Synthesis. Ablation studies consistently reinforce that high-performing RAG frameworks are modular, with com- plementary retrieval, filtering, and generation components. Performance degradation in ablation settings not only validates novel modules but also guides design toward more interpretable, efficient, and secure RAG pipelines. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 18 6 Evaluation and Benchmarking of RAG Systems Retrieval-Augmented Generation (RAG) systems introduce unique challenges for evaluation due to their hybrid architecture combining a retriever and a generator. Accurate evaluation demands assessing multiple interdependent components, including retrieval relevance, faithfulness of generated responses, and overall answer utility. In this section, we synthesize recent advancements in automated evaluation frameworks, retrieval quality assessment techniques, and benchmark construction to provide a comprehensive overview of evaluation practices in RAG systems. 6.1 Evaluation Dimensions The core dimensions [55] used to evaluate RAG systems include: (1) Context Relevance: Measures how pertinent the retrieved documents are to the input query. (2) Answer Faithfulness: Assesses whether the generated output remains grounded in the retrieved evidence. (3) Answer Relevance: Evaluates whether the output adequately addresses the user query. These dimensions are interdependent: poor context relevance often cascades into reduced faithfulness and answer relevance, underscoring the need for joint evaluation. Frameworks such as ARES and RAGAS have formalized these dimensions, incorporating both automated judgment and reference-free evaluation. 6.2 Automated Evaluation Frameworks ARES [55] introduces an LLM-based judge system that uses few-shot prompted language models to generate synthetic datasets. These judges are trained on three classification tasks corresponding to the core dimensions and use prediction- powered inference (PPI) to align model-based scoring with human judgment. ARES shows significant improvements in accuracy and annotation efficiency, outperforming RAGAS [17] by up to 59.3 percentage points in context relevance. RAGAS employs a modular framework that decomposes generated answers into atomic factual statements, then evaluates each against the retrieved context using LLMs. This structure provides high-resolution feedback, revealing which parts of an answer are hallucinated. These frameworks automate the evaluation of faithfulness, grounding, and contextual relevance‚Äîenabling scalable, reference-free analysis of RAG performance. 6.3 Evaluating Retrieval Quality eRAG [56] challenges traditional relevance label techniques by applying the RAG generator to each retrieved document individually. The performance of each document, assessed via downstream task metrics, serves as a relevance label. This method provides a retrieval-aware, document-level granularity and has shown significantly improved correlation with actual RAG performance. INFO-RAG introduces an unsupervised training paradigm that improves the LLM‚Äôs ability to refine retrieved information under three scenarios: redundant, noisy, or insufficient context. By viewing the LLM as an ‚Äúinformation refiner, ‚Äù it enables the model to extract relevant content, reject misinformation, and infer missing details‚Äîenhancing retrieval robustness without supervised relevance labels. uRAG proposes a unified retrieval system that serves multiple RAG models across diverse downstream tasks. It introduces a shared reranker trained on feedback signals (e.g., EM, accuracy) from various black-box LLMs, treating each LLM as a user of the search engine. uRAG‚Äôs training protocol enables evaluation and optimization of retrieval Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 19 based on downstream task performance, offering retrieval diagnostics grounded in actual utility rather than surface similarity. 6.4 Benchmarking RAG Capabilities As RAG systems mature, a growing suite of benchmarks has emerged to evaluate them across dimensions like robustness, factuality, adaptivity, and domain sensitivity. These benchmarks not only reflect the evolving needs of real-world RAG deployments but also shape future directions by surfacing recurrent failure modes and task-specific limitations. Robustness to retrieval noiseis a core requirement in operational RAG systems. RGB [7] evaluates four fundamental capacities‚Äînoise robustness, negative rejection, information integration, and counterfactual resistance‚Äîrevealing consistent weaknesses in LLMs when handling distracting or misleading context. Complementing this, RAG-Bench [18] introduces a noise-centric benchmark simulating three retrieval corruption types‚Äîrelevant-but-incomplete, irrelevant, and counterfactual‚Äîand applies adaptive adversarial training to improve model tolerance. These benchmarks enable fine-grained analysis of how retrieval perturbations degrade end-task performance and inform robust retrieval-policy design. Faithfulness and hallucination detection benchmarks have taken center stage in evaluating generation quality. RAGTruth [46] provides nearly 18,000 annotated examples from QA, summarization, and data-to-text generation, offering both response- and span-level hallucination labels across four types: subtle vs. evident, and conflict vs. baseless information. Uniquely, it supports training hallucination detectors and benchmarking span-level detection precision and recall‚Äîtasks not addressed by coarse-grained metrics. This makes it foundational for measuring factual integrity in RAG outputs. Reasoning and retrieval chaining are central to multi-hop question answering, where evidence spans multiple documents. MultiHop-RAG [63] targets this challenge through linked question-answer pairs, bridge entities, and explicit multi-hop query types, enabling systematic assessment of retrieval chaining, evidence linking, and document-level reasoning‚Äîall key bottlenecks in complex RAG workflows. Adaptive retrieval and necessity estimation are benchmarked in RetrievalQA [86], which mixes queries requiring external retrieval with those answerable via the base LLM alone. This design tests whether models can intelligently toggle retrieval based on query uncertainty, supporting the development of resource-efficient, retrieval-aware systems that avoid introducing unnecessary context. Domain-specific evaluation is exemplified by MIRAGE [48], a benchmark tailored to medical RAG. It contains 7,663 questions sourced from five clinical and biomedical QA datasets and incorporates real-world evaluation constraints: zero-shot generalization, multiple-choice formats, retrieval necessity assessment, and question-only retrieval. This multi-faceted setup tests reliability under high-stakes conditions where factual errors can be consequential. Cross-corpus and federated retrieval are explored in FeB4RAG [68], a benchmark constructed from 16 BEIR sub- collections. It evaluates federated retrieval through 790 conversational queries with LLM-graded relevance judgments and quantifies the impact of resource selection and result merging strategies. This benchmark surfaces key risks in multi-source RAG pipelines, especially retrieval inconsistency and hallucination amplification due to poor corpus coordination. Evaluation infrastructure and reproducibility are addressed by BERGEN [52], a benchmarking library designed to unify assessment across RAG components. It offers modular templates for measuring retrieval precision, generation faithfulness, and their interplay across datasets and model configurations. BERGEN facilitates consistent and extensible RAG benchmarking in both academic and applied settings. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 20 Table 4. Emerging Benchmarks for Evaluating Retrieval-Augmented Generation (RAG) Systems.This table summarizes recent benchmarks developed to assess key aspects of RAG systems, including robustness, multi-hop reasoning, medical-domain adaptation, and federated retrieval. These benchmarks differ in evaluation granularity‚Äîranging from query-level to document-level‚Äîand employ varied annotation methods such as manual labeling, programmatic perturbation, and LLM-based scoring. Distinctive features, such as noise stress-testing (RGB), zero-shot medical QA (MIRAGE), and federated source merging (FeB4RAG), support targeted evaluations of both retriever components and full RAG pipelines. Benchmark Evaluation Focus Granularity Annotation Type Unique Features Evaluation Tar- get RGB Robustness (noise, inte- gration, hallucination) Query-context pair None Stress tests for noise, contradiction, and multi- source fusion Full pipeline MultiHop-RAG Multi-hop reasoning and retrieval chaining Document-level Manual + derivedLinked multi-hop queries and bridge-entity chaining Full pipeline RAGTruth Hallucination detec- tion and factuality evaluation Response-level (yes/no), span- level (exact) Human-labeled 18,000+ examples, 4 hallu- cination types, span-level F1 Generator MIRAGE Medical domain QA under real-world con- straints Query-level Dataset-native Zero-shot, multi-choice, question-only retrieval (MEDRAG) Full pipeline FeB4RAG Federated retrieval evaluation Document + re- source LLM-labeled Measures retrieval + merg- ing across 16 BEIR sources Retriever RetrievalQA Adaptive retrieval ne- cessity detection Query-level Derived Queries with and without need for retrieval Retriever RAG-Bench Retrieval robustness to noise Query-level Programmatic Irrelevant, incomplete, and counterfactual retrieval noise Full pipeline BERGEN Retrieval, generation, and joint evaluation Query-context and document- level Configurable (task-dependent) Unified benchmarking li- brary across datasets and models Full pipeline This section outlines the rapidly evolving landscape of RAG evaluation and benchmarking. Future RAG development hinges not only on improving generation quality but also on designing principled, scalable, and interpretable evaluation strategies that reflect real-world usage and complexities. To advance the field meaningfully, the community must prioritize the creation of standardized, efficient, and adaptive evaluation protocols that can serve both research and production contexts. 7 Future Directions As Retrieval-Augmented Generation (RAG) systems continue to evolve, a number of unresolved challenges remain that limit their deployment in dynamic, open-ended, and high-stakes applications. These challenges span retrieval efficiency, semantic misalignment, hallucination control, generalization, and trust. Based on the synthesis of contemporary research gaps, we outline five interrelated future directions that represent promising trajectories for advancing the field. 7.1 Retrieval Adaptivity and Semantic Alignment Current RAG architectures often rely on static retrieval policies and fixed embedding transformations, limiting their adaptability to complex or evolving user queries. Future systems must support dynamically calibrated retrieval strategies Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 21 that adjust depth, modality, and source selection in response to task difficulty and contextual cues. This calls for co- optimized retriever‚Äìgenerator pipelines that leverage reinforcement signals, uncertainty estimates, or semantic control layers to align evidence retrieval with generative intent in real time. 7.2 Robustness under Noise and Adversarial Conditions Despite recent advances in noise filtering and adversarial training, RAG systems remain vulnerable to retrieval per- turbations, misleading content, and corpus-level poisoning attacks. Future work should move toward retrieval-aware adversarial defenses that incorporate noise-aware loss functions, retrieval-type-specific regularization, and semantic provenance filtering. This includes evaluation protocols that stress-test systems against contextually plausible yet misleading passages and group-triggered semantic attacks, as exemplified by recent backdoor threat models. 7.3 Multi-Hop Reasoning and Structured Compositionality Many knowledge-intensive tasks require aggregating evidence across multiple retrieval steps and reasoning over entity or schema-level structures. Current models exhibit limited capacity for compositional inference or procedural synthesis. Future RAG systems should support multi-turn retrieval‚Äìgeneration loops, structured subgoal decomposition, and graph-augmented reasoning pipelines that maintain discourse coherence and entity consistency across long-range dependencies. 7.4 Cross-Domain Generalization and Temporal Adaptivity RAG performance often degrades in the face of domain shifts, novel schema, or temporal drift. Addressing this will require pretraining retrieval modules on diverse proxy tasks, developing meta-retrievers capable of adapting to unseen query distributions, and incorporating recency-aware document scoring. Additionally, the design of temporally evolving benchmarks and evaluation suites will be necessary to assess the robustness of RAG systems under realistic, time-sensitive knowledge conditions. 7.5 Explainability, Personalization, and Trust Calibration As RAG systems are increasingly integrated into user-facing applications, demands for interpretability, personalization, and secure behavior intensify. Future architectures should expose transparent interfaces for explaining retrieval decisions and generation provenance, while supporting privacy-preserving personalization through user-clustered retrieval, memory-efficient modeling, or differential privacy mechanisms. Furthermore, integrating retrieval calibration signals‚Äîsuch as factual salience, source trustworthiness, or hallucination risk‚Äîcan enhance user trust and system accountability. References [1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations . https://openreview.net/forum?id=hSyW5go0v8 [2] Orlando Ayala and Patrice Bechard. 2024. Reducing hallucination in structured outputs via Retrieval-Augmented Generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), Yi Yang, Aida Davani, Avi Sil, and Anoop Kumar (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 228‚Äì238. doi:10.18653/v1/2024.naacl-industry.19 [3] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs.CL] https://arxiv.org/abs/1611.09268 Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 22 [4] Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024. Seven Failure Points When Engineering a Retrieval Augmented Generation System. In Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI (Lisbon, Portugal) (CAIN ‚Äô24). Association for Computing Machinery, New York, NY, USA, 194‚Äì199. doi:10.1145/3644815.3644945 [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS ‚Äô20). Curran Associates Inc., Red Hook, NY, USA, Article 159, 25 pages. [6] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation. In First Conference on Language Modeling . https://openreview.net/forum?id=tzE7VqsaJ4 [7] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence (AAAI‚Äô24/IAAI‚Äô24/EAAI‚Äô24). AAAI Press, Article 1980, 9 pages. doi:10.1609/ aaai.v38i16.29728 [8] Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Haodong Zhao, Ping Yi, Zhuosheng Zhang, and Gongshen Liu. 2024. TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models. https://openreview.net/forum?id=RfYD6v829Y [9] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2023. Lift yourself up: retrieval-augmented text generation with self-memory. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS ‚Äô23). Curran Associates Inc., Red Hook, NY, USA, Article 1899, 20 pages. [10] Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. 2024. xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token. InThe Thirty-eighth Annual Conference on Neural Information Processing Systems . https://openreview.net/forum?id=6pTlXqrO0p [11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03- 30-vicuna/ [12] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457 [cs.AI] https://arxiv.org/abs/1803.05457 [13] Gordon V. Cormack, Charles L A Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (Boston, MA, USA) (SIGIR ‚Äô09). Association for Computing Machinery, New York, NY, USA, 758‚Äì759. doi:10.1145/1571941.1572114 [14] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR ‚Äô24). Association for Computing Machinery, New York, NY, USA, 719‚Äì729. doi:10.1145/3626772.3657834 [15] Nguyen Nam Doan, Aki H√§rm√§, Remzi Celebi, and Valeria Gottardo. 2024. A Hybrid Retrieval Approach for Advancing Retrieval-Augmented Generation Systems. In Proceedings of the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024) , Mourad Abbas and Abed Alhakim Freihat (Eds.). Association for Computational Linguistics, Trento, 397‚Äì409. https://aclanthology.org/2024.icnlsp-1.41/ [16] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. 2025. From Local to Global: A Graph RAG Approach to Query-Focused Summarization. arXiv:2404.16130 [cs.CL] https://arxiv.org/abs/2404.16130 [17] Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. RAGAs: Automated Evaluation of Retrieval Augmented Generation. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations , Nikolaos Aletras and Orphee De Clercq (Eds.). Association for Computational Linguistics, St. Julians, Malta, 150‚Äì158. https://aclanthology.org/2024.eacl-demo.16/ [18] Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. 2024. Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 10028‚Äì10039. doi:10.18653/v1/2024.acl-long.540 [19] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval- augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 2 (2023), 1. [20] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022. Re2G: Retrieve, Rerank, Generate. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 2701‚Äì2715. doi:10.18653/v1/2022.naacl-main.194 [21] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 23 Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzm√°n, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur √áelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V√≠tor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 24 Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783 [22] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval- Augmented Generation for Textual Graph Understanding and Question Answering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=MPJ3oXtTZl [23] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics , Donia Scott, Nuria Bel, and Chengqing Zong (Eds.). International Committee on Computational Linguistics, Barcelona, Spain (Online), 6609‚Äì6625. doi:10.18653/v1/2020.coling-main.580 [24] Sebastian Hofst√§tter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR ‚Äô23). Association for Computing Machinery, New York, NY, USA, 1437‚Äì1447. doi:10.1145/3539618.3591687 [25] Jie Huang, Mo Wang, Yunpeng Cui, Juan Liu, Li Chen, Ting Wang, Huan Li, and Jinming Wu. 2024. Layered Query Retrieval: An Adaptive Framework for Retrieval-Augmented Generation in Complex Question Answering for Large Language Models. Applied Sciences 14, 23 (2024). doi:10.3390/app142311014 [26] Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis, Nikos Papasarantopoulos, and Jeff Pan. 2023. Retrieval Augmented Generation with Rich Answer Encoding. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers) , Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, and Adila Alfa Krisnadhi (Eds.). Association for Computational Linguistics, Nusa Dua, Bali, 1012‚Äì1025. doi:10.18653/v1/2023.ijcnlp-main.65 [27] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics, Online, 874‚Äì880. doi:10.18653/v1/2021.eacl-main.74 [28] Jisoo Jang and Wen-Syan Li. 2024. AU-RAG: Agent-based Universal Retrieval Augmented Generation. InProceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (Tokyo, Japan) (SIGIR-AP 2024). Association for Computing Machinery, New York, NY, USA, 2‚Äì11. doi:10.1145/3673791.3698416 [29] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL] https://arxiv.org/abs/2310.06825 [30] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L√©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th√©ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. 2024. Mixtral of Experts. arXiv:2401.04088 [cs.LG] https://arxiv.org/abs/2401.04088 [31] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. 2024. LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs. arXiv:2406.15319 [cs.CL] https://arxiv.org/abs/2406.15319 [32] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval Augmented Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 7969‚Äì7992. doi:10.18653/v1/2023.emnlp-main.495 [33] Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin Jin. 2024. RAGCache: Efficient Knowledge Caching for Retrieval- Augmented Generation. arXiv:2404.12457 [cs.DC] https://arxiv.org/abs/2404.12457 [34] Hailey Joren, Jianyi Zhang, Chun-Sung Ferng, Da-Cheng Juan, Ankur Taly, and Cyrus Rashtchian. 2025. Sufficient Context: A New Lens on Retrieval Augmented Generation Systems. InThe Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=Jjr2Odj8DJ [35] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 1601‚Äì1611. doi:10.18653/v1/P17-1147 [36] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 6769‚Äì6781. doi:10.18653/v1/2020.emnlp- main.550 [37] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019), 452‚Äì466. doi:10.1162/tacl_a_00276 [38] Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022. Generative Multi-hop Retrieval. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 1417‚Äì1436. doi:10.18653/v1/2022.emnlp-main.92 Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 25 [39] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 7871‚Äì7880. doi:10.18653/v1/2020.acl-main.703 [40] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems 33 (2020), 9459‚Äì9474. [41] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, Franck Dernoncourt, Daniel Preo≈£iuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational Linguistics, Miami, Florida, US, 881‚Äì893. doi:10.18653/v1/2024.emnlp-industry.66 [42] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 9802‚Äì9822. doi:10.18653/v1/2023.acl-long.546 [43] Nicholas Matsumoto, Jay Moran, Hyunjun Choi, Miguel E Hernandez, Mythreye Venkatesan, Paul Wang, and Jason H Moore. 2024. KRAGEN: a knowledge graph-enhanced RAG framework for biomedical problem solving using large language models. Bioinformatics 40, 6 (2024), btae353. [44] Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, and Maarten de Rijke. 2024. Ranked List Truncation for Large Language Model-based Re-Ranking. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR ‚Äô24). Association for Computing Machinery, New York, NY, USA, 141‚Äì151. doi:10.1145/3626772.3657864 [45] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 12076‚Äì12100. doi:10.18653/v1/2023.emnlp-main.741 [46] Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong Zhang. 2024. RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 10862‚Äì10878. doi:10.18653/v1/2024.acl-long.585 [47] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim√≥n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, ≈Åukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, ≈Åukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M√©ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O‚ÄôKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer√≥n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 26 Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] https://arxiv.org/abs/2303.08774 [48] Shintaro Ozaki, Yuta Kato, Siyuan Feng, Masayo Tomita, Kazuki Hayashi, Wataru Hashimoto, Ryoma Obara, Masafumi Oyamada, Katsuhiko Hayashi, Hidetaka Kamigaito, and Taro Watanabe. 2025. Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain. arXiv:2412.20309 [cs.CL] https://arxiv.org/abs/2412.20309 [49] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt√§schel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 2523‚Äì2544. doi:10.18653/v1/2021.naacl-main.200 [50] Zackary Rackauckas. 2024. Rag-Fusion: A New Take on Retrieval Augmented Generation. International Journal on Natural Language Computing 13, 1 (Feb. 2024), 37‚Äì47. doi:10.5121/ijnlc.2024.13103 [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 1‚Äì67. [52] David Rau, Herv√© D√©jean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, St√©phane Clinchant, and Vassilina Nikoulina. 2024. BERGEN: A Benchmarking Library for Retrieval-Augmented Generation. In Findings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 7640‚Äì7663. doi:10.18653/v1/ 2024.findings-emnlp.449 [53] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR abs/2403.05530 (2024). https://doi.org/10.48550/arXiv.2403.05530 [54] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends¬Æ in Information Retrieval 3, 4 (2009), 333‚Äì389. [55] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2024. ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 338‚Äì354. doi:10.18653/v1/2024.naacl-long.20 [56] Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in Retrieval-Augmented Generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR ‚Äô24). Association for Computing Machinery, New York, NY, USA, 2395‚Äì2400. doi:10.1145/3626772.3657957 [57] Alireza Salemi and Hamed Zamani. 2024. Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR ‚Äô24). Association for Computing Machinery, New York, NY, USA, 741‚Äì751. doi:10.1145/3626772.3657733 [58] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. REPLUG: Retrieval-Augmented Black-Box Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 8371‚Äì8384. doi:10.18653/v1/2024.naacl-long.463 [59] Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun Ren. 2024. Generate-then-Ground in Retrieval- Augmented Generation for Multi-hop Question Answering. InProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 7339‚Äì7353. doi:10.18653/v1/2024.acl-long.397 [60] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid Questions Meet Long-Form Answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 8273‚Äì8288. doi:10.18653/v1/2022.emnlp-main.566 [61] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 12991‚Äì13013. doi:10.18653/v1/2024.acl-long.702 [62] Viju Sudhi, Sinchana Ramakanth Bhat, Max Rudat, and Roman Teucher. 2024. RAG-Ex: A Generic Framework for Explaining Retrieval Augmented Generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR ‚Äô24). Association for Computing Machinery, New York, NY, USA, 2776‚Äì2780. doi:10.1145/3626772.3657660 Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 27 [63] Yixuan Tang and Yi Yang. 2024. MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries. InFirst Conference on Language Modeling. https://openreview.net/forum?id=t4eB3zYWBK [64] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L√©onard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am√©lie H√©liou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl√©ment Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Miku≈Ça, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl√©ment Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv:2403.08295 [cs.CL] https://arxiv.org/abs/2403.08295 [65] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 809‚Äì819. doi:10.18653/v1/N18-1074 [66] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL] https://arxiv.org/abs/2307.09288 [67] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop Questions via Single-hop Question Composition. Transactions of the Association for Computational Linguistics 10 (2022), 539‚Äì554. doi:10.1162/tacl_a_00475 [68] Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, and Guido Zuccon. 2024. FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR ‚Äô24). Association for Computing Machinery, New York, NY, USA, 763‚Äì773. doi:10.1145/3626772.3657853 [69] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023. Learning to filter context for retrieval-augmented generation. arXiv preprint arXiv:2311.08377 (2023). [70] Zheng Wang, Shu Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. 2024. M-RAG: Reinforcing Large Language Model Performance through Retrieval- Augmented Generation with Multiple Partitions. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1966‚Äì1978. doi:10.18653/v1/2024.acl-long.108 [71] Zilong Wang, Zifeng Wang, Long Le, Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, Chen-Yu Lee, and Tomas Pfister. 2025. Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting. InThe Thirteenth International Conference on Learning Representations . https://openreview.net/forum?id=xgQfWbV6Ey [72] Junde Wu, Jiayuan Zhu, and Yunli Qi. 2024. Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation. CoRR abs/2408.04187 (2024). https://doi.org/10.48550/arXiv.2408.04187 [73] Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang Li, Joyce C. Ho, Carl Yang, and Qi He. 2025. SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, Albuquerque, New Mexico, 11534‚Äì11550. https: //aclanthology.org/2025.naacl-long.575/ [74] Sheng Xu, Mike Chen, and Shuwen Chen. 2024. Enhancing Retrieval-Augmented Generation Models with Knowledge Graphs: Innovative Practices Through a Dual-Pathway Approach. In Advanced Intelligent Computing Technology and Applications: 20th International Conference, ICIC 2024, Tianjin, China, August 5‚Äì8, 2024, Proceedings, Part VI (Tianjin, China). Springer-Verlag, Berlin, Heidelberg, 398‚Äì409. doi:10.1007/978-981-97-5678-0_34 [75] Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and Xueqi Cheng. 2024. List-aware Reranking-Truncation Joint Model for Search and Retrieval- augmented Generation. In Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW ‚Äô24). Association for Computing Machinery, New York, NY, USA, 1330‚Äì1340. doi:10.1145/3589334.3645336 Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 28 [76] Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, and Jie Zhou. 2024. Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 133‚Äì145. doi:10.18653/v1/2024.acl-long.9 [77] Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2024) . ACM, 2905‚Äì2909. doi:10.1145/3626772.3661370 [78] Jiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun Chen, and Qian Lou. 2024. BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models. https://openreview.net/forum?id=G2p8TLuJgy [79] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective Retrieval Augmented Generation. https://openreview.net/forum?id= JnWJbrnaUE [80] Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang. 2024. IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR ‚Äô24). Association for Computing Machinery, New York, NY, USA, 730‚Äì740. doi:10.1145/3626772. 3657760 [81] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun‚Äôichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 2369‚Äì2380. doi:10.18653/v1/D18-1259 [82] Fuda Ye, Shuangyin Li, Yongqi Zhang, and Lei Chen. 2024. R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation. In Findings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 11584‚Äì11596. doi:10.18653/v1/2024.findings-emnlp.678 [83] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro. 2024. RankRAG: Uni- fying Context Ranking with Retrieval-Augmented Generation in LLMs. In NeurIPS. http://papers.nips.cc/paper_files/paper/2024/hash/ db93ccb6cf392f352570dd5af0a223d3-Abstract-Conference.html [84] Hamed Zamani and Michael Bendersky. 2024. Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR ‚Äô24). Association for Computing Machinery, New York, NY, USA, 2641‚Äì2646. doi:10.1145/3626772.3657923 [85] Shenglai Zeng, Jiankun Zhang, Pengfei He, Yiding Liu, Yue Xing, Han Xu, Jie Ren, Yi Chang, Shuaiqiang Wang, Dawei Yin, and Jiliang Tang. 2024. The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG). In Findings of the Association for Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 4505‚Äì4524. doi:10.18653/v1/2024.findings-acl.267 [86] Zihan Zhang, Meng Fang, and Ling Chen. 2024. RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering. In Findings of the Association for Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 6963‚Äì6975. doi:10.18653/v1/2024.findings-acl.415 [87] Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, and Min Zhang. 2024. SEER: Self-Aligned Evidence Extraction for Retrieval-Augmented Generation. InProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 3027‚Äì3041. doi:10.18653/v1/2024.emnlp-main.178 [88] Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, and Bin Wang. 2024. ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, Torino, Italia, 16263‚Äì16273. https://aclanthology.org/2024.lrec-main.1413/ [89] Kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang, Qianglong Chen, Zheng Chu, Jingchang Chen, and Bing Qin. 2024. An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1044‚Äì1069. doi:10.18653/v1/2024.acl-long.59 [90] Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, and Jindong Chen. 2025. Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=HE6pJoNnFp A Appendix To support transparency and reproducibility, we include in the Appendix the original benchmark scores reported in the primary publications of each RAG framework. These tables serve as the empirical source for the relative improvement Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 29 analyses presented in Sections 5. All values are cited from original papers, preserving reported metrics such as F1, EM, Accuracy, and FactScore. Where applicable, dataset splits, backbone models, and evaluation metrics are clearly labeled to ensure traceability. Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 30 Table 5. Reported Performance Scores for Short-Form QA Frameworks. Accuracy and Exact Match (EM) scores as reported in the original publications of short-form RAG frameworks. These values were used to compute the normalized improvements presented in Section 5. Taxonomy Framework Backbone Dataset Metric Raw LLM LLM+Retrieval Framework Score Retriever-Based RAG RQ-RAG LLaMA2-7B PopQA Acc 14.7 39.8 57.1 RQ-RAG LLaMA2-7B ARC-Challenge Acc 21.8 28.7 68.3 SimRAG LLaMA3-8B ARC-Challenge Acc ‚Äì 71.08 81.4 SimRAG LLaMA3-8B SciQ EM ‚Äì 20.8 57.5 SimRAG Gemma2-27B ARC-Challenge Acc ‚Äì 85.75 88.65 SimRAG Gemma2-27B SciQ EM ‚Äì 44.8 58.1 Re2G BART Large NQ Acc 45.22 ‚Äì 51.73 Re2G BART Large TriviaQA Acc 60.99 ‚Äì 76.27 FILCO LLaMA2-7B (Top-5) NQ EM ‚Äì 47.6 61.8 FILCO LLaMA2-7B (Top-5) TriviaQA EM ‚Äì 67.3 71.1 Generator-Based RAG SELF-RAG LLaMA2-7B PopQA Acc 14.7 38.2 54.9 SELF-RAG LLaMA2-7B TriviaQA Acc 30.5 42.5 66.4 SELF-RAG LLaMA2-7B ARC-Challenge Acc 21.8 48.0 67.4 SELF-RAG LLaMA2-13B PopQA Acc 14.7 45.7 55.8 SELF-RAG LLaMA2-13B TriviaQA Acc 38.5 47.0 69.3 xRAG Mistral-7B NQ EM 30.25 42.71 39.1 xRAG Mistral-7B TriviaQA EM 57.08 65.88 65.77 xRAG Mistral-7B WebQA EM 34.89 37.84 39.4 xRAG Mixtral-8x7B NQ EM 41.99 45.15 47.28 xRAG Mixtral-8x7B TriviaQA EM 71.1 70.34 74.14 xRAG Mixtral-8x7B WebQA EM 40.31 41.26 44.5 FiD-Light FiD+DPR TriviaQA EM 48.6 ‚Äì 57.6 FiD-Light FiD+DPR NQ EM 41.9 ‚Äì 53.2 R2AG LLaMA2-7B NQ Acc 0.38 ‚Äì 0.693 SELF-RAG LLaMA2-7B NQ Acc 0.38 ‚Äì 0.188 Hybrid RAG Stochastic RAG FiD-Light (T5-Base) NQ EM ‚Äì 45.6 46.2 Stochastic RAG FiD-Light (T5-Base) TriviaQA EM ‚Äì 57.6 59.7 Stochastic RAG FiD-Light (T5-XL) NQ EM ‚Äì 51.1 53.0 Stochastic RAG FiD-Light (T5-XL) TriviaQA EM ‚Äì 63.7 64.7 CRAG LLaMA2-7B NQ Acc 0.38 ‚Äì 0.397 CRAG LLaMA2-7B PopQA Acc 14.7 40.3 59.3 CRAG LLaMA2-7B ARC-Challenge Acc 21.8 46.7 54.8 Self-CRAG LLaMA2-7B PopQA Acc 14.7 40.3 61.8 Self-CRAG LLaMA2-7B ARC-Challenge Acc 21.8 46.7 67.2 TA-ARE GPT-3.5 RetrievalQA Acc 1.2 38.2 35.8 TA-ARE GPT-4 RetrievalQA Acc 2.4 46.0 46.4 TA-ARE LLaMA2-7B RetrievalQA Acc 2.0 36.0 30.7 Robustness-Based RAG RAAT LLaMA2-7B RAG-Bench (TQA/NQ/WebQ) EM 38.37 65.4 83.07 Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 31 Table 6. Reported Performance Scores for Multi-Hop QA Frameworks. Raw F1 and EM scores extracted from the original papers of multi-hop RAG systems, across datasets such as HotpotQA, 2Wiki, and MuSiQue. These scores form the basis of the comparative analysis in Section 5. Taxonomy Framework Backbone Dataset Metric Raw LLM LLM + Retrieval Framework Score Retriever-Based RQ-RAG LLaMA2-7B HotpotQA F1 6.6 16.7 62.6 RQ-RAG LLaMA2-7B 2Wiki F1 16 18.7 44.8 RQ-RAG LLaMA2-7B MuSiQue F1 3 7.4 41.7 RankRAG LLaMA3-8B HotpotQA F1 ‚Äì 43.3 46.7 RankRAG LLaMA3-8B 2Wiki F1 ‚Äì 27.9 36.9 RankRAG LLaMA3-70B HotpotQA F1 ‚Äì 44.6 55.4 RankRAG LLaMA3-70B 2Wiki F1 ‚Äì 31.9 43.9 LQR LLaMA3-8B MuSiQue F1 10.7 22.8 41.97 LQR LLaMA3-8B HotpotQA F1 22.71 46.15 69.96 LQR LLaMA3-8B 2Wiki F1 32.04 47.9 54.65 LongRAG GPT-4o HotpotQA EM 42.4 ‚Äì 64.3 LongRAG Gemini-1.5-Pro HotpotQA EM 33.9 ‚Äì 57.5 SEER LLaMA2-7B-Chat HotpotQA F1 0.5471 0.5826 0.604 FILCO LLaMA2-7B HotpotQA EM ‚Äì 61.5 65 Generator-Based R2AG LLaMA2-7B HotpotQA F1 8.52 ‚Äì 36.05 R2AG LLaMA2-7B MuSiQue F1 2.41 ‚Äì 16.87 R2AG LLaMA2-7B 2Wiki F1 6.34 ‚Äì 34.52 xRAG Mistral-7B HotpotQA EM 27.02 38.79 34.05 xRAG Mixtral-8x7B HotpotQA EM 32.87 43.46 39.66 INFO-RAG LLaMA2-7B HotpotQA EM 39.4 ‚Äì 46.56 INFO-RAG LLaMA2-13B HotpotQA EM 42.12 ‚Äì 51.48 INFO-RAG LLaMA2-13B-chat HotpotQA EM 61.23 ‚Äì 61.91 INFO-RAG LLaMA2-7B MuSiQue EM 25.95 ‚Äì 30.19 INFO-RAG LLaMA2-13B MuSiQue EM 25.78 ‚Äì 35.02 INFO-RAG LLaMA2-13B-chat MuSiQue EM 47.06 ‚Äì 47.93 Hybrid DRAGIN LLaMA2-13B-chat HotpotQA F1 30.97 37.06 42.38 DRAGIN LLaMA2-13B-chat 2Wiki F1 27.21 33.64 39.31 DRAGIN LLaMA2-7B-chat HotpotQA F1 27.45 24.99 33.44 DRAGIN LLaMA2-7B-chat 2Wiki F1 22.32 25.49 29.26 DRAGIN Vicuna-13B-v1.5 HotpotQA F1 32.56 35.31 41.64 DRAGIN Vicuna-13B-v1.5 2Wiki F1 22.32 25.64 35.16 FLAREdirect GPT-3.5 2Wiki F1 36.8 48.8 59.7 FLAREinstruct GPT-3.5 2Wiki F1 36.8 48.8 49.8 GenGround GPT-3.5 HotpotQA F1 42.28 47.8 52.26 GenGround GPT-3.5 MuSiQue F1 20.13 20.11 27.36 GenGround GPT-3.5 2Wiki F1 41.19 44.77 50.21 GenGround GPT-3.5 StrategyQA F1 68.13 71.78 77.12 Stochastic RAG FiD-Light (T5-Base) HotpotQA EM 25.6 ‚Äì 27.3 Stochastic RAG FiD-Light (T5-XL) HotpotQA EM 29.2 ‚Äì 31.1 Manuscript submitted to ACM Retrieval-Augmented Generation: A Survey 32 Table 7. Reported Robustness Scores for RAG Frameworks. Precision, recall, and FactScore values extracted from original publications, across multiple datasets. These scores serve as the empirical basis for the comparative robustness analysis in Section 5. Taxonomy Framework Backbone Metric (Dataset) LLM + Retrieval LLM + Retrieval + Framework Retriever-Based RAG FILCO RAG Precision (FEVER) 1.2 5.1 Re2G KGI0 Precision (NQ) 64.65 70.92 Re2G KGI0 Recall (NQ) 69.6 74.79 Re2G KGI0 R-Precision (NQ) 61.13 72.01 Re2G KGI0 Recall (TriviaQA) 63.12 73.16 Re2G KGI0 R-Precision (FEVER) 80.34 90.06 Re2G KGI0 Recall (FEVER) 86.53 92.91 Generator-Based RAG SELF-RAG LLaMA2-7B Precision (ASQA) 2.9 66.9 SELF-RAG LLaMA2-7B Recall (ASQA) 4 67.8 SELF-RAG LLaMA2-7B FactScore (ASQA) 78 81.2 SELF-RAG LLaMA2-13B Precision (ASQA) 2.3 70.3 SELF-RAG LLaMA2-13B Recall (ASQA) 3.6 71.3 SELF-RAG LLaMA2-13B FactScore (ASQA) 77.5 80.2 FiD-Light T5-Base FactScore (FEVER) ‚Äì 80.6 FiD-Light T5-XL FactScore (FEVER) ‚Äì 84.5 RAG w/ Rich Ans. Encoding RAG Recall (MSMARCO) 25.3 27.5 RAG w/ Rich Ans. Encoding RAG Recall (KILT WoW) 61.98 68.63 GenRT RAG Recall (NQ) 59.4 60.78 GenRT RAG Recall (TriviaQA) 68.22 70.01 Hybrid RAG CRAG LLaMA2-7B FactScore (Biography) 59.2 74.1 Self-RAG LLaMA2-7B FactScore (Biography) 59.2 81.2 Self-CRAG LLaMA2-7B FactScore (Biography) 59.2 86.2 Flare Instruct GPT-3.5 Precision (2Wiki) 48.6 49.1 Flare Instruct GPT-3.5 Recall (2Wiki) 51.5 52.5 Flare Direct GPT-3.5 Precision (2Wiki) 48.6 59.1 Flare Direct GPT-3.5 Recall (2Wiki) 51.5 62.6 Stochastic RAG FiD-Light (T5-Base) FactScore (FEVER) 80.6 81.3 Stochastic RAG FiD-Light (T5-XL) FactScore (FEVER) 84.5 84.8 DRAGIN LLaMA2-13B Precision (HotPotQA) 0.3711 0.4401 DRAGIN LLaMA2-13B Recall (HotPotQA) 0.374 0.411 DRAGIN VICUNA-13B Precision (HotPotQA) 0.3457 0.4226 DRAGIN VICUNA-13B Recall (HotPotQA) 0.352 0.389 Manuscript submitted to ACM