Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support Paulina Toro Isaza, Michael Nidd, Noah Zheutlin, Jae-wook Ahn, Chidansh Amitkumar Bhatt, Yu Deng, Ruchi Mahindru, Martin Franz, Hans Florian, Salim Roukos IBM Research ptoroisaza@ibm.com, mni@zurich.ibm.com, noah.zheutlin@ibm.com, jaewook.ahn@us.ibm.com, chidansh.amitkumar.bhatt@ibm.com, dengy@us.ibm.com, rmahindr@us.ibm.com, franzm@us.ibm.com, raduf@us.ibm.com, roukos@us.ibm.com Abstract Clients wishing to implement generative AI in the domain of IT Support and AIOps face two critical issues: domain cov- erage and model size constraints due to model choice lim- itations. Clients might choose to not use larger proprietary models such as GPT-4 due to cost and privacy concerns and so are limited to smaller models with potentially less domain coverage that do not generalize to the client’s domain. Re- trieval augmented generation is a common solution that ad- dresses both of these issues: a retrieval system first retrieves the necessary domain knowledge which a smaller generative model leverages as context for generation. We present a sys- tem developed for a client in the IT Support domain for sup- port case solution recommendation that combines retrieval augmented generation (RAG) for answer generation with an encoder-only model for classification and a generative large language model for query generation. We cover architecture details, data collection and annotation, development journey and preliminary validations, expected final deployment pro- cess and evaluation plans, and finally lessons learned. 1 Introduction The recent boost in performance and popularization of gen- erative models has resulted in clients across various domains requesting generative AI powered question-answering and recommendation systems. However, there are two critical issues facing many clients wishing to implement genera- tive AI: domain coverage and model size constraints due to model choice limitations. Much of the focus for gener- ative models has been on the general domain and only some specific tasks such as coding. Models that work on these domains might not necessarily work for a client’s targeted domain. Additionally, clients might choose not to leverage larger proprietary models such as GPT-4 because of cost and privacy concerns so clients are limited to smaller models with less domain coverage and likely lower out-of-the-box performance. We have faced both of these issues when building a solu- tion recommendation system for resolving IT support cases. Models and tasks within the domain of IT support and Ar- tificial Intelligence for IT Operations (AIOps) are under- researched. No IT support specific fine-tuned generative AI Copyright © 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. model exists and there are limited publicly available datasets on IT support tasks like question-answering (QA) or re- trieval over a corpus of IT support documents. Thus, it is non-trivial to evaluate out-of-the-box AI models on IT sup- port tasks as well as train and evaluate custom AI solutions in this domain. In specific, the IT support use case involved IT prod- uct support tickets that are opened by a customer and an- swered by a support agent after a series of interactions. It required a system that would respond only to cases that did not need any additional information or clarification from the customer. That is, the support case could be resolved based only on the initial information present in the case subject and description when the ticket was first opened. Addition- ally, the use case required that solutions be grounded in an official support document that would be presented to the cus- tomer as a link along with the summarized solution. Given these use case requirements, retrieval augmented generation (RAG) was a natural fit (Lewis et al. 2020). It is a common solution that addresses the two main issues of in- adequate generalizability to more niche domains and model size limits. It does so by using a retrieval system to first retrieves the necessary domain knowledge which a smaller generative model then leverages as context for answer gen- eration. We present the resulting system for IT support case solu- tion recommendation that combines an encoder-only model for classification, two dense embedding models for retrieval, and generative large language models for query and answer generation. The solution brings several novel contributions to the field of AIOps: • The first reported evaluation of a RAG system for IT sup- port incident resolution recommendation. • The first reported use of a classifier for determining sin- gle vs multi-turn IT support cases. • Evidence of substantial retrieval improvement using re-ranking based on a new model, IBM Slate 125m (IBM Research 2024) • A comparison of answer generation performance across diverse model sizes that shows smaller models can match or even beat the performance of very large models in the RAG incident remediation use case. Figure 1: System architecture 2 Architecture Given a support case subject, description, and product name, our system generates recommended solutions based on cor- pora of support documents. Our system consists of four ma- jor components as illustrated in Figure 1: an encoder-only transformer classifier, a query generation system, a retriever system, and an answer generator system. Preprocessing: Support cases are ingested with unstruc- tured text data fields of case subject, case description, prod- uct name, and product version number. The escape and non- ASCII characters are removed from the case subject and description, and the two fields are concatenated. We pre- process the product name by matching it to a dictionary of known product acronyms or alternative names to append to the query used in retrieval. Case Turn Classifier:The cleaned and concatenated case subject and description are fed into the classifier which de- termines if the support case is a single turn. ingle-turn cases are defined as those that can be resolved using only the infor- mation present in the case subject and description, without requiring any additional information or clarification from the customer. The classifier is an encoder-only transformer model IBM Slate 125m (IBM Research 2024) that was fine- tuned on almost 14,000 examples labeled by subject matter experts. If the case is predicted to be single turn, the case continues to the next step in the pipeline. Question Generator: The question generator summarizes the often vague case subjects and verbose, convoluted de- scriptions into concise text queries suitable for the retrieval system. The system prompts a large language model, Mix- tral 8x7B Instruct (Jiang et al. 2024), to generate a concise question based on the provided case subject and descrip- tion. Additional post-processing keeps only the first gener- ated sentence in case the generative model does not follow instructions and generates additional sentences beyond the first question. Retriever: Our documentation is retrieved from multiple data collections in a Milvus vector database, all indexed with the standard Slate-30M embedding. If the search stage re- quires top-3 documents, we search for 3 from each of these indexes, and then merge-sort based on the score before re- taining the top-k from the combined set. Having retrieved the top-3 documents, as ranked by a general-purpose embedding, we re-rank them using a Bi- Encoder model that has been fine-tuned using application- specific training data. Re-ranker scores are computed as co- sine similarities of a combination of the original case and the derived question, compared with the same passages (with re- calculated vectorizations) that were matched in the first pass. Answer Generator: For each of the top three documents retrieved, the answer generator produces an answer to the previously generated query by leveraging the top three re- ranked passages from the document. First, we split the re- trieved document content into 2500-token chunks with 250- token overlaps and use cosine similarity to pick the three most relevant chunk contexts. The answer generator prompts a large language model, Mixtral, to answer the query us- ing the three contexts. Finally, the system returns the three retrieved links with their corresponding generated answers. The results can then be displayed to the support agent in a graphic interface. 3 Data We collected almost 19,000 real support cases across nine software products: six to serve as training and three to serve as validation. Each case included a case subject and descrip- tion originally drafted by a customer when opening the case. Additionally, the three indices for documentation leveraged for retrieval had a total corpus of over 5 million documents. For each product, we asked five support agents who were singled out as product subject matter experts to carry out the following tasks for each of the nine products: 1) anno- tate single-turn vs. multi-turn label, 2) validate silver ground truth query based on case subject and description and pro- vide updated query as necessary, 3) provide link to relevant document in support corpus, and finally 4) copy and paste solution to query as found in the relevant support document. Tasks #2 through #4 were carried out only for cases that had been labeled as single-turn. After cleaning and removing missing annotations, we created a dataset of almost 19,000 support cases, with almost 3,000 cases for each training product and over 400 cases for each evaluation product. 4 Development and Validation Case Turn Classifier As our solution is meant to supply answers before involv- ing a support agent, we needed to develop a method for classifying incoming cases as single-turn vs. multi-turn. The classifier model is a binary encoder-only IBM Slate 125m model fine-tuned on the single-turn/multi-turn labels of al- most 7,000 unique cases across six software products. The final training data is around 14,000 cases as it includes two copies of a given case: one with tokens from both the case subject and description fields, and another with tokens only from the case subject.. Positive Train Eval Class % F1 P R In-Domain 13964 3512 25% 0.46 0.31 0.89 Out-of-Domain - 1375 53% 0.65 0.54 0.80 Table 1: Single-Turn classifier model performance on six in- domain training products and three out-of-domain evalua- tion products For our application, we prioritized correctly predict- ing single-turn cases (positive class) versus multi-turn cases (negative) emphasizing recall over precision. Table 1 presents the final fine-tuned model performance on the held-out evaluation set of the six products when using a classification threshold of 0.1. We found that performance varies widely depending on the product, ranging from F1 of 0.27 to 0.62 and recall from 0.75 to 0.98. The lower perfor- mance can be explained in part by the varying class imbal- ance across products (positive class proportion from 11% to 44%) as well as the products’ differing inter-annotator agreement (See Section 6). Despite this, the model still substantially outperforms random guessing of the classes. Hyper-parameters including batch size, learning rate, and dropout were determined based on a small grid search. We then evaluated the fine-tuned classifier on about 1,400 cases from three additional products that were not in the training set to validate if the model generalized to other products. The resulting F1 of 0.65, precision of 0.54, and recall of 0.80 for the three products suggests that classifier model generalizes well to products not seen during training even with substantially different class balance. Query Generator Model BertScore F1 ROUGE-L F1 Falcon-40B* 0.91 0.40 Mistral-Large-2 0.91 0.38 Mixtral-8x7B-Instruct 0.91 0.36 Granite-13B-Chat-v2 0.89 0.28 Table 2: Comparison of BertScore F1 and ROUGE-L F1 for different models on query generation task. BertScore is based on roberta-large embeddings. Falcon-40B scores are not comparable to the other models because the prompt used was different, and it was used to create the initial questions that were validated or edited by SMEs to create the ground truth. In order to create a concise query that could be used by the retriever, we generated a single sentence question based on the case subject and description. Our experiments over various open-source generative models (Table 2) and model availability in the client’s services led us to choose Mixtral- 8x7b-Instruct as the model for query generation which reli- ably reproduced the ground truth queries despite being a rel- atively small model with no domain knowledge. Note that the results are skewed for Falcon-40B (Almazrouei et al. 2023) as Falcon-40B generated the first pass of silver ground truth queries that were then edited by subject matter experts. Retriever Support experts supplied us with a collection of cases with one ground truth link each that a “correct” solution should reference; our evaluation is based on whether this link is contained in the top n links returned (for various values of n). Implementing this evaluation presented several chal- lenges: • URL Duplication: A single page of documentation often has several different URLs to identify it. • Subtle Content Variations: Documentation for the same topic in different versions of a product may have subtly different titles, like “How to update a list” and “Lists: Updating”. • Identical Content Across Different Documents: Docu- mentation for the same topic in different versions may be identical in which case results from different versions are still valid. Mitigating the first of these challenges, many of our docu- mentation pages include a “canonical link” in their metadata. In many cases, this allows us to identify identical links. The two issues with documentation evolution between versions are addressed with Rouge-1 scores, using a threshold of 0.90 as sufficiently similar to count as identical. For retrieval, a dedicated team is already responsible for maintaining indexed collections of the software product doc- umentation. This saves our project from gathering, maintain- ing, and indexing all of these documents, and its base Slate- 30M embedding returns a good first set of results. Re-ranking this first set allows us to use fine-tuning to improve performance without maintaining a parallel set of indexes. For this final task-specific fine-tuning stage, we used training data based on 1,430 questions with up to three matching passages per question extracted from documents identified in user interactions, together with negative exam- ples found using BM25 search. The resulting IBM Slate- 125M model was then distilled into the deployed IBM Slate- 30M model. To evaluate its effectiveness, we present recall before and after re-ranking with Google Search as a baseline in Figure 2. 0 - Completely 1 - somewhat 2 - Solution Rating irrelevant relevant/helpful in link Product A SME 58% (11) 0% (0) 42% (8) Tool 26% (5) 5% (1) 68% (13) Product B SME 20% (12) 48% (28) 28% (17) Tool 48% (29) 37% (22) 12% (7) Table 3: AB testing human evaluation of retrieved links We also conducted an AB test in which support agents of two products were provided with a link retrieved by the tool Figure 2: Retriever recall for top x (log scale), compar- ing with (yellow) and without (orange) re-ranking vs. direct Google search via SerpApi (grey) for 1729 customer issues over six products and a link provided by a subject matter expert. The source of the link was randomized as Source A or Source B so that, for example, Source A could be either our tool or an SME for any given case. The support agents were asked to rate each link as shown in Table 3 and to pick the better of the two solutions. The results for Product A show that support agents gave higher ratings to and preferred the links suggested by the tool over those from a SME. The results for Product B how- ever show higher scores for links provided by the SME but about half of the cases were still rated as having a somewhat helpful or complete link provided by our tool. Additionally, when directly asked to compare the recommendations, sup- port agents reported that the tool was more helpful or just as helpful as the SME link 69% of the time for Product A and 35% of the time for Product B. Answer Generator The final step of our solution takes in the generated query and top three most relevant retrieved passages as context to prompt the answer generator. In particular, the prompt asks the model to use the information within the provided con- texts to generate an answer. Additionally, if the context is insufficient, then the model is instructed to state that an ac- curate answer cannot be provided. To evaluate the answers, we used the subject matter ex- pert’s annotated ground truth answers and ground truth doc- uments verified to contain the answer to the question. We compared the answers generated by the answer extractor us- ing the ground truth document to the ground truth answer using BertScore (Zhang et al. 2020) and ROUGE-L F1. We evaluated different models and prompts to find the optimal combination and present the results of the models assessed in Table 4. While BertScore (roberta-large) F1 is rather low (in practice it ranges between 0.85-0.95), ROUGE-L F1, tra- ditionally a rather strict metric, shows promising results for Mixtral-8x7b-Instruct with a score of 0.41. Mixtral-8x7b- Instruct’s outperforms of GPT-4o, included as a baseline for larger models, in all three metrics, despite having substan- tially less parameters. Likewise, Granite-13B-Chat-v2 is not far behind GPT-4o despite its merely 13 billion parameters compared to GPT-4o’s rumored hundreds of billions or even trillions of parameters. This suggests that the RAG approach of smaller models leveraging retrieved context is a viable so- lution for IT incident resolution recommendation systems. Knowledge Infusion for Answer Generation Directly applying general foundational models to AIOps for answer generation tasks often does not yield optimal re- sults. The knowledge infusion approach involves adapting these pre-trained models to specific tasks through additional training on task-specific data. To further boost the results of our action generation task for AIOps domain, we employ the knowledge infusion methodology described in (Sudalairaj et al. 2024). First, we manually created a seed dataset with six tuples, each con- taining context and four related question-answer pairs. Then, we randomly selected fifteen documents from the corpus to guide synthetic data generation, using the seed dataset to replicate similar artifacts for each document. Using this seed dataset, we created 14,000 synthetic samples with the Mixtral-8x7B-Instruct model as the teacher. IBM’s Gran- ite 7B IL-Internal-Granite-7B-Base, a much smaller model, was fine-tuned with IT domain data to cater to the specific task of answer generation for the IT Support use case re- sulting in the InstructLab-IT model with domain knowledge infusion. To evaluate the quality improvement, we conducted a user study with 6 technical experts and forty test question-answer pairs per model. For each question, we retrieved context from a 1200-document corpus of six software products and used it to prompt each model separately for an answer. Our user study used a 0 to 1 rubric to evaluate answer correctness with clear descriptions for each level: • 0 = Incorrect: irrelevant or fails to answer the question. • 0.25 = Mostly Incorrect: Some details are correct, but key details are missing, fabricated, or mostly irrelevant. • 0.5 = Partially Correct: Most details are correct, but some key details are missing, fabricated, or include a lot of ir- relevant information. • 0.75 = Mostly Correct: Most details are accurate, with only minor gaps or irrelevant information. • 1 = Correct: Includes only relevant details. In Table 5, we present the final score for each model as the average of human annotators’ scores across 40 question-answer pairs of which InstructLab-IT emerged as the best model. While Llama-3.1-8b-Instruct performed slightly better than GPT-4o, the improvement in results with InstructLab-IT was very noticeable over both models. This is especially significant considering the model sizes: GPT- 4o (over 1 trillion parameters and 1.5 TB), Llama-3.1-8b- Instruct (8 billion parameters and 16 GB), and InstructLab- IT (7 billion parameters and 28 GB). These results signal that a smaller, domain-specific model tuned for a specific set of use cases may better meet client requirements. 5 Deployment The tool is currently integrated into the ticketing system but silently deployed (not visible to agents) for testing purposes. Model BertScore (roberta-large) F1 BertScore (deberta-xlarge-mnli) F1 ROUGE-L F1 GPT-4o (2024-08-06) 0.86 0.62 0.34 Mistral-Large-2 0.86 0.62 0.37 Mixtral-8x7B-Instruct 0.87 0.64 0.41 Granite-13B-Chat-v2 0.86 0.58 0.32 Table 4: Comparison of BertScore F1 and ROUGE-L F1 for different models performing the answer generation task. BertScore based on roberta-large embeddings Figure 3: Working mockup of online deployment UI of single system result with feedback items. Model Score GPT-4o 0.68 Llama-3.1-8b-Instruct 0.70 InstructLab-IT 0.76 Table 5: Comparison of analytic rubric scores for different models on answer generation task. We are currently working on refinements and integration and plan to deploy the system before the end of the year. We will incorporate feedback buttons for the tool once it is de- ployed online and visible to agents. In the customer support portal user interface, for a given case, support agents will see a suggested solution and link. This will include five-star ratings for accuracy and readability as well as a drop-down menu for feedback including the options: “useful”, “some- what useful”, “no useful suggestion”, and “need more client info”. See Figure 3 for a working mock-up of the user inter- face. 6 Lessons Learned Use Case Formation Defining the proper use case is probably the most critical step in developing a proper RAG recommendation applica- tion. Because of the expense of data collection, annotation, and development, any confusion or change in the exact use case and capabilities of the model can result in substantial delays and costs. For example, the first dataset that we considered for this use case was synthetic data for which subject matter ex- perts crafted questions based on support document titles, and then provided corresponding answers. When we compared this data to actual customer cases, we found the genuine questions to be more verbose and to contain more off-topic “noise.” Thus we decided to use the more challenging actual support ticket data for training and validation, as it appeared better suited to our final deployment than the cleaner syn- thetic data. We recommend spending time early on to understand how stakeholders will interact with the system, knowing that changes and evolution in the actual workflow may cause a decrease in system performance. Inter-Annotator Agreement Product Classifier Question Link Product 1 0.80 (20) 0.75 (16) 0.50 (16) Product 2 0.50 (20) 0.50 (10) 0.70 (10) Product 3 0.15 (20) 1.00 (3) 1.00 (3) Product 4 0.65 (20) 0.85 (13) 0.69 (13) Product 5 0.65 (20) 0.54 (13) 0.69 (13) Product 6 0.25 (20) 1.00 (4) 1.00 (4) Total 0.50 (120) 0.72 (59) 0.67 (59) Table 6: Inter-Annotator Agreement: Proportion of labels that 3 annotators agreed on. Total N in parenthesis. For ques- tion and link labels, proportions only calculated based on cases for which all 3 annotators labeled as single turn and evaluated quality of corresponding question and link. Three SMEs labeled a subset of twenty cases to determine inter-annotator agreement. The results in Table 6 show that labeling cases as single vs. multi-turn is not a trivial task and for most products, SMEs disagreed widely. Of the cases in which all three annotators agreed to be single-turn, agree- ment on the question and link quality was better but still raises questions about the validity of the training and evalu- ation data. In particular, the low agreement of the provided links can be explained by the fact that more than one link can potentially solve the same question and so neither annota- tor is necessarily wrong. This suggests that for ground truth data, we should consider a list of correct links instead of a single ground truth link for each question. The low agree- ment of single-turn vs. multi-turn labels also potentially ex- plains the lower performance of the classifier model if the model is attempting to learn from potentially conflicting in- formation. RAG Bottlenecks The major bottleneck in RAG systems is the retrieval com- ponent. As shown in Table 4, when given the correct con- text, LLMs can typically generate responses that match the ground truth answers. However, we cannot expect to gen- erate the correct answer if given the wrong contexts which happens for around 60% of the cases (Figure 2). For compar- ison, Google search limited to the corresponding domains indexed by the Milvus database performed worse at 30% R@3 compared to our method at 43% R@3 (See Figure 2). This implies, as other researchers have suggested, that the retrieval component in RAG is not a solved problem by any means. (Petroni et al. 2024; Cuconasu et al. 2024) 7 Related Work LLM-Based AIOps As software systems become more complex, Artificial In- telligence for IT Operations (AIOps) methods are widely used to manage software system failures and ensure the high availability and reliability of large-scale distributed software systems (Zhang et al. 2024). Machine learning and natural language processing methods such as LLMs have been used in AIOps for incident triage, data pre-processing, failure per- ception, root cause analysis, and auto remediation (Zhang et al. 2024). Historically and currently, many of these tasks including both incident triage and auto remediation have been treated as classification problems: for example, Ahmed et al. (2023a) treats incident resolution as a classification task matching incident tickets to a relatively small number of possible resolutions using the BERT model and embeddings. With the rise of better performing generative AI models, re- searchers have moved towards using these models to gener- ate solutions in the auto remediation task using prompting strategies (Ahmed et al. 2023b; Liu et al. 2024) or creating a model fine-tuned for a variety of IT tasks such as question- answering (Guo et al. 2023). Our use case can be considered an example of Zhang et al. (2024)’s ”Assisted Questioning”, an auto remediation task that involves utilizing LLMs to aid operations personnel in answering system-related queries. As far as we are aware, no current work exists that leverages a RAG-based approach to solve this task, although one does exist for a similar IT task of root cause analysis (Chen et al. 2024). The RAG- based approach was taken in lieu of fine-tuning such as in Guo et al. (2023)’s OWL model because of issues in real- world deployment due to its resource-intensive nature which requires significant computational resources and the inter- pretation of model decisions. Likewise, we discounted us- ing a simple prompting approach without retrieval because of client limitations in model choice that prevented us from using larger models. Retrieval and Retrieval Augmented Generation Methods for finding the relevant documents or pas- sages to answer a user query are typically divided into sparse (Robertson and Zaragoza 2009) and dense retrieval systems (Zhao et al. 2024). Our retrieval starts with a Mil- vus (Wang et al. 2021) vector database that has indexed the software support documentation with a general purpose dense embedding that serves multiple services. In our so- lution, we then make use of a popular optimization by re- ranking the first-pass result to obtain a more appropriate ranking for our particular application (Nogueira and Cho 2020; Han et al. 2020), giving us the results of a special- purpose index while still retaining the benefits of a central indexing service. Other popular improvement methods in- clude combining sparse and dense embeddings into a hybrid system (Luan et al. 2021). Retrieval augmented generation was developed to address cases in which large language models have not learned and stored domain knowledge through pre-training. Originally implemented by combining dense retrieval and a fine-tuned BART model, the method generalizes well to larger genera- tive models (Fan et al. 2024). The quality of the answer gen- eration can be improved through prompt engineering, refin- ing how the specific generative model is prompted with the context, question, and other instructions. (Liu et al. 2023). However, it has been noted that the retrieval component in RAG has been understudied in comparison to the genera- tion component despite its substantial impact on the final performance of such hybrid systems. (Petroni et al. 2024; Cuconasu et al. 2024) 8 Conclusion We were able to deliver a first working version of an IT product solution recommendation system employing RAG. To our knowledge, this is the first published architecture and performance metrics of such a RAG system in this do- main. Our system also differentiates itself with a few innova- tions including a component for classifying support cases as single-turn and a component for distilling verbose case de- scriptions into a query suitable for retrieval. We demonstrate that smaller models leveraging retrieved domain context can match or out-perform substantially larger models both with and without context (Ahmed et al. 2023b; Liu et al. 2024) particularly with knowledge infusion through fine-tuning. However, there are still many challenges in implementing RAG for IT support incident resolution including improving retrieval performance. We are collecting feedback from sup- port specialists using our current deployment, and intend to incorporate their advice into future improvements. References Ahmed, S.; Singh, M.; Doherty, B.; Ramlan, E.; Harkin, K.; Bucholc, M.; and Coyle, D. 2023a. Knowledge-based Intel- ligent System for IT Incident DevOps. In 2023 IEEE/ACM International Workshop on Cloud Intelligence & AIOps (AIOps), 1–7. Los Alamitos, CA, USA: IEEE Computer So- ciety. Ahmed, T.; Ghosh, S.; Bansal, C.; Zimmermann, T.; Zhang, X.; and Rajmohan, S. 2023b. Recommending Root-Cause and Mitigation Steps for Cloud Incidents Using Large Lan- guage Models. In Proceedings of the 45th International Conference on Software Engineering, ICSE ’23, 1737–1749. IEEE Press. ISBN 9781665457019. Almazrouei, E.; Alobeidli, H.; Alshamsi, A.; Cappelli, A.; Cojocaru, R.; Debbah, M.; Goffinet, E.; Heslow, D.; Lau- nay, J.; Malartic, Q.; Noune, B.; Pannier, B.; and Penedo, G. 2023. Falcon-40B: an open large language model with state-of-the-art performance. Chen, Y .; Xie, H.; Ma, M.; Kang, Y .; Gao, X.; Shi, L.; Cao, Y .; Gao, X.; Fan, H.; Wen, M.; Zeng, J.; Ghosh, S.; Zhang, X.; Zhang, C.; Lin, Q.; Rajmohan, S.; Zhang, D.; and Xu, T. 2024. Automatic Root Cause Analysis via Large Lan- guage Models for Cloud Incidents. In Proceedings of the Nineteenth European Conference on Computer Systems, Eu- roSys ’24, 674–688. New York, NY , USA: Association for Computing Machinery. ISBN 9798400704376. Cuconasu, F.; Trappolini, G.; Siciliano, F.; Filice, S.; Cam- pagnano, C.; Maarek, Y .; Tonellotto, N.; and Silvestri, F. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR ’24, 719–729. New York, NY , USA: Association for Computing Machinery. ISBN 9798400704314. Fan, W.; Ding, Y .; Ning, L.; Wang, S.; Li, H.; Yin, D.; Chua, T.-S.; and Li, Q. 2024. A Survey on RAG Meet- ing LLMs: Towards Retrieval-Augmented Large Language Models. arXiv:2405.06211. Guo, H.; Yang, J.; Liu, J.; Yang, L.; Chai, L.; Bai, J.; Peng, J.; Hu, X.; Chen, C.; Zhang, D.; Shi, X.; Zheng, T.; Zheng, L.; Zhang, B.; Xu, K.; and Li, Z. 2023. OWL: A Large Language Model for IT Operations. Han, S.; Wang, X.; Bendersky, M.; and Najork, M. 2020. Learning-to-Rank with BERT in TF-Ranking. arXiv:2004.08476. IBM Research, I. W. 2024. IBM slate-125m-english-rtrvr-v2 model card. https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze- data/fm-slate-125m-english-rtrvr-v2-model- card.html?context=wx&audience=wdp. Accessed: 2024-08-16. Jiang, A. Q.; Sablayrolles, A.; Roux, A.; Mensch, A.; Savary, B.; Bamford, C.; Chaplot, D. S.; de las Casas, D.; Hanna, E. B.; Bressand, F.; Lengyel, G.; Bour, G.; Lample, G.; Lavaud, L. R.; Saulnier, L.; Lachaux, M.-A.; Stock, P.; Subramanian, S.; Yang, S.; Antoniak, S.; Scao, T. L.; Gervet, T.; Lavril, T.; Wang, T.; Lacroix, T.; and Sayed, W. E. 2024. Mixtral of Experts. arXiv:2401.04088. Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V .; Goyal, N.; K ¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt ¨aschel, T.; Riedel, S.; and Kiela, D. 2020. Retrieval-augmented gen- eration for knowledge-intensive NLP tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS ’20. Red Hook, NY , USA: Curran Associates Inc. ISBN 9781713829546. Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig, G. 2023. Pre-train, Prompt, and Predict: A Systematic Sur- vey of Prompting Methods in Natural Language Processing. ACM Comput. Surv., 55(9). Liu, Y .; Pei, C.; Xu, L.; Chen, B.; Sun, M.; Zhang, Z.; Sun, Y .; Zhang, S.; Wang, K.; Zhang, H.; Li, J.; Xie, G.; Wen, X.; Nie, X.; Ma, M.; and Pei, D. 2024. OpsEval: A Compre- hensive IT Operations Benchmark Suite for Large Language Models. arXiv:2310.07637. Luan, Y .; Eisenstein, J.; Toutanova, K.; and Collins, M. 2021. Sparse, Dense, and Attentional Representations for Text Retrieval. Transactions of the Association for Compu- tational Linguistics, 9: 329–345. Nogueira, R.; and Cho, K. 2020. Passage Re-ranking with BERT. arXiv:1901.04085. Petroni, F.; Siciliano, F.; Silvestri, F.; and Trappolini, G. 2024. IR-RAG @ SIGIR24: Information Retrieval’s Role in RAG Systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’24, 3036–3039. New York, NY , USA: Association for Computing Machinery. ISBN 9798400704314. Robertson, S.; and Zaragoza, H. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr., 3(4): 333–389. Sudalairaj, S.; Bhandwaldar, A.; Pareja, A.; Xu, K.; Cox, D. D.; and Srivastava, A. 2024. LAB: Large-Scale Align- ment for ChatBots. arXiv:2403.01081. Wang, J.; Yi, X.; Guo, R.; Jin, H.; Xu, P.; Li, S.; Wang, X.; Guo, X.; Li, C.; Xu, X.; Yu, K.; Yuan, Y .; Zou, Y .; Long, J.; Cai, Y .; Li, Z.; Zhang, Z.; Mo, Y .; Gu, J.; Jiang, R.; Wei, Y .; and Xie, C. 2021. Milvus: A Purpose-Built Vector Data Management System. In Proceedings of the 2021 Interna- tional Conference on Management of Data , SIGMOD ’21, 2614–2627. New York, NY , USA: Association for Comput- ing Machinery. ISBN 9781450383431. Zhang, L.; Jia, T.; Jia, M.; Wu, Y .; Liu, A.; Yang, Y .; Wu, Z.; Hu, X.; Yu, P. S.; and Li, Y . 2024. A Survey of AIOps for Failure Management in the Era of Large Language Models. Zhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and Artzi, Y . 2020. BERTScore: Evaluating Text Generation with BERT. arXiv:1904.09675. Zhao, W. X.; Liu, J.; Ren, R.; and Wen, J.-R. 2024. Dense Text Retrieval Based on Pretrained Language Models: A Survey. ACM Trans. Inf. Syst., 42(4).