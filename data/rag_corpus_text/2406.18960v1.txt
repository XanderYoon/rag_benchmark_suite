arXiv:2406.18960v1 [cs.IR] 27 Jun 2024 A Surprisingly Simple yet Eﬀective Multi-/Q_uery Rewriting Method for Conversational Passage Retrieval Ivica Kostric University of Stavanger Stavanger, Norway ivica.kostric@uis.no Krisztian Balog University of Stavanger Stavanger, Norway krisztian.balog@uis.no ABSTRACT Conversational passage retrieval is challenging as it often requires the resolution of references to previous utterances and nee ds to deal with the complexities of natural language, such as coreference and ellipsis. To address these challenges, pre-trained seq uence-to- sequence neural query rewriters are commonly used to genera te a single de-contextualized query based on conversation hist ory. Pre- vious research shows that combining multiple query rewrite s for the same user utterance has a positive eﬀect on retrieval per for- mance. We propose the use of a neural query rewriter to gener- ate multiple queries and show how to integrate those queries in the passage retrieval pipeline eﬃciently. The main strengt h of our approach lies in its simplicity: it leverages how the beam se arch algorithm works and can produce multiple query rewrites at n o additional cost. Our contributions further include devisi ng ways to utilize multi-query rewrites in both sparse and dense ﬁrs t-pass retrieval. We demonstrate that applying our approach on top of a standard passage retrieval pipeline delivers state-of-the-art perfor- mance without sacriﬁcing eﬃciency. CCS CONCEPTS • Information systems → Query reformulation. KEYWORDS Conversational search; Conversational passage retrieval ; Neural query rewriting ACM Reference Format: Ivica Kostric and Krisztian Balog. 2024. A Surprisingly Simple yet Ef- fective Multi-Query Rewriting Method for Conversational Passage Re - trieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGI R ’24), July 14–18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3626772.3657933 1 INTRODUCTION The main objective of a conversational search system is to ef - fectively retrieve relevant answers to a wide range of infor ma- tion needs expressed in natural language [1]. A major diﬃcul ty Permission to make digital or hard copies of all or part of thi s work for personal or classroom use is granted without fee provided that copies ar e not made or distributed for proﬁt or commercial advantage and that copies bear this n otice and the full cita- tion on the ﬁrst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is pe rmitted. To copy other- wise, or republish, to post on servers or to redistribute to l ists, requires prior speciﬁc permission and/or a fee. Request permissions from permissi ons@acm.org. SIGIR ’24, July 14–18, 2024, Washington, DC, USA © 2024 Copyright held by the owner/author(s). Publication r ights licensed to ACM. ACM ISBN 979-8-4007-0431-4/24/07 https://doi.org/10.1145/3626772.3657933 lies in the conversational nature of the task, namely, that q ueries are often not standalone and need to be interpreted in the con - text of the user’s previous queries as well as the system’s an - swers to those [35]. Commonly, query rewriting (QR) addresses this by employing neural generative models to produce a sing le de-contextualized query at each conversation turn [9, 18, 3 3], and then feed that query to a retrieval pipeline. While this work s well in many cases, the rewritten query may incorrectly capture t he underlying intent, which leads to the retrieval of non-rele vant an- swers. The challenge arises from the discrete generation pr ocess, which does not accurately capture the underlying probabili ties or importance of terms. In this paper, we seek to improve retrieval performance by ge n- erating multiple queries and modeling the importance of ter ms based on their presence across the queries. We leverage the b eam search algorithm, commonly used in neural QR [10, 19]. Inste ad of keeping track of only the highest-likelihood sequence in a g reedy fashion, the algorithm tracks and considers the best /u1D458sequences at each generation step. We utilize the fact that the token probabilities are already computed in order to produce multiple rewrites a t no additional cost. Thus, the only modiﬁcation we need to make to the original beam search algorithm is to return all tracked sequ ences and their associated probabilities, as opposed to the singl e most probable sequence. The elegance of this method lies in its si mplic- ity; it is computationally inexpensive yet remarkably eﬀec tive. The main research question driving our investigation is: How can we eﬀectively and eﬃciently utilize multiple query rewr ites in conversational passage retrieval? To answer this question, we take into consideration that retrieval can be performed using ei ther sparse or dense retrieval methods. Sparse retrieval typica lly em- ploys pseudo-relevance feedback techniques to expand the q uery and bridge the vocabulary gap. Our method eﬀectively perfor ms both term-importance estimation and query expansion to rep re- sent the underlying information need better and improves MR R by 1.06–6.31 percentage points compared to using a single qu ery rewrite. Dense retrieval, based on contextual neural language mod- els, works better with natural language queries (in contras t to bag-of-words models of sparse retrieval). However, it is co mputa- tionally expensive and would scale linearly with the number of rewrites, rendering it impractical. Instead, we represent all query rewrites jointly by merging them into a single vector repres enta- tion in the learned embedding space by weighted average pool ing. Our method outperforms a single-query retrieval by 3.52–4. 45 per- centage points in absolut MRR score. In summary, the main contribution of this paper is a conversa - tional multi-query rewriting method, CMQR, that can be utilized in conversational passage retrieval and applied on top of any pipeline SIGIR ’24, July 14–18, 2024, Washington, DC, USA Ivica Kostri c and Krisztian Balog that uses generative QR. The novelty of our approach is twofold: (1) it generates multiple query rewrites at no extra cost compar ed to current neural QR approaches, (2) it eﬀectively utilizes th e gener- ated rewrites in both sparse and dense retrieval. Using the Q ReCC dataset for evaluation, we show that applying our method on t op of any pipeline featuring generative QR improves performance, re- sulting in state-of-the-art results. All resources developed for this paper (source code, query rewrites, and rankings) can be found at https://github.com/iai-group/sigir2024-multi-query- rewriting. 2 RELATED WORK We focus on the task of conversational passage retrieval, where the goal is to retrieve relevant passages to the user query from a large passage collection. Unlike generative approaches, here, h allucina- tions are ensured not to occur as answers can only come from th e collection. While there is some variety between retrieval p ipeline architectures for conversational search, the vast majorit y include QR, followed by a ﬁrst-pass candidate selection stage and th en by one or more re-ranking steps [13, 15, 19, 29, 33]. This setup p ro- vides a good balance between eﬃciency and eﬀectiveness [3]. We demonstrate the beneﬁts of our approach to ﬁrst-pass retrie val, us- ing both spare and dense retrieval methods. Automatic QR has a long tradition in IR, predominantly in query expansion, and has been shown eﬀective in a range of tasks [4]. Conversational query rewriting (CQR) aims to gen er- ate clear, de-contextualized queries from raw inputs by con sider- ing conversation context and addressing coreferences, ell ipsis [6], and topic transitions [29]. Crucially, it helps clarify and reﬁne the user’s needs in a dialogue setting [25]. Recently, neural re writ- ing methods leveraging large pre-trained language models, like GPT-2 [13, 28] and T5 [19], have become prevalent. While neur al rewriting methods tend to outperform traditional query expansion techniques [7, 8], the best results are achieved by combinin g the two [13, 19, 28]. Two previous CQR studies are particularly relevant to our work. Lin et al. [19] propose two query reformulation methods: one fo- cused on term importance and another on making human-like queries. They show that fusing ranked lists after separate r etrieval stages for both queries increases recall. However, fusing t he two lists after re-ranking showed no improvement. The main diﬀerence between this work and ours is that we generate multiple natur al language query rewrites. Mo et al. [23] presents two neural m od- els: one trained on rewriting queries and another to producepoten- tial answers to the query, the idea being that pre-trained la nguage models can directly answer questions by leveraging their in ternal knowledge. At inference, these potential answers are used t o ex- pand the query. Our approach diﬀers in that we use a single mod el to estimate term importance and pick expansion terms. In another line of recent research, deep neural networks are used to generate query embeddings directly from context [21 , 22, 34]. These embeddings, used in conjunction with dense re - trieval, can handle intricate conversational contexts mor e eﬀec- tively. While they integrate seamlessly with advanced neural mod- els for IR, they require systems capable of interpreting the m, po- tentially demanding more computational resources or addit ional processing steps. In contrast, the traditional approach of QR oﬀers better interpretability and ﬂexibility, translating complex conversa- tional contexts into standalone, understandable queries that can be processed directly and eﬃciently with existing retrieval p ipelines. 3 METHOD This section presents our method for generating multiple qu ery rewrites in Section 3.1. The integration of those rewrites i n sparse and dense retrievals is described in Sections 3.2 and 3.3, re spec- tively. 3.1 Conversational Multi-Query Rewriting 3.1.1 Problem Statement. Conversational query rewriting (CQR) is the task of generating an informative context-independent query from a raw query (i.e., context-dependent user utterance) based on conversation context (i.e., history). Formally, we let /u1D45E/u1D456 be the raw query at conversation turn /u1D456, and /u1D43B= ⟨/u1D45E1, /u1D45F1, /u1D45E2, /u1D45F2, ..., /u1D45E /u1D456− 1, /u1D45F/u1D456− 1⟩ be the conversation history up to that point, where /u1D45F/u1D457 is a re- sponse provided by the system to the /u1D457th query ( /u1D457∈ [ 1../u1D456 − 1]) . A context-independent query ˆ /u1D45E/u1D456 is to created from the raw query /u1D45E/u1D456 by considering the conversation history up to that point: ˆ/u1D45E/u1D456 = /u1D45F/u1D452/u1D464/u1D45F/u1D456/u1D461/u1D452( ˆ/u1D45E1, /u1D45F1, ˆ/u1D45E2, /u1D45F2, . . . , ˆ/u1D45E/u1D456− 1, /u1D45F/u1D4561, /u1D45E/u1D456) . The rewritten query ˆ/u1D45E/u1D456 is considered self-contained and can be used downstream in v ari- ous components of the retrieval pipeline. 3.1.2 Motivation. The majority of recent approaches employ gen- erative neural models for CQR [15]. However, these models of ten fail to ﬁnd omitted information or detect topic shifts in lon ger con- versations [31]. In some cases, query rewrites introduce ir relevant terms, while in other cases, relevant terms are missing (aki n to the notion of topic drift in pseudo relevance feedback [20, 27]) . We hypothesize that it is often too challenging to accurately c apture the user’s information need in a single query rewrite. There fore, instead of returning a single most likely rewrite, we propos e to re- turn the top /u1D45Bquery rewrites generated with the same model and then utilize these rewrites in all stages of the retrieval pi peline. More speciﬁcally, generative neural approaches to CQR com- monly use the beam search algorithm [11]. According to this t ech- nique, the probability scores of the /u1D458most likely sequences are kept while generating a rewrite. When generation ﬁnishes, t he se- quence with the highest score is returned. 3.1.3 Conversational Multi-/Q_uery Rewriting. Motivated by the above, we propose conversational multi-query rewriting (CMQR), which uses a ﬁne-tuned generative language model to generate the top /u1D45Bquery rewrites at each turn /u1D456, ˆ/u1D45E1 /u1D456, ˆ/u1D45E2 /u1D456, . . . , ˆ/u1D45E/u1D45B /u1D456 , according to their beam search score. Each query rewrite ˆ/u1D45E/u1D457 /u1D456 has an associated rewrite score: /u1D445/u1D446 ( ˆ/u1D45E /u1D457 /u1D456) = /u1D443( ˆ/u1D45E /u1D457 /u1D456| /u1D43B) = /parenlefttpA /parenleftexA /parenleftexA /parenleftbtA | ˆ/u1D45E/u1D457 /u1D456|/productdisplay.1 /u1D459=1 /u1D443( /u1D461/u1D459| /u1D461/u1D459− 1, . . . , /u1D461 1, /u1D43B) ) /parenrighttpA /parenrightexA /parenrightexA /parenrightbtA 1 | ˆ/u1D45E/u1D457 /u1D456| , where /u1D4611, . . . , /u1D461/u1D459are the predicted tokens, | ˆ/u1D45E/u1D457 /u1D456| is the sequence length of the /u1D457th query rewrite, and /u1D43Bis the conversation history (cf. Sec- tion 3.1.1). Considering that RS is the product of the probab ilities of all terms in a sequence, length normalization is applied t o avoid the query rewriters’ tendency to generate very short rewrit es. A Surprisingly Simple yet Eﬀective Multi-/Q_uery Rewriting Met hod for Conversational Passage Retrieval SIGIR ’24, July 14– 18, 2024, Washington, DC, USA Due to the quadratic complexity with respect to the input siz e, a common practice is to limit the input to a maximum of 512 to- kens [18]. To accommodate this restriction, we limit the con text to the previously rewritten utterances, ⟨ ˆ/u1D45E1, . . . , ˆ/u1D45E/u1D456− 1⟩, and the last system response, /u1D45F/u1D456− 1. We do not rewrite the very ﬁrst user ut- terance of a conversation under the assumption that it is alr eady self-contained and states the necessary context (i.e., ˆ/u1D45E1 = /u1D45E1). Next, we discuss how to utilize multiple query rewrites in va ri- ous components of a retrieval pipeline. 3.2 Sparse Retrieval Sparse retrieval relies on a bag-of-words text representat ion, where each query term contributes to the document relevance es- timate according to some scoring function, which is general ly of the form /u1D460/u1D450/u1D45C/u1D45F/u1D452( /u1D45E, /u1D451) = /summationtext.1 /u1D461∈/u1D45E /u1D464/u1D461,/u1D45E × /u1D464/u1D461,/u1D451 , where /u1D464/u1D461,/u1D45E and /u1D464/u1D461,/u1D451 are the term weights associated with query /u1D45Eand document /u1D451, respec- tively. Our interest is in setting the term query weights, /u1D464/u1D461,/u1D45E . In the most commonly used retrieval scoring functions (e.g., B M25), this weight is taken to be the frequency of the term in the quer y, i.e., /u1D464/u1D461,/u1D45E = /u1D450( /u1D461, /u1D45E) . In our approach, we construct a weighted bag- of-words query from all /u1D45Brewrites, where we set the weights for each term as the beam search score, i.e., /u1D445/u1D446( ˆ/u1D45E/u1D457 /u1D456) . For each unique term in such obtained collection of terms, the term weights f rom all rewrites are summed up and normalized. Eﬀectively, the method performs both query expansion and a re- estimation of term importance based on multiple query rewri tes. A similarity can be drawn to relevance feedback algorithms l ike RM3 [16], where two weighted queries are interpolated: the o rigi- nal query and the relevance language model query. The diﬀere nce is, here, we interpolate diﬀerent queries extracted from co nversa- tional context instead of retrieved documents and do not ass ign a pre-determined portion of the total weight mass to the orig inal query terms. Importantly, our method is seen as complementa ry to relevance feedback and can be combined with it. 3.3 Dense Retrieval Dense retrieval diﬀers from sparse retrieval in that it aims to com- pute a relevance score based on the similarity between queries and documents represented in a continuous embedding space inst ead of matching on exact terms. In the simplest form, this score c an be a dot product of the query and the document embedding vec- tors: /u1D460/u1D450/u1D45C/u1D45F/u1D452( /u1D45E, /u1D451) = ℎ/u1D45E · ℎ/u1D451 , where ℎ/u1D45E and ℎ/u1D451 are the learned query and document embedding vectors, respectively. We note that the learned embedding vectors can be pre-computed and stored fo r all documents in the collection, requiring only the computation of the query embedding vector at retrieval time. Given /u1D45Brewrites with associated weights, we ﬁrst generate em- beddings for all rewrites separately and scale them accordi ng to the associated weights. Then, the scaled embeddings are sum med up into a single vector ( ℎ/u1D45E/u1D456) that can be used in a regular dense retrieval system. Formally, the query representation at turn /u1D456is ob- tained by: ℎ/u1D45E/u1D456= /u1D45B/summationdisplay.1 /u1D457=1 /u1D452/u1D45B/u1D450/u1D45C/u1D451/u1D452/u1D45E ( ˆ/u1D45E/u1D457 /u1D456) /u1D445/u1D446( ˆ/u1D45E/u1D457 /u1D456) . Essentially, the ﬁnal query is a weighted centroid of the que ry rewrites. This adds robustness to dense retrieval as the cen ter of mass of multiple query rewrites will likely correspond better to the user’s information need than a single rewrite would. 4 EXPERIMENTAL SETUP We present the datasets we use in our experimental evaluatio n, in- troduce our baselines, and provide implementation details . 4.1 Dataset & Evaluation Metrics Following previous work [23, 30, 32], we use the QReCC [2] dataset, which contains 14k conversations with 80k question-answer pairs, split into training and test sets (63.5k and 16.4k, respecti vely). The dialogues are based on questions from QuAC [5], TREC CAsT 2019 [6], and Google Natural Questions (NQ) [14], with TREC CAsT appearing only in the test set. Following Ye et al. [32], test instances lacking valid gold passage labels are excluded fr om our analysis. Consequently, our dataset comprises 8,209 test i nstances, distributed as 6,396 for QuAC, 1,442 for NQ, and 371 for TREC- CAsT. For a comprehensive evaluation, we present experimen tal results not only on the overall dataset but also on each subse t. We use mean reciprocal rank (MRR), mean average precision (MAP), and Recall@10 (R@10) as our evaluation metrics. 4.2 Baselines We consider the following baselines for comparison: (1) Manual rewrite: Manually rewritten queries provided by the dataset. (2) T5QR/u1D440/u1D44E/u1D45B/u1D462/u1D44E/u1D459[18]: A strong T5-based [26] QR model. (3) Con- QRR [30]: A T5-based model, optimized for retrieval performanc e using reinforcement learning. (4) ConvGQR [23]: An approach employing two T5-based models: one creates a de-contextual ized query rewrite, the other predicts an answer to the query. The outputs are merged into a single query used for retrieval. (5 ) LLM/u1D44E/u1D451ℎ/u1D45C/u1D450[32]: An LLM query rewrite followed by an LLM query editor in an ad-hoc retrieval pipeline. The authors use Chat GPT 3.5 as their LLM. (6) T5QR/u1D43F/u1D43F/u1D440[32]: A sample of 10k datapoint is taken from the training set and run through the same approach as (4). The outputs are used to train a smaller, distilled model . For each variant of the retrieval pipeline, we use the same T5 - based QR approach, with a beam width of /u1D458= 10, but consider only the top rewrite. Wherever possible, i.e., code/model i s made publicly available, we reproduce results on our system usin g V100 GPUs. Otherwise, we report the numbers from the original pap ers (indicated by † ). 4.3 Implementation Details For QR, we ﬁne-tune a T5 model [26] starting from a t5-base1 checkpoint. We set the beam width to /u1D458= 10 for both single-query and multi-query approaches, as this was found to produce hig h- quality rewrites in [19]. For sparse retrieval, following [2], we employ the Pyserini [17] toolkit and use BM25 for retreival with hyparameters /u1D4581 = 0. 82 and /u1D44F= 0. 68. We generate dense embeddings using a GTR [24] 1https://huggingface.co/t5-base SIGIR ’24, July 14–18, 2024, Washington, DC, USA Ivica Kostri c and Krisztian Balog Table 1: Performance of sparse and dense retrieval with QR me thods. Bold and underlined indicate the best and second-bes t results, respectively. ∗ denotes signiﬁcant improvements with a t-test at p < 0.05 of C MQR over its single-query counterpart. Method QReCC QuAC NQ TREC-CAsT MRR MAP R@10 MRR MAP R@10 MRR MAP R@10 MRR MAP R@10 Sparse (BM25) Manual rewrite 39.81 38.45 62.65 40.32 38.98 62.90 40.78 39. 05 63.80 27.34 27.04 53.77 T5QR/u1D440/u1D44E/u1D45B/u1D462/u1D44E/u1D459 31.03 29.86 50.17 30.75 29.60 49.77 34.06 32.51 52.79 24.15 2 3.91 46.90 ConQRR† 38.30 – 60.10 39.50 – 61.60 37.80 – 58.00 19.80 – 43.50 ConvGQR 49.18 47.66 68.01 51.34 49.82 70.10 45.57 43.85 64.06 25.91 25.20 47.26 LLM/u1D44E/u1D451ℎ/u1D45C/u1D450† 49.39 47.89 67.01 53.01 51.52 70.46 41.57 39.69 59.63 17.43 17.08 36.25 T5QR/u1D43F/u1D43F/u1D440 46.72 45.19 64.00 50.13 48.64 67.50 39.26 37.14 55.99 16.98 1 6.97 34.64 CMQR(T5QR/u1D440/u1D44E/u1D45B/u1D462/u1D44E/u1D459) 37.34 ∗ 35.99∗ 58.42∗ 37.92∗ 36.57∗ 59.31∗ 38.05∗ 36.46∗ 57.31∗ 24.43 24.07 47.44 CMQR(ConvGQR) 50.24 ∗ 48.79∗ 69.87∗ 52.41∗ 50.98∗ 72.01∗ 46.83∗ 45.02∗ 65.34 26.14 25.78 50.49 CMQR(T5QR/u1D43F/u1D43F/u1D440) 50.73∗ 49.2∗ 69.25∗ 53.96∗ 52.50∗ 72.27∗ 44.11∗ 41.95∗ 62.40∗ 20.78∗ 20.46∗ 43.80∗ Dense (GTR) Manual rewrite 43.15 41.27 66.12 40.67 38.92 64.59 54.01 51.25 73.13 43.74 42.98 65.23 T5QR/u1D440/u1D44E/u1D45B/u1D462/u1D44E/u1D459 36.08 34.41 56.95 33.70 32.16 55.36 46.11 43.65 63.50 38.11 3 7.30 58.76 ConQRR† 41.80 – 65.10 41.60 – 65.90 45.30 – 64.10 32.70 – 55.20 ConvGQR 42.18 40.43 63.39 41.21 39.55 63.04 49.20 46.80 67.6 3 31.51 30.88 52.96 LLM/u1D44E/u1D451ℎ/u1D45C/u1D450† 44.99 43.19 67.34 45.21 43.48 68.30 47.64 45.20 67.27 30.91 3 0.48 51.03 T5QR/u1D43F/u1D43F/u1D440 42.46 40.67 64.47 42.78 41.06 65.61 44.78 42.29 63.48 28.02 2 7.55 48.65 CMQR(T5QR/u1D440/u1D44E/u1D45B/u1D462/u1D44E/u1D459) 40.53 ∗ 38.73∗ 63.15∗ 38.72∗ 37.02∗ 62.12∗ 48.50∗ 46.01∗ 67.67∗ 40.68∗ 39.91∗ 63.21∗ CMQR(ConvGQR) 45.82 ∗ 43.96∗ 69.75∗ 45.00∗ 43.20∗ 70.00∗ 51.95∗ 49.50∗ 71.17∗ 36.11∗ 35.50∗ 59.97∗ CMQR(T5QR/u1D43F/u1D43F/u1D440) 45.98∗ 44.17∗ 69.31∗ 45.82∗ 44.08∗ 70.02∗ 49.58∗ 47.14∗ 69.18∗ 34.69∗ 34.09∗ 57.64∗ model from a publicly available checkpoint. 2 The implementation is based on Faiss [12]. 5 RESULTS The evaluation results on the QReCC test set along with a brea k- down of speciﬁc subsets are reported in Table 1. The table is s plit into two groups: Sparse (Top) and dense retrieval (Bottom), with the same query rewriting methods in each group. When multi- ple queries are considered for a given method, it is indicate d by CMQR(*); in all our experiments, we consider 10 query rewrit es. Our main ﬁndings are as follows. First, the CMQR method con- sistently outperforms both its sparse and dense retrieval c ounter- parts across all datasets. This trend highlights CMQR’s eﬀe ctive- ness in improving retrieval metrics. We show signiﬁcant imp rove- ments in the range from 1.06 to 6.31 in sparse retrieval and 3. 52 to 4.45 in dense retrieval in terms of MRR (absolute percenta ge points). The biggest improvement is observed when adding CMQR to the weakest model (T5QR /u1D440/u1D44E/u1D45B/u1D462/u1D44E/u1D459), suggesting that the term im- portance and query expansion methods have a major eﬀect. Thi s observation is further supported by the smallest gain obser ved with ConvGQR, which has integrated query expansion. Second , for both the sparse and dense retrieval groups, our CMQR meth od consistently outperforms all other methods on the overall Q ReCC dataset, achieving the best and second-best results, there by set- ting a new state of the art. The strongest performance of CMQR with T5QR/u1D43F/u1D43F/u1D440, a single t5-base model, is notable for its compact 2https://huggingface.co/sentence-transformers/gtr-t5 -base size compared to previous best-performing methods, speciﬁ cally the dual- t5-base model (ConvGQR) and the two-step LLM model (LLM/u1D44E/u1D451ℎ/u1D45C/u1D450). Ye et al. [32] show LLM /u1D44E/u1D451ℎ/u1D45C/u1D450is 6 times slower than T5QR/u1D43F/u1D43F/u1D440making it impractical in real-world conversational ap- plications. Finally, manual rewrites, representing human eﬀort in query rewriting, show strong performance, especially on th e test- only TREC-CAsT dataset. However, CMQR methods still surpas s these human eﬀorts on the overall QReCC dataset, underscori ng the potential of automated systems to enhance retrieval tas ks. In- terestingly, adding CMQR to the model trained on manual rewrites (T5QR/u1D440/u1D44E/u1D45B/u1D462/u1D44E/u1D459) almost reaches the performance of manual rewrites, while CMQR applied to T5QR/u1D43F/u1D43F/u1D440shows an absolute improvement of 10.91 MRR percentage points compared to the manual rewrit e. 6 CONCLUSION In this paper, we developed a method for generating multiplequery rewrites for conversational search and explored how these c an be incorporated into sparse and dense retrieval. This approach diﬀers from the majority of previous work, where only a single query rewrite is used. We showed how multiple queries can be eﬃcien tly integrated at virtually no extra cost for both sparse and den se re- trieval. Furthermore, we demonstrated that our method can b e ap- plied on top of existing query rewriting methods that employ gen- erative query rewriting, yielding consistent improvement s across all methods and resulting in state-of-the-art performance . A Surprisingly Simple yet Eﬀective Multi-/Q_uery Rewriting Met hod for Conversational Passage Retrieval SIGIR ’24, July 14– 18, 2024, Washington, DC, USA In future work, we plan to employ multi-query rewrites also i n the re-ranking components of multi-stage retrieval pipeli nes and determine automatically the number of rewrites to consider . ACKNOWLEDGMENTS An unrestricted gift from Google partially supported this r esearch. REFERENCES [1] Avishek Anand, Lawrence Cavedon, Matthias Hagen, Hideo Joho, Mark Sander- son, and Benno Stein. 2021. Dagstuhl seminar 19461 on conver sational search: seminar goals and working group outcomes. ACM SIGIR Forum 54, 1 (2021), 3:1–3:11. [2] Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Sha yne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021. Open-Domain Question Answering Goes Conversational via Question Rewriting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa tional Linguistics: Human Language Technologies (NAACL ’21). 520–534. [3] Nima Asadi and Jimmy Lin. 2013. Eﬀectiveness/eﬃciency t radeoﬀs for candi- date generation in multi-stage retrieval architectures. I n Proceedings of the 36th international ACM SIGIR conference on Research and develop ment in information retrieval (SIGIR ’13). 997–1000. [4] Claudio Carpineto and Giovanni Romano. 2012. A Survey of Automatic Query Expansion in Information Retrieval. ACM Comput. Surv. 44, 1 (2012). [5] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yi h, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question Answerin g in Context. In Proceedings of the 2018 Conference on Empirical Methods in Na tural Language Processing (EMNLP ’18). 2174–2184. [6] Jeﬀrey Dalton, Chenyan Xiong, and Jamie Callan. 2019. CA sT 2019: The Con- versational Assistance Track Overview. In In Proceedings of the Twenty-Eighth Text REtrieval Conference (TREC ’19). [7] Jeﬀrey Dalton, Chenyan Xiong, and Jamie Callan. 2020. CA sT 2020: The Conver- sational Assistance Track Overview. In In Proceedings of the Twenty-Ninth Text REtrieval Conference (TREC ’20). [8] Jeﬀrey Dalton, Chenyan Xiong, and Jamie Callan. 2021. TR EC CAsT 2021: The Conversational Assistance Track Overview. InIn Proceedings of the Thirtieth Text REtrieval Conference (TREC ’21). [9] Ahmed Elgohary, Denis Peskov, and Jordan Boyd-Graber. 2 019. Can You Un- pack That? Learning to Rewrite Questions-in-Context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process ing and the 9th In- ternational Joint Conference on Natural Language Processin g (EMNLP-IJCNLP) (EMNLP-IJCNLP ’19). 5918–5924. [10] Jianfeng Gao, Chenyan Xiong, Paul Bennett, and Nick Cra swell. 2022. Neural Approaches to Conversational Information Retrieval. arXi v:arXiv:2201.05176 [11] Alex Graves. 2012. Sequence Transduction with Recurre nt Neural Networks. arXiv:1211.3711 [12] Jeﬀ Johnson, Matthijs Douze, and Hervé Jégou. 2021. Bil lion-Scale Similarity Search with GPUs. IEEE Transactions on Big Data 7, 3 (2021), 535–547. [13] Vaibhav Kumar and Jamie Callan. 2020. Making Informati on Seeking Easier: An Improved Pipeline for Conversational Search. In Findings of the Association for Computational Linguistics: EMNLP 2020 (EMNLP ’20). 3971–3980. [14] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld , Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin , Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming -Wei Chang, An- drew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.Natural Questions: A Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019), 452–466. [15] Weronika Lajewska and Krisztian Balog. 2023. From Base line to Top Performer: A Reproducibility Study of Approaches at the TREC 2021 Conve rsational As- sistance Track. In Advances in Information Retrieval: 45th European Conferen ce on Information Retrieval, ECIR 2023, Dublin, Ireland, Apri l 2–6, 2023, Proceedings, Part III (ECIR ’23). 177–191. [16] Victor Lavrenko and W. Bruce Croft. 2001. Relevance bas ed language models. In Proceedings of the 24th annual international ACM SIGIR confe rence on Research and development in information retrieval (SIGIR ’01). 120–127. [17] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Ya ng, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A Python Toolkit for R eproducible Infor- mation Retrieval Research with Sparse and Dense Representa tions. In Proceed- ings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’21). 2356–2362. [18] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, M ing-Feng Tsai, Chuan- Ju Wang, and Jimmy Lin. 2020. Conversational Question Refor mulation via Sequence-to-Sequence Architectures and Pretrained La nguage Models. arXiv:2004.01909 [19] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, M ing-Feng Tsai, Chuan- Ju Wang, and Jimmy Lin. 2021. Multi-Stage Conversational Pa ssage Retrieval: An Approach to Fusing Term Importance Estimation and Neural Query Rewrit- ing. ACM Transactions on Information Systems 39, 4 (2021), 1–29. [20] Craig Macdonald and Iadh Ounis. 2007. Expertise Drift a nd Query Expansion in Expert Search. In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management (CIKM ’07). 341–350. [21] Kelong Mao, Zhicheng Dou, and Hongjin Qian. 2022. Curri culum Contrastive Context Denoising for Few-shot Conversational Dense Retri eval. In Proceedings of the 45th International ACM SIGIR Conference on Research a nd Development in Information Retrieval (SIGIR ’22). 176–186. [22] Kelong Mao, Zhicheng Dou, Hongjin Qian, Fengran Mo, Xia ohua Cheng, and Zhao Cao. 2022. ConvTrans: Transforming Web Search Session s for Conversa- tional Dense Retrieval. In Proceedings of the 2022 Conference on Empirical Meth- ods in Natural Language Processing (EMNLP ’22) . 2935–2946. [23] Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huan g, and Jian-Yun Nie. 2023. ConvGQR: Generative Query Reformulation for Con versational Search. In Proceedings of the 61st Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers) (ACL ’23). 4998–5012. [24] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernand ez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfe i Yang. 2022. Large Dual Encoders Are Generalizable Retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process ing (EMNLP ’22). 9844–9855. [25] Gustavo Penha, Alexandru Balan, and Claudia Hauﬀ. 2019 . Introducing MANtIS: a novel Multi-Domain Information Seeking Dialogues Dataset. arXiv:1912.04639 [26] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee , Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Ex ploring the Lim- its of Transfer Learning with a Uniﬁed Text-to-Text Transfo rmer. Journal of Machine Learning Research 21, 140 (2020), 1–67. [27] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, a nd Gad Markovits. 2012. Predicting Query Performance by Query-Drift Estimat ion. ACM Trans. Inf. Syst. 30, 2 (2012). [28] Svitlana Vakulenko, Nikos Voskarides, Zhucheng Tu, an d Shayne Longpre. 2021. A Comparison of Question Rewriting Methods for Conversatio nal Passage Re- trieval. In Proceedings of the 43rd European Conference on IR Research (E CIR ’21). 418–424. [29] Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanou las, and Maarten de Ri- jke. 2020. Query Resolution for Conversational Search with Limited Supervision. In Proceedings of the 43rd International ACM SIGIR Conference o n Research and Development in Information Retrieval (SIGIR ’20). 921–930. [30] Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, Hanna neh Hajishirzi, Mari Ostendorf, and Gaurav Singh Tomar. 2022. CONQRR: Conversat ional Query Rewriting for Retrieval with Reinforcement Learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process ing (EMNLP ’22). 10000–10014. [31] Xinyi Yan, Charles L A Clarke, and Negar Arabzadeh. 2021 . WaterlooClarke at the TREC 2021 Conversational Assistant Track. In In Proceedings of the Thirtieth Text REtrieval Conference (TREC ’21). [32] Fanghua Ye, Meng Fang, Shenghui Li, and Emine Yilmaz. 20 23. Enhancing Con- versational Search: Large Language Model-Aided Informati ve Query Rewriting. In Findings of the Association for Computational Linguistics: EMNLP 2023 (EMNLP ’23). 5985–6006. [33] Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul Be nnett, Jianfeng Gao, and Zhiyuan Liu. 2020. Few-Shot Generative Conversational Query Rewriting. In Proceedings of the 43rd International ACM SIGIR Conference o n Research and Development in Information Retrieval (SIGIR ’20). 1933–1936. [34] Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and Zhiyu an Liu. 2021. Few- Shot Conversational Dense Retrieval. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SI- GIR ’21). 829–838. [35] Hamed Zamani, Johanne R. Trippas, Jeﬀ Dalton, and Filip Radlinski. 2023. Con- versational Information Seeking. Found. Trends Inf. Retr. 17, 3-4 (2023), 244–456.