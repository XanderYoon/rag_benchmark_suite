Evaluating the Robustness of Retrieval Pipelines with Query Variation Generators Gustavo Penha g.penha-1@tudelft.nl Delft University of Technology Delft, Netherlands Arthur CÃ¢mara a.barbosacamara@tudelft.nl Delft University of Technology Delft, Netherlands Claudia Hauff c.hauff@tudelft.nl Delft University of Technology Delft, Netherlands ABSTRACT Heavily pre-trained transformers for language modelling, such as BERT, have shown to be remarkably effective for Information Re- trieval (IR) tasks. IR benchmarks evaluate the effectiveness of (neu- ral) ranking models based on the premise that a single query is used to instantiate the underlying information need. However, previous research has shown that (I) queries generated by users for a fixed in- formation need are extremely variable and, in particular, (II) neural models are brittle and often easily make mistakes when tested with adversarial examples, i.e. examples with minimal modifications that do not change its label. Motivated by those observations we aim to answer the following question with our work: how robust are re- trieval pipelines with respect to different variations in queries that do not change the queriesâ€™ semantics? In order to obtain queries that are representative of usersâ€™ querying variability, we first created a taxon- omy based on the manual annotation of transformations occurring in a dataset (specifically UQV100) of user created query variations. For example, from the query â€˜cures for a bald spot â€™ to the variation â€˜cures for baldnessâ€™ we are applying aparaphrasing transformation that replaces words with synonyms. For each syntax-changing cat- egory of our taxonomy, we employ different automatic methods that when applied to a query generate a query variation. We con- duct experiments on two datasets (TREC-DL-2019 and ANTIQUE) and create a total of 2430 query variations from 243 topics across both datasets. Our experimental results for two different IR tasks reveal that retrieval pipelines are not robust to query variations that maintain the content the same, with effectiveness drops of âˆ¼20% on average when compared with the original query as provided in the datasets. Our findings indicate that further work is required to make retrieval pipelines with neural ranking models more robust and that IR collections should include query variations, e.g. using the methods proposed here, for a single information need to better understand models capabilities. The code and datasets are available at https://github.com/Guzpenha/query_variation_generators. 1 INTRODUCTION Heavily pre-trained transformers for language modeling such as BERT [19] have been shown to be remarkably effective for a wide range of Information Retrieval (IR) tasks [41, 44, 58]. Commonly, IR benchmarks organized as part of TREC or other evaluation cam- paigns, evaluate the effectiveness of ranking modelsâ€”neural or otherwiseâ€”based on small sets of topics and their corresponding relevance judgments. Importantly, each topic is typically repre- sented by a single query. However, previous research has shown Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY 2022. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn that queries created by users given a fixed information need may vary widely [6, 63]. In the UQV100 [5] dataset for instance, crowd workers on average created 57.7 unique queries for a given infor- mation need as instantiated as a backstory, e.g. â€œYou have heard quite a lot about cheap computing as being the way of the future, including one recent model called a Raspberry Pi. You start thinking about buying one, and wonder how much they cost. â€ Table 1: Examples of BERT effectiveness drops (nDCG@10 Î”) when we replace the original query from TREC-DL- 2019 by an automatic (except for the first two lines that were produced manually) query variation. We focus here on transformations that change the query syntax , but not its semantics . Original Query Query Variation nDCG@10 Î” popular food in switzerland popular food in zurich gen./specialization cost of interior con- crete flooring concrete flooring finishing aspect change what is theraderm used for what is thrraderm used for misspelling -1.00 (-100%) anthropological defi- nition of environment anthropological definition of environment naturality -0.15 ( -26%) right pelvic pain causes causes pelvic pain right ordering -0.18 ( -46%) define visceral what is visceral paraphrasing -0.26 ( -38%) We thus argue that it is necessary to investigate the robustness of retrieval pipelines in light of query variations (i.e., different ex- pressions of the same information need) that are likely to occur in practice. That different query variations lead to vastly different ranking qualities is anecdotally shown in Table 1 for a vanilla BERT model for ranking [41]. If, for example, the word order of the origi- nal query from TREC-DL-2019 right pelvic pain causes is changed to causes pelvic pain right , the retrieval effectiveness of the resulting ranking drops by 46%. Similarly, paraphrasing define visceral to what is visceral reduces the retrieval effectiveness by 38%. In our work, we quantify the extent to which different retrieval models are susceptible to different types of query variations as mea- sured by their drop in retrieval effectiveness. In contrast to prior works that either analyze behaviour of models when faced with modifications to the documents [32], analyze models through the arXiv:2111.13057v3 [cs.IR] 15 Feb 2022 Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gustavo Penha, Arthur CÃ¢mara, and Claudia Hauff lens of IR axioms [12, 48] or analyze NLP models via general natu- ral language text adversarial examples [23, 49], we instantiate our query variations based on user-created data. Concretely, we manu- ally label a large fraction of UQV100 queries1 and extract six types of frequently occurring query transitions: gen./specialization, as- pect change, misspelling, naturality, ordering and paraphrasingâ€”an example of each is shown in Table 1. The last four of these cate- gories change the query syntax but not its semantics. For each of the syntax-changing categories, we develop automated approaches that enable us to generate query variations of each category for any input query. With these query variation generators in place, we conduct extensive empirical work on the recent TREC-DL-2019 [16] and ANTIQUE [25] datasets to answer the following research ques- tion: Are retrieval pipelines robust to different variations in queries that do not change its semantics? To this end we consider seven ranking approaches: two traditional lexical models (BM25 [50] and RM3 [1]), two neural re-ranking approaches that do not make use of transformers (KNRM [57] and CKNRM [18]) and three transformer- based re-ranking approaches (EPIC [33], BERT [41] and T5 [42]). Additionally, motivated by the fact that certain query variations can improve the retrieval effectiveness compared to using the orig- inal query [8, 10], we study the combination of automatic query variations with rank fusion [15]. Our main findings are as follows: â€¢ The four types of syntax-changing query variations differ in the extent to which they degrade retrieval effectiveness: misspellings have the largest effect (with an average drop of 0.25 nDCG@10 points across seven retrieval models for TREC-DL-2019) while the word ordering has the least effect (with an average drop of nDCG@10 smaller than 0.01 for TREC-DL-2019). â€¢ Different types of ranking models make similar mistakes. For example, effectiveness decreases for models based on transformer language models are higher for naturality query variations com- pared to decreases when using traditional lexical models. â€¢ While rank fusion mitigates the drops in retrieval effectiveness when compared to using a single query variation, it does not achieve the full potential of the combination of query variations. An oracle that always select the best query achieves gains of 0.08 and 0.06 nDCG@10 points on TREC-DL-2019 and ANTIQUE respectively. Our work indicates that more research is required to improve the robustness of retrieval pipelines. Evaluation benchmarks should aim to have multiple query variations for the same information need in order to evaluate whether ranking pipelines are indeed robust, and we provide here a number of methods to automatically generate such query variations for any dataset. 2 RELATED WORK To put our work in context, we now describe prior research into query variations and then move on to research analyzing neural (IR) models. 2.1 Query Variation A number of studies have argued that evaluation in IR tasks should take into account multiple instantiations of the same information 1To our knowledge, UQV100 is the only publicly available dataset that contains a large number of query variations for a set of information needs. need, i.e. query variations, due to their impact on the effectiveness of ranking models [4â€“7, 11, 37, 52, 63]. Zuccon et al. [63] proposed a mean-variance framework to explicitly take into account query variations when comparing different IR systems. Bailey et al. [6] argued that a model should be consistent to different query varia- tions, and proposed a measure of consistency which gives additional information to effectiveness measurements. Besides a better evaluation of models, query variations can also be employed to improve the overall effectiveness of ranking mod- els, for instance by combining the different rankings obtained from them [8, 10] or by modelling relevance of multiple query varia- tions [30]. They have also shown to been helpful for the problem of query performance prediction [60]. Different methods to automatically generate query variations have been proposed. Benham et al. [9] proposed to obtain query expansions through a relevance model which is built by issuing the original query against an external corpora and expanding it with additional terms from the set of external feedback documents. Lu et al. [30] employed a query-url click graph and generated query variations automatically using a two-step backward walk process. Chakraborty et al. [13] generated query variations automatically based on an external knowledge base with a prior term distribution or by building a relevance model in a iterative manner. Our work differs from previous work on automatic query vari- ation generation in the following ways: ( I) our methods do not require access to external corpora, a relevance model or a query- url click graph; (II) we are not concerned with generating queries with the sole purpose of improving effectiveness, but in generating queries that are likely to occur in practice; and ( III) each of our generator methods follows a category of our taxonomy of query variations which allows us to diagnose ranking modelsâ€™ effective- ness by analyzing what types of variations are more detrimental to what ranking models. 2.2 Model Understanding The success of pre-trained transformer-based language models such as BERT [19] and T5 [47] on several IR benchmarksâ€”a comprehen- sive account of the effectiveness gains can be found in [29]â€”has lead to research on understanding their behaviour and the reasons behind their significant gains in ranking effectiveness [12, 32, 43, 46, 61]. CÃ¢mara and Hauff [12] showed that BERT does not adhere to IR axioms, i.e., heuristics that a reasonable IR model should fulfill, through the use of diagnostic datasets. MacAvaney et al. [32] ex- panded on the axiomatic diagnostic datasets [48] with ABNIRML, a framework to understand the behaviour of neural ranking models using three different strategies: measure and match (controlling certain measurements such as relevance or term frequency and changing another), manipulation of the documentsâ€™ text (for exam- ple by shuffling words or replacing it with the query) and through the transfer of Natural Language Processing (NLP) datasets (for example comparing documents that are more/less fluent or formal with inferred queries). We expand on MacAvaney et al. [32]â€™s work by proposing textual manipulationsâ€”unlike previous methods we are inspired by user-created variationsâ€”to the queries instead of the Evaluating the Robustness of Retrieval Pipelines with Query Variation Generators Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY documents and examine the robustness in terms of effectiveness of neural ranking models to such manipulations. A different direction of research in NLP has challenged how well current evaluation schemes through the use of held-out test sets are actually evaluating the desired capabilities of the models. For example, Gardner et al. [23] proposed the manual creation of con- trast setsâ€”small perturbations that preserve artifacts but change the true labelâ€”in order to evaluate the modelsâ€™ decision boundaries for different NLP tasks. They showed that the model effectiveness on such contrast sets can be up to 25% lower than on the original test sets. Inspired by behavioral testing, i.e. validating input out- put behaviour without knowledge about internal structure, from software engineering tests, Ribeiro et al. [49] proposed to test NLP models with three different types of tests: minimum functionality tests (simple examples where the model should not fail), label (such as positive, negative and neutral in sentiment analysis) invariant changes to the input, and modifications to the input with known outcomes. With such tests at hand they were able to find action- able failures in different commercial NLP models that had already been extensively tested. It has also been shown that neural models developed for different NLP tasks can be tricked by adversarial ex- amples [2, 22, 24], i.e. examples with perturbations indiscernible by humans which get misclassified by the model. In terms of queries modifications, [56, 62] found typos to be detrimental to the effec- tiveness of neural rankers. Wu et al. [56] analyzed the robustness of neural rankers with respect to three dimensions: difficult queries from similar distribution, out-of-domain cases, and defense against adversarial operations. Our work differs from the adversarial line of research by evaluating the robustness of models to query modifica- tions that could be generated by humans, i.e. transformations that naturally occur, and not modifications optimized to trick neural models. 3 AUTOMATIC QUERY V ARIATIONS We now first describe in Section 3.1 how we arrived at our query variation categories in a data-driven manner by annotating a large set of user-created query variations from UQV100. We end up with six categories: four that change the syntax (but not the semantics) and two that change the semantics. In our work, we focus on the four syntax-changing categories. In Section 3.2 we subsequently describe our methods to automatically generate query variations for each category of the taxonomy that does not change the query semantics. 3.1 UQV Taxonomy In order to better understand how queries differ when we compare different query variations for the same information need, we resort to analyzing variations from the UQV100 dataset. UQV100 contains query variations for 100 (sub)-topics from the TREC 2013 and 2014 web tracks, written by crowd workers who received a â€œbackstoryâ€ for each topic as a starting point. On average, UQV100 contains 57.7 spelling corrected (corrected by the UQV100 authors using the spelling service of the Bing search engine) query variations per topic. We consider a query variation pair {ğ‘ğ‘–, ğ‘ğ‘— } to be a set of two queries ğ‘ğ‘– and ğ‘ ğ‘— that were provided in UQV100 for the same back- story. In total, 365K such pairs exist; Table 2 (4th column) contains a number of {ğ‘ğ‘–, ğ‘ğ‘— } examples. We sampled 100 query variation pairs from the 365K available ones for manual annotation. Three authors of this paper (the â€œannotatorsâ€) performed an open card sort [55]. The annotators independently sorted the query variation pairs into different piles and named them, each representing a transformation ğ‘‡ that can be applied to ğ‘ğ‘– and then leads to ğ‘ ğ‘— , i.e. ğ‘‡ (ğ‘ğ‘– ) = ğ‘ ğ‘— . Multiple transformations might be applied to ğ‘ğ‘– in order to yield ğ‘ ğ‘— , e.g. ğ‘‡2 (ğ‘‡1 (ğ‘ğ‘– )) = ğ‘ ğ‘— . After the independent sorting step, the different piles were dis- cussed and merged where necessary, which yielded five categories of transformations. Since the UQV100 data used had already been spelling-corrected by its authors, we added the categorymisspellings. The resulting taxonomy can be found in Table 2. It contains a con- crete definition and examples for each of ourâ€”in totalâ€”six cate- gories: (I) generalization or specialization , (II) aspect change , (III) misspelling, (IV) naturality, (V) word ordering and (VI) paraphrasing. We observed two broad types of transformations: transformations that change the semantics of the query and transformations that do not change the semantics. The gen./specialization and aspect change transformations fall into the former type, whereas all other categories fall into the latter. We highlight here that unlike pre- vious categorizations that describe how users revise queries in e-commerce [3, 26], how to generate better queries to substitute the original query [ 28], how users reformulate queries in a ses- sion [27], we study here how to categorize query variations for the same information need which is a related but different problem. Having arrived at our six categories, our annotators then la- beled an additional set of 550 {ğ‘ğ‘–, ğ‘ğ‘— } randomly sampled pairs from UQV100 in order to determine the distribution of these categories in UQV100. Each {ğ‘ğ‘–, ğ‘ğ‘— } was labelled as belonging to one (or more) of the five categories (with the exception of misspelling which, as already stated, had already been corrected by the UQV100 authors). In order to determine the inter-annotator agreement, 25 {ğ‘ğ‘–, ğ‘ğ‘— } pairs were labelled by all three annotators, and 175 pairs were each labelled by a single annotator. The inter-annotator agreement [14] was moderate (Cohenâ€™sğœ… = 0.42); the disagreements were high- est for the naturality and paraphrasing categories. We found that a total of 56 {ğ‘ğ‘–, ğ‘ğ‘— } pairs had more than one category assigned to it2. The resulting distribution is shown in Table 2 (right-most column); the categories of query variations that change the query without changing its semantics account for 57% of all the transfor- mations. In contrast, 43% of query variations are semantic changes. Among the syntax-changing categories, we found naturality to be the most common with 33% of all transformations falling into this category. Having observed that query variations change the syntax, but not the semantics for the majority of cases,we focus in the remainder of our work on syntax-changing query varia- tions. We leave the exploration of query variation generators for gen./specialization and aspect change as future work. 3.2 Query Generators For each of the four syntax-changing categories, we explored differ- ent methods that generate query variations of the specified category. 2For example, the pair {â€œwhat is doctor zhivago all aboutâ€, â€œdr zhivago synopsisâ€} had both paraphrasing and naturality labels, as it goes from a natural language question to a keyword-base question and also paraphrases â€œdoctor [...] all aboutâ€ to â€œdr [...] synopsisâ€ Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gustavo Penha, Arthur CÃ¢mara, and Claudia Hauff Table 2: Taxonomy of query variations derived from a sample of the UQV100 dataset. Last column is the count of each query variation found on UQV100 based on manual annotation of tuples of queries for the same information need. * spelling errors were already fixed for the UQV100 pairs. Category Definition Changes Se- mantics {ğ‘ğ‘–, ğ‘ğ‘— } Examples from UQV100 Count (%) Gen./specialization Generalizes or specializes within the same information need. âœ“ american civil war â†” number of battles in south carolina during civil war 172 (26.34%) Aspect change Moves between related but different as- pects within the same information need. âœ“ what types of spiders can bite you while gar- dening â†” signs of spider bite 111 (17.00%) Misspelling Adds or removes spelling errors. raspberry pi â†” raspeberry pi * Naturality Moves between keyword queries and natural language queries. how does zinc relate to wilsonâ€™s disease â†” zinc wilsonâ€™s disease 118 (18.07%) Ordering Changes the order of words carotid cavernous fis- tula treatment. â†” treatment carotid cav- ernous fistula 37 ( 5.67%) Paraphrasing Rephrases the query by modifying one or more words. cures for a bald spot â†” cures for baldness 215 (32.92%) Table 3: Example of applying each method ğ‘€ for the query â€˜what is durable medical equipment consist of â€™ from TREC-DL-2019 resulting in valid examples. Rightmost columns indicate the total percentage of valid queries by automatic query variation method based on manual annotation of queries from the test sets. Category Method Name ğ‘€(â€˜what is durable medical equipment consist of â€™) TREC-DL-2019 ANTIQUE Misspelling NeighbCharSwap what is durable mdeical equipment consist of 100.00% 99.50% RandomCharSub what is durable medycal equipment consist of 97.67% 91.00% QWERTYCharSub what is durable medical equipment xonsist of 97.67% 98.50% Naturality RemoveStopWords what is durable medical equipment consist of 86.05% 99.50% T5DescToTitle what is durable medical equipment consist of 81.40% 68.00% Ordering RandomOrderSwap medical is durable what equipment consist of 100.00% 100.00% Paraphrasing BackTranslation what is sustainable medical equipment consist of 53.49% 46.50% T5QQP what is durable medical equipment consist of 60.47% 52.50% WordEmbedSynSwap what is durable medicinal equipment consist of 62.79% 62.00% WordNetSynSwap what is long lasting medical equipment consist of 37.21% 35.50% After an initial exploration of different query generator methods for each category, and filtering approaches that did not generate valid variations for the category and approaches that have high correlation with each other, we employed a total of ten different methods. These methods are listed in Table 3, each with an example transformation. We explain each one in more detail in this section. A method ğ‘€ğ¶ receives as input a query ğ‘ and outputs a query variation Ë†ğ‘: ğ‘€ğ¶ (ğ‘) = Ë†ğ‘. While most of the methods can generate multiple variations for a single input query (for example by replacing different words of the same query by synonyms or by including several spelling mistakes), for the experiments in the paper we resort to using a single query variation per method which already yields enough data for analysis (see Â§ 4.1). Inspired by adversarial examples, we aim to make minimal perturbations to the input text when possible, e.g. replace only one word by a synonym, increasing the chances of obtaining valid variations. 3.2.1 Misspelling. The three methods in this category add one spelling error to the query; the query term an error is introduced in is chosen uniformly at random. NeighbCharSwap Swaps two neighbouring characters from a ran- dom query term (excluding stopwords3). RandomCharSub Replaces a random character from a random query term (excluding stopwords) with a randomly chosen new ASCII character. 3We use the NLTK english stopwords list for all the methods; it is available at https: //www.nltk.org/. Evaluating the Robustness of Retrieval Pipelines with Query Variation Generators Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY QWERTYCharSub Replaces a random character of a random query term (excluding stopwords) with another character from the QW- ERTY keyboard such that only characters in close proximity are chosen, replicating errors that come from typing too quickly. 3.2.2 Naturality. The two methods in this category transform natural language queries into keyword queries. RemoveStopWords Removes all stopwords from the query. T5DescToTitle Applies an encoder-decoder transformer model (here we employ T5 [47]) that we fine-tuned on the task of gen- erating the title of a TREC topic title based on the TREC topic description. For example, a title and description tuple from â€˜trec- robust04â€™: â€˜Evidence that rap music has a negative effect on young people.â€™â†’ â€˜Rap and Crimeâ€™. We collect pairs of title and description from eleven datasets available through the IR datasets library [34]: trec-robust04, trec-tb-2004, aquaint/trec-robust-2005, gov/trec- web-2002, ntcir-www-2, ntcir-www-3, trec-misinfo-2019, cord19/trec- covid, dd-trec-2015, dd-trec-2016 and dd-trec-2017. Over- all, we fine-tuned our model on 1322 description/title tuples. 3.2.3 Ordering. In this category, we employ only one basic method to shuffle words as done by previous research on the order of words [32, 45]. RandomOrderSwap Randomly swap two words of the query. 3.2.4 Paraphrasing. The four methods in this category change one or more query terms in the process of paraphrasing. BackTranslation Applies a translation method to the query to a pivot language, i.e. an auxiliary language, and from the pivot language back to the original language of the query, i.e. English. In our experiments we employ the M2M100 [20] model, a multilin- gual model that can translate between any pair of 100 languages, and we use â€˜Germanâ€™as the pivot language, which yielded better resultsâ€”shown by manual inspection of the generated variationsâ€” than the other two languages for which the model had the most data for training (â€˜Spanishâ€™and â€˜Frenchâ€™). This technique has been used before as a way to generate paraphrases [21, 36]. T5QQP Applies an encoder-decoder transformer model (here we employ T5 [47]) that was fine-tuned on the task of generating a paraphrase question from the original question 4. The model employs the Quora Question Pairs5 dataset for fine-tuning, which has 400k pairs of questions like the following: â€˜How do you start a bakery? â€™ â†’ â€˜How can one start a bakery business? â€™. We also tested T5 models fine-tuned for PAWS [59] and the combination of PAWS and Quora Question Pairs, but the manual inspection of the generated queries revealed that T5 fine-tuned for Quora Question Pairs generated a higher number of valid variations. WordEmbedSynSwap Replaces a non-stop word by a synonym as defined by the nearest neighbour word in the embedding space according to a counter fitted-Glove embedding which yields better synonyms than standard Glove embeddings [39]. WordNetSynSwap Replaces a non-stop word by a the first synonym found on WordNet 6. If there are no words with valid synonyms it will not output a valid variation. 4As available here https://huggingface.co/ramsrigouthamg/t5_paraphraser 5https://www.kaggle.com/c/quora-question-pairs 6https://wordnet.princeton.edu/ Table 4: Statistics of the datasets. TREC-DL-2019 ANTIQUE #Q train 367013 2426 #Q valid 5193 - #Q test 43 200 # terms/ Q test 5.51 10.51 # valid query variations 334 1706 4 EXPERIMENTAL SETUP In this section we describe our experimental setup aimed to answer the following research question: are retrieval pipelines robust to different variations in queries that do not change its semantics? 4.1 Datasets We consider the following datasets in our experiments: TREC-DL- 2019 [16] for the passage retrieval task and ANTIQUE [ 25] for non-factoid question answering task, containing 43 and 200 queries respectively in their test sets. For each of the test set queries, we generate one query variation for each of the proposed methods, and we use the manual annotation described in this section ( Â§4.4) to take into account only the valid generated query variations in our experiments. The statistics of the datasets can be found in Table 4. 4.2 Ranking Models We use different ranking models that cover from lexical traditional models (Trad) such as BM25, to neural ranking models (NN) such as KNRM and neural ranking models that employ transformer-based language models (TNN) such as BERT. For all of our experiments, we apply BM25 as a first stage retriever and re-rank the top 100 results with the neural ranking models, which is an established and efficient approach [29]. For BM25 [50] and RM3 [1] we resort to the default hyperpa- rameters and implementation provided by the PyTerrier toolkit [35]. We trained the kernel-based ranking models KNRM [57] and CK- NRM [18] on the training sets of TREC-DL-2019 and ANTIQUE using default settings from the OpenNIR [31] implementation. For the BERT-based methods EPIC [33], an efficiency focused model that encodes query and documents separately, andBERT [41], also known as monoBERT, which concatenates query and the document and makes predictions based on the[CLS] token representation, we fine-tune the bert-base-uncased model for the train datasets. For T5 [47] we use the monoT5 [42] implementation from PyTerrier T5 plugin 7 which has the pre-trained weights for MSMarco [40] by the original authors of monoT5. 4.3 Query Generators Implementation As for our methods of generating query variations, forT5DescToTitle and T5QQP we rely on pre-trained T5 models (t5-base) and we fine- tune them using the Huggingface transformers library [ 54]. For BackTranslation we use the facebook/m2m100_418M pre-trained 7https://github.com/terrierteam/pyterrier_t5 Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gustavo Penha, Arthur CÃ¢mara, and Claudia Hauff model from the transformers library8. For all other methods, we use the implementations from the TextAttack [38] library. 4.4 Quality of Query Generators Given the automatic nature of the methods we introduced, we need to evaluate their quality: how good are these methods at generating query variations that a user would also generate? To this end, we consider two properties of the generated queries: (I) Ë†ğ‘ maintains the same semantics asğ‘, and (II) the syntax difference between ğ‘ and Ë†ğ‘ can be attributed to the category ğ¶. All pairs of ğ‘ and Ë†ğ‘ = ğ‘€ (ğ‘) from the test sets of TREC-DL-2019 (43 queries) and ANTIQUE (200 queries) for each of the 10 automatic variation methods went to the following process. First, we automatically set the variations frommisspelling9 and ordering as valid, since they are rule based transformations to the input. Then all transformations that generate a variation that is identical to the input query ( Ë†ğ‘ = ğ‘€ (ğ‘) = ğ‘) was automatically set to invalid. The annotators (the authors) then annotated independently the remaining 1371 pairs of {ğ‘, Ë†ğ‘} for the two mentioned properties (binary labels). The percentage of queries that are valid (both desired properties) are displayed at the right-most columns of Table 3 for the 10 automatic variation methods used in the paper and all combinations of {ğ‘, Ë†ğ‘} (2430). We find the methods in the paraphrasing category to yield the largest percentage of invalid query variations: fewer than 38% of query variations generated via WordNetSynSwap are valid. A man- ual inspection of the invalid queries reveal the following insights: (I) T5DescToTitle at times removes query terms that are impor- tant for the query and thus change its semantics (e.g. â€˜ if i had a bad breath what should i do â€™â†’ â€˜if i had a â€™ ), (II) BackTranslation and T5QQP methods can generate an identical copy of the input query which was automatically labelled as invalid (e.g. â€˜ what is dark energyâ€™â†’ â€˜what is dark energy â€™) and (III) transformations that replace words by their presumed synonyms (WordEmbedSynSwap and WordNetSynSwap) at times adds words that are not in fact syn- onymous in the query context (e.g. â€˜what is dark energy â€™â†’ â€˜what is blackness energyâ€™ and â€˜what is a active margin â€™â†’ â€˜what is a active borderâ€™). To evaluate the robustness of the ranking models, we re- sort to using only the valid queries as defined by the manual annotations. Overall, we have thus 2,040 valid queries for datasets TREC-DL-2019 and ANTIQUE that we employ in the experiments that follow. 5 RESULTS In this section we first describe our main results on the robustness of models to query variations, analyzing them by category of variation and by category of ranking model. We then move on to discussing the fusion of the ranking list obtained by the query variations. 8https://huggingface.co/facebook/m2m100_418M 9misspelling methods can generate invalid queries when all words of the query are stop-words (e.g. â€˜how is it being you â€™ from ANTIQUE would generate the same query as output since there is no non stop-words to modify) âˆ’1.0 âˆ’0.5 0.0 0.5 10 100 1000 Reâˆ’ranking threshold nDCG@10 Î” category misspelling naturality ordering paraphrase Figure 1: Distribution of nDCG@10 Î” for different re- ranking thresholds when using BERT as a re-ranker. 5.1 Robustness to Query Variations In order to explore the robustness of our three types of ranking models (traditional, neural and transformer-based), we compare the effectiveness of our models when we replace the original query with the respective query variation. The results of this experiment are displayed in Table 5 for both the TREC-DL-2019 and ANTIQUE datasets. Each row shows the effectiveness of the ranking models (columns) when using the queries obtained from each automatic query variation method. The last column (#ğ‘„) displays the number of valid queries generated by each query variation method; the invalid queries are replaced with the original ones10. The results show that for most of the query variations and ranker combinations we observe a statistical significant effectiveness drop (49 out of 70 times for TREC-DL-2019 and 54 out of 70 times for ANTIQUE), and that no set of query variations improves statisti- cally over using the original query. If we look into the percentage of overall effectiveness decreases considering only the valid queries, we see on average that the models become 20.62% and 19.21% less effective for TREC-DL-2019 and ANTIQUE respectively. This an- swers our main research question indicating that retrieval pipelines are not robust to query variations . This confirms previous empirical evidence that query variations induce a big vari- ability effect on different IR systems [ 6, 63]. We show that even with newer large-scale collections such as TREC-DL-2019, pipelines with neural ranking models are not robust to such variations. There are several potential explanations for this drop in effective- ness besides the lack of robustness of neural rankers. The first-stage ranker may be the point of failure, being unable to retrieve suffi- ciently many relevant documents for the neural rankers to re-rank. It is also possible that the query variations lead to unjudged doc- uments being ranked highly by the retrieval pipelines, which in the standard retrieval evaluation setup are considered non-relevant. We now present two experiments to show that these alternative explanations are not the cause in drop of retrieval effectiveness. Letâ€™s focus first on the first-stage ranker. Figure 1 shows the effect of increasing the re-ranking threshold on the distribution of nDCG@10 Î” when using BERT, revealing that although the number 10While rows are directly comparable, methods with fewer valid queries are a lower bound of the potential decreases in effectiveness. Evaluating the Robustness of Retrieval Pipelines with Query Variation Generators Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY Table 5: Effectiveness (nDCG@10) of different methods for TREC-DL-2019 and ANTIQUE when faced with different query variations. Bold indicates the highest values observed for each model and â†“/â†‘ subscripts indicate statistically significant losses/improvements, using two-sided paired Studentâ€™s T-Test at 95% confidence interval when compared against the same model with the original queries. #ğ‘„ indicates the number of valid query variations for the method (invalid query variations are replaced by the original query). TREC-DL-2019 Category Query Variation BM25 RM3 KNRM CKNRM EPIC BERT T5 #Q - original query 0.4795 0.5156 0.5015 0.4931 0.6240 0.6449 0.6997 43 Misspelling NeighbCharSwap 0.2747â†“ 0.2748â†“ 0.3164â†“ 0.3085â†“ 0.3894â†“ 0.4161â†“ 0.4950â†“ 43 RandomCharSub 0.2314â†“ 0.2333â†“ 0.2363â†“ 0.2263â†“ 0.2950â†“ 0.3283â†“ 0.3963â†“ 42 QWERTYCharSub 0.2435â†“ 0.2504â†“ 0.2672â†“ 0.2965â†“ 0.3512â†“ 0.3867â†“ 0.4458â†“ 42 Naturality RemoveStopWords 0.4778 0.5113 0.4842 0.4756 0.6214 0.6390 0.6866 37 T5DescToTitle 0.4215 0.4344 â†“ 0.3920â†“ 0.3928â†“ 0.5063â†“ 0.5361â†“ 0.5710â†“ 35 Ordering RandomOrderSwap 0.4795 0.5156 0.5015 0.4708 0.6227 0.6349 0.6970 43 Paraphrasing BackTranslation 0.3964â†“ 0.4195â†“ 0.3927â†“ 0.3605â†“ 0.5301â†“ 0.5467â†“ 0.6058â†“ 23 T5QQP 0.4722 0.5043 0.4541 â†“ 0.4609 0.6045 0.6396 0.7048 26 WordEmbedSynSwap 0.3530â†“ 0.3539â†“ 0.3824â†“ 0.3680â†“ 0.4746â†“ 0.4721â†“ 0.5603â†“ 27 WordNetSynSwap 0.3488â†“ 0.3650â†“ 0.3807â†“ 0.3605â†“ 0.4488â†“ 0.4474â†“ 0.5451â†“ 16 ANTIQUE Category Query Variation BM25 RM3 KNRM CKNRM EPIC BERT T5 #Q - original query 0.2286 0.2169 0.2175 0.2065 0.2663 0.4212 0.3338 200 Misspelling NeighbCharSwap 0.1559â†“ 0.1478â†“ 0.1590â†“ 0.1451â†“ 0.1841â†“ 0.2868â†“ 0.2514â†“ 199 RandomCharSub 0.1623â†“ 0.1593â†“ 0.1558â†“ 0.1476â†“ 0.1887â†“ 0.2797â†“ 0.2486â†“ 182 QWERTYCharSub 0.1613â†“ 0.1527â†“ 0.1602â†“ 0.1550â†“ 0.1922â†“ 0.2987â†“ 0.2664â†“ 197 Naturality RemoveStopWords 0.2270 0.2160 0.2222 0.2153 0.2693 0.3830â†“ 0.3200â†“ 199 T5DescToTitle 0.1673â†“ 0.1646â†“ 0.1601â†“ 0.1669â†“ 0.2000â†“ 0.2695â†“ 0.2397â†“ 136 Ordering RandomOrderSwap 0.2286 0.2168 0.2178 0.1978 â†“ 0.2665 0.4134 â†“ 0.3248â†“ 200 Paraphrasing BackTranslation 0.1618â†“ 0.1546â†“ 0.1602â†“ 0.1438â†“ 0.2036â†“ 0.3045â†“ 0.2584â†“ 93 T5QQP 0.2201 0.2065 0.2095 0.1962 0.2614 0.3926 â†“ 0.3214â†“ 105 WordEmbedSynSwap 0.1759â†“ 0.1719â†“ 0.1902â†“ 0.1690â†“ 0.2142â†“ 0.3245â†“ 0.2828â†“ 124 WordNetSynSwap 0.1791â†“ 0.1751â†“ 0.1957â†“ 0.1765â†“ 0.2117â†“ 0.3244â†“ 0.2733â†“ 71 of relevant documents on the re-ranking set increases (e.g. BM25 has Recalls @10, @100 and @1000 on average of 0.06, 0.25 and 0.48 for misspelling query variations), BERT still struggles (negative Î”) with query variations11. This indicates that even if we increase the number of relevant documents in the list to be re-ranked, the re-rankers still fail when facing the query variations. To further isolate the effect of the first-stage retrieval module, we analyzed whether the effectiveness of the pipelines would not degrade in case the first-stage retrieval was performed on the orig- inal query. In this experiment only the re-ranker models use the query variations and we check whether the effectiveness drops persist. The results reveal that there are still statistically significant effectiveness drops when only the re-ranker models use the query variations, although in smaller magnitude. While the drops in ef- fectiveness of the pipelines when using query variations for the 11Similar results are obtained for other neural rankers. entire pipeline are on average of 20% in nDCG@10, when using the query variations only for re-ranking they are of 9%.This indicates that not only the first stage retrieval module is not robust to query variations, but also the neural re-rankers . Letâ€™s now focus on the matter of unjudged documents. It is pos- sible that we are underestimating the effectiveness of the retrieval pipelines when facing query variations if ( I) the number of un- judged documents in the top-10 ranked lists increases and (II) they turn out to be relevant. When counting the amount of judged doc- uments in the top-10 ranked lists of the retrieval pipelines, we find that on average the number actually increases (4.30% for TREC-DL- 2019 and 0.36% for ANTIQUE), meaning that the performance drops of the retrieval pipelines can not be attributed to un- judged documents being brought up in the ranking by the query variations. Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gustavo Penha, Arthur CÃ¢mara, and Claudia Hauff TRECâˆ’DLâˆ’2019ANTIQUE BM25 RM3 KNRM CKNRM EPIC BERT T5 âˆ’1.0 âˆ’0.5 0.0 0.5 âˆ’1.0 âˆ’0.5 0.0 0.5 nDCG@10 Î” category misspelling naturality ordering paraphrase Figure 2: Distribution of nDCG@10 Î” when replacing the original query by the methods of each category. Table 6: Pearson correlation between the nDCG@10 Î” of ranking models when we replace the original query by query varia- tions of different categories for the ANTIQUE dataset. Misspelling Naturality BM25 RM3 KNRM CKNRM EPIC BERT T5 BM25 RM3 KNRM CKNRM EPIC BERT T5 BM25 1.00 0.88 0.54 0.54 0.58 0.48 0.51 BM25 1.00 0.90 0.25 0.26 0.43 0.34 0.25 RM3 1.00 0.52 0.53 0.58 0.44 0.47 RM3 1.00 0.22 0.25 0.39 0.29 0.24 KNRM 1.00 0.74 0.67 0.60 0.68 KNRM 1.00 0.63 0.54 0.40 0.40 CKNRM 1.00 0.63 0.54 0.59 CKNRM 1.00 0.46 0.35 0.39 EPIC 1.00 0.67 0.74 EPIC 1.00 0.48 0.50 BERT 1.00 0.80 BERT 1.00 0.57 T5 1.00 T5 1.00 Ordering Paraphrasing BM25 RM3 KNRM CKNRM EPIC BERT T5 BM25 RM3 KNRM CKNRM EPIC BERT T5 BM25 1.00 1.00 -0.01 0.01 0.00 -0.06 0.06 BM25 1.00 0.88 0.32 0.32 0.48 0.41 0.32 RM3 1.00 -0.01 0.01 0.00 -0.06 0.06 RM3 1.00 0.33 0.29 0.50 0.42 0.35 KNRM 1.00 0.07 0.02 -0.01 -0.03 KNRM 1.00 0.58 0.52 0.32 0.43 CKNRM 1.00 0.00 0.01 0.01 CKNRM 1.00 0.42 0.35 0.41 EPIC 1.00 -0.15 -0.03 EPIC 1.00 0.53 0.61 BERT 1.00 0.16 BERT 1.00 0.66 T5 1.00 T5 1.00 5.1.1 Robustness by Query Variation Category. In order to study the effect of each query variation category, Figure 2 displays the nDCG@10 Î” (difference in effectiveness when replacing the orig- inal query by its variation) distribution per category and model. Although some queries variations have a positive effect (points with positive Î”), the distributions are mostly skewed towards effec- tiveness decreases (negative Î”). First we see that the on average the decreases are higher for the misspelling category: -0.25 and -0.08 of nDCG@10 Î” for TREC-DL- 2019 and ANTIQUE respectively. We hypothesize that the effect is higher on TREC-DL-2019 due to it having smaller queries than TREC-DL-2019 (see average number of terms per query on Table 4). The second highest effect on both datasets are the query varia- tions from the paraphrasing category (-0.08 and -0.03 of nDCG@10 Î”) followed by naturality (-0.05 and -0.03). Compared to the mis- spelling variations which in most cases degrade the effectiveness of our models, paraphrasing and naturality have more queries for which the effect is positive, rendering the overall nDCG@10 Î” smaller. Queries from theordering category have the least effect (less than 0.01). Since traditional methods are in fact bag-of-words models, changing the word order will not have any effect on them, which makes the average of all modelsâ€™ nDCG@10 Î” closer to zero. In the following section, we take a further look at how each type of ranking model is affected by each query variation method. Evaluating the Robustness of Retrieval Pipelines with Query Variation Generators Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY BM25 BERT KNRM RM3 T5 CKNRM EPIC BM25 BERT KNRM RM3 T5 CKNRM EPIC TRECâˆ’DLâˆ’2019 ANTIQUE âˆ’600 âˆ’400 âˆ’200 0 âˆ’600 âˆ’400 âˆ’200 0 âˆ’200 âˆ’100 0 100 TSNE_0 TSNE_1 a a aNN TNN Trad Figure 3: tSNE dimensionality reduction where each model is represented by the nDCG@10 Î” values obtained for each query and variation method ( #ğ‘„ Ã— #ğ‘€). 5.1.2 Robustness by Model Category. When we consider how dif- ferent models are affected by the query variations, we see from Figure 2 that with the exception of ordering, which has no effect on BM25, RM3 and KNRM, other transformations have a similar over- all distribution of nDCG@10 Î” amongst different models. In order to understand if models (and category of models) make mistakes on the same queries, we label the models as follows: BM25 and RM3 are labelled as Trad (lexical matching), KNRM and CKNRM (neural network based) are labelled as NN and EPIC, BERT, T5 are labelled as TNN (transformer language model based). We then represent each model with the nDCG@10 Î” values obtained for each query and variation method resulting in a total of #ğ‘„ Ã— #ğ‘€ features per model. In order to visualize them we reduce this representation to 2 factors with tSNE [53], as shown in Figure 3. We observe that even though models have similar magnitudes and direction of nDCG@10 Î”s, classes of models as indicated by color are clustered indicating that the query variations have similar effects for each type of model. We make a similar observation when we consider the correlation of nDCG@10 Î” amongst models for the different types of transforma- tions as displayed in Table 6, where there are groups with higher similarity that roughly correspond to the different types of models. While Trad models have decreases of -0.03 (TREC-DL-2019) and -0.01 (ANTIQUE) nDCG@10 for naturality query variations, the ef- fect is higher on TNN: -0.05 and -0.04 respectively. This is evidence that neural ranking models based on heavily pre-trained language models have a slight preference for natural language queries as op- posed to keyword queries, which is a finding aligned with previous work [17]. Another interesting finding is that the word order does not have a great effect on TNN models (decreases smaller than 0.01). This is in line with recent research that indicates that the word order might not be as important as initially thought for transformer models [45, 51]. 5.2 Fusing Query Variations Although on average query variations make models less effective, there are cases when there are effectiveness gains (as shown with TRECâˆ’DLâˆ’2019 ANTIQUE 0 3 6 9 0 10 20 30 T5 BERT EPIC CKNRM KNRM RM3 BM25 Number queries with higher nDCG@10 than original query category misspelling naturality ordering paraphrase Figure 4: Distribution of query variations that are better than the original query by model and category of query vari- ation. the positive nDCG@10 Î” in Figure 2). This motivates the combina- tion of different query variations to obtain better ranking effective- ness. In order to understand whether we can improve effectiveness of models by combining different query variations, we compare different methods for combining queries, as displayed in Table 7. ğ‘…ğ‘…ğ¹ ğ¶ indicates that we fuse the results obtained from the query variations obtained after applying ğ‘€ğ¶ methods using the Recipro- cal Rank Fusion (RRF) method [15], and ğ‘…ğ‘…ğ¹ ğ´ğ‘™ğ‘™ fuses the results obtained by all query variation methods12. First we see that there is potential to have significant effective- ness gains, as shown by the last line (best query) where we always use the query with the highest retrieval effectiveness amongst query variations and the original query. The results show that combining query variations with RRF is better than using query variations individually (Table 5), and sometimes it is even the same as using the original query (no statistical difference). Our results indicate that while rank fusion mitigates the decreases in effective- ness of different query variations ( ğ‘…ğ‘…ğ¹ ğ´ğ‘™ğ‘™ decreases are of 3% and 10% nDCG@10 for TREC-DL-2019 and ANTIQUE re- spectively when compared to the original query), it does not improve the effectiveness over using the original query . 5.2.1 When are query variations better? To better understand when models benefit from different query variations, we plot the distri- bution of query variations that improve over the original query by ranking model and query variation category in Figure 4. We see that overall the queries obtained through the naturality and paraphrasing methods are the ones that improve over the original queries the most. Intuitively, paraphrasing query variations can potentially rewrite the query with better terms (e.g. â€˜why do crim- inals practice crime â€™â†’ â€˜why do criminals practice misdemeanour â€™ +0.13 nDCG@10 for BERT usingWordEmbedSynSwap), make queries grammatically correct (e.g. â€˜how sun rises â€™â†’ â€˜how does the sun riseâ€™ +0.03 nDCG@10 for BERT using T5QQP) and also corrects spelling mistakes (e.g. â€˜what is sosiologyâ€™â†’ â€˜what is sociologyâ€™ +0.47 nDCG@10 for BERT using BackTranslation). naturality methods make the queries shorter (e.g. â€˜who is robert gray â€™â†’ â€˜robert grayâ€™ 12ordering was not included in the experiments as a separated row since it only has one method, but it is included in the ğ‘…ğ‘…ğ¹ ğ´ğ‘™ğ‘™ method. Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gustavo Penha, Arthur CÃ¢mara, and Claudia Hauff Table 7: Effectiveness (nDCG@10) of different methods when employing rank fusion (RRF) of the rankings obtained by using different sets of queries, e.g. ğ‘…ğ‘…ğ¹ misspelling fuses queries generated by misspelling methods. Bold indicates the highest values observed for each model and â†“/â†‘ subscripts indicate statistically significant losses/improvements, using t-test when compared against the same model with the original queries. TREC-DL-2019 BM25 RM3 KNRM CKNRM EPIC BERT T5 original query 0.4795 0.5156 0.5015 0.4931 0.6240 0.6449 0.6997 ğ‘…ğ‘…ğ¹ Misspelling 0.3038â†“ 0.3092â†“ 0.3233â†“ 0.3175â†“ 0.3839â†“ 0.4164â†“ 0.4650â†“ ğ‘…ğ‘…ğ¹ Naturality 0.4751 0.4972 0.4855 0.4639 0.5901 0.6161 0.6628 ğ‘…ğ‘…ğ¹ Paraphrasing 0.4742 0.4865 0.4805 0.4330 â†“ 0.5847 0.6120 0.6629 ğ‘…ğ‘…ğ¹ ğ´ğ‘™ğ‘™ 0.4746 0.4976 0.5027 0.4951 0.5902 â†“ 0.6031â†“ 0.6453â†“ best query 0.5401â†‘ 0.5774â†‘ 0.6055â†‘ 0.6121â†‘ 0.6990â†‘ 0.7194â†‘ 0.7595â†‘ ANTIQUE BM25 RM3 KNRM CKNRM EPIC BERT T5 original query 0.2286 0.2169 0.2175 0.2065 0.2663 0.4212 0.3338 ğ‘…ğ‘…ğ¹ Misspelling 0.1712â†“ 0.1661â†“ 0.1758â†“ 0.1659â†“ 0.2060â†“ 0.2750â†“ 0.2435â†“ ğ‘…ğ‘…ğ¹ Naturality 0.1847â†“ 0.1866â†“ 0.2029 0.2033 0.2406 â†“ 0.3173â†“ 0.2707â†“ ğ‘…ğ‘…ğ¹ Paraphrasing 0.1909â†“ 0.1848â†“ 0.1917â†“ 0.1762â†“ 0.2380â†“ 0.3394â†“ 0.2882â†“ ğ‘…ğ‘…ğ¹ ğ´ğ‘™ğ‘™ 0.1998â†“ 0.1970â†“ 0.2150 0.2037 0.2430 â†“ 0.3179â†“ 0.2729â†“ best query 0.2716â†‘ 0.2682â†‘ 0.2985â†‘ 0.2843â†‘ 0.3370â†‘ 0.4488â†‘ 0.3925â†‘ +0.34 nDCG@10 for BERT using RemoveStopWords), removing un- necessary information from the original query on certain cases. 6 CONCLUSION In this work we studied the robustness of ranking models when faced with query variations. We first described a taxonomy of trans- formations between two queries for the same information need that characterizes how exactly a query is modified to arrive at one of its variants. We found six different types of transformations, and we focused our experiments on the ones that do not change the query semantics: misspelling, naturality, ordering and paraphrasing. They account for 57% of observed variations in the UQV100 dataset. For each of these four categories we proposed different methods to automatically generate a query variation based on an input query. We studied the quality of the generated query variations, and based only on the valid ones we analyzed how robust retrieval pipelines are to them. Our experimental results on two different datasets quantify how much each model is affected by each type of query variation, demonstrating large effectiveness drops of 20% on aver- age when compared to the original queries from the test sets. We found rank fusion techniques to somewhat mitigate the drops in effectiveness. Our work highlights the need of creating test collections that include query variations to better understand model effectiveness. As future work, we believe that it is important to study (I) how to automatically generate high quality query variation generators for categories that do change the semanticsâ€”while maintaining the same information needâ€”of the query and (II) techniques to improve the robustness of existing ranking pipelines. REFERENCES [1] Nasreen Abdul-Jaleel, James Allan, W Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D Smucker, and Courtney Wade. 2004. UMass at TREC 2004: Novelty and HARD. Computer Science Department Faculty Publication Series (2004), 189. [2] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. 2018. Generating natural language adversarial examples. arXiv preprint arXiv:1804.07998 (2018). [3] Yuki Amemiya, Tomohiro Manabe, Sumio Fujita, and Tetsuya Sakai. 2021. How Do Users Revise Zero-Hit Product Search Queries?. InAdvances in Information Re- trieval, Djoerd Hiemstra, Marie-Francine Moens, Josiane Mothe, Raffaele Perego, Martin Potthast, and Fabrizio Sebastiani (Eds.). Springer International Publishing, Cham, 185â€“192. [4] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2015. User variability and IR system evaluation. In Proceedings of The 38th International ACM SIGIR conference on research and development in Information Retrieval . 625â€“634. [5] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2016. UQV100: A test collection with query variability. InProceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval . 725â€“728. [6] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2017. Retrieval consistency in the presence of query variations. In Proceedings of the 40th In- ternational ACM SIGIR Conference on Research and Development in Information Retrieval. 395â€“404. [7] Nicholas J Belkin, Colleen Cool, W Bruce Croft, and James P Callan. 1993. The ef- fect multiple query representations on information retrieval system performance. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval . 339â€“346. [8] Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and Joseph A Shaw. 1995. Com- bining the evidence of multiple query representations for information retrieval. Information Processing & Management 31, 3 (1995), 431â€“448. [9] Rodger Benham, J Shane Culpepper, Luke Gallagher, Xiaolu Lu, and Joel Macken- zie. 2018. Towards Efficient and Effective Query Variant Generation.. InDESIRES. 62â€“67. [10] Rodger Benham, Joel Mackenzie, Alistair Moffat, and J Shane Culpepper. 2019. Boosting search performance using query variations. ACM Transactions on Information Systems (TOIS) 37, 4 (2019), 1â€“25. [11] Chris Buckley and Janet Walz. 1999. The TREC-8 query track. In TREC. [12] Arthur CÃ¢mara and Claudia Hauff. 2020. Diagnosing bert with retrieval heuristics. In European Conference on Information Retrieval . Springer, 605â€“618. Evaluating the Robustness of Retrieval Pipelines with Query Variation Generators Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY [13] Anirban Chakraborty, Debasis Ganguly, and Owen Conlan. 2020. Retrievability based Document Selection for Relevance Feedback with Automatically Gener- ated Query Variants. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management . 125â€“134. [14] Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement 20, 1 (1960), 37â€“46. [15] Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval. 758â€“759. [16] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. 2020. Overview of the trec 2019 deep learning track. arXiv preprint arXiv:2003.07820 (2020). [17] Zhuyun Dai and Jamie Callan. 2019. Deeper text understanding for IR with contextual neural language modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval . 985â€“988. [18] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional neural networks for soft-matching n-grams in ad-hoc search. InProceedings of the eleventh ACM international conference on web search and data mining . 126â€“134. [19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 4171â€“4186. [20] Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Sid- dharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaud- hary, et al. 2020. Beyond english-centric multilingual machine translation. arXiv preprint arXiv:2010.11125 (2020). [21] Christian Federmann, Oussama Elachqar, and Chris Quirk. 2019. Multilingual whispers: Generating paraphrases with translation. In Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019) . 17â€“26. [22] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. 2018. Black-box genera- tion of adversarial text sequences to evade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops (SPW) . IEEE, 50â€“56. [23] Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. 2020. Evaluating modelsâ€™ local decision boundaries via contrast sets. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. 1307â€“1323. [24] Siddhant Garg and Goutham Ramakrishnan. 2020. Bae: Bert-based adversarial examples for text classification. arXiv preprint arXiv:2004.01970 (2020). [25] Helia Hashemi, Mohammad Aliannejadi, Hamed Zamani, and W Bruce Croft. 2020. ANTIQUE: A non-factoid question answering benchmark. In European Conference on Information Retrieval . Springer, 166â€“173. [26] Sharon Hirsch, Ido Guy, Alexander Nus, Arnon Dagan, and Oren Kurland. 2020. Query reformulation in E-commerce search. In Proceedings of the 43rd Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval. 1319â€“1328. [27] Bernard J Jansen, Danielle L Booth, and Amanda Spink. 2009. Patterns of query re- formulation during web searching. Journal of the american society for information science and technology 60, 7 (2009), 1358â€“1371. [28] Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substitutions. In Proceedings of the 15th international conference on World Wide Web. 387â€“396. [29] Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2020. Pretrained transformers for text ranking: Bert and beyond. arXiv preprint arXiv:2010.06467 (2020). [30] Xiaolu Lu, Oren Kurland, J Shane Culpepper, Nick Craswell, and Ofri Rom. 2019. Relevance Modeling with Multiple Query Variations. In Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval . 27â€“34. [31] Sean MacAvaney. 2020. OpenNIR: A Complete Neural Ad-Hoc Ranking Pipeline. In WSDM 2020. [32] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman Cohan. 2020. ABNIRML: Analyzing the Behavior of Neural IR Models. arXiv preprint arXiv:2011.00696 (2020). [33] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli Goharian, and Ophir Frieder. 2020. Expansion via prediction of importance with contextualization. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 1573â€“1576. [34] Sean MacAvaney, Andrew Yates, Sergey Feldman, Doug Downey, Arman Cohan, and Nazli Goharian. 2021. Simplified Data Wrangling with ir_datasets. In SIGIR. [35] Craig Macdonald and Nicola Tonellotto. 2020. Declarative Experimentation inInformation Retrieval using PyTerrier. InProceedings of ICTIR 2020 . [36] Jonathan Mallinson, Rico Sennrich, and Mirella Lapata. 2017. Paraphrasing revisited with neural machine translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. 881â€“893. [37] Alistair Moffat, Falk Scholer, Paul Thomas, and Peter Bailey. 2015. Pooled evalu- ation over query variations: Users are as diverse as systems. In proceedings of the 24th ACM international on conference on information and knowledge management . 1759â€“1762. [38] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations . 119â€“126. [39] Nikola MrkÅ¡iÄ‡, Diarmuid O SÃ©aghdha, Blaise Thomson, Milica GaÅ¡iÄ‡, Lina Rojas- Barahona, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. 2016. Counter-fitting word vectors to linguistic constraints. arXiv preprint arXiv:1603.00892 (2016). [40] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In CoCo@ NIPS. [41] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019). [42] Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020. Document ranking with a pretrained sequence-to-sequence model. arXiv preprint arXiv:2003.06713 (2020). [43] Harshith Padigela, Hamed Zamani, and W Bruce Croft. 2019. Investigating the suc- cesses and failures of BERT for passage re-ranking.arXiv preprint arXiv:1905.01758 (2019). [44] Gustavo Penha and Claudia Hauff. 2020. Curriculum learning strategies for ir. In European Conference on Information Retrieval . Springer, 699â€“713. [45] Thang M Pham, Trung Bui, Long Mai, and Anh Nguyen. 2020. Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks? arXiv preprint arXiv:2012.15180 (2020). [46] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531 (2019). [47] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim- its of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 (2019). [48] DaniÃ«l Rennings, Felipe Moraes, and Claudia Hauff. 2019. An axiomatic approach to diagnosing neural IR models. In European Conference on Information Retrieval . Springer, 489â€“503. [49] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. arXiv preprint arXiv:2005.04118 (2020). [50] Stephen E Robertson and Steve Walker. 1994. Some simple effective approxi- mations to the 2-poisson model for probabilistic weighted retrieval. In SIGIRâ€™94. Springer, 232â€“241. [51] Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela. 2021. Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. arXiv preprint arXiv:2104.06644 (2021). [52] Karen Spark-Jones. 1975. Report on the need for and provision of anâ€™idealâ€™information retrieval test collection.Computer Laboratory (1975). [53] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008). [54] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations . Association for Computational Linguistics, Online, 38â€“45. https://www.aclweb.org/anthology/2020.emnlp- demos.6 [55] Jed R Wood and Larry E Wood. 2008. Card sorting: current practices and beyond. Journal of Usability Studies 4, 1 (2008), 1â€“6. [56] Chen Wu, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. 2021. Are Neural Ranking Models Robust? arXiv preprint arXiv:2108.05018 (2021). [57] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval. 55â€“64. [58] Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Simple applications of bert for ad hoc document retrieval. arXiv preprint arXiv:1903.10972 (2019). [59] Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-X: A Cross- lingual Adversarial Dataset for Paraphrase Identification. In Proc. of EMNLP. [60] Oleg Zendel, Anna Shtok, Fiana Raiber, Oren Kurland, and J Shane Culpepper. 2019. Information needs, queries, and query performance prediction. In Proceed- ings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 395â€“404. [61] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2020. An analysis of BERT in document ranking. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . 1941â€“1944. Woodstock â€™18, June 03â€“05, 2018, Woodstock, NY Gustavo Penha, Arthur CÃ¢mara, and Claudia Hauff [62] Shengyao Zhuang and Guido Zuccon. 2021. Dealing with Typos for BERT-based Passage Retrieval and Ranking. arXiv preprint arXiv:2108.12139 (2021). [63] Guido Zuccon, Joao Palotti, and Allan Hanbury. 2016. Query variations and their effect on comparing information retrieval systems. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management . 691â€“700.