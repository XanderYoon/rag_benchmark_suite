Federated Recommendation via Hybrid Retrieval Augmented Generation Huimin Zeng Zhenrui Yue Qian Jiang Dong Wang Unversity of Illinois at Urbana-Champaign {huiminz3, zhenrui3, qianj3, dwang24}@illinois.edu Abstract Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR sys- tems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and het- erogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recom- mendation scenarios. Yet, LLM-based rec- ommenders encounter challenges such as low inference efficiency and potential hallucina- tion, compromising their performance in real- world scenarios. To this end, we propose GPT- FedRec, a federated recommendation frame- work leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mech- anism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, the retrieved results are converted into text prompts and fed into GPT for re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to ex- tract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. In addi- tion, the RAG approach also prevents LLM hal- lucination, improving the recommendation per- formance for real-world users. Experimental results on diverse benchmark datasets demon- strate the superior performance of GPT-FedRec against state-of-the-art baseline methods. 1 Introduction Recommendation systems play a vital role in aiding users to discover pertinent content of their interests, such as online commerce (Ying et al., 2018), social media (Fan et al., 2019). The prevailing strategy in developing recommendation systems involves ex- tracting usersâ€™ personalized preferences from their historical data. However, the concerns on data pri- vacy have prompted strict regulations on data gov- Client 1 Server Q: Iâ€™ve watched these movies, any recommendations? A: 11 Poor generalization on cold-start users. Federated RecSys Client 2 User: query with unseen movie IDs? Data: Data: ðŸ˜” Figure 1: Under data heterogeneity and data sparsity, traditional ID-based recommenders fail to recommend correct items to the cold-start users. ernance (e.g., GDPR 1). Such regulations limit the development of modern recommendation systems. To address this issue, federated recommenda- tion (FR) emerges as a new paradigm facilitating privacy-preserving recommendations (Ammad-Ud- Din et al., 2019; Zhang et al., 2024). In FR, a cen- tral server stores a global recommendation model, while a set of clients contain local private data. These clients collaboratively train the global model without sharing their private data. Note that the concept "client" does not necessarily refer to a sin- gle human user, but can also represent a local data server with a small dataset. For clarity, we con- sistently use "client" to represent a federated client, and use "user" to denote a human user of a recommender system. However, traditional FR systems usually rep- resent users/items with discrete identities (Zhang et al., 2024). As such, they may suffer from de- graded performance, due to the data sparsity and heterogeneity in the federated setting (Wu et al., 2022; Zhang et al., 2024). In FR, the clients might only contain the data of a few or even a single user (data sparsity). Training an ID-based recommender on such sparse data is prone to overfitting (Zhang 1General Data Protection Regulation:https://gdpr-info.eu/ arXiv:2403.04256v1 [cs.IR] 7 Mar 2024 et al., 2024). Moreover, data sparsity may also trig- ger data heterogeneity. That is, the item scopes of local datasets are only subsets of the entire item scope, and different local clients may have different item scopes (non-i.i.d. data). As shown in Figure 1, a test userâ€™s data contains novel items never seen in the local clients (i.e., a cold-start user). Conse- quently, the FR system could not make meaningful recommendations. Since the locally trained models may never see certain items during training, aggre- gating such sparsely-trained models results in poor generalization (Zhang et al., 2024). On the other hand, large language models (LLMs) exhibit strong generalization abilities in diverse tasks, thanks to the extensive knowledge learned from the massive-scale real-world data. Re- cently, LLMs as recommenders also demonstrate impressive performance in different recommenda- tion tasks (Harte et al., 2023; Li et al., 2023c; Bao et al., 2023; Zhang et al., 2023; Yue et al., 2023a). Therefore, employing LLMs for FR has the poten- tial to address data sparsity and data heterogene- ity. Yet, existing LLM-based recommenders suf- fer from issues like incomplete recommendations, low inference efficiency and potential hallucination (Yue et al., 2023a; Li et al., 2023b), impairing their applicability in real-world scenarios. Besides, in FR, it is neither affordable nor feasible for local clients to finetune LLMs with limited computa- tional resources and data. In addition, the next- word-generating nature of LLMs might generate irrelevant words in terms of recommendation, lead- ing to incomplete recommendations and low infer- ence efficiency (Yue et al., 2023a). Finally, as a consequence of hallucination, LLM-based recom- menders may recommend non-existing or make-up items, compromising user experience in real-world applications. To this end, in this work, we propose GPT- FedRec, a novel and effective two-stage frame- work for federated recommendation. Leveraging both the generalized features within training data and pretrained knowledge within LLMs (e.g., Chat- GPT), GPT-FedRec not only provides a complete solution in a data sparse and heterogeneous FR set- ting, but also outperforms existing methods with improved recommendation performance. As shown in Figure 2, GPT-FedRec operates in two stages. The first stage employs a novel hybrid retrieval mechanism to generate recommendation candidates using generalized data features. We pro- pose to adopt small-scale ID-based retrievers to capture the ID-based user patterns, and dense text- based retrievers to extract robust and generalized semantic features from item descriptions. The ra- tionale behind this design is: while the ID-based user history is informative in terms of representing user-item dynamics, the textual item descriptions (e.g., titles, categories) contain more generalized semantic features. Leveraging such generalized features improves generalization of the recommen- dation, especially in data-sparse and heterogeneous FR settings (Zhang et al., 2024). Consider the movie recommendation in Figure 1. The descrip- tions of these movies provide more information for the recommender: the movies watched by the new user could be depicted in texts like "war, action movies", which is semantically closer to the "sci-fi, action movies" on client 2, compared to the "love, romantic movies" on client 1. In the second stage, the retrieved candidates are fed into an LLM for re-ranking. This process leverages the pretrained generalized knowledge of LLM, thereby further improving the generalization of the recommenda- tion. Moreover, conditioning LLM re-rank on the first-stage results effectively prevents hallucination. This enhances recommendation performance for real-world users. Finally, due to space limit, we narrow down the scope of this work to the sequen- tial recommendation, for its superior performance over traditional recommendater systems (e.g., ma- trix factorization). We summarize the contributions of our paper as follows2: 1. To the best of our knowledge, GPT-FedRec is the first FR framework that uses hybrid RAG and LLMs. More importantly, GPT-FedRec withstands the critical challenge of data spar- sity and data heterogeneity in FR, achieving promising performance. 2. Technically, GPT-FedRec employs arbitrary ID-based retriever and arbitrary text-based re- triever to perform the hybrid retrieval. More- over, as a RAG method, GPT-FedRec does not require the finefuning of the LLMs. 3. We evaluate GPT-FedRec in the FR setting. On different benchmark datasets, experimen- tal results suggest that GPT-FedRec achieves considerable improvements in recommenda- tion performance, while outperforming state- of-the-art baselines. 2We adopt publicly available datasets and release the code at https://github.com/huiminzeng/GPT-FedRec.git. Text Description (e.g. movie titles, genres) ID-based Retriever Text-based Retriever Hybrid Retrieval ID 1 ID 2 ID 3 Prompt ### Iâ€™ve watched the following movies in the past in order:{history}. ### There is also candidate pool: {candidates}. ### {task instruction} RAG-based Re-ranking Parse outputs Re-ranked Candidates Figure 2: GPT-FedRec. The first stage involves a hybrid retrieval with an ID-based retriever and a text-based retriever. The second stage performs re-ranking via the retrieved results and RAG of the prompted GPT. 2 Related Work Federated Recommendation. Existing feder- ated recommendation (FR) systems are usually ID-based models: users/items are represented with unique identities (IDs). For instance, FCF (Ammad-Ud-Din et al., 2019), FedRec (Lin et al., 2020), FedMF (Chai et al., 2020) extend classic ID-based matrix factorization into federated mode. In comparison, Wu et al. (2022) train ID-based sequential recommenders to domain-disentangled features in a federated fashion, achieving better per- formance. Luo et al. (2022) aggregates ID-based model with a client utility-aware protocol for better federated recommendation. However, in heteroge- neous settings, these ID-based FR methods could not generalize well to cold-start users, because the items in the cold-start user data might never be seen during training (Zhang et al., 2024). Target- ing at this issue, Zhang et al. (2024) proposed a text-based FR solution, namely TransFR. Trained using textual features, TransFR demonstrates bet- ter generalization on heterogeneous FR settings. However, TransFR suffers from data sparsity and is not capable of handling long user-item sequences. Compared to existing FR methods, the hybrid re- trieval mechanism in GPT-FedRec is designed to address the data sparsity and heterogeneity in FR. Unlike TransFR, GPT-FedRec is capable of model- ing long sequences. Natural Language for Recommendation. The inherent generality of textual features empowered the generalizability and transferability of recom- menders. For instance, to achieve better transfer- ability, Hou et al. (2023a, 2022) train sequential rec- ommenders using language-model-encoded item texts. Similary, in (Li et al., 2023a; Geng et al., 2022), pretrained language models are finetuned using item descriptions and then used as recom- menders. These text-based recommender systems demonstrate state-of-the-art generalizability in dif- ferent recommendation scenarios. However, they usually require rich textual data to enable fine- funing and could not handle long user histories. On the other hand, LLMs as recommenders have gained increased attention for LLMsâ€™ impressive generalization ability. The mainstream LLM-based recommenders exploits the pretrained knowledge within LLMs to perform next-item recommenda- tion (Hou et al., 2023b; Sileo et al., 2022; Sun et al., 2023; He et al., 2023). For example, He et al. (2023) employees LLMs as conversational agents to understand user preferences and improve recommendation. Another stream of LLM-based recommendation designs tuning strategies tailored for specific subtasks (e.g., rating prediction) to im- prove recommendation performance (Chen, 2023; Kang et al., 2023; Yue et al., 2023a). However, ex- isting LLM-based recommenders mainly generate recommendation candidates in an autoregressive fashion. As such, the generated content might be irrelevant to the items of interests or even are hallu- cinated, leading to undesired performance in real- world applications. As such, we design a retrieval augmented recommendation framework, where the RAG approach effectively reduces hallucination and improves recommendation performance. 3 Preliminaries Data. We adopt the sequential recommendation setting to illustrate the data format. In sequential recommendation, a data point is the historical data of a user: a sequence of interacted items x (sorted by timestamps) within her history. A sequence x is a list of items [x1, x2, ..., xl] of length l. Each ele- ment in x belongs to the item scope I that contains all items: xi âˆˆ I . The goal of a recommender is to predict the next user-item interaction xl+1 âˆˆ I based on the user history x. In our experiments, xl+1 is used as ground truth y (i.e., y = xl+1). Model. An ID-based recommender directly takes item sequences as input, and maps them into new items as recommendations. Given an input se- quence x, an ID-based recommender computes a score vector over the item scope I, and recom- mends items with highest probabilities. In compar- ison, a text-based recommender usually transforms discrete item IDs into descriptions: transforming a sequence of items (x) into a sequence of descrip- tions. After trained on such text sequences, the text-based recommender generates textual IDs (i.e., IDs in text format) or item titles as the final recom- mendation. Federated Recommendation. In FR, a central server stores a global recommender, while a set of local clients store their respective datasets. The goal of FR is to collaboratively train the global rec- ommender without clients sharing their private data. Assume there are K clients in an FR application. Each client contains a local dataset Dk with |Dk| item sequences. In FR, the item scopes covered by local datasets may be smaller than the entire item scope: I k âŠ† I and âˆªkI k âŠ† I (data spar- sity). In addition, some local clients may contain unique items only present in their datasets (data het- erogeneity). The test user can also introduce new, unseen items (i.e., cold-start users). As in Figure 1, the clients contain disjoint movies, and the test user data involves new, unseen movies. Therefore, the challenges of data sparsity, data heterogeneity and non-generalizabilty on cold-start users drive the development of GPT-FedRec. For simplicity, un- less specified, notations without client index k represent the data of an arbitrary client. 4 Algorithm The overview of GPT-FedRec is in Figure 2. To address the aforementioned challenges, we firstly develop a hybrid retrieval mechanism using a small- scale ID-based retriever and a dense text-based retriever to retrieve generalized candidates (Sec- tion 4.1). Then, based on the retrieved results, we establish a retrieval augmented recommendation pipeline using LLMs (Section 4.2). 4.1 Hybrid Retrieval ID-based Retriever. Given existing ID-based recommenders, any of them could be used by GPT- FedRec to retrieve potential candidates. In our implementation, we choose LRURec (Yue et al., 2023b) as the ID-based retriever for its state-of-the- art performance and light-weight design, avoiding extra communication costs in FR settings. Formally, we define the ID-based retriever asfI, parameterized by Î¸I. As shown in Figure 2, for a piece of user data, fI takes its interacted item sequence x as input, i.e., fI(x). fI returns a vec- tor of similarity scores over the item scope I. To train LRURec on each local client, we optimize the cross-entropy loss over the local dataset: Lce = E(x,y)âˆ¼Dk[L(fI(x), y)]. (1) During federated training, the weights of fI are sent to the global server for aggregation. However, it is foreseeable that in a data sparse and data het- erogeneous setting, the predicted scores returned by a local fI are skewed towards the items present in its local dataset. That is, a locally trained fI is prone to retrieve the items present its local datasets, rendering its unsatisfactory performance on test data with unseen new items. Text-based Retriever. In the data sparse and data heterogeneous FR setting, the inadequacy of ID-based retriever motivates us to employ an ad- ditional dense, more generalizable text-based re- triever to build GPT-FedRec. To illustrate the ne- cessity of using text-based retriever, consider the example in Figure 1. In this example, client 1, client 2 and the test user have mutually exclusive item scopes. This is a typical data heterogeneous case. In the discrete ID space, a movie with a spe- cific ID can only appear in one party. An ID-based retriever would fail to capture the correlation be- tween some movie IDs cross parties, leading to degraded performance. In a sharp contrast, in the text space, the movies across these parties may still share common, generalized features (e.g., "war, action movies" of the new user and "sci-fi, war movies" on client 2), despite the different movie IDs. As such, text-based retriever could capture such generalized features, overcoming data hetero- geneity that defects ID-based retrievers. To this end, we propose to use E5 (Wang et al., 2022) as the text-based retriever. E5 is a transformer-based language model, pretrained for text-retrieval tasks. Formally, we define the text- based retriever as fT , parameterized by Î¸T . To adapt E5 into our FR application, we finetune it using text descriptions of items and the InfoNCE loss (Oord et al., 2018) on each local client. In par- ticular, for each training samplex = [x1, x2, ..., xl] and its ground-truth item y, we firstly transform Text Description (e.g. movie titles, genres) ID-based Retriever Txt-R. Client 1 Client 2 Client 3 Text-based Retriever Client Training ID-R. ID 1 ID 2 ID 3 CE InfoNCE Global Aggregation Figure 3: The training and aggregation process of GPT-FedRec. On clients, the ID-based retriever and the text-based retriever are trained using local data. them into input texts t and ground-truth text ty using the item meta data (e.g., titles, categories, genres of the items) using the E5 template. t = [t1, t2, ..., tl], where ti = Template1(xi), ty = Template2(y). (2) In Equation 2, Template1 is the E5 template with "query" as prefix, Template2 is the E5 template with "passage" as prefix. (More details about E5 templates in Appendix B.) Then, with prepared text templates, fT is finetuned with the InfoICE loss: Linf o = âˆ’ 1 |D| |D|X m log es(q(m),p(m)) es(q(m),p(m)) + P n es(q(m),Â¯p(n)) , where q = fT (t), p = fT (ty), Â¯p = fT (Â¯y), Â¯y âˆˆ I \ y (3) In Equation 3, we use superscript (m) to index dif- ferent users (not client). As for s(q, p), it refers to a scoring function between the encoded query q and an encoded positive passage p or negative pas- sage Â¯p. In our implementation, the score function is cosine similarity scaled by a temperature Ï„: s(p, q) = cos(p, q)/Ï„. (4) By finetuning E5 using Equation 3, the text-based retriever learns to assign high similarity between user sequences and their ground-truth items in the text space. More importantly, since E5 is pre- trained, it has already encoded prior knowledge of language patterns. Therefore, the text-based re- triever is capable of extracting generalized textual features from item descriptions, despite the data sparsity and data heterogeneity across clients. Finally, after training the ID-based retriever (f k I ) and the text-based retriever ( f k T ) on each local client, we adopt FedAvg (McMahan et al., 2017) to perform global model aggregation, obtaining the global ID-based retriever and text-based retriever: Î¸âˆ— (Â·) = 1P k |Dk| KX k=1 |Dk| Â· Î¸k (Â·) s.t. Î¸ k I = arg minLce(Dk, fI), Î¸k T = arg minLinf o(Dk, fT ), k âˆˆ {1, ..., K}. (5) Hybrid Retrieval. To generate the retrieval re- sults for any user sequence x, we compute a weighted sum of the normalized prediction scores returned by the aggregated ID-based retriever and text-based retriever through the Tikhonov principle (Tikhonov, 1963): Ë†Phybrid = Î» Â· Ïƒ(fI(x)) + (1 âˆ’ Î») Â· Ïƒ(fT (t)), (6) where Ïƒ is the softmax opernation. Then, the items of top-N scores within Ë†Phybrid are retrieved as can- didates. These candidates form a candidate set Ë†I. Ë†I is the result of the first stage of GPT-FedRec. We highlight that the candidate set Ë†I is retrieved based on the hybrid scores Ë†Phybrid. Compared to traditional ID-based FR models, the hybrid scores incorporate both domain-generalized features from the text-based retriever and the representative ID- based user patterns. As such, GPT-FedRec is equipped with generalization ability by design, and can overcome the data sparsity and data hetero- geneity issue in FR applications, achieving better recommendation performance. 4.2 Hybrid Retrieval Augmented Generation RAG-based Recommendation. After the hybrid retrieval stage, GPT-FedRec further employs an LLM to perform re-ranking among the retrieved candidates within Ë†I. This design is to exploit the pretrained knowledge encoded in LLMs, so that the generalization of recommendations is further enhanced. Moreover, the re-ranking process of the LLM is conditioned on the retrieved results, hallu- cination could be effectively avoided: the LLM is explicitly instructed to only re-rank the candidates within Ë†I, and they are retrieved from real-world data during the first stage. Finally, for efficiency, we intend to prompt an LLM for recommendation without finetuning it, and treat the LLM-based re- ranking process as retrieval augmented generation (De Cao et al., 2020; Tay et al., 2022). LLM. In this work, we adopt GPT-3.5-Turbo (OpenAI) from OpenAI to build GPT-FedRec. GPT-3.5-Turbo is a closed-source LLM with abil- ities of solving many complex tasks (OpenAI). Since in a data sparse FR setting, it is infeasible to finetune LLM using such limited data, we select GPT-3.5-Turbo for its powerful zero-shot general- ization ability. To construct a text prompt for GPT-3.5-Turbo, inspired by (Hou et al., 2023b), we start it with a description of both user history and candidate item using their titles and categories/genres. Then, an in- struction to the task is appended to the description. In addition to the text prompts, we also feed sys- tem prompts to GPT, so that its mindset is adjusted to the concrete recommendation application (e.g., recommending movies or cosmetics). A simplifed prompt template looks like: ### system: You are a helpful { role}, {role description }. ### user: Iâ€™ve browsed the fol- lowing items in the past in order: {history}. There is also candidate pool: {candidates}. {task instruction }. In the above template, role and role description are replaced with a concrete real- world job (e.g., shopping assistant) and its job de- scription (e.g., recommending products for cus- tomers). history is instantiated as item texts containing item titles, possibly with item cat- egories/genres. As for candidates, it is re- placed with the hybrid retrieval results. task instruction shall be replaced with a direct re- rank instruction. We also add additional formatting instructions in the template for post-processing pur- poses. The detailed, complete prompt template is summarized in Appendix B. Post-processing. As mentioned, we do not ac- cess the model weights or output logits of LLMs. The re-rank results are generated in free texts (e.g., item titles) despite the formatting instructions in the prompts. Therefore, we apply fuzzy matching to transform the generated textual recommenda- tions (e.g., item titles) into a ranked list of item IDs: Ë†IRAG to perform evaluation. 5 Experiments 5.1 Experimental Setup Datasets. We select 5 benchmark datasets from different domains to evaluate GPT-FedRec. They are Beauty, Games, Toys, Auto (He and McAuley, 2016; McAuley et al., 2015) and ML-100K (Harper and Konstan, 2015). The first four datasets are Amazon review datasets consisting of user feed- back on different categories of products. ML-100K is a benchmark movie recommendation dataset. Following (Chen, 2023; Yue et al., 2022), we pre- process the raw data with 5-core, and construct training sequences in chronological order. The de- tailed dataset statistics are in Table 3 (Appendix A). Baselines. We adopt three groups of baseline methods for comparison. (1) Group 1: ID-based FR models, namely FedSAS, FedLRU and CF-FedSR. FedSAS and FedLRU are federated version of SAS- Rec (Kang and McAuley, 2018) and LRURec (Yue et al., 2023b) with FedAvg (McMahan et al., 2017) as the aggregation protocol. CF-FedSR (Luo et al., 2022) uses a client utility-aware aggregation proto- col for better FR. (2) Group 2: Text-based models, i.e., TransFR (Zhang et al., 2024), P5 (Geng et al., 2022) and RecFmr (Li et al., 2023a) and UniSRT (Hou et al., 2022). TransFR trains a BERT-like retriever. UniSR T trains SASRec using BERT- encoded item texts. P5 and RecFmr finetune pre- trained T5 or LongFormer with item texts and use the model as recommender. (3) Group 3: The hy- brid UniSRIT (Hou et al., 2022), an ID-augmented version of UniSRT. Note that P5, RecFmr, UniSRT and UniSRIT are not originally designed for FR. We use FedAvg as the aggregation protocol to ex- tend them into our FR setting. We did not include LLM-based recommenders as baselines, because existing LLM-based recommenders solely focus on the ranking stage, and could not provide a complete solution for FR: they can only rank ground-truth items along with other sampled negative items. The can not generate candidates from the entire item scope from scratch, which is required by FR. Dataset Metric ID-Based Text-Based Hybrid FedSAS FedLRU CF-FedSR TransFR P5 RecFmr. UniSR T UniSRIT GPT-FedRec Beauty R@5â†‘ 0.0153 0.0218 0.0204 0.0059 0.0013 0.0313 0.0231 0.0247 0.0348 N@5â†‘ 0.0101 0.0145 0.0138 0.0037 0.0007 0.0167 0.0159 0.0166 0.0233 R@10â†‘ 0.0241 0.0336 0.0306 0.0088 0.0024 0.0533 0.0347 0.0349 0.0563 N@10â†‘ 0.0129 0.0183 0.0170 0.0047 0.0011 0.0238 0.0196 0.0199 0.0302 Games R@5â†‘ 0.0289 0.0391 0.0366 0.0059 0.0019 0.0399 0.0327 0.0412 0.0471 N@5â†‘ 0.0195 0.0257 0.0235 0.0037 0.0010 0.0227 0.0218 0.0274 0.0331 R@10â†‘ 0.0419 0.0613 0.0560 0.0096 0.0042 0.0706 0.0522 0.0621 0.0764 N@10â†‘ 0.0237 0.0329 0.0298 0.0049 0.0017 0.0326 0.0281 0.0342 0.0406 Toys R@5â†‘ 0.0094 0.0182 0.0162 0.0067 0.0156 0.0476 0.0323 0.0183 0.0419 N@5â†‘ 0.0070 0.0133 0.0125 0.0043 0.0080 0.0250 0.0225 0.0125 0.0268 R@10â†‘ 0.0124 0.0243 0.0211 0.0106 0.0240 0.0739 0.0495 0.0271 0.0720 N@10â†‘ 0.0080 0.0153 0.0141 0.0055 0.0107 0.0355 0.0280 0.0155 0.0364 Auto R@5â†‘ 0.0214 0.0607 0.0464 0.0107 0.0060 0.0536 0.0500 0.0464 0.0643 N@5â†‘ 0.0138 0.0341 0.0294 0.0072 0.0027 0.0264 0.0324 0.0372 0.0390 R@10â†‘ 0.0357 0.0786 0.0679 0.0143 0.0083 0.0750 0.0679 0.0821 0.0964 N@10â†‘ 0.0187 0.0398 0.0361 0.0085 0.0035 0.0332 0.0384 0.0488 0.0492 ML-100K R@5â†‘ 0.0183 0.0459 0.0550 0.0092 0.0002 0.0001 0.0183 0.0183 0.0642 N@5â†‘ 0.0085 0.0327 0.0402 0.0035 0.0001 0.0001 0.0150 0.0075 0.0362 R@10â†‘ 0.0367 0.1101 0.1009 0.0092 0.0002 0.0091 0.0183 0.0459 0.1468 N@10â†‘ 0.0145 0.0538 0.0550 0.0035 0.0001 0.0031 0.0150 0.0166 0.0621 Average R@5â†‘ 0.0187 0.0371 0.0350 0.0077 0.0050 0.0345 0.0313 0.0298 0.0505 N@5â†‘ 0.0118 0.0241 0.0239 0.0045 0.0025 0.0182 0.0215 0.0202 0.0313 R@10â†‘ 0.0302 0.0616 0.0553 0.0105 0.0070 0.0546 0.0445 0.0504 0.0896 N@10â†‘ 0.0156 0.0320 0.0304 0.0054 0.0034 0.0256 0.0258 0.0270 0.0437 Table 1: Main results on recommendation performance under different FR schemes. The best results are highlighted in bold and the second best results are highlighted with underline. FR Setup. Since the training of some baseline methods could not converge under a large num- ber of clients, we compare all methods under a setting of 5 clients. Moreover, to simulate data sparsity and data heterogeneity, the local datasets are sparsely sampled from the original datasets and are guaranteed to be mutually exclusive in terms of users. The remaining users are used as cold- start test users. The detailed local datasets statistics are summarized in Table 4 (Appendix A). Note that the test users in our FR setting are cold-start users, whose historical data are not used for model training. This is fundamentally different from the common centralized sequential recommendation setting, where the historical data of all users are used to train the model with the last item used for testing. Our cold-start setting is more realistic in the FR setting, and also more appropriate to test the generalization of the trained FR model. Implementation. To implement GPT-FedRec, we train LRURec from scratch, and finetune the pretrained E5 (e5-base-v2) using the training data. When training LRURec, the learning rate is initial- ized as 1e-3, and the number of local/global epochs is 80/5 (for Beauty, Games, Toys) and 60/5 (for Auto and ML-100k). As for E5, the learning rate is initialized as 1e-6, and the number of local/global epochs is 2/2. We use AdamW as the optimizer for both LRURec and E5. When generating the candidate set Ë†I in the first stage, we pick the top-20 items from the hybrid score Ë†Phybrid. To evaluate the recommendation performance, we select the commonly used normalized discounted cumula- tive gain (NDCG@N) and recall (Recall@N) with N âˆˆ [5, 10]. The predictions are ranked against all items in the dataset. More implementation details on baseline methods are in Appendix A. 5.2 Evaluation: Overall Recommendation Performance The first set of results are reported in Table 1, where we compare the recommendation perfor- mance of all schemes. In Table 1, for clarity, we highlight the best results in bold and underline the second best results. Note that the ranking results are from the entire item scope (i.e., a complete ranking of all items). From Table 1, it is observed: (1) GPT-FedRec generally achieves better recom- 0 0.1 0.3 0.5 0.7 0.9 1.0 0.01 0.02 0.03 0.04 0.05 0.06Performance R@5 N@5 R@10 N@10 0 0.1 0.3 0.5 0.7 0.9 1.0 0.02 0.04 0.06 (a) Beauty 0 0.1 0.3 0.5 0.7 0.9 1.0 0.000 0.025 0.050 0.075 (b) Games 0 0.1 0.3 0.5 0.7 0.9 1.0 0.00 0.02 0.04 0.06 (c) Toys 0 0.1 0.3 0.5 0.7 0.9 1.0 0.00 0.05 0.10 (d) ML-100K Figure 4: Sensitivity Analysis. We change Î» in Equa- tion 6, and evaluate the recommendation performance based on the hybrid retrieval results of stage 1. mendation performance than the baseline meth- ods. For instance, compared to the second best baseline method over all datasets, GPT-FedRec achieves 36.12%, 29.88%, 45.44% and 36.56% average improvements w.r.t. all 4 metrics, respec- tively. (2) GPT-FedRec achieves satisfying perfor- mance on ML-100K, whereas some baseline meth- ods, especially text-based ones, could barely con- verge. This is expected, because some of them (e.g., RecFmr, UniSR) are pretrained using Amazon re- view datasets, whose domain is different from the domain of movie recommendation. Moreover, the user history of ML-100K is much longer than that in Amazon review datasets. Since the text-based baseline methods could not handle long user his- tory, they are prone to suffer from performance degradation. On the contrary, GPT-FedRec lever- ages hybrid retrieval and exploits the pre-trained knowledge in LLMs, thus enjoying a more compre- hensive understanding of user preferences. 5.3 Sensitivity Analysis In this section, we conduct a sensitivity analysis w.r.t. Î± in Equation 63. This is a key factor affect- ing the hybrid retrieval before LLM. In particular, we change it from 0 to 1 and keep other configura- tions the same. The results are shown in Figure 4. We observe that the balance between ID-based re- trieval and text-based retrieval play an important role in terms of hybrid retrieval. As shown in Fig- ure 4, there exists an optimal Î± between 0 and 1 for different datasets respectively, indicating the necessity of hybrid retrieval in GPT-FedRec. 3Due to the limit budget of calling GPT API, we only perform sensitivity analysis for the first stage of GPT-FedRec. Dataset Metric GPT-FedRecw/o LRU w/o E5 w/o LLM Beauty R@5â†‘ 0.0348 0.0306 0.0254 0.0348 N@5â†‘ 0.0233 0.0203 0.0156 0.0232 R@10â†‘ 0.0563 0.0504 0.0478 0.0520 N@10â†‘ 0.0302 0.0267 0.0227 0.0288 Games R@5â†‘ 0.0471 0.0425 0.0438 0.0435 N@5â†‘ 0.0331 0.0259 0.0290 0.0281 R@10â†‘ 0.0764 0.0779 0.0722 0.0689 N@10â†‘ 0.0406 0.0372 0.0381 0.0364 Table 2: Ablation Study. We mask out different com- ponents of GPT-FedRec separately, and evaluate the recommendation performance. 5.4 Ablation Study Finally, we conduct an ablation study to evalu- ate the contribution of each key module of GPT- FedRec, namely the hybrid retrieval in the first stage and the LLM-based re-ranking in the second stage. The results are reported in Table 2. In Ta- ble 2, w/o LRU refers to GPT-FedRec only with E5 as retriever in the first stage, and w/o E5 indicates that there is only LRURec in the first stage. w/o LLM represents GPT-FedRec without the LLM- based re-rank. As expected, we observe that both hybrid retrieval and LLM-based re-ranking con- tributes to a better recommendation performance. For instance, without either module, the recommen- dation performance basically drops. Such results also validate the merit of our design: our hybrid re- trieval mechanism and the LLM-based re-ranking are indeed effective in terms of addressing the data sparsity and data heterogeneity in FR, which leads to better recommendation performance. 6 Conclusion This work presents a novel federated recommen- dation framework that exploits ChatGPT and a novel hybrid retrieval mechanism. GPT-FedRec provides an effective privacy-aware solution to build a recommender system for data-sparse and data-heterogeneous federated recommendation sce- narios. We highlight the significance of this work: despite the active research on federated recommen- dation, existing methods largely suffer from the data sparsity and data heterogeneity issue in FR. In contrast, our work is deliberately designed to overcome such an issue, achieving generalized rec- ommendation performance. Finally, within GPT- FedRec, there is also a hybrid RAG mechanism to prevent LLM hallucination, improving the au- thenticity of recommendation results in real-world applications. 7 Limitations One limitation of this work is that our method in- troduces extra hyperparameters. For different ap- plications, one might need to finetune these hy- perparameters, which brings extra computational cost. Another limitation of this work is that our method does not take the inherent bias of GPT into account. However, it is known that such pretrained LLMs usually have encoded the bias in the pre- training data (e.g., stereotypical data, racism and hate speech). Such bias could have negative ethical implications on the downstream FR applications. Therefore, a future research direction is to develop a benign and fair FR framework. References Muhammad Ammad-Ud-Din, Elena Ivannikova, Suleiman A Khan, Were Oyomno, Qiang Fu, Kuan Eeik Tan, and Adrian Flanagan. 2019. Feder- ated collaborative filtering for privacy-preserving personalized recommendation system. arXiv preprint arXiv:1901.09888. Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effec- tive and efficient tuning framework to align large lan- guage model with recommendation. arXiv preprint arXiv:2305.00447. Di Chai, Leye Wang, Kai Chen, and Qiang Yang. 2020. Secure federated matrix factorization. IEEE Intelli- gent Systems, 36(5):11â€“20. Zheng Chen. 2023. Palr: Personalization aware llms for recommendation. arXiv preprint arXiv:2305.07622. Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval. arXiv preprint arXiv:2010.00904. Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph neural networks for social recommendation. In The world wide web conference, pages 417â€“426. Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, person- alized prompt & predict paradigm (p5). In Proceed- ings of the 16th ACM Conference on Recommender Systems, pages 299â€“315. F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. Acm trans- actions on interactive intelligent systems (tiis), 5(4):1â€“ 19. Jesse Harte, Wouter Zorgdrager, Panos Louridas, As- terios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023. Leveraging large language models for sequential recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 1096â€“1102. Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web, pages 507â€“517. Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Ma- jumder, Nathan Kallus, and Julian McAuley. 2023. Large language models as zero-shot conversational recommenders. In Proceedings of the 32nd ACM international conference on information and knowl- edge management, pages 720â€“730. Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023a. Learning vector-quantized item representation for transferable sequential recom- menders. In Proceedings of the ACM Web Confer- ence 2023, pages 1162â€“1171. Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022. Towards universal sequence representation learning for recom- mender systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 585â€“593. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023b. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845. Wang-Cheng Kang and Julian McAuley. 2018. Self- attentive sequential recommendation. In 2018 IEEE International Conference on Data Mining (ICDM), pages 197â€“206. IEEE. Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Mah- eswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do llms understand user preferences? evaluating llms on user rating pre- diction. arXiv preprint arXiv:2305.06474. Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023a. Text is all you need: Learning language representa- tions for sequential recommendation. arXiv preprint arXiv:2305.13731. Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023b. Large language models for generative rec- ommendation: A survey and visionary discussions. arXiv preprint arXiv:2309.01157. Zhiyu Li, Yanfang Chen, Xuan Zhang, and Xun Liang. 2023c. Bookgpt: A general framework for book rec- ommendation empowered by large language model. Electronics, 12(22):4654. Guanyu Lin, Feng Liang, Weike Pan, and Zhong Ming. 2020. Fedrec: Federated recommendation with ex- plicit feedback. IEEE Intelligent Systems, 36(5):21â€“ 30. Sichun Luo, Yuanzhang Xiao, Yang Liu, Congduan Li, and Linqi Song. 2022. Towards communication efficient and fair federated personalized sequential recommendation. In 2022 5th International Con- ference on Information Communication and Signal Processing (ICICSP), pages 1â€“6. IEEE. Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-based recom- mendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval, pages 43â€“52. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273â€“1282. PMLR. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. OpenAI. Chatgpt: Optimizing language models for dialogue. openai. Damien Sileo, Wout V ossen, and Robbe Raymaekers. 2022. Zero-shot recommendation as language mod- eling. In European Conference on Information Re- trieval, pages 223â€“230. Springer. Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large lan- guage models as re-ranking agent. arXiv preprint arXiv:2304.09542. Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. 2022. Transformer memory as a differentiable search index. Advances in Neural In- formation Processing Systems, 35:21831â€“21843. Andrei Nikolaevich Tikhonov. 1963. On the solution of ill-posed problems and the method of regularization. In Doklady akademii nauk, volume 151, pages 501â€“ 504. Russian Academy of Sciences. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly- supervised contrastive pre-training. arXiv preprint arXiv:2212.03533. Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and Xing Xie. 2022. Fedcl: Federated contrastive learning for privacy-preserving recommendation. arXiv preprint arXiv:2204.09850. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombat- chai, William L Hamilton, and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowl- edge discovery & data mining, pages 974â€“983. Zhenrui Yue, Sara Rabhi, Gabriel de Souza Pereira Moreira, Dong Wang, and Even Oldridge. 2023a. Llamarec: Two-stage recommendation using large language models for ranking. arXiv preprint arXiv:2311.02089. Zhenrui Yue, Yueqi Wang, Zhankui He, Huimin Zeng, Julian McAuley, and Dong Wang. 2023b. Linear recurrent units for sequential recommendation. arXiv preprint arXiv:2310.02367. Zhenrui Yue, Huimin Zeng, Ziyi Kou, Lanyu Shang, and Dong Wang. 2022. Defending substitution-based pro- file pollution attacks on sequential recommenders. In Proceedings of the 16th ACM Conference on Recom- mender Systems, pages 59â€“70. Honglei Zhang, He Liu, Haoxuan Li, and Yidong Li. 2024. Transfr: Transferable federated recommen- dation with pre-trained language models. arXiv preprint arXiv:2402.01124. Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recom- mendation as instruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001. A Implementation Details A.1 Datasets Details and FR Setup Overall Dataset Statistics. The detailed dataset statistics (after 5-core processing) is reported in Table 3. According to Table 3, it is observed that the averaged sequence length in ML-100K is significantly larger than that of other datasets. In addition, the density of ML-100K is also much higher than others. Finally, the Beauty, Games, Toys and Auto datasets are all Amazon review datasets, whose domain is rather different from that of ML-100K. Datasets users Items Interaction Length Density Beauty 22332 12086 198K 8.88 7e-4 Games 15264 7676 147K 9.69 1e-3 Toys 19412 11924 167K 8.63 7e-3 Auto 1281 844 8K 6.70 8e-3 ML-100K 610 3650 89K 146.70 4e-2 Table 3: Overall dataset statistics. Federated Dataset Statistics. In our experiments, we randomly sample a set of users from the original datasets, and randomly distributed them onto different local clients. In terms of users, it is guaranteed that the local datasets are mutually exclusive: one user could one exist in one local dataset or only in the test dataset. The per-client statistics is summarized in Table 4. Note that considering the dataset size and training convergence, we sampled different number of training users for different datasets. For each dataset, after sampling the training users, the remaining users are used for validation and evaluation. It is observed that for Beauty, Games and Toys, the test item scope is significantly larger than each local item scope, leading to a data sparse and data heterogeneous FR setting. Datasets Users/Client Items/Client Test Users Test Items Beauty 1000 4161 17332 12086 Games 1000 3826 10264 7614 Toys 1000 4412 14412 11766 Auto 200 503 281 591 ML-100K 100 3134 109 3060 Table 4: Averaged statistics on local clients and the statistics of test users. A.2 Implementation. GPT-FedRec. To implement GPT-FedRec, we train LRURec from scratch, and finetune the pretrained E5 (e5-base-v2) using the training data. When training LRURec, the learning rate is initialized as 1e-3, and the number of local/global epochs is 80/5 (for Beauty, Games, Toys) and 60/5 (for Auto and ML-100k). As for E5, the learning rate is initialized as 1e-6, and the number of local/global epochs is 2/2. We use AdamW as the optimizer for both LRURec and E5. When generating the candidate set Ë†I in the first stage, we pick the top-20 items from the hybrid score Ë†P. In the second stage, when performing the LLM-based re-ranking, the ideal procedure is to re-rank the candidate sets of all users. However, due to the limited GPT API query budget, such an ideal evaluation procedure is too expensive and infeasible. Moreover, since we are only interested in the recommendation performance w.r.t. the top-20 items, it is equivalent and sufficient to only perform the LLM re-ranking for a subset of test users, whose candidate set includes the ground-truth item. This procedure is meaningful and fair. To see this, we emphasize that we only feed the predicted top-20 items into LLM for re-ranking. In this setting, for a test user, if the ground-truth item is not within these predicted top-20 items, then the ground-truth item will not participate in the LLM re-ranking. As such, re-ranking will not affect the ranking of the ground-truth item, and therefore, will not change the values of Recall@5, Recall@10, NDCG@5 and NDCG@10 either. Technically, to determine whether the recommendation results of a test user should be re-ranked or not, we first validate whether the ground-truth item is included its predicted candidate set (i.e., top-20 items). That is, for a test user, if the ground-truth item is within the predicted top-20 items, then the predicted top-20 items will be fed into LLM for re-ranking. Finally, when post-processing the generated texts, we also filter out the re-ranked results without ground-truth items. This procedure makes sense, because the re-ranking process is expected to explicitly focuses on re-ranking the top-20 items with ground-truth items in them. However, if a generated re-ranked item list does not contain the ground-truth item, we ignore this re-ranked list and use the candidate set from stage one as the final recommendation for this test user. For instance, we observe that GPT-3.5 may ignore some technical parameters within some product titles or abbreviate some product titles. This leads to a discrepancy between the true titles of the ground-truth items and the generated ones, even if in human eyes they may represent the same product. Such generated results are treated as noisy generation and are ignored when calculating the evaluation metrics in our evaluation. Baselines. For all baseline methods, we refer to the original papers and the official implementations. Moreover, for non-FR methods, we used the same code of FedAvg used for GPT-FedRec to perform global model aggregation. For all baselines, we train the models by starting with the default training/finetuning configuration. For FedSAS, FedLRU, CF-FedSR and TransFR, the models are trained from scratch. In comparison, we load the released pretrained weights for P5, RecFmr and UniSR, and finetune them in our FR setting. Finally, we observed training divergence and overfitting of some baseline methods, and therefore, adjusted both local epochs and global epoch until find the best performance. B Prompt Engineering E5 Templates. We follow the instructions in (Wang et al., 2022), and design our E5 templates as follows: ### query: {history} ### passage: {candidate}, where history shall be replaced with history item titles and genres/categories. candidate shall be replaced with the title and category/genre of a single candidate item. Moreover, in our experiments, we notice that it is more beneficial to only use last several movies for the ML-100K dataset, and does not use genre. For instance, an exemplar complete template on ML-100K is like (note that the history does not need to be the complete history and the description of genres may be removed for better performance): ### query: The Shawshank Redemption, a movie about Thriller ; Ex Machina , a movie about Sci-Fi, Thriller ; Unchained, a movie about Drama, Western . ### passage: Whiplash, a movie about Drama . GPT-3.5-Turbo Templates. Inspired by (Hou et al., 2023b), we use the following templates to prompt GPT-3.5-Turbo: ### system: You are a helpful {role}, {role description }. ### user: Iâ€™ve browsed the following items in the past in order: {history}. There is also candidate pool: {candidates}. {task instruction }. In the above template, role and role description are replaced with a concrete real-world job (e.g., movie reviewer) and its job description (e.g., recommending movies for people). history is instantiated as item texts containing item titles and item categories/genres. As for candidates, it is replaced with the hybrid retrieval results. task instruction shall be replaced with a direct re-rank order. We also add additional formatting instructions in the template for post-processing purposes. An exemplar prompt for ML-100K is like: ### system: You are a movie fan and movie reviewer . Therefore, people might ask you to recommend movies. ### user: Iâ€™ve browsed the following items in the past in order: The Shawshank Redemption, a movie about Thriller ; Ex Machina , a movie about Sci-Fi, Thriller ; Unchained, a movie about Drama, Western . There is also candidate pool: 1. The Green Mile (1999) 2. Pulp Fiction (1994) 3. Seven (1995) Please rank these movies by measuring the possibilities that I would like to watch next most, according to my watching history. Please think step by step. Please show me your ranking results with order numbers. Split your output with line break. You MUST rank the given candidate movies. You can not generate movies that are not in the given candidate list .