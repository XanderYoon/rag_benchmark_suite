Predicting Retrieval Utility and Answer Quality in Retrieval-Augmented Generation Fangzheng Tian , Debasis Ganguly , and Craig Macdonald University of Glasgow, Glasgow, UK f.tian.1@research.gla.ac.uk,debasis.ganguly@glasgow.ac.uk, craig.macdonald@glasgow.ac.uk Abstract.The quality of answers generated by large language models (LLMs) in retrieval-augmented generation (RAG) is largely influenced by the contextual information contained in the retrieved documents. A key challenge for improving RAG is to predict both the utility of retrieved documents—quantified as the performance gain from using context over generation without context—and the quality of the final answers in terms of correctness and relevance. In this paper, we define two prediction tasks within RAG. The first is retrieval performance prediction (RPP), which estimates the utility of retrieved documents. The second is generation performance prediction (GPP), which estimates the final answer quality. We hypothesise that in RAG, the topical relevance of retrieved doc- uments correlates with their utility, suggesting that query performance prediction (QPP) approaches can be adapted for RPP and GPP. Beyond these retriever-centric signals, we argue that reader-centric features, such as the LLM’s perplexity of the retrieved context conditioned on the in- put query, can further enhance prediction accuracy for both RPP and GPP. Finally, we propose that features reflecting query-agnostic docu- ment quality and readability can also provide useful signals to the pre- dictions. We train linear regression models with the above categories of predictors for both RPP and GPP. Experiments on the Natural Ques- tions (NQ) dataset show that combining predictors from multiple feature categories yields the most accurate estimates of RAG performance. Keywords:Large Language Models·Retrieval Augmented Generation ·Query Performance Prediction·Perplexity·Readability. 1 Introduction Retrieval-Augmented Generation (RAG) is a framework that integrates the in- herent parametric knowledge of large language models (LLMs) with retrieved documents to supplement knowledge [38], and mitigate hallucinations [32]. Fig- ure1showshowalistoftop-kdocumentsretrievedforaqueryactsasasourcefor contextualgenerationviaanLLM,aprocessoftencalled“k-shotgeneration” [24]. Although the standard RAG framework follows a static workflow that treats all queries uniformly [34,38], recent studies have shown the potential of adopting arXiv:2601.14546v1 [cs.IR] 20 Jan 2026 2 Tian et al. ΔP LLM LLM zero-shot generation k-shot generation (RAG) U Retriever Context Perplexity Confidence in Answer Pre-generation Predictors Post- generation GPP RPPDocument Quality P0 Pk dk a0 ak q QPP Approaches Fig.1: An illustration of how the proposed prediction tasks fit into a standard RAG workflow. Retrieval Performance Prediction (RPP) estimates the gain in answer quality ofk-shot relative to zero-shot (see Equation (1)). Generation Performance Prediction (GPP) directly estimates thek-shot answer quality. Both predictions rely on informa- tion extracted from the retrieved documents and the generated answer (as shown in the blue area at the bottom). The grey area denotes components not accessible to any predictors. dynamic, query-specific workflows [7]. In such dynamic workflows, the number of retrieved documents or even the choice of ranking model may vary depending on the characteristics of the query [30,33,48,58]. It has been shown that predicting the retrieval quality in a standard IR task by query performance prediction (QPP) models [18,21,28] leads to effective adaptive IR pipelines [17]. This motivates us to hypothesise that accurately predicting both the utility of retrieved documents as RAG context, where utility is defined as the answer quality gain ofk-shot generation relative to zero-shot (U in Figure 3) [55], and the quality of the generated answers can further enhance RAG effectiveness. The primary objective of this work is therefore to investigate methods for estimating context utility and answer quality in RAG, while leaving the downstream integration of these estimators into adaptive RAG systems for future research. To further clarify the two types of predictors examined in this paper, we consider two central questions when predicting RAG performance: –Do the retrieved documents improve answer quality compared to zero-shot generation? We call this taskretrieval performance prediction (RPP). –Do the retrieved contexts provide sufficient information for the LLM to gen- erate correct answers that satisfy the query’s information need? We denote this asgeneration performance prediction (GPP). These two predictor components are illustrated in the bottom-right part of Fig- ure 1. In addressing both RPP and GPP, we posit that the relevance of retrieved documents is a likely indicator of both context utility and answer quality, as con- ditional generation with relevant documents potentially leads to factually cor- rect and relevant answers [55]. However, since the ground-truth relevance is not known to a RAG system, the relevance estimated from existing QPP approaches may act as an effective proxy [54]. Predicting Retrieval Utility and Answer Quality in RAG 3 However, different from standard IR, where the end user of the retrieved documents is a human, in RAG, the retrieved documents are consumed by an LLM [46]. As a result, RPP and GPP should account not only for topical rele- vance but also for how the context interacts with the LLM [1]. This motivates us to explore indicators of quality other than relevance alone. One of the factors on which the utility of a document depends is the per- plexity, which captures how misaligned the retrieved context is with an LLM’s own internal knowledge and semantics [52]. To illustrate, in response to a sam- ple query from the NQ dataset (as used in our experiments) “Who won the most MVP awards in the NBA?”, a known relevant document’s content –“the award a record six times. He is also the only player...” is topically related. However, as it omits the player’s name, when used as a RAG context, this docu- ment does not lead to the correct answer despite a zero-shot generation correctly outputting “Kareem Abdul-Jabbar”. Because the player’s name is omitted, the LLM cannot reliably associate this relevant document with the target query, which manifests as higher conditional perplexity. This example suggests that the perplexity of the retrieved context, reflecting the LLM’s uncertainty when conditioning on the input query, can serve as a useful predictive signal for both RPP and GPP. Beyond relevance and perplexity,query-agnosticdocument quality charac- teristics such as readability and relevance priors may also influence the utility of retrieved contexts and the quality of generated answers. As an analogy, for human readers, overly complex text can limit usability [25], and low-quality documents may fail to support any query [4]. In this work, we examine two variants of RPP and GPP: (a)pre-generation (PreGen) and (b)post-generation(PostGen). The PreGen predictor relies solely on information from the retriever, making it more efficient since it does not require the generator’s output. By contrast, the PostGen predictor can exploit additional signals, such as the perplexity of the generated answers [52], thereby offering improved effectiveness at the expense of a modest reduction in efficiency. Our Contributions.In summary, the main contributions of this paper are: –We apply existing QPP approaches for two novel prediction tasks in a RAG setting: retrieval performance prediction (RPP), and generation performance prediction (GPP). –In addition to relevance estimation via QPP, we further propose to leverage context perplexity and query-agnostic document quality measures to improve RPP and GPP. –We show that an ensemble of predictors learned via linear regression consis- tently outperforms the individual predictors across different retrieval models and context sizes. 2 Related Work Predicting RAG Performance.In retrieval-augmented generation (RAG), standard IR metrics (e.g., nDCG) are insufficient proxies for utility because re- 4 Tian et al. trieved documents are consumed by an LLM rather than directly by human users [46]. Prior work shows that context utility depends on multiple factors, including the context length [55], relevant document position [39], prompt struc- ture[12],andknowledgeconflictsbetweentheretrieveddocumentandtheLLM’s parametric knowledge [41]. Existing approaches to predicting RAG answer qual- ity mainly focus on uncertainty estimation. Answer-level semantic uncertainty is used to infer generation quality [14,22,47,52], but typically requires multiple sampled generations, limiting its practicality. Token-level uncertainty has also been used to identify unreliable spans in the answer [33,45,59]. In this line of work, [29] defines context utility as the reduction in answer uncertainty; how- ever, this notion is disconnected from factuality and relevance, which are central to downstream evaluation. Beyond uncertainty-based signals, some supervised methods predict whether retrieval is necessary by estimating query complex- ity [31,58]. More recently, [60,61] directly apply LLMs to assess the utility of individual retrieved documents. Query Performance Prediction (QPP).QPP methods estimate retrieval effectiveness by leveraging information from the top-retrieved documents and the query [6,28,43]. Score-based approaches examine retrieval score distribution to assess the separation between relevant and non-relevant documents [11,13,50], and the coherence of top-ranked results [2]. Embedding-based methods exploit dense query–document representations to capture structural topology [21] and inter-document coherence [51,56]. Supervised models directly use the text of the query and the top-retrieved documents to predict IR metrics [3,16,18,20]. Reader-Oriented Document Evaluations.For human readers, document utility is often assessed through readability [27]. Readability measurements typ- ically rely on features such as sentence length, word difficulty, and syntactic complexity [10,36,53]. Beyond readability, the inherent quality of a document may also limit its usefulness, with some documents containing little or no valu- able information [4]. Recent approaches, such as QualT5 [8], explicitly estimate query-agnosticdocumentquality.InRAG,whethertheseconstraintsaffectLLMs as the new “readers” remains underexplored, highlighting potential differences in how humans and LLMs use retrieved text. Research Gap.Prior work has not yet systematically studied context utility and answer quality prediction in RAG. The closest effort, [61], assesses the utility of individual retrieved documents rather than predicting the utility of RAG contexts concatenated from multiple documents. Moreover, it relies on LLM- based utility estimation and does not explore the use of existing retrieval analysis tools, such as QPP or document quality signals, in the RAG setting. Predicting Retrieval Utility and Answer Quality in RAG 5 3 Predicting Retrieval Utility and Answer Quality In this section, we formally define the tasks of RPP and GPP along with their prediction targets. We then introduce three types of predictors and propose combining them to improve prediction accuracy. 3.1 Prediction Task Description Preliminaries.Letθ R denote a retriever that maps a queryqto a ranked list of documents{d 1, . . . , dk}, andθ G denote a generator that maps a prompt to an answera. Thek-shot answer is denoted asa k =θ G(q;θ R(q)k), where the prompt consists of the queryqandθ R(q)k, the latter denoting the top-k retrieved documents obtained fromθR as context forq. Similarly, we can obtain a zero-shot answer, asa0 =θ G(q;∅). Ifk-shot RAG is helpful, we would expect the generated answera k to be enhanced compared to the zero-shot answera0. Indeed, [55] definedcontext utilityas the actual influence of retrieved documents on downstream answer quality. Specifically, similar to [55], we define the utility Uof the top-kretrieved documents (when used as RAG context) as: U(θ R(q)k) =P(θ G(q;θ R(q)k))−P(θ G(q;∅)),(1) wherePis a task-specific performance measure (e.g., F1 score for QA on NQ [9]). Retrieval Performance Prediction (RPP).The first prediction module in our RAG workflow, RPP, estimates the utility of retrieval results. The goal is to decide whether using the retrieved context is beneficial compared to zero-shot generation, without directly observing bothk-shot and zero-shot performances as in Equation (1). Different from QPP in IR, which focuses on topical relevance, RPP targets context utilityUof the top-kretrieved documentsθR(q)k, reflecting its support for LLM to answer the queryq. Formally, an RPP modelϕRP P : q, θR(q)k 7→R, where the output is an estimate of the utilityUofθ R(q)k. The prediction may be based on features of the query, the retrieved context, or (in post-generation settings) the generated answer. An accurate RPP model should assign higher predicted scores to the contexts that provide higher utility for answeringqin RAG. Generation Performance Prediction (GPP).The second prediction mod- ule, GPP, differs from RPP in its prediction target. While RPP estimates the utility of retrieved documents relative to zero-shot generation, GPP directly predicts the quality of the generator’s final output under the current RAG con- figuration. A good GPP model can therefore indicate how reliable the produced answer will be, taking into account both the retrieved context and the LLM’s inherent knowledge. Formally, the target of GPP is the answer qualityP(ak), computed against ground truth answers. A GPP predictor is defined asϕGP P :q, a k 7→R, where the output is an estimate of the answer quality. An accurate GPP model should 6 Tian et al. assign higher scores to answers that better satisfy the information need of the input query. Equivalently, when ranking a set of generated answers, the GPP- induced ranking should be positively correlated with the ground-truth ranking by answer quality. Similar to RPP, GPP can draw on multiple signals from both the retrieved context and the generated output, which we introduce next in Section 3.2. 3.2 Information Sources for RPP and GPP Accurate predictions for a RAG system require analyses from multiple perspec- tives. Each analysis yields features related to retrieval utility and answer quality, which can be formulated as a predictorϕ. We group these predictors into three categories: (i) predicted relevance of the context, (ii) perplexity of the context or the generated answer, and (iii) intrinsic quality of the documents. Context Relevance Predictors.The first category of predictors predicts the relevance of the retrieved context, adapted from existing query performance prediction (QPP) approaches. Since the goal of a retriever is to return relevant documents, QPP methods analyse retrieval results from the retriever’s perspec- tive, and we therefore describe these asretriever-centric. These predictors rely on signals available directly from the retrieval process, such as retrieval scores, dense embeddings, and document texts. QPP encompasses a diverse set of methods, each targeting a different aspect of retrieval quality. Some estimate the upper bound of list relevance [56], while others examine the distribution of the retrieval scores [11,50]. Still others focus on the coherence of retrieved documents [21], which can also influence RAG performance. However, all QPP methods treat documents as independent units, making it difficult to capture cross-document dependencies and sentence-level interactions that emerge once the documents are concatenated and presented as a single context to an LLM in RAG [1]. Perplexity-based predictorsIn contrast to QPP approaches, the second cat- egory of predictors isreader-centric, analysing the RAG context and answer from the LLM’s perspective. These predictors leverage the token probabilities assigned by the model to estimate its internal certainty: given a query, how well the retrieved context aligns with the model’s expectations, and given both query and context, how confident it is in its generated answer [35]. A widely-used mea- sure of an LLM’s confidence in its output isanswer perplexity(PerpA), defined as the exponential of the mean negative log-probability of generated answer to- kens [49]. Higher perplexity PerpAis empirically associated with increased risk of hallucination [32,59]. We extend this perplexity-based measurement to evaluate the retrieved con- text before generation. Given a query, if a retrieved context causes a lower perplexity as analysed by an LLM, it may indicate greater consistency with the model’s expectations and inherent knowledge. To distinguish it from post- generation PerpA, we refer to this measure ascontext perplexity(PerpC). Predicting Retrieval Utility and Answer Quality in RAG 7 You are an expert at answering questions based on your own knowledge and related context. Please answer this question based on the given context within 5 words. You should put your answer inside <answer> and </answer>. Question:Who won the most MVP awards in the NBA? Doc 1: .... Doc 2: .... Now start your answer. <answer> Fig.2: RAG prompt for generating answers for NQ datasets. The first part of an answer unit <answer> is put at the end to prompt the LLM to yield immediate answers. Document quality measurements.Unlike the previous two categories, the third category of predictors isquery-agnostic, providing estimations from the in- trinsic complexity and quality of the retrieved documents. Readability measures assess text difficulty through factors such as word familiarity, sentence structure, and character statistics [27]. More recently, supervised quality models estimate theinherentusefulnessofadocumentindependentofanyquery[8].Thesepredic- torscomplementretriever-centricandreader-centricsignalsbyofferingabaseline view of the context’s quality, regardless of query or model interpretation. 3.3 Combining Multiple Predictions Each potential predictor for RPP and GPP captures a different aspect of RAG performance. Hence by combining them, we can potentially perform a more comprehensive [5] analysis of the retrieval results and generated answers. By leveraging the complementary benefits of the predictors, such combinations can potentially lead to better estimates for both RPP and GPP, as also observed in QPP applications [23]. In particular, we construct an ensemble of the predictors by applying linear regression to capture the relationship between their outputs and the target values —context utility for RPP and answer quality for GPP—on training queries. Formally, L(Φ, Y;w) = X q∈Q (y(q)−w·ϕ(q)) 2,(2) wherew∈R n are the learnable weights of the predictor outputϕ(q)∈R n for queryq,Qis a set of training queries with ground-truth labels, andy(q) denotes either the context utility for RPP, or the answer quality for GPP. This loss function measures the squared error between the predictionw·ϕ(q)and the target metric’s ground truthy(q). Minimising this loss corresponds to finding weightsthatproducepredictionsthatareasclosetothegroundtruthaspossible, thereby achieving higher accuracy in RPP and GPP. 4 Experimental Setup Research Questions.To assess our framework’s ability to predict context utility (RPP) and answer quality (GPP), we pose four research questions. RQ-1 8 Tian et al. evaluates existing QPP methods when applied directly to the two prediction tasks. RQs 2-4 progressively enrich the predictor set in the linear regression model described in Section 3.3, trained for both tasks using the loss in Equa- tion (2). Together, these RQs examine the contribution of different predictor categories and their combinations to RPP and GPP accuracy. –RQ-1: How accurate are existing QPP approaches for RPP and GPP? –RQ-2: Does adding reader-centric context perplexity (PerpC) improve pre- diction accuracy over QPP alone? –RQ-3: Do query-agnostic document quality and readability metrics provide additional predictive value for RPP and GPP? –RQ-4: Does integrating pre-generation predictors with post-generation answer perplexity (PerpA) improve prediction accuracy? Datasets.We evaluate the prediction accuracy of our proposed approaches on Natural Question [37] (NQ), a widely-used open-domain QA dataset. Answer quality is evaluated by F1 score against the provided golden answer [9]. Context documents for NQ queries are retrieved from a snapshot of English Wikipedia from 2018. The accuracy of predictors in RPP and GPP is tested on the NQ test set (3610 queries). Prediction accuracy is measured by Spearman’sρcorrelation between the predicted scores and the ground-truth targets, specifically context utility (1) for RPP, and answer quality (F1 Score) for GPP. RAG Configurations.To assess the applicability of our predictors for RPP and GPP, we experiment with three retrieval configurations1: – BM25[44]: A lexical retriever based on term frequency, inverse document frequency, and document length normalisation. – BM25≫MonoT5[42]: A retrieve-and-rerank pipeline, where the top-100 BM25 results are re-ranked using MonoT5, a cross-encoder re-ranker. – E5[57]: A BERT-based bi-encoder trained with contrastive learning, leverag- ing large batch sizes and in-batch negatives. The above three rankers yield different overall answer quality on NQ in our experiments, with average F1 scores of 0.3284, 0.4019, and 0.4798, respectively (top-2 retrieved documents as context). We vary the RAG context size withk∈ {2,3,5,7,10}. For answer generation, we employ an 8-bit quantised, instruction-tuned version of Llama-3-8B2 [19], guided by the prompt template shown in Figure 2 that instructs the model to produce concise and factually correct answers. 4.1 Investigated Predictors QPP Approaches.We experiment with existing QPP approaches: – NQC[50], which analyses the variance of retrieval scores of the top-retrieved documents, integrating with query term frequency. 1 We perform all indexing and retrieval using the PyTerrier framework [40]. 2 QuantFactory/Meta-Llama-3-8B-GGUF Predicting Retrieval Utility and Answer Quality in RAG 9 Table 1: Prediction accuracy of single QPP predictors in RPP (left) and GPP (right). Results are reported on the NQ test set using the top-2 retrieved documents as RAG context (k=2). The best method for each retrieval configuration is bold-faced. RPP GPP QPP Method BM25 MonoT5 E5 BM25 MonoT5 E5 NQC 0.0694 -0.0643 0.0994 0.0610 -0.1288 0.1210 MaxScore 0.1485 0.1625 0.1288 0.1609 0.2218 0.2344 DenseQPP 0.17390.20270.10250.2166 0.29920.1922 A-Pair-Ratio 0.1565 0.01360.19130.1353 -0.04830.2470 BERT-QPP0.18980.1474 0.1352 0.2079 0.1641 0.1630 – MaxScore[56], which uses the score of the top-ranked document as an esti- mation of the retrieved context’s upper bound on relevance. – Dense-QPP[21], which computes the volume of the minimal hypercube cov- ering the embeddings of the query and its top-kdocuments (kequals the context size). E5 is used to obtain the embeddings [57]. – A-Pair-Ratio[56], which measures coherence by comparing pairwise simi- larities between the top-5 and bottom-5 documents from the top-100. – BERT-QPP[3], a cross-encoder trained to predict MRR from the text of a query and the corresponding top-retrieved document. Perplexity-Based Predictors.As introduced in Section 3.2, we use context perplexity (PerpC) and answer perplexity (PerpA) as reader-centric predictors. Both predictors are derived from the model’s next-token probabilities provided by Llama-3-8B. In our experiments, PerpCserves as a pre-generation predictor, while PerpAis used as an additional post-generation predictor for both RPP and GPP. For implementation, we compute PerpAand PerpCas the exponentials of theaveragelog-probabilities(withoutnegation),sothathighervaluescorrespond positively with answer quality and context utility. Document Quality Measures.To capture both document- and context-level signals,wecomputethemaximum,minimum,andaveragescoreacrossindividual documents, as well as a direct score for the concatenated context, using the following document readability and quality measures: – TraditionalReadabilityMetrics,includingDale-Chall[15]andSpache[53], which rely on word familiarity and lexical difficulty; Flesch–Kincaid [27] and GunningFog[26],whichanalysesentencestructurecomplexity;Coleman–Liau[10], which uses character-based counts as proxies for reading effort. – QualT5[8]: a supervised T5-based model trained to estimate document qual- ity in a query-agnostic manner, designed to assess the intrinsic usefulness of documents for downstream retrieval tasks. We use linear regression (Equation (2)) to combine 2 or more predictors, trained on the NQ dev set (8757 queries), using context utility (U) and answer quality (P) as targets for RPP or GPP, respectively. 10 Tian et al. Table 2: Prediction accuracy of combining various predictor groups using linear regres- sion, fork= 2retrieved documents. The best scores for the PreGen and the PostGen settings are bold-faced separately. A “†” indicates statistical significance (Fisher’sz, 95% confidence) of a PreGen predictor over a QPP-only setting, whereas a “‡” indicates statistical significance of a PostGen predictor over a PreGen setting. Features (ϕ) RPP GPP Type PerpAQPP PerpCRead Qual BM25 MonoT5 E5 BM25 MonoT5 E5 Pre Gen ✓0.2146 0.20080.21550.2454 0.2861 0.2952 ✓ ✓0.2365 0.2001 0.2110 0.2821 † 0.30400.3094 ✓ ✓ ✓0.24190.2136 0.21010.2948 † 0.3071 0.3075 ✓ ✓ ✓ ✓0.23800.2350 † 0.2093 0.2945† 0.3253† 0.3088 Post Gen ✓0.1468 0.1351 0.1470 0.3037 0.2579 0.2873 ✓ ✓0.2387 0.2283 0.2513 ‡ 0.3507‡ 0.3616‡ 0.3915‡ ✓ ✓ ✓0.2534 0.2280 0.2474 ‡ 0.3653‡ 0.3726‡ 0.3988‡ ✓ ✓ ✓ ✓0.25730.24040.2476 ‡ 0.3729‡ 0.3737‡ 0.3977‡ ✓ ✓ ✓ ✓ ✓0.25390.25630.2472 ‡ 0.3727‡ 0.3856‡ 0.3974‡ 5 Results We first report the results using the top-2 retrieved documents as context in Section 5.1 to answer the research questions. Then, we discuss how prediction accuracy varies with context size in Section 5.2. 5.1 Main Observations RQ1: Usefulness of QPP approaches for RPP and GPP.Table 1 reports the results of existing QPP approaches for RPP and GPP with each column corresponding to a particular ranker. Overall, QPP estimates are positively cor- related with both context utility (RPP) and answer quality (GPP), confirming that topical relevance remains an informative signal for predicting RAG perfor- mance, consistent with prior findings [55]. From Table 1, we observe that QPP methods generally achieve higher correlations in GPP than in RPP, suggesting that answer quality is easier to approximate than predicting utility because it likely depends not only on the retrieved context’s relevance but also on how that information interacts with the LLM’s inherent knowledge [41]. The most effec- tive QPP predictor varies with the retriever model used – consistent with earlier findings that QPP accuracy is ranker-dependent [23]. For BM25 and MonoT5, DenseQPP leads to the best GPP results indicating that the density of the doc- ument embeddings is a useful indicator of the answer quality. However, the ef- fective performance of MaxScore with MonoT5 and A-pair ratio on E5 indicates that no QPP approach works consistently the best across all rankers. To conclude for RQ-1, existing QPP approaches exhibit different appli- cability to RPP and GPP in the RAG pipeline. While some approaches achieve good accuracy, their accuracy is not consistent across retrieval configurations. Predicting Retrieval Utility and Answer Quality in RAG 11 RQ-2: Effectiveness of combining QPP approaches with context per- plexity (PerpC).Table 2 reports results when combining multiple predictors using linear regression as introduced in Section 3.3. The first row in the upper part shows that aggregating multiple QPP predictors generally outperforms the best individual QPP method in Table 1, especially for BM25 and E5. The second row in Table 2 shows the results of combining QPP and reader- centric PerpC. Adding PerpCconsistently improves prediction accuracy in GPP across all retrievers (e.g., the significant improvement from 0.2454 to 0.2821 with BM25), confirming our hypothesis that context perplexity influences the LLM to digest the context and produce high-quality answers. For RPP, however, gains in prediction accuracy appear only with BM25. This suggests that PerpCis more discriminative for weaker retrievers, where the perplexity of retrieved contexts varies more sharply between low- and high-utility cases. To conclude for RQ-2, combining PerpCwith QPP predictors yields more significant gains in accuracy with weaker retrievers and is especially effective for predicting answer quality rather than predicting context utility. RQ-3: Effectiveness of leveraging query-agnostic predictors.The last two rows in the upper part of Table 2 report results when adding query-agnostic readability (Read.) and document quality (Qual.) predictors to the predictor combination. Compared with QPP and PerpC, we observe small but consis- tent gains, mostly for BM25 and MonoT5. For BM25, adding readability alone already achieves the best pre-generation results. These findings suggest that doc- ument quality metrics, originally designed for human readers, offer only limited additional value for RAG prediction, where the reader is an LLM. To conclude for RQ-3, document quality and readability predictors can slightly improve RPP and GPP accuracy when combined with QPP and PerpC. RQ-4: Effectiveness of Leveraging Post-Generation Prediction.The lower part of Table 2 reports results for answer perplexity (PerpA) and its in- tegration with pre-generation predictors. Stand-alone PerpAshows a moderate positive correlation with answer quality in GPP (Spearman’sρat 0.25–0.30), but weaker correlations with retrieval utility in RPP, since utilityUalso depends on how retrieved documents interact with the LLM’s inherent knowledge, a factor not fully reflected in the LLM’s confidence of the answer [49]. When PerpAis combined with pre-generation signals, prediction accuracy improvesforbothRPPandGPPcomparedtoeitherPerpAorthepre-generation signalsalone.Forinstance,inGPPwithE5,combiningPerpAwithQPPachieves a correlation of 0.3915, compared to 0.2873 for PerpAalone and 0.2952 for QPP. These improvements are consistently significant under Fisher’sz-test, indicat- ing that the model’s self-confidence, captured by PerpA, complements the pre- generation signals from the context to yield stronger predictions. Nonetheless, this benefit comes at the cost of waiting for the full answer to be generated. To conclude for RQ-4, incorporating post-generation PerpAconsistently improves RPP and GPP accuracy over pre-generation predictors. 12 Tian et al. 2 3 5 7 10 Context Size (k) 0.2 0.3 0.4Spearman's (a) RPP Accuracy 2 3 5 7 10 Context Size (k) (b) GPP Accuracy A-Pair-Ratio PreGen-Ensmbl ProbA PostGen-Ensmbl Fig.3: Prediction accuracy with varying context size from 2 to 10. Sub-graph (a) shows results for RPP, and (b) for GPP. The retriever is E5 for both settings. 5.2 Prediction Accuracy for Varying Context Size (k) As the size of the RAG context (in terms of the number of documents in the context) changes, both answer quality and context utility may vary [55], which in turn affects the prediction accuracy of RPP and GPP. Figure 3 shows the results for the E5 retriever across context sizes from 2 to 10. We compare: (1) the best- performing pre-generation method (A-Pair-Ratio, see Table 1); (2) the combina- tion of all pre-generation predictors (PreGen-Ensmbl); (3) the post-generation predictor PerpA; and (4) the full combination of all pre- and post-generation predictors (PostGen-Ensmbl). At each context size, ensembles consistently outperform individual predic- tors: PreGen-Ensmbl is always more accurate than the best single pre-generation method, and PostGen-Ensmbl yields the best overall performance. This aligns with our earlier findings in RQs, where combining predictors produces more accurate predictions than using them in isolation. Overall, the accuracy of pre- generation predictors declines askgrows for both RPP and GPP, reflecting the increasing difficulty of analysing long concatenated contexts. Although combin- ingallpre-generationpredictorsimprovesaccuracycomparedtothebestindivid- ual predictor, it cannot counteract the downward trend. The gains achieved by combining pre-generation predictors become larger askin RPP, suggesting that reader-centric and query-agnostic signals help capture inter-document relations missed by QPP alone, especially for long RAG contexts. By contrast, the post-generation predictor PerpAimproves steadily as the context size increases. When incorporated with pre-generation predictors, they yield the best overall prediction performance. However, askincreases, the con- tribution of pre-generation signals in the combination diminishes, as shown by the convergence of the curves of PerpAand PostGen-Ensmbl in Figure 3. 6 Conclusions In this paper, we propose two prediction tasks for RAG:RPP, targeting context utility, andGPP, targeting answer quality. Our experiments show that a range Predicting Retrieval Utility and Answer Quality in RAG 13 of different information sources — relevance estimations from QPP models, per- plexity of retrieved and generated content, readability and relevance priors — is useful for RPP and GPP. An ensemble approach learned via linear regres- sion consistently outperforms the individual predictors across different rankers and context sizes. We also find that predicting utility (RPP) is inherently harder thanpredictinganswerquality(GPP),sinceutilitydependsnotonlyonrelevance but also on how retrieved context interacts with an LLM’s inherent knowledge. While our results shed light on prediction-driven analysis for RAG, effectively leveraging such predictions in per-query context optimisation requires improv- ing predictor precision. In addition, evaluating RPP and GPP across a broader range of datasets and LLMs is important for future work, as context utility may vary with task and model choice. Disclosure of Interests.There is no competing interest. References 1. Alaofi, M., Arabzadeh, N., Clarke, C.L.A., Sanderson, M.: Generative Information Retrieval Evaluation (2024),https://arxiv.org/abs/2404.08137 2. Arabzadeh, N., Bigdeli, A., Zihayat, M., Bagheri, E.: Query Performance Predic- tion Through Retrieval Coherency. In: Advances in Information Retrieval: 43rd Eu- ropean Conference on IR Research, ECIR 2021, Virtual Event, March 28 – April 1, 2021, Proceedings, Part II. p. 193–200. Springer-Verlag, Berlin, Heidelberg (2021), https://doi.org/10.1007/978-3-030-72240-1_15 3. Arabzadeh, N., Khodabakhsh, M., Bagheri, E.: BERT-QPP: Contextualized Pre- trained transformers for Query Performance Prediction. In: Proceedings of the 30th ACM International Conference on Information & Knowledge Management. p. 2857–2861. CIKM ’21, Association for Computing Machinery, New York, NY, USA (2021),https://doi.org/10.1145/3459637.3482063 4. Azzopardi, L.: Theory of Retrieval: The Retrievability of Information. In: Proceed- ings of the 2015 International Conference on The Theory of Information Retrieval. p. 3–6. ICTIR ’15, Association for Computing Machinery, New York, NY, USA (2015).https://doi.org/10.1145/2808194.2809444 5. Bendersky, M., Croft, W.B., Diao, Y.: Quality-Biased Ranking of Web Documents. In: Proceedings of the Fourth ACM International Conference on Web Search and Data Mining. p. 95–104. WSDM ’11, Association for Computing Machinery, New York, NY, USA (2011),https://doi.org/10.1145/1935826.1935849 6. Carmel, D., Kurland, O.: Query Performance Prediction for IR. In: Proceed- ings of the 35th International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval. p. 1196–1197. SIGIR ’12, Association for Comput- ing Machinery, New York, NY, USA (2012),https://doi.org/10.1145/2348283. 2348540 7. Chandra, M., Ganguly, D., Ounis, I.: One Size Doesn’t Fit All: Predicting the Number of Examples for In-Context Learning. In: Advances in Information Re- trieval: 47th European Conference on Information Retrieval, ECIR 2025, Lucca, Italy, April 6–10, 2025, Proceedings, Part I. p. 67–84 (2025).https://doi.org/ 10.1007/978-3-031-88708-6_5 14 Tian et al. 8. Chang, X., Mishra, D., Macdonald, C., MacAvaney, S.: Neural Passage Quality Es- timation for Static Pruning. In: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 174–185. SIGIR ’24, Association for Computing Machinery, New York, NY, USA (2024), https://doi.org/10.1145/3626772.3657765 9. Chen, A., Stanovsky, G., Singh, S., Gardner, M.: Evaluating Question Answering Evaluation. In: Proceedings of the 2nd Workshop on Machine Reading for Question Answering. pp. 119–124. Association for Computational Linguistics, Hong Kong, China (Nov 2019),https://aclanthology.org/D19-5817/ 10. Coleman, M., Liau, T.: A Computer Readability Formula Designed for Machine Scoring. Journal of Applied Psychology60, 283–284 (04 1975).https://doi.org/ 10.1037/h0076540 11. Cronen-Townsend, S., Zhou, Y., Croft, W.B.: Predicting Query Performance. p. 299–306. SIGIR ’02, Association for Computing Machinery, New York, NY, USA (2002).https://doi.org/10.1145/564376.564429 12. Cuconasu, F., Trappolini, G., Siciliano, F., Filice, S., Campagnano, C., Maarek, Y., Tonellotto, N., Silvestri, F.: The Power of Noise: Redefining Retrieval for RAG Systems. In: Proceedings of the 47th International ACM SIGIR Confer- ence on Research and Development in Information Retrieval. p. 719–729. SI- GIR ’24, Association for Computing Machinery, New York, NY, USA (2024), https://doi.org/10.1145/3626772.3657834 13. Cummins, R., Jose, J., O’Riordan, C.: Improved Query Performance Prediction Using Standard Deviation. In: Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 1089–1090. SIGIR ’11, Association for Computing Machinery, New York, NY, USA (2011), https://doi.org/10.1145/2009916.2010063 14. Dai,L.,Xu,Y.,Ye,J.,Liu,H.,Xiong,H.:SePer:MeasureRetrievalUtilityThrough The Lens Of Semantic Perplexity Reduction. In: The Thirteenth International Conferenceon LearningRepresentations(2025),https://openreview.net/forum? id=ixMBnOhFGd 15. Dale, E., Chall, J.S.: A Formula for Predicting Readability. Educational Research Bulletin27(1), 11–28 (1948),http://www.jstor.org/stable/1473169 16. Datta, S., Ganguly, D., Greene, D., Mitra, M.: Deep-QPP: A Pairwise Interaction- based Deep Learning Model for Supervised Query Performance Prediction. In: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. p. 201–209. WSDM ’22, Association for Computing Machinery, New York, NY, USA (2022),https://doi.org/10.1145/3488560.3498491 17. Datta, S., Ganguly, D., MacAvaney, S., Greene, D.: A deep learning approach for selective relevance feedback. In: Advances in Information Retrieval: 46th European Conference on Information Retrieval, ECIR 2024, Glasgow, UK, March 24–28, 2024, Proceedings, Part II. p. 189–204. Springer-Verlag, Berlin, Heidelberg (2024), https://doi.org/10.1007/978-3-031-56060-6_13 18. Datta, S., MacAvaney, S., Ganguly, D., Greene, D.: A ’Pointwise-Query, Listwise- Document’ based Query Performance Prediction Approach. In: Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Infor- mation Retrieval. p. 2148–2153. SIGIR ’22, Association for Computing Machinery, New York, NY, USA (2022).https://doi.org/10.1145/3477495.3531821 19. Dubey, A., et al.: The Llama 3 Herd of Models (2024),https://arxiv.org/abs/ 2407.21783 Predicting Retrieval Utility and Answer Quality in RAG 15 20. Ebrahimi, S., Khodabakhsh, M., Arabzadeh, N., Bagheri, E.: Estimating Query Performance Through Rich Contextualized Query Representations. In: Advances in Information Retrieval. pp. 49–58. Springer Nature Switzerland, Cham (2024) 21. Faggioli, G., Ferro, N., Muntean, C.I., Perego, R., Tonellotto, N.: A Geomet- ric Framework for Query Performance Prediction in Conversational Search. In: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 1355–1365. SIGIR ’23 (2023),https: //doi.org/10.1145/3539618.3591625 22. Farquhar, S., Kossen, J., Kuhn, L., Gal, Y.: Detecting Hallucinations in Large Language Models Using Semantic Entropy. Nature (London)630(8017), 625–630 (2024) 23. Ganguly, D., Datta, S., Mitra, M., Greene, D.: An Analysis of Variations in the Effectiveness of Query Performance Prediction. In: ECIR (1). Lecture Notes in Computer Science, vol. 13185, pp. 215–229. Springer (2022) 24. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang, M., Wang, H.: Retrieval-Augmented Generation for Large Language Models: A Survey (2024),https://arxiv.org/abs/2312.10997 25. Garbacea, C., Guo, M., Carton, S., Mei, Q.: Explainable Prediction of Text Com- plexity: The Missing Preliminaries for Text Simplification. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 1086–1097. Association for Computational Linguistics, Online (Aug 2021),https://aclanthology.org/2021.acl-long.88/ 26. Gunning, R.: The Technique of Clear Writing. McGraw-Hill, New York, revised edition. edn. (1968) 27. Hayakawa, S.I.: Marks of Readable Style. American speech20(1), 63–64 (1945) 28. He, B., Ounis, I.: Query Performance Prediction. Information Systems31(7), 585– 594 (2006).https://doi.org/https://doi.org/10.1016/j.is.2005.11.003, (1) SPIRE 2004 (2) Multimedia Databases 29. Huly, O., Carmel, D., Kurland, O.: Predicting RAG Performance for Text Com- pletion. In: Proceedings of the 48th International ACM SIGIR Conference on Re- search and Development in Information Retrieval. p. 1283–1293. SIGIR ’25 (2025). https://doi.org/10.1145/3726302.3730062 30. Huly, O., Pogrebinsky, I., Carmel, D., Kurland, O., Maarek, Y.: Old IR Methods Meet RAG. In: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 2559–2563. SIGIR ’24 (2024).https://doi.org/10.1145/3626772.3657935 31. Jeong, S., Baek, J., Cho, S., Hwang, S.J., Park, J.: Adaptive-RAG: Learning to adapt retrieval-augmented large language models through question complexity. In: Proceedings of the 2024 Conference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp. 7036–7050. Association for Computational Linguistics, Mexico City, Mexico (Jun 2024),https://aclanthology.org/2024.naacl-long.389/ 32. Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A., Fung, P.: Survey of Hallucination in Natural Language Generation. ACM Comput. Surv.55(12) (Mar 2023).https://doi.org/10.1145/3571730,https://doi.org/ 10.1145/3571730 33. Jiang, Z., Xu, F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan, J., Neubig, G.: Active Retrieval Augmented Generation. In: Proceedings of the 16 Tian et al. 2023 Conference on Empirical Methods in Natural Language Processing. pp. 7969– 7992. Association for Computational Linguistics, Singapore (Dec 2023).https: //doi.org/10.18653/v1/2023.emnlp-main.495 34. Jin,B.,Zeng,H.,Yue,Z.,Yoon,J.,Arik,S.,Wang,D.,Zamani,H.,Han,J.:Search- R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning (2025),https://arxiv.org/abs/2503.09516 35. Kadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., Schiefer, N., Hatfield-Dodds, Z., DasSarma, N., Tran-Johnson, E., Johnston, S., El-Showk, S., Jones, A., Elhage, N., Hume, T., Chen, A., Bai, Y., Bowman, S., Fort, S., Gan- guli, D., Hernandez, D., Jacobson, J., Kernion, J., Kravec, S., Lovitt, L., Ndousse, K., Olsson, C., Ringer, S., Amodei, D., Brown, T., Clark, J., Joseph, N., Mann, B., McCandlish, S., Olah, C., Kaplan, J.: Language Models (Mostly) Know What They Know (2022),https://arxiv.org/abs/2207.05221 36. Kincaid, P., Fishburne, R.P., Rogers, R.L., Chissom, B.S.: Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel (1975),https://api. semanticscholar.org/CorpusID:61131325 37. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kel- cey, M., Chang, M.W., Dai, A.M., Uszkoreit, J., Le, Q., Petrov, S.: Natural ques- tions: A benchmark for question answering research. Transactions of the Associ- ation for Computational Linguistics7, 452–466 (2019),https://aclanthology. org/Q19-1026/ 38. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-Augmented Generation for Knowledge-Intensive NLP tasks. In: Proceedings of the 34th Inter- national Conference on Neural Information Processing Systems. NIPS ’20, Curran Associates Inc., Red Hook, NY, USA (2020),https://arxiv.org/abs/2005.11401 39. Liu, N.F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., Liang, P.: Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics12, 157–173 (2024).https://doi. org/10.1162/tacl_a_00638 40. Macdonald, C., Tonellotto, N., MacAvaney, S., Ounis, I.: PyTerrier: Declarative Experimentation in Python from BM25 to Dense Retrieval. In: Proceedings of the 30th ACM International Conference on Information & Knowledge Management. p. 4526–4533. CIKM ’21, Association for Computing Machinery, New York, NY, USA (2021).https://doi.org/10.1145/3459637.3482013 41. Marjanovic, S.V., Yu, H., Atanasova, P., Maistro, M., Lioma, C., Augenstein, I.: DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models. In: Findings of the Association for Computational Linguistics: EMNLP 2024. pp. 14346–14360. Association for Computational Linguistics, Miami, Florida, USA (Nov 2024).https://doi.org/10.18653/v1/2024.findings-emnlp.838 42. Nogueira, R.F., Jiang, Z., Pradeep, R., Lin, J.: Document Ranking with a Pre- trained Sequence-to-Sequence Model. In: Findings of the Association for Compu- tational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020. Findings of ACL, vol. EMNLP 2020, pp. 708–718. Association for Computational Linguistics (2020).https://doi.org/10.18653/V1/2020.FINDINGS-EMNLP.63 43. Raiber, F., Kurland, O.: Query-Performance Prediction: Setting the Expecta- tions Straight. In: Proceedings of the 37th International ACM SIGIR Confer- ence on Research & Development in Information Retrieval. p. 13–22. SIGIR Predicting Retrieval Utility and Answer Quality in RAG 17 ’14, Association for Computing Machinery, New York, NY, USA (2014).https: //doi.org/10.1145/2600428.2609581 44. Robertson, S., Walker, S., Jones, S., Hancock-Beaulieu, M.M., Gatford, M.: Okapi at TREC-3. In: Overview of the Third Text REtrieval Conference (TREC-3). pp. 109–126. Gaithersburg, MD: NIST (January 1995),https://www.microsoft.com/ en-us/research/publication/okapi-at-trec-3/ 45. Roy, N., Ribeiro, L.F.R., Blloshmi, R., Small, K.: Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA. In: Findings of the Association for Computational Linguistics: EMNLP 2024. pp. 10604–10625. Association for Computational Linguistics, Miami, Florida, USA (Nov 2024), https://aclanthology.org/2024.findings-emnlp.622/ 46. Salemi, A., Zamani, H.: Evaluating Retrieval Quality in Retrieval-Augmented Generation. In: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 2395–2400. SIGIR ’24, Association for Computing Machinery, New York, NY, USA (2024),https: //doi.org/10.1145/3626772.3657957 47. Schick, T., Dwivedi-Yu, J., Dessí, R., Raileanu, R., Lomeli, M., Hambro, E., Zettle- moyer,L.,Cancedda,N.,Scialom,T.:Toolformer:languagemodelscanteachthem- selves to use tools. In: Proceedings of the 37th International Conference on Neural Information Processing Systems. NIPS ’23, Curran Associates Inc., Red Hook, NY, USA (2023) 48. Sciavolino, C., Zhong, Z., Lee, J., Chen, D.: Simple Entity-Centric Questions Challenge Dense Retrievers. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 6138–6148. Association for Com- putational Linguistics, Online and Punta Cana, Dominican Republic (Nov 2021), https://aclanthology.org/2021.emnlp-main.496/ 49. Shorinwa, O., Mei, Z., Lidard, J., Ren, A.Z., Majumdar, A.: A Survey on Un- certainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions. ACM Comput. Surv.58(3) (Sep 2025),https: //doi.org/10.1145/3744238 50. Shtok, A., Kurland, O., Carmel, D., Raiber, F., Markovits, G.: Predicting Query Performance by Query-Drift Estimation. ACM Trans. Inf. Syst.30(2) (May 2012). https://doi.org/10.1145/2180868.2180873 51. Singh, A., Ganguly, D., Datta, S., McDonald, C.: Unsupervised query performance prediction for neural models with pairwise rank preferences. In: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Infor- mation Retrieval. p. 2486–2490. SIGIR ’23, Association for Computing Machinery, New York, NY, USA (2023),https://doi.org/10.1145/3539618.3592082 52. Sorensen, T., Robinson, J., Rytting, C., Shaw, A., Rogers, K., Delorey, A., Khalil, M., Fulda, N., Wingate, D.: An Information-theoretic Approach to Prompt Engi- neering Without Ground Truth Labels. In: Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 819–862 (May 2022),https://aclanthology.org/2022.acl-long.60/ 53. Spache,G.:ANewReadabilityFormulaforPrimary-GradeReadingMaterials.The Elementary School Journal53(7), 410–413 (1953),https://doi.org/10.1086/ 458513 54. Tian,F.,Fang,J.,Ganguly,D.,Meng,Z.,Macdonald,C.:AmIontheRightTrack? What Can Predicted Query Performance Tell Us about the Search Behaviour of Agentic RAG (2025),https://arxiv.org/abs/2507.10411 18 Tian et al. 55. Tian, F., Ganguly, D., Macdonald, C.: Is Relevance Propagated from Retriever to Generator in RAG? In: Advances in Information Retrieval: 47th European Confer- ence on Information Retrieval, ECIR 2025, Lucca, Italy, April 6–10, 2025, Proceed- ings, Part I. p. 32–48 (2025).https://doi.org/10.1007/978-3-031-88708-6_3 56. Vlachou, M., Macdonald, C.: Coherence-based Query Performance Measures for Dense Retrieval. In: Proceedings of the 2024 ACM SIGIR International Confer- ence on Theory of Information Retrieval. p. 15–24. ICTIR ’24, Association for Computing Machinery, New York, NY, USA (2024),https://doi.org/10.1145/ 3664190.3672518 57. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., Wei, F.: Text Embeddings by Weakly-Supervised Contrastive Pre-training (2024), https://arxiv.org/abs/2212.03533 58. Wang, X., Sen, P., Li, R., Yilmaz, E.: Adaptive Retrieval-Augmented Generation for Conversational Systems (2024),https://arxiv.org/abs/2407.21712 59. Xu, S., Pang, L., Shen, H., Cheng, X.: A Theory for Token-Level Harmoniza- tion in Retrieval-Augmented Generation. In: The Thirteenth International Con- ference on Learning Representations (2025),https://openreview.net/forum?id= tbx3u2oZAu 60. Zhang, H., Tang, M., Bi, K., Guo, J., Liu, S., Shi, D., Yin, D., Cheng, X.: Utility- focused LLM annotation for retrieval and retrieval-augmented generation. In: Pro- ceedings of the 2025 Conference on Empirical Methods in Natural Language Pro- cessing. pp. 1683–1702. Association for Computational Linguistics, Suzhou, China (Nov 2025),https://aclanthology.org/2025.emnlp-main.88/ 61. Zhang, H., Zhang, R., Guo, J., de Rijke, M., Fan, Y., Cheng, X.: Are large lan- guage models good at utility judgments? In: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 1941–1951. SIGIR ’24, Association for Computing Machinery, New York, NY, USA (2024),https://doi.org/10.1145/3626772.3657784